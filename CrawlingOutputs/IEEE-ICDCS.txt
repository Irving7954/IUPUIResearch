2536
The Clouds distributed operating system: functional description, implementation details and related work
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
A description is given of Clouds, an operating system designed to run on a set of general-purpose computers that are connected via a medium-to-high-speed local area network. The structure of Clouds promotes transparency, support for advanced programming paradigms, and integration of resource management, as well as a fair degree of autonomy at each site. The system structuring paradigm chosen for Clouds is an object/thread model. All instances of services, programs, and data in Clouds are encapsulated in objects. The concept of persistent objects does away with the need for file systems, replacing it with a more powerful concept, namely, the object system. The facilities in Clouds include integration of resources by location transparency; support for various types of atomic operations, including conventional transactions; advanced support for achieving fault tolerance; and provisions for dynamic reconfiguration.<<ETX>>
[resource management, fault tolerance, distributed processing, general-purpose computers, location transparency, object/thread model, Computer science, Operating systems, Computer network reliability, local area network, operating systems (computers), functional description, Computer networks, dynamic reconfiguration, Resource management, Local area networks, Clouds distributed operating system]
The design of a high-performance file server
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
The Bullet server is a file server that outperforms traditional file servers by more than a factor of three. It achieves high throughput and low delay by a software design radically different from that of file servers currently in use. Whereas files are normally stored as a sequence of disk blocks, each Bullet server file is stored contiguously, both on disk and in the server's random access memory cache. Furthermore, it uses the concept of an immutable file to improve performance, to enable caching, and to provide a clean semantic model to the user. The authors describe the design and implementation of the Bullet server in detail, present measurements of its performance, and compare this performance to that of the SUN file server running on the same hardware.<<ETX>>
[Bullet server, software design, Random access memory, distributed processing, File servers, Throughput, caching, Delay, Software design, File systems, network operating systems, file servers, SUN file server, clean semantic model, file server, immutable file, random access memory cache, low delay, Scattering, Read-write memory, performance evaluation, Data structures, high throughput, Computer science, performance measurements, disk, contiguous storage]
Bounding sequence numbers in distributed systems: a general approach
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The authors present a general methodology for bounding the range of sequence numbers utilized in a distributed program to order events. This methodology requires explicit knowledge of bounds on the rate at which processes may increment sequence numbers and the time required to transmit a message. It may also require the inclusion of additional synchronization in a distributed application which utilizes the bounded sequence numbers. The methodology is demonstrated in three contexts. It is shown how a scheme for bounding sequence numbers on requests for network mutual exclusion is consistent with the methodology. Sequence numbers are bound in a network mutual exclusion protocol. The methodology is utilized to bound the size of sequence numbers used in successive fault-tolerant broadcasts among a group of fail-stop processors. In all three cases, with totally different message-passing patterns and means of incrementing sequence numbers, consistent application of the methodology is stressed.<<ETX>>
[Heart, Protocols, sequence numbers, computer networks, distributed processing, Educational institutions, fault-tolerant broadcasts, Synchronization, Distributed computing, bounding, Computer science, Fault tolerance, network mutual exclusion protocol, Message passing, fail-stop processors, Broadcasting, message-passing patterns, distributed systems, synchronization, protocols, distributed program, message switching, event ordering, Clocks]
A statistical approach for parallel optimization with application to VLSI placement
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
A massively parallel optimization approach based on simple neighborhood search techniques was developed and applied to the problem of VLSI cell placement. Statistical models are developed to analyze the performance of the approach in general, and to derive statistical bounds on the quality of obtainable results. The results of these analyses suggest a simple framework for approximate solution of difficult problems. The approach is inherently parallel, and it can be implemented on any type of parallel computer. It was implemented on a simulated hypercube MIMD machine using Cosmic C on a network of Sun workstations. The method is empirically verified.<<ETX>>
[statistical bounds, Costs, Computational modeling, VLSI, Sun workstations, Very large scale integration, Application software, Sun, Concurrent computing, optimisation, VLSI placement, Cosmic C, Search methods, parallel optimization, neighborhood search techniques, Simulated annealing, Hypercubes, circuit layout CAD, search problems, Testing, statistical approach, simulated hypercube MIMD machine]
Efficient implementation of barrier synchronization in wormhole-routed hypercube multicomputers
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
Practical and efficient implementations of barrier synchronization for wormhole-routed hypercube multicomputers are presented. Both broadcast and multicast barrier synchronization are considered. For systems that do not support hardware broadcast or multicast, a software U-cube tree is proposed. This method generalizes to n-dimensional meshes. Performance measurements for several barrier synchronization techniques implemented on a 64-node nCUBE-2 are given.<<ETX>>
[Measurement, n-dimensional meshes, hardware broadcast, Switches, Routing, hypercube networks, wormhole-routed hypercube multicomputers, software U-cube tree, synchronisation, Computer science, performance measurements, Communication system software, Unicast, barrier synchronization, Hypercubes, 64-node nCUBE-2, Iterative algorithms, Hardware, Iterative methods]
A general architecture for load balancing in a distributed-memory environment
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
The goal of load balancing is to assign to each node a number of tasks proportional to its performance. On distributed-memory machines, it is important to take data dependencies into account when distributing tasks, since they have a big impact on the communication requirements of the distributed application. The authors present a load balancing architecture that can deal with applications with heterogeneous tasks. The idea is to provide a set of load balancers that are effective for different types of homogeneous tasks, and to allow users to combine these load balancers for applications with heterogeneous tasks. This architecture was implemented on the Nectar multicomputer and performance results are presented for several applications with homogeneous and heterogeneous tasks.<<ETX>>
[data dependencies, load balancing, distributed-memory environment, Application software, Distributed computing, communication requirements, Programming profession, Information systems, homogeneous tasks, Computer science, Program processors, resource allocation, general architecture, Computer architecture, distributed memory systems, multiprogramming, Parallel processing, Load management, Computer industry, Nectar multicomputer, distributed memory machines, performance results, heterogeneous tasks, load balancers]
A competitive analysis for retransmission timeout
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
Protocols that provide reliable communication on top of a network that can lose packets rely on periodically retransmitting packets. The choice of retransmission timeout critically affects system performance. This paper presents a first step toward a theoretical study of the choice of retransmission timeout, based on competitive analysis. In general, competitive analysis compares the performance of an on-line algorithm to the performance of an optimal off-line algorithm, which has access to more information. In this content, the job of an algorithm is to choose the retransmission timeout interval; an off-line algorithm knows the exact message delays, while an on-line algorithm only knows upper and lower bounds on the delays. The performance measure of interest is the expected value of a linear combination of the number of packets used and the amount of time elapsed. An on-line algorithm for choosing the retransmission timeout is presented that is optimal with respect to the difference between its performance and that of an optimal off-line algorithm. The algorithm is also analyzed with respect to the ratio of its performance and that of an optimal off-line algorithm.
[Algorithm design and analysis, Protocols, message delays, online algorithm, optimal offline algorithm, upper bounds, Time measurement, retransmission timeout, Delay, lower bounds, Information analysis, System performance, performance measure, delays, system performance, reliable communication, Performance analysis, Telecommunication network reliability, protocols, offline algorithm, competitive analysis]
The relative importance of concurrent writers and weak consistency models
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
This paper presents a detailed comparison of the relative importance of allowing concurrent writers versus the choice of the underlying consistency model. Our comparison is based on single- and multiple-writer versions of a lazy release consistent (LRC) protocol, and a single-writer sequentially consistent protocol, all implemented in the CVM software distributed shared memory system. We find that in our environment, which we believe to be representative of distributed systems today and in the near future, the consistency model has a much higher impact on overall performance than the choice of whether to allow concurrent writers. The multiple writer LRC protocol performs an average of 9% better than the single writer LRC protocol, but 34% better than the single-writer sequentially consistent protocol. Set against this, MW-LRC required an average of 72% memory overhead, compared to 10% overhead for the single-writer protocoIs.
[weak consistency models, paged storage, distributed shared memory, Access protocols, memory protocols, Educational institutions, Computer science, concurrent writers, protocol, distributed memory systems, shared memory systems, Hardware, LRC, lazy release consistent]
An open architecture for secure interworking services
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
There is a developing need for applications and distributed services to cooperate or interoperate. The article describes an architectural approach to security. The key idea is that a process is the universal client entity; a process may act on behalf of an identified individual as in traditional security schemes. More generally, a process may adopt an application specific name or role, and this is used as the basis for authentication in Oasis. A service may then be written in terms of service specific categories of clients, decoupled from the mechanisms used to specify and enforce access control policy. This approach allows great flexibility when integrating a number of services, and reduces the mismatch of policies that is common in heterogeneous systems. In addition, Oasis services may be integrated with alternative authentication and access control schemes, providing a truly open architecture. A flexible security definition is meaningless if not backed by a robust and efficient implementation. Oasis has been fully implemented, and is inherently distributed and scalable. We describe the general approach, then concentrate on revocation, where security designs are most often criticised. Oasis is unique in supporting the rapid and selective revocation of privileges which can cascade between services and organisations.
[Access control, Oasis services, open systems, traditional security schemes, internetworking, Fasteners, Proposals, flexible security definition, Distributed computing, access control policy, heterogeneous systems, revocation, access control schemes, authorisation, Robustness, alternative authentication, authentication, Architecture, service specific categories, open architecture, security designs, computer network management, secure interworking services, architectural approach, Authentication, Information security, message authentication, inherently distributed, application specific name, universal client entity, distributed services]
Accelerated heartbeat protocols
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
Heartbeat protocols are used by distributed programs to ensure that if a process in a program terminates or fails, then the remaining processes in the program terminate. We present a class of heartbeat protocols that tolerate message loss. In these protocols, a root process periodically sends a beat message to every other process then waits to receive a reply beat message from every other process. If the root process does not receive a reply (possibly due to message loss), the root process reduces by half the period for sending beat messages. We show that in practical situations, the parameters of these protocols can be chosen to achieve a good compromise between three contradictory objectives: reduce the rate of sending beat messages, reduce the detection delay, and still keep the probability of premature termination small.
[message loss, Protocols, message passing, fault tolerance, process termination, Delay, Read only memory, software fault tolerance, distributed programs, program termination, Heart beat, Fault detection, heartbeat protocols, detection delay, Heart rate detection, Computer networks, computer network reliability, Acceleration, protocols, Detection algorithms, beat message]
Design considerations for distributed caching on the Internet
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
We describe the design and implementation of an integrated architecture for cache systems that scale to hundreds or thousands of caches with thousands to millions of users. Rather than simply try to maximize hit rates, we take an end-to-end approach to improving response time by also considering hit times and miss times. We begin by studying several Internet caches and workloads, and we derive three core design principles for large scale distributed caches: minimize the number of hops to locate and access data on both hits and misses; share data among many users and scale to many caches; and cache data close to clients. Our strategies for addressing these issues are built around a scalable, high-performance data-location service that tracks where objects are replicated. We describe how to construct such a service and how to use this service to provide direct access to remote data and push-based data replication. We evaluate our system through trace-driven simulation and find that these strategies together provide response time speedups of 1.27 to 2.43 compared to a traditional three-level cache hierarchy for a range of trace workloads and simulated environments.
[client-server systems, Identity-based encryption, Engineering profession, Laboratories, NASA, client server system, cache storage, hit rates, trace-driven simulation, Network servers, push-based data replication, response time, Internet workloads, data-location service, Internet, Web sites, Web server, distributed caching, large scale distributed cache, Propagation delay]
Performance evaluation of soft real-time scheduling for multicomputer cluster
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
We consider scheduling soft real-time tasks for a cluster which consists of multiple servers. Each server provides identical services where an incoming task can be assigned to one of the servers. The question we investigate is which assignment produces the best performance given two alternative approaches to queue the arriving tasks: a centralized approach or a distributed one. A missed deadline criterion is used to evaluate these approaches. The influence of dispatching policies on the performance of the distributed approach is also considered.
[Real time systems, workstation clusters, centralized approach, Delay, soft real-time scheduling, Computer architecture, scheduling, Hardware, task assignment, multicomputer cluster, software performance evaluation, multiprocessing systems, multiple servers, performance evaluation, missed deadline criterion, task queueing, Computer science, distributed approach, dispatching policies, Processor scheduling, real-time systems, Games, Dispatching, Time factors, Queueing analysis]
Using predeclaration for efficient read-only transaction processing in wireless data broadcast
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Wireless data broadcast allows a large number of users to retrieve data simultaneously in mobile databases, resulting in an efficient way of using the scarce wireless bandwidth. The efficiency of data access methods, however, is limited by an inherent property that data can only be accessed strictly sequentially by users. The paper addresses the issue of ensuring consistency and currency of data items requested in a certain order by wireless read-only transactions. To properly cope with the inherent property of data broadcast, we explore a predeclaration-based query optimization and devise three predeclaration-based transaction processing methods.
[transaction processing, Protocols, radio data systems, wireless read-only transactions, Data engineering, consistency, Delay, query processing, mobile computing, Databases, data broadcast, Bandwidth, distributed databases, Broadcasting, currency, data access methods, client-server systems, wireless data broadcast, predeclaration-based query optimization, predeclaration-based transaction processing methods, Information retrieval, mobile databases, read-only transaction processing, Query processing, concurrency control, System recovery, Telephone sets]
A multi-threading model for distributed mobile objects and its realization in FarGo
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
We present a novel multi-threading programming model for frameworks that combine distributed and mobile objects, and corresponding compile-time and runtime support that realize this model. Key contributions include an automatic thread partitioning scheme that provides the abstraction of logical distributed threads, while permitting arbitrary migration of components which contain parts of the distributed threads in themselves. The distributed mobile multi-threading model is transparent to the application programmer, who uses normal syntax to encode multi-threading applications, but can lower the overhead associated with the model by specifying the migratable components. The multi-threaded model was implemented in FarGo, a Java-based middleware for dynamically-relocatable distributed applications.
[Java, automatic thread partitioning scheme, dynamically-relocatable distributed applications, multi-threading, Encoding, multithreading programming model, Yarn, Middleware, program compilers, compile-time support, Programming profession, runtime support, Condition monitoring, Mobile agents, distributed mobile objects, FarGo, Cities and towns, syntax, Large-scale systems, Logic, distributed object management, distributed programming, logical distributed threads, middleware]
Modular composition and verification of transaction processing protocols
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Establishing the correctness of reliable distributed protocols supporting critical applications necessitates modular/compositional approaches to tackle the inherent complexity of these protocols. Efforts involved in the specification and verification of these reliable distributed protocols can be considerably reduced if the protocol is composed utilizing smaller components (building-blocks) possessing individual functionalities that are integral parts of the overall protocol operation. In this paper we present the modular composition of a transaction processing protocol, namely the three-phase commit (3PC) protocol utilizing concepts of category theory. Specifically, we illustrate how the overall global properties of the protocol can be proved by utilizing constructs of local sub-properties of the inherent building blocks of the 3PC protocol.
[transaction processing, Instruments, distributed processing, Multicast protocols, Reliability engineering, transaction processing protocol, State-space methods, Application software, Formal specifications, distributed protocol, modular composition, Distributed computing, formal specification, formal verification, three-phase commit protocol, category theory, Virtual prototyping, computational complexity]
An autonomous and decentralized protocol for delay sensitive overlay multicast tree
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Here, we present a protocol for dynamically maintaining a degree-bounded delay sensitive spanning tree in a decentralized way on overlay networks. The protocol aims at repairing the spanning tree autonomously even if multiple node's leave operations or failures (disappearances) occur simultaneously or continuously in a specified period. It also aims at maintaining the diameter (maximum delay) of the tree as small as possible. The simulation results using ns-2 have shown that the protocol could keep reasonable diameters compared with the existing centralized static algorithm even if many node's participations and disappearances occur frequently.
[Technological innovation, Costs, Peer to peer computing, trees (mathematics), distributed processing, Multicast protocols, Delay, delay sensitive overlay multicast tree, Information science, Multicast algorithms, Unicast, Tree graphs, multicast protocols, autonomous decentralized protocol, Internet]
Explicit Combinatorial Structures for Cooperative Distributed Algorithms
25th IEEE International Conference on Distributed Computing Systems
None
2005
Cooperation in distributed settings often involves activities that must be performed at least once by the participating processors. When processor failures or delays occur, it becomes unavoidable that some tasks are done redundantly. To make efficient use of the available processors, several distributed algorithms schedule the activities of the processors in terms of permutations of tasks that need to be performed at least once. This paper presents the first explicit practical deterministic construction of sets of permutations with certain combinatorial properties that immediately make practical several deterministic distributed algorithms. These algorithms solve a variety of problems, for example, cooperation in shared-memory and message-passing settings, and the gossip problem. Prior to this work, the most efficient algorithms for some of these problems were primarily of theoretical interest - they relied on permutations that are known to exist, but very expensive to construct, with the cost of construction being at least exponential in the size of the permutations. In this paper, the explicitly constructed permutations are ultimately used directly to produce practical instances of several classes of efficient deterministic algorithms. Most importantly, for all of these algorithms, the schedule construction cost is reduced from exponential to polynomial, at the expense of slight detuning, at most polylogarithmic, of the efficiency of these algorithms
[Costs, combinatorial mathematics, reliability, processor failures, Distributed computing, system recovery, Delay, processor scheduling, explicit combinatorial structures, cooperative distributed algorithms, processor delays, polylogarithmic detuning, schedule construction cost reduction, Polynomials, explicit permutation construction, redundancy, Distributed algorithms, polynomials, deterministic distributed algorithms, task permutations, Scheduling algorithm, Computer science, Processor scheduling, distributed algorithms, Collaboration, deterministic construction, Artificial intelligence]
Robust Accounting in Decentralized P2P Storage Systems
26th IEEE International Conference on Distributed Computing Systems
None
2006
A peer-to-peer (P2P) storage system allows a network of peer computers to increase the availability of their data by replicating it on other peers in the network. In such networks, a central challenge is preventing "freeloaders\
[Computer science, Intelligent networks, Costs, File systems, Peer to peer computing, Prototypes, Data engineering, Robustness, Computer networks, Security]
Protocol Design and Optimization for Delay/Fault-Tolerant Mobile Sensor Networks
27th International Conference on Distributed Computing Systems
None
2007
While extensive studies have been carried out in the past several years for many sensor applications, they cannot be applied to the network with extremely low and intermittent connectivity, dubbed the delay/fault-tolerant mobile sensor network (DFT-MSN). Without end-to-end connections due to sparse network density and sensor node mobility, routing in DFT-MSN becomes localized and ties closely to medium access control, which naturally calls for merging Layer 3 and Layer 2 protocols in order to reduce overhead and improve network efficiency. DFT-MSN is fundamentally an opportunistic network, where the communication links exist only with certain probabilities and become the scarcest resource. At the same time, the sensor nodes in DFT-MSN have very limited battery power like those in other sensor networks. Clearly, there is a tradeoff between link utilization and energy efficiency. To address this tradeoff, we develop a cross-layer data delivery protocol for DFT-MSN, which includes two phases, i.e., the asynchronous phase and the synchronous phase. In the first phase, the sender contacts its neighbors to identify a set of appropriate receivers. Since no central control exists, the communication in the first phase is contention-based. In the second phase, the sender gains channel control and multicasts its data message to the receivers. Furthermore, several optimization issues in these two phases are identified, with solutions provided to reduce the collision probability and to balance between link utilization and energy efficiency. Our results show that the proposed cross-layer data delivery protocol for DFT-MSN achieves a high message delivery ratio with low energy consumption and an acceptable delay.
[cross-layer data delivery protocol, fault tolerance, wireless sensor networks, Merging, Communication system control, Access protocols, sparse network density, medium access control, Batteries, access protocols, delay fault-tolerant mobile sensor network, Design optimization, Centralized control, Fault tolerance, optimisation, sensor node mobility, telecommunication network routing, Media Access Protocol, Energy efficiency, Routing protocols]
Identifying Frequent Items in P2P Systems
2008 The 28th International Conference on Distributed Computing Systems
None
2008
As peer-to-peer (P2P) systems receive growing acceptance, the need of identifying 'frequent items' in such systems appears in a variety of applications. In this paper, we define the problem of identifying frequent items (IFI) and propose an efficient in-network processing technique, called in-network filtering (netFilter), to address this important fundamental problem. netFilter operates in two phases: 1) candidate filtering: data items are grouped into item groups to obtain aggregates for pruning of infrequent items; and 2) candidate verification: the aggregates for the remaining candidate items are obtained to filter out false frequent items. We address various issues faced in realizing netFilter, including aggregate computation, candidate set optimization, and candidate set materialization. In addition, we analyze the performance of netFilter, derive the optimal setting analytically, and discuss how to achieve the optimal setting in practice. Finally, we validate the effectiveness of netFilter through extensive simulation.
[in-network filtering, Filtering, peer-to-peer computing, identifying frequent items problem, Peer to peer computing, Maintenance engineering, information filtering, Partitioning algorithms, netFilter, Optimization, Heart beat, Aggregates, peer-to-peer systems, P2P systems]
CAP: A Context-Aware Privacy Protection System for Location-Based Services
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
We address issues related to privacy protection in location-based services (LBS). Most existing research in this field either requires a trusted third-party (anonymizer) or uses oblivious protocols that are computationally and communicationally expensive. Our design of privacy-preserving techniques is principled on not requiring a trusted third-party while being highly efficient in terms of time and space complexities. The problem has two interesting and challenging characteristics: First, the degree of privacy protection and LBS accuracy depends on the context, such as population and road density, around a user's location. Second, an adversary may violate a user's location privacy in two ways: (i) based on the user's location information contained in the LBS query payload, and (ii) by inferring a user's geographical location based on its device's IP address. To address these challenges, we introduce CAP, a Context-Aware Privacy-preserving LBS system with integrated protection for data privacy and communication anonymity. We have implemented CAP and integrated it with Google Maps, a popular LBS system. Theoretical analysis and experimental results validate CAP's effectiveness on privacy protection, LBS accuracy, and communication Quality-of-Service.
[privacy-preserving techniques, Protocols, Google maps, communication anonymity, Mobile communication, ubiquitous computing, Distributed computing, Location-Based Service, Privacy, context-aware privacy protection system, trusted third-party, geographical location, location-based services, Protection, quality-of-service, Context-aware services, Context, oblivious protocols, Anonymity, Information retrieval, Spatial databases, IP address, quality of service, Global Positioning System, data privacy]
Nefeli: Hint-Based Execution of Workloads in Clouds
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Virtualization of computer systems has made feasible the provision of entire distributed infrastructures in the form of services. Such services do not expose the internal operational and physical characteristics of the underlying machinery to either users or applications. In this way, infrastructures including computers in data-centers, clusters of workstations, and networks of machines are shrouded in "clouds". Mainly through the deployment of virtual machines, such networks of computing nodes become cloud-computing environments. In this paper, we propose Nefeli, a virtual infrastructure gateway that is capable of effectively handling diverse workloads of jobs in cloud environments. By and large, users and their workloads remain agnostic to the internal features of clouds at all times. Exploiting execution patterns as well as logistical constraints, users provide Nefeli with hints for the handling of their jobs. Hints provide no hard requirements for application deployment in terms of pairing virtual-machines to specific physical cloud elements. Nefeli helps avoid bottlenecks within the cloud through the realization of viable virtual machine deployment mappings. As the types of jobs change over time, deployment mappings must follow suit. To this end, Nefeli offers mechanisms to migrate virtual machines as needed to adapt to changing performance needs. Using our prototype system, we show significant improvements in overall time needed and energy consumed for the execution of workloads in both simulated and real cloud computing environments.
[Cloud computing, virtual machine deployment mappings, virtual infrastructure gateway, distributed processing, hint based execution, Virtual machining, Nefeli, job handling, Application software, Distributed computing, Machinery, Voice mail, cloud computing environment, supervisory programs, computer systems virtualization, virtual machines, Computer networks, Workstations, Resource management, Virtual prototyping, distributed infrastructures]
SOFA: A Sleep-Optimal Fair-Attention Scheduler for the Power-Saving Mode of WLANs
2011 31st International Conference on Distributed Computing Systems
None
2011
Mobile devices adopt the IEEE 802.11 PSM (Power-Saving Mode) scheme and its enhancements to reduce their energy consumption when using Wi-Fi interfaces. However, the capability of PSM to save energy is limited when the WLANs are highly congested by other Wi-Fi clients. In this paper, instead of further pursuing the trade-off between power saving and the incurred delay on the client side, we take a different approach and explore the energy saving potential by considering the scheduling policy on the Access Point (AP) side. We find that the traditional packet-level first-come-first-serve policy is not sleep optimal since it keeps the PSM clients awake unnecessarily. We propose SOFA, an AP-centric scheme, which helps PSM clients save energy by minimizing the time they are forced to stay awake while down link traffic is being transmitted to other clients. SOFA delivers down link packets to the PSM clients in an optimal sequence, such that several objective are simultaneously achieved: (i) system-sleep optimality, (ii) energy-fairness, (iii) attention fairness, and (iv) no unnecessary deferral of packets beyond a beacon period. First, it determines an attention quota for each client at the beginning of each beacon period, without requiring any knowledge of available wireless capacity. Then it takes the attention "quota'' and attention request as inputs to decide the down link packet scheduling. We prove the stability and optimality of SOFA. Simulation results shows SOFA dramatically decreases the energy consumption of PSM clients in a crowded WLAN, especially for those clients with small attention requests.
[IEEE 802.11, Energy consumption, sleep-optimal, Downlink, downlink traffic, power-saving, Delay, Wireless communication, Wi-Fi interfaces, power-saving mode clients, access point-centric scheme, IEEE 802.11 Standards, Computer aided manufacturing, energy-fairness, system-sleep optimality, WLAN, attention fairness, Sleep, sleep-optimal fair-attention scheduler, mobile devices, MAC protocol, traditional packet-level first-come-first-serve policy, wireless LAN, telecommunication traffic, mobile handsets]
Shuffling with a Croupier: Nat-Aware Peer-Sampling
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Despite much recent research on peer-to-peer (P2P) protocols for the Internet, there have been relatively few practical protocols designed to explicitly account for Network Address Translation gateways (NATs). Those P2P protocols that do handle NATs circumvent them using relaying and hole-punching techniques to route packets to nodes residing behind NATs. In this paper, we present Croupier, a peer sampling service (PSS) that provides uniform random samples of nodes in the presence of NATs in the network. It is the first NAT-aware PSS that works without the use of relaying or hole-punching. By removing the need for relaying and hole-punching, we decrease the complexity and overhead of our protocol as well as increase its robustness to churn and failure. We evaluated Croupier in simulation, and, in comparison with existing NAT-aware PSS', our results show similar randomness properties, but improved robustness in the presence of both high percentages of nodes behind NATs and massive node failures. Croupier also has substantially lower protocol overhead.
[NAT, Protocols, Gossip Peer Sampling, P2P Networks, Peer to peer computing, Estimation, Internet, IP networks, Servers, Relays]
Watchmen: Scalable Cheat-Resistant Support for Distributed Multi-player Online Games
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Multi-player online games are inherently distributed applications, and a wide range of distributed architectures have been proposed. However, only few successful commercial systems follow such approaches, even given their benefits, due to one main hurdle: the easiness with which cheaters can disrupt the game state computation and dissemination, perform illegal actions, or unduly gain access to sensitive information. The challenge is that any measures used to address cheating must meet the heavy scalability and tight latency requirements of fast paced games. We propose Watchmen, the first distributed scalable protocol designed with cheat detection and prevention in mind that supports fast paced games. It is based on a randomized dynamic proxy scheme for both the dissemination and verification of actions. Furthermore, Watchmen reduces the information exposed to players close to the minimum required to render the game. We build our proof-of-concept prototype on top of Quake III. We show that Watchmen, while scaling to hundreds of players and meeting the tight latency requirements of first person shooter games, is able to significantly reduce opportunities to cheat, even in the presence of collusion.
[Fast Paced, Quake III, Avatars, Scalability, Subscriptions, distributed processing, Cheat Detection, FPS, action dissemination, Servers, Security, scalable cheat-resistant support, Cheat Prevention, cheat detection, computer games, distributed multiplayer online games, Bandwidth, action verification, illegal actions, protocols, randomized dynamic proxy scheme, Watchmen, cheat prevention, Peer-to-Peer, first person shooter games, Distributed Scalable Protocol, game state dissemination, distributed scalable protocol, Multi-player Games, game state computation, Games, distributed architectures]
STI-BT: A Scalable Transactional Index
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
In this article we present STI-BT, a highly scalable, transactional index for Distributed Key-Value (DKV) stores. STI-BT is organized as a distributed B+Tree and adopts an innovative design that allows to achieve high efficiency in large-scale, elastic DKV stores. We have implemented STI-BT on top of a mainstream open-source DKV store and deployed it on a public cloud infrastructure. Our extensive experimental study reveals the efficiency of our solution with demonstrable scalability in a cluster of 100 commodity machines, and speed ups with respect to state of the art solutions of up to 5.4x.
[Context, transaction processing, Scalability, public domain software, Strong consistency, distributed B+Tree, scalable transactional index, open-source DKV store, Concurrent computing, public cloud infrastructure, Serializable Transactions, database indexing, STI-BT, Data locality, distributed databases, distributed key-value stores, Peer-to-peer computing, tree data structures, cloud computing, Indexing, Clocks, Distributed B+ Tree]
A Novel Dynamic En-Route Decision Real-Time Route Guidance Scheme in Intelligent Transportation Systems
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
In an intelligence transportation system (ITS), to increase traffic efficiency, a number of dynamic route guidance schemes have been designed to assist drivers in determining the optimal route for their travels. In order to determine optimal routes, it is critical to effectively predict the traffic condition of roads along the guided routes based on real-time traffic information to mitigate traffic congestion and improve traffic efficiency. In this paper, we propose a Dynamic En-route Decision real-time Route guidance (DEDR) scheme to effectively mitigate road congestion caused by the sudden increase of vehicles and reduce travel time. Particularly, DEDR considers real-time traffic information generation and transmission. Based on the shared traffic information, DEDR introduces Trust Probability to predict traffic conditions and dynamically en-route determine alternative optimal routes. In addition, DEDR considers multiple metrics to comprehensively assess traffic conditions and drivers can determine optimal route with individual preference of these metrics during travel. DEDR also considers effects of external factors (e.g., Bad weather, incidents, etc.) on traffic conditions. Through a combination of extensive theoretical analysis and simulation experiments, our data shows that DEDR can greatly increase the efficiency of an ITS in terms of great time efficiency and balancing efficiency in comparison with existing schemes.
[road traffic, Real-time traffic information, balancing efficiency, En-route guided route decision, Conferences, Dynamic route Guidance Systems, intelligent transportation systems, ITS, Intelligence transportation systems, dynamic en-route decision real-time route guidance scheme, Distributed computing, trust probability, road congestion mitigation, traffic information generation, external factors, real-time systems, optimal routes, DEDR scheme, time efficiency, road traffic condition, intelligence transportation system]
Virtualized Network Coding Functions on the Internet
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Network coding is a fundamental tool that enables higher network capacity and lower complexity in routing algorithms, by encouraging the mixing of information flows in the middle of a network. Implementing network coding in the core Internet is subject to practical concerns, since Internet routers are often overwhelmed by packet forwarding tasks, leaving little processing capacity for coding operations. Inspired by the recent paradigm of network function virtualization, we propose implementing network coding as a new network function, and deploying such coding functions in geo-distributed cloud data centers, to practically enable network coding on the Internet. We target multicast sessions (including unicast flows as special cases), strategically deploy relay nodes (network coding functions) in selected data centers between senders and receivers, and embrace high bandwidth efficiency brought by network coding with dynamic coding function deployment. We design and implement the network coding function on typical virtual machines, featuring efficient packet processing. We propose an efficient algorithm for coding function deployment, scaling in and out, in the presence of system dynamics. Real-world implementation on Amazon EC2 and Linode demonstrates significant throughput improvement and higher robustness of multicast via coding functions as well as efficiency of the dynamic deployment and scaling algorithm.
[Cloud computing, Amazon EC2, Heuristic algorithms, information flow mixing, Network Function Virtualization, Throughput, virtualisation, relay nodes, network capacity, cloud computing, packet forwarding tasks, bandwidth efficiency, network coding, Internet routers, multicast robustness, virtualized network coding functions, core Internet, Encoding, computer centres, dynamic coding function deployment, Linode, Network Coding, telecommunication network routing, virtual machines, Network coding, Peer-to-peer computing, Internet, geo-distributed cloud data centers]
Riptide: Jump-Starting Back-Office Connections in Cloud Systems
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Large-scale cloud networks are constantly driven by the need for improved performance in communication between datacenters. Indeed, such back-office communication makes up a large fraction of traffic in many cloud environments. This communication often occurs frequently, carrying control messages, coordination and load balancing information, and customer data. However, ensuring such inter-datacenter traffic is delivered efficiently requires optimizing connections over large physical distances, which is non-trivial. Worse still, many large cloud networks are subject to complex configuration and administrative restrictions, limiting the types of solutions that can be implemented. In this paper, we propose improving the efficiency of datacenter to datacenter communication by learning the congestion level of links in between. We then use this knowledge to inform new connections made between the relevant datacenters, allowing us to eliminate the overhead associated with traditional slow-start processes in new connections. We further present Riptide, a tool which implements this approach. We present the design and implementation details of Riptide, showing that it can be easily executed on modern Linux servers deployed in the real world. We further demonstrate that it successfully reduces total transfer times in a production global-scale content delivery network (CDN), providing up to a 30% decrease in tail latency. We further show that Riptide is simple to deploy and easy to maintain within a complex existing network.
[Cloud computing, Protocols, complex networks, CDN, large-scale cloud networks, Linux servers, resource allocation, cloud systems, administrative restrictions, Production, customer data, global-scale content delivery network, datacenter to datacenter communication, cloud computing, slow-start processes, load balancing information, Riptide, Maintenance engineering, cloud environments, interdatacenter traffic, computer centres, cloud networks, Linux, Load management, Delays, back-office communication, jump-starting back-office connections]
Microeconomic algorithms for load balancing in distributed computer systems
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
A novel approach to allocating and sharing communication and computational resources in a distributed system is described. The approach, which is based on concepts drawn from microeconomics, uses algorithms that are competitive rather than cooperative. The effectiveness of these concepts is demonstrated by describing an economy that improves the performance of a distributed system by implementing load balancing. In this economy, competition sets prices for the resources in the system. Jobs complete for the resources by issuing bids, and the resource allocation decisions are made through auctions held by the processors. The benefits of the method include limited complexity and algorithms that are intrinsically decentralized and modular. Simulation studies show that these economies achieve substantial performance benefits.<<ETX>>
[distributed computer systems, microeconomics, load balancing, Computational modeling, Computer simulation, Power generation economics, distributed processing, performance evaluation, Distributed computing, Computer science, supervisory programs, performance benefits, resource allocation decisions, Bandwidth, Microeconomics, Parallel processing, Load management, Resource management, auctions]
Adaptive optimal load balancing in a heterogeneous multiserver system with a central job scheduler
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
The authors formally state and discuss the response time minimization problem. They show the equivalence of this problem to a weighted least square load balancing problem. They also discuss the correct solution of these problems. Next they describe and analyze a class of server idle time measurements. Finally, a class of adaptive algorithms is presented, and their performance is studied via simulation experiments.<<ETX>>
[Algorithm design and analysis, multiprocessor systems, distributed processing, performance evaluation, Routing, Time measurement, response time minimization, Delay, Least squares methods, Scheduling algorithm, simulation experiments, Multiprocessing systems, Convergence, queueing problems, heterogeneous multiserver system, server idle time measurements, adaptive optimal load balancing, Load management, adaptive algorithms, Performance analysis, weighted least square load balancing problem, central job scheduler]
Evaluation dynamic processing of distributed queries
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
Several issues that arise in processing distributed queries dynamically are investigated. The effect of inaccuracies in estimation on the cost/delay of strategies is considered. Two methods than can be used to decide whether or not to correct a strategy are examined and compared. The problem of aborting operations in process is also examined, i.e. whether operations which are in progress should be aborted or whether they should be permitted to complete.<<ETX>>
[dynamic processing, cost, Heuristic algorithms, Delay estimation, Data engineering, Computer science, Condition monitoring, query processing, distributed queries, delay, Query processing, Councils, Distributed databases, distributed databases, Cost function, Dynamic programming, inaccuracies, aborting operations]
Query optimization in distributed logic-oriented object bases
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
A logic-oriented object base is defined to be a deductive database based on an object data model. Like conventional databases, logic-orientated object bases can be constructed on top of computer networks in such a way that the distribution of logical and physical components of the system is kept hidden from the users. Objects are organized hierarchically and are retrieved through customized methods. The problem of query optimization in such an environment is investigated.<<ETX>>
[computer networks, Relational databases, Information retrieval, Partitioning algorithms, Intelligent networks, optimisation, conjunctive queries, query optimization, Query processing, object based systems, distributed databases, deductive database, Cost function, Data models, Computer networks, Database systems, Logic, distributed logic-oriented object bases, object data model]
On the communication cost of distributed database processing
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
Various communication aspects of locally distributed database processing are studied, using some distributed sorting and distributed hash-based join algorithms as examples. The algorithms are implemented on diskless workstations connected by an Ethernet network to simulate a distributed main memory system environment. This experimental testbed is described. Raw communication performance data (i.e. memory-to-memory data transfer timing) are presented. The effects of the underlying distributed operating system and the speed of the processor on the communication performance are shown. Two distributed sorting algorithms are used as examples to study the issue of concurrent transmissions of messages. Distributed hash join is used as a case study for communication/local-processing tradeoff. The idea of load sharing among a number of sites to speed up the join operation is introduced.<<ETX>>
[Context, concurrent message transmission, Ethernet networks, Computational modeling, Magnetic resonance, distributed main memory system environment, performance evaluation, distributed sorting algorithms, communication cost, memory-to-memory data transfer timing, Ethernet network, Sorting, Operating systems, distributed database processing, distributed operating system, Distributed databases, distributed databases, sorting, diskless workstations, Parallel processing, Cost function, communication performance data, Workstations, distributed hash-based join algorithms]
Modeling and performance bounds for concurrent processing
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
A novel graph-theoretic model for describing the relation between a decomposed algorithm and its execution in a multiprocessor environment is developed. Called ATAMM, the model consists of a set of Petri-net marked graphs that incorporates the general specifications of a data-flow architecture. The model is useful for representing decision-free algorithms having large-grained, computationally complex primitive operations. Performance measures of computing speed and throughput capacity are defined. The ATAMM model is used to develop analytically lower bounds for these parameters.<<ETX>>
[multiprocessing systems, multiprocessing programs, Process control, Communication system control, performance evaluation, Throughput, decision-free algorithms, Parallel architectures, Flow graphs, Application software, graph-theoretic model, Petri-net marked graphs, concurrent processing, throughput capacity, directed graphs, ATAMM, Signal processing algorithms, Computer architecture, computing speed, data-flow architecture, Data flow computing, Performance analysis, performance bounds, multiprocessor environment]
Performance analysis of degradable multiprocessor system using time scale decomposition of GSPNs
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
A time-scale decomposition (TSD) algorithm of a class of generalized stochastic Petri net (GSPN) models for performance evaluation of degradable multiprocessor systems is presented. The failure and repair rates in a multiprocessor system are an order of magnitude smaller than the rates of job arrival and completion; therefore, the GSPN models of these systems are decomposed into a hierarchical sequence of aggregated subnets, each of which is valid at a certain time scale. These smaller subnets are involved in isolation, and their solutions are combined to get the solution of the whole system. The algorithm is described step-by-step using a simple example. Then a large degradable multiprocessor system, which is intractable using conventional GSPN solution techniques, is analyzed. The complexity of the TSD algorithm is an order of magnitude smaller.<<ETX>>
[Availability, multiprocessing systems, Electric breakdown, Petri nets, shared memory multiprocessor systems, performance evaluation, Time measurement, State-space methods, Multiprocessing systems, Degradation, Concurrent computing, generalized stochastic Petri net, Stochastic systems, directed graphs, repair rates, time scale decomposition, fault tolerant computing, Performance analysis, degradable multiprocessor system]
Network disconnection in distributed systems
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
A probabilistic measure of network fault-tolerance expressed as the probability of occurrence of a disconnection is proposed. A simple analytical approximation to the disconnection probability is derived and verified using a Monte Carlo simulation. The effects of the disconnection probability on the overall system's reliability are analyzed using a proposed measure of network resilience.<<ETX>>
[multiprocessing systems, Multiprocessor interconnection networks, network fault-tolerance, computer networks, probabilistic measure, performance evaluation, disconnection probability, Resilience, Intelligent networks, Fault tolerance, Network topology, network resilience, Fault tolerant systems, Failure analysis, analytical approximation, Computer networks, fault tolerant computing, Large-scale systems, Reliability, Monte Carlo simulation]
Query scheduling and site selection algorithms for a cube-connected multicomputer system
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
Query scheduling and site selection algorithms for read-only queries on a cube-connected multicomputer are presented. An architecture model for the system is provided, and a site-selection algorithm that determines where to execute the upcoming operation sequence is developed. The query trees of queries entering the system are converted into operation sequence trees. Operation sequences belonging to a query are queued until the query is initiated. Two query selection policies are presented. A simulation comparing the two policies is described, and the simulation results are discussed. Scheduling restrictions that guarantee the avoidance of deadlock in both algorithms are presented.<<ETX>>
[Job shop scheduling, multiprocessing systems, trees (mathematics), simulation, Relational databases, avoidance of deadlock, Topology, site selection algorithms, Application software, trees, Scheduling algorithm, query scheduling, cube-connected multicomputer system, Bandwidth, System recovery, scheduling, Hypercubes, read-only queries, Hardware, Assembly]
Near optimal embedding of binary tree architecture in VLSI
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
An efficient scheme is presented for embedding a complete binary tree architecture in a two-dimensional array of processing elements. The scheme utilizes almost 100% of the processing elements in the array as actual computing elements, with small and asymptotically optimal propagation delay. The maximum edge length is optimal for trees with up to six levels. The scheme is compared with other designs proposed in the literature and shown to be significantly better.<<ETX>>
[Dictionaries, VLSI, maximum edge length, two-dimensional array, trees (mathematics), Very large scale integration, asymptotically optimal propagation delay, near optimal embedding, binary tree architecture, Application software, Distributed computing, Tiles, computer architecture, Binary trees, Computer architecture, Parallel processing, processing elements, Silicon, actual computing elements, Propagation delay]
Data editing: faster convergence for synchronous approximate agreement
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
The protocol for synchronous approximate agreement presented by D. Dolev, et al., (J. ACM, vol.33, no. 3, pp. 499-516, 1986) exhibits the undesirable property that a fault processor, by the dissemination of a value far removed from the values held by good processors, can delay the termination of the protocol by an arbitrary amount of time. Such behavior is clearly undesirable in a fault-tolerant control system subject to hard real-time constraints. A mechanism is proposed by which discarding data suspected of being from failed processors can lead to quicker, predictable, convergence on an agreement value. Under specific assumptions about the nature of values transmitted by failed processors relative to those transmitted by good processors, Monte Carlo simulation results are presented that illuminate the tradeoff between accelerated convergence and the accuracy of the agreed-upon value.<<ETX>>
[Actuators, Protocols, fault processor, Redundancy, NASA, fault-tolerant control system, real-time constraints, Aerospace electronics, distributed processing, Educational institutions, synchronous approximate agreement, Convergence, Resilience, Computer science, protocol, real-time systems, accelerated convergence, Hardware, fault tolerant computing, protocols, failed processors]
Dynamic partitioning in a class of parallel systems
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
Systems that have the capability of executing tasks with various sizes and computation structures simultaneously are considered. When some of the tasks are completed, part of the system becomes idle and is reconfigured for new tasks. Inappropriate reconfiguration strategies can create resource fragments and result in a loss of computation power. This problem can be alleviated by partitioning the system dynamically. Partitioning consists of two processes: splitting the system or a subsystem into smaller subsystems, and combining unallocated subsystems into larger subsystems. These two processes are investigated analytically for a lattice model that uses a special partial ordering relation on a set. The complexities of these two processes are also analyzed. The method can be applied to any system that can be modeled by a lattice.<<ETX>>
[resource fragments, Costs, Multiprocessor interconnection networks, Lattices, Power system modeling, parallel processing, Computer science, Concurrent computing, parallel systems, dynamic partitioning, lattice model, Computer networks, Hardware, Large-scale systems, Resource management]
Reliable group communication in distributed systems
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
The design and implementation of a reliable group communication mechanism is presented. The mechanism guarantees a form of atomicity in that the messages are received by all operational members of the group or by none of them. Since the overhead in enforcing the order of messages is nontrivial, the mechanism provides two types of message transmission: one guarantees delivery of the messages in the same order to all members of a group, and the other guarantees only atomicity with messages delivered in some arbitrary order. The message-ordering property can be used to simplify distributed database and distributed processing algorithms. The mechanism can survive despite process, host, and communication failures.<<ETX>>
[distributed database, message transmission, atomicity, computer networks, reliability, Maintenance engineering, Reliability engineering, message-ordering, distributed processing algorithms, reliable group communication mechanism, Computer science, Distributed processing, Fault tolerance, Multicast algorithms, Councils, arbitrary order, Distributed databases, distributed databases, Aging, distributed systems, fault tolerant computing, Workstations, protocols]
An interconnection network that exploits locality of communication
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
Several patterns of structure and locality of communication among software components assigned to processors are considered. In each case, a mapping between components and processors is identified so that high-volume communication paths correspond to connections supported most efficiently by the interconnection network. It is shown that locality of communication can have a profound effect on the efficiency of communication in a multicomputer. Several types of multiprocessors and multicomputers being designed for applications in artificial intelligence are likely to exhibit locality of communication of a form suitable for use by such interconnection networks.<<ETX>>
[mapping, Multiprocessor interconnection networks, multiprocessor interconnection networks, locality, Routing, Application software, multicomputer, Programming profession, artificial intelligence, Concurrent computing, Program processors, interconnection network, computer architecture, Binary trees, Computer applications, Large-scale systems, communication, software components, multiprocessors, Artificial intelligence]
A multi-access window protocol for transmission of time constrained messages
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
A novel window protocol for transmitting time-constrained messages in a multiaccess network is proposed that explicitly takes time constraints into account. The window is formed on the basis of latest time to send a message (LS). A newly arriving message is immediately considered for transmission if its LS is less than those of the all pending messages in the system. As a result, the protocol closely approximates the optimal minimum-laxity-first policy. A performance evaluation by simulation shows that the protocol performs well in a wide range of environments, even under overloaded conditions.<<ETX>>
[Real time systems, Performance evaluation, data communication systems, overloaded conditions, simulation, Access protocols, performance evaluation, Educational institutions, Mathematics, digital simulation, History, Information science, Analytical models, multiaccess network, optimal minimum-laxity-first policy, time constrained messages, multi-access systems, Performance analysis, multi-access window protocol, Time factors, protocols]
A parallel algorithm for colouring graphs
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
A simple and effective parallel algorithm to color the nodes of a graph is presented. The algorithm is useful in its own right. It also provides a vehicle to demonstrate the tradeoffs between performance and speed implicit in many parallel algorithms. The algorithm is used when the parallel machine has at least as many processors as there are nodes to color.<<ETX>>
[performance-speed tradeoffs, parallel algorithms, Optimizing compilers, Communication system control, parallel algorithm, Very large scale integration, Parallel machines, performance evaluation, Mathematics, Graph theory, Parallel algorithms, graph colouring, Vehicles, Trademarks, Resource management]
On the bandwidth of a multi-stage network in the presence of faulty components
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
The authors analyze the performance (over time) of multistage multiprocessors in the presence of faults. A commonly used measure for the performance of an interconnection network is the bandwidth, BW(t), defined as the expected number, at time, t, of requests for the shared memory which are accepted per cycle. They present two models for calculating the bandwidth of the multistage network. These models generalize previously suggested models to allow the presence of faulty links, faulty processors, and faulty memories. The first model is computationally simple but too pessimistic, since it assumes that a memory request blocked by the network is lost. The second model assumes that when a processor's memory request is blocked, it reissues its request in the consecutive network cycle. This model which is computationally more complex, provides an upper estimate for the network bandwidth. The second model allows the calculation of other measures for the system's performance.<<ETX>>
[Multiprocessor interconnection networks, bandwidth, multiprocessor interconnection networks, Switches, Very large scale integration, performance evaluation, faulty processors, Power system modeling, Degradation, multistage network, Intelligent networks, faulty components, upper estimate, System performance, multistage multiprocessors, Bandwidth, fault tolerant computing, Performance analysis, Numerical models, shared memory, faulty memories]
Reliable broadcast in networks with nonprogrammable servers
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
The problem of implementing reliable broadcast in ARPA-like computer networks is studied. The environment is characterized by the absence of any multicast facility on the communication by the absence of any multicast facility on the communications subnetwork level. Thus, broadcast has to be implemented directly on hosts. A reliable broadcast protocol is presented and evaluated with respect to several important performance criteria.<<ETX>>
[Protocols, Laboratories, computer networks, Telecommunication traffic, reliability, performance evaluation, Application software, Computer science, Intelligent networks, Network servers, reliable broadcast protocol, Computer network reliability, performance criteria, Broadcasting, ARPA-like computer networks, reliable broadcast, Telecommunication network reliability, protocols, nonprogrammable servers]
DAPHNE: support for distributed applications programming in heterogeneous computer networks
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
A description is given of DAPHNE, a system of tools and run-time support routines that allow programs to be broken into parts for distributed execution on different nodes of a heterogeneous computer network. This approach serves as a natural basis for classical network services such as remote file access or remote login while at the same time allowing arbitrary distributed applications to be written in a standard programming language. The pivot of DAPHNE is a remote procedure call mechanism that is specifically adapted to a heterogeneous environment, notably heterogeneous systems software. The current language context of DAPHNE is Modula-2. Prototype software exists for a local area network including Unix and MS-DOS systems.<<ETX>>
[Unix, distributed processing, remote procedure call mechanism, MS-DOS, run-time support routines, Intelligent networks, Runtime, Operating systems, distributed execution, Computer networks, System software, heterogeneous systems software, Mathematical programming, computer networks, remote login, Application software, remote file access, High level languages, DAPHNE, Computer languages, local area network, Open systems, Modula-2, heterogeneous computer networks, programming environments, distributed applications programming]
Dynamic page migration in multiprocessors with distributed global memory
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
Multiprocessor architectures using point-to point interconnects and executing parallel algorithms require careful partitioning and allocation of data and processes. For a given interconnection, optimal allocation may be difficult to achieve in some cases because the data-dependent behavior of some algorithms may be impossible to predict. A mechanism called the pivot mechanism is introduced and described that controls the dynamic migration of data pages between neighboring memory modules during program execution. Flexible data migration serves the dual purpose of making algorithms less machine-specific and making possible efficient algorithm execution that is impossible to achieve by using static data allocation.<<ETX>>
[Context, distributed global memory, parallel algorithms, Costs, multiprocessing systems, virtual storage, distributed processing, Data engineering, data-dependent behavior, Parallel algorithms, Distributed computing, Programming profession, Concurrent computing, data allocation, multiprocessor architectures, point-to point interconnects, Program processors, Prototypes, Computer architecture, dynamic page migration, partitioning]
A software architecture for network communication
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
A communication abstraction called real-time message streams (RMS) is proposed. An RMS is a simplex (unidirectional) stream with several performance and security parameters that express the needs of RMS clients and the capabilities of the RMS provider. RMS providers can eliminate unnecessary or redundant work and can optimally schedule resources; RMS clients can use the parameters to select optimal methods for achieving reliability and flow control. RMS is the communication primitive of the DASH distributed system, which is currently being developed. The role of RMS in the DASH communication architecture and techniques and algorithms for providing RMS at different system levels are described.<<ETX>>
[Real time systems, real-time message streams, distributed processing, network communication, Communication system security, Distributed computing, software architecture, Software architecture, Computer network reliability, communication abstraction, Computer architecture, DASH distributed system, reliability control, RMS clients, software engineering, Large-scale systems, unidirectional stream, Data security, data communication systems, performance parameters, RMS provider, simplex stream, flow control, security parameters, Computer science, real-time systems, DASH communication architecture, Telecommunication network reliability]
Voting with ghosts
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
A mechanism called voting with ghosts (VWG) is proposed for maintaining consistency of replicated data. VWG is an improvement of the weighted voting (WV) algorithm; it performs as well as the available copies (AC) algorithm, but unlike AC, works correctly in the face of network partitioning. A detailed description of the VWG method is given, and it is analyzed in the presence of node crashes and network partitions. Its performance is compared with that of WV and AC.<<ETX>>
[Availability, weighted voting, Ethernet networks, node crashes, performance evaluation, available copies, Computer crashes, Partitioning algorithms, Computer science, Voting, Operating systems, distributed databases, network partitioning, fault tolerant computing, Token networks, voting with ghosts, maintaining consistency, replicated data]
Breakpoints and halting in distributed programs
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
Interactive debugging requires that the programmer be able to half a program at interesting points in its execution. The authors define distributed breakpoints and present an algorithm for implementing the detection points and an algorithm for halting a distributed program in a consistent state. Events that can be partially ordered are defined as detectable and form the basis for the breakpoint predicates. From the breakpoint definition, an algorithm is obtained that can be used in a distributed debugger to detect these breakpoints. The halting algorithm extends K.M. Chandy and L. Lamport's (1985) algorithm for recording global state and solves the problem of processes that are not fully connected or frequently communicating.<<ETX>>
[program debugging, Event detection, Debugging, distributed processing, distributed debugger, Yarn, Programming profession, Delay, distributed programs, detection points, distributed breakpoints, breakpoint predicates, Detectors, breakpoint definition, interactive debugging, halting algorithm, Contracts, algorithm]
Hierarchical object groups in distributed operating systems
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
A description is given of a novel concept and mechanism, called an object group, that realizes hierarchical, integrated object processing and management. Object groups can be used as a software basis for constructing object-based distributed systems. They support hierarchical control of objects, group-oriented communications, group access control, group resource management, and some generic operations on groups, providing consistent and uniform interfaces. Applications include job control, parallel processing, servers, object pools, hierarchical resource management, and so on. Some design issues are presented, and an implementation on a network-transparent global distributed system, which is under development, is discussed.<<ETX>>
[Access control, job control, hierarchical object groups, Communication system control, Process control, group-oriented communications, distributed processing, group resource management, object pools, hierarchical resource management, parallel processing, software basis, Information science, Network servers, servers, Operating systems, Permission, Signal processing, Parallel processing, operating systems (computers), group access control, Resource management, distributed operating systems]
A general approach to recognizing event occurrences in distributed computations
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
A generalized approach to the task of monitoring a distributed computation for event occurrences is presented that involves analyzing the characteristics of both the behavior to be recognized and the environment in which it can occur. The monotonicity of an event, which is a measurement of the stableness of its state, is the primary feature used in characterizing events. A general classification system for monitoring strategies is also presented. The classifications are based on the view of time used by the monitoring system, the delay between the occurrence of an event and its recognition, and the assertions that can be made about the system state at the point of recognition. The properties of an event and a computation can be used to determine the applicability of various types of monitoring strategies.<<ETX>>
[Event detection, Computerized monitoring, Delay systems, Delay effects, NASA, Debugging, monitoring strategies, Aerospace electronics, distributed processing, distributed computation, Character recognition, Distributed computing, Computer science, classification system, supervisory programs, recognizing event occurrences, delays, recognition delay]
A comparison of preemptive and non-preemptive load distributing
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
The authors consider whether the addition of a migration facility to a distributed scheduler already capable of placement can significantly improve performance. They examine performance over a broad range of workload characteristics and file system structures. They find that while placement alone is capable of large improvement in performance, the addition of migration can achieve considerable additional improvement.<<ETX>>
[distributed scheduler, Resumes, distributed processing, performance evaluation, file system structures, Distributed computing, migration facility, Processor scheduling, File systems, performance, preemptive load distribution, scheduling, operating systems (computers), distributed systems, workload characteristics, Resource management, nonpreemptive load distribution]
Maintaining consistency in distributed software engineering environments
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
The authors argue that immutability is a suitable base on which to build distributed software engineering environments. They discuss the various approaches to maintaining consistency in immutable object systems and compare D.P. Reed's model of time domain addressing (ACM Trans. Comput. Syst., vol.1, no.1, p.3-23, Feb. 1983), with their own model of domain relative addressing. They demonstrate the suitability of domain relative addressing for use in distributed-software engineering environments.<<ETX>>
[immutability, Project management, version control, distributed processing, Transaction databases, Personnel, configuration control, Distributed computing, consistency, Programming environments, Software development management, Concurrent computing, Distributed databases, immutable object systems, Database systems, distributed software engineering environments, programming environments, Software engineering, transaction control, domain relative addressing]
Distributed upcalls: a mechanism for layering asynchronous abstractions
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
Procedure calls provide a synchronous interface to call downward through successive layers of abstraction, and remote procedure calls allow the layers to reside in different address spaces. A design is given for distributed upcalls, a mechanism for propagating upcalls across address space boundaries. Distributed upcalls provide a natural complement to remote procedure calls, allowing the user to arbitrarily place abstractions in the server or in the client. A server structuring system called CLAM, which is currently being used to support an extensible window management system, is presented. The CLAM system, including distributed upcalls, remote procedure call extensions to C++, dynamic loading, and basic window classes, is currently running under 4.3BSD Unix on Microvax workstations.<<ETX>>
[Protocols, Debugging, distributed processing, dynamic loading, asynchronous abstractions layering, CLAM, Programming profession, Network servers, Computer languages, server structuring system, window management system, Communication channels, multiprogramming, User interfaces, distributed upcalls, Frequency, remote procedure calls, Workstations, Microvax workstations, Protection, synchronous interface, 4.3BSD Unix]
An architectural development and performance of a real time LAN
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
The real-time facilities of existing local area networks (LANs) which primarily address the needs of non-real-time data traffic are reviewed. An architecture called Instanet is introduced to support the needs of distributed real-time systems. Instanet incorporates a logically separate channel to arbitrate on the access rights of different priorities of traffic. An overview of the Instanet architecture is given, and analytical and simulation results of its performance under different traffic loadings are presented.<<ETX>>
[Real time systems, distributed real-time systems, Integrated circuit interconnections, Telecommunication traffic, Sensor fusion, distributed processing, performance evaluation, Sensor systems, LAN interconnection, local area networks, logically separate channel, Intelligent sensors, Instanet, performance, architectural development, real-time systems, simulation results, Traffic control, Parallel processing, traffic loadings, Local area networks, real time LAN]
Rapid prototyping of concurrent programming languages
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
A novel approach is proposed for automatic generation of concurrent interpreters for formal specifications of the programming languages. It consists of a formal notation and supporting algorithms. The formal notation is called action equations, which are attribute grammars extended by the concepts of events and unification. The supporting algorithms include preprocessing algorithms to generate the interpreters and evaluation algorithms embedded in the generated interpreters. Attribute grammars are reviewed, the extension to the action equations paradigm is explained, and the synthesis of action equations with a unification strategy as a means for specifying and implementing synchronization primitives is presented. The approach is illustrated by a specification of CSP. A brief comparison to related work is included.<<ETX>>
[formal notation, Distributed computing, parallel programming, Program processors, Prototypes, Production, concurrent interpreters, automatic generation, events, synchronization primitives, CSP, rapid prototyping, preprocessing algorithms, Formal specifications, unification, formal specifications, attribute grammars, synchronisation, Computer science, program interpreters, action equations, Computer languages, concurrent programming languages, grammars, Differential equations]
Bridge: a high performance file system for parallel processors
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
A parallel file system called Bridge that distributes each file across multiple storage devices and processors has been designed and prototyped. The approach is based on the notion of an interleaved file, in which consecutive logical blocks are assigned to different physical nodes. Naive programs are able to access files just as they would with a conventional file system, while more sophisticated programs can export pieces of their code to the processors managing the data, for optimum performance. Early analytical and experimental results indicate that Bridge will deliver good parallel speedup for configurations in excess of 32 nodes with disks. High performance is achieved by exporting the I/O-related portions of an application into the processors closest to the data. A pair of applications that illustrate this technology is presented. General issues in the design of algorithms for Bridge are discussed.<<ETX>>
[Application software, parallel processing, Multiprocessing systems, Bridges, Computer science, high performance file system, interleaved file, File systems, Utility programs, Aggregates, optimum performance, Prototypes, Bandwidth, file organisation, operating systems (computers), Computer performance, parallel processors, Bridge]
Efficient deadlock resolution for lock-based concurrency control schemes
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
A distributed algorithm is proposed for detection and resolution of resource deadlocks in object-oriented distributed systems. The algorithm can be used in conjunction with concurrency control algorithms that are based on the semantic lock model. To drastically reduce message traffic, the algorithm properly identifies and eliminates redundant messages. It is shown that its worst and average time complexities are O(ne), where e is the number of edges in the waits-for graph and n is the number of vertices.<<ETX>>
[Object oriented modeling, object-oriented distributed systems, distributed processing, redundant messages, Control systems, Concurrency control, Brazil Council, semantic lock model, time complexities, system recovery, Delay, Computer science, distributed algorithm, directed graphs, message traffic, System recovery, Traffic control, operating systems (computers), waits-for graph, lock-based concurrency control schemes, Distributed algorithms, Detection algorithms, computational complexity]
Sharing jobs among independently owned processors
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
In a network of independently owned processors (e.g. workstations), load-balancing schemes cannot consider the whole network as one unit and thus try to optimize the overall performance. Instead, they have to consider the needs of the resource owners. For this type of environment, load sharing is a more appropriate goal. An approach is proposed in which each machine in the network determines the amount of sharing it is willing to do. The scheme, called High-Low makes sure that the service provided to local jobs of a lightly doped node does not deteriorate by more than a predefined amount. It simultaneously helps improve the service at heavily loaded nodes. It is shown empirically that load sharing in a network of workstations can be effective even with a simple-minded allocation of jobs.<<ETX>>
[Job listing service, resource owners, load sharing, data communication systems, Laboratories, Master-slave, distributed processing, performance evaluation, Throughput, local area networks, Delay, Radio access networks, independently owned processors, Computer science, lightly doped node, network of workstations, load-balancing schemes, Load management, Time sharing computer systems, Workstations, High-Low]
Critical path analysis for the execution of parallel and distributed programs
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
The authors present the design, implementation, and testing of the critical path analysis technique using the IPS performance measurement tool for parallel and distributed programs. They create a precedence graph of a program's activities (program activity graph) with the data collected during the execution of a program. The critical path, the longest path in the program activity graph, represents the sequence of the program activities that take the longest time to execute. Various algorithms are developed to track the critical path from this graph. The events in this path are associated with the entities in the source program, and the statistical results are displayed on the basis of the hierarchical structure of the IPS. The test results from the measurement of sample programs show that the knowledge of the critical path in a program's execution helps users identify performance problems and better understand the behavior of a program.<<ETX>>
[Measurement, critical path analysis, Costs, distributed processing, History, Application software, Programming profession, parallel programming, Information analysis, distributed programs, Processor scheduling, IPS performance measurement tool, software analysis, (program activity graph, program analysis, software engineering, Resource management, precedence graph, Contracts, Testing]
Condor-a hunter of idle workstations
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
The design, implementation, and performance of the Condor scheduling system, which operates in a workstation environment, are presented. The system aims to maximize the utilization of workstations with as little interference as possible between the jobs it schedules and the activities of the people who own workstations. It identifies idle workstations and schedules background jobs on them. When the owner of a workstation resumes activity at a station, Condor checkpoints the remote job running on the station and transfers it to another workstation. The system guarantees that the job will eventually complete, and that very little, if any, work will be performed more than once. A performance profile of the system is presented that is based on data accumulated from 23 stations during one month.<<ETX>>
[Algorithm design and analysis, Condor scheduling system, Resumes, idle workstations, performance profile, Quality of service, Interference, implementation, workstation environment, distributed processing, Displays, Delay, workstations, Processor scheduling, performance, design, Cities and towns, scheduling, Workstations, Protection]
Designing large electronic mail systems
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
Three methods with varying degrees of flexibility are investigated: (1) Mail systems with syntax-directed naming which identify users by names that are syntactically structured according to user locations. Algorithms for load balancing among mail servers, system reconfiguration, and efficient message delivery are developed and tested using simulation. (2) Mail systems with limited location-independent access which allow users to access them from one primary location and a number of secondary locations. Procedures that keep track of users who migrate from their primary location and redirect their mail are presented. (3) The attribute-based mail system which provides maximum flexibility to users by allowing them to identify one or more mail recipients by attributes instead of only by precise names. It can also be used in mass distribution of electronic mail. An algorithm for efficient broadcasting and searching using a minimum-weight spanning tree is investigated. Criteria for evaluating electronic mail systems are presented.<<ETX>>
[System testing, Costs, load balancing, electronic mail, reliability, attribute-based mail system, Electronic mail, syntax-directed naming, Postal services, Information systems, Network servers, flexibility, mail servers, Broadcasting, large electronic mail systems, Internetworking, limited location-independent access, mail redirection, efficiency, cost, Power generation economics, performance evaluation, minimum-weight spanning tree, message delivery, User interfaces, system reconfiguration]
High-speed transformation of primitive data types in a heterogeneous distributed computer system
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
The performance of heterogeneous distributed computed systems is severely limited by the inability to transfer primitive data types between dissimilar processors because of the different representations used. The use of hardware transformation units is proposed to reduce the time required to transfer the data. The transformation units must know the source processor type, data type, and destination processor type and must be capable of performing all required transformations. Three location schemes for the transformation units are local, central, and distributed. The three schemes are compared for twelve factors, and performance models for each scheme are developed.<<ETX>>
[heterogeneous distributed computer system, source processor type, Military computing, primitive data types, high speed transformation, distributed processing, performance evaluation, Data structures, Distributed computing, parallel processing, Concurrent computing, Laser radar, performance, Message passing, Hardware, Radar signal processing, Springs, High speed optical techniques]
Supplying high availability with a standard network file system
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
The design of a network file service that is tolerant to fail-stop failures and that can be run on top of a standard network file service is described. The fault-tolerance is completely transparent, so the resulting file system supports the same set of heterogeneous workstations and applications as the chosen standard, To demonstrate that the design can provide the benefit of highly available files at a reasonable cost to the user, a prototype has been implemented using the Sun NFS protocol. The approach is not limited to being used with NFS and should apply to any network file service built along the client-server model.<<ETX>>
[Sun NFS protocol, Design methodology, fail-stop failures, fault-tolerance, client-server model, Access protocols, File servers, local area networks, Sun, Counting circuits, Network servers, Fault tolerance, File systems, heterogeneous workstations, Voting, network file service, file servers, Broadcasting, highly available files, fault tolerant computing, protocols]
Dynamic definition of entries and attributes in the directory service
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
Mechanisms are specified for achieving integrity controls over entries and attributes within a directory information base to facilitate the dynamic management of directory information. The existence of the commonly understood directory data and architectural models is assumed. Two directory objects, attribute definitions and entry definitions, are described; they specify the structure of classes of entries and attributes, respectively. The issues of scope and dynamic creation of definitions are given particular attention. A solution is proposed for problems directory management, which is not covered by present standards.<<ETX>>
[integrity controls, Access protocols, directory management, Application software, directory service, Environmental management, Computer science, entry definitions, directory information base, Standards organizations, Distributed databases, Open systems, distributed databases, scope, User interfaces, Writing, Data models, attribute definitions]
A new node-join-tree distributed algorithm for minimum weight spanning trees
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
A distributed algorithm that uses a node-join-tree approach for the minimum-spanning-tree problem in a communication network is developed. The algorithm needs at most (2e+n(n-1)/4) messages in O(n/sup 2/) time on a general random graph. In the best case, it needs only 2e messages in O(log n) time. The parameters e and n are the number of edges and nodes, respectively. Although the worst-case performance is not better than that of tree-join-tree algorithms under some extreme cases, simulation results show that it provides better performance in most cases. The algorithm is initialized from a single node, so that there is no need to wake up all nodes at the beginning. It is less complicated than other algorithms, so that a reliable implementation is easier to achieve. The results can be used to improve the algorithms for many other problems in distributed computing, such as leader-election, node-counting, deadlock-resolution, and message-broadcasting.<<ETX>>
[communication network, trees (mathematics), distributed processing, node-join-tree distributed algorithm, Distributed computing, minimum weight spanning trees, Delay, distributed computing, Information science, Tree graphs, simulation results, System recovery, Computer networks, Iterative algorithms, Communication networks, Distributed algorithms, computational complexity]
Reduction of communication delays in hypercube programs based on run time statistics
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
A method is presented for reducing communication delays in hypercube programs by attempting to reduce the number of intermediate nodes involved in interprocess communications. In general, the strategy is to observe a program's interprocess communication pattern during a number of executions. When enough observations have been made, a new mapping of processes to nodes is created. This new mapping is designed to move processes with high levels of stable interprocess communication as close to one another physically as possible. All further program executions then use the new process-to-node mapping, thereby increasing the overall speed of the program. Results from an implementation on the Intel iPSC/d5 are presented, and the overall cost effectiveness of the technique is discussed.<<ETX>>
[Process design, Costs, multiprocessing systems, communication delays reduction, mapping, Delay effects, Circuits, Topology, interprocess communications, Intel iPSC/d5, Statistics, Programming profession, parallel programming, Computer science, Concurrent computing, hypercube programs, Hypercubes, operating systems (computers), run time statistics]
Data reorganization in a dynamically reconfigurable environment
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
The authors consider data reorganization in a dynamically reconfigurable distributed database system. The network environment they study is a dynamic system prototype called Driftwood. They present reallocation strategies corresponding to different network reconfiguration and then address the nonblocking reorganization approach. They cover the implementation strategies, impact on user transaction processing, and efficiency. The authors present an algorithm to construct a parallel reorganization scheduler that is expected to minimize the system recovery time.<<ETX>>
[parallel algorithms, reallocation strategies, system recovery time, File servers, Data engineering, Scheduling, dynamic system prototype, system recovery, Computer science, Network servers, Intelligent networks, Network topology, nonblocking reorganization, Distributed databases, Database systems, fault tolerant computing, dynamically reconfigurable environment, Driftwood, user transaction processing, Monitoring, dynamically reconfigurable distributed database, parallel reorganization scheduler]
Distributed debugging tools for heterogeneous distributed systems
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
A description is given of a collection of tools that form an implementation of event-based abstraction (EBBA), a paradigm for high-level debugging of distributed systems. The tools are capable of operating effectively in a heterogeneous environment containing processors of varying design and power. Toolset users construct libraries of behavior models and observe the behavior of the system through the models. The toolset is a collection of components that are collectively a distributed system for debugging distributed systems. The components can be combined in varying ways to provide levels of debugging service appropriate for the resources available at individual nodes.<<ETX>>
[Process design, high-level debugging, behavior models, program debugging, Laboratories, Buildings, distributed processing, Distributed computing, Power system modeling, Software debugging, Information science, heterogeneous distributed systems, event-based abstraction, distributed systems, Libraries, Error correction, software tools, Monitoring]
Architecture and implementation of the access mechanism for a bus-structured multiservice LAN
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
The design and implementation of a high-speed access mechanism for a fiber-optic multiservice LAN is presented. The handling of stream-type and burst-type communications is accomplished through the use of a distributed hybrid switching protocol that uses an asynchronous dynamic time-division-multiplexing technique. The network comprises a write channel in which each network node transmits and a read channel where the transmitted information is duplicated and thus received by the network nodes. In parallel with the write channel, a sense channel is responsible for detecting the right of access for the node. The protocol provides bounded delay for the packet-switched traffic and small buffering memory for the circuit-switched traffic, which together with the network bit rate and the node processing time and capacity set the network efficiency. Experimental results are presented for the node processing time and the network speed.<<ETX>>
[time division multiplexing, Circuits, fiber-optic, packet switching, Telecommunication traffic, local area networks, bus-structured multiservice LAN, write channel, Optical signal processing, access mechanism, bounded delay, Bandwidth, design, packet-switched traffic, protocols, Local area networks, Optical packet switching, Optical fibers, Packet switching, Access protocols, implementation, asynchronous dynamic time-division-multiplexing technique, read channel, stream-type communications, sense channel, Optical buffering, buffering memory, burst-type communications, node processing time, distributed hybrid switching protocol]
Scheduled and nonscheduled access in a distributed system based upon a functional communication model
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
Request-response communication is studied in the framework of the functional communication model. A scheme that uses the channel access algorithm for distributed scheduling is investigated. A timed token protocol is used to control the two access modes, the scheduled access, with communication initiated by a server, and the nonscheduled access when a client sends a request to a functional group without prior consent from any of the servers in the group. In case of scheduled communication, the server guarantees the acceptance of the request.<<ETX>>
[scheduled access, server, functional communication model, Communication system control, Process control, Access protocols, distributed processing, distributed system, Lighting control, Yarn, Delay, Scheduling algorithm, nonscheduled access, Processor scheduling, Broadcasting, request response communication, timed token protocol, Workstations, protocols, channel access algorithm]
Synchronization and scheduling in ALPS objects
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
The synchronization and scheduling mechanisms used in an object-oriented concurrent programming language called ALPS are described. The main contributions of ALPS are the concept of a manager process and the concept of a hidden procedure array. An object can have a special high-priority process called a manager which intercepts entry calls and implements the necessary synchronization and scheduling for the object. The manager can be programmed to implement any form of preprocessing and postprocessing for the entry calls and to monitor the object. The concept of a hidden procedure array allows an entry procedure to be exported as a single procedure but implemented as a procedure array. When multiple calls to the entry procedure arrive at the object, each call gets attached to a different element of the hidden procedure array. This simplifies the programming of concurrency within an object. The language mechanisms are illustrated using several examples.<<ETX>>
[hidden procedure array, manager, scheduling mechanisms, high-priority process, Scattering, high level languages, manager process, Delay, Scheduling algorithm, parallel programming, Programming environments, synchronisation, Concurrent computing, Computer languages, Information science, Processor scheduling, object-oriented concurrent programming language, Message passing, scheduling, entry calls, synchronization, Monitoring, ALPS objects]
Object memory and storage management in the Clouds kernel
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
Clouds is a distributed object-based operating system designed to support fault tolerance, location independence, and an action/object programming environment. Some of the key issues in supporting Clouds are the availability of object memory, object location, and object recovery. Object memory provides a set of global, persistent, named address spaces for sorting objects. The address spaces resemble conventional segmentation schemes, but are persistent and thus replace both the computational and storage systems used in conventional schemes by a more powerful paradigm. The object location system provides transparent object operation invocation mechanisms throughout the distributed environment. The object recovery system supports recoverable objects through shadowing and two-phase commit techniques to allow atomicity of actions. The key issues in the design and implementation of the object memory and storage management system are briefly described.<<ETX>>
[Clouds, object memory, distributed processing, storage management, object recovery, Operating systems, Fault tolerant systems, sorting, distributed object-based operating system, Kernel, Gold, action/object programming environment, fault tolerance, Clouds kernel, location independence, Programming environments, Shadow mapping, shadowing, Computer science, Memory management, address spaces, Software systems, operating systems (computers), fault tolerant computing, programming environments]
A class of optimal decentralized commit protocols
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
The message complexity of decentralized commit protocols is studied. It is shown that O(kNN/sup 1/k/) messages are necessary if only k rounds of message interchanges are allowed. It is also shown that O(Nln N) is a message lower bound for any decentralized commit protocol. A class of decentralized commit protocols that need O(kNN/sup 1/k/) messages and use k rounds of message interchanges is proposed. If k=ln N, then a decentralized commit protocol that needs O(Nln N) messages only is obtained. The communication scheme is also used to derive some decentralized consensus protocols.<<ETX>>
[message interchanges, Protocols, Automation, finite automata, finite state automata, Educational institutions, transaction execution, decentralized consensus protocols, Concurrent computing, Computer science, message complexity, optimal decentralized commit protocols, O(kNN/sup 1/k/) messages, Writing, protocols]
Performance considerations for distributing services-a case study: mass storage
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
The partitioning of operating system services in a distributed system and its impact on performance are discussed. An examination is made of the tradeoff between partitioning a service at a higher layer, which could potentially result in greater computation at the server, and partitioning at a lower layer, which could result in less service-related computation at the server but may require more communication with the server. As a case study, the performance implications of providing alternate types of distributed mass storage services are considered, focusing on those partitionings that result in a file and a disk service. A detailed multiclass closed queuing network model of the remote service with users on workstations is used in the study.<<ETX>>
[Performance evaluation, magnetic disc storage, disk service, Computer aided software engineering, queueing theory, distributed processing, performance evaluation, distributed system, Throughput, remote service, Partitioning algorithms, service-related computation, mass storage, multiclass closed queuing network model, Operating systems, High performance computing, System performance, User interfaces, operating systems (computers), Hardware, Workstations, distributed mass storage services, performance considerations]
Finding idle machines in a workstation-based distributed system
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
The design and performance of scheduling facilities for finding idle hosts in a workstation-based distributed system are described. The focus is on the tradeoffs between centralized and decentralized architectures with respect to scalability, fault tolerance, and simplicity of design, as well as several implementation issues of interest when using multicast communication. It is concluded that the principal tradeoff between the two approaches is that a centralized architecture can be scaled to a significantly greater degree and can more easily monitor global system statistics, while a decentralized architecture is simpler to implement.<<ETX>>
[System testing, Scalability, Multicast communication, distributed processing, idle hosts, Distributed computing, scalability, Condition monitoring, Fault tolerance, Statistical distributions, Computer architecture, design, multicast communication, scheduling, Workstations, fault tolerance, Computerized monitoring, performance evaluation, workstations, workstation-based distributed system, performance, scheduling facilities, decentralized architectures, fault tolerant computing, centralised architectures]
An application for a distributed computer architecture-realtime data processing in an autonomous mobile robot
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
A description is given of a data-processing structure for real-time systems with sensory feedback and a distributed computer architecture that supports it. The computer system is implemented within an autonomous mobile robot and is based on a set of independent processing modules, each of which consists of a processing core, a transport unit, and a common memory between them. Communication between modules is done in two ways. For command transportation, a direct point-to-point connection is used and transport of raw and processed sensor data is done by broadcasting. Transportation is controlled by an adjustable timetable, which is maintained by the supervisor, who slows down the driving speed and adjusts the timetable if the module does not process the given task in time. This facility allows speed of the robot to be adapted to the current processing capacity and to the complexity of the environment.<<ETX>>
[Real time systems, autonomous mobile robot, point-to-point connection, sensor data, Transportation, Communication system control, distributed processing, Mobile communication, vehicles, Mobile robots, Distributed computing, adjustable timetable, Feedback, Computer architecture, Broadcasting, common memory, robots, independent processing modules, distributed computer architecture, Application software, realtime data processing, broadcasting, transport unit, real-time systems, processing capacity, sensory feedback]
Fault tolerant remote procedure call
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
A scheme is presented that makes a remote procedure call (RPC) mechanism fault-tolerant to hardware failures. Fault tolerance is provided by replicating the procedure at a group of nodes, called a cluster. The copies in a cluster are linearly ordered. A call to a procedure is sent to the first copy in the cluster and is propagated internally to all other copies. In the event of failures, the first copy in the cluster that has not failed returns the result to the caller. The scheme is transparent to the user and supports nested procedure calls. It has been implemented on a network of Sun workstations making use of Sun's existing RPC mechanism.<<ETX>>
[hardware failures, cluster, Sun workstations, fault tolerant remote procedure call, Drives, distributed processing, Educational institutions, Sun, Distributed computing, Computer science, Concurrent computing, Fault tolerance, Computer languages, nested procedure calls, Hardware, fault tolerant computing, Workstations]
Hierarchical distributed simulations
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
Running simulations in a distributed manner by decentralizing the advancement of clock potentially allows significant speedup. However, because time may not advance at the same rate in the target system and the testbed system, deadlocks that do not occur in the target system can occur during distributed simulation. Previous approaches to deadlock resolution incur either high computation overhead through centralized coordination or high communication overhead through a fully distributed solution. Hierarchical decentralized algorithms that take advantage of the locality of these deadlocks are presented. Overheads associated with time advancement are computed analytically, so that appropriate clustering policies can be designed.<<ETX>>
[System testing, asynchronous computation model, hierarchical decentralized algorithms, distributed processing, digital simulation, system recovery, communication overhead, deadlock resolution, Degradation, Analytical models, deadlocks, parallel algorithms, clock advancement, Computational modeling, Circuit simulation, performance evaluation, Topology, distributed simulation, Computer science, computation overhead, System recovery, time advancement algorithm, clustering policies, Timing, Clocks, computational complexity]
A generalized conformance test tool for communication protocols
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
The major objectives of testing the conformance of protocol implementations to standards are discussed. A design is presented for a generalized conformance test tool that permits testing of different protocols, that can be tailored to different test configurations, and is portable to different environments. Both interactive and automatic modes of testing are supported. The integration of three different test description languages, TTCN, LOTOS, and CRS, is discussed.<<ETX>>
[System testing, LOTOS, Protocols, CRS, ISO standards, communication protocols, Control systems, TTCN, test description languages, Communication standards, standards, Telegraphy, Automatic testing, automatic modes, interactive modes, generalized conformance test tool, Open systems, Telephony, Standards development, protocols]
Exploiting symmetries for low-cost comparison of file copies
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
A novel technique for comparison of remotely located file copies is examined. With this technique up to two differing pages can be located, and any other number of multiple differing pages can be detected. It uses a communication overhead of O(log/sup 2/(N)), where N is the number of pages in the file. It is based on a set of symmetries of a hypercube with dimension log(N).<<ETX>>
[Costs, hypercube, Humans, reliability, Transaction databases, Sun, communication overhead, Computer science, file copies comparison, symmetries, distributed databases, data availability, O(log/sup 2/(N)), Broadcasting, Hypercubes, Hardware, Database systems, remotely located file copies, Contracts]
Performance analysis of multiple bus interconnection networks with hierarchical requesting model
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
The performance of multiple-bus networks with full bus-memory connection, single bus-memory connection, and partial bus-memory connection are presented. A type of multiple-bus network, called a partial bus network with K classes, is proposed. Under a nonuniform requesting model called a hierarchical requesting model, the performance of the above multiple-bus networks is analyzed. The costs and fault-tolerant capabilities of each are evaluated and compared with one another. It is shown that the proposed networks are useful in applications requiring high performance and degree of fault tolerance with moderate cost.<<ETX>>
[multiple bus interconnection networks, Costs, Multiprocessor interconnection networks, multiprocessor interconnection networks, nonuniform requesting model, Very large scale integration, performance evaluation, performance model, Throughput, Multiprocessing systems, fault-tolerant capabilities, Fault tolerance, Bandwidth, Computer architecture, Performance analysis, hierarchical requesting model, partial bus network, Joining processes]
An accounting service for heterogeneous distributed environments
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
The integration of accounting functions and an account server into the distributed academic computing network operating system (DACNOS), which is an NOS built on top of a heterogeneous collection of networked computers, is described. The accounting is supported by NOS-kernel mechanisms that can be tailored to the needs of the heterogeneous servers. Accounting data is collected and stored by an account server in the NOS that manages the balances and the resource consumption data of clients. Depending on privileges, accounting data is accessible to clients, servers, and an account server administrator who may use this information for various resource and request management purposes.<<ETX>>
[Context, clients, distributed processing, resource consumption data, Distributed computing, distributed academic computing network operating system, accounting service, Network servers, Computer languages, servers, Operating systems, network operating systems, heterogeneous distributed environments, Computer architecture, Computer networks, Resource management, Kernel, Protection]
A shared dataspace language supporting large-scale concurrency
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
The authors are currently evaluating the use of shared dataspace paradigm as the basis for a novel programming language, called SDL (Shared Dataspace Language), that supports large-scale concurrency. Their goal is to develop the software support needed for the design, analysis, understanding, and testing of programs involving many thousands of concurrent processes running on a highly parallel multiprocessor. The authors provide an overview of the key SDL features, using small examples to illustrate its power and flexibility.<<ETX>>
[Software testing, Availability, shared dataspace paradigm, high level languages, Spatial databases, parallel multiprocessor, Transaction databases, Cultural differences, Programming profession, parallel programming, Concurrent computing, Computer science, Computer languages, large-scale concurrency, shared dataspace language, programming language, SDL, Large-scale systems]
An analysis of multiprocessing speedup with emphasis on the effect of scheduling methods
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
The speedup issue is reexamined for message-passing multiprocessors and computer networks in which interprocessor communication overhead is considered undesirable but significant. A unified model of speedup ) is developed for analyzing the system in terms of communication overhead, scheduling, and the application algorithm. The model can be used to assess quantitatively the impact on overall system performance of the interprocessor communication overhead and its interaction with different scheduling methods. Whereas most authors treat speedup as a measure of improved algorithms or improved systems, the presented model integrates the effects of these factors and of scheduling. Two concepts, hidden overhead and efficiency loss, are introduced to clarify the effect of the scheduling factor. The concepts are illustrated using two scheduling methods designed for use in systems with significant communication overhead.<<ETX>>
[Algorithm design and analysis, multiprocessing systems, multiprocessing speedup, computer networks, scheduling methods, performance evaluation, speedup, scheduling factor, Application software, interprocessor communication overhead, Scheduling algorithm, Degradation, efficiency loss, message-passing multiprocessors, Processor scheduling, System performance, Computer architecture, scheduling, system performance, Computer networks, Velocity measurement, Distributed algorithms, hidden overhead]
Toward automating analysis support for developers of distributed software
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
A constrained expression approach to analyzing large-scale software is presented. Its advantages include broad applicability and reasonable efficiency relative to other proposed approaches. An overview is given of the current status of work on tools supporting analysis of distributed software systems. The constrained expression approach is outlined, and it is shown how it can be used to analyze distributed software. This is illustrated by a description of a recent experiment. An improved prototype toolset currently being built is described. Plans for enhanced tools and further experimentation are summarized.<<ETX>>
[inequality generator constrained expression analysis, Software prototyping, program debugging, program analysis toolset, Laboratories, Programming, distributed processing, distributed software systems, large-scale software, parallel programming, prototype toolset, Information science, distributed software analysis, constrained expression approach, Prototypes, Software systems, event based analysis, Hardware, Robustness, software engineering, behaviour generator, Software tools, Lifting equipment]
Imbedding gradient estimators in load balancing algorithms
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
The problem of efficiently determining the optimum threshold parameter values for a decentralized load balancing algorithm is investigated. Simulation is used to study the behavior of a gradient-based decentralized optimization algorithm for obtaining good values. The algorithm computes the incremental job delay as a function of changes in both the local and remote job arrival rate. Estimators for these two quantities that are embedded in the optimization algorithm are described. Several experiments designed to evaluate the performance of the algorithm in a stationary environment and in an environment where there are changes in the workload are presented. The results indicate that the estimators are accurate, the algorithm chooses good thresholds, and the resultant response time of jobs is near optimal.<<ETX>>
[Algorithm design and analysis, distributed computer systems, Delay effects, load balancing algorithms, Delay estimation, incremental job delay, distributed processing, performance evaluation, remote job arrival rate, Distributed computing, Yarn, Information analysis, optimisation, supervisory programs, optimum threshold parameter, response time, Optimal control, delays, gradient-based decentralized optimization algorithm, Load management, Iterative algorithms, Contracts]
An extended token bus protocol for embedded networks
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
A network environment is considered that can exist in commercial and military products, such as automatic guided vehicles, that use sensors and processors for movement, attack, defense and communication. A bus protocol called IMAP (improved token bus multiaccess protocol) is proposed for these embedded networks. Two modes of operation are defined for the protocol. The normal mode occurs when token passing is done in a random order and the token remains within a cluster of active stations. For a nonuniform traffic condition, this mode of operation can be interrupted by a mode in which token bus operation is carried out and the token is passed through every station. The performance (channel utilization and delay characteristics) of IMAP are compared to those of the token bus and CSMA/CD (carrier sense multiaccess with collision detection) and shown to be superior.<<ETX>>
[Performance evaluation, Protocols, bus protocol, computer networks, Telecommunication traffic, Aerospace electronics, network environment, Delay, Vehicles, embedded networks, Temperature sensors, military products, IMAP, CSMA/CD, commercial products, Traffic control, Broadcasting, channel utilization, delay characteristics, protocols, Local area networks, extended token bus protocol]
Shadow editing: a distributed service for supercomputer access
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
The use of long-haul networks by the scientific community to access resources at supercomputing centers has created a need for high-level user services and tools to provide transparent and efficient access to remote resources. A way to provide a computational service for supercomputer access in a distributed environment is described. The issues involved in the design of such a service are discussed, and attention is focused on a remote job submission facility. A prototype implementation is also described.<<ETX>>
[long-haul networks, distributed processing, supercomputer access, distributed environment, Distributed computing, Intelligent networks, Operating systems, remote job submission facility, Prototypes, computer facilities, supercomputing centers, Computer networks, Manufacturing, high-level user services, shadow editing, mainframes, computer networks, shadow processing, Supercomputers, Job design, NSFret, Computer science, remote resources, computational service, Internet]
Load sharing in hybrid distributed-centralized database systems
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
In a hybrid distributed-centralized database system architecture, some transactions run at (geographically) distributed systems, and other transactions at a central computing complex. Static and dynamic load-sharing strategies are studied for such systems. The strategies take into account not only the difference in load at different sites, but also the effect of routing on data-contention and transaction-abort probabilities. Five dynamic strategies are examined and are compared with an optimal static strategy. A dynamic strategy that is based on analytical estimates of the effect of routing on all transactions in the system, rather than that on the incoming transaction alone, is found to be the best.<<ETX>>
[load-sharing, Communication system control, transaction-abort probabilities, Access protocols, Length measurement, performance evaluation, Routing, hybrid distributed-centralized database systems, Distributed computing, Delay, Centralized control, Concurrent computing, Analytical models, routing, distributed databases, data-contention, distributed systems, Database systems]
Analysis of distributed systems with many identical processes
[1988] Proceedings. The 8th International Conference on Distributed
None
1988
The symmetry of distributed systems that have one or more sets of identical processes, is used to reduce the state space for automatic analysis techniques. A model called the Synchronous Token-based Communicating State Model (STOCS) is proposed to facilitate specification and analysis of symmetric distributed systems. Symbolic and inductive techniques to analyze the STOCS are described. The techniques are demonstrated by analyzing the 2-out-of-3, readers-writers, dining philosophers, and mutual exclusion problems.<<ETX>>
[Protocols, state space, mutual exclusion, distributed processing, 2-out-of-3, symbolic analysis, parallel programming, Concurrent computing, Network servers, Network topology, symmetry, Broadcasting, distributed systems, software engineering, Safety, Logic, automatic analysis techniques, dining philosophers, Reasoning about programs, specification, identical processes, State-space methods, readers-writers, Computer science, (STOCS), Synchronous Token-based Communicating State Model, inductive techniques, state-space methods]
Securely replicating authentication services
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
A framework for designing a type of distributed authentication protocol is given, whose security and availability are higher compared to those of centralized ones. It uses the technique of secret sharing and introduces a cross checksum scheme to achieve secure replication. Fewer than a certain number of malicious servers cannot damage security except by causing denial of service, and this only happens when too many honest servers accidentally fail at the same time. The protocol is suited to an environment where no trustworthiness of any server is permanently guaranteed. The approach is general enough not to rely on any particular authentication protocol. Existing implementations need minor modification. Only a short piece of code is needed to run the implementations as many times as required. Hence, different centralized protocols can be incorporated into one distributed protocol.<<ETX>>
[Protocols, code, securely replicating authentication services, distributed processing, availability, Counting circuits, Network servers, trustworthiness, security, Hardware, Cryptography, protocols, secret sharing, denial of service, Availability, Knowledge based systems, cross checksum scheme, malicious servers, honest servers, secure replication, environment, security of data, distributed authentication protocol, Authentication, Information security]
QuickSilver support for access to data in large, geographically dispersed systems
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
QuickSilver is a distributed operating system that is intended for large, geographically dispersed systems. The authors describe how clients and data servers interact in the QuickSilver system to access distributed data in the context of large, geographically dispersed systems in which there are many different kinds of data servers. An interface is defined that provides a uniform way for clients and servers to exchange information about each other and to access data objects. The interface attempts to minimize the number of messages needed for this. It supports efficient interaction with higher-level client and server semantics by appropriate use of high-level abstractions and operations, and by selectively exposing the parameters governing communication connections to higher layers of the software.<<ETX>>
[clients, geographically dispersed systems, distributed processing, semantics, interface, Distributed computing, Delay, QuickSilver, Network servers, operations, Operating systems, network operating systems, distributed operating system, communication connections, high-level abstractions, software layers, Context, Teleprinting, data objects, data servers, information exchange, data access, messages, Joining processes, distributed data, parameters]
A randomized technique for remote file comparison
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
A technique for file comparison is presented that is based in a set of signatures that are selected by a randomized algorithm. The sites performing the comparison agree on this randomized set of signatures before any comparison takes place. This technique proves to be very competitive with previously published algorithms. It has an advantage over previous techniques in that one can set up the algorithm to diagnose up to a given number of different pages. This is done by changing the total number of bits sent to guarantee that the expected number of falsely diagnosed pages remains under a given level. A metric for comparing the complexity of file comparison techniques is introduced, based on the number of bits that the algorithm needs to send in order to diagnose a given number of differing pages while keeping the probability of false diagnosis under a certain level of confidence.<<ETX>>
[complexity, Protocols, set of signatures, Plasma welding, bits sent, Humans, distributed processing, confidence level, falsely diagnosed pages, sites, randomized algorithm, Application software, expected number, probability of false diagnosis, Computer science, randomized technique, metric, Binary trees, remote file comparison, Computer networks, Hardware, computational complexity]
Decentralized evaluation of associative and commutative functions
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
A family of distributed algorithms for decentralized evaluation of associative and commutative functions is presented. It is shown that if N is the member of processes which cooperate to evaluate such a function, then for each positive integer c<or=1 there is an algorithm in the family that carries out the computation in c rounds of message exchange and requires a total of cN(N/sup 1/c/-1) messages to be sent. Using c as a design parameter, this family of algorithms permits a tradeoff between the number of rounds of message exchange and the total number of messages passed among the processes. The class of functions considered underlies many decentralized protocols, such as decentralized extrema finding and distributed transaction commit.<<ETX>>
[Algorithm design and analysis, Protocols, message exchange, decentralized extrema finding, Communication system control, decentralized protocols, distributed processing, associative functions, Centralized control, computation, Computer science, design parameter, commutative functions, distributed algorithms, distributed transaction commit, Computer networks, decentralized evaluation, Telecommunication network reliability, Communication networks, protocols, Distributed algorithms, message switching, rounds, Clocks]
Detecting termination of distributed computations by external agents
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
An algorithm is presented that defects for termination of distributed computations by an auxiliary controlling agent. The algorithm assigns a weight W, 0<W<or=1, to each active process and to each message in transit. The algorithm maintains that the sum of all the weights related to the computation is equal to one. The controlling agent terminates the algorithm if its weight equals one. A space-efficient scheme is proposed to encode the weights such that an active process can send a very large number of messages before reaching a weight equal to one. Thus, in the proposed encoding scheme, each process and message needs only a small number of bits to encode the weight; the processes can be almost free from the delays of waiting for a supply of weights from the controlling agent.<<ETX>>
[encode, transit message, Communication system control, distributed processing, bits, termination detection, weight, Distributed computing, Delay, space-efficient scheme, Computer science, Weight control, distributed computations, Message passing, Councils, Communication channels, active process, Computer industry, auxiliary controlling agent, external agents, Contracts, algorithm]
Replicated transactions
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
A scheme to replicate transactions is described. The scheme allows a k-replicated transaction to survive (k-1) failures. No coordination among the k replicas is needed until one of them reaches the end and proceeds to abort the others. Consequently, the scheme avoids the overhead and delay caused by failure detection reconfiguration, and synchronization during the k replicas' execution. A robust commit protocol to choose the transaction replica that should be committed and a procedure to choose the nodes on which a transaction replica is executed are described. The goal of the procedure is to maximize reliability.<<ETX>>
[replicated transactions, Protocols, overhead, reliability, database management systems, Distributed computing, Delay, synchronisation, Concurrent computing, Computer science, Condition monitoring, Fault tolerance, Content addressable storage, delay, failure detection reconfiguration, synchronization, Robustness, fault tolerant computing, Timing, protocols, robust commit protocol]
A distributed algorithm for minimum weight spanning trees based on echo algorithms
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
A distributed algorithm based on echo algorithms is presented which constructs the minimum-weight spanning tree in a general undirected graph. In the worst case, the algorithm needs at most (2m+2(n-1) log (n/2)) messages and (2d log n) time, where d is the diameter of the network. In the best case, it needs only 2m messages and 2d time. The algorithm works in phases, and each phase consists of a down wave and an up wave. When the number of fragments is cut by at least one half in each phase, it needs at most O(log n) phases. On average, the algorithm needs only (m) messages and O(d) time. The algorithm itself is simple, so that other distributed algorithms based on it can be derived more easily.<<ETX>>
[distributed processing, simple, Complexity theory, up wave, Distributed computing, worst case, Information science, Tree graphs, Broadcasting, fragments, Distributed algorithms, phases, Testing, network diameter, down wave, Nominations and elections, trees (mathematics), echo algorithms, minimum weight spanning trees, distributed algorithm, best case, System recovery, messages, general undirected graph, time, computational complexity]
A dynamic information-structure mutual exclusion algorithm for distributed systems
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
A dynamic information-structure mutual-exclusion algorithm is presented for distributed systems whose information structure evolves with time as sites learn about the state of the system through messages. It is shown that the algorithm achieves mutual exclusion and is free from starvation. Unlike Maekawa-type algorithms, the proposed algorithm is not prone to deadlocks. This is because its information structure forms a staircaselike structure which in conjunction with timestamp ordering eliminates the possibility of deadlock. Besides having the flavor of dynamic information structure, the algorithm adapts itself to heterogeneous or fluctuating traffic conditions to optimize the performance. An asymptotic analysis of the performance of the algorithm for low and heavy traffics of requests for critical section execution is carried out.<<ETX>>
[Algorithm design and analysis, heterogeneous traffic conditions, mutual exclusion, distributed processing, sites, low traffics, Distributed computing, starvation, Information science, fluctuating traffic conditions, Permission, distributed systems, Performance analysis, Communication networks, deadlocks, critical section execution, heavy traffics, asymptotic analysis, staircaselike structure, Partitioning algorithms, performance optimisation, dynamic information-structure mutual exclusion algorithm, Message passing, System recovery, messages, timestamp ordering, Clocks]
A toolkit for automated support of Ada tasking analysis
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
A discussion is presented of research on the development of a toolkit that supports general static analysis using a Petri net framework for Ada tasking. The toolkit integrates some custom and general-purpose tools. The custom tools were defined and implemented specifically for research in Ada tasking analysis; the general-purpose tools are Petri net tools developed to support arbitrary Petri-net-based research. The analysis toolkit is divided into two major subsystems. The first is the front-end translator subsystem, which translates Ada source (or design-level source specified in a design language called Ada Tasking Language) into a Petri net format. The translation allows one to base current and future analysis techniques on a model that is both theoretically mature and actively investigated. The second is the back-end information display subsystem, which receives user queries and presents tasking analysis results. Example Ada tasking programs are used to demonstrate the utility of the tools individually as well as collectively.<<ETX>>
[general-purpose tools, program testing, Petri net framework, Petri nets, distributed processing, Displays, general static analysis, Distributed computing, Information analysis, Concurrent computing, program interpreters, front-end translator subsystem, Filters, Ada tasking analysis, toolkit, back-end information display subsystem, System recovery, Skeleton, automated support, FETs, Software engineering, Ada]
Adaptive transaction routing in a heterogeneous database environment
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
The issue of transaction routing in a heterogeneous database environment is examined where transaction characteristics like reference locality implies that certain processing systems can be identified as being, in general, more suitable than others for a given transaction class. Routing which ignores these distinctions in an attempt to balance system load can degrade system performance. An adaptive routing strategy is considered which: (1) estimates the response time based on a steady-state analysis; (2) monitors how well actual response time conforms to the estimate; and (3) adaptively adjusts future estimates through a feedback control based on (2). It is found that the adaptive strategy greatly enhances performance robustness as compared to the results of the strategy without feedback. The feedback process used alleviates the sensitivity to accurate estimations of workload and system parameters. Various simulation studies are used to illustrate the adaptive strategy's robustness.<<ETX>>
[transaction processing, reference locality, feedback control, performance evaluation, system parameters, Routing, workload, Transaction databases, Steady-state, database management systems, Adaptive control, Delay, Degradation, simulation studies, Programmable control, heterogeneous database environment, System performance, Feedback, steady-state analysis, system performance, Robustness, adaptive transaction routing]
The reliability of regeneration-based replica control protocols
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
Several strategies for replica maintenance are considered, and the benefits of each are analyzed. Formulas describing the reliability of the replicated data object are presented, and closed-form solutions are given for the tractable cases. Numerical solutions, validated by simulation results, are used to analyze the tradeoffs between reliability and storage cost. With estimates of the mean times to site failure and repair in a given system, the numerical techniques presented can be applied to predict the fewest number of replicas required to provide the desired level of reliability.<<ETX>>
[Availability, Costs, numerical solutions, closed-form solutions, Process control, Access protocols, reliability, Closed-form solution, Time measurement, Maintenance, Steady-state, Application software, Analytical models, simulation results, distributed databases, fault tolerant computing, protocols, replica maintenance, regeneration-based replica control protocols]
Optimizing and evaluating algorithms for replicated data concurrency control
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
Many algorithms for replicated data concurrency control are based on voting methods. Techniques are developed for optimizing the assignment of votes in an environment where intersite communications costs are nonuniform and individual site reliabilities vary. These techniques apply to all algorithms that are based on voting. Availability is considered as a realistic measure of reliability, and so is incorporated in an optimization model. The optimization model is based on minimizing communications costs subject to a given availability constraint. A semi-exhaustive algorithm is described for solving this model. The algorithm utilizes a signature-based method for identifying equivalent vote combinations and an efficient technique for computing availability. It is compared to an equal vote assignment to estimate the extent of possible savings in communications costs.<<ETX>>
[Laboratories, Communication system control, distributed processing, Reliability engineering, semi-exhaustive algorithm, signature-based method, optimisation, algorithm evaluation, Voting, distributed databases, Cost function, intersite communications costs, votes assignment, nonuniform, availability constraint, optimization model, algorithm optimization, Concurrency control, Partitioning algorithms, individual site reliabilities, Cyclotrons, synchronisation, Computer science, equivalent vote combinations, voting methods, minimization, concurrency control, replicated data concurrency control, Power engineering and energy]
Static allocation of periodic tasks with precedence constraints in distributed real-time systems
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
Using two branch-and-bound (B&B) algorithms, an optimal solution is proposed to the problem of allocating (or assigning with the subsequent scheduling considered) periodic tasks to a set of heterogeneous processing nodes (PNs) of a distributed real-time system. The allocation objective is to minimize the maximum normalized task response time, called the system hazard, subject to precedence constraints among the tasks to be allocated. First, the task system is modeled with a task graph, which describes computation and communication modules as well as the precedence constraints among them. Second, the exact system hazard of a complete assignment is determined so that an optimal assignment can be derived. This exact cost is obtained by optimally scheduling the modules assigned to each PN with a B&B algorithm guided by the dominance relationship between simultaneously schedulable modules. Third, to reduce the amount of computation needed for an optimal assignment, a lower-bound system hazard that is obtainable with a polynomial time algorithm is derived.<<ETX>>
[Real time systems, static allocation, distributed real-time systems, Laboratories, distributed processing, B&amp;B algorithm, Distributed computing, Delay, periodic tasks, dominance relationship, branch-and-bound algorithms, scheduling, Cost function, Polynomials, task system, precedence constraints, minimize, task graph, system hazard, Hazards, optimal solution, polynomial time algorithm, Scheduling algorithm, computation, communication modules, Processor scheduling, heterogeneous processing nodes, maximum normalized task response time, PNs, Time factors, computational complexity]
CONCERT: a high-level-language approach to heterogeneous distributed systems
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
Concert, a high-level-language approach to programming heterogeneous distributed systems, is described. The Concert model introduces a small set of language extensions into conventional procedural languages. These language extensions support a cooperative peer process model which addresses in the distributed environment the same issues addressed by language semantics in the conventional environment. The Concert implementation provides layered support for these language extensions, bridging a different source of heterogeneity at each layer. A prototype Concert system currently includes C programs running on OS/2 on multiple PS/2s communicating via calls with one another as well as with PL/I programs running on VM/370.<<ETX>>
[Protocols, Laboratories, high level languages, distributed processing, distributed environment, language semantics, multiple PS/2s, conventional procedural languages, heterogeneous distributed systems, Operating systems, Prototypes, high-level-language approach, Workstations, Virtual manufacturing, layered support, programming, OS/2, cooperative peer process model, PL/I programs, VM/370, Multitasking, Computational Intelligence Society, Concert model, language extensions, Programming profession, heterogeneity, calls, C programs, Writing]
Simultaneous regions: a framework for the consistent monitoring of distributed systems
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
A technique is presented by which state information can be organized into unified, consistent representations of the system state through the creation of simultaneous regions. This method provides a general, yet efficient means of establishing the simultaneous relationship necessary for the monitoring and recognition of event occurrences. The types of events for which a computation can be monitored are described. The methods of utilizing logical clocks and global snapshots are then presented and the reasons why they are not appropriate for use in event evaluation are discussed. The technique for establishing simultaneous regions is then presented and the behaviour of the monitoring and recognition protocol is examined in the context of specific monitoring examples. The correctness of the protocol is proved.<<ETX>>
[Event detection, consistent representations, logical clocks, distributed processing, recognition, event occurrences, system state, simultaneous regions, Distributed computing, Information analysis, Concurrent computing, protocol, distributed systems, state information, protocols, Monitoring, Testing, global snapshots, consistent monitoring, simultaneous relationship, correctness, NASA, Reflection, event types, Computer science, unified, behaviour, event evaluation, Clocks]
On reliability modelling of fault-tolerant distributed systems
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
The problem of predicting the reliability of a distributed system based on the principles of Byzantine agreement is addressed. The system is considered inoperable or failed if Byzantine agreement cannot be guaranteed. The reliability models depend on a unified model of interactive consistency, which is based on a unique fault taxonomy appropriate for distributed systems. The unified model takes advantage of the fact that some faults may not be of an arbitrary nature, while still allowing for the fact that some faults may be arbitrary. A closed-form expression for the reliability and the mean time to failure of systems base on the unified model is derived. Each processor is allowed to have multiple failure modes, and the contribution of the interactive consistency algorithm is explicitly taken into account. The practical value of this unified model in designing ultrareliable systems is demonstrated by several examples.<<ETX>>
[ultrareliable systems, Costs, Byzantine agreement, Taxonomy, Humans, multiple failure modes, reliability, Predictive models, distributed processing, Maintenance, failed, mean time to failure, interactive consistency algorithm, Certification, fault taxonomy, Fault tolerant systems, closed-form expression, Failure analysis, unified model, fault tolerant computing, Reliability, fault-tolerant distributed systems, inoperable, reliability modelling]
An environment for prototyping distributed applications
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
An environment for prototyping distributed applications is described that supports different communication primitives with specified delays, and provides primitives to aid debugging and evaluation. The environment also supports heterogeneous computation in which processes can execute on different hardware. Different source languages can be used for coding different modules of the processes. The system has a centralized control and monitoring facility which is based on the Suntools window system. This approach allows users to develop distributed applications using existing languages and packages, as opposed to restricting users to using only specialized specification languages, and extends the type of work done by R. Hayes et al. (1988) by providing primitives to assist in profiling and instrumenting the applications.<<ETX>>
[program debugging, prototyping distributed applications, distributed processing, Displays, Educational institutions, Application software, Distributed computing, Sun, source languages, Computer science, environment, Message passing, communication primitives, Prototypes, User interfaces, debugging, Hardware, centralized control, monitoring facility, programming environments, Suntools window system]
On the design of resilient protocols for spanning tree problems
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
A method to design resilient protocols for spanning tree problems is proposed. The method consists of a generic resilient protocol that can be used for any spanning tree problem. A resilient protocol for a particular spanning tree problem can be obtained by adjusting various run-time parameters of the generic protocol. The generic protocol is designed by using the concepts of replacement sets that contain the links that can be used to replace a link that fails and replacement values that measure the change in the spanning tree when a link in a replacement set is used to recreate a spanning tree. The approach is as general as the common approach (for the case of spanning trees) because the generic protocol can solve a new spanning tree problem without an individual design. The protocols obtained with this method are efficient and easy to implement and verify because they are all derived from the generic protocol.<<ETX>>
[Design methodology, run-time parameters, trees (mathematics), Telecommunication traffic, spanning tree problems, Sorting, Runtime, resilient protocols, Spread spectrum communication, design, Channel allocation, Radio network, Routing protocols, fault tolerant computing, Communication networks, protocols, Local area networks, replacement sets]
Voting with bystanders
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
A voting protocol overcoming the usual requirements of a minimum number of three copies to be of any practical use and relatively high number of read and write requests is presented. It provides a significant amount of fault-tolerance with as few as two replicas. This protocol, voting with bystanders (VWB), applies to all networks consisting of local area network segments that are immune to partial failures linked by gateways that might fail. A stochastic analysis of the protocol under general Markovian assumptions is presented showing that VWB provides excellent read availabilities and good write availabilities with as few as two or three replicas.<<ETX>>
[Availability, gateways, Protocols, voting protocol, fault-tolerance, Redundancy, replicas, partial failures, local area networks, Computer science, voting with bystanders, File systems, Voting, Equipment failure, Markov processes, Communications technology, fault tolerant computing, local area network segments, protocols, read and write requests, general Markovian assumptions, Protection, Local area networks, stochastic analysis]
A flexible algorithm for replicated directory management
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
A replicated directory management algorithm is presented that supports full location-transparency by allowing migration and reconfiguration at the granularity of individual names. Initially, all names in a directory (including names not yet in use) are controlled by the same set of sites, but control over any individual name (or range of names) can be migrated, e.g. to the set of sites that control the data object associated with the name. The method is based on a directory replication algorithm of J.J. Bloch et al. (1987), incorporating two extensions: (1) allowing control over individual names in a directory to migrate independently; and (2) allowing the definition of relationships among names, such that if control over a given name migrates, then those names that depend on it are automatically migrated as well. Finally, it is stated that the major cost of the algorithm, compared to simpler approaches that do not provide the same degree of location transparency, is the additional complexity and difficulty of obtaining a consistent view of the entire directory.<<ETX>>
[flexible algorithm, Control systems, reconfiguration, Partitioning algorithms, replicated directory management, Distributed computing, Information technology, Technology management, File systems, data object, Voting, full location-transparency, Automatic control, migration, file organisation, Database systems, Contracts, directory replication algorithm]
Message complexity of simple ring-based election algorithms-an empirical analysis
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
Several variants of the simple Chang-Roberts algorithm are simulated. The empirical analysis shows that the algorithms compare very favorably to other ring-based election algorithms. For various ring sizes and numbers of concurrent starters the average message complexity, its distribution, and its standard deviation were studied. Simulation results show that the algorithm is far better than the rather conservative mathematical estimations indicate.<<ETX>>
[Algorithm design and analysis, empirical analysis, Computational modeling, Nominations and elections, distributed processing, ring sizes, Concurrency control, Computer crashes, distribution, simple ring-based election algorithms, Computer science, concurrent starters, message complexity, Chang-Roberts algorithm, Distributed algorithms, message switching, computational complexity, standard deviation]
Marionette: a system for parallel distributed programming using a master/slave model
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
Marionette, a software package for distributed parallel programming in an environment of networked heterogeneous computer systems is described. It uses a master/slave model in which otherwise sequential application programmes can invoke worker operations (asynchronous remote procedure calls executed by slave processes) and context operations (updates to slaves' process states). The master and slaves also interact through shared data structures that can be modified only by the master. The Marionette runtime system is a heterogeneous remote procedure call package. It maintains the consistency of shared data structures, recovers transparently from slave processor failure, and assigns operations to slaves. The Marionette system includes tools for debugging, automated compilation of program binaries for multiple architectures, and distributing binaries to remote file systems. Measurements of a UNIX-based implementation to Marionette and a parallel ray-tracing renderer are presented.<<ETX>>
[program debugging, shared data structures, networked heterogeneous computer systems, Master-slave, distributed processing, master/slave model, Distributed computing, transparent recovery, tools, parallel programming, worker operations, Concurrent computing, automated compilation, context operations, software packages, debugging, Computer networks, program binaries, software tools, asynchronous remote procedure calls, process states, slave processor failure, Marionette runtime system, heterogeneous remote procedure call package, Data structures, Application software, software package, remote file systems, parallel distributed programming, consistency maintenance, parallel ray-tracing renderer, environment, Parallel programming, Software packages, UNIX-based implementation, sequential application programmes, distributing binaries, Packaging, heterogeneous RPC package, multiple architectures, Context modeling, distributed parallel programming]
Application of Petri net models for the evaluation of fault-tolerant techniques in distributed systems
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
Analytical models are presented that use Petri nets for fault-tolerant schemes used in distributed systems. These models are used in the quantitative evaluation and selection of good fault-tolerant schemes for specific system configurations. Several different fault-tolerant schemes that can be modeled using Petri nets are discussed in detail. These schemes include rollback recovery with checkpointing, recovery blocks, N-version programming, and conversations. After a brief review of Petri net models, extension of the Petri net models to incorporate fault-tolerant schemes is considered. A methodology for evaluating a fault-tolerant scheme for a specific system configuration and the steps involved in building a Petri net model of a fault-tolerant system are described. The subnet primitives involved in building these models are identified and an algorithm for building the models automatically is described. Examples illustrating this extended Petri net model are discussed and numerical results are presented to show the applicability of the models.<<ETX>>
[Checkpointing, fault-tolerant techniques, checkpointing, specific system configurations, Petri nets, subnet primitives, distributed processing, fault-tolerant schemes, Distributed computing, analytical models, conversations, quantitative evaluation, N-version programming, Fault tolerance, Analytical models, Fault tolerant systems, distributed systems, Hardware, rollback recovery, algorithm, fault-tolerant system, Educational institutions, recovery blocks, Application software, Petri net models, Computer science, selection, fault tolerant computing]
Message-optimal incremental snapshots
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
The problem of obtaining a global state or snapshot of a distributed processing system is considered. A message-efficient protocol is presented for obtaining incremental snapshots. First, worst-case lower bounds on the number of messages used by any protocol that solves the incremental snapshot problem are given, and then a protocol that is message-efficient is provided. This protocol obtains a snapshot of the system using the most recent snapshot. The message complexity of the incremental snapshot protocol matches the two lower bounds simultaneously, and hence the protocol is asymptotically message optimal. For applications in database systems, debugging distributed programs, monitoring events, or checkpointing, the protocol is asymptotically message optimal. Because of its simplicity, the protocol can be used readily, and since it uses the minimum number of additional messages, the throughput of the distributed system is not adversely affected.<<ETX>>
[checkpointing, Protocols, Event detection, distributed processing, global state, Distributed computing, monitoring events, Distributed processing, message-efficient protocol, message complexity, debugging distributed programs, throughput, protocols, message switching, Computerized monitoring, worst-case lower bounds, distributed processing system, Computer crashes, Application software, Software debugging, Computer science, asymptotically message optimal, System recovery, computational complexity, incremental snapshots, database systems]
Fault-tolerant distributed systems based on broadcast communication
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
Distributed systems present problems of maintaining consistency of distributed data in the presence of faults. These problems are currently solved by agreement protocols that require many messages to be exchanged between processors with adverse effects on system performance. An approach is presented to the design of fault-tolerant distributed systems that avoids this message exchange, resulting in systems that are substantially more efficient. This approach is based on broadcast communication over a local area network such as the Ethernet, and on two novel protocols: the Trans protocol which provides efficient reliable broadcast communication, and the Total protocol which, with high probability, promptly takes a total order on messages and achieves distributed agreement even in the presence of a fault. Reliable distributed operations, such as locking, update, and commitment, require only a single broadcast message rather than the several tens of messages required by current algorithms.<<ETX>>
[Algorithm design and analysis, Ethernet networks, Total protocol, distributed processing, update, local area networks, broadcast communication, Operating systems, Fault tolerant systems, Distributed databases, Broadcasting, Robustness, protocols, Local area networks, locking, Multicast protocols, commitment, broadcast message, consistency maintenance, broadcasting, distributed agreement, local area network, Ethernet, Trans protocol, fault tolerant computing, Telecommunication network reliability, fault-tolerant distributed systems, distributed data]
Missing-partition dynamic voting scheme for replicated database systems
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
A replication control protocol utilizing dynamic voting is presented for ensuring database correctness so that the system behaves like a one-copy database to the users. The protocol dynamically adjusts vote assignment of data items in response to failures and recoveries, thus maintaining higher data availability than static voting schemes in the event of network partitioning. Unlike existing dynamic voting schemes, it supports inexpensive read operations which access one copy, rather than all copies, of each data item read. Since read operations outnumber write operations in most applications, this protocol enjoys better performance. With this protocol, transactions run in one of three modes: normal mode, missing-partition mode, or pseudo-normal mode. Because a partition number and a last current copy cardinality are associated with each copy, read operations only require one copy of a data item when run in the normal mode.<<ETX>>
[Availability, replication control protocol, missing-partition dynamic voting scheme, missing-partition mode, Communication system control, Access protocols, vote assignment, normal mode, Control systems, Concurrency control, Partitioning algorithms, Transaction databases, database management systems, replicated database systems, one-copy database, pseudonormal mode, Voting, Distributed databases, data availability, network partitioning, Database systems, protocols]
Reliable distributed sorting through the application-oriented fault tolerance paradigm
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
The design and implementation of a reliable version of the distributed bitonic sorting algorithm using the application-oriented fault tolerance paradigm on a commercial multicomputer is described. Sorting assertions in general are discussed and the bitonic sort algorithm is introduced. Faulty behavior is discussed and a fault-tolerant parallel bitonic sort developed using this paradigm is presented. The error coverage and the response of the fault-tolerant algorithm to faulty behavior are presented. Both asymptotic complexity and the results of run-time experimental measurements on an Ncube multicomputer are given. The authors demonstrate that the application-oriented fault tolerance paradigm is applicable to problems of a noniterative nature.<<ETX>>
[Algorithm design and analysis, multiprocessing systems, Peer to peer computing, Software algorithms, faulty behaviour, application-oriented fault tolerance paradigm, implementation, distributed processing, error coverage, Application software, Sorting, Computer science, reliable distributed sorting, Fault tolerance, Fault detection, asymptotic complexity, sorting, design, Ncube multicomputer, Hardware, fault tolerant computing, commercial multicomputer, Testing]
Implementing location independent invocation
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
A brief overview is presented of work on building a highly distributed office application based on mobile objects. The authors explain the techniques used to find the target of an invocation and describe how the technique is implemented. Location-independent invocation (LII) is presented as a conceptual service that is independent of any particular application, operating system, or programming language. LII completely removes remote call processing buildings from the view of application programmer. It is shown how LII can be implemented without language or system support in any environment that provides reliable interprocess communication. The indications for LII are studied, i.e. under what circumstances the proposed abstractions are beneficial. A description is given of an application domain in which LII is useful and the core services that support it. An object-finding algorithm is described. The relationship of LII to earlier work on object-finding and location independence is included.<<ETX>>
[application domain, Protocols, Buildings, distributed processing, File servers, Data structures, Electronic mail, Programming profession, Computer languages, highly distributed office application, File systems, Algorithms, Operating systems, location independent invocation implementation, mobile objects, interprocess communication, object-finding algorithm]
A time-out based resilient token transfer algorithm for mutual exclusion in computer networks
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
Two algorithms, control token transfer algorithm (CTTA) and regenerate, are proposed to realize mutual exclusion in a computer network environment using a resilient circulating token. The algorithm CTTA uses a message called token to transfer the privilege of entering a critical region among the participating sites. The algorithm regenerate checks whether the token is lost during system failures, and regenerates it if necessary. Failures in a computer network are classified into three types. The execution of these two algorithms is described for each type of system failure. To detect system failures, a time-out mechanism based on message delay is used.<<ETX>>
[failures, Communication system control, Process control, computer networks, mutual exclusion, message delay, Control systems, time-out based resilient token transfer algorithm, Delay, control token transfer algorithm, Centralized control, Intelligent networks, Tiles, regenerate, Chromium, Computer networks, Computer network management]
Adaptive load sharing in heterogeneous systems
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
The performance characteristics of simple load-sharing algorithms are studied for heterogeneous distributed systems. It is assumed that non-negligible delays are encountered in transforming jobs from one node to another and in gathering remote state information. The effects of these delays on the performance of two algorithms called Forward and Reverse are analyzed. Queuing theoretic models are formulated for each of the algorithms operating in heterogeneous systems under the assumption that the job arrival process at each node is Poisson and the service times and job transfer time are exponentially distributed. The models are solved using the matrix-geometric solution technique. The models are tested with regard to the effects of varying thresholds, the impact of changing the probe limit, and the determination of the optimal response times over a large range of loads and delays. Wherever relevant, the results of the models are compared with M/M/1, random assignment, and the M/M/K models.<<ETX>>
[Algorithm design and analysis, matrix-geometric solution, random assignment, Reverse, distributed processing, optimal response times, heterogeneous systems, Analytical models, distributed systems, job arrival process, Performance analysis, performance characteristics, Probes, Contracts, Testing, Load modeling, queueing theory, Delay effects, performance evaluation, queueing theoretic models, adaptive load sharing, Computer science, Forward, Queueing analysis, M/M/1, M/M/K models]
Capacity testing a HYPERchannel-based local area network
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
Using a hardware monitor, the capacity of a mature (over 10 years old), heavily loaded (5 Cray supercomputers, 3 Ethernet gateways, 13 terminal concentrators, 5000 terminal, 40 node) two-trunk, HYPERchannel-based, high-data-rate local area network was tested. Results indicate that the current load could be intensified by 40% before the network reaches saturation. As the load approaches saturation, overly persistent, low-level protocols begin thrashing, effectively destabilizing the network and destroying much of its potential capacity. The steps needed to extend its lifespan and improve its performance under heavy loads are recommended.<<ETX>>
[Ethernet networks, Protocols, Laboratories, Telecommunication traffic, performance evaluation, local area networks, HYPERchannel-based local area network, Prototypes, Hardware, Resource management, capacity testing, Local area networks, Monitoring, Testing, hardware monitor, low-level protocols]
Message ordering in a multicast environment
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
Three ordering properties are characterized, namely, single-source, multiple-source, and multiple-group ordering, and their solutions are discussed. The multiple-group ordering property, which guarantees that two messages destined to two processes are delivered in the same relative order, even if they originate at different sources and are addressed to different multicast groups, is concentrated on. A protocol that solves the multiple-group ordering problem is presented. The issues of performance and reliability are addressed by providing comparisons with other techniques for ordering multicasts. In many cases this new algorithm solves the problem with greater efficiency than previous solutions without sacrificing reliability. It is pointed out that one disadvantage of the technique is that sometimes sites are required to handle messages which they do not need to deliver locally. These so-called extra nodes, however, do not occur frequently according to the experiments presented.<<ETX>>
[multiple-source, computer networks, Banking, reliability, distributed processing, performance evaluation, single-source, Multicast protocols, multicast environment, Transaction databases, multiple-group ordering, Application software, Computer science, Concurrent computing, Multicast algorithms, protocol, performance, System recovery, Software systems, message ordering, protocols, Contracts]
A new approach to hypercube network analysis
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
A bit-positional notation is proposed for representing the interconnection topology of a hypercube. This approach greatly helps in the analysis of basic properties of a hypercube network. Some systematic procedures are derived to expand binary trees and map them into higher-dimensional hypercubes. The authors show that an n-cube network can be enhanced by adding at most 2/sup n-2/ diagonal links, which makes it possible to map a complete binary tree into a hypercube with any one of its nodes as the root. The problem of mapping mesh networks into hypercube networks is discussed.<<ETX>>
[binary trees, Multiprocessor interconnection networks, Computer aided manufacturing, multiprocessor interconnection networks, trees (mathematics), Application software, Distributed computing, hypercube network analysis, Concurrent computing, Mesh networks, interconnection topology, Network topology, Binary trees, Computer architecture, Hypercubes, bit-positional notation, map]
Performance implications of design alternatives for remote procedure call stubs
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
The authors take efficient kernel-level support as a given, and study the performance implications of design alternatives one level up-in the stubs, which insulate the client and server from details about network communication. These alternatives represent a collection of approaches to achieving standard remote procedure call of semantics. Consideration is given to the performance implications of compiled vs. interpreted stubs, procedural vs. inline code for moving data to/from packet buffers, block copy vs. individual data item copy moving data to/from packet buffers, and the presence or absence of byte swapping.<<ETX>>
[Transport protocols, kernel-level support, server, remote procedure call stubs, byte swapping, compiled stubs, interpreted stubs, block copy, distributed processing, network communication, procedural stubs, semantics, Network servers, Materials science and technology, network operating systems, telecommunication networks, client, inline code, Communication networks, design alternatives, data moving, Insulation, performance implications, performance evaluation, Encoding, Application software, High level languages, Programming profession, Computer science, individual data item copy, packet buffers]
Collecting unused processing capacity: an analysis of transient distributed systems
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
Distributed systems having large numbers of idle computers and workstations are analyzed using a very simple model of a distributed program (a fixed amount of work) to see how the use of transient processors affects the program's service time. The probability density of the length of time it takes to finish a fixed amount of work is determined. An equation is given for the main result for an M-processor network. Simulations confirm that Brownian motion with drift is an accurate model of system performance. With large programs that run for a long time relative to the length of available and nonavailable periods, the central limit-theorem applies, and the Brownian-motion-with-drift model remains good regardless of the distributions of the available and the nonavailable periods. Under these assumptions, the distribution of finishing time is very tight about its mean and well approximated by a normal distribution.<<ETX>>
[accurate model, Costs, Laboratories, probability density, M-processor network, Microcomputers, distributed processing, performance evaluation, Distributed computing, Concurrent computing, Brownian motion, central limit-theorem, system performance, Time sharing computer systems, transient distributed systems, Computer networks, Workstations, distributed program, Transient analysis, Contracts]
An approach to verification of communication in distributed computing system software
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
An approach is presented for verifying the communication among modules in distributed computing system software. This approach is based on the inductive assertion method. The inference rules used in this approach are derived for verifying the partial correctness of communicating sequential modules. In this approach, the virtual circuits are used for synchronous message-passing. The advantage of this approach is that proofs of the satisfaction and noninterference are not needed, since no assumptions about the effects of receiving messages are made in the sequential proofs and the uses of shared auxiliary variables and universal assertions are carefully controlled during the process verification. Without these proofs, the user only needs to deal with the individual modules instead of the entire distributed computing system. The technique for detecting the deadlock of a program is given.<<ETX>>
[program verification, Circuits, distributed processing, inductive assertion method, deadlock, Distributed computing, Industrial electronics, synchronous message-passing, System software, verification of communication, shared auxiliary variables, inference rules, Logic programming, virtual circuits, inference mechanisms, universal assertions, modules, Communication system software, Communication industry, Electronics industry, concurrency control, Computer industry, communicating sequential modules, distributed computing system software, Software engineering]
Distributed query processing in Cronus
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
Four distributed query processing strategies under consideration for incorporation into the Cronus object-oriented distributed computing environment are described. Under Cronus, object storage can be embedded inside applications, and facilities are provided to distribute and replicate automatically the data maintained by the application. The performance of the query processing strategies is analyzed within this environment. The resulting evaluation points out the factors to consider in developing query processing strategies for replicated, object-oriented data, and the impact of those factors on the cost and optimality of the strategies. The evaluation led to the adoption of an optimistic, decentralized strategy which increases parallelism and decreases the size of messages at the expense of an increase in the number of messages transferred.<<ETX>>
[object-oriented programming, Object oriented databases, cost, optimality, Memory, distributed query processing, Relational databases, information retrieval, parallelism, decentralized strategy, Distributed computing, Database languages, Cronus object-oriented distributed computing environment, performance, Query processing, Storage automation, distributed databases, Parallel processing, Cost function, Performance analysis]
Low cost algorithms for message delivery in dynamic multicast groups
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
Heuristics for constructing low-cost multicast spanning trees in a dynamic environment are investigated. Two heuristic algorithms are developed that update the multicast tree incrementally as the membership changes and reduce the total bandwidth required for sending data and control messages. The broadcast tree based algorithm makes use of a tree structure in the network, and the other algorithm joins a new member to the node that is nearest to it and is already in the multicast tree. A simulation model is used to study the performance of the algorithms and to compare them with the best-known heuristic algorithm developed by D.W. Wall (1980) which computes the multicast tree structure for a given membership of the group.<<ETX>>
[Costs, Wall, tree structure, Multicast communication, distributed processing, membership, network, heuristic programming, dynamic multicast groups, control messages, Bandwidth, Broadcasting, Computer networks, dynamic environment, message switching, incremental update, broadcast tree based algorithm, new member, Availability, Tree data structures, heuristic algorithms, bandwidth, computer networks, trees (mathematics), Application software, Computer science, node, Multicast algorithms, performance, multicast spanning trees, data messages, message delivery, simulation model]
A protocol for timed atomic commitment
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
A model and correctness criteria for timed atomic commitment (TAC) are presented which require the processes to be functionally consistent, but allow the outcome to include an exceptional state, indicating that timing constraints have been violated. Correct TAC behavior is defined by presenting an abstract description of the processes involved in the commitment and minimal correctness criteria for their behavior. The correctness criteria capture the intuitive notion that an exception outcome should only occur in the presence of faults, and an aborted outcome should only occur if faults occur or some process votes no. A centralized two-phase commit protocol was modified to meet the correctness criteria by introducing deadlines on the various stages the participants go through (voting and performing), and on the decision phase for the coordinator. The deadlines are derived using several system parameters: maximum message delay, clock drift, and execution time. The protocol is then shown to be correct.<<ETX>>
[centralized two-phase commit protocol, Protocols, correct, distributed processing, voting, Telecommunication control, abstract description, faults, deadlines, exception outcome, Concurrent computing, Information science, minimal correctness criteria, Chemical products, protocols, model and correctness criteria, exceptional state, decision phase, timed atomic commitment, coordinator, Process control, maximum message delay, performing, system parameters, Chemical processes, clock drift, functionally consistent, timing constraints, execution time, TAC, Belts, Timing, Time factors, aborted outcome]
The Camelot library: A C language extension for programming a general purpose distributed transaction system
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
The Camelot library extends the programming language to provide a high-level programming interface to Camelot, a general-purpose distributed transaction system. The Camelot library is implemented as a collection of C functions and macros. The interface presented by the library also provides a concise high-level model of the services offered by a general-purpose transaction system. A broad overview of the interface is given, and implementation experience is briefly summarized.<<ETX>>
[transaction processing, Costs, macros, Access protocols, distributed processing, Computer crashes, user interfaces, services, C language, Power system modeling, Programming profession, Camelot library, high-level programming interface, Computer science, Computer languages, Software libraries, C language extension, Operating systems, Storage automation, C functions, programming language, subroutines, general purpose distributed transaction system]
Fault-tolerant extensions of complete multipartite networks
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
The authors studied the design of a fault-tolerant extension for a graph G which can survive at most m node failures, and which contains the minimum number of nodes and the fewest possible edges when the nonredundant graph (G) is a complete multipartite graph. After developing a characterization for m-fault-tolerant extensions and for optimal m-fault-tolerant extensions of a complete multipartite graph, this characterization is used to develop a procedure to construct an optimal m-fault-tolerant extension of any complete multipartite graph, for any m>or=0. The procedure is only useful when the size of the graph is relatively small, since the search time required is exponential. Several necessary conditions on any (optimal) m-fault-tolerant extension of a complete multipartite graph are proved. These conditions allow identification of some optimal m-fault-tolerant extensions of several special cases of a complete multipartite graph without performing any search.<<ETX>>
[Tree data structures, exponential, search time, graph theory, nonredundant graph, distributed processing, edges, Mathematics, Application software, fault-tolerant extension, Fault tolerance, complete multipartite networks, complete multipartite graph, Tree graphs, Computer network reliability, necessary conditions, Computer networks, fault tolerant computing, node failures, characterization]
Fault-tolerant analysis and algorithms for a proposed augmented binary tree architecture
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
An augmented binary (AB) tree architecture is proposed with a view to providing fault tolerance. This architecture is an augmentation of an n-level full binary tree with n redundant nodes and 2/sup n/+3n-6 redundant links. The AB tree can be configured into a full binary tree even when one node is faulty at each level. While functionally equivalent to the RAE-tree, the proposed AB tree has a regular topology, reduced number of maximum input-output channels per processor, and fewer wire crossovers when implemented using very large-scale integration layout. A reconfiguration algorithm, which constructs an n-level full binary tree from an n-level faulty AB tree, is given. A distributed fault diagnosis algorithm is given which runs concurrently on each nonfaulty processor, enabling each nonfaulty processor to identify all faulty processors.<<ETX>>
[Algorithm design and analysis, algorithms, distributed fault diagnosis, reconfiguration algorithm, Very large scale integration, distributed processing, very large-scale integration layout, Topology, Wafer scale integration, Wire, Fault diagnosis, Fault tolerance, RAE-tree, computer architecture, Binary trees, Computer architecture, Parallel processing, fault tolerant computing, fault tolerant analysis, augmented binary tree architecture, full binary tree]
Distributed diagnosis of Byzantine processors and links
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
The problem of correctly identifying the faulty processors and links in a distributed system where faulty behavior is unrestricted (Byzantine) is examined. A very general class of algorithms called evidence-based diagnosis algorithms is proposed that encompasses all past approaches to the diagnosis problem. An algorithm is presented which is proven optimal in this class. It is further shown that, in the worst case, no evidence-based diagnosis algorithm can guarantee that its diagnosis is both correct and complete, when evidence can be false. It is argued both analytically and from experimental data that in systems of N processors of which t can be faulty, the complexity of this algorithm is O(max(2 to the power of t/sup 2/, N/sup 2/)).<<ETX>>
[Algorithm design and analysis, Performance evaluation, System testing, complexity, Protocols, evidence-based diagnosis algorithms, Terminology, distributed processing, faulty processors, Byzantine links, Fault diagnosis, Computer science, Fault detection, fault tolerant computing, Byzantine processors, distributed diagnosis]
Minimizing control overheads in adaptive load sharing
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
Two algorithms are developed for minimizing control overheads in exchanging state information arising from the control messages used in determining the load levels at other servers. In the first algorithm, the load levels at other servers are guessed using a simple heuristic algorithm. Such a model is found to provide significant improvements compared to the no-load sharing case. The second algorithm improves upon the first one by replacing some unnecessary task transfers by a single probe. The simulation results obtained from these algorithms are presented and compared to an algorithm based on random selection of destinations for transfer tasks. It was concluded that a load sharing policy should try to maximize the success rate in finding good destinations for transfer tasks while minimizing the control overheads.<<ETX>>
[algorithms, Stability, Computational modeling, Computer simulation, Heuristic algorithms, computer networks, distributed processing, Lighting control, minimizing control overheads, adaptive load sharing, Adaptive control, Delay, random selection, heuristic algorithm, Programmable control, Network servers, simulation results, model, state information, Testing]
Some graph partitioning problems and algorithms related to routing in large computer networks
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
The problem of partitioning a large computer network into clusters in order to reduce the amount of network resources consumed by the routing algorithm is addressed. The clustering problem is formulated as a general graph partitioning problem. It is shown that the problem of partitioning a graph into a minimum number of clusters with unit weight vertices and a given weight bound on the cluster size is NP-complete if each cluster is required to be internally connected. It is also shown that if a diameter bound is imposed on the cluster instead of the weight bound, then the problem is NP-complete, even when cluster connectivity is not required. An optimum partitioning algorithm is presented for the latter problem when the graph is a tree. An optimum partitioning algorithm is presented for another problem in which each cluster is required to contain exactly one of a set of specified vertices called cluster heads.<<ETX>>
[algorithms, Costs, unit weight vertices, Circuits, graph theory, computer networks, Routing, NP-complete, Partitioning algorithms, Application software, Intelligent networks, routing, Network topology, Clustering algorithms, large computer networks, Packaging, network resources, Computer networks, graph partitioning problems, clusters]
A shared dataspace model of concurrency-language and programming implications
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
A language paradigm called shared dataspace is defined that causes computations to be performed using an anonymous, content-addressable communication medium acted upon by atomic transactions. To probe the essence of this paradigm, a relatively simple shared dataspace language called Swarm is defined. An overview is presented of the Swarm language. A formal operational model for the language is given and some of the programming implications and distinctive features of the model and language are discussed. Swarm programming strategies are examined using a series of related example programs.<<ETX>>
[atomic transactions, formal languages, Logic programming, Knowledge based systems, content-addressable communication medium, Data structures, language, Database languages, Vehicles, concurrency, Concurrent computing, Computer science, Computer languages, Swarm, Technology management, formal operational model, shared dataspace model, concurrency control, Parallel processing, programming]
Transparent concurrent execution of mutually exclusive alternatives
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
The task of concurrently computing alternative solutions to a problem where only one of the solutions is needed is examined. In this case the rule for selecting between the solutions is faster first, where the first successful alternative is selected. For problems where the required execution time is unpredictable, this method shows substantial execution time performance increases over other methods. In order to test the utility of the design, it is used for two application areas: distributed execution of recovery blocks and OR-parallelism in Prolog. The authors present: (1) a model for selection of alternatives in a sequential setting: (2) a transformation that allows alternatives to execute concurrently; (3) a description of the semantics-preservation mechanism; and (4) parameterization of where the performance improvements can be expected. Additionally, examples of application areas for the method are given.<<ETX>>
[concurrently computing, Performance gain, distributed processing, rule, semantics-preservation, OR-parallelism, Concurrent computing, alternative solutions, storage management, transparent concurrent execution, Databases, distributed execution, first successful alternative, mutually exclusive alternatives, performance improvements, alternatives selection model, Testing, concurrent execution, sequential setting, parameterization, Prolog, Time measurement, Explosions, recovery blocks, Application software, transformation, faster first, Computer science, Memory management, Clocks]
Performance modeling of the modified mesh-connected parallel computer
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
A message-passing computer architecture called the modified mesh-connected parallel computer (MMCPC) is proposed and studied. The MMCPC is designed to be general-purpose parallel architecture suitable for wafer-scale integration. Generalized stochastic Petri nets (GSPNs) are used to model the behavior of the MMCPC. The GSPN performance modeling results show a need for a new processing element (PE). A PE architecture, able to handle data processing and message passing concurrently, is proposed, and the silicon overhead is estimated in comparison with transputerlike PEs. Based on the proposed PE, optimum sizes of the MMCPC for different program structures are derived. A two-dimensional fast Fourier transform problem is used as an example to demonstrate that the MMCPC is a cost-effective performance-enhancement architecture to a real problem.<<ETX>>
[parallel architectures, Petri nets, Stochastic processes, processing element, performance evaluation, Data processing, message-passing computer architecture, parallel architecture, Parallel architectures, Wafer scale integration, modified mesh-connected parallel computer, stochastic Petri nets, Semiconductor device modeling, Concurrent computing, wafer-scale integration, Message passing, performance modeling, program structures, Computer architecture, Silicon, silicon overhead, two-dimensional fast Fourier transform]
Immediate ordered service in distributed systems
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
A technique is presented by which a centralized service facility in a distributed system can honor client requests in the global order in which those requests were made. Unlike traditional approaches to this problem, the server provides immediate service to its clients. Immediate service requires that, upon receipt of a request which is the oldest unserved message, the server must grant the service immediately. No additional message passing may be used by the server in order to serve that request. The approach requires that certain information be appended to every message in the system. This piggybacked information allows the server to infer the order in which service should be granted to arriving request messages. Increased message length is thus explicitly traded for fewer messages and faster response to requests at the server. That overhead is proportional to the number of processors in the system. In time-critical distributed applications immediate service offers a means by which service response time can be substantially reduced. It is argued that, under certain reasonable assumptions about the network and process behavior, immediate service can outperform the logical timestamp approach for very large systems (and certainly for small systems).<<ETX>>
[Actuators, server, time-critical distributed applications, NASA, service response time, Aerospace electronics, distributed processing, Educational institutions, Displays, Research and development, Computer science, Weapons, Message passing, centralized service facility, immediate order service, distributed systems, Resource management, protocols]
A model for concurrent checkpointing and recovery using transactions
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
Concurrent checkpointing and recovery using a concurrent transaction processing model which consists of four types of atomic operation and five types of conflict is developed. Each checkpoint/rollback transaction is executed by multiple processes in the system. They can be executed concurrently. It is shown that the consistency of recovery lines and rollback lines established by checkpoint transactions and rollback transactions can be achieved by enforcing serializability on the corresponding transactions. There are two advantages in using a transaction model for concurrent checkpointing and recovery: (1) it is easier to find algorithms to solve a transaction processing problem; and (2) based on this model, related issues of the two corresponding problems can be thought of uniformly. This model clarifies the concepts of concurrent checkpointing and recovery, and brings more ideas for designing algorithms.<<ETX>>
[Checkpointing, Algorithm design and analysis, rollback, Protocols, NASA, Merging, Interference, information retrieval, concurrent checkpointing, Transaction databases, recovery, database management systems, transactions, atomic operation, Computer bugs, concurrency control, System recovery, serializability, Hardware, model, conflict]
Linking consistency with object/thread semantics: an approach to robust computation
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
An object/thread based paradigm is presented that links data consistency with object/thread semantics. The paradigm can be used to achieve a wide range of consistency semantics from strict atomic transactions to standard process semantics. The paradigm supports three types of data consistency. Object programmers indicate the type of consistency desired on a per-operation basis, and the system performs automatic concurrency control and recovery management to ensure that those consistency requirements are met. This allows programmers to customize consistency and recovery on a per-application basis without having to supply complicated, custom recovery management schemes. The paradigm allows robust and nonrobust computation to operate concurrently on the same data in a well-defined manner. The operating system need support only one vehicle of computation-the thread.<<ETX>>
[robust computation, Clouds, operating system, distributed processing, Concurrency control, strict atomic transactions, standard process semantics, Yarn, Distributed computing, per-operation basis, Programming profession, Vehicles, data consistency, Concurrent computing, recovery management, Operating systems, concurrency control, consistency semantics, operating systems (computers), Robustness, object/thread semantics, automatic concurrency control, Joining processes]
Verifying finite state real-time discrete event processes
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
Decision procedures are presented for checking a small but useful class of properties for any finite-state system consisting of real-time discrete-event processes. A timed transition model (TTM) is used for representing real-time discrete-event processes, and real-time temporal logic (RTTL) is the assertion language in which the property to be verified is stated. The relationship of TTMs to other formalisms is summarized along with a complete definition of TTMs and an overview of RTTL. The construction of reachability graphs is discussed, two different procedures are presented for constructing reachability graphs, and the corresponding decision procedures are given.<<ETX>>
[Real time systems, Heart, finite-state system, reachability graphs, finite automata, timed transition model, Computational modeling, graph theory, Time to market, real-time discrete-event processes, real-time temporal logic, Computer science, assertion language, Message passing, real-time systems, decision procedures, Concrete, Safety, Logic, Clocks]
Fast Ring: a distributed architecture and protocol for local area distributed processing
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
A ring-based media access control protocol and architecture, Fast Ring, which combines the best features of the token ring and contention ring, is proposed. For this protocol, a free token circulates on the ring when the ring is idle. A ready station can transmit either by capturing the token or sensing the ring idle. The protocol works in such a way that the ready node which captures the free token, before or while transmitting, is able to continue transmission. All the other contending nodes have to stop their transmissions and then send the abort signal when they receive the upstream transmission. After its transmission, the successful station puts a free token on the ring and the protocol enters the token mode. It behaves like the token ring protocol until the ring becomes idle again when all the ready nodes complete their transmissions. Comparison of the performance of Fast Ring with those of contention ring, token ring, and carrier-sense multiaccess with collision detection protocols shows that the Fast Ring outperforms all these local area network protocols over the whole throughput range at all transmission rates.<<ETX>>
[Ethernet networks, Fast Ring, token ring, Access protocols, distributed processing, local area distributed processing, Throughput, local area networks, Distributed computing, distributed architecture, Delay, Degradation, Distributed processing, ring-based media access control protocol, protocol, Computer architecture, Media Access Protocol, contention ring, ready node, Token networks, protocols, free token]
Implementation of the conversion scheme in loosely coupled distributed computer systems
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
Several different approaches are discussed for implementing conversations in loosely coupled distributed computer systems (DCSs). Important implementation factors considered include the control of exits of processes on completion of their conversation tasks and the approach to execution of the conversation acceptance test. Two different exit control strategies, one in a synchronous manner and the other in an asynchronous manner, and three different approaches to execution of the conversation acceptance test, centralized, decentralized, and semi-centralized, are examined and compared in terms of system performance and implementation cost. The effectiveness of these execution approaches also depends on the way conversations are structured initially by program designers. Therefore, the two major types of conversation structures, name-linked recovery block and abstract data type conversations are examined to analyze which execution approaches are the most efficient for each conversation structure. These results provide guidelines for implementing conversations in loosely coupled DCSs.<<ETX>>
[name-linked recovery block, System testing, Costs, conversation acceptance test, distributed processing, performance evaluation, Control systems, decentralized, exit control strategies, Distributed computing, abstract data type conversations, Centralized control, Guidelines, loosely coupled distributed computer systems, centralized, Computer science, System performance, implementation factors, Distributed control, system performance, data structures, protocols, Contracts, conversion scheme]
Efficient algorithms for resource allocation in distributed and parallel query processing environments
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
Several effective algorithms are presented for the optimal allocation of computer resources in a proposed stream-oriented parallel-processing scheme for database operations. These algorithms can be utilized to obtain the optimal allocation of memory resources for every type of query in sequential-processing environments, parallel-processing environments with shared-memory multiprocessors, and distributed-processing environments. The computation complexities of the proposed algorithms are analyzed and used to clarify the effectiveness of those algorithms.<<ETX>>
[Algorithm design and analysis, storage allocation, algorithms, stream-oriented parallel-processing scheme, distributed processing, database management systems, parallel processing, Concurrent computing, Distributed processing, Information science, storage management, resource allocation, database operations, Microprocessors, Distributed databases, Parallel processing, Computer networks, memory resources, computation complexities, shared-memory multiprocessors, sequential-processing environments, Query processing, optimal allocation, parallel query processing environments, Resource management, computational complexity]
Intelligent routers
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
The authors describe a mechanism known as intelligent routers that provides a means to implement distributed applications in a heterogeneous environment. A router is a process that may explicitly request to migrate to another machine so it can avail itself of that machine's capabilities. The router environment supports the migration despite differences in the computer hardware, operating systems, and communication networks in the target set of computers. Three main goals are described for intelligent router systems: that the routers be self-directing; that the systems accommodate heterogeneity; and that the systems be robust. A description of the architecture of an intelligent router system is presented. Also presented is an implementation of a particular intelligent router system. This prototype supports simultaneous execution of multiple routers on a variety of hardware and operating systems; however, it is not yet robust. Finally, worm programs, Knos, and routers are compared and their differences are explored.<<ETX>>
[Knos, computer networks, distributed processing, Application software, distributed applications, prototype, Machine intelligence, heterogeneity, worm programs, Operating systems, self-directing, Prototypes, intelligent routers, Computer architecture, heterogeneous environment, Computer networks, Hardware, Robustness, Communication networks, Intelligent systems]
Analysis of communicating processes for non-progress
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
The problem of testing two processes (specified as finite-state machines) communicating asynchronously with each other using send and receive commands over a set of message types is considered for two forms of nonprogress: deadlock and unspecified reception. Since the nonprogress problem is undecidable, a dataflow approach is used to obtain sufficient conditions under which the two processes are free of deadlock and unspecified reception. The approximation analysis is based on weakening the receive operation. Polynomial time algorithms are presented to perform the analysis. This problem arises in the context of dataflow analysis of the processes that communicate by message passing and in the context of showing correctness of protocol specifications. Diagrams are provided for some networks that can be certified to be free of unspecified receptions using the algorithms. The problem of testing for deadlock in more than two processes still remains open.<<ETX>>
[Context, Algorithm design and analysis, Data analysis, Protocols, unspecified reception, correctness, finite automata, nonprogress, approximation analysis, deadlock, dataflow approach, sufficient conditions, Sufficient conditions, communicating processes, Message passing, polynomial time algorithms, concurrency control, protocol specifications, System recovery, Polynomials, Performance analysis, protocols, finite-state machines, Testing]
An analysis of distributed shared memory algorithms
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
Results obtained in a study of algorithms to implement a distributed-shared memory in a distributed (loosely coupled) environment are described. Distributed-shared memory is the implementation of shared memory across multiple nodes in a distributed system. This is accomplished using only the private memories of the nodes by controlling access to the pages of the shared memory and transferring data to and from the private memories when necessary. Alternative algorithms are analyzed to implement distributed-shared memory. The algorithms are analyzed and compared over a wide range of conditions. Application characteristics are identified which can be exploited by the algorithms. The conditions under which the algorithms analyzed perform better or worse than the other alternatives are shown. Results are obtained via simulation using a synthetic reference generator.<<ETX>>
[Algorithm design and analysis, Communication system control, simulation, Read-write memory, distributed processing, Information retrieval, Discrete event simulation, Distributed computing, Optimization, synthetic reference generator, Message passing, distributed loosely coupled environment, distributed shared memory algorithms, Hardware, Performance analysis, private memories]
A high performance virtual token-passing multiple-access method for multiple-bus local networks
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
A new integrated demand-assignment multiple-access (DAMA) method for multiple-bus local networks (MBLNs) is proposed and evaluated. The performance of this method is shown to be superior to independent token-passing as well as to previously proposed integrated access schemes. V-STIA, a virtual token-passing extension of a modified explicit token-passing scheme called single-token integrated access (STIA), delivers the best overall performance at medium to heavy loads while achieving good light-load performance without collision detection. Consequently, V-STIA makes the fewest possible demands on interface capabilities for DAMA support in MBLNs. Additionally, V-STIA implementation does not require simultaneous transmit capability for correct operation and greatly reduces design complexity. Performance advantages of MBLNs over an equivalent bandwidth single-bus DAMA system are established.<<ETX>>
[MBLNs, single-token integrated access, Costs, Stability, Redundancy, performance evaluation, Application software, Distributed computing, DAMA, multiple-bus local networks, integrated demand-assignment multiple-access, V-STIA, Network topology, Aggregates, Bandwidth, virtual token-passing multiple-access method, Computer networks, multi-access systems, protocols, token networks, Propagation delay, STIA]
Generating a fault tolerant global clock in a high speed distributed system
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
A technique is described for constructing a fault-tolerant global clock in a point-to-point distributed system with an arbitrary topology, which constitutes a wide-area network. It is assumed that the network is constructed of optical links with very high transmission rates. The approach used is to generate a global clock from the ensemble of the local transmission clocks, and not to synchronize these high-speed clocks directly. The steady-state algorithm which generates the global system clock is executed in hardware by the network interface of each node. As a result, it is possible to estimate accurately intermodal delays and thereby to achieve a much tighter synchronization than with other methods. The basic synchronization time step is proportional to the error or uncertainty in the measurement of the end-to-end network delay rather than to the actual value of the end-to-end network delay. Node and network models are presented, and the synchronization condition is defined. The synchronization algorithm, its bound, and its correctness proof are presented. A procedure is described for detecting and isolating a faulty component, while maintaining the integrity of the global clock.<<ETX>>
[node models, intermodal delays, Steady-state, wide-area network, faulty component, Fault tolerance, network models, Network topology, arbitrary topology, Fault tolerant systems, distributed databases, point-to-point, Optical fiber communication, Hardware, optical links, local transmission clocks, Delay effects, Delay estimation, steady-state algorithm, high speed distributed system, fault tolerant global clock, Synchronization, correctness proof, synchronisation, synchronization, fault tolerant computing, Clocks]
HPC/VORX: a local area multicomputer system
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
A system is described that combines the major strengths of local area networks and multicomputers, namely resource sharing over geographically significant distances and small communication latencies. The result is a new architecture called a local area multicomputer (LAM). The system that is described attempts to solve the seemingly incompatible needs of the two application domains by utilizing a high-capacity, low-latency interconnection network called the HPC supported by the VORX multiprocessor computing environment. For systems with hundreds of nodes, the network capacity is in the Gb/s range, and the latency for small messages is about 10 mu s. The HPC can connect resources located several kilometers apart. The VORX environment provides the necessary program development tools and resource management functions needed to experiment with distributed applications. Two applications are outlined to show the diverse uses for a LAM system and the status of an 80-node system that includes 70 adjunct processors and ten SUN 3 workstations/fileservers.<<ETX>>
[geographically significant distances, application domains, Multiprocessor interconnection networks, HPC/VORX, local area networks, Delay, program development tools, SUN 3 workstations, interconnection network, network capacity, Computer architecture, Parallel processing, Computer networks, Workstations, software tools, communication latencies, Local area networks, multiprocessing systems, fileservers, Application software, Sun, resource sharing, local area multicomputer system, Resource management, resource management functions]
A service execution mechanism for a distributed environment
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
A service execution mechanism is designed to provide users transparent access to computational services in a distributed environment. The central idea in this approach is that computations available to the user in a distributed system should be treated as services, where a service is a user-level computation that is offered by one or more machines. The identification of a service is separate from its execution for all computations available to the user. This abstraction allows the details of performing a service to be hidden from the user, and allows the user to specify what services he/she would like to use and not be concerned with where or how the services are invoked. The author's experience with a prototype implementation is reviewed. It is concluded that the service mechanism is a small cost in the total time to execute a simple local service, and an insignificant cost for more computation-intensive services. For services that are invoked remotely, additional costs may be incurred for probing during selection, but these costs are negligible in comparison to the costs of invocation.<<ETX>>
[Computer interfaces, abstraction, distributed processing, distributed environment, Distributed computing, Engines, Centralized control, transparent access, Operating systems, prototype implementation, Prototypes, service execution mechanism, Tail, computational services, user-level computation, Computer networks, Workstations, Local area networks]
Evaluation of concurrent pools
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
Performance considerations affecting the design of a mechanism that preserves locality and avoids high-latency remote references called the concurrent pools data structure are explored. The effectiveness of three different implementations of concurrent pools is evaluated. Experiments performed on a BBN Butterfly multiprocessor under a variety of workloads shown that the three implementations perform similarly well for light workloads, but that with stressful workloads it appears that a simple algorithm can provide better performance than a complex algorithm, designed to keep remote accesses to a minimum. Implementations can benefit by taking into account information on the nature of the operations performed by each process to help balance the elements among processes that need them.<<ETX>>
[Algorithm design and analysis, concurrent pools, Interference, data structure, locality, Data structures, complex algorithm, Distributed computing, Computer science, Concurrent computing, BBN Butterfly multiprocessor, Parallel programming, concurrency control, Concrete, data structures, Performance analysis, Local area networks, performance considerations]
A probabilistic approach to distributed clock synchronization
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
A probabilistic method is proposed for reading remote clocks in distributed systems subject to unbounded random communication delays. The method can achieve clock synchronization precisions superior to those attainable by previously published clock synchronization algorithms. The method can be used to improve the precision of both internal and external synchronization algorithms. The approach is probabilistic because it does not guarantee that a processor can always read a remote clock with an a priori specified precision; however, by retrying a sufficient number of times, a process can read the clock of another process with a given precision with a probability as close to one as desired. An important characteristic of the method is that, when a process succeeds in reading a remote clock, it knows the actual reading precision achieved. The use of the remote clock reading methods is illustrated by presenting a time service which maintains externally (and, hence, internally) synchronized clocks in the presence of process, communication, and clock failures.<<ETX>>
[Algorithm design and analysis, Protocols, Delay effects, Shape measurement, Humans, unbounded random communication delays, distributed processing, Synchronization, probabilistic approach, synchronisation, Network servers, remote clock, distributed systems, Timing, Communication networks, distributed clock synchronization, Clocks]
Performance analysis of synchronous packet networks with priority queueing disciplines
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
Models and analytical techniques are developed to evaluate the performance of time-synchronous packet networks with priority queueing disciplines. In cases which require approximation, the model was validated through extensive computer simulation and compared to analytical results. A local first-order Markov model based on a burstiness criterion is used to approximate the delay incurred by each class of traffic in a network. The application of this model to selected networks shows that it accurately predicts behavior of tandem (i.e. with and without departures), unidirectional, and bidirectional loop networks.<<ETX>>
[models, queueing theory, priority queueing disciplines, approximation, Buffer storage, computer networks, packet switching, bidirectional loop networks, performance evaluation, Cellular networks, local first-order Markov model, Analytical models, Satellites, Viterbi algorithm, unidirectional networks, synchronous packet networks, Traffic control, Markov processes, computer simulation, Communication system traffic control, Performance analysis, Queueing analysis, Local area networks, performance analysis]
Fault tolerance in a very large database system: a strawman analysis
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
A simple model is used to study the effect of fault-tolerance techniques and system design on system availability. A generic multiprocessor architecture is used that can be configured in different ways to study the effect of system architectures. Important parameters studied are different system architectures and hardware fault-tolerance techniques, mean time to failure of basic components, database size and distribution, interconnect capacity, etc. Quantitative analysis compares the relative effect of different parameter values. Results show that the effect of different parameter values on system availability can be very significant. System architecture, use of hardware fault tolerance (particularly mirroring), and data storage methods emerge as very important parameters under the control of a system designer.<<ETX>>
[Availability, system architectures, Costs, multiprocessing systems, very large database system, Memory, system design, Control systems, database management systems, strawman analysis, mean time to failure, Fault tolerance, generic multiprocessor architecture, data storage methods, Fault tolerant systems, Prototypes, Computer architecture, system availability, Database systems, Hardware, fault tolerant computing, model, fault-tolerance techniques]
Transparent distributed object management under completely decentralized control
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
An experimental distributed system based on an integrated system design was built that incorporates user-level or environmental application requirements into the design issues of the distributed operating system Dragon Slayer. The key for making use of the Dragon Slayer services, in supporting applications requesting a high amount of reliability under completely decentralized control, is the definition of the concept of distributed objects. These were used as the basis for a paradigm of distributed computing that allows users to neglect the distribution of services and responses. How distributed objects are managed in Dragon Slayer is outlined. The distributed objects are logical objects which may be partitioned. The parts or fractions may be distributed over several nodes. They may even exist in multiple copies or version. In order to prepare the ground for requirements regarding distributed object management, a taxonomy of object-oriented approaches and their methods of managing distributed object operations is given. A set of necessary conditions for the design of distributed operating system services that are to support such management methods under completely decentralized control is determined.<<ETX>>
[completely decentralized control, integrated system design, transparent distributed object management, Peer to peer computing, Taxonomy, Project management, reliability, distributed processing, Control systems, decentralised control, Distributed computing, Adaptive control, Programmable control, logical objects, environmental application requirements, Operating systems, necessary conditions, Distributed control, operating systems (computers), operating system Dragon Slayer, Local area networks]
An application of group testing to the file comparison problem
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
The file comparison problem involves the detection of differences between two copies of the same file located at different sites in a distributed computing system. The file is assumed to be partitioned into n pages, and a signature (checksum) is available for each page. Some ideas from nonadaptive group testing are used to obtain a solution to this problem for the case of arbitrary d, where d is a fixed bound on the maximum number of differing pages. A deterministic construction is presented which, for the special case d=2, improves upon previous results. For values of n<2/sup 20/ and d<or=10, the construction results in a substantially smaller number of composite signatures than the best-known asymptotic upper bound.<<ETX>>
[distributed processing, Computer crashes, asymptotic upper bound, Application software, Distributed computing, distributed computing system, file comparison, Computer science, Upper bound, group testing, Distributed databases, checksum, deterministic construction, file organisation, fault tolerant computing, nonadaptive group testing, Aircraft, Contracts, Testing]
Performance of a decentralized knowledge base system
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
The binary predicate execution model (BPEM) is a computational model that combines logic programming, semantic nets, and message-driven computation into a paradigm for the construction of highly parallel knowledge-base systems. Simulation results are presented that demonstrate the ability of BPM to exploit effectively the resources of a loosely coupled computer network consisting of large numbers of independent processing elements. These simulations suggest performance on the order of 10/sup 5/ logical inferences per second for 256 processing elements in an n-cube configuration. A very important feature of the BPEM is that it scales-up linearly under simple OR-parallelism and AND-parallelism. Hence, the BPEM can scale-up to exploit parallelism efficiently in very large semantic networks and knowledge bases.<<ETX>>
[message-driven computation, distributed processing, digital simulation, Data mining, Distributed computing, OR-parallelism, Concurrent computing, decentralized knowledge base system, loosely coupled computer network, knowledge based systems, Parallel processing, logic programming, Computer networks, binary predicate execution model, Logic programming, Object oriented modeling, Computational modeling, Computer simulation, computer networks, performance evaluation, AND-parallelism, Computer science, highly parallel knowledge-base systems, simulation results, computational model, semantic nets]
Initializing hypercubes
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
Solutions are presented for both complete and incomplete hypercube initialization problems. Current hypercube computers require that the node number and link numbers be hard-wired into each processor and link. The author introduces algorithms for determining node and link numbers on the fly whenever the system is power up. This makes it possible to exchange or replace nodes without the need of manually setting the node number or attaching interconnect cables to particular hardware ports. One assumption made by the algorithms is that each node has a unique identifier, called the processor identifier, that is used to resolve conflicts introduced by symmetry. It is shown to be efficient for each node to know its own link count to determine a numbering. By communicating among themselves, the nodes can determine the total number of nodes in the cube.<<ETX>>
[algorithms, multiprocessing systems, node number, Routing, hypercube initialization problems, Topology, hypercube computers, Cables, Manufacturing processes, link numbers, Broadcasting, System recovery, Hypercubes, Hardware, Joining processes, processor identifier]
Programming the twisted-cube architectures
[1989] Proceedings. The 9th International Conference on Distributed Computing Systems
None
1989
A network is proposed that preserves all of the properties of the hypercube, but has a diameter which is only about half of that of the hypercube. This network is self-routing, in the sense that there is a simple distributed routing algorithm which guarantees optimal paths between any pair of vertices. This fact, together with other properties such as regularity, symmetry, high connectivity, and a simple recursive structure, implies that the multiply twisted cube is an alternative to the ordinary hypercube for massively parallel architectures. Single-input multiple-data stream algorithm were developed which utilize the new architecture. The multiply-twisted hypercube architecture can be used to profitably emulate the ordinary hypercube. Some of the basic properties of this network are discussed, the programming issues are emphasized, and it is shown that any hypercube algorithm can be mapped to run on the new architecture. In many cases this mapping results in a substantial reduction in the running time due to more efficient routing of data between processors.<<ETX>>
[SIMD algorithms, recursive structure, optimal paths, Terminology, parallel architectures, Routing, high connectivity, massively parallel architectures, Parallel architectures, regularity, Delay, Concurrent computing, self-routing, distributed routing algorithm, Network topology, symmetry, Computer architecture, Hypercubes, Computer networks, programming, hypercube properties, twisted-cube architectures]
A constructive approach to the design of distributed systems
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
A constructive design approach to distributed systems is described. The approach is illustrated by a model airport shuttle system, which is implemented in an environment for distributed programming called Conic. The main principles on which the constructive approach is based are those of explicit system structure and context-independent components. Structure is explicitly described and preserved during the software development process, from initial design to actual system construction and evolution. Thus the main structural design information is retained in the constructed system itself. The second principle that of context independence of components, reduces the design and implementation effort by facilitating early identification of component types and component interface specifications.<<ETX>>
[context-independent components, software development process, Programming, distributed processing, Educational institutions, LAN interconnection, Application software, Formal specifications, Distributed computing, structural design information, Concurrent computing, Computer languages, Message passing, airport shuttle system, Conic, distributed systems, component interface specifications, Software tools, distributed programming]
Using combination of join and semijoin operations for distributed query processing
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
A combination of join and semijoin operations is applied to minimize the communication cost for distributed query processing. A formula is developed to estimate the cardinality of a relation resulting from join operations specified by a query graph. Two important concepts which occur with the use of join operations as reducers in query processing are studied and exploited, namely, gainful semijoins and pure join attributes. Some semijoins, though not profitable themselves, may benefit from the execution of subsequent joint operations and become profitable owing to the use of join operations as reducers. Such a semijoin is termed a gainful semijoin. Also, join attributes which are not part of the output attributes are referred to as pure join attributes. A formula to estimate the cardinality of a relation resulting from a projection operation is derived. The results show the attractiveness of the approach of applying a combination of joins and semijoins as reducers to distributed query processing.<<ETX>>
[query graph, graph theory, distributed query processing, Relational databases, Distributed computing, cardinality, database theory, gainful semijoins, Tree graphs, Query processing, distributed databases, reducers, Cost function, Database systems, Polynomials, pure join attributes, projection, Data communication]
An implementation of N-party synchronization using tokens
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
In N-party synchronization, an arbitrary number of processes synchronize together and in their synchronized state these processes can execute an arbitrary computation consisting of asynchronous message transfers among them and local computations. The author presents an algorithm to implement N-party synchronization on a distributed system where processes communicate with each other via asynchronous message communication over one-to-one communication lines. The algorithm is developed in stages for better understandability. N-party synchronization is a generalization of the usual two-party synchronization.<<ETX>>
[parallel algorithms, Protocols, programming theory, distributed processing, distributed system, parallel programming, asynchronous message communication, N-party synchronization, Message passing, Intrusion detection, Signal processing, tokens, asynchronous message transfers, one-to-one communication lines]
An abortion-free distributed deadlock detection/resolution algorithm
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
A distributed deadlock detection/resolution algorithm is proposed. In this algorithm, when a deadlock cycle is detected, it is resolved by reordering the wait-for relations between pairs of transactions. Therefore, no transaction abortions are necessary to resolve deadlock cycles. This results in fewer messages and smaller transaction response time. The correctness of this abortion-free algorithm is proved. The abortion-free algorithm can be extended to handle read/write lock requests and to distinguish between transaction classes.<<ETX>>
[transaction processing, distributed processing, transaction response time, Abortion, system recovery, transaction abortions, Delay, Degradation, Distributed databases, distributed databases, Database systems, Probes, deadlock cycle, abortion-free algorithm, transaction classes, Concurrency control, Transaction databases, read/write lock requests, abortion-free distributed deadlock detection/resolution algorithm, concurrency control, System recovery, DISF, transaction reordering, Detection algorithms, wait-for relations]
Distributed processing of multimedia information
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
Requirements for distributed processing of multimedia information are summarized and compared with recent efforts at standardization. An approach toward an integrated information and communication model, which can be viewed as a special form of a model for open distributed applications, is outlined. Additionally, synchronization aspects of isochronous and anisochronous communication are outlined. The model is still under development. Especially in the areas of modeling individual information types and communication protocols, more detailed work needs to be done.<<ETX>>
[Costs, Humans, distributed processing, multimedia systems, standardization, Broadband communication, distributed applications, Communication standards, synchronisation, anisochronous communication, Distributed processing, multimedia information, information modelling, Information processing, synchronization, Hardware, Workstations, Standards development, Optical sensors]
File access characterization of VAX/VMS environments
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
A comprehensive analysis of file access behavior of several commercial production VAX/VMS environments, based on detailed I/O traces, is presented. The characterization focuses on file control operations (such as opens and closes) which are often expensive in both processing and I/O requirements. The motivation for this analysis is the need to design good data management algorithms for distributed file systems. These algorithms are particularly important for managing client caches. Most of the operations for managing client caches are performed on file opens and closes. The authors present quantitative data on the number of file opens and closes, their timing characteristics, and their declared intent to allow sharing or have exclusive access. The precise dynamic sharing of files is examined.<<ETX>>
[Algorithm design and analysis, Availability, Production systems, VAX/VMS environments, file access behavior, file control operations, distributed file systems, distributed processing, dynamic sharing, File servers, I/O traces, exclusive access, Functional analysis, Voice mail, closes, Network servers, File systems, client caches, Systems engineering and theory, file organisation, operating systems (computers), opens, data management algorithms, Business]
Slow memory: weakening consistency to enhance concurrency in distributed shared memories
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The use of weakly consistent memories in distributed shared memory systems to combat unacceptable network delay and to allow such systems to scale is proposed. Proposed memory correctness conditions are surveyed, and how they are related by a weakness hierarchy is demonstrated. Multiversion and messaging interpretations of memory are introduced as means of systematically exploring the space of possible memories. Slow memory is presented as a memory that allows the effects of writes to propagate slowly through the system, eliminating the need for costly consistency maintenance protocols that limit concurrency. Slow memory processes a valuable locality property and supports a reduction from traditional atomic memory. Thus slow memory is as expressive as atomic memory. This expressiveness is demonstrated by two exclusion algorithms and a solution to M.J. Fischer and A. Michael's (1982) dictionary problem on slow memory.<<ETX>>
[storage allocation, Transient response, unacceptable network delay, Protocols, Dictionaries, weakly consistent memories, distributed processing, exclusion algorithms, Delay, Concurrent computing, Computer science, locality property, Databases, distributed shared memories, dictionary problem, concurrency control, multiversion memory, messaging memory, slow memory, Space exploration, writes, memory correctness conditions]
Intelligent caching for remote file service
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
Limitations of current disk block caching strategies are discussed. A model for providing remote file service using knowledge-based caching algorithms is proposed. The knowledge-based algorithms generate expectations of user process behavior which are used to provide hints to the file server. Te research involved gathering trace data from a modified Unix kernel and conducting trace-driven simulation of remote file server models. Performance improvements of up to 340% were observed for knowledge-based caching in simulated file service. Comparisons are made between conventional, knowledge-based, and optimal models. Extensions to general caching are discussed.<<ETX>>
[Unix, Costs, Uncertainty, buffer storage, remote file service, trace data, Predictive models, distributed processing, File servers, Unix kernel, Application software, trace-driven simulation, Computer science, disk block caching, Operating systems, knowledge based systems, user process behavior, knowledge-based caching algorithms, file organisation, operating systems (computers), Computer networks, file server, Kernel, Contracts]
A graphical composition theorem for networks of LOTOS processes
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
An overview of some LOTOS (Language of Temporal Ordering Specification) constructs for expressing the behavior of distributed and concurrent systems is given. Some existing equivalent laws for LOTOS behavior expressions that involve the parallel composition operator and that are based on the notion of bisimulation equivalence are recalled. The graphical representation of the parallel composition of several LOTOS processes as a network of interconnected boxed is ambiguous. However, under sufficiently general conditions such graphical representation is sound; a method for deriving from any such graph a family of strongly equivalent LOTOS expressions that describe the intended process composition is introduced and proved correct. The method legitimizes the adoption of a graphical (or an equivalent textual) shorthand for such multiple compositions. It can be used for transforming the structure of parallel LOTOS expressions, and it is a generalization of previously known algebraic laws.<<ETX>>
[process gate network, Language of Temporal Ordering Specification, Formal languages, parallel composition operator, distributed processing, graphical composition theorem, LOTOS processes, Electronic mail, Graphics, Bridges, analysis by transformation, concurrent systems, specification languages, LOTOS behavior expressions, graphical representation, equivalent laws, Libraries, bisimulation equivalence, protocols]
A model of the F18 mission computer software for pre-run-time scheduling
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The development of pre-run-time schedulers for certain hard real-time applications is investigated. This scheduling scheme requires the modeling of the program as a set of processes with corresponding timing specifications, such as release times, computation times, and deadlines. The models must specify precedence and exclusion constraints between pairs of processes. It is processed by an offline pre-run-time scheduling algorithm that computes a schedule for use at run time, ensuring that all static timing constraints of the system are observed. The technique works best in cases in which most processes are periodic, which is true for the F-18. A modeling process for the M18 Mission Computer software that is designed to lead the application of pre-run-time scheduling to the software and is applicable to other such hard systems is described.<<ETX>>
[Software maintenance, Military computing, pre-run-time scheduling, Software performance, distributed processing, deadlines, precedence, release times, F/A-18 aircraft, Runtime, real-time applications, F18 mission computer software, exclusion constraints, scheduling, CF-188 avionics subsystems, operation flight program, Testing, aircraft instrumentation, computation times, Application software, timing specifications, static timing constraints, Scheduling algorithm, Processor scheduling, real-time systems, Timing, Aircraft]
On fairness as an abstraction for the design of distributed systems
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
A fairness property, called U-fairness, is studied in the context of the design of distributed systems with multiparty interactions. This is done with an overlapping model of concurrency. A distributed algorithm implementing the fairness notion is presented. U-fairness is shown to be more appropriate to the design of distributed systems than other known fairness notions because it provides an abstraction for stable property detection whereas the other fairness notions do not.<<ETX>>
[parallel algorithms, multiparty interactions, distributed processing, Microelectronics, State-space methods, Proposals, Concurrent computing, overlapping concurrency model, distributed algorithm, stable property detection, fairness property, U-fairness, Broadcasting, distributed systems, Concrete, Distributed algorithms, distributed coordination]
Performance analysis of a hierarchical quorum consensus algorithm for replicated objects
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
A class of algorithms for synchronizing a large number of identical copies of an object is described, and its performance is evaluated. The method is based on organizing a group of objects into a multilevel hierarchy and extending the quorum consensus algorithm to such an environment. The performance of the algorithm is compared, in terms of availability and message cost, with that of two other methods, majority voting and dynamic voting using a simulation model. Although the author's method uniformly outperforms the other two methods in terms of average message cost, no single method is found to dominate in terms of availability. The conditions under which each method performs well are identified.<<ETX>>
[Costs, dynamic voting, hierarchical quorum consensus algorithm, majority voting, availability, replicated objects, synchronization algorithms, File systems, Voting, distributed databases, Performance analysis, Assembly, message cost, Availability, multilevel hierarchy, performance evaluation, Concurrency control, Application software, switching theory, Organizing, database theory, synchronisation, concurrency control, simulation model, Reliability, performance analysis]
Job scheduling on a hypercube
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The problem of scheduling n independent jobs on an m-dimensional hypercube system to minimize the finish time is discussed. After some preliminary definitions are given, it is shown that a simple nonpreemptive algorithm, called largest dimension first, can have an asymptotic optimal performance and an absolute bound no worse than 2-1/2/sup m/. An absolute lower bound for the schedules generated by any algorithm using decreasing dimension lists is proved. A preemptive scheduling algorithm is also given.<<ETX>>
[Algorithm design and analysis, Heuristic algorithms, preemptive scheduling algorithm, finish time, hypercube networks, Scheduling algorithm, Information science, largest dimension first, nonpreemptive algorithm, Processor scheduling, m-dimensional hypercube system, asymptotic optimal performance, Communication channels, scheduling, Hypercubes, Approximation algorithms, Computer networks]
A new distributed optimistic concurrency control method and a comparison of its performance with two-phase locking
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
A distributed optimistic concurrency control (OCC) method followed by locking, such that locking is an integral part of distributed validation and two-phase commit is presented. This OCC method ensures that a transaction failing its validation will not be reexecuted more than once, in general. Furthermore, deadlocks, which are difficult to handle in a distributed environment, are avoided by serializing lock requests. Implementation details are outlined, and the performance of the schemes is compared with distributed two-phase locking (2PL) through a detailed simulation, which incorporates queueing effects at the devices of the computer systems, buffer management, concurrency control, and commit processing. It is shown that in the case of higher data contention levels, the hybrid OCC method allows a much higher maximum transaction throughput than distributed 2PL. The performance of the method with respect to variable-size transactions is reported. It is shown that by restricting the number of restarts to one, the performance achieved for variable-size transactions is comparable to fixed-size transactions with the same mean size.<<ETX>>
[transaction processing, distributed validation, Computational modeling, two-phase commit, Optimization methods, Throughput, Control systems, Concurrency control, Transaction databases, restarts, queueing effects, transaction throughput, performance, commit processing, buffer management, Distributed databases, concurrency control, distributed databases, System recovery, Database systems, Hardware, deadlocks, distributed two-phase locking, distributed optimistic concurrency control]
Routing objects on action paths
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The author presents a comprehensive routing system for dispatching an object between action stops, finding the principals (on the basis of functional information), notifying them in turn when their action is required, and potentially relocating the routed object to their nodes. Additional mechanisms for nagging principles who forget to act on the object, reporting progress (or lack thereof) to other principles, handling routing exceptions, sharing action paths, and routing an object on a parallel path are discussed. The routing system is built as a generic service layer above object-support services. It is easy to use, flexible, and reliable.<<ETX>>
[nagging principles, Biomedical equipment, object-oriented programming, object-oriented approach, sharing action paths, Medical control systems, Medical services, distributed processing, action stops, functional information, Routing, Control systems, routing exceptions, Hospitals, generic service layer, object-support services, Automatic control, Dispatching, Marketing and sales, Manufacturing, parallel path]
Disk cache performance for distributed systems
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The influence of client and server cache sizes and the number of clients on caching performance is studied through trace-driven simulation. The results indicate that the locality of reference in disk block reference patterns allows relatively small caches to reduce significantly the number of disk accesses required. File server cache performance is significantly different from client cache performance owing to the capture of disk block references by the client caches. The major factor influencing overall miss ratio statistics (actual disk reference frequencies) is found to be the maximum of the server cache size and the size of client caches.<<ETX>>
[Costs, buffer storage, disk accesses, distributed processing, performance evaluation, server cache sizes, disk block reference patterns, File servers, Throughput, Cache storage, Distributed computing, Statistics, trace-driven simulation, disk reference frequencies, Network servers, overall miss ratio statistics, System performance, file servers, caching performance, Frequency, client cache performance, Workstations, file server cache performance]
Structure-oriented computer architectures
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
Structure-oriented computer architecture is a research direction that tries to join the parallel processing facilities and decentralized control of multiprocessors or data-flow architectures with the efficient memory access and pipelining techniques of data structure architectures. The main concepts and the motivation for introducing this term are shown. Data structure architectures are compared with the successful concept of multiprocessors, and some limits of the latter are shown. The term structure-oriented computer architectures is introduced for a class of compound architectures built up from two rival architecture classes. Some effects of these architectures on software quality are discussed. An example of a proposed structure-oriented computer architecture is presented.<<ETX>>
[Process design, parallel architectures, Data structures, Vectors, Mathematics, software quality, parallel processing, memory access, pipelining, parallel programming, Pipeline processing, data-flow architectures, data structure architectures, decentralized control, Computer architecture, Software quality, compound architectures, Parallel processing, Distributed control, Hardware, data structures, structure-oriented computer architectures, multiprocessors]
Fast recovery in distributed shared virtual memory systems
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The problem of system failure and recovery of distributed shared virtual memory (DSVM) is studied. Most DSVM systems use the notion of tokens to indicate a site's access rights on the data pages it caches, and a locating scheme to get to the most up-to-date version of a data page. The problem is to recovery this token directory after a site has failed. The authors' solution is to treat the token directory at each site as a fragment of a global token database and the page migration activities as token transactions that update this distributed database. By the use of the unilateral commit protocol for token transactions, fast recovery of the token state at minimal run-time overhead of token transaction execution is achieved.<<ETX>>
[Protocols, caches, token directory, Random access memory, distributed processing, Distributed computing, system recovery, Concurrent computing, Intelligent networks, minimal run-time overhead, Distributed databases, distributed shared virtual memory systems, Permission, data pages, Computer networks, tokens, Local area networks, distributed database, system failure, virtual storage, Transaction databases, unilateral commit protocol, token transactions, global token database, fault tolerant computing, page migration]
On the relative execution times of distributed protocols
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
A formalism for comparing the average execution time of distributed protocols is provided. The comparisons are made independently of the properties of the network on which the protocols are executed. The formalism takes into account computation time, the time to transfer information, the time spent by a site waiting to synchronize with other sites, and the overlap among them. A framework in which the information transfer and synchronization requirements of a protocol are separately and explicitly specified is developed. A knowledge formalism is used to specify the protocol's specification requirements. Transformations on protocols which may change the synchronization structure, the information transferred, or the amount of local computation are defined. It is shown that, if a sequence of such transformations can be applied to a protocol to obtain another protocol, the final protocol runs at least as fast as the initial. Two notions of comparison, containment and reducibility, are given, and their properties are explored. Several protocols, including those for atomic commitment and snapshot recording, are analyzed to illustrate the technique.<<ETX>>
[atomic commitment, snapshot recording, Protocols, local computation, protocol transformation, distributed protocols, Read-write memory, distributed processing, relative execution times, Time measurement, Synchronization, synchronization requirements, synchronisation, Computer science, computation time, analysis by transformation, concurrency control, reducibility, Broadcasting, specification requirements, protocols, information transfer time, Clocks]
Traffic routing for multi-computer networks with virtual cut-through capability
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The problem of selecting routes for interprocess communication in a network with virtual cut-through capability, while balancing the network load and minimizing the number of times that a message gets buffered, is addressed. The approach taken is to formulate the route selection problem as a minimization problem, with a link cost function that depends on the traffic through the link. The form of this cost function is derived on the basis of the probability of establishing a virtual cut-through route. It is shown that this route selection problem is NP-hard, and so an approximate algorithm that tries to reduce the cost incrementally by rerouting traffic is developed. The performance of this algorithm is evaluated for two popular network topologies: the hypercube and the C-wrapped hexagonal mesh.<<ETX>>
[NP-hard, network load, link cost function, Multiprocessor interconnection networks, Communication system control, multiprocessor interconnection networks, Telecommunication traffic, message buffering, Delay, C-wrapped hexagonal mesh, approximate algorithm, interprocess communication, virtual cut-through capability, Cost function, Hypercubes, Packet switching, hypercube, probability, virtual cut-through route, Routing, Application software, traffic routing, route selection problem, minimization problem, network topologies, multi-computer networks, Telecommunication network reliability, computational complexity]
Support for continuous media in the DASH system
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The DASH resource model is defined as a basis for reserving and scheduling resources (disk, CPU, network, etc.) involved in end-to-end handling of continuous-media (information flowing continuously over real time i.e. digital audio or digital video) data. The model uses primitives that express work-load characteristics and performance requirements, and defines an algorithm for negotiated reservation of distributed resources. This algorithm is embodied in the session reservation protocol, a backward-compatible extension of the Internet Protocol. Hardware trends and future applications that motivate the DASH resource model are described. The performance requirements for using continuous media and the limitations of existing systems are discussed. The DASH resource model for reserving and scheduling resources is presented. The DASH kernel is briefly described.<<ETX>>
[negotiated reservation of distributed resources, Protocols, digital audio, continuous-media, session reservation protocol, distributed processing, resource allocation, continuous real time systems, Video compression, Telephony, scheduling, TCP/IP, Hardware, Workstations, protocols, digital video, performance requirements, Object oriented modeling, continuous data, DASH resource model, work-load characteristics, performance evaluation, Scheduling, Application software, Optical arrays, Computer science, real-time systems, Internet Protocol]
Highly concurrent directory management in the Galaxy distributed system
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
An efficient method of consistency control of replicated directories is presented. By taking advantage of special characteristics of directories, the method achieves fast access to directories and high concurrency in updating directory replicas. The algorithm differs from conventional mechanisms for concurrency control of replicated data in two aspects: It does not use global locks or global timestamp orderings. Updating operations can proceed without being in synchronization. The algorithm can survive both node failure and network failure. The directory problem, design objectives and related works are described. The system model and consistency control requirements are defined, and the data structures and algorithm are presented. The fault tolerance and recovery mechanism of the approach are discussed, as is the applicability of the algorithm. The approach is evaluated and compared with other works. The detailed algorithm and consistency proof are given.<<ETX>>
[fault tolerance, replicated directories, concurrent directory management, Control systems, Concurrency control, Galaxy distributed system, consistency control, Information management, Distributed computing, Concurrent computing, fault recovery, Information science, Fault tolerance, storage management, Content addressable storage, Fault tolerant systems, concurrency control, data structures, IEEE directories, directory replicas, algorithm]
The synchronic group: a concurrent programming concept and its proof logic
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The authors have developed a programming logic for Swarm that is similar in style to that of UNITY. They show how the proof logic for Swarm can be extended to accommodate the dynamic formation of synchronic groups specified by the run-time redefinition of the synchrony relation. The basic Swarm notation is reviewed, the notation for the synchrony relation is introduced, and the concept of a synchronic group is discussed. The use of synchronic groups is illustrated by means of a program for labeling regions in an image unbounded on one side. A UNITY-style assertional programming logic for Swarm without the synchrony relation is reviewed, and the logic is generalized to accommodate synchronic groups.<<ETX>>
[Logic programming, Computational modeling, dynamic formation of synchronic groups, concurrent programming, UNITY, Data structures, dynamically created transactions, synchronic group, Vehicle dynamics, parallel programming, Vehicles, proof logic, Computer science, Concurrent computing, Computer languages, Swarm, Production, synchrony relation, shared dataspace language, parallel languages, programming logic, run-time redefinition]
Regeneration-based multiversion dynamic voting scheme for replicated database systems
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
In replicated database systems, a replication control protocol is needed to ensure one-copy serializability. The author incorporates the concept of a regeneration into the missing-partition dynamic voting scheme to design a replication control protocol. Like the original missing-partition dynamic voting scheme, this protocol supports an inexpensive read operation which accesses one copy, rather than all copies, of each data item read. By incorporating the concept of regeneration and keeping multiple versions for each data item in the database, higher data availability is maintained. To support data regeneration, a replicated directory architecture for the proposed replication control protocol is designed, and it not only supports regeneration of replicated data items, but also provides inexpensive, high-availability directory services, which help maintain database availability.<<ETX>>
[Availability, replication control protocol, missing-partition dynamic voting scheme, data regeneration, Communication system control, Access protocols, multiversion dynamic voting scheme, Control systems, Concurrency control, Computer crashes, Transaction databases, switching theory, database theory, replicated database systems, one-copy serializability, Voting, Distributed databases, concurrency control, distributed databases, data availability, Database systems, replicated directory architecture]
Two adaptive location policies for global scheduling algorithms
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
Two location policies that, by adapting to the system load, capture the advantages of receiver-initiated, sender-initiated, and symmetrically initiated algorithms are presented. A key feature of these location policies is that they are general and can be used in conjunction with a broad range of existing transfer policies. By means of simulation, two representative algorithms making use of these adaptive location policies are shown to be stable and to improve performance significantly relative to nonadaptive policies.<<ETX>>
[sender-initiated, Costs, Ethernet networks, distributed processing, representative algorithms, Distributed computing, Scheduling algorithm, simulation study, global scheduling algorithms, Processor scheduling, resource allocation, symmetrically initiated algorithms, scheduling, Performance analysis, Central Processing Unit, adaptive location policies, receiver-initiated, Probes]
Overview of disaster recovery for transaction processing systems
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
An overview is given of the major issues involved in maintaining an up-to-date backup copy of a database, kept at a remote site. A method is presented for performing this task without impairing the performance at the primary site. The method is scalable, and it is particularly suitable for multiprocessor systems. The mechanism is relatively straightforward and can be implemented using well-known concepts and techniques, such as locking and logging.<<ETX>>
[Availability, transaction processing, Protocols, multiprocessor systems, Software performance, locking, disaster recovery, Transaction databases, backup copy, Delay, Multiprocessing systems, Computer science, database, Earthquakes, concurrency control, distributed databases, Computer errors, logging, Hardware, primary site, remote site]
Specification and identification of events for debugging and performance monitoring of distributed multiprocessor systems
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The design of a debugging and performance analysis system that includes a specification language for process-level events and hardware for nonintrusive identification of these events during the execution of parallel and distributed application for a nonshared memory system is presented. The design is based on a formal event/action model and a layered architecture model that have been previously presented. Background, related work, and specification, and identification of events are discussed.<<ETX>>
[multiprocessing systems, distributed multiprocessor systems, Computerized monitoring, Laboratories, Debugging, distributed processing, performance evaluation, Data structures, process-level events, formal event/action model, Application software, distributed application, Programming profession, layered architecture model, Multiprocessing systems, Condition monitoring, nonintrusive identification, specification languages, Parallel processing, Hardware, nonshared memory system, performance analysis, specification language]
Optimal static load balancing of multi-class jobs in a distributed computer system
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
This model is an extension of the Tantawi and Towsley (1985) single-job-class model as applied to a multiple-job-class model. Some properties of the optimal solution are shown. On the basis of these properties, a straightforward and efficient algorithm for optimal load balancing of multiclass jobs is derived. The performance of this algorithm is compared with that of two other well-known algorithms for multiclass jobs, the flow deviation (FD) algorithm and the Dafermos algorithm. The authors' algorithm and the FD algorithm both require a comparable amount of storage that is far less than that required by the Dafermos algorithm. Numerical experiments show that for obtaining the optimal solution the authors' algorithm and the Dafermos algorithm require comparable computation times that are far less than that of the FD algorithm.<<ETX>>
[Transportation, optimal static load balancing, Telecommunication traffic, distributed processing, performance evaluation, distributed computer system, Mathematics, multi-class jobs, Distributed computing, Delay, Computer science, Dafermos algorithm, resource allocation, Load management, Computer networks, Mathematical model, Communication networks, flow deviation algorithm]
Implementing fault-tolerant distributed applications using objects and multi-coloured actions
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The authors develop some control structures suitable for composing fault-tolerant distributed applications using atomic actions (atomic transactions) as building blocks. The authors describe how such structures may be implemented using the concept of multicolored actions. The reasons why other control structures, in addition to nested and concurrent atomic actions, are desirable and are identified, and three structures are proposed: serializing actions, glued actions, and top-level independent actions. A number of examples are used to illustrate their usefulness. A technique based on the concept of multicolored actions is presented as a uniform basis for implementing all of the three action structures presented.<<ETX>>
[atomic actions, Laboratories, concurrent atomic actions, distributed processing, multi-coloured actions, Control systems, control structures, serializing actions, Distributed computing, system recovery, Fault tolerance, Fault tolerant systems, Workstations, atomic transactions, object-oriented programming, Object oriented modeling, Computational modeling, multicolored actions, glued actions, fault-tolerant distributed applications, Computer crashes, concurrency control, Distributed control, top-level independent actions, fault tolerant computing]
Asynchronous remote operation execution in distributed systems
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The authors present the design and implementation of an asynchronous remote operation execution facility, futures, that retains the benefits of the remote procedure call (RPC) abstraction but allows execution to proceed locally in parallel with remote execution and provides extensive support for managing replies. It is shown how this facility can be easily used to support many common interprocess communication styles, including RPC, multicast, broadcast, returning incremental results, and the multiplexing of multiple remote computations. A flow control mechanism for futures is described.<<ETX>>
[broadcast, reply management, distributed processing, futures, flow control, Distributed computing, Delay, multicast, interprocess communication styles, Message passing, multiplexing, asynchronous remote operation execution, Parallel processing, Broadcasting, User interfaces, distributed systems, remote procedure call, remote execution, protocols, multiple remote computations, Contracts, returning incremental results]
Programming language support for multicast communication in distributed systems
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
A spectrum of abstractions for multicast communications is introduced in increasing order of both desirability and semantic level: functional mapping, iterators, and streams. Examples of distributed algorithms from the literature are used to illustrate the expressive power of each mechanism. Streams, in particular, provide first-class status for multicast communication in progress and can be implemented efficiently in typical multicast communication architectures. Related work and the criteria for multicast support are discussed. The layers of a typical distributed system that are relevant to language support for multicast communication are discussed.<<ETX>>
[Transport protocols, iterators, Communication system control, high level languages, functional mapping, streams, Multicast communication, distributed processing, Multicast protocols, Data structures, language support, abstractions, Computer science, Computer languages, type safety, first class continuations, distributed algorithms, multicast communication, Internet, Safety, Distributed algorithms]
Optimal routing in the de Bruijn networks
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The problem of optimal routing in an interconnection network, called the de Bruijn network, where the sites are linked in the form of a de Bruijn graph, is considered. The distance functions for both the undirected and directed de Bruijn graphs are provided. The optimal routing problem is then reduced to that of pattern matching. Morris and Pratt's (1970) failure function and Weiner's (1973) prefix tree are used to develop algorithms that find the shortest paths in the unidirectional and bidirectional de Bruijn networks, respectively. These algorithms are linear in time and space (in the diameter of the graph). When approximately implemented, these linear algorithms have constant factors low enough to make them of practical use.<<ETX>>
[pattern matching, Multiprocessor interconnection networks, Neodymium, graph theory, multiprocessor interconnection networks, Shift registers, prefix tree, bidirectional de Bruijn networks, Multiprocessing systems, Intelligent networks, Tree graphs, interconnection network, Computer networks, distance functions, directed de Bruijn graphs, optimal routing, undirected de Bruijn graphs, computer networks, Routing, failure function, unidirectional de Bruijn networks, shortest paths, Bidirectional control, linear algorithms, Pattern matching]
Real-time synchronization protocols for shared memory multiprocessors
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
A priority-based synchronization protocol that explicitly uses shared-memory primitives is defined and analyzed. A solution that has been proposed for bounding and minimizing synchronization delays in real-time systems is briefly reviewed. The waiting times introduced by synchronization requirements in multiple-processor environments are identified, and a set of goals for priority-based multiprocessor synchronization protocols is derived. The underlying priority consideration for a shared memory synchronization protocol are studied and priority assignments to be used by the protocol are derived.<<ETX>>
[Real time systems, Availability, synchronization delays, multiprocessing systems, waiting times, Delay systems, Process control, Access protocols, distributed processing, shared memory multiprocessors, shared-memory primitives, priority-based synchronization protocol, Scheduling algorithm, synchronisation, priority assignments, Processor scheduling, delays, real-time systems, Hardware, multiprocessor synchronization protocols, Resource management, protocols, Board of Directors]
Concurrent analysis of structures on shared memory machines
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
A practical solution to the structural analysis problem in a parallel processing environment is investigated through the use of the notion of cheap concurrency and the concept of thread. Portions of a structural analysis code implemented in C are parallelized using the encore parallel threads on an encore multimax multiprocessor computer. The issues of racing condition, synchronization, and mapping are considered and discussed. Results are reported on the effect of amount and frequency of shared memory access on the speedup. A discussion of the overhead time required for creating threads and a comparison of the overall computational time performance using two examples are presented.<<ETX>>
[racing condition, C, parallel architectures, Yarn, parallel machines, shared memory machines, parallel programming, Concurrent computing, cheap concurrency, Parallel processing, Assembly, Civil engineering, mapping, encore parallel threads, shared memory access, structural analysis code, Vectors, civil engineering computing, Equations, Stress, encore multimax multiprocessor computer, Computer languages, synchronization, parallel processing environment, Frequency synchronization]
Towards a combinative distributed operating system in CLUSTER 86
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
CODOSO, a combinative distributed operating system family that serves as an integrated environment for building various distributed operating systems according to different application requirements, is discussed. The basis of the CODOSO project is CLUSTER 86, an object-oriented programming languages. CODOSO is designed in an object model based on the language, in which the entire system is implemented. The authors discuss how the CODOSO distributed computing models are built up through CLUSTER 86. They outline the design philosophy and the implementation strategy, and present a brief comparison with some other related work.<<ETX>>
[Tree data structures, object-oriented programming, Object oriented modeling, Buildings, high level languages, behavior objects, distributed processing, Application software, Distributed computing, Sun, distributed activity models, Computer science, Computer languages, Operating systems, network operating systems, distributed computing models, object-oriented programming languages, combinative distributed operating system, Object oriented programming, CODOSO, CLUSTER 86]
Open commit protocols for the tree of processes model
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The authors propose three different two-phase commit (2PC) protocols that ensure that, even in the presence of permanent node failures, all sane nodes participating in a particular transaction eventually terminate the transaction in a consistent way. A node is defined to be sane if it eventually recovers from failures. These protocols, called open 2PC protocols, are based on a tree-of-processes model and provide means for transferring the coordinator function within process trees. Besides describing the open 2PC protocols in detail, the authors compare these protocols with regard to their message and time complexity. They also include a discussion of related work.<<ETX>>
[Availability, open 2PC protocols, tree of processes model, process trees, Protocols, open two phase commit protocols, ISO, failure recovery, Banking, Switches, Standardization, time complexity, sane nodes, coordinator function, Network servers, message complexity, Libraries, Workstations, Communication networks, protocols]
Performance evaluation of real-time locking protocols using a distributed software prototyping environment
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The problem of priority scheduling in real-time database systems is addressed. A prototyping environment for investigating distributed software is presented. Specific priority-based real-time locking protocols are discussed, together with a performance study which illustrates the use of the prototyping environment for evaluation of synchronization protocols for real-time database systems. The priority ceiling protocol, which achieves a high degree of schedulability and system predictability, is investigated, and its performance is compared with that of other techniques and design choices. It is shown that this technique might be appropriate for real-time transaction scheduling since it is very stable over the wide range of transaction sizes, and compared with two-phase locking protocols, it reduces the number of deadline-missing transactions.<<ETX>>
[Real time systems, priority-based real-time locking protocols, system predictability, Protocols, software prototyping, schedulability, distributed processing, synchronization protocols, performance study, Distributed databases, Prototypes, distributed databases, scheduling, Database systems, distributed software prototyping environment, priority ceiling protocol, Software prototyping, Job shop scheduling, real-time database systems, performance evaluation, Transaction databases, synchronisation, Computer science, real-time transaction scheduling, deadline-missing transactions, real-time systems, real-time locking protocols, Timing]
Applying Petri net reduction to support Ada-tasking deadlock detection
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The application of Petri net reduction to Ada-tasking deadlock detection is investigated. Net reduction can ease reachability analysis by reducing the size of the net while preserving relevant properties. By combining Petri net theory and knowledge of Ada-tasking semantics some specific efficient reduction rules are derived for Petri net models of Ada-tasking. A method by which a useful description of a detected deadlock state can be easily obtained from the reduced net's information is suggested.<<ETX>>
[Algorithm design and analysis, reachability analysis, program verification, Petri nets, Laboratories, distributed processing, model reduction, Ada-tasking deadlock detection, Displays, Face detection, Reachability analysis, reduction rules, Information analysis, analysis by transformation, Petri net reduction, Ada-tasking semantics, concurrency control, System recovery, Software systems, Petri net theory, FETs, Ada]
Distributed Hartstone: a distributed real-time benchmark suite
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
An extension of the uniprocessor Hartstone benchmark for the distributed real-time environment, called the Distributed Hartstone benchmark, is described. The Distributed Hartstone measures system performance in the critical areas of communication latency and bandwidth, protocol preemptability, and priority queueing at the protocol and media access levels. Areas of the system which are particularly important for distributed, real-time computing are described. On the basis of the requirements that specify various areas of the system that a distributed real-time benchmark must stress, a series of task sets in the style of the Hartstone benchmarks are given. The benchmark results from a distributed real-time operating system (ARTS testbed) are given.<<ETX>>
[Real time systems, distributed real-time operating system, Distributed Hartstone benchmark, Area measurement, bandwidth, distributed real-time benchmark suite, distributed processing, performance evaluation, communication latency, priority queueing, Distributed computing, Delay, Stress, distributed real-time environment, protocol preemptability, System performance, Operating systems, network operating systems, real-time systems, Bandwidth, Benchmark testing, system performance, Media Access Protocol]
A rate-based congestion avoidance and control scheme for packet switched networks
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
A congestion avoidance and control scheme that monitors the incoming traffic to each destination and provides rate-based feedback information to the sources of bursty traffic so that sources of traffic can adjust their packet rates to match the network capacity is described. The congestion avoidance mechanism at nodes on the periphery of the network controls incoming traffic so that it does not exceed the capacity of paths to different destinations. The congestion control mechanism at each node monitors the performance of adjacent links and generates rate control messages that warn the sources of traffic before congestion develops. Some existing schemes are reviewed, and the congestion avoidance and control scheme and its applicability to various transport protocols are discussed. Experiments show that the scheme is effective in preventing congestion inside the network and that it manages to restrict the traffic on any overloaded path to 80%-90% of its capacity.<<ETX>>
[bursty traffic sources, performance monitoring, packet switching, rate control messages, Telecommunication traffic, packet switched networks, destination, Delay, Degradation, Intelligent networks, incoming traffic monitoring, Feedback, network capacity, Traffic control, Communication system traffic control, rate based congestion control, protocols, rate-based feedback information, packet rates, Packet switching, switching theory, Computer science, nodes, transport protocols, paths, Resource management, adjacent links, rate-based congestion avoidance]
Symmetry in spite of hierarchy
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The authors present a revolving hierarchical scheme in which the logical position of a process in the hierarchy changes with time so that the reorganization of hierarchy is achieved concurrently with its use. The technique is useful for repeated computation of global functions that require information from all processes. It results in algorithms that are not only fair to all nodes, but also less expensive in terms of messages. The reduction in the number of messages is achieved by reusing messages for more than one computation of the global function. The technique is illustrated for hierarchical snapshot computation and distributed branch-and-bound problems.<<ETX>>
[restricted message reception, distributed branch and bound problems, distributed processing, distributed snapshot, distributed branch-and-bound problems, Synchronization, Distributed computing, Relays, Concurrent computing, Fault tolerance, global functions, message reduction, hierarchical snapshot computation, concurrency control, results dissemination, revolving hierarchical scheme, System recovery, Workstations, message reuse, logical position, Clocks, global function]
Performance of a hierarchically interconnected multiprocessor
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
A queuing model of a parallel processor with an interconnection network incorporating a hierarchy of paths is developed and analyzed. The model captures the behavior of the processors, the interconnection network, and the storage modules. The network considered includes fast paths that operate in the absence of contention and alternate paths with contention resolution. The network overall performance is shown to be close to that of a contention-free network of fast paths. It is shown that, as the load varies, this hierarchical interconnection network is robust with respect to ideal networks with no delay. An analysis of the effects of hot spots shows that processor throughput is limited by storage rather than communications bandwidth and that an upper bound on the processor utilization is inversely proportional to the miss probability. The analysis suggests that a fetch-and-add network could be incorporated into a connection hierarchy whose average performance is close to that of a network with no combining, so this may be an effective way to handle hot spots without much penalty to overall system performance.<<ETX>>
[Costs, queuing model, Multiprocessor interconnection networks, Circuits, multiprocessor interconnection networks, alternate paths, Switches, Throughput, Delay, interconnection network, load, Bandwidth, Robustness, storage modules, connection hierarchy, queueing theory, fetch-and-add network, parallel processor, performance evaluation, contention resolution, processor throughput, Communication switching, Upper bound, hierarchically interconnected multiprocessor, fast paths, hot spots, system performance]
Tarmac: a language system substrate based on mobile memory
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
Tarmac, a language system substrate on which systems for distributed parallel programming can be built, is described. A model of shared global state, called mobile memory, which is provided by Tarmac, is discussed. The basic unit of state in this model can be viewed both (1) as a block of memory that can be directly accessed by machine instructions and (2) as a logical entity with a globally unique name that may be efficiently located, copied, and moved. To support higher level synchronization models, the movements of a memory unit may optionally enable computations. The implementation and performance of Tarmac are discussed. Tarmac is contrasted with other systems for parallel distributed programming.<<ETX>>
[shared global state, Object oriented modeling, mobile memory, virtual storage, Tarmac, globally unique name, Distributed computing, parallel distributed programming, parallel programming, Computer science, Concurrent computing, Computer languages, storage management, Parallel programming, Operating systems, Computer networks, logical entity, Object oriented programming, parallel languages, higher level synchronization models, Mobile computing, language system substrate, distributed parallel programming]
Opportunistic evaluation of communication link loads
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
Algorithms for measuring the loads on the communication links in a distributed system are presented. Timestamped messages are used to measure latencies across physical and virtual links in the system. The algorithms do not generate any additional message traffic. Instead, load measurement information is opportunistically piggybacked onto application messages. The link load values can be used to initiate process migration and for dynamic flow control. An example of how loads on virtual links may be measured is given.<<ETX>>
[load measurement algorithms, Laboratories, latencies, Communication system control, distributed processing, distributed system, Control systems, load measurement information, Delay, physical links, process migration, Fluid flow measurement, Runtime, timestamped messages, Communication system traffic control, application messages, switching theory, message traffic, Distributed control, dynamic flow control, Load management, virtual links, Joining processes, communication link loads, link load]
A heterogeneous distributed file system
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The demands on a heterogeneous distributed file system are outlined, and the design and implementation of a prototype to meet these demands are described. This prototype, the heterogeneous computer systems file system (HFS), provides a network-wide file system supporting a simple record-oriented file model. Through this standard file model, the HFS provides global access to files stored locally in many different file types. The HFS is implemented as a set of HFS servers, one running on each participating host. Each HFS server extends its host's local file system by fielding remote requests for files stored locally, translating those requests into the appropriate local file system calls, and returning any information so obtained. This prototype HFS implementation is used on a network composed of VAX systems running Unix, Sun systems running 4.2BSD Unix, and Xerox Dandelions running XDE.<<ETX>>
[local file system, Costs, HFS servers, Sun systems, VAX systems, distributed processing, File servers, file types, Distributed computing, global access, Network servers, File systems, Operating systems, Prototypes, Xerox Dandelions, host, XDE, Hardware, remote requests, heterogeneous distributed file system, heterogeneous computer systems file system, 4.2BSD Unix, Sun, record-oriented file model, Computer science, file organisation, network-wide file system]
Axon network virtual storage for high performance distributed applications
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
An introduction to the Axon architecture is presented. Axon is a host communication architecture for the distributed systems which can support interprocess communication with high throughput and low latency across a very high-speed internetwork. Network virtual storage (NVS) in the Axon host communication architecture for distributed applications is described. NVS extends segmented paged virtual storage management and address translation mechanisms to include segments located across an internetwork. This provides the ability to use efficiently the shared-memory paradigm in distributed systems for high-performance applications, such as scientific visualization and imaging.<<ETX>>
[Nerve fibers, Visualization, address translation mechanisms, high performance distributed applications, Axon architecture, Computational modeling, very high-speed internetwork, virtual storage, distributed processing, Axon host communication architecture, segmented paged virtual storage management, Image segmentation, High performance computing, Operating systems, network operating systems, interprocess communication, Bandwidth, Computer architecture, National electric code, Axon network virtual storage, Workstations, shared-memory paradigm]
Structural properties of incomplete hypercube computers
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
Incomplete hypercubes are analyzed. The elementary properties of complete hypercubes and a routing algorithm for incomplete hypercubes are briefly reviewed. Structural properties, including diameter, mean message traversal, and traffic density, of incomplete hypercube computers with size 2/sup n/+2/sup k/, 0<or=k<n, are investigated and presented. It is shown that traffic density in such an incomplete hypercube is bounded by two, despite its structural nonhomogeneity. Thus, cube links can easily be constructed so as to avoid any single point of congestion, guaranteeing good performance.<<ETX>>
[traffic density, mean message traversal, Routing, hypercube networks, Topology, Research and development, Degradation, Concurrent computing, structural properties, diameter, routing algorithm, Computer architecture, System recovery, Broadcasting, Hypercubes, complete hypercubes, cube links, congestion, incomplete hypercube computers]
Application-level programming
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The use of a declarative language, called Dura, designed to support application-level programming is illustrated by distributed avionics system. The authors show how the language is used to describe the application, its components and structure; how the run-time executive provides support for fault-tolerance by reconfiguration of the application; and how an interactive interface to the executive supports debugging and monitoring of the application.<<ETX>>
[Dura, fault-tolerance, Communication system control, high level languages, declarative language, Debugging, application reconfiguration, distributed avionics system, Aerospace electronics, distributed processing, triplicated flight control system, Application software, application-level programming, parallel programming, Fault tolerance, interactive interface, Runtime, aerospace computer control, fault tolerant computing, Resource management, Telecommunication network reliability, Monitoring, Software engineering]
History, an intelligent load sharing filter
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The author proposes a filter component to be included in a load-sharing algorithm to detect short-lived jobs not worth considering for remote execution. Three filters are presented. One filter, called History, detects short-lived jobs by using job names and statistics based on previous executions. Job traces are allocated from diskless work stations connected by a local area network and supported by a distributed file system. Trace-driven simulation is then used to evaluate History with respect to the other filters. Two load-sharing algorithms show significant improvement of the mean job response ratio when the History filter is added.<<ETX>>
[short lived job detection, Computational modeling, distributed processing, load-sharing algorithms, short-lived jobs, local area networks, History, trace-driven simulation, mean job response ratio, Filters, File systems, resource allocation, High performance computing, Statistical distributions, local area network, job names, History filter, Computer networks, diskless work stations, Workstations, intelligent load sharing filter, remote execution, Communication networks, Local area networks]
Performance evaluation of the backtrack-to-the-origin-and retry routing for hypercycle-based interconnection networks
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
Hypercycles, a class of multidimensional graphs that are generalizations of the n-cube are presented. These graphs are obtained by allowing each dimension to incorporate more than two elements and a cyclic interconnection strategy. Hypercycles offer simple routing and the ability, given a fixed degree, to choose among a number of different size graphs. These graphs can be used in the design of interconnection networks for distributed systems tailored specifically to the topology of a particular application. A back track-to-the-origin-and-retry routing, whereby paths that block at intermediate nodes are abandoned and a new attempt is made, is presented. Intermediate nodes are chosen at random at each point from among those that form the shortest paths from a source to a destination. Simulation results that establish the performance of a variety of configurations are presented. An initial attempt at constructing a hypercycle-based router is discussed.<<ETX>>
[Embedded computing, Multidimensional systems, Multiprocessor interconnection networks, Computational modeling, graph theory, topology, performance evaluation, destination, Routing, hypercube networks, source, backtrack-to-the-origin-and retry routing, Network topology, nodes, Message passing, multidimensional graphs, paths, Hypercubes, distributed systems, Computer networks, Joining processes, dimension, hypercycle-based interconnection networks]
On replay detection in distributed systems
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
Various approaches to the problem of replay detection in distributed systems are briefly reviewed. An approach based on combining a variable-size time-window mechanism with a challenge mechanism is proposed. This approach has the following properties: (1) it does not depend on clock synchronization, (2) it allows the setting of a minimum server's memory-buffer size in a way that ensures acceptance of all legitimate client requests and (3) it is robust without requiring stable (nonvolatile) memory for the server buffer needed to save past client requests.<<ETX>>
[service request messages, Data security, guaranteed continuous service availability, distributed processing, challenge mechanism, Educational institutions, memory-buffer size, legitimate client requests, Synchronization, Servers, Communication system security, variable-size time-window mechanism, Degradation, Nonvolatile memory, network operating systems, Propagation losses, distributed systems, fault tolerant computing, second chance approach, Cryptography, replay detection, Clocks]
Causal distributed breakpoints
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The authors define a causal distributed breakpoint, which is initiated by a sequential breakpoint in one process of a distributed computation and restores each process in the computation to its earliest state that reflects all events that happened before the breakpoint. An algorithm for finding the causal distributed breakpoint, given a sequential breakpoint in one of the processes, is presented. Approximately consistent checkpoint sets are used for efficiently restoring each process to its state in a causal distributed breakpoint. Causal distributed breakpoints assume deterministic processes that communicate solely by messages. The dependencies that arise from communication between processes are logged. Dependency logging and approximately consistent checkpoint sets are implemented on a network of SUN workstations running the V-System. Overhead on the message-passing primitives varies between 1% and 14% for dependency logging. Execution time overhead for a 200*200 Gaussian elimination is less than 4% and generates a dependency log of 288 kbytes.<<ETX>>
[program debugging, sequential breakpoint, message-passing, distributed processing, approximately consistent checkpoint sets, distributed computation, Distributed computing, Sun, message passing overheads, Programming profession, parallel programming, dependency logging, Computer science, Message passing, Gaussian elimination, V-System, Workstations, causal distributed breakpoint, SUN workstations, Contracts]
Protection in the BirliX operating system
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The user-interface-level and implementation-level protection mechanisms of the BirliX operating system are described and motivated. Descriptions are provided of subject restriction and object protection as complementary user-interface-level mechanisms for enforcing security policies by building small domains of protection. Traceability is used as a basis for tracing back violations of policies to answerable humans.<<ETX>>
[Access control, Humans, distributed processing, BirliX operating system, implementation-level protection mechanisms, user interfaces, acl, Security, Computer crime, Privacy, Operating systems, network operating systems, Permission, subject restriction, Protection, identification tags, user interface level security, message protection, traceability, abstract data type management, Computer science, adt, security of data, object protection, security policies, User interfaces, access control lists, security domains]
A basic unit of computation in distributed systems
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The authors define basic units of computation in distributed systems, whether communicating synchronously or asynchronously, as comprising indivisible logical units of computation that take the system from one ground state to another. It is explained how a computation can be viewed as a partial order over the basic units of the computation. The problem of detecting the basic units is considered. One algorithm for creating ground states during a computation in an asynchronously communicating system with FIFO channels is given, and an existing algorithm that implicitly creates ground states in a synchronously communicating system is referenced. The significance of the basic unit is explained, and its applications are given.<<ETX>>
[Checkpointing, parallel algorithms, ground states, Stationary state, Communication system control, Debugging, distributed processing, asynchronously communicating system, synchronously communicating system, Mathematics, Distributed computing, indivisible logical units of computation, Concurrent computing, Information science, Runtime, Communication channels, distributed systems, partial order]
Multidimensional voting: a general method for implementing synchronization in distributed systems
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
A concept called multidimensional voting, in which the vote and quorum assignments are k-dimensional vectors of nonnegative integers and each dimension is independent of the others, is introduced. Multidimensional voting is more powerful than traditional weighted voting because it is equivalent to the general method for achieving synchronization in distributed systems which is based on coteries (sets of groups of nodes), but its implementation is easier than that of coteries. An efficient algorithm for finding a multidimensional vote assignment for any given coterie is described and examples of its use are shown. It is shown how multidimensional voting can be used to easily implement novel algorithms for synchronizing access to replicated data or to ensure mutual exclusion. These algorithms cannot be implemented by traditional weighted voting.<<ETX>>
[Availability, System testing, Multidimensional systems, Costs, mutual exclusion, distributed processing, k-dimensional vectors, nonnegative integers, synchronisation, Computer science, nodes, Voting, Fault tolerant systems, concurrency control, Writing, Permission, synchronization, distributed systems, quorum assignments, vote assignments, protocols, coteries, multidimensional voting, replicated data]
Achieving high availability in a replicated file system by dynamically ordering transactions
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
A pessimistic consistency-control algorithm is presented. The principle of the algorithm is to order transactions dynamically in the global serialization graph of the system. This order is built by associating numbers with transactions in such a way that these numbers decrease along any path in the graph. Transaction numbers correspond to an arbitrary ordering of replicated objects. The model of the distributed system and the transaction manager are described. Replicated objects and the basic access strategy used to illustrate the method are introduced. Both the access algorithm, with a proof of its correctness, and the recovery algorithm are described. The generalization of the method to other pessimistic algorithms and several related issues are discussed.<<ETX>>
[transaction processing, global serialization graph, Protocols, Heuristic algorithms, Design methodology, Optimization methods, replicated file system, distributed processing, pessimistic consistency-control algorithm, distributed system, system recovery, access algorithm, File systems, basic access strategy, distributed databases, Robustness, Database systems, transaction manager, Availability, recovery algorithm, Partitioning algorithms, dynamic transaction ordering, high availability, concurrency control, Frequency, adaption algorithm, fault tolerant computing]
A branch-and-bound-with-underestimates algorithm for the task assignment problem with precedence constraint
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The problem of finding an optimal assignment of task modules with a precedence relationship in a distributed computing system is considered. The objective of task assignment is to minimize the task turnaround time. The problem is known to be NP-complete for more than three processors. To solve the problem, a well-known state-space reduction technique, branch-and-bound-with-underestimates, is applied, and two underestimate functions are defined. Through experiments, their effectiveness is shown by comparing the proposed algorithm with both Wang and Tsai's (1988) algorithm and the A* algorithm with h(x)=0.<<ETX>>
[Costs, state-space reduction technique, Minimax techniques, distributed processing, precedence constraint, NP-complete, State-space methods, Distributed computing, distributed computing system, Computer science, Distributed processing, optimisation, resource allocation, Microprocessors, System performance, task turnaround time, branch-and-bound-with-underestimates algorithm, Computer applications, optimal assignment, task assignment, State estimation, state-space methods]
Hierarchical communication in cube-connected multiprocessors
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
Interconnection structures that can provide access to multiple levels of a shared memory hierarchy in a multiprocessor are investigated. The results are also applicable to distributed memory architectures in which localities of communication can be statically defined. All the structures presented conform in some fashion to the binary cube topology with per-processor logic cost ranging from O(log N) to O(log/sup 2/N). The results illustrate that without resorting to separate networks for access at each level, several architectures can provide fast access at lower levels in the hierarchy and progressively slower access at higher levels. Even at the highest communication level (corresponding to system wide communication), messages encounter less delay than in a nonhierarchical access situation.<<ETX>>
[Context, architectures, Costs, interconnection structures, shared memory hierarchy, distributed memory architectures, binary cube topology, Memory architecture, Laboratories, multiprocessor interconnection networks, cube-connected multiprocessors, Delay, Communication switching, Network topology, Processor scheduling, Hypercubes, hierarchical communication, Logic, computational complexity]
Reducing host load, network load, and latency in a distributed shared memory
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
Changes made to a distributed shared memory that runs of Sun workstations under SunOS 4.0 operating systems, called Mether, are outlined. Protocols developed to use Mether and that minimize host load, network load, and latency are discussed. The changes to Mether involve exposing an inconsistent store to the applications and making access to the consistent and inconsistent versions very convenient, providing both demand-driven and data-driven semantics for updating pages, and allowing the user to specify that only a small subset of a page needs to be transferred.<<ETX>>
[storage allocation, distributed shared memory, network load, Sun workstations, Access protocols, latency, Read-write memory, distributed processing, performance evaluation, Sun, Delay, SunOS 4.0, Intelligent networks, Network servers, storage management, data-driven semantics, Mether, Operating systems, page updating, Permission, demand driven semantics, host load, protocols, Testing]
A file assignment problem model for extended local area network environments
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
A file assignment problem (FAP) designed specifically for file servers and work stations on an extended local area network (ELAN) is formulated and solved. Key properties of such an environment are modeled onto the FAP formulation. The FAP problem is NP-hard, and the approximate solution technique adopted uses Lagrangian (dual) relaxation. The dual FAP is solved by use of an accelerated subgradient method. The approach is efficient and also provides an estimate, called the approximate relative duality gap, of the quality of the solution. In all instances in which the method has been employed, the approximate relative duality gap is less than 1%. The algorithm is illustrated by several examples.<<ETX>>
[Optical fibers, Spine, accelerated subgradient method, Financial advantage program, File servers, Optical fiber LAN, local area networks, Lagrangian dual relaxation, Lagrangian functions, Bridges, extended local area network environments, resource allocation, work stations, file servers, Systems engineering and theory, file assignment problem model, Joining processes, Local area networks, approximate relative duality gap]
Performance analysis of a broadcast star local area network with collision avoidance
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The performance of a broadcast star network is analyzed, under the assumption of synchronous operation of a network. An exact analysis for a broadcast star network with an infinite station population, an exact analysis for a small network with a finite station population, and an approximate analysis for a large network with a finite station population are presented. In synchronous operation, the channel time is slotted, and stations transmit only at the beginning of a slot. The throughput and the distribution of transmission delays are obtained. It is shown through simulations that a broadcast star operating under synchronous mode yields better performance than when operating under asynchronous mode, where transmissions of packets are not confined to the beginning of slots and stations start transmission at any time.<<ETX>>
[broadcast star local area network, collision avoidance, finite station population, Switches, Telecommunication traffic, local area networks, synchronous operation, Broadcasting, Hardware, Performance analysis, throughput, Local area networks, slotted, Packet switching, Access protocols, performance evaluation, exact analysis, infinite station population, broadcast star network, Road accidents, delays, LAN, channel time, transmission delays, Collision avoidance, performance analysis]
Low cost comparisons of file copies
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The author present a file comparison scheme in which the signatures of individual pages are compared by means of a supersignature calculated from the individual page signature. A supersignature is obtained as a power series in a primitive root within the Galois field with 2/sup n/ elements. The coefficients are the signatures of the individual pages. With this scheme errors, such as missing, altered, or incorrectly placed pages, can be detected with the very probability, and an error diagnosis can be found if discrepancies are detected.<<ETX>>
[Costs, Galois field, Maintenance engineering, Data structures, error diagnosis, error detection, Galois fields, file comparison, Computer science, Content addressable storage, replicated database, Databases, supersignature, distributed databases, primitive root, fault tolerant computing, individual page signature]
Design issues in the implementation of remote rendezvous
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
A case study of the design and performance of remote rendezvous, an interprocessor communication mechanism, is presented. Remote rendezvous extends the Ada rendezvous model to handle distributed communication. In many ways its semantics and implementations are similar to remote procedure call and send-receive-reply. The design is compared with that of similar systems. The major contribution of this work is a description of real design choices that trade space for reliability, real-time response, and ease of application evolution.<<ETX>>
[Real time systems, Protocols, send-receive-reply, Ada rendezvous model, Programming, distributed processing, Telecommunications, semantics, Application software, Delay, remote rendezvous, Concurrent computing, File systems, performance, interprocessor communication, Robustness, remote procedure call, protocols, Sprites (computer), distributed communication]
Semantics of optimistic computation
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The derivation of a semantically equivalent optimistic computation from a pessimistic computation by application-independent transformations is discussed. Computations are modeled by program dependence graphs (PDGS). The semantics of a computation is defined by a mapping from an initial state to a final state and is realized by a graph rewriting system. Semantics-preserving transformations are applied to PDGS of the pessimistic computation to produce an optimistic version. The transformations result from guessing data values and control flow decisions in the computation. The transformations are used to derive an optimistic version of fault tolerance based on message logging and checkpointing. The transformations yield an optimistic version similar to optimistic fault-tolerance algorithms reported in the literature, although additional application-dependent transformations are necessary to produce a realistic optimistic implementation.<<ETX>>
[Checkpointing, checkpointing, Performance gain, distributed processing, application-independent transformations, pessimistic computation, Constraint optimization, Concurrent computing, message logging, application-dependent transformations, Fault tolerance, program dependence graphs, Parallel processing, Data flow computing, control flow decisions, rewriting systems, fault tolerance, Computational modeling, semantic-preserving transformations, Computer science, semantically equivalent optimistic computation, graph rewriting system, Performance loss, fault tolerant computing, optimistic fault-tolerance algorithms]
A comprehensive-based database language and its distributed execution
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The authors describe a way of noticeably reducing the description cost of database operations executed in distributed computing environments through the design of a declarative language to describe database operations, the development of program transformation techniques to improve efficiency at execution time, and the clarification of prerequisites to execute the programs in distributed computing environments. With the language, database operations are described as functions which manipulate streams. To describe stream manipulation at a higher level, the language SPL (Set Programming Language) is based on mathematical comprehensive notation for sets (ZF expressions). With this language, database operation implementors need not specify any communication primitives; a language processing system automatically translates the programs into procedural programs which include communication primitives.<<ETX>>
[Costs, high level languages, query languages, description cost, set theory, Database languages, Distributed computing, distributed computing, SPL, Concurrent computing, Information science, Set Programming Language, database operations, Microprocessors, Distributed databases, distributed databases, distributed execution, Parallel processing, Hardware, comprehensive-based database language, ZF expressions, declarative language, Transaction databases, program processors, set notation, stream manipulation, communication primitives, program transformation]
A study of communication resource allocation in a distributed system
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The authors introduce two techniques for allocating and managing buffers in support of distributed system operation and evaluate the resulting system behavior. For this evaluation a methodology that permits the development of an analytic model is introduced. With the help of this model, it becomes possible to fine-tune the buffer allocation policies in realistic system configurations.<<ETX>>
[Availability, Costs, buffer storage, Communication system control, distributed processing, performance evaluation, Distributed computing, Delay, communication resource allocation, resource allocation, buffer allocation policies, System performance, Optical buffering, Bandwidth, analytic model, Resource management, distributed system operation, High speed optical techniques, realistic system configurations]
Extending distributed shared memory to heterogeneous environments
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
The problems of building a distributed shared memory system on a network of heterogeneous machines are discussed. An existing algorithm (due to K. Li, 1986) that implements distributed shared memory is extended to a heterogeneous environment. An implementation that runs on Sun and DEC Firefly multiprocessor workstations connected by Ethernet is described. Related implementation and performance issues are discussed. On the basis of measurements of the applications ported to the system, it is concluded that heterogeneous distributed shared memory is not only feasible but can also be compared in performance to its homogeneous counterpart.<<ETX>>
[Ethernet networks, shared address space, SMI Sun-3 workstations, distributed processing, performance issues, page size request-response mode, Concurrent computing, storage management, heterogeneous distributed shared memory, interprocess communication, Workstations, network of heterogeneous machines, virtual storage, heterogeneous environments, Access protocols, Read-write memory, performance evaluation, Data structures, Partitioning algorithms, DEC Firefly multiprocessor workstations, Sun, Programming profession, Message passing, Ethernet, Mermaid]
Allocation and scheduling of complex periodic tasks
Proceedings.,10th International Conference on Distributed Computing Systems
None
1990
A static algorithm for allocating and scheduling components of complex periodic tasks across sites in distributed systems is discussed. Besides dealing with the periodicity constraints (which have been the sole concern of many previous algorithms), this algorithm handles precedence, communication, and fault-tolerance requirements of subtasks of the tasks. The algorithm determines the allocation of subtasks of periodic tasks to sites, the scheduled start times of subtasks allocated to a site, and the schedule for communication along the communication channel(s). Experimental evaluation of the algorithm shows that the heuristics and search techniques incorporated in the algorithm are extremely effective. Specifically, they show that, if a task set can be feasibly allocated and scheduled, the algorithm is highly likely to find it without any backtracking during the search.<<ETX>>
[Real time systems, TV, time critical systems, complex periodic task scheduling, fault-tolerance, distributed processing, periodicity constraints, Robotics and automation, communication requirements, Scheduling algorithm, precedence, Orbital robotics, scheduled start times, Fault tolerance, Information science, task allocation, Processor scheduling, resource allocation, real-time systems, scheduling, distributed systems, fault tolerant computing, Timing, Time factors]
Implementing consistency control mechanisms in the Clouds distributed operating system
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
Implementation of a kernel-level consistency control mechanism called invocation-based consistency control (IBCC) is presented. IBCC supports general-purpose persistent object-based distributed computing. It provides mechanisms that support a range of powerful, well-defined consistency semantics. IBCC mechanisms can also be used to implement custom recovery and synchronization. An operating-system-level implementation of IBCC as part of the Clouds distributed operating system that uses memory faulting to initiate locking and intermediate version creation is also given. Performance aspects are discussed, as well as the overhead incurred by supporting IBCC in terms of additional data structures needed in the operating system, and the additional amount of required code.<<ETX>>
[Clouds, persistent object-based distributed computing, intermediate version creation, Control systems, custom recovery, Distributed computing, Research and development, Information systems, Power engineering computing, Operating systems, network operating systems, data structures, Protection, object-oriented programming, memory faulting, locking, Data structures, invocation-based consistency control, Programming profession, performance, concurrency control, consistency semantics, synchronization, Clouds distributed operating system, kernel-level consistency control]
Detection and exploitation of file working sets
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
The work habits of many individuals yield file access patterns that are quite pronounced and can be regarded as defining working sets of files used for particular applications. A client management technique for detecting these patterns and then exploiting them to successfully prefetch files from servers is described. Trace-driven simulations show the technique substantially increases file cache hit rate in a single-user environment. Successful file prefetching carries three major advantages: applications run faster, there is less burst load placed on the network, and properly loaded client caches can better survive network outages. The technique requires little extra code, and-because it is simply an augmentation of the standard LRU client cache management algorithm-is easily incorporated into existing software.<<ETX>>
[file working sets, Prefetching, file access patterns, Software algorithms, File servers, trace driven simulations, Code standards, single-user environment, Computer science, Network servers, Tree graphs, File systems, servers, file cache hit rate, network outages, LRU client cache management algorithm, network operating systems, burst load, file organisation, Software standards, Workstations, pattern detection, file prefetching]
Exploiting problem dynamics through result sharing in dataflow environments
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
The notion of CPRS (concurrent processing which result sharing) graph decomposition is presented which is based on problem dynamics, and the validity of the CPRS scheme in a dataflow environment is explored. A dynamic dataflow architecture to execute programs under the new model of computation, called the CPRS architecture, is presented. An analytical modeling, based on a closed queueing network model, is used to study the effect of result sharing on the performance of the CPRS architecture.<<ETX>>
[queueing theory, Computational modeling, graph decomposition, distributed processing, performance evaluation, Throughput, problem dynamics, dynamic dataflow architecture, CPRS architecture, dataflow environments, Pipeline processing, Concurrent computing, closed queueing network model, Analytical models, performance, concurrent processing which result sharing, Computer architecture, Performance analysis, Problem-solving, Resource management, Queueing analysis]
Performance comparisons of buffer coherency policies
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
A comparison is made of four buffer coherency policies: check on access, check on access with periodic notification, selective notification, and broadcast invalidation. These policies differ in their basic approaches on how and when the invalidated granules are identified, and hence, achieve different tradeoffs between buffer hits and overhead of notifications. Analytic models are developed to evaluate the buffer hit probability, CPU overhead, and overall response time under these coherency policies. The analysis is validated through simulations. It is found that the difference in buffer hit probabilities is very sensitive to the skewness of the data access and is further affected by the number of nodes, update rates, and the buffer size.<<ETX>>
[transaction processing, Costs, File servers, Delay, buffer size, selective notification, Analytical models, Network servers, storage management, check on access, Distributed databases, distributed databases, Broadcasting, Local area networks, buffer hit probability, buffer storage, queueing theory, probability, performance evaluation, broadcast invalidation, periodic notification, database theory, invalidated granules, nodes, CPU overhead, update rates, data access, Tin, buffer coherency policies, overall response time]
A model of naming for fine-grained service specification in distributed systems
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
A syntax-oriented model for naming feature-based specification of services is provided. The model allows a service to evolve or reconfigure in functionality by adding and removing features and still coexist with its previous versions. The model's two aspects are examined. With this model for specifying services, name server functions may be factorized from service specific functions and implemented in a generic fashion in terms of parse and match operations and function invocations. This model can provide significant extension to such naming schemes as X.500 and the Universal Naming Protocol in supporting feature-based service interfaces.<<ETX>>
[Protocols, syntax-oriented model, Universal Naming Protocol, feature-based service interfaces, distributed processing, File servers, Distributed computing, fine-grained service specification, name server functions, X.500, service specific functions, Operating systems, naming model, network operating systems, Computer architecture, distributed systems, Workstations, feature-based specification]
Using multiple replica classes to improve performance in distributed systems
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
Replication has been primarily used as a means of increasing availability in distributed systems. It is known that replication can mitigate the costs of accessing remotely stored data in distributed systems. Replication control protocols in the literature have stopped short of addressing availability and performance concerns. These issues are addressed by contributing a classification of replicas with each class having different consistency requirements. Metareplicas keep track of up-to-date replicas for recently accessed objects and changes in data reference localities. Thus they allow many transaction operations to synchronously execute at only a single (and often local) replica. Pseudoreplicas are non-permanent replicas that facilitate localized execution of transaction operations. True replicas are permanent replicas that increase the availability of operations and data. A replication control protocol is presented.<<ETX>>
[Availability, pseudoreplicas, Protocols, Costs, performance concerns, Computational modeling, transaction operations, multiple replica classes, performance evaluation, Data engineering, classification, Distributed computing, consistency, Computer science, Concurrent computing, Content addressable storage, performance, localized execution, distributed databases, remotely stored data, data reference localities, distributed systems, protocols, Protection]
Aggressive transmissions over redundant paths
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
Fault-tolerant computer systems have redundant paths connecting their components. Given these paths, it is possible to use aggressive techniques to reduce the average value and variability of the response time for critical messages. One technique is to send a copy of a packet over an alternate path before it is known if the first copy failed or was delayed. A second technique is to split a single stream of packets over multiple paths. The authors analyze both approaches and show that these techniques can provide significant improvements over conventional, conservative mechanisms.<<ETX>>
[Costs, Protocols, queueing theory, redundant paths, Redundancy, aggressive transmissions, multiple paths, computer networks, packet switching, Switches, Delay, alternate path, fault tolerant computer systems, packet stream, response time, Bandwidth, critical messages, Optical fiber communication, Hardware, fault tolerant computing, Error correction codes, redundancy, Data communication]
Electing leaders based upon performance: the delay model
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
In a distributed system an algorithm used to select a distinguished node or leader in the system is known as a leader election algorithm. Leader election algorithms are examined that attempt to locate the leader at a good node (from a performance standpoint) in the system. In the preference-based approaches examined, each node in the system uses locally available information to vote for the various candidates (potential leaders) on the basis of the performance level it would realize under each of them. The preference-based leader election algorithms proposed and examined are simple, and are shown to perform almost as well as a traditional optimization-based approach to leader election.<<ETX>>
[Algorithm design and analysis, Delay systems, Nominations and elections, distributed processing, performance evaluation, distributed system, leaders election, locally available information, performance level, Partitioning algorithms, Distributed computing, delay model, Voting, preference-based approaches, delays, Computer networks, fault tolerant computing, Communication networks, Resource management, leader election algorithm]
Hybrid partitioning for particle-in-cell simulation on shared memory systems
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
The authors explore parallel processing issues, such as relationships between speedup and problem partitioning schemes, and problem size and time duration of each iteration for PIC (particle in cell) method on different multiprocessors. A partitioning scheme, hybrid partitioning, is introduced. Hybrid partitioning has evolved out of two general approaches to PIC problem decomposition on multiprocessors, partitioning particles and partitioning the space. The authors chose the shared memory multiprocessor environment for analyzing the parallel (distributed computing) algorithms. Two different BBN Butterfly machines (GP1000 and TC2000) were employed as testbeds.<<ETX>>
[System testing, Semiconductor devices, problem size, particle-in-cell simulation, Plasma simulation, Particle scattering, statistical models, shared memory systems, shared memory multiprocessor environment, multiprocessors, circuit layout CAD, problem decomposition, time duration, parallel algorithms, Computational modeling, Binary search trees, speedup, circuit analysis computing, Supercomputers, Partitioning algorithms, BBN Butterfly machines, Plasma devices, Physics, GP1000, TC2000, problem partitioning, performance analysis]
A service acquisition mechanism for the client/service model in Cygnus
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
Three of the most important issues in exploiting network servers concern how to specify services so that service-server bindings can be changed dynamically without disturbing clients, how to make clients resilient to network or server failure, and how to accommodate server protocol heterogeneity to provide a single system view to the clients. A service acquisition mechanism is presented for solving these issues. This mechanism is designed under a client/service model in which the abstraction of service is a first-class entity. The components of the mechanism are discussed.<<ETX>>
[Runtime environment, Protocols, network servers, server failure, computer networks, server protocol, Ice, client/service model, Distributed computing, network failure, Network servers, Intelligent networks, Operating systems, Cygnus, Computer architecture, Computer networks, service-server bindings, service acquisition, Kernel]
A randomized voting algorithm
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
A randomized algorithm for vote assignment is described. Given a collection of sites where copies of a file are replicated, and the individual site reliabilities, the algorithm assigns votes to sites so as to maximize overall availability. It is based on the concept of simulated annealing which has been successfully applied for various complex problems (for instance, the traveling salesman problem) for which no efficient optimal algorithm is known. The authors tested their algorithm for 5-, 6-, 7-, 8-, and 9-site examples and compared the vote assignments produced by it with those produced by an optimal algorithm based on solving an integer programming model. The results produced by the randomized algorithm were remarkably close to those produced by the optimal one.<<ETX>>
[Availability, randomized voting algorithm, simulated annealing, traveling salesman problem, integer programming, site reliabilities, vote assignment, Traveling salesman problems, Linear programming, integer programming model, Databases, Voting, Query processing, concurrency control, Simulated annealing, distributed databases, Packaging, optimal algorithm, Testing]
An object-oriented approach to formal specification of reactive systems
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
An object-oriented approach to formal specification of reactive systems is described. The main contributions of the work are: a language for specifying and implementing reactive modules; inheritance operations for such specifications and implementations; a formal semantics for such reactive programs and for the inheritance operations in terms of partial orders of events; treatment of specifications as types of modules and treatment of modules as first-class objects; and discussion of subtyping among specifications.<<ETX>>
[object-oriented programming, Object oriented modeling, reactive systems, first-class objects, language, Formal specifications, formal specification, reactive programs, subtyping, reactive modules, inheritance operations, Computer science, formal semantics, Councils, Prototypes, specification languages, Safety, Logic, Object oriented programming, Informatics, Contracts]
Integrated design, simulation, and verification of real-time systems
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
The goal of this research is to develop a design methodology that allows performance characteristics of a system to be monitored through all stages of its development, from initial design to system deployment. The use of partially implemented performance specifications (PIPS) as the design paradigm for real-time systems is proposed. A PIPS model is a partially implemented system where some system components exist as simulation models and others as operational subsystems. The model may be executed in an appropriate operating environment to determine stochastic performance of a proposed design. Proof axioms have also been defined for the modeling primitives to allow absolute timing constraints to be verified for the model.<<ETX>>
[Real time systems, simulation models, program verification, Computational modeling, Computer simulation, Design methodology, Computerized monitoring, Stochastic processes, distributed processing, operational subsystems, operating environment, formal specification, Computer science, partially implemented performance specifications, Stochastic systems, absolute timing constraints, real-time systems, stochastic performance, Timing, Safety, verification]
A hierarchical modeling of availability in distributed systems
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
A two-level hierarchical model is proposed to analyze the availability of distributed systems as perceived by their users. At the higher level (user level), the availability of the tasks (processes) is analyzed in terms of the availability of the system components. At the lower level (component level), detailed Markov models are developed to analyze the component availabilities. These models take into account the hardware/software failures, congestion and collisions in communication links, allocation of resources, and the redundancy level. Also presented is the availability analysis of some of the services provided by the unified workstation environment (UWE) currently being implemented at AT&T Bell Laboratories.<<ETX>>
[Process design, Laboratories, user level, distributed processing, communication links, Throughput, Reliability engineering, unified workstation environment, availability, Distributed computing, Design engineering, AT&amp;T Bell Laboratories, redundancy level, resource allocation, distributed systems, Hardware, Workstations, Availability, two-level hierarchical model, UWE, collisions, hierarchical modeling, component level, system components, Markov processes, Markov models, Resource management, congestion]
Crash recovery with little overhead
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
Recovering from processor failures in distributed systems is an important problem in the design and development of reliable systems. Two solutions to this problem which involve very little overhead are presented. Without appending any information to the messages of the application program, it is shown that it is possible to recover from failures using O( mod V mod mod E mod ) messages where mod V mod is the number of processors and mod E mod is the number of communication links in the system. The second algorithm can be used to recover from processor failures without forcing nonfaulty processors to roll back under certain conditions.<<ETX>>
[Checkpointing, nonfaulty processors, Protocols, crash recovery, processor failures, communication links, Computer crashes, History, system recovery, reliable systems, Delay, Computer science, IEL, application program, Fault tolerant systems, file organisation, operating systems (computers), distributed systems, Hardware, fault tolerant computing]
Supporting resource discovery among public Internet archives using a spectrum of information quality
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
Wide area networks offer access to an increasing number and variety of resources. Yet, it is difficult to locate resources of interest, because of the scale and decentralized nature of the environment. The authors are interested in supporting a global confederation of loosely cooperating systems and users that share far more resources than can be completely organized. Therefore, mechanisms are needed to support incremental organization of the resources, based on the efforts of many geographically decentralized individuals, and a range of different information sources of varying degrees of quality. The authors describe a prototype implementation of a set of mechanisms intended to explore this problem in the specific domain of public Internet archives, accessible via the anonymous file transfer protocol. This is an interesting test case, because it encompasses a very large scale, administratively decentralized collection of resources, with considerably practical value.<<ETX>>
[Wide area networks, wide area networks, resource discovery, computer networks, Telecommunication traffic, Access protocols, Computer science, incremental organization, anonymous file transfer protocol, Prototypes, Software quality, public Internet archives, Internet, Large-scale systems, IP networks, protocols, Testing]
Load balancing on generalized hypercube and mesh multiprocessors with LAL
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
A typical nearest neighbor balancing strategy, called LAL (local average load), in which the workload of a processor is averaged among its nearest neighbors at discrete time steps is investigated. The underlying systems considered are multiprocessor systems interconnected by generalized hypercube (GHC), mesh and loop structures. It is assumed that the amount of computation tasks arriving at or finished by a processor at each time step can be described by a random variable with some general distribution. With some general assumptions about these random variables, it is shown that the expected difference between the actual load of a processor and the average load of the system is zero and the variance of this difference is bounded by a constant independent of time.<<ETX>>
[random variable, Throughput, hypercube networks, mesh multiprocessors, Distributed computing, Delay, Nearest neighbor searches, Multiprocessing systems, Runtime, generalized hypercube, nearest neighbor balancing, computation tasks, LAL, scheduling, Load management, Hypercubes, Random variables, Resource management, local average load]
Approximations of the mean resequencing waiting time in M/GI/c systems
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
The problem of obtaining approximate formulas for mean resequencing waiting times of M/GI/c queueing systems is considered. Two assumptions, commonly used in the study of M/GI/c systems, to derive the formulas are adopted. The formulas are quite accurate and the accuracy increases as c becomes larger. The relative differences between simulation results and the ones calculated by the formulas are below 4%, even for larger server utilization ( rho ). For small rho , the differences are below 1%. Both small rho , the differences are below 1%. Both numerical and simulation results indicate that when the squared coefficient of variation of service time distribution is greater than 1, the mean resequencing waiting time is likely to be very large.<<ETX>>
[queueing theory, mean resequencing waiting time, Circuits, M/GI/c queueing systems, computer networks, performance evaluation, squared coefficient of variation, approximate formulas, Delay, Computer science, Upper bound, service time distribution, Numerical simulation, Computer networks, Random variables, Artificial intelligence]
Models of parallel learning systems
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
The technique of parallel processing is applied to concept learning. The learning strategies can be divided into two classes: top-down learning and bottom-up learning. Based on the partition of learning tasks on the multiple processors and the principle of divide-and-conquer, respectively, two corresponding parallel learning models are proposed. It is shown that these two models can be easily embedded into two practical and commonly used architectures: the MIMD shared memory architecture and the SIMD shared memory architecture. The ID3 and the version space learning strategies are parallelized to show how a parallel top-down learning or a parallel bottom-up learning strategy can work well.<<ETX>>
[version space learning, top-down learning, concept learning, Memory architecture, Knowledge based systems, divide-and-conquer, Parallel machines, parallel learning systems, parallel processing, parallel programming, learning systems, Learning systems, Concurrent computing, Computer science, Information science, Machine learning, Parallel processing, SIMD shared memory architecture, ID3, MIMD shared memory architecture, Artificial intelligence, bottom-up learning]
A synchronization mechanism for an object oriented distributed system
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
A mechanism for synchronizing shared objects in a distributed system based on persistent, typed objects is presented. Synchronization constraints are expressed as separate control clauses and are factorized for a class of objects. The interference of this mechanism with inheritance and transactions is examined and solutions are proposed. Examples of synchronized objects are provided, and a semaphore-based implementation of the mechanism is described.<<ETX>>
[Encapsulation, object-oriented programming, control clauses, distributed processing, shared objects, inheritance, object oriented distributed system, typed objects, transactions, Interference constraints, Distributed computing, Delay, synchronisation, Concurrent computing, Condition monitoring, synchronization mechanism, Physics computing, Message passing, semaphore, Computer networks, synchronized objects, Object oriented programming, persistent objects]
An implementation of F-channels, a preferable alternative to FIFO channels
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
An F-channel can permit as much concurrency as a nonFIFO channel and yet retain the properties of a FIFO channel that leads to simplicity of reasoning in design and proofs of the correctness of distributed algorithms. This is one of the major reasons that makes it preferable to a FIFO channel and make its implementation important. This and other reasons for its superiority over a FIFO channel are discussed, and it is proposed that systems should provide implementation of F-channels. Reasons are discussed for this proposal which are based on the principle that a system should provide an interface to the users that makes algorithm design easy and that leads to efficient user efforts and system operation. Implementation of an F-channel on top of a nonFIFO channel is presented.<<ETX>>
[Algorithm design and analysis, programming theory, FIFO channels, correctness proofs, reasoning, distributed processing, Proposals, user interface, concurrency, Concurrent computing, distributed algorithms, concurrency control, F-channels, Distributed algorithms, algorithm design]
A checkpointing scheme for heterogeneous distributed database systems
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
For efficient construction of the distributed database from media failure, a transaction-consistent checkpointing algorithm is proposed for heterogeneous distributed database systems. For full design autonomy and increased availability on the heterogeneous distributed database systems, the proposed algorithm never enforces termination of normal operations of transactions and changes of checkpointing algorithms in local database systems. The global checkpoints generated by the algorithm can be used to reconstruct the previous consistent states of a database efficiently.<<ETX>>
[Checkpointing, Algorithm design and analysis, transaction processing, Software maintenance, checkpointing scheme, Moon, full design autonomy, Transaction databases, media failure, global checkpoints, Computer science, Distributed databases, distributed databases, transaction-consistent checkpointing algorithm, heterogeneous distributed database systems, Database systems, fault tolerant computing, Communication networks, Telecommunication network reliability]
Rectifying corrupted files in distributed file systems
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
A probabilistic comparison algorithm is presented which requires O(f log n) bits to be transmitted to identify the corrupt pages in a file (where n is the number of pages and f is the maximum number of pages that could be corrupted), which improves on previous results on the growth of communicated bits as functions of both n and of f. If both copies compared are corrupt, only twice the number of bits is required as for the previous case. Further, if multiple copies are used for comparison, then the product of the number of copies times the number of bits sent from each of these copies to the comparison site grows as O(f log n). A lower bound which establishes the optimality of the algorithm to within a constant factor is provided.<<ETX>>
[Costs, communicated bits, distributed file systems, distributed processing, corrupted files rectification, Educational institutions, Complexity theory, probabilistic comparison algorithm, lower bound, Intelligent networks, File systems, Distributed databases, Computer errors, file organisation, Internet]
Parallel programming: achieving portability through abstraction
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
An introduction to the Unity syntax and logic is given. The idea of abstract programs is outlined. Examples are used to illustrate the usefulness of abstract programs. These are: sorting an array and summing an array.<<ETX>>
[Design methodology, Instruments, summing, abstraction, abstract programs, parallel programming, Computer science, portability, Computer languages, software portability, Parallel programming, sorting, logic, Unity syntax]
Implementing and programming causal distributed shared memory
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
A simple owner protocol for implementing a causal distributed shared memory (DSM) is presented, and it is argued that this implementation is more efficient than comparable coherent DSM implementations. Moreover, it is shown that writing programs for causal memory is no more difficult than writing programs for atomic shared memory.<<ETX>>
[storage allocation, Protocols, Read-write memory, distributed processing, Educational institutions, Registers, owner protocol, programming causal distributed shared memory, Distributed computing, Delay, Intersymbol interference, Writing, Broadcasting, coherent DSM implementations, protocols, programming]
Super Star: a new optimally fault tolerant network architecture
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
An interconnection topology that can be used to design communication architecture for distributed systems with an arbitrary number of computing nodes is proposed. The design is based on a novel generalization of the concept of star graphs. The proposed topology is shown to be incrementally extensible in steps of 1, optimally fault tolerant, and its diameter is sublogarithmic in the number of nodes.<<ETX>>
[computing nodes, multiprocessor interconnection networks, optimally fault tolerant network architecture, distributed processing, Mathematics, communication architecture, Distributed computing, Delay, Fault tolerance, Network topology, Fault tolerant systems, generalization, Computer architecture, Parallel processing, Hypercubes, distributed systems, fault tolerant computing, Communication networks]
A transfer policy for global scheduling algorithms to schedule tasks with deadlines
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
The authors present a load index to characterize the system state that is more conducive to apply preventive and corrective measures, and a transfer policy which takes preventive measures by doing anticipatory task transfers in addition to corrective measures. Key features of this transfer policy are: (1) it is general and can be used in conjunction with a broad range of existing location policies, (2) it does not require the capability to transfer partially executed tasks, and (3) it adapts better to the system state by looking-ahead. A simulation study shows that an algorithm making use of the transfer policy and the load index reduces the number of deadline misses significantly when compared to algorithms taking only corrective measures.<<ETX>>
[Real time systems, transfer policy, load index, distributed processing, deadlines, Scheduling algorithm, Delay, corrective measures, simulation study, Information science, global scheduling algorithms, Processor scheduling, scheduling, looking-ahead, tasks]
Dynamic reconfiguration of distributed programs
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
A general framework is developed for reconfiguring applications dynamically, where developers may alter the application without loss of service. After presenting the overall framework within which reconfiguration is possible, a description is given of the formal approach for programmers to capture the state of a process abstractly. An environment to support experimentation with dynamic reconfiguration is then described.<<ETX>>
[Software maintenance, Instruments, Software performance, distributed processing, Educational institutions, Reliability engineering, Application software, Programming profession, distributed programs, Geometry, Computer science, formal approach, framework, dynamic reconfiguration, Contracts, programming]
Nectar CAB: a high-speed network processor
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
Nectar is a high-speed fiber-optic network developed as a network backplane to support distributed and heterogeneous computing. A distinguishing component of the Nectar network is a highly programmable network processor called the communication accelerator board (CAB). The Nectar CAB has a flexible architecture, where almost all interactions between the network and the host are programmable. This structure allows arbitrary protocols to be implemented, evaluated, and utilized. A description is presented of the design, implementation, and usage of the Nectar CAB, and performance implications of its hardware features are discussed.<<ETX>>
[communication accelerator board, optical fibres, performance implications, Protocols, highly programmable network processor, computer networks, Switches, Optical fiber networks, performance evaluation, Application software, Engines, distributed computing, heterogeneous computing, High-speed networks, Computer architecture, high-speed network processor, Computer networks, Hardware, optical communication, protocols, network backplane, Contracts, Nectar CAB, fiber-optic network]
Dynamic reconfiguration in an object-based programming language with distributed shared data
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
An approach to dynamic reconfiguration in online distributed applications based on a data sharing model is presented. The data sharing model consists of facets, objects, and processes, with facets as the unit of sharing. The primary contribution of this work is the addition of important special cases of dynamic reconfiguration, without resorting to general dynamic allocation. A metaphor consistent with PROFIT's data sharing model for expressing dynamic reconfiguration facilities within the programming language is proposed.<<ETX>>
[object-oriented programming, processes, Instruments, object-oriented databases, distributed shared data, high level languages, objects, Financial management, Control systems, Distributed computing, facets, Computer languages, Databases, online distributed applications, distributed databases, dynamic reconfiguration, data sharing model, Dynamic programming, Stock markets, profit, object-based programming language, Portfolios, Monitoring]
Load balancing with network cooperation
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
A detailed analytical and simulation model that accurately captures the effect of communication delay for local area networks is presented. To demonstrate the framework, load sharing algorithms are presented and evaluated both with and without the effect of the communication network delay. The algorithms use the Ethernet communication protocol to their advantage and provide superior performance compared to several published algorithms. The strong performance results for the new algorithms demonstrate that a load sharing algorithm can cooperate rather than compete with a communication network.<<ETX>>
[Ethernet networks, Protocols, load sharing algorithms, load balancing, Delay effects, Telecommunication traffic, network cooperation, performance evaluation, local area networks, communication delay, Ethernet communication protocol, Analytical models, performance, delays, simulation model, Load management, Computer networks, Performance analysis, Communication networks, protocols, Local area networks, communication network delay]
Processor allocation vs. job scheduling on hypercube computers
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
The roles of processor allocation and job scheduling in achieving good performance on hypercube computers are compared. It is shown that the choice of job scheduling discipline has a dramatic effect on performance. A family of scheduling disciplines, called Scan, with particular performance advantages is proposed. Furthermore, it is shown that if Scan scheduling is used, the choice of processor allocation strategy has negligible effect on performance. As a result, complex allocation strategies can be replaced by a simple O(n) strategy.<<ETX>>
[Stochastic processes, performance evaluation, Dynamic scheduling, hypercube networks, Topology, processor allocation, hypercube computers, parallel processing, Information science, Processor scheduling, performance, Communication channels, scheduling, Hypercubes, Scan, Reflective binary codes, Resource management, job scheduling]
Heterogeneous process migration by recompilation
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
An approach to heterogeneous process migration that involves building a machine-independent migration program that specifies the current code and data state of the process to be migrated is described. When this program is compiled and executed on the target machine, it will first reconstruct the process's state and then continue the normal execution of the now-migrated process. The principal advantage of this approach is that it hides the details of code and data translation in the compilers for each machine.<<ETX>>
[Availability, Instruction sets, machine-independent migration program, Buildings, heterogeneous process migration, Application software, program compilers, Computer science, Databases, network operating systems, recompilation, Computer architecture, Writing, Hardware, data translation, System software]
Performance of transaction scheduling policies for parallel database systems
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
The impact of transaction scheduling in the shared-nothing type of architecture is studied. An abstract model of the shared-nothing type of architecture is used. Four transaction scheduling policies are considered. These policies can be broadly divided into two classes: policies that work independent of the current system state and policies that use the current system state information. The authors consider two policies belonging to each category-one policy works independent of the transaction characteristics and the other policy requires transaction size information. The results, obtained via simulation, indicate that system-state-dependent policies provide substantial performance advantages over the other group of policies.<<ETX>>
[transaction processing, transaction size information, Multiprocessor interconnection networks, Scalability, performance evaluation, Concurrency control, Transaction databases, shared nothing architecture, parallel programming, transaction scheduling policies, Computer science, parallel database systems, Processor scheduling, performance, System performance, Computer architecture, distributed databases, scheduling, Database systems, Joining processes]
Group communication in the Amoeba distributed operating system
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
Primitives for broadcast communication that have been integrated with the Amoeba distributed operating system are introduced. The semantics of the broadcast primitives are simple and easy to understand, but are still powerful. The proposed primitives, for example, guarantee global ordering of broadcast messages. The proposed primitives are also efficient: a reliable broadcast can be done in just slightly more than two messages, so the performance is comparable to a remote procedure call. In addition, the primitives are flexible; user applications can, for example, trade performance against fault-tolerance.<<ETX>>
[Protocols, fault-tolerance, Satellite broadcasting, Data structures, Mathematics, semantics, Radio broadcasting, Application software, broadcast communication, group communication, Amoeba distributed operating system, broadcast messages, Fault tolerance, Casting, performance, Operating systems, broadcast primitives, Intersymbol interference, network operating systems, reliable broadcast, remote procedure call, global ordering]
Regeneration with virtual copies for replicated databases
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
The consistency control problem for replicated data in a distributed computing system (DCS) is considered. An algorithm is proposed to dynamically regenerate copies of data objects in response to node failures and network partitioning in the system. The DCS is assumed to have strict consistency constraints for data object copies. The algorithm combines the advantages of voting-based algorithms and regeneration mechanisms to maintain mutual consistency of replicated data objects in the case of node failures and network partitioning. The algorithm extends the feasibility of regeneration to DCS on wide-area networks (WANs), and is able to satisfy user queries as long as there is one current partition in the system.<<ETX>>
[user queries, replicated databases, Heuristic algorithms, Merging, virtual storage, Computer crashes, consistency control, Partitioning algorithms, Distributed computing, voting-based algorithms, distributed computing system, virtual copies, Voting, wide-area networks, Distributed databases, distributed databases, network partitioning, Writing, Distributed control, node failures, Local area networks, regeneration mechanisms]
Efficient distributed algorithms for leader election in complete networks
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
An efficient protocol for leader election in an asynchronous complete network is presented. The time complexity of the protocol is better than the currently known protocols for this problem. A message optimal protocol is presented that requires O(N/log N) time, where N is the number of nodes in the network. Also given is family of protocols with message and time complexities O(Nk) and O(N/k) respectively, where log N<or=k<or=N. Many problems such as spanning tree construction, computing a global function, etc., are equivalent to leader election in terms of their message and time complexities, and therefore the author's results also improve the time complexity of these problems.<<ETX>>
[Protocols, leader election, Nominations and elections, trees (mathematics), spanning tree construction, distributed processing, time complexity, message optimal protocol, Time measurement, Intelligent networks, message complexity, protocol, distributed algorithms, Communication networks, protocols, Distributed algorithms, computational complexity, complete networks]
Overlapping window algorithm for computing GVT in Time Warp
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
Techniques are proposed for computing a global virtual time (GVT), which is the minimum local virtual time of processes in Time Warp. The algorithm computes a conservative estimate of GVT using an approach which is considerably simpler than previous algorithms for computing GVT. This algorithm does not require a global synchronization of processors. An inherent problem is GVT computation relates to handling messages in transit. Several alternatives are proposed for solving the transient message problem. The algorithm is suitable for distributed shared memory machines such as the BBN Butterfly and message passing machines with a variety of interconnection networks.<<ETX>>
[minimum local virtual time, global virtual time, Computational modeling, Multiprocessor interconnection networks, Moon, Time warp simulation, Time Warp, message passing machines, distributed processing, circuit analysis computing, BBN Butterfly, Discrete event simulation, interconnection networks, Computer science, Processor scheduling, Message passing, transient message problem, distributed shared memory machines, hybrid partitioning, computing GVT, window algorithms overlapping, Frequency synchronization, Clocks]
DisCo specification language: marriage of actions and objects
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
The potential of the action-oriented paradigm has been explored in the development of a specification language, DisCo, which can be characterized as both action-oriented and object-oriented. Its possibilities are introduced by contrasting them to the more familiar process-oriented approaches. Its execution model is state-based and leads to direct application of temporal logic in formal reasoning. Action-orientation allows a natural support for such forms of modularity that cut across process boundaries. At the same time, process-oriented abstractions are retained by object-orientation and the use of hierarchical state chart structures. The novel aspects of modularity are illustrated by a protocol example. The language is semi-executable.<<ETX>>
[hierarchical state chart structures, Protocols, object-oriented programming, Object oriented modeling, action-oriented, temporal logic, Specification languages, object-oriented, formal reasoning, Distributed computing, DisCo specification language, process-oriented, Algebra, Automata, specification languages, Animation, Hardware, Logic]
ASTRA-an asynchronous remote procedure call facility
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
A transport-independent asynchronous RPC (remote procedure call) mechanism (ASTRA) that combines the advantages of both RPC and message-passing IPC (interprocess communication) has been designed and implemented. ASTRA calls do not block the caller (client) and the replies can be received as and when they are needed, thus allowing the client execution to proceed locally in parallel with the server invocation. All the calls are received and executed by the server in the order called by the client. ASTRA is unique among other asynchronous RPC systems in allowing its users to explicitly specify whether low-latency or high-throughput is required for a call, and in providing highly optimized lightweight intramachine calls. ASTRA is built within the framework of the SHILPA distributed computing environment.<<ETX>>
[Transport protocols, Costs, client execution, Buildings, distributed processing, ASTRA, Yarn, Information technology, Environmental economics, transport-independent asynchronous RPC, intramachine calls, caller, Data conversion, Operating systems, Message passing, low-latency, SHILPA distributed computing environment, interprocess communication, high-throughput, message-passing IPC, Error correction, remote procedure call, server invocation]
Performance optimization of integrated network control schemes in packet-switched networks
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
Integrated schemes of routing, flow control, and congestion control in packet-switched networks are investigated for performance optimization. After the control parameters and objective function are introduced, an efficient method is derived to compute the gradient vector and Hessian matrix of such an objective function. The method, used with a general nonlinear programming strategy, solves this nonlinear optimization problem. Several examples which showed efficient and stable behavior were investigated. The results obtained by the proposed method are compared with exact values and are shown to be accurate.<<ETX>>
[nonlinear programming, Communication system control, Optimization methods, packet switching, integrated network control, nonlinear optimization, Intelligent networks, routing, control parameters, Traffic control, Communication system traffic control, Computer networks, gradient vector, objective function, computer networks, performance evaluation, packet-switched networks, congestion control, Routing, Educational institutions, flow control, Sliding mode control, matrix algebra, Hessian matrix, performance optimization, Resource management]
A comparison of two approaches to build reliable distributed file servers
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
Several existing distributed file systems provide reliability by server replication. An alternative approach is to use dual-ported disks accessible to a server and a backup. The two approaches are compared by examining an example of each. Deceit is a replicated file server that emphasizes flexibility. HA-NFS is an example of the second approach that emphasizes efficiency and simplicity. The two file servers run on the same hardware and implement SUN's NFS protocol. The comparison shows that replicated servers are more flexible and tolerant of a wider variety of faults. On the other hand, the dual-ported disks approach is more efficient and simpler to implement. When tolerating single failure, dual-ported disks also give somewhat better availability.<<ETX>>
[SUN's NFS protocol, HA-NFS, Protocols, Costs, computer networks, reliability, distributed processing, File servers, Deceit, dual-ported disks, Maintenance, reliable distributed file servers, Computer science, server replication, Network servers, File systems, file servers, Concrete, Hardware, fault tolerant computing, protocols]
Supporting the development of network programs
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
Programmers who want to do network computing face several challenges: the network and attached systems are shared resources with an unpredictable behavior and network communication primitives are often hard to use. The programming environment developed for the Nectar system addresses both problems. It provides simple and efficient communication primitives, and an efficient monitoring kernel that allows both programmers and programming tools to monitor the behavior of the program in the dynamic network environment. Experience shows that monitoring the progress of applications interactively is both desirable and practical.<<ETX>>
[Computerized monitoring, Nectar system, network computing, monitoring kernel, Application software, Distributed computing, programming tools, Programming profession, Programming environments, Concurrent computing, programming environment, network operating systems, Computer applications, Hypercubes, shared resources, network communication primitives, Computer networks, programmers, Dynamic programming, development of network programs, programming environments, dynamic network environment]
On the validity of the global time assumption
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
Concurrency in distributed systems is usually modeled by a non-deterministic choice, i.e., a concurrent execution that is a partial order on events is equated with the set of total orders obtained from its interleavings. The validity of this interleaving (or global time) assumption is examined. A novel construction for atomic registers is presented; this construction is correct if the proof is based on partial orders, but is incorrect if all possible interleavings are confused with partial orders in the reasoning.<<ETX>>
[concurrent execution, reasoning, distributed processing, interleavings, Computer science, Concurrent computing, atomic registers, global time assumption, concurrency control, Interleaved codes, distributed systems, validity, partial order]
DRISP: a versatile scheme for distributed fault-tolerant queues
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
A versatile scheme for implementing fault-tolerant queues in a distributed system is proposed. Based on the combination of two simple concepts, by distributing replications of a queue and conducting intelligent-sequential probe (DRISP), the backbone of this work is unsophisticated, yet powerful. A description is presented of the protocols for handling a replicated and distributed queue, which is different from a replicated file in the sense that FIFO order should be maintained in a queue. The replicated distributed queue supported by these algorithms provides dynamic fault tolerance, high availability, and uniform load balancing with small storage space requirements and low communication cost. It is also adaptable to environment changes. Consistency is guaranteed.<<ETX>>
[storage space requirements, Costs, Heuristic algorithms, replicated queue, Spine, distributed fault-tolerant queues, distributed processing, availability, Fault tolerance, Fault tolerant systems, FIFO order, protocols, Probes, versatile scheme, Availability, queueing theory, Access protocols, performance evaluation, uniform load balancing, Computer science, replications, dynamic fault tolerance, Load management, fault tolerant computing, DRISP, intelligent-sequential probe]
An efficient submesh allocation strategy for mesh computer systems
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
A processor allocation strategy is proposed which can apply to any mesh system and recognize submeshes with arbitrary sizes at any location in a mesh system. The proposed strategy allocates a submesh of exactly the size requested by an incoming task, completely avoiding internal fragmentation. Because of its efficient allocation, this strategy exhibits better performance than an earlier allocation strategy based on the buddy principle. An efficient implementation of this strategy is presented. Extensive simulation runs were carried out to collect experimental performance measures of interest under different allocation schemes for comparison.<<ETX>>
[Costs, Power system management, Very large scale integration, distributed processing, Supercomputers, Topology, Parallel architectures, performance measures, Delay, simulation runs, performance, Memory management, processor allocation strategy, Prototypes, buddy principle, Resource management, submesh allocation strategy, mesh computer systems]
BNB self-routing permutation network
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
A self-routing permutation network capable of routing all n factorial permutations of its n inputs to its n outputs is presented. The network implements the binary radix sorting on the structure of the generalized baseline network, a modified model of the original baseline network. The network has O(N log/sup 3/ N) hardware complexity and O(log/sup 3/ N) delay time for N-inputs. The network makes use of the localized bit information instead of the global information in routing procedure. This strategy leads to the reduction of both the hardware and the delay time compared with other comparable networks. The resulting hardware is simple, and has a good regularity.<<ETX>>
[Switching systems, Delay effects, multiprocessor interconnection networks, Switches, generalized baseline network, Routing, n factorial permutations, Sorting, Cellular networks, Concurrent computing, BNB self-routing permutation network, delay time, delays, National electric code, binary radix sorting, Hardware, Computer networks, hardware complexity, computational complexity]
The Stealth distributed scheduler
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
The justification, design, and performance of the Stealth distributed scheduler is discussed. The goal of Stealth is to exploit the unused computing capacity of a workstation-based distributed system (WDS) without undermining the predictability in quality of service that a WDS provides to workstation owners. It is shown that the liberal approach taken by the Stealth distributed scheduler is a promising method of exploiting the vast quantity of unused computing capacity typically present in a WDS, while preserving predictability of service for workstation owners.<<ETX>>
[Resumes, Stealth distributed scheduler, computer networks, Quality of service, distributed processing, performance evaluation, Supercomputers, Distributed computing, Information science, Processor scheduling, performance, workstation-based distributed system, Aggregates, Workstations, System software, Resource management]
Multicast group membership management in high speed wide area networks
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
An application for multicast service for high-speed WANs (wide area networks) which is capable of exploiting multicast hardware is described. Modularity and low cost area achieved by assigning to distinct components the separate problems of (1) naming groups, (2) finding group members in a network, (3) configuring multicast hardware, and (4) delivering multicast messages in sequence. The overall organization of the service is given, along with the methods used to solve the first two subproblems.<<ETX>>
[Wide area networks, Costs, computer networks, Switches, Telecommunication traffic, distributed processing, multicast group membership management, high speed wide area networks, Concurrent computing, Intelligent networks, Multicast algorithms, High-speed networks, modularity, file organisation, Hardware, Resource management]
Efficient distributed processor level fault diagnosis of multiple processor systems
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
A distributed processor level fault diagnosis algorithm is proposed where every processor will come up with its own conclusion about the status of other processors and also the links connecting it to its neighboring processors. Performance is improved by considering a dynamic testing strategy where each processor is assigned a small number of processors to test it, ideally two. Then, when some processors or links fail, a new testing assignment is made among the processors. Simulation results are presented to show that the proposed algorithm is effective in reducing the testing overhead.<<ETX>>
[Performance evaluation, System testing, multiprocessing systems, Heuristic algorithms, simulation, performance evaluation, distributed processor level fault diagnosis, multiple processor systems, Fault diagnosis, Tree graphs, System performance, dynamic testing, Communication channels, fault tolerant computing, Joining processes]
Membership algorithms for asynchronous distributed systems
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
Algorithms for solving the processor membership problem in asynchronous distributed systems that are subject to processor and communication faults are presented. These algorithms are based on the placement of a total order on broadcast messages. The types of systems for which each of these algorithms is appropriate are characterized in terms of the properties of the communication mechanisms and the availability of stable storage. In the absence of stable storage or a mechanism for distinguishing promptly delivery messages, it is shown that no membership algorithm exists.<<ETX>>
[Algorithm design and analysis, Availability, Real time systems, Protocols, delivery messages, communication faults, distributed processing, processor membership problem, Mechanical factors, Transaction databases, asynchronous distributed systems, broadcast messages, communication mechanisms, Casting, Fault tolerant systems, network operating systems, Broadcasting, fault tolerant computing, membership algorithms, stable storage, Marine vehicles, processor faults]
Static analysis of concurrent software for deriving synchronization constraints
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
A static analysis method is introduced for detecting synchronization errors. This method is to derive constraints on the feasible synchronization sequences of a concurrent program (or program module) P according to P's syntactic and semantic information. These constraints, called feasibility constraints for P, can be compared with constraints in the specification of P to detect specification-dependent errors and can be analyzed to detect specification-independent errors such as deadlock. Feasibility constraints for P can be used to improve the accuracy of existing methods for deriving approximations of the set of feasible SYN-sequences of P.<<ETX>>
[synchronization constraints, distributed processing, static analysis, syntactic information, deadlock, Application software, feasibility constraints, parallel programming, Software development management, synchronisation, Computer science, Computer languages, synchronization sequences, concurrent software, synchronization errors, Computer errors, System recovery, Packaging, Software systems, semantic information, specification-dependent errors, SYN-sequences]
Performance analysis of DQDB behaviour with priority levels
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
The bandwidth balancing method (BBM) introduced into the latest versions of the DQDB protocol is discussed. An extended analysis is presented of the behavior of the protocol with the BBM incorporated, assuming that more than one priority level is active in the network. The results are oriented towards evaluation of the effect the BBM has on access delay and bandwidth sharing in overload conditions.<<ETX>>
[access delay, Delay effects, computer networks, Access protocols, DQDB protocol, performance evaluation, Throughput, overload conditions, Telecommunications, Proposals, priority levels, Standards publication, Counting circuits, bandwidth sharing, DQDB behaviour, Bandwidth, bandwidth balancing method, Performance analysis, protocols, performance analysis, Propagation delay]
CLIDE: a distributed, symbolic programming system based on large-grained persistent objects
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
Clouds LISP distributed environments (CLIDE) is a distributed, persistent object-based symbolic programming system being implemented on the Clouds distributed operating system. LISP environment instances are stored as large-grained persistent objects, enabling users on many machines to share the contents of these environments through interenvironment evaluations. CLIDE provides a comprehensive research environment for distributed symbolic language, invocation and consistency semantics, and an implementation vehicle for the construction of the symbolic processing portions of complex megaprogrammed systems.<<ETX>>
[Cloud computing, symbolic programming system, distributed processing, symbolic processing portions, Distributed computing, Multiprocessing systems, Vehicles, CLIDE, Operating systems, network operating systems, distributed operating system, complex megaprogrammed systems, object-based symbolic programming, large-grained persistent objects, Object oriented programming, distributed programming, object-oriented programming, Computational modeling, Educational institutions, Application software, symbolic language, invocation, consistency semantics, LISP, Artificial intelligence, programming environments]
Probabilistic clock synchronization in large distributed systems
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
Synchronization algorithm is proposed which uses one of two probabilistic techniques to estimate remote clock values, and uses an interactive convergence algorithm on the resulting estimates to adjust the local clock. The algorithm does not require master/slave clocks and reduces the number of messages needed. As a result it is suitable for use in large distributed systems.<<ETX>>
[Real time systems, Laboratories, NASA, Software algorithms, probabilistic clock synchronisation, Master-slave, distributed processing, Synchronization, large distributed systems, Distributed computing, synchronisation, clocks, remote clock values, Hardware, interactive convergence algorithm, Contracts, Clocks]
A dynamic load balancing policy with a central job dispatcher (LBC)
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
A dynamic load balancing policy with a central job dispatcher, called the LBC policy, is proposed for distributed systems. The design of this policy is motivated by the operation of a single-queue-multi-server queuing system. The average job response time of this policy is the same as that of a single-queue-multi-server system which is the best achievable performance when the communication delay is reduced to zero. Hence, this policy is expected to provide near minimum average job response time for distributed systems with high-speed communication subnets. The performance of this policy is studied for systems with non-negligible job transfer delays in the following three aspects: average job response time, overhead due to information exchanges, and sensitivity to heterogeneous load.<<ETX>>
[queueing theory, dynamic load balancing policy, Delay effects, Delay estimation, distributed processing, performance evaluation, Time measurement, central job dispatcher (LBC), Distributed computing, communication delay, Least squares methods, heterogeneous load, LBC policy, performance, average job response time, scheduling, single-queue-multi-server queuing system, Load management, distributed systems, Computer networks, Communication networks, Time factors, high-speed communication subnets]
Flexible, fault-tolerant routing criteria for circuit-switched hypercubes
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
A set of routing criteria is proposed for circuit-switched hypercubes that exploit the flexibility provided by the hypercube. The routing criteria are provably deadlock-free and route messages along shortest paths. The number of shortest paths allowed by the routing criteria is more than one for most source-destination pairs. It is shown that the flexibility provided by the routing criteria can be used to limit the negative effects due to component-failures. The exact number of disrupted source-destination pairs are derived in the presence of a single faulty link or a single faulty node. It is shown that these numbers can be minimized using the relabeling techniques proposed. It is shown that the criteria, if used effectively, lead to a significant improvement in performance over the e-cube routing strategy for non-uniform traffic.<<ETX>>
[Flexible printed circuits, circuit-switched hypercubes, Integrated circuit interconnections, Routing, hypercube networks, disrupted source-destination pairs, negative effects, Circuit faults, deadlock-free, single faulty node, Switching circuits, component-failures, Fault tolerance, concurrency control, fault-tolerant routing criteria, shortest paths, single faulty link, System recovery, Traffic control, Parallel processing, e-cube routing, Hypercubes, fault tolerant computing]
Co-scheduling compute-intensive tasks on a network of workstations: model and algorithms
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
The problem of using the idle cycles of a number of high-performance workstations, interconnected by a high-speed network, for solving computationally intensive tasks is discussed. The classes of distributed applications examined require some form of synchronization among the sub-tasks, hence the need for coscheduling to guarantee that sub-tasks start at the same time and execute at the same pace on a group of workstations. A model of the system that allows the definition of an objective function to be maximized is presented. Then a quadratic time and linear space algorithm is derived for computing the optimal coscheduling.<<ETX>>
[algorithms, Laboratories, computer networks, linear space algorithm, performance evaluation, high-performance workstations, coscheduling computer-intensive tasks, synchronisation, Concurrent computing, Intelligent networks, Processor scheduling, High-speed networks, High performance computing, network of workstations, Parallel processing, scheduling, idle cycles, quadratic time algorithm, synchronization, Computer networks, model, Workstations, Resource management]
Reliable broadcast protocol for selectively partially ordering PDUs (SPO protocol)
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
Methods to provide reliable broadcast communication for multiple entities in distributed systems by using unreliable broadcast communication services are discussed. In real distributed applications, each entity sends every PDU (protocol data unit) to only the subset, rather than all the entities, and each entity receives only PDUs destined to it from some entity in the same order as they were sent. Such a broadcast service is named an SPO service (service for selectively partially ordering PDUs). How to design a protocol which provides the SPO service for entities by using unreliable broadcast networks in the presence of lost PDUs is discussed. The SPO service can be a useful facility in designing and implementing distributed systems like distributed database systems.<<ETX>>
[Ethernet networks, Communication system control, Access protocols, Reliability engineering, multiple entities, Centralized control, distributed database systems, SPO protocol, reliable broadcast communication, distributed databases, Broadcasting, Media Access Protocol, Systems engineering and theory, distributed systems, Database systems, protocol data unit, Telecommunication network reliability, protocols, selectively partially ordering PDUs]
The hyper-deBruijn multiprocessor networks
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
Hypercube and deBruijn networks each possess certain desirable properties. The architecture proposed is a combination of the hypercube and deBruijn architectures. Thus, it provides flexibility in terms of connections per node and the level of fault-tolerance. The graph structure allows a direct decomposition of the network into VLSI building blocks. These networks possess logarithmic diameter, optimal connectivity and simple routing algorithms amenable to networks with faults. These hyper-deBruijn networks admit many computationally important subnetworks such as rings, multidimensional meshes, complete binary trees and mesh of trees with perfect dilation, in addition to being pancyclic.<<ETX>>
[Multiprocessor interconnection networks, fault-tolerance, optimal connectivity, multidimensional meshes, multiprocessor interconnection networks, Very large scale integration, hypercube networks, VLSI building blocks, mesh of trees, Concurrent computing, routing algorithms, Fault tolerance, Tree graphs, Network topology, Computer architecture, Hypercubes, Computer networks, trees (mathematics), Routing, logarithmic diameter, deBruijn networks, rings, direct decomposition, complete binary trees, fault tolerant computing, graph structure]
Using tracing to direct our reasoning about distributed programs
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
Two principles are proposed for proving and tracing distributed programs: it is necessary to assert in proofs only what can be readily traced, and trace just what can be asserted in the proofs. A proof system and tracing strategy are described for CSP programs based on these principles, using vector time and control variables, not auxiliary variables, to represent control state, and stressing local rather than global reasoning.<<ETX>>
[Software testing, System testing, cognitive systems, control variables, tracing, proof system, Stress control, reasoning, Control systems, Educational institutions, inference mechanisms, artificial intelligence, parallel programming, distributed programs, control state, Computer science, Sequential analysis, vector time, Computer hacking, CSP programs, proofs, Writing, Error correction]
Distributed processing of filtering queries in HyperFile
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
A language has been developed for queries which serves as an extension of the browsing model of hypertext systems. The query language and data model fit naturally into a distributed environment. A simple and efficient method is discussed for processing distributed queries in this language. Results of experiments run on a distributed data server using this algorithm are presented.<<ETX>>
[HyperFile, data model, Filtering, object-oriented databases, filtering queries, Memory, hypermedia, browsing model, Very large scale integration, distributed processing, Information retrieval, query languages, distributed environment, Hypertext systems, Database languages, Graphics, Computer science, Distributed processing, hypertext systems, distributed queries, distributed databases, Data models, query language, distributed data server]
Real-time communication in multi-hop networks
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
A scheme is developed for providing predictable interprocess communication in real-time systems with (partially connected) point-to-point interconnection networks, which provides guarantees on the maximum delivery time for messages. This scheme is based on the concept of a real-time channel, a unidirectional connection between source and destination. A real-time channel has parameters which describe the performance requirements of the source-destination communication, e.g., from a sensor station to a control site. Methods to compute guarantees for the delivery time of messages belonging to real-time channels are examined. Problems associated with allocating buffers for these messages are addressed, and a scheme which preserves delivery time guarantees is developed.<<ETX>>
[Real time systems, performance requirements, Multiprocessor interconnection networks, Laboratories, Communication system control, computer networks, Sensor phenomena and characterization, performance evaluation, buffers allocation, Delay, real-time channel, Intelligent networks, source-destination communication, real-time systems, Spread spectrum communication, Computer networks, Time factors, predictable interprocess communication, point-to-point interconnection networks]
A DAG-based algorithm for distributed mutual exclusion
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
A token-based distributed mutual exclusion algorithm is presented. The algorithm assumes a fully connected, reliable physical network and a directed acyclic graph (DAG) structured logical network. The number of messages required to provide mutual exclusion is dependent upon the logical topology imposed on the nodes. Using the best topology, the algorithm attains comparable performance to a centralized mutual exclusion algorithm; i.e., three messages per critical section entry. The algorithm achieves minimal heavy-load synchronization delay and imposes very little storage overhead.<<ETX>>
[minimal heavy-load synchronization delay, Heuristic algorithms, DAG-based algorithm, distributed mutual exclusion, distributed processing, performance evaluation, token-based, Distributed computing, logical network, Delay, Network topology, performance, directed graphs, delays, Permission, token networks, reliable physical network, token-based algorithm]
Critical factors in NUMA memory management
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
The authors identify and incorporate the critical factors that influence nonuniform memory access (NUMA) memory management into their performance metrics. Using trace-driven simulations, it is shown that under certain conditions no replication is better than replication. It is also concluded that the effectiveness of replication depends on: (1) the ratio of access times to remote and local memory, (2) virtual address assignment to data, (3) data sharing characteristics, (4) overhead of enforcing consistency, and (5) amount of physical memory available relative to the data sharing characteristics.<<ETX>>
[Measurement, Computational modeling, critical factors, Random access memory, virtual address assignment, data sharing characteristics, nonuniform memory access, consistency, Delay, local memory, Degradation, Computer science, trace-driven simulations, storage management, remote memory, physical memory, Memory management, Platinum, NUMA memory management, performance metrics, Large-scale systems, Kernel]
An object-based approach to implementing distributed concurrency control
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
Distributed concurrency control has been implemented by representing in-progress transactions as simulated objects, called transaction objects, that use normal message passing facilities. The implementation of an optimistic mechanism has been completed using transaction objects and a two-phase locking mechanism has been designed. The tradeoffs made and lessons learned, dealing with transactions both on objects and as objects, are discussed.<<ETX>>
[in-progress transactions, message passing, Computational modeling, Computer simulation, object-oriented databases, simulated objects, Concurrency control, Yarn, two-phase locking, Design optimization, Counting circuits, Concurrent computing, Computer science, Computer languages, transaction objects, Message passing, concurrency control, distributed databases, distributed concurrency control, optimistic mechanism]
A comparative study of three token ring protocols for real-time communications
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
When developing distributed scheduling algorithms such as communication protocols, issues in achieving optimal policy and minimizing overhead must be addressed. This problem is examined in the context of a specific distributed system-the token ring communication network. Three token ring protocols are considered which are representative of many existing ones in the sense that they incorporate message time constraints at different levels and implement the earliest deadline first transmission (scheduling) policy at different degrees with different overheads. Through a worst-case analysis, the performance of these three token ring protocols is compared. It is concluded that to evaluate a distributed scheduling algorithm such as a communication protocol, it is necessary to not only consider the scheduling policy employed but also to take into account the overhead incurred due to the implementation of the scheduling policy.<<ETX>>
[Context, optimal policy, Protocols, Laboratories, message time constraints, Telecommunications, token ring protocols, worst-case analysis, Scheduling algorithm, real-time communications, earliest deadline first transmission, Computer science, real-time systems, distributed scheduling algorithms, scheduling, Token networks, Performance analysis, Time factors, Communication networks, protocols, token networks]
A token based distributed mutual exclusion algorithm based on quorum agreements
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
A token-based mutual exclusion algorithm is presented which uses data structures similar to coteries, called quorum agreements. The performance of the algorithm depends on the quorum agreements used. When a good quorum agreement is used, the overall performance of the algorithm compares favorably with the performance of other mutual exclusion algorithms.<<ETX>>
[Algorithm design and analysis, Automation, quorum agreements, distributed processing, performance evaluation, Data structures, Distributed computing, performance, Permission, Automatic control, System recovery, token based distributed mutual exclusion algorithm, data structures, Performance analysis, token networks, coteries]
Performance of local area network protocols for hard real-time applications
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
Simulation experiments show that the token ring protocol gave a lower average message delay at low transfer rates, but the token bus protocol gave a better overall performance for applications where only average delay is of interest. On the other hand, in hard real-time systems, the criterion of importance is not the average message delay, but the maximum message delay and the ability to meet deadlines. Slotted ring in this case is a much better protocol than the others because of its low maximum message delay and more predictable message delay. Because of this, and because the average performance of the slotted ring remains good as the size or the transfer rate of the network increases, the slotted ring protocol is preferred over the token ring and token bus protocols for hard real-time systems.<<ETX>>
[Real time systems, token ring protocol, simulation, maximum message delay, Access protocols, performance evaluation, Throughput, local area networks, token bus protocol, Application software, Delay, Computer science, hard real-time applications, delays, real-time systems, local area network protocols, slotted ring protocol, Hardware, Token networks, protocols, Distributed algorithms, Local area networks, average delay]
Distributed garbage collection of active objects
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
Distributed automatic garbage collection of objects possessing their own thread of control is discussed. The relevance of garbage collection and concurrent objects to distributed applications is briefly discussed, and the specific model of concurrent objects used is explained. The collector comprises a collection of independent local collectors, one per node, loosely coupled to a distributed global collector. The mutator (application), the local collectors, and the global collector run concurrently. The synchronization necessary to achieve correct and efficient concurrent operation between the collectors is presented. One interesting aspect of the distributed collector is the termination algorithm.<<ETX>>
[Object oriented modeling, concurrent objects, distributed processing, distributed garbage collection, Yarn, Environmental management, Distributed computing, Programming profession, synchronisation, storage management, Runtime, independent local collectors, concurrency control, Automatic control, synchronization, termination algorithm, Resource management, Object oriented programming, Kernel]
Debugging and performance monitoring for distributed systems: problems and prospects
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
This paper discusses the following: effective instrumentation as the key to effective monitoring; distributed monitoring systems as a basis for general purpose distributed multiprocessors; problems and prospects; performance monitoring, fact and fancy; and software aspects.<<ETX>>
[Software maintenance, multiprocessing systems, performance monitoring, Life testing, Debugging, Software performance, distributed processing, performance evaluation, general purpose distributed multiprocessors, Application software, software aspects, Organizing, Degradation, computer debugging, Software systems, debugging, distributed systems, Monitoring, Software engineering]
Modeling wormhole routing in a hypercube
[1991] Proceedings. 11th International Conference on Distributed Computing Systems
None
1991
An analytical model for the performance evaluation of asynchronous hypercubes is presented. This analysis is aimed at modeling a deadlock-free wormhole routing scheme prevalent on second-generation hypercube systems. Probability of blocking and average message delay are discussed. The communication traffic to find the probability of blocking is the starting point. The traffic analysis can capture any message destination distribution. The average message delay that consists of two parts is found. The analysis is extended to virtual cut-through routing and random wormhole routing techniques. The validity of the model is demonstrated.<<ETX>>
[Protocols, Laboratories, performance evaluation, Routing, hypercube networks, analytical model, message destination distribution, Delay, average message delay, Analytical models, wormhole routing modelling, virtual cut-through routing, delays, blocking, Traffic control, System recovery, Hypercubes, traffic analysis, Performance analysis, asynchronous hypercubes, communication traffic, telecommunication traffic, Power engineering and energy]
Distributed, scalable, and static parallel arc consistency algorithms on private memory machines
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
Several arc consistency algorithms for sequential and parallel processing computers are reviewed. Three distributed parallel arc consistency algorithms-DSPAC-1, DSPAC-2, and DSPAC-3-are introduced and compared with existing algorithms. Through actual machine experimentation the time required for the DSPAC algorithms was measured and compared with that for existing sequential algorithms. Results indicate that the parallel arc consistency algorithms are very effective and that scalability can be efficiently maintained.<<ETX>>
[parallel algorithms, private memory machines, parallel processing computers, Scalability, Image processing, Image retrieval, DSPAC-3, DSPAC-2, DSPAC-1, Information retrieval, Time measurement, Distributed computing, parallel machines, scalability, Concurrent computing, Image databases, sequential processing computers, Information processing, protocols, Artificial intelligence, static parallel arc consistency algorithms, sequential algorithms]
End-to-end scheduling to meet deadlines in distributed systems
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
Algorithms for scheduling a class of systems in which all the tasks execute on different processors in turn in the same order are described. This end-to-end scheduling problem is known as the flow-shop problem. Two cases in which the problem is tractable are presented, and a heuristic for the NP-hard general case is evaluated. The traditional flow-shop model is generalized in two directions. First, an algorithm for scheduling flow shops in which tasks can be serviced more than once by some processors is presented. Second, a heuristic algorithm for scheduling flow shops with periodic tasks is described. Scheduling systems with more than one flow shop are considered.<<ETX>>
[Job shop scheduling, Heuristic algorithms, NP-hard general case, distributed processing, heuristic, Distributed computing, end-to-end scheduling, Scheduling algorithm, Computer science, heuristic algorithm, Intelligent networks, Processor scheduling, heuristic programming, scheduling, flow-shop problem, distributed systems, Timing, Time factors, Communication networks, computational complexity]
MINTABS: early experiences with a new paradigm for programming SIMD computers
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
A programming paradigm for massively parallel SIMD computers is described. The paradigm requires that each program be compiled into a special instruction set. Initially, the MINTABS software architecture's instruction set is used. A process is assigned to a processing element (PE) by storing its instructions and data at that PE. The control unit broadcasts a fixed control algorithm and the PEs use it to interpret their own program. Initial experiments have demonstrated an increase in throughput by a factor of more than 700 on an 8192-processor machine. The throughput also increases linearly with the number of processors.<<ETX>>
[instruction sets, Costs, Computational modeling, fixed control algorithm, processing element, Throughput, Linear programming, programming SIMD computers, parallel processing, Programming profession, Counting circuits, Concurrent computing, MINTABS, 8192-processor machine, instruction set, Kinematics, Parallel processing, massively parallel SIMD computers, programming environments, Testing]
Sparse time versus dense time in distributed real-time systems
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
The restriction of the significant event occurrences, i.e., the sending and receiving of messages in a distributed real-time system, to the lattice points of a globally synchronized space/time lattice are discussed. One dimension of this space/time lattice represents the progression of time, the other dimension denotes the computational processes in the system. It is shown that this additional constraint simplifies solutions to agreement problems. After an analysis of the interdependence among temporal order, causal order, receive order, and the limits of time measurement in a distributed real-time system, criteria for the selection of the lattice points of this space/time lattice are presented.<<ETX>>
[Real time systems, Protocols, globally synchronized space/time lattice, distributed real-time systems, temporal order, Lattices, distributed processing, Distributed computing, computational processes, sparse time, causal order, dense time, agreement problems, Intelligent actuators, performance evaluation, Sensor systems and applications, receive order, Time measurement, Intelligent sensors, space/time lattice, real-time systems, time measurement, lattice points, Frequency synchronization, Clocks]
Distributed concurrency control with limited wait-depth
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
Distributed wait-depth-limited (DWDL) concurrency control, a locking-based method that limits the wait-depth of blocked transactions to one, which assures that deadlocks are resolved as part of regular transaction processing is described. The performance of DWDL is compared with that of distributed two-phase locking (2PL) and the wound-wait concurrency control method through a detailed simulated. Results show that DWDL behaves similarly to 2PL for low data contention levels, but at high lock contention levels, DWDL outperforms the other methods to a significant degree.<<ETX>>
[Availability, transaction processing, Costs, Scalability, limited wait-depth, wound-wait concurrency control, Concurrency control, performance, concurrency control, Bandwidth, distributed databases, Tin, System recovery, Database systems, Hardware, Robustness, distributed concurrency control, deadlocks, distributed two-phase locking, locking-based method, regular transaction processing]
Guaranteeing synchronous message deadlines with the timed token protocol
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
The problem of guaranteeing synchronous message deadlines in token ring networks in which the timed token medium access control protocol is used is discussed. Synchronous capacity, defined as the maximum time for which a node can transmit its synchronous messages every time it receives the token, is a key parameter in the control of synchronous message transmission. To ensure the transmission of synchronous messages before their deadlines, synchronous capacities must be properly allocated to individual nodes. Several synchronous capacity allocation schemes are analyzed in terms of their ability to satisfy deadline constraints of synchronous messages. It is shown that an inappropriate allocation of the synchronous capacities could cause message deadlines to be missed, even if the synchronous traffic is extremely low. The normalized proportional allocation scheme, which can guarantee the synchronous message deadlines for synchronous traffic of up to 33% of available utilization is proposed.<<ETX>>
[Optical fibers, synchronous capacities, FDDI, Spine, deadline constraints, Access protocols, synchronous traffic, Application software, timed token medium access control protocol, normalized proportional allocation scheme, Space stations, token ring networks, Computer science, synchronous message deadlines, Lakes, Media Access Protocol, timed token protocol, Token networks, protocols, token networks]
Design, implementation, and evaluation of Virtual Internet Protocol
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
The design and implementation of the Virtual Internet Protocol (VIP) are described. The VIP was implemented by modifying an operating system kernel based on 4.3BSD. The overhead of VIP is compared to that of IP. Measured results indicate that VIP can achieve host migration transparency in the Internet with negligible overhead.<<ETX>>
[Wide area networks, host migration transparency, Laboratories, operating system kernel, internetworking, 4.3BSD, Access protocols, Routing, Application software, Computer science, Databases, Virtual Internet Protocol, network operating systems, Computer networks, Internet, IP networks, protocols]
Huffman trees as a basis for a dynamic mutual exclusion algorithm for distributed systems
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
A distributed mutual exclusion algorithm based on Huffmann trees is described. A request message is passed from a leaf node all the way to the root, signaling that a node is allowed to enter the critical section. Fault tolerance is achieved by forming the Huffman tree dynamically. It is shown that the approach has broad applications in distributed systems where nodes are often organized as a logical tree for ease of coordination. High fault tolerance is achieved, since a faulty node can be detected by its neighbor and thus be deleted from the tree.<<ETX>>
[Tree data structures, Huffman trees, fault tolerance, Heuristic algorithms, distributed processing, Distributed computing, Research and development, Fault tolerance, request message, leaf node, dynamic mutual exclusion algorithm, Fault detection, System recovery, Permission, distributed systems, Database systems, tree data structures, logical tree, protocols, Huffman coding, distributed mutual exclusion algorithm]
Formal analysis of waiting times for distributed real-time processes
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
An approach to automated timing analysis of distributed real-time programs is presented. The method is based on the static analysis of the task system and generation of global operation (GO) paths for which the timing analysis is applied. It is shown that a closed form solution algorithm is NP-complete. In order to more efficiently determine maximum waiting times for tasks sharing resources, even for large programs with many tasks, a reduced flow graph problem is defined by neglecting the differences in the execution times of the local operations. A solution for this problem gives an upper bound for the original analysis problem. A conjecture that the reduced problem is NP-complete even for two tasks is disproved by giving a formally correct polynomial time solution algorithm. The maximum number of steps for computing the maximum waiting for the reduced problem is linear with respect to the numbers of task's server requests.<<ETX>>
[Algorithm design and analysis, Software testing, System testing, global operation, waiting times, distributed processing, static analysis, Closed-form solution, NP-complete, formal specification, distributed real-time processes, formally correct polynomial time solution, Computer science, Upper bound, real-time systems, formal analysis, Polynomials, Timing, Logic, Time factors, automated timing analysis, task system, computational complexity]
Distributed cooperative control for sharing applications based on multiparty and multimedia desktop conferencing system: MERMAID
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
A control scheme for sharing applications that is based on MERMAID, a distributed multiparty desktop conferencing system, is described. The control scheme design is based on a replicated architecture which runs a copy of the application at every site or workstation. It is shown that the approach is applicable to developing shared applications, with slight application-independent modifications, to sharing existing single-user applications. The shared-application structure and the functions required for shared applications are discussed. A method for sharing applications for cases in which external resources are accessed is discussed.<<ETX>>
[application-independent modifications, Video sharing, Meetings, Programming, shared-application structure, Application software, MERMAID, Videoconference, teleconferencing, multiparty conferencing, Graphics, replicated architecture, distributed cooperative control, Information processing, groupware, Distributed control, Collaborative work, multimedia desktop conferencing system, Workstations]
Priority-based total and semi-total ordering broadcast protocols
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
Broadcast protocols that provide priority-based receipt ordering of protocol data units (PDUs) for entities in a cluster are discussed. Three distributed broadcast protocols that provide the priority-based receipt ordering of PDUs by using a single channel system like Ethernet and radio systems are presented. The protocols are priority-based total ordering (PriTO), priority-based semitotal ordering (PriSO) and fast PriO (FPriO).<<ETX>>
[Ethernet networks, semi-total ordering broadcast, protocol data units, Collaborative software, Communication system control, distributed processing, priority-based broadcast protocol, Data engineering, Radio broadcasting, FPriO, Intersymbol interference, Ethernet, Distributed control, Collaborative work, Media Access Protocol, Telecommunication network reliability, protocols, priority-based receipt ordering, distributed broadcast protocols, radio systems, PriTO, PriSO]
Optimal coteries for rings and related networks
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
Although finding an optimal coterie for a general graph G is computationally intractable, it is shown that it can be easily found if G is a ring. Since the solution is already known when G is a complete graph, it is implied that an optimal coterie can be obtained if every biconnected component of G consists of a single edge, a ring, or a complete graph.<<ETX>>
[Availability, general graph, biconnected component, Electrical capacitance tomography, Distributed computing, optimal coteries, Physics computing, rings, Bidirectional control, Polynomials, protocols, token networks, complete graph]
Real-time lock-based concurrency control in distributed database systems
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
In a real-time database system, it is difficult to meet all timing constraints due to the consistency requirements of the underlying database. Real-time database transaction scheduling requires the development of efficient concurrency control protocols that try to maximize the number of transactions satisfying their real-time constraints. Several distributed, lock-based, real-time, concurrency control protocols are described. The relative performance of the protocols in a nonreplicated database environment is reported. The protocols take the real-time requirements of the transactions into account in ordering data accesses, while maintaining data consistency by enforcing serializability.<<ETX>>
[Real time systems, distributed database systems, Distributed databases, distributed databases, real-time lock-based concurrency control, scheduling, serializability, consistency requirements, Database systems, protocols, real-time requirements, Job shop scheduling, Access protocols, performance evaluation, Concurrency control, data integrity, Transaction databases, Scheduling algorithm, data consistency, performance, timing constraints, concurrency control, real-time systems, System recovery, Timing]
Beyond micro-kernel design: decoupling modularity and protection in Lipto
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
It is argued that a modular operating system architecture should provide support for modularity independent of protection domains. Given such support, modules and interfaces can be designed according to sound software engineering principles, without concern for cross-domain invocation costs. The partitioning of modules into domains and across machines becomes a matter of configuration, rather than design. Current micro-kernel-based architectures do not sufficiently address this issue since their communication mechanisms are designed for the nonlocal, i.e., cross-domain, case. An architecture that provides location-transparent binding and access of modules optimized for the local case, thereby decoupling the orthogonal concepts of modularity and protection, is proposed.<<ETX>>
[Costs, interfaces, decoupling modularity, Sun, micro-kernel design, Design optimization, Computer science, location-transparent binding, Design engineering, Acoustical engineering, Operating systems, network operating systems, modular operating system architecture, Modems, partitioning, software engineering, Protection, Contracts]
Asynchronous unison
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
Unbounded and bounded designs of asynchronous unison systems are discussed. It is shown that both systems are stabilizing in the sense that their steady state behaviors do not depend on their initial states. The systems can therefore tolerate memory and reconfiguration faults that may yield them in arbitrary states. It is also shown that unison systems are useful in designing multiphase systems.<<ETX>>
[multiphase systems, reconfiguration faults, distributed processing, Steady-state, Synchronization, bounded designs, Computer science, memory faults, asynchronous unison systems, Fault tolerant systems, steady state behaviors, unbounded designs, fault tolerant computing, Clocks]
Performance analysis of hierarchical task queue organization for parallel systems
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
A hierarchical task queue organization that avoids the task queue bottleneck associated with the centralized organization and provides performance better than centralized and distributed organizations is proposed. A detailed performance analysis shows that the hierarchical organization is less sensitive to parameters like the branching factor and transfer factor. Therefore, it is suitable for large parallel systems.<<ETX>>
[parallel systems, Processor scheduling, branching factor, Parallel processing, performance evaluation, Performance analysis, hierarchical task queue organization, parallel processing, Queueing analysis, performance analysis, transfer factor]
Data base recovery in shared disks and client-server architectures
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
Solutions to the problem of performing recovery correctly in shared-disks (SD) and client-server (CS) architectures are presented. In SD, all the disks containing the data bases are shared among multiple instances of the database management system (DBMS). In CS, the server manages the disk version of the data base. The clients, after obtaining database pages from the server, cache them in their buffer pools. Clients perform their updates on the cached pages and produce log records. In write-ahead logging (WAL) systems, a monotonically increasing value called the log sequence number (LSN) is associated with each log record. Every database page contains the LSN of the log record describing the most recent update to that page. This is required for proper recovery after a system failure. A technique for generating monotonically increasing LSNs in SD and CS architectures without using synchronized clocks is presented.<<ETX>>
[cached pages, database management system, Access protocols, performance evaluation, File servers, Control systems, log sequence number, Synchronization, database management systems, shared disks, client-server architectures, log records, write-ahead logging, buffer pools, database recovery, database pages, Tin, multiple instances, data bases, Clocks]
TQL: a tasking query language for concurrent program analysis
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
A tasking query language (TQL) for aiding very general analysis of Ada tasking in a Petri-net-based environment is discussed. An important principle of TQL's design is that of hiding the formalism upon which the analysis framework is built. Instead, TQL defines a language by which queries of Ada interactions themselves can be expressed. Examples of TQL's capabilities are presented, and a sample analysis session using the gas station program is described.<<ETX>>
[Algorithm design and analysis, Laboratories, Petri nets, gas station program, tasking query language, query languages, State-space methods, History, Database languages, parallel programming, TQL, Ada tasking, Concurrent computing, Petri-net-based environment, Processor scheduling, User interfaces, System recovery, Software systems, concurrent program analysis, sample analysis, Ada]
Process-channel/sub agent/-process model of asynchronous distributed communication
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
A simple model of asynchronous distributed communication, the process-to-channel/sub agent/-to-process (PCP) model, is presented. This model is used as a framework for presenting channel grammars that specify channel behavior. The PCP model, in conjunction with channel grammars, provides a versatile and succinct mechanism for specifying and comparing different types of channels, as well as for designing channel implementations. The usefulness of these tools is demonstrated by presenting an implementation of hierarchical F-channels, which are channels based on hierarchical communication speeds and F-channels.<<ETX>>
[Communicating sequential processes, hierarchical communication speeds, communicating sequential processes, grammars, asynchronous distributed communication, channel behavior, distributed processing, Languages, channel grammars, hierarchical F-channels, process-to-channel/sub agent/-to-process, Distributed computing]
Open CSCW systems: will ODP help?
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
The need for open computer support for cooperative work (CSCW) systems is examined. Their requirements and impact on distributed systems are discussed. The Mocca project for developing an environment which will support open CSCW systems is described. It is shown that the open distributed systems (ODP) experience and approach can be a valuable tool in realizing open CSCW systems. Similarly, the ODP standardization effort can benefit both from the experience of CSCW application developers and from the requirements which CSCW systems place on their distributed platform.<<ETX>>
[open computer support for cooperative work, open systems, open distributed systems, Cooperative systems, Microcomputers, distributed processing, standardization, Application software, Distributed computing, Distributed processing, Mocca project, Sociology, Open systems, groupware, Collaborative work, distributed systems, Computer networks, Organizational aspects]
A highly adaptive media access protocol for dual bus metropolitan area networks
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
A protocol for dual bus networks that does not show the disadvantages inherent in the IEEE 802.6 standard for metropolitan area networks is proposed. The protocol obtains a fair distribution of bandwidth after a time equal to one round-trip delay and allows a full utilization of the available bandwidth. The protocol is derived from a fair and waste-free bandwidth allocation scheme. The features of the protocol can be included in the existing IEEE 802.6 standard. It is shown that the proposed protocol has significant performance advantages over the IEEE 802.6 standard.<<ETX>>
[Optical fibers, Metropolitan area networks, Delay effects, Access protocols, performance evaluation, distribution of bandwidth, IEEE 802.6 standard, Communication standards, Computer science, highly adaptive media access protocol, dual bus metropolitan area networks, metropolitan area networks, Bandwidth, Media Access Protocol, Computer networks, Data communication, protocols, round-trip delay, waste-free bandwidth allocation]
An implementation of flush channels based on a verification methodology
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
Flush channels generalize more conventional asynchronous message passing protocols. A distributed system that uses flush channels allows a programmer the flexibility of specifying the delivery order of each message in relation to other messages transmitted on the channel. An implementation technique that follows directly from a verification methodology for flush channels is presented. A relatively formal argument in support of the technique is included.<<ETX>>
[Out of order, Protocols, message passing, Circuits, verification methodology, distributed processing, distributed system, Educational institutions, Distributed computing, formal argument, Delay, Programming profession, Computer science, formal verification, Message passing, flush channels, Bandwidth, protocols, asynchronous message passing protocols]
An efficient protocol for voting in distributed systems
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
A voting protocol that can reduce the communication costs in distributed systems significantly is proposed. The technique arranges nodes in small intersecting groups, such that a site, in absence of failures, needs to communicate only with members of its group to collect the quorum. A method for constructing such logical groups is presented. It is shown that the message overhead of any operation in a system of N nodes is O( square root N) when there are no or few failures in the system. The availability and the communication overheads of the proposed protocol are compared with those of existing protocols.<<ETX>>
[Availability, Costs, Communication system control, Access protocols, Educational institutions, voting, Partitioning algorithms, logical groups, Computer science, Weight control, protocol, Voting, Fault tolerant systems, communication costs, communication overheads, distributed systems, fault tolerant computing, message overhead, protocols, computational complexity]
Replication control for distributed real-time database systems
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
An algorithm that integrates real-time scheduling with replication control is presented. The algorithm adopts a token-based scheme for replication control and attempts to balance the urgency of real-time transactions with the conflict resolution policies. In addition, the algorithm employs epsilon-serializability (ESR), a correctness criterion that is less stringent than conventional one-copy serializability. The algorithm is flexible and very practical, since no prior knowledge of data requirements or execution time of each transaction is required.<<ETX>>
[Real time systems, transaction processing, Paramagnetic resonance, distributed real-time database systems, token-based scheme, real-time transactions, Control systems, epsilon-serializability, Concurrency control, Spatial databases, Transaction databases, data requirements, Concurrent computing, conflict resolution, replication control, real-time systems, execution time, distributed databases, Distributed control, scheduling, correctness criterion, Database systems, Timing, real-time scheduling]
Dynamic hierarchical caching in large-scale distributed file systems
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
A simple method for constructing dynamic heirarchies on a file-by-file basis is described. The results of a trace-driven simulation of a dynamic hierarchical file system are presented. A reduction in server traffic of a factor of more than two for shared files compared with a flat scheme is obtained, without a large increase in client access time. Low-overhead techniques for maintaining cache consistency by detecting missed cache invalidation are discussed.<<ETX>>
[buffer storage, Scalability, distributed processing, File servers, dynamic hierarchical caching, Distributed computing, Information technology, Organizing, Delay, trace-driven simulation, Degradation, Computer science, File systems, server traffic, large-scale distributed file systems, flat scheme, file organisation, Large-scale systems, cache consistency, discrete event simulation]
Using a finite projective plane with a duality for decentralized consensus protocols
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
An efficient communication structure based on a finite projective plane with a duality is presented. The communication structure halves the number messages required in two rounds as compared to a communication structure based on a finite projective plane. It is shown that a finite projective plane with a duality can be constructed from a difference set, and that the communication structure presented has two kinds of symmetry.<<ETX>>
[Algorithm design and analysis, number messages, Laboratories, communication structure, Communication system control, Multicast protocols, decentralized consensus protocols, Distributed computing, duality, Centralized control, Fault tolerance, difference set, finite projective plane, Broadcasting, protocols, Distributed algorithms, Clocks]
On deriving distributed programs from formal specifications of functional requirements and architectural constraints
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
A method for formal derivation of distributed programs is presented. The approach uses programwide assertions to formulate safety and progress properties of computations. Assertions are used to state functional requirements of the program as well as structural and behavioral constraints imposed by the choice of underlying architecture. The significance of this latter feature is that it offers a single, unified formal framework for integrating functional requirements and architectural constraints.<<ETX>>
[Computational modeling, Design methodology, formal derivation, behavioral constraints, structural constraints, Drives, distributed processing, Formal specifications, Application software, formal specification, formal specifications, distributed programs, Computer science, safety, Computer architecture, Constraint theory, Hardware, functional requirements, Logic, architectural constraints]
An adaptive scheduling scheme for dynamic service time allocation on a shared resource
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
A scheduling scheme that allows a number of customers to share a common resource in an efficient and fair way is presented. Each customer is allowed to use the resource for an amount of time that does not exceed a certain limit, the limit being a function of the waiting time elapsed between the time of its last request and the time of access to the resource. After expiration of the service time allocated to a customer, if more service is still needed, the customer has to re-enter the request queue and issue a new service request. The scheme combines the advantages of both processor-sharing and first-come, first-served disciplines in a dynamic way. The applicability and the advantages of the scheme in both open and closed system environments are discussed.<<ETX>>
[Production systems, processor-sharing, Job shop scheduling, dynamic service time allocation, adaptive scheduling scheme, Switches, performance evaluation, Dynamic scheduling, Delay, open system environment, Adaptive scheduling, Manufacturing processes, Processor scheduling, resource allocation, Feedback, closed system environments, scheduling, shared resource, Resource management]
Net level aggregation using nonlinear optimization for the solution of hierarchical generalized stochastic Petri nets
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
An approach to the hierarchical evaluation of large generalized stochastic Petri net performance models of computer and communication systems is presented. In this method, complex submodels are replaced by substitute nets of lower complexity which bear the same delay time distribution of tokens. An optimum substitute net is found, using nonlinear optimization and matching the delay time distributions of the original subnet and the substitute net. The model reduction approach is demonstrated by means of an example.<<ETX>>
[net level aggregation, nonlinear programming, Petri nets, Stochastic processes, Research and development, substitute nets, nonlinear optimization, lower complexity, Computer architecture, Reduced order systems, hierarchical generalized stochastic Petri nets, Communication networks, stochastic processes, Delay effects, performance evaluation, State-space methods, performance models, model reduction approach, hierarchical evaluation, Stochastic systems, Tin, delay time distribution, computer and communication systems, optimum substitute net, computational complexity]
RAID organization and performance
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
A disk array architecture that generalizes the RAID (redundant arrays of inexpensive disks) level V data organization while providing excellent storage utilization, response times, and fault tolerance is discussed. A key feature of the approach is that reliability groups can contain several check data disks beyond the single parity disk. RAID response times for fault-free and failure recovery operations are presented.<<ETX>>
[transaction processing, magnetic disc storage, Costs, fault tolerance, Laboratories, Redundancy, failure recovery, Control systems, Spatial databases, Transaction databases, database management systems, Delay, fault-free, level V data organization, Runtime, redundant arrays of inexpensive disks, performance, storage utilization, Operating systems, Fault tolerant systems, disk array architecture, fault tolerant computing, RAID organization, response times]
TrActorS: a transactional actor system for distributed query processing
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
A transaction management model for distributed information systems that utilizes actor-like objects (TrActor) which deviate from conventional actors in their objects types, messages types, and mail queue is described. The objects are used in conjunction with distributed blackboarding techniques for addressing the process of reasoning about the need for cooperation and negotiation between the disparate information sources in a distributed information network. A variant of the nested transaction model is proposed as an information processing framework for the management model. A language for this framework is also proposed, and the feasibility of this approach is demonstrated by means of a comprehensive example.<<ETX>>
[transaction processing, TrActorS, distributed blackboarding, management model, distributed query processing, reasoning, actor-like objects, Research and development, query processing, mail queue, distributed databases, Communications technology, distributed information systems, transactional actor system, Object oriented modeling, messages types, transaction management model, Transaction databases, Identity management systems, Computer science, objects types, Query processing, Message passing, Australia, Artificial intelligence]
An efficient data interface for heterogeneous distributed environments
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
A multi-language-based data interface system for heterogeneous distributed processing is introduced. A prototyped environment based on this system is discussed, and an evaluation of the prototyped system is presented. It is shown that by keeping the syntax of the specification language flexible and close to existing high-level languages, a user can learn the interface language quickly. Semantically, this data interface views structured data as consisting of two parts: the data values themselves and the representation of the structure among the data values. Through this separation, it is possible to have pipelined data type checking and data conversion operations.<<ETX>>
[Software prototyping, multi-language-based data interface system, distributed processing, Data structures, user interfaces, Application software, data conversion operations, Uninterruptible power systems, Computer science, Distributed processing, data interface, Data conversion, structured data, Operating systems, heterogeneous distributed environments, Prototypes, Focusing, pipelined data type checking, specification languages, syntax, prototyped environment, programming environments, specification language]
Reconfigurable cube architecture for parallel computation
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
The construction of a dynamic cube network architecture that extends the capabilities of hypercube architectures with only a small increase in hardware complexity is described. The network is self-routing in the sense that there is a simple distributed routing algorithm which guarantees optimal paths between any pair of the vertices. This, together with other improvements in embedding properties, makes the dynamic cube an attractive alternative to the ordinary hypercube for massively parallel architectures.<<ETX>>
[parallel computation, optimal paths, parallel architectures, Switches, Routing, hypercube networks, massively parallel architectures, Partitioning algorithms, hypercube architectures, Concurrent computing, self-routing, distributed routing algorithm, Network topology, reconfigurable architectures, Computer architecture, Broadcasting, Hypercubes, Computer networks, fault tolerant computing, dynamic cube network architecture, Performance analysis, hardware complexity, reconfigurable cube architecture, computational complexity]
Comet: a toolkit for multiuser audio/video applications
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
Comet, a UNIX/C++ toolkit for writing programs, such as teleconferencing systems and collaborative editors, that involve multiple users and use digital audio video, is described. Comet provides a simple programming interface: the application builds a graph of objects representing speakers and microphones, mixers, files, etc. Comet realizes the graph by creating and interconnecting processes and audio/video I/O servers. It handles implementation details such as process placement, data type conversion, and resource reservation.<<ETX>>
[Unix, process placement, data type conversion, Master-slave, File servers, C language, teleconferencing, Network servers, File systems, microphones, groupware, multiuser audio/video applications, collaborative editors, writing programs, Hardware, Workstations, software tools, teleconferencing systems, resource reservation, Application software, Microphones, Teleconferencing, Comet, toolkit, UNIX/C++ toolkit, mixers, Collaboration, files, programming interface]
Multicast models and routing algorithms for high speed multi-service networks
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
The abstraction of tree structured communication channels used in wide-area packet multicasting are generalized by introducing the notions of acyclic graph and two user specifiable attributes, data directionality and transport quality of service. The abstraction is useful for multiservice networks in which applications have diverse transport requirements such as variable data rates and unidirectional data flow. Canonical multicast primitives suitable for a variety of data delivery requirements that allow flexible creation and removal of edges in a graph are presented. Simple and extensible routing algorithms to realize the primitives are also presented.<<ETX>>
[transport quality, unidirectional data flow, Packet switching, tree structured communication channels, Optical switches, wide-area packet multicasting, Routing, high speed multi-service networks, Communication switching, routing algorithms, Multicast algorithms, Tree graphs, directed graphs, Communication channels, Computer architecture, data directionality, variable data rates, Computer networks, tree data structures, protocols, acyclic graph, user specifiable attributes, Digital TV, multicast models]
Constructing protocol converters from service specifications
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
A four-step algorithm for constructing a protocol converter using the top-down approach is proposed. The algorithm involves construction of a system graph from the required service specification and the service specifications of the target protocols; construction of a service converter and new protocols services from the constructed system graph modification of the given protocols entities, to support the new service specifications, and derivation of a final converter by integrating the service converter and the constructed protocol entities. The protocol converter is guaranteed not only to satisfy the conformity property, but also to be free from deadlock and livelock.<<ETX>>
[Protocols, Automation, Image converters, Humans, conformance testing, conformity property, formal specification, protocol convertors construction, Information science, four-step algorithm, USA Councils, top-down approach, System recovery, Safety, protocols, service specifications]
Distributed file allocation with consistency constraints
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
The resource allocation problem in distributed computing systems that have strict mutual consistency requirements is discussed. A model that incorporates the behavior of consistency control algorithms which ensure that mutual consistency of replicated data is preserved even when communication links of the computer network and/or computers on which the files reside fail is presented. The constrained resource allocation problem is formulated as a mixed nonlinear integer program. An efficient algorithm is proposed to solve this problem. The performance of the algorithm is evaluated in terms of its accuracy, efficiency and execution times, using a representative problem set.<<ETX>>
[Availability, mixed nonlinear integer program, Communication system control, Financial advantage program, data integrity, Distributed computing, distributed computing systems, mutual consistency requirements, computer network, resource allocation, Computer network reliability, distributed databases, execution times, Distributed control, Cost function, Computer networks, distributed file allocation, Resource management, Telecommunication network reliability, computers, consistency constraints, replicated data]
On conformance in the context of open systems
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
Conformance is one of the key notions in the context of open systems and therefore must be precisely defined. Existing formalizations define conformance independently of the environment into which the open system will be embedded. It is argued that it is, in general, not adequate to guarantee conformance in an intuitive sense. Conformance should relate the visible behavior defined by two specifications, which conceptually occurs at external interaction points. Based on this assumption, an additional condition which is sufficient to overcome the difficulty is identified. The condition is based on compatibility between interaction points, a notion which is introduced and formally defined. An approach to verify this condition and some general results are presented. The application of this approach to open systems interconnection (OSI) is outlined.<<ETX>>
[Computer science, System testing, Protocols, open systems, Open systems, open systems interconnection, conformance, Manufacturing, Standards development, conformance testing, protocols, visible behavior]
A real-time algorithm for fair interprocess synchronization
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
The implementation of nondeterministic pairwise synchronous communication among a set of asynchronous processes is modeled as a binary interaction problem. An algorithm for this problem, which satisfies a strong fairness property that guarantees freedom from process starvation, is described. The message and time complexities are independent of the total number of processes in the system. The ways in which the algorithm may be extended to cope with fail-stop process failures are discussed.<<ETX>>
[Algorithm design and analysis, Computational modeling, nondeterministic pairwise synchronous communication, time complexities, communication complexity, synchronisation, Computer science, Fault tolerance, Computer languages, message complexity, real-time algorithm, asynchronous processes, Fault tolerant systems, Abstracts, binary interaction problem, fail-stop process failures, fair interprocess synchronization, computational complexity, strong fairness property]
Multitasking in multistage interconnection network machines
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
Cubic and noncubic task allocation algorithms for multistage-interconnection-network (MIN)-based multiprocessors are presented. Conflicts in passage through the network and inability to partition the system effectively are the main bottlenecks in a MIN-based system. To solve both problems, a renaming scheme called bit reversal (BR) matching pattern is proposed. This matching pattern minimizes conflicts and partitions the system completely into subsystems. Simulation results that show the advantage of allocation algorithms using the proposed matching pattern in terms of system efficiency, delay, and task miss ratio are presented.<<ETX>>
[task miss ratio, Multiprocessor interconnection networks, Delay systems, multiprocessor interconnection networks, multitasking, Intelligent networks, renaming scheme, resource allocation, scheduling, Hypercubes, matching pattern, bit reversal, multistage interconnection network machines, Multitasking, Magnetic heads, Partitioning algorithms, Scheduling algorithm, noncubic task allocation algorithms, system completely, Processor scheduling, delay, multiprogramming, cubic task allocation algorithms, allocation algorithms, system efficiency, Pattern matching, computational complexity]
Token allocation in distributed systems
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
Allocation and reallocation techniques for resources in a distributed database (DDB) system are discussed. An abstract model of the DDB, which partitions data among a set of nodes in a network, is presented. Initial resource allocation and demand driven borrowing policies are investigated using the model. It is shown that a single token borrowing policy which attempts to correct the greatest waste of resources in the system, achieves a cost within several percent of the unachievable lower bound, and multiple token borrowing polices, which anticipate future need and keep the system balanced with respect to the remaining number of tokens, perform much better than those that only borrow the needed amount.<<ETX>>
[distributed database, Costs, demand driven borrowing policies, token allocation, Transaction databases, Application software, resources allocation, Delay, single token borrowing policy, Computer science, resource allocation, abstract model, Distributed databases, distributed databases, Cities and towns, Inventory control, distributed systems, Marketing and sales, Resource management]
Blending hierarchical and attribute-based file naming
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
A framework that treats hierarchical naming as a special case of attribute-based naming with constant rules is discussed. The framework allows the construction of a name-spaces-blending hierarchy with characteristics of flat attribute-based systems and provides a comprehensive mechanism for combining these name spaces in flexible ways to build up large distributed structures. The feasibility of this approach is demonstrated by implementing a prototype file and directory server. Client workstations require minimal modifications in order to access this service through the NFS (network file server) protocol.<<ETX>>
[prototype file and directory server, Access protocols, File servers, attribute-based file naming, distributed structures, Organizing, flat attribute-based systems, hierarchical file naming, network file server, protocol, File systems, Animal structures, Prototypes, file organisation, Libraries, Data models, Database systems, Workstations, constant rules, name-spaces-blending hierarchy]
Fault-tolerant multi-destination routing in hypercube multicomputers
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
Multicast algorithms for faulty hypercube multicomputers are discussed. Two types of algorithms are proposed. Type I algorithms have the following features: they are distributed, in the sense that the same algorithm is implemented in all involved nodes and based on local information only; they can always find shortest paths from the source to all destinations whenever such exist; and they are easy to implement in hardware. The algorithms deal with nodes faults only, and they cannot deliver messages to those destinations that can be reached through some longer paths. Type II algorithms deal with both link faults and node faults. They can deliver messages to all reachable destinations if the total number of faults is less than the dimension of the hypercube. However, these algorithms are not easy to implement in hardware.<<ETX>>
[Liver, hypercube multicomputers, Very large scale integration, Routing, hypercube networks, Computer science, Fault tolerance, Multicast algorithms, Unicast, distributed algorithms, multicast algorithms, shortest paths, Broadcasting, fault tolerant multidestination routing, Hypercubes, Hardware, fault tolerant computing]
Type-specific coherence protocols for distributed shared memory
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
The concept of a structured distributed shared memory in which memory units are objects is introduced. The coherence of object replicas is maintained by type-specific coherence protocols that are based on the semantics of operations on objects. The aim is to reduce message traffic and operation latency in many common situations. The protocols subsume traditional distributed shared memory protocols based on the read/write model.<<ETX>>
[Protocols, type-specific coherence protocols, distributed shared memory, Read-write memory, Data structures, Delay, Concurrent computing, Computer science, read/write model, Distributed databases, Coherence, distributed memory systems, Traffic control, operation latency, shared memory systems, Hardware, protocols]
A comparison of regression-based load sharing strategies for distributed database environments
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
A set of strategies that address the problem which unreliable state information poses to useful load sharing in a distributed database system is examined. In this environment, sites must solve the problem of how alternative sites should be selected to process incoming transactions, given that the information on which the decision is based exhibits varying degrees of obsolescence. A set of regression-based adaptive strategies in which a feedback mechanism is used to compensate for obsolete information is presented. Transaction response time under the different adaptive strategies is analyzed, and the reasons for the performance differences are discussed.<<ETX>>
[Costs, transaction response time, Routing, regression-based load sharing strategies, Delay, feedback mechanism, feedback, resource allocation, Bayesian methods, Feedback, Distributed databases, distributed databases, Frequency, Database systems, distributed database environments, Performance analysis, Queueing analysis, regression-based adaptive strategies]
On serializability of distributed nested transactions
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
A model of nested transactions in distributed database systems is presented. The modeling approach is based on conflict serializability extended to accommodate multilevel transactions. Based on these definitions, serialization graph testing for nested transactions is discussed. Three concurrency control algorithms and proofs of their correctness are presented. The algorithms are an adaptation of serialization graph testing, an adaptation of the timestamp ordering protocol, and a variation of an optimistic protocol presented by H.T. Kung and J.T. Robinson (1981).<<ETX>>
[transaction processing, optimistic protocol, Protocols, multilevel transactions, Concurrency control, History, Scheduling algorithm, distributed database systems, Concurrent computing, Computer science, serialization graph testing, concurrency control algorithms, concurrency control, distributed databases, System recovery, serializability, timestamp ordering protocol, Database systems, protocols, distributed nested transactions, Testing]
Delegation through access control programs
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
Access control programs (ACPs), which permit controlled delegation of access rights to untrusted computer hosts, are discussed. Existing delegation protocols for distributed systems provide a way for a client to transfer its access rights to an intermediary, but provide only limited facilities for restricting the rights granted to the intermediary. ACPs are small programs that encode arbitrary specifications of delegated access rights. They are created and digitally signed by a client and passed to a server through an intermediary. When processing a request from the intermediary, the server executes the access control program to decide whether or not to grant the intermediary's request. Examples of ACPs used in a variety of applications are presented. A sample implementation of ACPs in the Andrew File System is described.<<ETX>>
[Access control, access rights, access control programs, arbitrary specifications, Access protocols, distributed processing, File servers, Control systems, Distributed computing, Concurrent computing, controlled delegation, delegation protocols, File systems, authorisation, Permission, file organisation, distributed systems, Andrew File System, protocols]
A general method to define quorums
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
Composition, a general method for constructing quorum sets, coteries, and bicoteries, is discussed. It is shown that composition provides a natural method for constructing quorum structures in an arbitrary network or even in a collection of interconnected networks, and that the resulting structures, called composite structures, can be efficiently evaluated. In particular, an efficient method for determining if a given set contains a quorum of a composite structure is presented. If this method is used, it is not necessary to compute and store all of the quorums of the composite structure in advance.<<ETX>>
[Availability, Protocols, bicoteries, Communication system control, Nominations and elections, Reliability theory, Data structures, Game theory, Boolean functions, interconnected networks, Voting, composition, quorum sets, Set theory, composite structures, protocols, coteries]
An architecture of a threaded many-to-many remote procedure call
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
The client-agent-server architecture, which serves as a flexible mechanism for many-to-many remote procedure calls, is discussed. The architecture provides a transparent view of the server to a client. The client accesses the server group through the agent while retaining only an abstract view of the server. The call primitives for the client, agent groups, hook construction and primitives (hooks are basic building block which a server group provides to the agent group), and server groups used in the architecture are described. Implementation issues are reviewed.<<ETX>>
[client-agent-server architecture, Switches, File servers, Application software, Yarn, agent groups, hook construction, Computer science, Cellular networks, Network servers, Fault tolerance, primitives, Wireless networks, threaded many-to-many remote procedure call, Computer architecture, remote procedure calls, call primitives]
Performance prediction modeling of multicomputers
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
An efficient execution model for tree structured computations is presented. A general framework for analyzing the performance of this type of computation for any given topology is discussed. The framework is used to derive models for two widely used parallel programming strategies: processor farms and divide and conquer. The models were validated on a large multicomputer, and it was shown that their accuracy is such that they can be used to predict the performance of applications that use the above strategies. The use of these models to evaluate performance and to restructure the application to improve performance is discussed.<<ETX>>
[Costs, multiprocessing systems, processor farms, Predictive models, performance evaluation, execution model, Topology, Application software, parallel programming, tree structured computations, Computer science, Parallel programming, Microprocessors, multicomputers, Parallel processing, performance prediction modelling, Hardware, divide and conquer, Performance analysis]
Performance of disk arrays in transaction processing environments
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
Analytical models for evaluating the performance of arrayed disk subsystems in normal, degraded, and rebuild mode are developed. Models for estimating rebuild time under the assumption that user requests get priority and can preempt rebuild activity are also developed. It is found that uncached disk arrays perform much worse than uncached nonarrays. It is determined that caches are necessary in arrayed subsystems. Furthermore, in degraded and rebuild modes, large building blocks minimize the worst response time seen by any user, whereas small building blocks minimize the average response time.<<ETX>>
[transaction processing, disk arrays performance, uncached disk arrays, performance evaluation, uncached nonarrays, database management systems, analytical models, Delay, Degradation, Analytical models, Disk drives, Databases, Failure analysis, Automatic control, Telephony, Robustness, Protection, transaction processing environments]
Exploiting iteration-level parallelism in dataflow programs
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
An approach to extracting iteration-level parallelism from dataflow programs is presented. The method exploits the single-assignment principle, which guarantees that any data value has exactly one producer. To minimize interprocessor communication, the code is modified so that, at run time, each producer executes on the processor that holds the corresponding data. Overhead resulting from possibly remote read accesses is alleviated by a software technique similar to caching. The performance of this process-oriented dataflow system (PODS) is demonstrated using the hydrodynamics simulation benchmark called SIMPLE, in which a 19-fold speedup on a 32-processor architecture has been achieved.<<ETX>>
[software technique, Logic programming, single-assignment principle, Hydrodynamics, Data mining, Programming profession, Delay, parallel programming, hydrodynamics simulation benchmark, Computer science, Computer languages, Parallel programming, Computer architecture, interprocessor communication, Parallel processing, SIMPLE, software engineering, dataflow programs, iteration level parallelism exploitation, process-oriented dataflow system]
A unified approach to fault-tolerant routing
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
A theoretical study of the connectivity and fault tolerance of Cartesian product networks is presented. The theoretical results are used to synthesize provably correct adaptive fault-tolerant algorithms from ones written for the component networks. The theoretical foundations that relate the connectivity of a Cartesian product network, the connectivity of the component networks, and the number of faulty components are established. It is shown that the connectivity of a product network is at least the sum of the connectivities of its factor networks. Based on the constructive connectivity proof, an adaptive, generic, distributed algorithm that can perform successful point-to-point routing in product networks, in the presence of faults, is devised. A proof of correctness of the algorithm is provided.<<ETX>>
[Algorithm design and analysis, fault-tolerant routing, Cartesian product networks, Multiprocessor interconnection networks, multiprocessor interconnection networks, Routing, provably correct adaptive fault-tolerant algorithms, proof of correctness, Parallel algorithms, point-to-point routing, Concurrent computing, Fault tolerance, distributed algorithm, faulty components, constructive connectivity proof, Network topology, connectivity, Hypercubes, Network synthesis, Computer networks, fault tolerant computing]
An algorithmic method for protocol conversion
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
An algorithmic method for constructing protocol converters to achieve interoperability between networks that implement incompatible protocols is presented. The converter constructed using this approach does not cause unspecified receptions or deadlocks if the original protocols are safe. The method is guaranteed to generate a valid converter if one exists for the given protocols, with the conversion requirements defined by an ordered mapping set. Since the approach consists of six algorithms, it can be automated, thereby substantially reducing the complexity of the converter construction process. A protocol conversion example involving incompatible protocols is used to illustrate the method.<<ETX>>
[complexity, Protocols, open systems, ordered mapping set, interoperability, protocol converters, protocol conversion, Automata, Computer architecture, System recovery, Computer networks, protocols, algorithmic method, Artificial intelligence, computational complexity]
An epistemic logic based synthesis of communication services and protocols
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
The advantages of an initial specification based on the knowledge properties of the service users and service providers for specifying the communication mechanisms of a layer is discussed. This particular way of proceeding with specification causes the designer to express pertinent high level properties that enable the designer to rationally infer a complete specification of the mechanistic attributes of an operational specification.<<ETX>>
[Context-aware services, Context, Protocols, Nonhomogeneous media, specification, Mechanical factors, formal specification, mechanistic attributes, epistemic logic based synthesis, knowledge representation, knowledge properties, service providers, operational specification, Error correction, Logic, protocols, communication services, service users]
An analysis of the join the shortest queue (JSQ) policy
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
An analytical method is developed to analyze the performance of the join the shortest queue (JSQ) policy for systems with N identical queues, N>or=2. No simulation result is used to refine the analytical model. A birth-death Markov process is used to model the evolution of the total number of jobs in the system. An iterative procedure is developed to estimate the average service rates for different states. The average job response time is then obtained. Extensive simulations are performed to study the accuracy of the analysis. Results show that this method provides estimates within 3.5% of the average job response times for N up to 64.<<ETX>>
[average service rates, performance evaluation, Routing, Time measurement, Delay, simulations, Computer science, Analytical models, join the shortest queue, average job response time, Traffic control, Markov processes, birth-death Markov process, Load management, iterative procedure, Performance analysis, State estimation, Queueing analysis, performance analysis]
A distributed Lisp programming system: implementation and usage
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
The implementation details of a Lisp environment built on top of a distributed operating system are presented. The system provides transparent distribution, protected large-grain persistent heaps, concurrency within each environment and seamless sharing of Lisp data structures between separate environments.<<ETX>>
[Runtime environment, Terminology, transparent distribution, Data structures, Educational institutions, Distributed computing, Programming profession, concurrency, Concurrent computing, distributed Lisp programming system, network operating systems, distributed operating system, protected large-grain persistent heaps, LISP, Hardware, data structures, seamless sharing, Workstations, Lisp environment, Protection, programming environments]
Test result analysis and diagnostics for finite state machines
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
An algorithm that localizes the faulty transition in a deterministic finite state machine (FSM) once the fault has been detected is presented. The diagnostic algorithm generates, if necessary, additional diagnostic test cases which depend on the observed symptom and which permit the location of the detected fault. The algorithm guarantees the diagnosis of any single fault in an FSM. An application example, explaining the functioning of the algorithm, is provided.<<ETX>>
[System testing, Protocols, Data analysis, Optimization methods, faulty transition, Mechanical systems, conformance testing, finite state machines, functioning, protocol testing, Fault detection, Automata, test result diagnostics, Hardware, protocols, test result analysis, Artificial intelligence, Medical diagnostic imaging]
Validation and performance evaluation of the partition and replicate algorithm
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
The partition-and-replicate-strategy (PRS) algorithm for distributed query processing is evaluated and its performance is validated. Although in principle PRS is better than single-site processing, early experimental results indicate the contrary. Based on experimental results, the factor which causes performance deterioration is identified and a remedy is provided. As a result, it is shown that the PRS strategy outperforms single-site processing in a realistic environment and that various parameters, such as the number of processing sites, partitioning speed relative to join speed, and sizes of the join relations, affect the performance of the PRS strategy significantly. Among these parameters, the algorithm is most sensitive to the partition speed.<<ETX>>
[join relations, distributed query processing, performance evaluation, partitioning speed, Partitioning algorithms, partition algorithm, Computer science, query processing, join speed, replicate algorithm, Query processing, Distributed databases, distributed databases, single-site processing]
A simulation analysis of faults and conflicts in a multicast-connected multi-path cube-based network
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
The problem of reconfiguration in a multicast-connected fault-tolerant multiple-path multistage interconnection networks (MIN) is discussed. The results of a simulation study conducted to evaluate the performance of the complementary two-stage cube (C2SC) network, a four-path cube-based network, under faults in the multicast mode are presented. The performance index used is the probability of failure to provide multicast connections due to faults and conflicts using two paths, three paths, and four paths out of all four possible paths. The results indicate that the probability of failure to provide multicast connection using two paths is comparable to that for using three and four paths. This implies that in order to reconfigure the C2SC MIN under faults, it is sufficient to consider only two paths.<<ETX>>
[Costs, Multiprocessor interconnection networks, multiprocessor interconnection networks, Switches, performance evaluation, digital simulation, reconfiguration, Communication switching, complementary 2-stage cube network, simulation study, Analytical models, Intelligent networks, Fault tolerance, Multicast algorithms, reconfigurable architectures, four-path cube-based network, Hardware, fault tolerant computing, performance index, Performance analysis, simulation analysis, fault tolerant networks, multicast-connected multi-path cube-based network]
Scheduling directed task graphs on multiprocessors using simulated annealing
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
Two algorithms based on simulated annealing for minimizing the schedule length of a directed task graph on a multiprocessor system are presented. The first algorithm uses the schedule length itself as the cost function to be minimized. The second algorithm uses an indirect cost function that minimizes the total communication overhead and the total loss of parallelism simultaneously. Implementation results indicate that the first algorithm is superior to the second one in terms of the schedule length it gives, but the second algorithm is superior to the first in terms of algorithm run time.<<ETX>>
[simulated annealing, Computational modeling, Instruments, Heuristic algorithms, Optimization methods, multiprocessor interconnection networks, Optimal scheduling, communication overhead, Scheduling algorithm, indirect cost function, Processor scheduling, directed graphs, Clustering algorithms, Simulated annealing, schedule length, scheduling, directed task graphs scheduling, Polynomials, multiprocessors]
A transaction model for multidatabase systems
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
A transaction model for multidatabase system (MDBS) applications in which global subtransactions may be either compensatable or retriable is presented. In this model compensation and retrying are used for recovery purposes. However, since such executions may no longer consist of atomic transactions, a correctness criterion that ensures that transactions see consistent database states is necessary. A commit protocol and a concurrency control scheme that ensures that all generated schedules are correct are also presented. The commit protocol eliminates the problem of blocking, which is characteristics of the standard 2PC protocol. The concurrency control protocol can be used in any MDBS environment irrespective of the concurrency control protocol followed by the local DBMSs in order to ensure serializability.<<ETX>>
[transaction processing, Protocols, Laboratories, Control systems, Spatial databases, Concurrency control, Transaction databases, standard 2PC protocol, database management systems, Information technology, concurrency control, transaction model, National electric code, multidatabase systems, correctness criterion, serializability, Database systems, protocols, commit protocol]
Leveraged computing: a task distribution protocol
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
A protocol for controlling a client-server interaction that can exploit underutilized local-area-network (LAN)-based computing resources is discussed. A queuing simulation is used to assess the impact of three sizes of task chunk and two variations of task mix within those chunks in two different LAN environments. The practicality of the protocol in an executive information system or decision support system (EIS/DSS) environment is verified using results of a prototype implementation.<<ETX>>
[Decision support systems, Computer interfaces, Protocols, client-server interaction, Microcomputers, queuing simulation, local area networks, task distribution protocol, Application software, executive information system, Distributed computing, local-area-network, decision support systems, LAN-based computing resources, decision support system, Computer networks, Workstations, protocols, Local area networks, Contracts]
Efficient decentralized consensus protocols in a distributed computing system
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
Two classes of efficient decentralized consensus protocols for a distributed computing system consisting of an arbitrary number of nodes, one without an initiator and the other with an initiator, are described. It is shown that the protocol without an initiator can be systematically executed and completed in the minimal number of steps. The protocol with an initiator is divided into three phases: broadcasting phase, shuffling phase, and confirming phase. It is proved that under the protocol with initiator, a distributed system of p nodes reaches consensus with an initiator in the minimal number of steps required. The total number of messages required by the protocol with initiator is derived.<<ETX>>
[Availability, Protocols, Costs, distributed processing, shuffling phase, decentralized consensus protocols, confirming phase, Distributed computing, distributed computing system, broadcasting phase, Distributed processing, Message passing, High performance computing, Microprocessors, Broadcasting, arbitrary number of nodes, protocols, Clocks]
A new communication tool: time dependent multimedia document
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
Time dependent multimedia document (TDMD), a tool that supports human communication under distributed computing environments, is described. TDMD supports various types of communication media, including audio, video, text, graphics, and image. Multimedia presentation is formally defined, and it is shown that, TDMD can be represented uniquely as a tree structure. In the structure, hyperlink information can be included by introducing graphical interface buttons, which are treated the same way as other media. The content information included in a button is its visual appearance as a metaphor and the name of a destination node from which successive presentation begins when the button is selected. By using such link information, several browsing paths can be included in TDMD. Buttons also give viewers control over the speed of a presentation.<<ETX>>
[graphical user interfaces, tree structure, Humans, Communication system control, distributed processing, multimedia systems, destination node, video, graphics, Distributed computing, formal specification, Tree graphs, SGML, tree data structures, audio, text, protocols, Tree data structures, image, Authoring systems, communication tool, Multimedia computing, human communication, browsing paths, time dependent multimedia document, Graphics, hyperlink information, telecommunications computing, distributed computing environments, Layout, graphical interface buttons]
A user-oriented synthetic workload generator
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
A user-oriented synthetic workload generator that simulates user file access behavior on the basis of real workload characterizations is described. The workload generator is designed for experiments and simulations related to file system analysis. The model for this workload generator is user-oriented and job-specific, represents file I/O operations at the system call level, allows general distributions for usage measures, and assumes independence in the file I/O operation stream. The workload generator consists of three parts that handle specification of distributions, creation of an initial file system, and selection and execution file I/O operations. Results from experiments on a network file system (NFS) verify that the workload generator can produce realistic workloads and demonstrate the application of the generator.<<ETX>>
[real workload characterizations, network file system, Computational modeling, Computer simulation, Fasteners, performance evaluation, Application software, Sun, simulations, file system analysis, Analytical models, user file access behavior, File systems, High performance computing, Character generation, file servers, file I/O operations, Particle measurements, user-oriented synthetic workload generator]
Reliability of cluster-based multiprocessor systems
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
The reliability of a cluster-based multiprocessor system is analytically evaluated using modeling techniques. The results indicate that the cluster-based system has low reliability relative to the crossbar and multiple-bus systems. The weaknesses of the cluster-based system architecture, as far as reliability is concerned, are identified and a slight modification of the system architecture is proposed as a remedy. Using the same analytical modeling techniques, the reliability of the modified cluster-based systems's reliability is close to that of crossbar and multiple-bus systems.<<ETX>>
[Costs, multiprocessing systems, modeling, Multiprocessor interconnection networks, cluster-based system architecture, multiprocessor interconnection networks, Switches, reliability, Reliability engineering, multiple-bus systems, Multiprocessing systems, Computer science, Analytical models, Computer architecture, cluster-based multiprocessor systems, fault tolerant computing, Performance analysis, Telecommunication network reliability, crossbar]
How to migrate processes in distributed computing systems-a Markov team approach
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
A process migration mechanism offers a means to exploit the performance reserves present in networks of workstations used as personal computers by allowing migration of processes from overloaded to underused processors. Several distributed operating systems provide such a facility, the benefits of its use depending on the specification of a proper process migration policy. An analytical model, the Markov team model, to assist in the design of such a policy is presented. This model is derived from results of classical team theory and Markov decision processes. The special case of homogeneous distributed computing systems and methods for parameter estimation are discussed. Numerical examples are used to demonstrate the benefits of using this model.<<ETX>>
[Process design, Parameter estimation, Microcomputers, specification, Distributed computing, formal specification, Delay, Computer science, distributed computing systems, Intelligent networks, Operating systems, Markov decision processes, network operating systems, Markov processes, performance reserves, Sampling methods, parameter estimation, Workstations, Markov team approach, processes migration, networks of workstations, distributed operating systems]
Synchronization and concurrency measures for distributed computations
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
Several qualitative measures that quantify the degree of concurrency in a distributed computation are presented. The measures characterize the synchronization constraints inherent in a distributed computation and are independent of the underlying system running the computation. The measures are defined by two well defined abstractions, called cone and cylinder, to which simple measures can be associated: volume, weight, and height. Simple ways to compute the measures are proposed. The mechanism uses two types of vector clocks that trace the history of the computation. It is shown that the measures can be easily incorporated into any system to analyze distributed executions.<<ETX>>
[Algorithm design and analysis, vector clocks, Computational modeling, Performance gain, distributed processing, weight, Time measurement, Distributed computing, cone, Delay, synchronisation, abstractions, Concurrent computing, volume, distributed computations, concurrency control, Computer applications, Gain measurement, cylinder, Distributed algorithms, concurrency measures, height]
Axiomatic test sequence generation for extended finite state machines
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
A test suite generation method is proposed for conformance testing of communication protocols to solve the problem of generating test sequences for protocol specification models that have memories. A program verification technique called axiomatic semantics, is used to test protocols specified by extended finite state machines (EFSMs). While an EFSM is verified by the technique, observable events are recorded. By carefully manipulating the execution path in EFSM, the observed events can be used to examine the correctness of the protocol implementations.<<ETX>>
[Protocols, correctness, program verification, axiomatic test sequence generation, communication protocols, Flow graphs, conformance testing, finite state machines, execution path, Distributed computing, formal specification, generating test sequences, Information science, Computer network reliability, Automata, axiomatic semantics, Hardware, Telecommunication network reliability, protocols, Contracts, Testing, extended finite state machines, protocol specification models]
Adaptive routing in mesh-connected networks
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
It is shown that wormhole routing in mesh-connected networks can be deadlock free and adaptive without the addition of channels to the basic topology. Several partially adaptive routing algorithms for 2-D and 3-D meshes are described and simulated for a variety of conditions. Simulations of policies for selecting input channels show that transmitting extra information in the header flits can reduce communication latencies at high network throughputs. Simulations of policies for selecting output channels show that avoiding turns reduces latencies at high throughputs. Unrestricted nonminimal routing is found to reduce latencies slightly at low throughputs but increase latencies significantly at high throughputs. For nonuniform traffic patterns, a partially adaptive routing algorithm performs better than a nonadaptive one.<<ETX>>
[mesh-connected networks, multiprocessor interconnection networks, Glass, Telecommunication traffic, distributed processing, Routing, Throughput, deadlock free, Delay, Computer science, Intelligent networks, wormhole routing, Network topology, 2D meshes, 3-D meshes, System recovery, Traffic control, communication latencies, nonuniform traffic patterns]
Optical communication architectures for multimedia conferencing in distributed systems
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
The design of techniques and protocols for media mixing and communication architectures that optimize the performance of media mixing are discussed. The performance of monostage and multistage techniques for mixing are compared. Centralized and distributed architectures are analyzed for their suitability to both monostage and multistage media mixing, and hierarchical architectures that significantly reduce bandwidth consumption are presented. Algorithms for designing hierarchies that optimize real-time end-to-end delays and a packet train protocol in which the routing nodes can be used as mixers are proposed.<<ETX>>
[optical communication architectures, distributed processing, multimedia systems, packet train protocol, multistage techniques, Multimedia communication, Videoconference, Distributed computing, teleconferencing, Delay, hierarchical architectures, Computer architecture, Bandwidth, distributed systems, Optical fiber communication, optical communication, protocols, Multimedia systems, real-time end-to-end delays, Application software, multimedia conferencing, performance optimisation, delays, Streaming media, distributed architectures, media mixing, monostage techniques]
Exploiting a weak consistency to implement distributed tuple space
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
Multiprotocol tuple space (MTS), a distributed implementation of tuple space, is discussed. Although tuple space provides primitives convenient for describing several types of communication and data sharing, its naive implementation in distributed environments is not efficient without special hardware. MTS is a replication-based implementation on conventional workstations and a network. It provides several different replication-maintenance protocols that exploit weak consistency. The selection of protocols to achieve efficient replication maintenance is based on communication patterns.<<ETX>>
[Ethernet networks, Law, replication-maintenance protocols, distributed tuple space, weak consistency, Access protocols, communication patterns, data integrity, distributed environments, Distributed computing, Programming profession, Space stations, replication-based implementation, Information science, multiprotocol tuple space, distributed databases, replication maintenance, Hardware, Workstations, protocols, Legal factors, data sharing]
An all-sharing load balancing protocol in distributed systems on the CSMA/CD local area network
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
A protocol for dynamic load balancing in distributed systems on the CSMA/CD local area network is presented. Using the protocol, the workload is evenly distributed throughout the system when load-balancing activity is triggered, and effective load distribution is accomplished through transmission of load-balancing messages in a collision-free manner. Analytical and simulation results are presented to show the efficiency of the protocol.<<ETX>>
[Protocols, collision-free manner, distributed processing, Dynamic scheduling, load distribution, local area networks, Multiaccess communication, Computer science, Intelligent networks, Analytical models, resource allocation, all-sharing load balancing protocol, simulation results, Load management, distributed systems, CSMA/CD local area network, Performance analysis, Local area networks, Monitoring, carrier sense multiple access]
Probabilistic characterization of algebraic protocol specifications
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
A generative model for extending algebraic protocol specifications with probabilities is presented. The approach associates a simple probabilistic characterization with each algebraic operator occurrence in a behavior expression. The result is a compact notation in which the assignment of probabilities is more straightforward than with transition-based models. It is shown that an equivalent state machine with probabilities attached to transitions can be constructed automatically from an algebraic specification with probabilistic characterizations attached to operators. Specifically, it is shown how a probabilistic state machine can be derived from a basic LOTOS expression enriched by a probabilistic characterization. As an application example, a stop-and-wait protocol is examined.<<ETX>>
[Algorithm design and analysis, LOTOS expression, System testing, Protocols, probabilistic characterisation, equivalent state machine, Specification languages, formal specification, algebraic protocol specifications, Performance analysis, Carbon capture and storage, Standards development, protocols, stop-and-wait protocol, probabilities, algebraic operator occurrence]
Garbage collection of a distributed heap
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
A practical, fault-tolerant method for reclaiming inaccessible objects in a distributed heap is presented. The algorithm is general and does not require homogeneous components. It reclaims inaccessible objects in a timely fashion, including those that reside on inaccessible cycles. It allows each computer that contains parts of the heap to garbage collect independently according to its storage requirements, using whatever algorithm it chooses. A highly available service is used to store information about the intercomputer references. The computers containing parts of the heap communicate with the central service only periodically. By using the service the overhead at each node is minimized.<<ETX>>
[Availability, Access protocols, intercomputer references, Computer crashes, fault-tolerant method, Synchronization, garbage collection, Distributed computing, Counting circuits, storage management, distributed heap, Object detection, inaccessible cycles, Computer networks, data structures, storage requirements, Communication networks, Clocks]
Naming and addressing of objects without unique identifiers
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
The hierarchical naming scheme, a method for naming and addressing shared objects in large-scale distributed computer systems, is described. The scheme constructs global identifications (IDs) and addresses from locally unique ones. The IDs have relative representations that are translated if they are transmitted among naming contexts. This ensures uniqueness of IDs while preserving efficiency and availability. Methods that make the scheme suitable for mobile naming contexts and dynamic reconfiguration of networks and systems are reviewed. Implementation issues and solutions are presented.<<ETX>>
[Context, Laboratories, addressing shared objects, large-scale distributed computer systems, distributed processing, Mobile communication, Computer science, network operating systems, uniqueness, Intrusion detection, Broadcasting, hierarchical naming scheme, Load management, file organisation, global identifications, dynamic reconfiguration, Large-scale systems, Joining processes, Local area networks]
Communication-oriented assignment of task modules in hypercubic multicomputers
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
The problem of mapping a task that is composed of interacting modules onto a hypercube multicomputer is formulated and solved by minimizing an objective function called the communication traffic. The objective function allows module assignments to be found with the usual straightforward combinatorial optimization techniques. The problem of finding an assignment that minimizes the communication traffic is proved to be NP-hard, so a standard state-space search algorithm as well as other heuristic algorithms are used to find optimal/suboptimal solutions. The relative performances of various algorithms are evaluated using simulations. The assignments obtained from these algorithms are evaluated using an event-driven simulator to learn how they perform in real-world execution environments.<<ETX>>
[Performance evaluation, Costs, NP-hard, combinatorial mathematics, Laboratories, hypercubic multicomputers, hypercube networks, Fault tolerance, module assignments, Parallel processing, Hypercubes, communication-oriented assignment, Computational efficiency, discrete event simulation, event-driven simulator, search problems, real-world execution environments, objective function, task modules, interacting modules, heuristic algorithms, state-space search algorithm, Time measurement, Computer science, combinatorial optimization, Timing, communication traffic, computational complexity]
A decentralized deadlock-free concurrency control method for multidatabase transactions
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
A global concurrency control mechanism for multidatabase systems that preserves the autonomy of local databases and is free from global deadlocks is presented. The mechanism extends the notion of timestamps to a multidatabase environment to enforce the global serialization order through additional data operations on a data item stored in local systems. The main advantage of the mechanism is that it allows a fully distributed architecture, in which concurrency control decisions can be made on the basis of locally available information. Since no centralized information is maintained by the mechanism, it provides a higher degree of fault tolerance and allows incremental growth.<<ETX>>
[transaction processing, decentralized deadlock-free concurrency control, fault tolerance, timestamps, Control systems, Concurrency control, decentralised control, Application software, Computer science, global serialization order, Processor scheduling, concurrency control, fully distributed architecture, System recovery, Database systems, fault tolerant computing, multidatabase transactions, local databases]
Improving the reliability of name resolution mechanism in distributed operating systems
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
A way to improve the reliability of the name resolution mechanism in distributed operating systems is proposed. It is shown that the name resolution mechanism can be made highly reliable by achieving a high degree of reliability of the name resolution operation for a limited number of pairs. To take care of the changing reliability needs of a particular user for different objects with the passage of time, the system should provide the flexibility to dynamically change the degree of reliability of the name resolution operation for certain pairs. Implementation examples are presented.<<ETX>>
[Degradation, Operating systems, name resolution mechanism, network operating systems, reliability, file organisation, Application software, distributed operating systems]
Communication structures and paradigms for distributed conferencing applications
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
Communication paradigms and structures for workstation software supporting distributed conferencing applications is discussed. The communication system supports an abstraction of event clusters in which the various events generated by an application activity are bundled into one composite event. The communication and synchronization requirements for implementing the event cluster framework are identified and specified as ordering constraints on the individual events in various clusters. Two types of ordering are identified: causal ordering that is application specifiable, and total ordering that is system imposed. Two application models-multiplayer game and designer workbench-that are representative of many multiuser conferencing applications are introduced.<<ETX>>
[Transport protocols, Availability, total ordering, Ethernet networks, distributed conferencing applications, event clusters, designer workbench, communication structures, paradigms, Application software, Distributed computing, teleconferencing, synchronisation, Graphics, High-speed networks, network operating systems, workstation software, Broadcasting, synchronization, Mice, Workstations, causal ordering, ordering constraints, multiplayer game]
MCP: a protocol for coordination and temporal synchronization in multimedia collaborative applications
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
The multiflow conversation protocol (MCP), which provides two communication abstractions is discussed. MCP provides a token-based mechanism for concurrency control among participants of a multipoint connection and includes a communication abstraction called a multiflow conversation. A conversation may consist of one or more connections, and MCP enforces temporal synchronization in the delivery of traffic over participant connections. Delivery of traffic in a conversation is also based on a notion of causality that takes into account the delay constraints associated with real-time traffic.<<ETX>>
[Transport protocols, Communication system control, delay constraints, Telecommunication traffic, Displays, coordination, temporal synchronization, communication abstraction, multiflow conversation protocol, protocols, Manufacturing automation, multimedia collaborative applications, Concurrency control, Application software, synchronisation, Computer science, communication abstractions, Collaboration, concurrency control, delays, multipoint connection, Streaming media, MCP, token-based mechanism, real-time traffic]
Distributed constraint satisfaction for formalizing distributed problem solving
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
Viewing cooperative distributed problem solving (CDPS) as distributed constraint satisfaction provides a useful formalism for characterizing CDPS techniques. This formalism and algorithms for solving distributed constraint satisfaction problems (DCSPs) are compared. A technique called asynchronous backtracking that allows agents to act asynchronously and concurrently, in contrast to the traditional sequential backtracking techniques used in constraint satisfaction problems, is presented. Experimental results show that solving DCSPs in a distributed fashion is worthwhile when the problems solved by individual agents are loosely coupled.<<ETX>>
[distributed constraint satisfaction, Laboratories, cooperative distributed problem solving, distributed processing, problem solving, State-space methods, sequential backtracking, Intelligent agent, asynchronous backtracking, Computer science, Problem-solving, Resource management, constraint handling, Artificial intelligence]
Analyzing self-stabilization with finite-state machine model
[1992] Proceedings of the 12th International Conference on Distributed Computing Systems
None
1992
An approach to analyzing self-stabilization based on the finite-state machine model is presented. A finite-state machine is used to model the behavior of each node in a distributed system. when the self-stabilizing algorithms are applied. The approach is useful for analyzing the correctness of self-stabilizing algorithms and their time complexity. A self-stabilizing algorithm for finding maximal matching is used as an example to show how the finite-state machine model is applied. A simpler proof for the correctness and an upper bound of the time complexity tighter than the one proved by a variant function are attained.<<ETX>>
[Algorithm design and analysis, Tree data structures, Protocols, distributed system, time complexity, variant function, finite-state machine model, finite state machines, self-stabilisation, Computer science, maximal matching, Upper bound, Fault detection, Councils, Fault tolerant systems, fault tolerant computing, Distributed algorithms, Contracts, computational complexity]
Sharing complex objects in a distributed PEER environment
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
For distributed computing environments, required for computer integrated manufacturing and other engineering applications, it is most important to support the sharing and exchange of complex objects among cooperating sites, while preserving their autonomy. Specification of complex objects and their object boundaries in a federated database are described. Each database, as well as the entire federation, is modeled as a collection of related objects. Complex objects are defined as subgraphs of the entire object base and are specified by a root object and a collection of paths. A complex object can be distributed over several sites. A method is described that ensures referential integrity while maintaining the autonomy of each database. Different linearization techniques of complex objects are supported to enable applications to retrieve complex objects as single entities. This model is implemented in PEER, a federated, object-oriented database system developed for engineering applications.<<ETX>>
[object-oriented database system, Object oriented databases, Object oriented modeling, Computer integrated manufacturing, object-oriented databases, linearization techniques, Maintenance engineering, engineering computing, Data engineering, Application software, Distributed computing, complex objects, federated database, engineering applications, distributed PEER environment, distributed computing environments, Linearization techniques, distributed databases, Systems engineering and theory, Database systems, computer integrated manufacturing]
A formal assessment of synchronous testability for communicating systems
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
A testability assessment approach for multicomponent systems is proposed within the framework of formal description techniques. The approach relies on the formal definition of implementation conformance with respect to a given specification. It provides measures of user confidence and guidelines for an automatic design analysis. Both items are based on test requirements. The proposed technique for testability assessment formalizes the common-sense statement that testability of a component is degraded when this component cannot be tested in isolation. Furthermore, it gives a precise answer to the question: What are the limits of a test taking into consideration the constraints of a given environment?.<<ETX>>
[System testing, distributed processing, testability assessment, automatic design analysis, conformance testing, formal specification, formal description techniques, Guidelines, Degradation, synchronous testability, System recovery, Robustness, communicating systems, multicomponent systems]
IPL: a multidatabase transaction specification language
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
A multidatabase system (MDBS) integrates preexisting and heterogeneous databases in a distributed environment. A multidatabase transaction is a consistent and reliable execution of an application over a multidatabase system. The authors summarize the characteristics of multidatabase transactions and present a multidatabase transaction specification language, the InterBase Parallel Language (IPL). IPL allows users to write MDBS transactions by specifying all associated actions, their sequences, control flow, and data flow among subtransactions, and yet retaining the autonomies of the preexisting software systems. IPL also allows users to specify different commit protocols for different subtransactions and to control the atomicity and isolation granularity of an MDBS transaction. IPL components and design issues are described in detail. The implementation of IPL is also discussed.<<ETX>>
[transaction processing, Parallel languages, Protocols, IPL, commit protocols, heterogeneous databases, isolation granularity, Control systems, distributed environment, Distributed computing, consistent reliable execution, control flow, Distributed databases, distributed databases, specification languages, Database systems, subtransactions, protocols, action sequences, parallel languages, preexisting software systems, preexisting databases, atomicity, Specification languages, Transaction databases, Application software, InterBase Parallel Language, multidatabase transaction specification language, design issues, Computer industry, actions specification, data flow]
An algorithm for distributed groupware applications
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
Computer supported cooperative work (CSCW) is a rapidly growing field. Real-time groupware systems are addressed that allow a group of users to edit a shared document. The architecture and concurrency control algorithm used in this system are described. The algorithm is based on the semantics of the application and can be used by the developers of other groupware systems. The authors begin by introducing the notion of a purely replicated architecture and then present GroupDesign, a shared drawing tool implemented with this architecture. They then present the main parts of the algorithm that implement the distribution. The algorithm gives the best response time for the interface and reduces the number of undo and redo operations when conflicts occur.<<ETX>>
[Real time systems, computer supported cooperative work, real-time groupware, Collaborative software, CAD, Concurrency control, Computer crashes, Electronic mail, Application software, shared drawing tool, Delay, distributed groupware applications, Interactive systems, Fault tolerant systems, distributed algorithms, replicated architecture, concurrency control, groupware, shared document, Collaborative work, CSCW, GroupDesign]
Diagnosis of single transition faults in communicating finite state machines
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
The authors propose a generalized diagnostic algorithm for the case where more than one fault (output or transfer) may be present in one of the transitions of a deterministic system represented by a set of communicating finite state machines (CFSMs). Such an algorithm localizes the faulty transition in the distributed system once the fault has been detected. It generates, if necessary, additional diagnostic test cases which depend on the observed symptoms and which permit the location of the detected faults. The algorithm guarantees the correct diagnosis of any single or double fault (output and/or transfer) in at most one of the transitions of a deterministic system which is represented by a set of communicating FSMs. A simple example is used to demonstrate the functioning of the different steps of the proposed diagnostic algorithm.<<ETX>>
[Decision support systems, Software testing, System testing, program testing, Predictive models, single transition faults, finite state machines, parallel programming, Fault diagnosis, Communication system software, Fault detection, Automata, Software systems, communicating finite state machines, diagnostic test cases, Artificial intelligence]
Distributed shared repository: a unified approach to distribution and persistency
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
The authors propose an information management system providing distribution and persistency. By separating context from virtual address space, the system has a unified approach for both distribution and persistency. The former is achieved by moving contents between sites and the latter by moving contents between virtual address space and persistent storage. Contents include any information including data, program, and even the state of execution of a program. Contents are stored persistently in a logical space termed the distributed shared repository (DSR). A programming model for the DSR is proposed. Using the model, persistency, fine-grain mobility of information, and the passing of various distributed parameters can be obtained. The implementation anti experimental performance of the system are also presented.<<ETX>>
[Design automation, Information management, Distributed computing, parallel programming, Distributed processing, Information science, logical space, programming model, persistency, information management system, groupware, Hardware, distributed shared repository, virtual address space, distributed parameters, persistent storage, DSR, Collaborative software, virtual storage, fine-grain mobility, Application software, unified approach, Computer languages, distributed memory systems, Collaborative work]
Distributed divergence control for epsilon serializability
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
Epsilon serializability (ESR) allows for more concurrency by permitting nonserializable interleavings of database operations among epsilon transactions (ETs). The authors present the design of distributed divergence control (DDC) algorithms for ESR in homogeneous and heterogeneous distributed databases. They first present a strict two-phase locking DDC algorithm (S2PLDDC) and an optimistic DDC algorithm (ODDC) for homogeneous distributed databases, where the local orderings of all the sub-ETs of a distributed ET are the same, and the total inconsistency of a distributed ET is simply the sum of that of all its sub-ETs. A superdatabase DDC algorithm is described for heterogeneous distributed databases, where the local orderings of all the sub-ETs of a distributed ET may not be the same, and the total inconsistency of a distributed ET may be greater than the sum of that of all its sub-ETs. As a result, in addition to local divergence control in each site, a global mechanism is needed to guarantee ESR.<<ETX>>
[Algorithm design and analysis, transaction processing, heterogeneous distributed databases, S2PLDDC algorithm, Control systems, strict two-phase locking algorithm, global mechanism, Strontium, database operations, Distributed databases, superdatabase algorithm, distributed databases, Automatic control, nonserializable interleavings, Paramagnetic resonance, homogeneous distributed databases, epsilon transactions, Transaction databases, epsilon serializability, database theory, concurrency, Computer science, concurrency control, distributed divergence control, Distributed control, optimistic algorithm, local orderings, Interleaved codes, inconsistency, local divergence control]
Run-time support and storage management for memory-mapped persistent objects
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
The authors present the design and implementation of a persistent store called SPOMS. SPOMS is a runtime system that provides a store for persistent objects and is language independent. The objects are created via calls to SPOMS, and, when used, SPOMS directly maps them into the spaces of all requesting processes. The objects are stored in native format and are concurrently sharable. The store can handle distributed applications. The system uses the concept of a compiled class to manage persistent objects. The compiled class is a template that is used to create and store objects in a language independent manner and so that object reuse can occur without recompilation or relinking of an application that uses it. A prototype of SPOMS has been built on top of the Mach operating system. The motivations, the design, and implementation details are presented. Related and future work are discussed.<<ETX>>
[template, requesting processes, run-time support, native format, concurrently sharable, distributed applications, SPOMS, Concurrent computing, storage management, Runtime, Operating systems, network operating systems, Mach operating system, language independent manner, Object oriented programming, memory-mapped persistent objects, object-oriented programming, compiled class, Object oriented databases, Data security, object-oriented databases, object reuse, Spatial databases, Concurrency control, Application software, Programming profession, OOP, persistent objects]
Fast message ordering and membership using a logical token-passing ring
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
The Totem protocol supports consistent concurrent operations by placing a total order on broadcast messages. This total order is achieved by including a sequence number in a token circulated around a logical ring that is imposed on a set of processors in a broadcast domain. A membership algorithm handles reconfiguration, including restarting of a failed processor and remerging of a partitioned network. Effective flow-control allows the protocol to achieve message ordering rates two to three times higher than the best prior protocols. The single-ring total ordering protocol of Totem provides fault-tolerant agreed and safe delivery of messages within a broadcast domain.<<ETX>>
[safe delivery, Protocols, restarting, broadcast domain, Merging, sequence number, partitioned network, membership algorithm, Throughput, flow-control, reconfiguration, Delay, broadcast messages, Fault tolerance, Fault tolerant systems, Broadcasting, Totem protocol, message ordering, protocols, logical token-passing ring, fault-tolerant, single-ring total ordering protocol, total order, Partitioning algorithms, Maintenance, Computer science, failed processor, fault tolerant computing, token networks]
Responsive aperiodic services in high-speed networks
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
A fast packet-switched network is considered that transmits fixed-sized packets or cells and provides connection-oriented services. A number of rate-based service disciplines that offer timing guarantees have recently been proposed. These disciplines work well for connections which exhibit low burstiness or whose burstiness can be managed by admission control. However, some messages have irregular arrival patterns, but must be delivered with low delay. These aperiodic messages include connection set-up/tear-down messages, link-slate updates, and other status or alarm messages. The authors propose techniques that significantly minimize aperiodic message delay without jeopardizing guarantees made to existing connections. Simple hardware implementations of these techniques that can be embedded in output queues of fast packet-switches are also described.<<ETX>>
[responsive aperiodic services, timing guarantees, fast packet-switched network, alarm messages, packet switching, Switches, Telecommunication traffic, connection set-up/tear-down messages, admission control, output queues, Delay, high-speed networks, link-slate updates, Intelligent networks, Network servers, High-speed networks, hardware implementations, Bandwidth, Traffic control, scheduling, aperiodic message delay, message passing, queueing theory, burstiness, computer networks, Scheduling, fast packet-switches, connection-oriented services, fixed-sized packets, arrival patterns, Timing, rate-based service disciplines]
Average message overhead of replica control protocols
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
Management of replicated data has received considerable attention in the last few years. Several replica control schemes have been proposed which work in the presence of both node and communication link failures. However, this resiliency to failure inflicts a performance penalty in terms of the communication overhead incurred. Though the issue of performance of these schemes, from the standpoint of availability of the system, has been well addressed, the issue of message overhead has been limited to the analysis of worst-case and best-case message bounds. In this paper, we compare several well-known replica management protocols and control schemes in terms of their average-case message overhead. We also consider the tradeoff between the message overhead and availability, and we define the system model considered. Analytical expressions are derived for five well-known replica control protocols. The results are discussed with numerical examples.<<ETX>>
[Protocols, average message overhead, Communication system control, Control systems, system recovery, communication overhead, communication link failures, replica control protocols, Voting, replica management protocols, distributed databases, Permission, Performance analysis, protocols, Availability, replicated data management, performance penalty, Educational institutions, failure resiliency, database theory, Computer science, Binary trees, system availability, fault tolerant computing, node failures]
Distributed modeling and implementation of high performance communication architectures
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
High performance distributed computing systems require high performance communication systems. Distributed modeling and implementation of these communication systems is important. Toward this goal, the authors refine the process-to-channel/sub agent/-to-process (PCP) model of asynchronous distributed communication. While the PCP model provides a versatile and succinct mechanism for specifying and comparing different types of channels, it is inherently centralized. The refined model presented here, the process-to-channel/sub agent/-to-channel/sub agent/-to-process (PCCP) communication model, is amenable to distributed modeling and implementation of channels. The usefulness of the PCCP model is demonstrated by presenting a distributed implementation of hierarchical F-channels.<<ETX>>
[Algorithm design and analysis, high performance communication architectures, message passing, telecommunication channels, asynchronous distributed communication, high performance distributed computing systems, process-to-channel-to-channel-to-process, distributed processing, PCCP communication model, refined model, Distributed computing, Road transportation, High performance computing, Message passing, PCP model, Computer architecture, Distributed control, Parallel processing, Libraries, hierarchical F-channels, Distributed algorithms, process-to-channel-to-process, distributed modeling]
Hardware assist for distributed shared memory
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
The use of software implemented distributed shared memory (SDSM) to provide shared memory programming environments on networks of workstations and message-passing parallel computers has become quite popular. However, the memory reference patterns of many shared memory programs lead to poor performance on such systems. The authors propose hardware assist to improve the performance of SDSM systems faced with problematic reference patterns. An example of such a system is described. Operating system software in Mach is used to provide internode sharing in the example system, but is assisted through hardware support for maintaining update-based coherence of replicated pages. Simulations driven by hardware-collected parallel reference traces are used to provide an indication of the expected performance of the system.<<ETX>>
[update-based coherence, Costs, Protocols, distributed shared memory, operating system software, shared memory programs, replicated pages, Concurrent computing, Operating systems, shared memory programming environments, shared memory systems, Hardware, Computer networks, Workstations, message passing, performance evaluation, simulations, workstations, hardware-collected parallel reference traces, Parallel programming, High performance computing, memory reference patterns, Coherence, distributed memory systems, system performance, operating systems (computers), SDSM systems, programming environments, Mach, internode sharing, message-passing parallel computers]
Collective learning of action sequences
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
Learning in multiagent systems is a new research field in distributed artificial intelligence. The author investigates an action-oriented approach to delayed reinforcement learning in reactive multiagent systems and focuses on the question of how the agents can learn to coordinate their actions. Two basic algorithms, the ACE algorithm and the AGE algorithm (ACE and AGE stand for Action Estimation and Action Group Estimation, respectively), for the collective learning of appropriate action sequences are introduced. Both algorithms explicitly take into consideration that (i) each agent typically knows only a fraction of its environment, (ii) the agents typically have to cooperate in solving tasks, and (iii) actions carried out by the agents can be incompatible. The experiments described illustrate these algorithms and their learning capacities.<<ETX>>
[collective learning, Multiagent systems, AGE algorithm, Humans, action-oriented approach, Power system modeling, Delay, distributed artificial intelligence, Machine learning, ACE algorithm, Robustness, cooperative systems, learning (artificial intelligence), action sequences, Artificial intelligence, multiagent systems]
An open commit protocol preserving consistency in the presence of commission failures
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
Most of the proposed commit protocols assume that all participants of a transaction are sane, i.e., they only fail with omission failures and eventually recover. Unfortunately, this assumption is not realistic for open distributed systems (ODSs), which can be divided into a trusted and a nontrusted domain. While nodes in the trusted domain are assumed to be sane, nontrusted nodes may fail permanently and with commission failures. The open commit protocols presented are based on a model for consistency checking. The protocol also tolerates any number of commission failures in the nontrusted domain of an ODS. It guarantees that the trusted participants of a transaction terminate in a way that preserves consistency in the trusted domain, which generally does not mean that all trusted participants have to terminate consistently. The protocol groups those trusted participants that have to terminate consistently to maintain data consistency, and ensures that in each group the participants terminate in the same way. The advantages of the protocol are a simplified commit processing and a reduced message complexity. The message complexity of this protocol exceeds that of traditional two-phase commit protocols by no more than two messages for most practical cases.<<ETX>>
[Protocols, message passing, commission failures, Terminology, open systems, open distributed systems, Banking, Standardization, Computer crashes, communication complexity, consistency checking, Home computing, Waste materials, message complexity, trusted participants, open commit protocol, nontrusted domain, Communication networks, protocols]
Deadlock prevention in the RTC programming system for distributed real-time applications
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
The RTC distributed real-time programming system was implemented using AND-OR locking of system resources to meet real-time and concurrency control requirements. Since RTC processes can hold locks while acquiring others, deadlock is possible and therefore a deadlock prevention technique was implemented for AND-OR locking in such systems. The authors briefly discuss the RTC programming system, illustrate the system's use in programming a timed version of the classic dining philosophers example, describe the deadlock prevention technique, and show how it is applied in the RTC dining philosophers example.<<ETX>>
[Real time systems, deadlock prevention, Protocols, timed version, high level languages, RTC programming system, parallel programming, Information science, classic dining philosophers example, Prototypes, distributed real-time applications, system resources, software tools, programming, deadlock prevention technique, Concurrency control, Application software, Computer science, Computer languages, RTC programming language, RTC dining philosophers example, concurrency control, real-time systems, RTC distributed real-time programming system, System recovery, operating systems (computers), AND-OR locking, Timing]
Distributed application framework for large scale distributed systems
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
The author introduces an infrastructure to develop large scale distributed applications, called the distributed application framework (DAF). With DAF, services provided by a distributed system are implemented as individual shared libraries that could be linked and loaded into a running process. A generic server is able to dynamically link these libraries to provide those services to its clients. To manage services in a system, DAF applies the notion of name space used by distributed file systems. That is, each server provides a Unix-filesystem-like name space to name and manage available services. The result is that managing services on a server is similar to handling files on a file system. A prototype of DAF has been implemented on top of Sun OS 4.1 using the Sun remote procedure call (RPC), external data representation (XDR), and dynamic linker.<<ETX>>
[Unix, external data representation, network servers, distributed file systems, RPC, running process, distributed application framework, File servers, Unix-filesystem-like name space, file system, generic server, parallel programming, Network servers, File systems, Operating systems, Sun remote procedure call, Prototypes, Libraries, Large-scale systems, Sun OS, large scale distributed systems, name space, XDR, DAF, Application software, Sun, Programming profession, dynamic linker, infrastructure, file organisation, remote procedure calls, individual shared libraries]
Disk cache replacement policies for network fileservers
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
Trace driven simulations were used to study the performance of several disk cache replacement policies for network file servers. It is shown that locality based approaches, such as the common least recently used (LRU) policy, which are known to work well on stand-alone disked workstations and at client workstations in distributed systems, are inappropriate at a fileserver. Quite simple frequency based approaches do better. More sophisticated frequency based policies (eg., that take into account the file type) may offer additional improvements.<<ETX>>
[network fileservers, buffer storage, Computational modeling, performance evaluation, disk cache replacement policies, File servers, digital simulation, trace driven simulations, network file servers, file type, Network servers, frequency based approaches, common least recently used policy, System performance, file servers, Frequency, locality based approaches, distributed systems, Computer networks, Workstations]
Scheduling cooperative work: viewing distributed systems as both CSP and SCL
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
The authors propose to view cooperating tasks in distributed systems as both CSPs: (communicating sequential processes) and SCLs (sequential cooperating layers). This dual, complementary, view of distributed systems is based on the notion of communication closed layers. The same state-space abstraction and global assertions that were used for the design, development, verification, and testing of such systems are used as the guide and support for scheduling. With a given partition and a given mapping the scheduling guarantees the best performance. It takes advantage of the syntactic visibility of the logic behind task cooperation.<<ETX>>
[Real time systems, System testing, CSP, communicating sequential processes, state-space abstraction, sequential cooperating layers, scheduling cooperative work, global assertions, Distributed computing, syntactic visibility, Sequential analysis, cooperating tasks, groupware, scheduling, communication closed layers, Collaborative work, distributed systems, task cooperation, Logic, Intelligent systems, Distributed algorithms, SCL, state-space methods]
Supporting reliable and atomic transaction management in multidatabase systems
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
Transaction management in multidatabase systems (MDBSs) is complicated by the autonomy requirement, especially in the case of failure. We demonstrate necessary and sufficient conditions for supporting reliable and atomic transaction management in MDBSs. Most previous work assumes single version histories and conflict serializability; this precludes the use of multiversion scheduling protocols in the local database systems. To deal with multiple versions, it is necessary to extend conflict serializability to one-copy serializability. A decentralized transaction management scheme is presented for use in MDBSs which assumes local histories are one-copy serializable and cascadeless. Only a minimum access restriction is imposed on global update subtransactions. Our scheme not only ensures global serializability in the face of failures, but also ensures freedom from global deadlocks.<<ETX>>
[transaction processing, cascadeless local histories, autonomy requirement, reliability, Reliability engineering, History, Proposals, global serializability, system recovery, Sufficient conditions, Engineering management, global update subtransactions, distributed databases, scheduling, Database systems, minimum access restriction, multiple versions, conflict serializability, global deadlock freedom, Access protocols, Scheduling, Transaction databases, database theory, one-copy serializability, configuration management, atomic transaction management, concurrency control, failure, multiversion scheduling protocols, decentralized transaction management scheme, System recovery, multidatabase systems]
Intelligent job selection for distributed scheduling
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
A key issue in distributed scheduling is selecting appropriate jobs to transfer. A job selection policy that considers the diversity of job behaviors is proposed. A mechanism used in artificial neural networks, called weight climbing, is employed. Using this mechanism, a distributed scheduler can learn the behavior of a job from its past executions and make a correct prediction about whether transferring the job is worthwhile. A scheduler using the proposed job selection policy has been implemented and experimental results show that it is able to learn job behaviors fast, make decisions accurately and adjust itself promptly when system configuration or program behaviors are changed. In addition, the selection policy introduces only negligible time and space overhead.<<ETX>>
[Power system management, program behaviors, system configuration, past executions, Job design, intelligent job selection, distributed scheduling, Distributed computing, job selection policy, Information science, artificial neural networks, weight climbing, Processor scheduling, Neural networks, network operating systems, Keyboards, job behaviors, scheduling, Mice, Workstations, Resource management, neural nets]
Uniform reliable multicast in a virtually synchronous environment
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
The authors present the definition of and solution to the uniform reliable multicast problem in the virtually synchronous environment defined by the Isis system. A uniform reliable multicast of a message m has the property that if m has been received by any destination process (faulty or not), then m is received by all processes that reach a decision. Uniform reliable multicast provides a solution to the distributed commit problem. Two multicast primitives are defined in the virtually synchronous model: reliable multicast (called view-atomic) and uniform reliable multicast (called uniform view-atomic). The view-atomic multicast is used to implement the uniform view-atomic primitive. As view-atomicity is based on the concept of process group membership, a connection is established between the process group membership and the distributed commit problems.<<ETX>>
[Context, uniform reliable multicast, Protocols, message passing, distributed commit problem, process group membership, multicast primitives, reliable multicast, distributed processing, Computer crashes, Isis system, Delay, uniform view-atomic, Asynchronous communication, Message passing, uniform reliable multicast problem, Intersymbol interference, virtually synchronous environment, Broadcasting, view-atomic, protocols, view-atomicity, Contracts]
Structuring distributed shared memory with the pi architecture
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
The authors examine the use of a new architecture for the design of system software. The architecture allows the construction of flexible system software components, and the resulting realizations can be tailored to the needs of various applications. The focus is on the application of that architecture to distributed shared memory (DSM). The system software architecture, called pi , defines elements of system software as generalized objects. These x objects have two interfaces: one for usage and one for additional control. pi enables system software components to evolve as computer hardware evolves and it addresses the demands emerging classes of applications like object-oriented databases and multimedia. The pi architecture and its use in a DSM subsystem for clustered workstations are discussed.<<ETX>>
[Computer interfaces, flexible system software components, Object oriented databases, distributed shared memory, Multimedia systems, software design, object-oriented databases, Multimedia databases, Control systems, DSM subsystem, Application software, Software design, multimedia, systems analysis, Computer architecture, distributed memory systems, computer hardware, file organisation, shared memory systems, pi architecture, Hardware, System software, software tools, clustered workstations]
Real-time schedulability of two token ring protocols
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
When designing real-time communication protocols, the primary objective is to guarantee the deadlines of synchronous messages while sustaining a high aggregate throughput. The authors compare two token ring protocols for their suitability in hard-real-time systems. A priority driven protocol (e.g., IEEE 802.5) allows implementation of a priority based real-time scheduling discipline like the rate monotonic algorithm. A timed token protocol (e.g., FDDI) provides guaranteed bandwidth and bounded access time for synchronous messages. These two protocols are studied by deriving their schedulability criteria, i.e., the conditions which determine whether a given message set can be guaranteed. Using these criteria, the average performance of these protocols is evaluated under different operating conditions. It is observed that neither protocol dominates the other for the entire range of system parameter space. The conclusion is that the priority driven protocol performs better at low bandwidths (1-10 Mb/s) while the timed token protocol has a superior performance at higher bandwidths.<<ETX>>
[Real time systems, operating conditions, deadlines, synchronous messages, Whales, Bandwidth, scheduling, timed token protocol, Token networks, protocols, hard-real-time systems, message passing, FDDI, real-time communication protocols, bounded access time, Access protocols, priority driven protocol, token ring protocols, system parameter space, Computer science, Processor scheduling, real-time systems, Media Access Protocol, guaranteed bandwidth, real-time schedulability, aggregate throughput, rate monotonic algorithm, Time factors, token networks, priority based real-time scheduling discipline]
A state-aggregation method for analyzing dynamic load-balancing policies
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
Exact performance analyses of dynamic load-balancing policies for distributed systems are very difficult because the state space is multidimensional and load-balancing decisions are state-dependent. A state-aggregation method is proposed to analyze the performance of dynamic load-balancing policies. Those states with the same number of jobs are aggregated into a single state. The number of jobs in the system is modeled by a birth-death Markov process. The state transition rates are estimated by an iterative procedure. The proposed state-aggregation method is applied to analyze the performance of a particular dynamic load-balancing policy, namely a symmetric policy with threshold value equal to one. Extensive simulations were performed to study the accuracy of the state-aggregation method. This method provides accurate performance estimates for the symmetric policy for systems of various sizes when the mean job transfer delay is small compared to the average job service time.<<ETX>>
[symmetric policy, distributed processing, Steady-state, Distributed computing, dynamic load-balancing policies, multidimensional state space, state dependent load-balancing decisions, resource allocation, job service time, threshold value, distributed systems, Performance analysis, state transition rates, state-aggregation method, performance analyses, Delay effects, Delay estimation, performance evaluation, State-space methods, simulations, Computer science, Poisson equations, delays, accurate performance estimates, Markov processes, birth-death Markov process, Load management, iterative procedure, job transfer delay, performance analysis, state-space methods]
Extraction of logical concurrency in distributed applications
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
Some methods are discussed to extract and represent the concurrency inherent in distributed applications. Concurrency is an application level property that is characterized completely by the set of messages exchanged in the application and the causal ordering relationship among the messages. The concurrency may be viewed at the logical level in the application rather than at the physical message communication level. The concurrency has a direct relationship to the message delivery performance in the underlying computation such as asynchronism in message delivery protocols. The authors quantitatively analyze how the application level concurrency influences the execution of the application. The analysis is based on methods for representing concurrency and deriving measures of concurrency from the representations. System level execution models based on ISIS, x-kernel and the causal broadcast communication system are used as case studies in the analysis. The analysis is in general useful in the design of applications and to compare their projected performance levels in an implementation-independent manner.<<ETX>>
[Algorithm design and analysis, system level execution models, Protocols, message exchange, projected performance levels, case studies, Data mining, Distributed computing, parallel machines, distributed applications, parallel programming, Concurrent computing, x-kernel, logical concurrency, implementation-independent manner, Broadcasting, Performance analysis, causal ordering relationship, Monitoring, asynchronism, message passing, message delivery protocols, message delivery performance, application level property, Intersymbol interference, ISIS, Computer applications, physical message communication level, logical level, application level concurrency, causal broadcast communication system]
Event ordering in a shared memory distributed system
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
Past research has concentrated on ordering events in a system where processes communicate through messages. The authors look at issues in ordering events in a distributed system based on shared objects that interact via remote procedure calls (RPCs). They derive clock conditions for ordering operations on an object and provide clock maintenance schemes for time-stamping execution events. An object clock is associated with every shared object for clock exchange among processes. A clock maintenance algorithm is incrementally presented for objects where operations are atomic and an algorithm is described for large-grained objects where operations are nested and non-atomic.<<ETX>>
[distributed processing, Control systems, clock maintenance schemes, Yarn, Runtime, Automatic control, shared memory systems, clock conditions, execution events, message passing, Memory architecture, Debugging, shared objects, clock exchange, large-grained objects, Educational institutions, clock maintenance algorithm, Programming profession, time-stamping, shared memory distributed system, nested operations, Load management, remote procedure calls, messages, shared object, nonatomic operations, event ordering, Clocks]
Evaluating caching schemes for the X.500 directory
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
The OSI (Open Systems Interconnection) X.500 directory system and other distributed naming systems use name caching to minimize the cost of name lookups for nonlocal names. The authors evaluate the impact of name caching on the performance of the OSI directory system. They consider the issues of cache sizing and cache replacement policies. It was found that a locality of reference property holds in name resolution requests, and hence name caching does increase performance significantly. Using trace-driven simulation, it is shown that small caches (smaller than 30 items) yield hit ratios up to 60% and decrease the average name resolution time by 60%. For small caches, the LRU (least recently used) replacement policy is better than other implementable policies. Large caches yield predictably larger hit ratios. For large caches, however, the LRU policy is not better than a random replacement policy. It was also found that partitioning the cache buffer into a small number of independent caches, each one associated with a different kind of name request, further decreases the average name resolution time.<<ETX>>
[least recently used replacement policy, Costs, Protocols, open systems, Open Systems Interconnection, X.500 directory system, Electronic mail, cache replacement, Domain Name System, storage management, Web and internet services, Distributed databases, name resolution requests, cache buffer, Bandwidth, distributed databases, name resolution time, large caches, distributed database, buffer storage, caching scheme evaluation, cost, nonlocal names, OSI, name lookups, Information retrieval, trace-driven simulation, standards, name caching, Authentication, cache sizing, Open systems, OSI directory system, partitioning, distributed naming systems]
Development of a collaborative application in CSDL
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
Cooperative system programming deals with four topics: multiuser interfaces, coordination, shared workspace, and networking control. The goal of CSDL (Cooperative Systems Design Language) is to cover all these aspects. The authors present the development of a system in CSDL. The system allows a group of physically distributed users to edit a document concurrently. It permits sharing the single-user editor xedit by multiplexing the application's outputs to each participant, while inputs come from one user at a time. A simple floor control policy allows participants to designate who has that right. A detailed presentation of the coordination layer, and a discussion of system architecture are included.<<ETX>>
[Availability, shared workspace, Cooperative Systems Design Language, Cooperative systems, CSDL, Control systems, user interfaces, coordination layer, collaborative application, floor control policy, Teleconferencing, multiuser interfaces, Collaboration, Prototypes, concurrency control, system architecture, groupware, physically distributed users, Collaborative work, Computer networks, Hardware, Workstations, software tools, networking control]
Proxy-based authorization and accounting for distributed systems
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
A unified model is presented for authentication, authorization, and accounting that is based on proxies. It is shown that the proxy model for authorization can be used to support a wide range of authorization and accounting mechanisms. The proxy model strikes a balance between access-control-list anti capability-based mechanisms, allowing each to be used where appropriate and allowing their use in combination. The author describes how restricted proxies can be supported using existing authentication methods.<<ETX>>
[computer networks, accounting, proxy model, Electronic commerce, Authorization, Network servers, access-control-list anti capability-based mechanisms, Authentication, file servers, message authentication, unified model, authorisation, distributed systems, Computer networks, proxy-based authorization, Cryptography, authentication]
Average case behavior of election algorithms for unidirectional rings
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
The problem of election for asynchronous rings of processors is considered. Because of its many applications, this problem is important for both the practical and theoretical points of view. Thus, the availability of an algorithm that is good in both the average and the worst case has significant meaning. As an effort to achieve this goal, a new algorithm is designed and is simulated sequentially and analyzed by statistical methods. The statistical analysis demonstrates that the algorithm proposed is near optimal in the average case while its worst case message complexity is still O(n lg n). This is accomplished with the cost of one more bit in every message. This result is very interesting because it is contrary to the common belief that algorithms with good worst case complexity perform worse in the average case.<<ETX>>
[Algorithm design and analysis, Computer aided software engineering, Costs, message passing, multiprocessing systems, Statistical analysis, Nominations and elections, Educational institutions, communication complexity, Distributed computing, Organizing, worst case, Analytical models, message complexity, distributed algorithms, Whales, average case, statistical analysis, asynchronous rings, average case behavior, election algorithms, statistical methods, unidirectional ring]
A performance study of general grid structures for replicated data
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
Recently, there has been considerable interest in the study of replica-control protocols which are based on organizing several copies of an object into logical structures, such as rectangular grids. In addition to high availability, another objective in exploiting such structures is to improve the degree of load sharing in a system. The authors extend the scope of grid structures to general grids, which allow holes in various positions of a rectangular structure and are useful to consider because they often produce availabilities that are higher than solid grids, where every position must be occupied by a node. In addition to proposing an improvement to the existing grid protocol, new insights are offered into the performance of the grids, from both availability and load sharing points of view. Algorithms for designing grids to maximize availability independently and also in conjunction with a load sharing constraint are given.<<ETX>>
[Availability, Algorithm design and analysis, general grid structures, Protocols, load sharing, rectangular grids, distributed processing, performance evaluation, logical structures, Computer science, performance study, resource allocation, Voting, Engineering management, distributed algorithms, Permission, Cost function, data structures, protocols, replicated data]
Deriving protocol specifications from service specifications in extended FSM models
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
The authors propose a synthetic technique to derive a correct protocol specification from a given service specification modeled as a nondeterministic extended finite state machine (EFSM). Each EFSM has a finite state control and a finite number of registers. In the model, the next state and the next values of the registers are determined depending on not only the current state and input but also the current values of the registers. The registers correspond to the system resources and they are allocated to some of the protocol entities in a distributed system. The derived protocol entities' specifications satisfy the resource allocation specified by the designer. A procedure solving 0-1 integer linear programming problems is used to reduce the number of the messages exchanged among the protocol entities.<<ETX>>
[integer programming, nondeterministic extended finite state machine, Access protocols, finite state machines, Distributed computing, formal specification, 0-1 integer linear programming problems, Automata, protocol specifications, Communication channels, Automatic control, Integer linear programming, extended FSM models, Resource management, Telecommunication network reliability, Communication networks, protocols, Testing, service specifications, finite state control]
Deadline assignment in a distributed soft real-time system
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
In a distributed environment, tasks often have processing demands on multiple sites. A distributed task is usually divided up into several subtasks, each one to be executed at some site in order. In a real-time system, an overall deadline is usually specified by an application designer indicating when a distributed task is to be finished. To study the subtask deadline assignment problem a simple model of the system and tasks is postulated. The focus is on soft real-time systems. In such systems, it is very difficult to guarantee that all deadlines will be met, and hence one tries to minimize the number of deadlines that are missed. The authors examine (through simulations) four strategies for subtask deadline assignment in a distributed soft real-time environment.<<ETX>>
[Real time systems, processing demands, distributed task, subtask deadline assignment, application design, distributed soft real-time system, Sensor phenomena and characterization, distributed processing, Radar tracking, Spatial databases, Sensor systems, simulations, Computer science, Image sensors, Processor scheduling, Surveillance, deadline assignment, multiple sites, real-time systems, Radar imaging, scheduling, operating systems (computers), real-time system]
Laura: a coordination language for open distributed systems
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
Open distributed systems are an emerging class of distributed systems that have to take into account a number of heterogeneities in the system components and possibly high dynamics in the system structure by unrestrictedly joining and leaving agents. Uncoupled processing is a basis for a solution of the coordination problem that arises. Laura is a coordination language for open distributed systems following this paradigm by introducing a service space via which agents offer and request services without knowing about each other. They place and withdraw forms from the service space describing the type of service offered or requested. Type conformance based on subtyping determines if the forms match. An initial Laura implementation is described and further issues such as scalability and a role model are considered.<<ETX>>
[type conformance, open systems, coordination language, Scalability, open distributed systems, parallel programming, scalability, Computer architecture, service space, Hardware, Computer networks, parallel languages, Availability, heterogeneities, Multimedia systems, uncoupled processing, Computer science, Computer languages, emerging class, Laura, Open systems, system components, Software systems, joining agents, role model, leaving agents]
A subsystem for swapping and mapped file I/O on top of Chorus
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
Chorus is a micro-kernel-based distributed operating system architecture. The authors explore the architectural and implementational issues involved in constructing a distributed paging service in the Chorus environment. Apart from outlining the pager architecture, they provide insight into how the characteristic goals of a critical distributed application on top of the Chorus system may be put into practice. The respective Chorus features are thereby judged in view of their suitability with respect to the pager implementation. The results of an experimental evaluation of the pager are included.<<ETX>>
[micro-kernel-based distributed operating system architecture, Chorus, Buildings, critical distributed application, virtual storage, distributed paging service, input-output programs, implementational issues, Yarn, Research and development, experimental evaluation, pager architecture, Parallel programming, Operating systems, Emulation, network operating systems, Broadcasting, mapped file I/O, Kernel, Assembly]
Replicated RPC using Amoeba closed group communication
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
Since remote procedure call (RPC) has become the method of choice for client-server communication in a distributed operating system, providing a fault-tolerant RPC mechanism is crucial to ensuring system reliability. The author presents a replicated RPC library for the Amoeba distributed operating system. The library's RPC protocol is presented in detail, including preliminary performance figures. The protocol is distinguished by its use of closed process groups in conjunction with the coordinator-cohort method of computation. The method presented is applicable to any system supporting closed process groups and totally ordered multicast.<<ETX>>
[replicated RPC library, Optimization methods, system reliability, File servers, replicated RPC, client-server communication, Fault tolerance, Operating systems, totally ordered multicast, Fault tolerant systems, network operating systems, distributed operating system, Libraries, remote procedure call, protocols, Amoeba closed group communication, closed process groups, performance figures, performance evaluation, Multicast protocols, fault-tolerant RPC mechanism, Amoeba distributed operating system, Programming profession, Computer science, Packaging, remote procedure calls, fault tolerant computing, RPC protocol, coordinator-cohort method]
A low-level processor group membership protocol for LANs
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
Presents a processor group membership protocol designed to run on top of a local area network. The protocol maintains information about a selected group of stations that explicitly join the protocol by keeping a replica of a global membership table at every member. Additionally, the protocol guarantees that a given station always occupies the same entry in the table. As a result, table indexes uniquely and universally identify a station and can thus be used as short identifiers. The interest of a processor group membership is twofold: it is a powerful auxiliary for process group membership management and it provides support for efficient message addressing.<<ETX>>
[replica, Very large scale integration, distributed processing, local area networks, low-level processor group membership protocol, table lookup, Fault tolerance, Fault tolerant systems, efficient message addressing, global membership table, distributed systems, Communication system traffic control, protocols, Marine vehicles, Local area networks, table indexes, fault tolerance, Power system management, identifiers, communication protocols, information maintenance, Multicast protocols, network stations, process group membership management, distributed algorithms, real-time systems, local area network, Media Access Protocol, LANs, fault tolerant computing, Energy management]
Providing performance guarantees in an FDDI network
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
A network subsystem supporting a continuous media file system must guarantee a minimum throughput, a maximum delay, and a maximum jitter. The authors present a transport protocol that provides these guarantees. To support different types of service, the protocol is built from modules selected to meet the requirements of each communication session. A buffering technique is used to provide jitter guarantees. To provide throughput and delay guarantees, network performance is optimized based on the required transfer rate. The effects of controlling transmission rate and packet size are presented. The resulting transport protocol is modeled on a simulated FDDI (fiber distributed data interface) network and the results are analyzed. It is shown that the protocol provides the required guarantees for the anticipated types of traffic.<<ETX>>
[Transport protocols, Communication system control, Jitter, multimedia systems, Throughput, transfer rate, minimum throughput, FDDI network, maximum delay, Analytical models, transport protocol, File systems, network performance, performance guarantees, maximum jitter, network subsystem, Communication system traffic control, Size control, protocols, transmission rate, buffer storage, FDDI, continuous media file system, packet size, fiber distributed data interface, Optical buffering, buffering technique, file organisation, communication session, simulated FDDI network]
Approximate analysis of priority scheduling systems using stochastic reward nets
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
Presents a performance analysis of a heterogeneous multiprocessor system where tasks may arrive from Poisson sources as well as by spawning and probabilistic branching of other tasks. Non-preemptive priority scheduling is used between different tasks. Stochastic reward nets are used as the system model, and are solved analytically by generating the underlying continuous-time Markov chain. An approximation technique is used, that is based on fixed-point iteration to avoid the problem of a large underlying Markov chain. The iteration scheme works reasonably well, and the existence of a fixed point for the iterative scheme is guaranteed under certain conditions.<<ETX>>
[iterative methods, Delay, Multiprocessing systems, resource allocation, Feedback, scheduling, Performance analysis, continuous-time Markov chain, spawning, stochastic reward nets, multiprocessing systems, queueing theory, heterogeneous multiprocessor system, nonpreemptive priority scheduling systems, Poisson sources, probabilistic branching, performance evaluation, State-space methods, Computer science, Processor scheduling, Stochastic systems, Markov processes, fixed-point iteration, Resource management, Queueing analysis, performance analysis, approximate analysis, task arrival]
Degradable agreement in the presence of Byzantine faults
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
The authors consider a system consisting of a sender that wants to send a value to certain receivers. Byzantine agreement protocols have previously been proposed to achieve this in the presence of arbitrary failures. The imposed requirement typically is that the fault-free receivers must all agree on the same value. An agreement protocol is proposed that achieves Lamport's Byzantine agreement (L. Lamport et al., 1982) up to a certain number of faults and a degraded form of agreement with a higher number of faults. The degraded form of agreement allows the fault-free receivers to agree on at most two different values, one of which is necessarily the default value. The proposed approach is named degradable agreement. An algorithm for degradable agreement is presented along with bounds on the number of nodes and network connectivity necessary to achieve degradable agreement.<<ETX>>
[Protocols, degradable agreement, software reliability, Byzantine agreement protocols, distributed processing, fault-free receivers, Byzantine faults, default value, Synchronization, system recovery, synchronisation, Degradation, Computer science, network connectivity, arbitrary failures, fault tolerant computing, Safety, protocols, Clocks]
Minimal-delay decentralized maintenance of processor-group membership in TDMA-bus LAN systems
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
Decentralized approaches to processor-group maintenance (GMM) are aimed at facilitating every active node in a real-time LAN system to maintain timely and consistent knowledge about the health status of all cooperating nodes and to recognize newly joining nodes. A practical scheme for this decentralized GMM (DGMM) in TDMA (time division multiple access) bus based real-time LAN systems, called here the periodic reception history broadcast (PRHB) scheme, was initially formulated by H. Kopetz et al. (1989) for application environments where the fault frequency is relatively low such that no more than one node fails in any interval of two TDMA cycle duration. The authors develop a major extension of the scheme, PRHB with multiple fault detection (PRHB/MD), which is applicable to environments where the fault frequency is much higher-to be more specific, where up to a half of the nodes map experience faults within any interval of three TDMA cycle duration. The scheme does not impose any limit on the number of transient faults of links that any one node may experience. The scheme yields the minimal detection delay for all major fault types and the delay does not exceed two TDMA cycles for the worst fault type. This detection delay characteristic is a significant improvement over those of previously developed DGMM schemes.<<ETX>>
[Real time systems, active node, Time division multiplexing, local area networks, History, system recovery, Delay, Time division multiple access, minimal-delay decentralized maintenance, Broadcasting, processor-group membership, Local area networks, real-time LAN system, Maintenance engineering, transient faults, bus based real-time LAN, processor-group maintenance, multiple fault detection, fault frequency, detection delay characteristic, Fault detection, time division multiple access, real-time systems, local area network, Frequency, fault types, fault tolerant computing, periodic reception history broadcast, decentralized GMM, TDMA-bus LAN systems]
Performance of co-scheduling on a network of workstations
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
In a set of high performance workstations connected by a network, many workstations may be underutilized by their owners. While each workstation may be primarily responsible for executing its owner's tasks with the highest priority, the unused processing capacity may be made available to computationally intensive tasks submitted externally to the system. Static co-scheduling for such an environment has been considered previously (M.J. Atallah et al., 1991), where the goal was to maximize the speedup by partitioning the task among many workstations. The authors consider the problem from the system point of view, and develop a queuing model and efficient algorithms to minimize the mean response time. The results obtained show that significant improvements in the mean response time can be obtained through co-scheduling over that of the M/M/m system where each task would be assigned to a single workstation as a whole.<<ETX>>
[single workstation, static co-scheduling, computer networks, co-scheduling, queuing mode, Partitioning algorithms, Delay, workstations, mean response time, M/M/m system, high performance workstations, Processor scheduling, resource allocation, Operating systems, High performance computing, network operating systems, scheduling, Load management, Computer networks, Workstations, Resource management, unused processing capacity, Monitoring, computationally intensive tasks]
Coherence in naming in distributed computing environments
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
Many different kinds of names (identifiers) are used in computer systems. Names are resolved (interpreted) in a context. A context is a function that maps names to entities. Multiple contexts allow the flexibility of giving different meanings to a name in different parts of the system; however, there are situations where it is desirable for the meaning of a name to be the same in different parts. This property is called coherence in naming. Since the meaning of a name depends on the context selected, the analysis of coherence is based on the notion of closure mechanisms-implicit rules that select a context for resolving names. The authors define coherence and show how it is affected by various closure mechanisms. Then they present several approaches for dealing with the lack of coherence. Incoherence arises from selecting an incorrect context, and consequently, closure mechanisms are involved in the solutions.<<ETX>>
[Pervasive computing, Algorithm design and analysis, incoherence, Laboratories, incorrect context, distributed processing, Application software, Distributed computing, Sun, implicit rules, Milling machines, Computer science, closure mechanisms, distributed computing environments, computer systems, context, Concrete, Hardware, data structures, coherence in naming]
Dynamic reconfiguration in distributed systems: adapting software modules for replacement
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
Dynamic reconfiguration of a distributed application is the act of changing the configuration of the application as it executes. Examples of configuration changes are replacing a software component (module), moving a module to another machine, and adding or removing a module from the application. The extension to a reconfiguration platform described automatically prepares a module for participation in reconfiguration. A machine-independent method for automatically installing this functionality in the application, given a set of reconfiguration points designated by the programmer, is presented. The focus is on the difficult problem of capturing and restoring the state of a module during a procedure call, when the activation record stack contains crucial parts of the process state.<<ETX>>
[activation record stack, Software maintenance, procedure call, Software algorithms, Communication system control, distributed processing, program execution, Educational institutions, Application software, software module adaptation, Yarn, Programming profession, Computer science, supervisory programs, distributed application reconfiguration, process state, Software systems, remote procedure calls, distributed systems, dynamic reconfiguration, machine-independent method, Contracts, programming]
Inter-machine protocols for electronic libraries
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
Emerging technology permits electronic document libraries which store large numbers of images for access in wide-area networks. The expected statistics and security needs of electronic library applications favor new client-server session protocols. The author proposes a scheme that makes communications efficient in a well-defined sense. DocSS, a document storage subsystem, implements the scheme. An early version is in production in human service (welfare, taxation, police) and engineering applications.<<ETX>>
[distributed processing, visual databases, electronic document libraries, DocSS, welfare, client-server session protocols, security, engineering applications, wide-area networks, Production, Cities and towns, Libraries, Workstations, inter-machine protocols, document storage subsystem, Biomedical engineering, Biomedical imaging, Access protocols, library automation, Statistics, Chemistry, human service, police, Insurance, taxation, statistics]
Decentralized consensus protocols with multi-port communication
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
The authors develop efficient decentralized consensus protocols for a distributed system with multi-port communication. Two classes of decentralized consensus protocols are considered: the one without an initiator and the one with an initiator. The case of one-port communication is first presented, i.e., each node can send out one message in one step, and then results are derived for the case of multi-port communication, i.e., each node can send out more than one message in one step. Given an arbitrary number of nodes in a system, the proposed protocols can reach the consensus in the minimal numbers of message steps. The number of messages incurred by each algorithm is also derived.<<ETX>>
[Protocols, message passing, initiator, distributed system, multi-port communication, Synchronization, decentralized consensus protocols, Distributed computing, message steps, Message passing, distributed algorithms, Broadcasting, Cost function, protocols, Clocks, one-port communication]
Using group communication to implement a fault-tolerant directory service
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
Group communication is an important paradigm for building distributed applications. The authors discuss a fault-tolerant distributed directory service based on group communication, and compare it with the previous design and implementation based on remote procedure call (RPC). The group directory service uses an active replication scheme and, when triplicated, can handle 627 lookup operations per second and 88 update operations per second (using nonvolatile RAM). This performance is better than the performance for the RPC implementation and it is even better than the performance for directory operations under SunOS, which does not provide any fault tolerance at all. The conclusion is that the implementation using group communication is simpler and has better performance than the one based on remote procedure call, supporting the claim that a distributed operating system should provide both remote procedure call and group communication.<<ETX>>
[software reliability, fault-tolerant directory service, directory operations, Mathematics, distributed applications, fault-tolerant distributed directory service, Fault tolerance, Network servers, nonvolatile RAM, Operating systems, active replication scheme, network operating systems, distributed operating system, distributed databases, SunOS, remote procedure call, update operations, group directory service, Buildings, Read-write memory, Application software, group communication, Programming profession, Computer science, remote procedure calls, fault tolerant computing, Telecommunication network reliability, lookup operations]
Maintaining information about persistent replicated objects in a distributed system
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
Presents a general model for persistent replicated object management and identify what metainformation about objects needs to be maintained by a naming and binding service to ensure that objects named by application programs are bound to only those object replicas which are in a mutually consistent state. These ideas are developed within the framework of a distributed system in which application programs are composed of atomic actions (atomic transactions) manipulating persistent (long-lived) objects.<<ETX>>
[Protocols, atomic actions, distributed processing, distributed system, persistent replicated object management, Distributed computing, Concurrent computing, distributed databases, application programs, Hardware, Workstations, object-oriented methods, mutually consistent state, Local area networks, Availability, atomic transactions, object-oriented programming, Computational modeling, object-oriented databases, object naming service, information maintenance, Computer crashes, binding service, Robust control, object metainformation, long-lived object manipulation, object replicas]
Failure evaluation of disk array organizations
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
The authors present an evaluation of some of the disk array organizations proposed in the literature. They evaluate three alternatives for sparing, hot sparing, distributed sparing, and parity sparing, and two options for data layout, regular RAID5 and block designs, and systems based on combinations of these data layout and sparing alternatives. The performance of these organizations is evaluated with different reconstruction strategies. It is shown that parity sparing and distributed sparing have better performance and shorter reconstruction times than hot sparing. It is shown that both block designs as a data layout policy and distributed sparing as a sparing policy reduce the reconstruction time after a failure. The impact of reconstruction strategies is studied, and it is shown that, at higher workloads, choice of reconstruction strategy has a significant impact on the performance of the systems.<<ETX>>
[data layout, hot sparing, reconstruction times, multiprocessing systems, failure evaluation, input-output programs, data layout policy, system recovery, reconstruction strategies, Organizing, disk array organizations, sparing alternatives, storage management, parity sparing, distributed sparing, performance, High performance computing, block designs, Protection, regular RAID5]
General structured voting: a flexible framework for modelling cooperations
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
Data replication schemes and mutual exclusion protocols can be regarded as special instances of cooperation schemes, which describe the interaction of independent nodes within a computer network to achieve a common goal. The author presents a new mechanism called general structured voting for cooperation management and demonstrates its use for handling instances of these problem domains. The proposed approach is shown to be very flexible, covers a wide range of scenarios, and supports an easy tailoring. In particular, it supports the switching from one cooperation scheme to another, e.g., as a reaction to changing network characteristics, simply by modifying the parameters of the model, avoiding a time and money consuming modification of the implementation.<<ETX>>
[modelling cooperations, cooperation scheme, computer networks, cooperation management, cooperation schemes, Computer crashes, general structured voting, computer network, flexible framework, changing network characteristics, Network topology, data replication schemes, Voting, distributed algorithms, independent nodes, problem domains, distributed databases, groupware, mutual exclusion protocols, Computer network management, special instances]
Distribution and inheritance in the HERON approach to heterogeneous computing
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
HERON is a platform for object-oriented distributed computing in an open systems environment. We try to achieve a degree of distribution transparency previously known only from special distributed programming systems, while at the same time accommodating heterogeneous, autonomous computer systems. Distributed programs are written in Eiffel. The Eiffel language system is not modified: HERON employs proxies for remote object invocation and a flexible configuration procedure for building servers and distributed programs. In addition to regular objects, two kinds of distributed objects are supported by the proxy generator: dispersed objects and objects fragmented by remote inheritance. They contribute to distribution transparency both for distributed programs and for client/server systems.<<ETX>>
[open systems, HERON, flexible configuration procedure, Standardization, distributed processing, inheritance, Distributed computing, Research and development, Network operating systems, Concurrent computing, Distributed processing, remote inheritance, servers, Operating systems, client/server systems, Eiffel language system, heterogeneous autonomous computer systems, proxy generator, object-oriented distributed computing, Object oriented programming, object-oriented programming, distributed programming systems, distribution transparency, dispersed objects, open systems environment, Programming environments, fragmented objects, Open systems, distributed objects, remote object invocation, heterogeneous computing platform]
k-coteries for fault-tolerant k entries to a critical section
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
The authors extend the concept of coterie into k-coterie for k entries to a critical section. A structure named Cohorts is proposed to construct quorums in a k-coterie. The solution is resilient to node failures and/or network partitioning and has a low communication cost. The Cohorts structure is further improved to increase the availabilities of 1-entry critical sections.<<ETX>>
[Access control, Costs, fault-tolerant k entries, Fault tolerance, resource allocation, Fault tolerant systems, Broadcasting, distributed systems, access control, Distributed algorithms, Contracts, Cohorts, communication cost, quorums, critical section, Resilience, Computer science, 1-entry critical sections, Councils, distributed algorithms, concurrency control, network partitioning, fault tolerant computing, k-coteries, node failures]
Practical considerations for non-blocking concurrent objects
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
An important class of concurrent objects are those that are nonblocking, that is, whose operations are not contained within mutually exclusive critical sections. A nonblocking object can be accessed by many threads at a time, yet update protocols based on atomic compare-and-swap operations can be used to guarantee the object's consistency. The author examines the compare-and-swap operation in the content of contemporary bus-based shared memory multiprocessors, although the results generalize to distributed shared memory multiprocessors. He describes an operating system-based solution that permits the construction of a nonblocking compare-and-swap function on architectures that only support more primitive atomic primitives such as test-and-set or atomic exchange. Several locking strategies are evaluated that can be used to synthesize a compare-and-swap operation, and it is shown that the common techniques for reducing synchronization overhead in the presence of contention are inappropriate when used as the basis for nonblocking synchronization. A simple synchronization strategy is described that has good performance because it avoids much of the synchronization overhead that normally occurs when there is contention.<<ETX>>
[System testing, bus-based shared memory multiprocessors, Software performance, atomic exchange, Yarn, Concurrent computing, Information science, synchronization overhead, nonblocking concurrent objects, shared memory systems, update protocols, protocols, Contracts, nonblocking synchronization, locking strategies, operating system, test-and-set, Access protocols, threads, contention, synchronisation, Computer science, distributed shared memory multiprocessors, concurrency control, US Government, primitive atomic primitives, System recovery, atomic compare-and-swap operations, operating systems (computers), mutually exclusive critical sections]
Distributed active catalogs and meta-data caching in descriptive name services
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
Today's global internetworks challenge the ability of name services and other information services to locate data quickly. The authors introduce distributed active catalog and meta-data caching for optimizing queries in this environment. The active catalog constrains the search space for a query by returning a list of data repositories where the answer to the query is likely to be found. Meta-data caching improves performance by keeping frequently used characterizations of the search space close to the user, and eliminating active catalog communication and processing costs. When searching for query responses, the techniques contact only the small percentage of the data repositories with actual responses, resulting in search times of a few seconds. A distributed active catalog and meta-data caching method was implemented in a prototype descriptive name service called Nomenclator. Performance results for Nomenclator in a search space of 1000 data repositories are presented.<<ETX>>
[Costs, Information filtering, query responses, Distributed computing, query processing, storage management, Web and internet services, Prototypes, prototype descriptive name service, distributed databases, Information filters, search space, active catalog, buffer storage, global internetworks, Scholarships, meta-data caching, query optimisation, Nomenclator, name services, information services, data repositories, search times, distributed active catalogs, Query processing, descriptive name services, distributed active catalog, Catalogs, Indexing]
Analysis of multicast-based object replication strategies in distributed systems
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
The author presents an analysis of three object replication strategies based on reliable multicast protocols. A multiclass queuing model is used for obtaining performance characteristics of each strategy. The strategies are compared for a wide set of parameters showing the best strategy suitable for a given situation: the parallel update strategy is interesting for low load conditions and for the low read/write ratio.<<ETX>>
[Availability, object-oriented programming, queueing theory, Object oriented modeling, Laboratories, Binary search trees, Multicast protocols, parallel update strategy, Delay, Computer science, low read/write ratio, low load conditions, Atomic layer deposition, Message passing, multiclass queuing model, reliable multicast protocols, multiprogramming, Parallel processing, distributed systems, object-oriented methods, performance characteristics, protocols, multicast-based object replication strategies]
Reconfiguration of spanning trees in networks in the presence of node failures
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
Connectivity among a set of user entities in a network can be provided by a network level abstraction of an acyclic graph (or spanning tree). The authors discuss the reconfiguration of a graph in the presence of failures of network nodes. A reconfiguration manifests itself as a graph fragmentation problem, whereby two or more disjoint subgraphs attempt to connect with one another to form a composite graph. Fragment interconnection requires contention resolution between fragments to avoid cycles. Two classes of contention resolution algorithms applicable for environments with a potentially large number of fragments are presented. They are based on preestablished ranking among fragments and random arbitration among fragments. The algorithms have been evaluated by simulation and compared. The algorithms are useful in supporting data multicasting across workstations and distributed computations involving data on different machines.<<ETX>>
[Protocols, wide area networks, trees (mathematics), spanning trees, Distributed computing, Concurrent computing, Intelligent networks, Multicast algorithms, data multicasting, Network topology, Tree graphs, distributed computations, wide-area networks, network level abstraction, Computer networks, fault tolerant computing, Workstations, Large-scale systems, node failures, acyclic graph]
Composition of concurrent programs
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
A model and a notation are developed for specifying the composition of concurrent programs. The work is based on the observation that the composition of concurrent programs often requires not only intraprocessor coordination but also interprocessor coordination. A notation is developed for explicitly specifying both forms of coordination within a single uniform framework. Much prior work has either ignored the interprocessor coordination aspects of composition, or treated it in a manner separate from the intraprocessor coordination aspects.<<ETX>>
[programming theory, Law, multiprocessing programs, interprocessor coordination, Computational modeling, Lattices, concurrent program composition specification, intraprocessor coordination, formal specification, Concurrent computing, Computer languages, notation, Legal factors]
Evaluation of closely coupled systems for high performance database processing
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
Closely coupled systems aim at a more efficient communication and cooperation between processing nodes compared to loosely coupled systems. This can be achieved by using globally shared semiconductor memory to speed up the exchange of messages or to store global data structures. For distributed database processing, the database sharing (shared disk) architecture can benefit most from such a close coupling. The author presents a detailed simulation study of closely coupled database sharing systems. A shared store called global extended memory (GEM) was used for system-wide concurrency and coherency control, and to improve input/output (I/O) performance. The performance of such an architecture is evaluated and compared with loosely coupled database sharing systems employing the primary copy approach for concurrency and coherency control. In particular, the impact of different update strategies (FORCE vs. NOFORCE) and workload allocation schemes (random vs. affinity-based routing) is studied. The use of shared disk caches implementing a global database buffer is also considered. Simulation results are presented for synthetically generated debit-credit workloads and a real-life workload represented by a database trace.<<ETX>>
[storage allocation, closely coupled systems, Scalability, globally shared semiconductor memory, lobal data structures, workload allocation schemes, Relational databases, Control systems, database trace, database sharing architecture, Semiconductor memory, simulation study, Concurrent computing, global extended memory, Operating systems, distributed database processing, synthetically generated debit-credit workloads, Distributed databases, distributed databases, shared memory systems, data structures, processing nodes, real-life workload, loosely coupled database sharing systems, input/output performance, global database buffer, coherency control, performance evaluation, system-wide concurrency, Data structures, Transaction databases, shared disk caches, Computer science, high performance database processing, primary copy approach, closely coupled database sharing systems, concurrency control, update strategies, semiconductor storage]
Asynchronous event handling in distributed object-based systems
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
Discusses the design and the operating system support necessary for providing asynchronous event handling in distributed, passive object-based programming environments, where objects are potentially shared by disparate applications. We discuss the necessity of thread-based as well as object-based event notification and how a variety of hard-to-solve distributed programming issues can be handled by using the approach outlined in the design. The usefulness of the design is considered by using some examples. The implementation strategy and related work in this area are discussed.<<ETX>>
[Cloud computing, object-oriented programming, multiprocessing programs, thread-based event notification, Process control, asynchronous event handling, shared objects, Educational institutions, implementation strategy, distributed programming environments, Yarn, Distributed computing, object-based event notification, Programming environments, concurrency, Computer science, Concurrent computing, distributed object-based systems, passive object-based programming environments, Operating systems, operating system support, Signal processing, operating systems (computers), programming environments]
Delivering multicast messages in networks with mobile hosts
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
There is a strong trend toward integrating portable computers within existing data networks. Traditional network protocols were designed assuming a static view of network connectivity. A mobile host can connect to the network from different locations at different times. This has led to the emergence of a new set of problems with regard to addressing schemes and network protocols for accommodating mobility within existing networks. The authors present a protocol for delivering a multicast message exactly once to a group of mobile destinations. The protocol is based on a system model derived from an architecture developed by J. Ioannidis et al. (1991), for mobile internetworking, relying on mobile support stations within the fixed network to communicate with mobile hosts.<<ETX>>
[Portable computers, fixed network, Wireless application protocol, static view, internetworking, existing data networks, mobile support stations, Postal services, Intelligent networks, File systems, mobile hosts, Distributed databases, network connectivity, Internetworking, multicast messages, protocols, message passing, Multicast protocols, Routing, mobile internetworking, system model, network protocols, Computer science, addressing schemes, multiprogramming, portable computers, mobile destinations]
Termination detection in a very general distributed computing model
[1993] Proceedings. The 13th International Conference on Distributed Computing Systems
None
1993
Termination detection constitutes one of the basic problems of distributed computing, and many distributed algorithms have been proposed to solve it, but all these algorithms consider a very simple model for the underlying application programs: for processes of such programs, nondeterministic constructs are allowed, but each 'receive' statement (request) concerns only one message at a time. A more realistic and very general model of distributed computing is first presented, allowing a request to be atomic on several messages and to obey AND/OR/AND-OR/k-out-of-n/etc. request types. Within this framework, two definitions of termination are proposed and discussed. Then, accordingly, two distributed algorithms for detecting these terminations are presented and evaluated; they differ in the information they use and in the time they need to claim termination.<<ETX>>
[Algorithm design and analysis, atomic request, Computational modeling, termination detection, request types, nondeterministic constructs, Distributed computing, distributed algorithms, general distributed computing model, Distributed control, Broadcasting, application programs, receive statement, messages, Distributed algorithms, Detection algorithms]
Efficient failure discovery with limited authentication
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
Solutions for agreement problems in distributed systems can generally be divided into two classes: authenticated protocols and non-authenticated protocols. Authenticated protocols make use of authenticated messages, i.e., the messages can be signed in a way that a signed message can be assigned unambiguously to the signer. Little has been said about how to achieve this kind of authentication; in some settings this is impossible without a trusted dealer or other mechanisms outside the system. In this paper, we introduce and investigate a weaker kind of authentication, local authentication. It can be achieved within a distributed system with an arbitrary number of arbitrary faults. We then show that Failure Discovery, a problem introduced by Hadzilacos and Halpern, can be solved with authenticated protocols even if only local authentication is available. Since authenticated protocols for this problem have linear message complexity, as opposed to quadratic complexity in the non-authenticated case, the effort of establishing local authentication once results in a substantial reduction of messages in subsequent failure-discovery protocols.
[local authentication, Protocols, authenticated protocols, authenticated messages, distributed processing, communication complexity, failure analysis, nonauthenticated protocols, Authentication, distributed systems, fault tolerant computing, efficient failure discovery, protocols, linear message complexity, failure-discovery protocols, limited authentication]
Fault-tolerant external clock synchronization
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
We address the problem of how to integrate fault-tolerant internal and external clock synchronization. We propose a new algorithm which provides both external and internal clock synchronization for as long as no more than F reference time servers out of a total of 2F+1 are faulty. When the number of faulty reference time servers exceeds F, the algorithm degrades to a fault-tolerant internal clock synchronization algorithm. We prove that at least 2F+1 reference time servers are necessary for achieving external clock synchronization when up to F reference time servers can suffer arbitrary failures, thus our algorithm provides maximum fault-tolerance. The algorithm is also optimal in another sense: we show that the maximum deviation between reference time and the clocks of nonreference time servers is minimal.
[internal clock synchronization, fault-tolerant external clock synchronization, reliability, reference time servers, Synchronization, synchronisation, external clock synchronization, Degradation, clocks, fault-tolerant internal clock synchronization algorithm, Fault tolerance, real-time systems, fault tolerant computing, Clocks]
Performance impact of scheduling discipline on adaptive load sharing in homogeneous distributed systems
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
Load sharing is a technique to improve the performance of distributed systems by distributing the system workload from heavily loaded nodes, where service is poor, to lightly loaded nodes in the system. Previous studies have considered two adaptive load sharing policies: sender-initiated and receiver-initiated. In the sender-initiated policy, a heavily loaded node attempts to transfer work to a lightly loaded node and in the receiver-initiated policy a lightly loaded node attempts to get work from a heavily loaded node. Almost all the previous studies assumed the first-come/first-served node scheduling policy; furthermore, analysis and simulations in these studies have been done under the assumption that the job service times are exponentially distributed and the job arrivals form a Poisson process (i.e., job inter-arrival times are exponentially distributed). The goal of this paper is to fill the void in the existing literature. We study the impact of these assumptions on the performance of the sender-initiated and receiver initiated policies. We consider three node scheduling policies-first-come/first-served (FCFS), shortest job first (SJF), and round robin (RR) policies. Furthermore, we also look at the impact of variance in the inter-arrival times and in the job service times. Our results show that: (i) When non-preemptive node scheduling policies (FCFS and SJF) are used, the receiver-initiated policy is (substantially) more sensitive to variance in inter-arrival times than the sender-initiated policies and the sender-initiated policies are relatively more sensitive to the variance in job service times; (ii) When the preemptive node scheduling policy (RR) is used, the sender-initiated policy provides a better performance than the receiver-initiated policy.
[job service times, inter-arrival times, scheduling discipline, round robin policy, homogeneous distributed systems, adaptive load sharing, first-come/first-served policy, Poisson process, processor scheduling, sender-initiated policy, job inter-arrival times, node scheduling policies, Adaptive scheduling, Analytical models, nonpreemptive node scheduling policies, resource allocation, shortest job first policy, receiver-initiated policy, Round robin]
Guaranteeing end-to-end deadlines in ATM networks
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
We address the issue of guaranteeing the end-to-end deadlines of hard real-time connections in an ATM network. In an ATM network, a set of hard real-time connections can be admitted only if the end-to-end delays of cells belonging to individual connections are not more than their deadlines. We systematically decompose an ATM network into constant delay and variable delay servers to facilitate the delay analysis. Effective traffic description is the key part of such a process. We propose a comprehensive traffic description function that provides adequate information about the worst case traffic behavior of connections anywhere in the network. We also study some simple approximations of this function that perform reasonably well in practice. We analyze and compare the performance of ATM networks with FCFS and WRR link scheduling policies under different loading conditions.
[Telecommunication traffic, link scheduling policies, performance evaluation, asynchronous transfer mode, Network servers, real-time systems, end-to-end deadlines, scheduling, hard real-time connections, traffic description, loading conditions, Performance analysis, ATM networks]
Implementing sequentially consistent shared objects using broadcast and point-to-point communication
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
A distributed algorithm that implements a sequentially consistent collection of shared read/update objects using a combination of broadcast and point-to-point communication is presented and proved correct. This algorithm is a generalization of one used in the Orca shared object system. The algorithm caches objects in the local memory of processors according to application needs; each read operation accesses a single copy of the object, while each update accesses all copies. Copies of all the objects are kept consistent using a strategy based on sequence numbers for broadcasts. The algorithm is presented in two layers. The lower layer uses the given broadcast and point-to-point communication services, plus sequence numbers, to provide a new communication service called a context multicast channel. The higher layer uses a context multicast channel to manage the object replication in a consistent fashion. Both layers and their combination are described and verified formally, using the I/O automaton model for asynchronous concurrent systems.
[Context, Context-aware services, telecommunication services, object replication, context multicast channel, point-to-point communication, broadcast communication, local memory, distributed algorithm, Multicast algorithms, distributed algorithms, asynchronous concurrent system, Automata, sequentially consistent collection, Orca shared object system, Broadcasting, Distributed algorithms, sequentially consistent shared objects]
Hardware for fast global operations on workstation cluster multicomputers
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
The results presented show that the addition of a secondary network with a wide-tree topology and one or more coordination processors (COPs) to a workstation cluster multicomputer can speed up synchronization, broadcasting, reduction, and several other important global operations by two or three orders of magnitude. The wide tree topology contributes to this by allowing a large number of nodes to simultaneously send commands to a COP and allowing a COP to broadcast a computed value to a large number of nodes simultaneously. Hardware based protection mechanisms in the COP system permit direct user access to the COP network and thus eliminate the overhead of software protocol layers for these global operations. Although the COP system provides a large performance gain for global operations, it increases overall system cost by only 2-3%.
[Costs, wide tree topology, hardware based protection mechanisms, Access protocols, Performance gain, performance evaluation, local area networks, workstation cluster multicomputers, software protocol layers, secondary network, broadcasting, workstations, Network topology, Broadcasting, shared memory systems, synchronization, Hardware, Workstations, coordination processors, reduction, Protection, wide-tree topology, direct user access, fast global operations]
Configuration-level optimization of RPC-based distributed programs
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
Many strategies for improving performance of distributed programs can be described abstractly in terms of an application's overall configuration. But previously those techniques would need to be implemented manually, and the resulting programs, though yielding good performance, are more expensive to build and much less easy to reuse. This paper describes research towards an automatic system for introducing performance improvement techniques based upon an application's configuration description.
[Sequences, Costs, distributed processing, Educational institutions, Programming profession, configuration-level optimization, Computer science, Computer languages, optimisation, performance, Employment, DNA, Writing, remote procedure calls, Functional programming, software performance evaluation, RPC-based distributed programs]
A distributed table-driven route selection scheme for establishing real-time video channels
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
To guarantee the delivery of real-time messages before their deadline, a real-time connection or channel must be established before the transmission of any real-time messages. During this channel-establishment phase, one must first select a route between the source and destination of this channel and then reserve sufficient resources along this route so that the worst-case end-to-end delay over the selected route may not exceed the user-specified delay bound. We propose a table-driven distributed route-selection scheme that is guaranteed to find a "qualified" route, if any, that meets the performance requirement of the requested channel without compromising any of the existing guarantees. The proposed scheme uses the Bellman-Ford shortest path algorithm to build real-time delay tables, and hence, can solve the route-selection problem by a simple table look-up. Several examples are presented to demonstrate the effectiveness of the proposed distributed route-selection scheme.
[table-driven distributed route-selection scheme, performance evaluation, distributed table-driven route selection scheme, worst-case end-to-end delay, real-time delay tables, distributed route-selection scheme, Delay, channel-establishment phase, real-time systems, telecommunication network routing, real-time messages, interactive television, user-specified delay bound, real-time video channels, Bellman-Ford shortest path algorithm, multimedia communication]
Characterizing and detecting the set of global states seen by all observers of a distributed computation
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
A consistent observation of a given distributed computation is a sequence of global states that could be produced by executing that computation on a monoprocessor system. Therefore a distributed execution generally accepts several consistent observations. This paper concentrates on what all these observations have in common. An abstraction called common global state is defined. A necessary and sufficient condition characterizing such states is given. A monitor-based algorithm that detects them is also presented and proved correct. Previous works on detection of unstable properties of distributed computations are revisited and explained with this abstraction. Moreover other uses of such particular states are sketched.
[Sufficient conditions, monoprocessor system, common global state, distributed execution, distributed processing, distributed computation, global states detection, necessary and sufficient condition, Distributed computing, formal specification, monitor-based algorithm, unstable properties]
Test sequence generation from formal specifications of distributed programs
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
An abstract program is a formal specification that describes the valid behavior of a distributed program without describing particular implementation mechanisms that achieve this behavior. Valid behavior can be modeled as the possible sequences of events that may be observed of a conforming concrete implementation of the abstract program. In this paper, we address the problem of how to select event sequences from an abstract program to test its concrete implementation. Sequencing constraints make explicit certain types of required properties that are expressed only implicitly by an abstract program. The sequencing constraints derived from an abstract program can be used to guide the selection of event sequences during testing. We describe a constraint notation called CSPE and show how to achieve coverage and detect violations of abstract CSPE constraints. Abstract constraints address the problem of how to compare two programs written at different levels of abstraction. Results of an empirical study of CSPE-based testing are reported.
[program testing, abstract program, test sequence generation, possible sequences, Formal specifications, event sequences, constraint notation, formal specification, formal specifications, distributed programs, Concrete, CSPE, conforming concrete implementation, Testing]
A distributed K-mutual exclusion algorithm
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
This paper presents a token-based K-mutual exclusion algorithm. The algorithm uses K tokens and a dynamic forest structure for each token. This structure is used to forward token requests. The algorithm is expected to minimize the number of messages and also the delay in entering the critical section, at low as well as high loads. The paper presents simulation results for the proposed algorithm and compares them with three other algorithms. Unlike previous work, our simulation model assumes that a finite (non-zero) overhead is encountered when a message is sent or received. The simulation results show that, as compared to other algorithms, the proposed algorithm achieves lower delay in entering critical section as well as lower number of messages, without a significant increase in the size of the messages.
[delay, distributed algorithms, simulation results, dynamic forest structure, digital simulation, token networks, Delay, token-based algorithm, distributed K-mutual exclusion algorithm]
Parallel processing on networks of workstations: a fault-tolerant, high performance approach
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
One of the most sought after software innovation of this decade is the construction of systems using off-the-shelf-workstations that actually deliver and even surpass, the power and reliability of supercomputers. Using completely novel techniques: eager scheduling, evasive memory layouts and dispersed data management it is possible to build an execution environment for parallel programs on workstation networks. These techniques were originally developed in a theoretical framework for an abstract machine which models a shared memory asynchronous multiprocessor. The network of workstations platform presents an inherently asynchronous environment for the execution of our parallel program. This gives rise to substantial problems of correctness of the computation and of proper automatic load balancing of the work amongst the processors, so that a slow processor will not hold up the total computation. A limiting case of asynchrony is when a processor becomes infinitely slow, i.e. fails. Our methodology copes with all these problems, as well as with memory failures. An interesting feature of this system is that it is neither a fault-tolerant system extended for parallel processing nor is it parallel processing system extended for fault tolerance. The same novel mechanisms ensure both properties.
[high performance approach, memory failures, Environmental management, parallel processing, parallel programming, parallel programs, abstract machine, shared memory asynchronous multiprocessor, Fault tolerance, execution environment, Fault tolerant systems, Parallel processing, scheduling, Workstations, networks of workstations, Technological innovation, dispersed data management, correctness, Power system management, eager scheduling, performance evaluation, automatic load balancing, Supercomputers, fault-tolerant approach, Memory management, evasive memory layouts, fault tolerant computing, Power system reliability]
Newtop: a fault-tolerant group communication protocol
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
A general purpose group communication protocol suite called Newtop is described. It is assumed that processes can simultaneously belong to many groups, group size could be large, and processes could be communicating over the Internet. Asynchronous communication environment is therefore assumed where message transmission times cannot be accurately estimated, and the underlying network may well get partitioned, preventing functioning processes from communicating with each other. Newtop can provide causality preserving total order delivery to members of a group, ensuring that total order delivery is preserved for multi-group processes. Both symmetric and asymmetric order protocols are supported, permitting a process to use say symmetric version in one group and asymmetric version in other.
[Fault tolerance, Asynchronous communication, Protocols, fault-tolerant group communication protocol, Newtop, fault tolerant computing, Internet, asynchronous communication environment, protocols, message transmission times, total order delivery]
A new protocol for bandwidth regulation of real-time traffic classes in internetworks
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
A novel bandwidth regulation mechanism is proposed which improves the ability of a packet-switching network to cope with multiple real-time and non-real-time traffic classes. The mechanism achieves regulation of link bandwidth at two levels. At the first level, bandwidth is dynamically regulated between different traffic classes. The concept of 'inter-class regulation' is introduced which enforces that the bandwidth left unused by a traffic class is divided among traffic classes with high bandwidth demands. At the second level, bandwidth regulation is enforced on end-to-end traffic streams, so-called flows, such that flows from the same class with identical routes have the same throughput constraints. This concept is referred to as 'intra-class regulation'. A simple distributed protocol is presented that achieves a intra-class and inter-class regulation in a general internetwork. The effectiveness of the protocol is demonstrated by simulation experiments.
[Protocols, internetworking, Telecommunication traffic, Throughput, intra-class regulation, distributed protocol, end-to-end traffic streams, internetworks, transport protocols, network operating systems, real-time systems, Bandwidth, bandwidth regulation protocol, inter-class regulation, real-time traffic classes, packet-switching network, Internet]
Adaptive placement of method executions within a customizable distributed object-based runtime system: design, implementation and performance
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
This paper presents the design and implementation of a mechanism aimed at enhancing the performance of distributed object-based applications. This goal as achieved by means of a new algorithm implementing placement of method executions that adapts to processors' load and to objects' characteristics, the latter allowing to approximate the cost of methods' remote execution. The behavior of the proposed placement algorithm is examined by providing performance measures obtained from its integration within a customizable distributed object-based runtime system. In particular, the cost of method executions using our algorithm is compared with the cost resulting from the standard placement technique that consists of executing any method on the storing node of its embedding object.
[Costs, object-oriented programming, performance, method execution, placement algorithm, distributed processing, distributed object-based runtime system, embedding, object-based applications, placement technique]
MASSIVE: a distributed virtual reality system incorporating spatial trading
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
MASSIVE is a distributed virtual reality system. It provides rich facilities to support user interaction and cooperation via text, audio and graphics media and interaction is controlled by a spatial model of interaction. The communications architecture is based on processes communicating via typed connections which have interfaces on both ends and which integrate RPCs, attributes and streams in a common context. A spatial interface trading service, the aura manager, has been developed to support interface trading in a spatial context. The concepts embodied in the aura manager can be useful in other interface trading situations, especially where notions of "space" and "meeting" may be applied.
[Context, Context-aware services, virtual reality, Communication system control, MASSIVE, typed connections, streams, distributed processing, distributed virtual reality system, user interaction, spatial interface trading service, Graphics, aura manager, Virtual reality, groupware, Streaming media, communications architecture, remote procedure calls, attributes, naming services]
An object-based distributed computing environment based on a reflective architecture
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
This paper presents an object-based distributed computing environment based on a reflective architecture for industrial large-scale distributed systems. This distributed computing environment uses a compiler-based reflection technique to realize industrial distributed systems with standard workstations. A multiple-world model is also presented, in which a distributed system consists of hierarchical worlds that contain related objects. The distributed computing environment, based on the reflective architecture and the multiple-world model, provides object management and communication management functions, such as prioritized communications, high availability and reliability, nonstop maintenance and extension, and hierarchical transparency.
[Availability, object-oriented programming, communication management functions, prioritized communications, reliability, hierarchical transparency, distributed processing, reflective architecture, multiple-world model, Reflection, nonstop maintenance, Maintenance, object management, Distributed computing, Environmental management, industrial large-scale distributed systems, high availability, object-based distributed computing environment, Computer architecture, Computer industry, Large-scale systems, Workstations]
A tool for monitoring software-heterogeneous distributed object applications
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
The next decade will bring radical changes to the way we do information processing, as applications composed of many cooperating distributed subsystems, exploiting different computational paradigms, become more common. This situation increases substantially demands in the area of system management, correctness analysis, understanding, debugging and performance evaluation. Managed Object-based Distributed Monitoring System (MODIMOS) is a project aimed at development of an adaptable platform for visualization of distributed applications, built of interoperating heterogeneous components. MODIMOS is expandable, allowing to add new monitored environments. It employs various management mechanisms to cope with gathered information size and complexity.
[Visualization, object-oriented programming, open systems, cooperating distributed subsystems, Project management, system management, Debugging, distributed processing, correctness analysis, performance evaluation, Application software, Distributed computing, computational paradigms, supervisory programs, software-heterogeneous distributed object applications, Information processing, system monitoring, understanding, debugging, interoperating heterogeneous components, Performance analysis, software tools, Monitoring]
Techniques for global optimization of message passing communication on unreliable networks
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
In this paper, we present techniques to improve the performance of parallel and distributed applications running on distributed systems built with unreliable local-area networks. The optimizations tailor the message passing system used by the application to the communication pattern exhibited by the application. The optimizations are global and application dependent since communication patterns vary from application to application. The techniques improve both the execution times and scalability of many parallel applications as well as distributed system services.
[message passing, Scalability, performance evaluation, local area networks, message passing system, global optimization, scalability, performance, Message passing, execution times, unreliable networks, distributed systems, communication pattern, local-area networks, message passing communication, Local area networks]
Distributed algorithms for detecting conjunctive predicates
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
This paper discusses efficient distributed detection of global conjunctive predicates in a distributed program. Our methods correctly detect the first consistent cut in which the predicate is true, even if the predicate is unstable. Previous work in detection of such predicates is based on a centralized checker process. In this paper we introduce algorithms which distribute the computation and space requirements of the detection procedure. Two algorithms are presented. The first algorithm requires O (n/sup 2/ m) time and space where m is the number of messages sent by any process and n is the number of processes over which the predicate is defined. This algorithm has identical time complexity to the original centralized algorithm. However computation, space and message requirements are distributed evenly over the n processes. The second algorithm requires O(Nm) total work, where N is the total number of processes in the system. The relative values of n and N determine which algorithm is more efficient for a specific application. Parallelism can be introduced into either distributed algorithm, reducing the average case time complexity. We show that the worst-case time complexity can not be improved beyond O(mn) with any on-line detection algorithm.
[time complexity, centralized checker process, Distributed computing, message requirements, distributed detection, distributed algorithms, Parallel processing, global conjunctive predicates, conjunctive predicates detection, distributed program, Distributed algorithms, Detection algorithms, computational complexity]
Distributed pipeline scheduling: end-to-end analysis of heterogeneous, multi-resource real-time systems
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
This paper presents an hierarchical end-to-end analysis technique that decomposes the very complex heterogeneous multi-resource scheduling problem into a set of single resource scheduling problems with well defined interactions. We define heterogeneity both in resource types, e.g., CPU, and in scheduling policies, e.g., rate-monotonic scheduling. This analysis technique is one phase of our systems integration framework for designing large-scale, heterogeneous, distributed real-time systems whose timing properties can be strictly controlled and analyzed. This approach, denoted the Distributed Pipelining Framework, exploits the natural pipelining execution pattern found in a large number of continuous (periodic) applications executing over heterogenous resources. A teleconference application is used in this paper to show the utility of the approach.
[Real time systems, Control system analysis, rate-monotonic scheduling, Control systems, Large scale integration, large-scale heterogeneous distributed real-time systems, processor scheduling, Pipeline processing, natural pipelining execution pattern, Teleconferencing, resource allocation, real-time systems, single resource scheduling problems, teleconference application, distributed pipeline scheduling, Timing, pipeline processing, heterogeneous multi-resource real-time systems, hierarchical end-to-end analysis technique]
"Computer Supported Cooperative Work - New Challenges or Old Problems?"
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
false
[Collaborative work]
Message logging: pessimistic, optimistic, and causal
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
Message logging protocols are an integral part of a technique for implementing processes that can recover from crash failures. All message logging protocols require that, when recovery is complete, there be no orphan processes, which are surviving processes whose states are inconsistent with the recovered state of a crashed process. We give a precise specification of the consistency property "no orphan processes". From this specification, we describe how different existing classes of message logging protocols (namely optimistic, pessimistic, and a class that we call causal) implement this property. We then propose a set of metrics to evaluate the performance of message logging protocols, and characterize the protocols that are optimal with respect to these metrics. Finally, starting from a protocol that relies on causal delivery order, we show how to derive optimal causal protocols that tolerate f overlapping failures and recoveries for a parameter f:1/spl les/f/spl les/n.
[Protocols, message passing, overlapping failures, message logging protocols, specification, optimal causal protocols, Computer crashes, protocols, formal specification, system recovery, consistency property]
Distributed implementation of multi-rendezvous in LOTOS using the orthogonal communication structure in Linda
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
We argue that the programming simplicity and message passing complexity of implementing LOTOS multi-rendezvous largely depends on the underlying communication structure. The programming simplicity and low complexity of our algorithm are due to our viewing the multi-rendezvous problem as a distributed mutual exclusion problem. Using the parallel programming environment of Linda, we present an efficient algorithm to easily implement multi-rendezvous in LOTOS. The orthogonal communication structure in Linda allows us to separate synchronization concerns in multi-rendezvous from concerns in interprocess communication in an implementation. The concept, of Table Space in Linda and the associated in, out, and rd primitives for managing the tuple space lead to an elegant programming style for distributed implementation of multi-rendezvous.
[low complexity, LOTOS, message passing, multi-rendezvous, distributed mutual exclusion problem, parallel programming, Linda, Table Space, Parallel programming, Message passing, tuple space, interprocess communication, distributed implementation, specification languages, multi-rendezvous problem, orthogonal communication structure, parallel programming environment, programming simplicity, protocols, underlying communication structure, synchronization concerns]
Maintaining consistency of data in mobile distributed environments
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
To deal with the frequent, foreseeable and variable disconnections that occur in a mobile environment, we introduce a flexible, two-level consistency model. Semantically related or closely located data are grouped together to form a cluster. While all data inside a cluster are mutually consistent, degrees of inconsistency are allowed among data at different clusters. To take advantage of the predictability of disconnections, and to accommodate mobility, the cluster configuration is dynamic. We allow transactions to exhibit certain degrees of tolerance for inconsistencies by introducing strict and weak operations. Weak operations are operations that can be executed under weaker consistency requirements. We define correctness criteria for schedules that involve weak operations and compare them with traditional serializability criteria. Finally, we argue that our model is appropriate for a variety of other environments including very large databases and multidatabases.
[correctness criteria, Databases, very large databases, mobile distributed environments, two-level consistency model, data integrity, cluster configuration, data consistency]
Probing and fault injection of protocol implementations
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
Ensuring that a distributed system with strict dependability constraints meets its prescribed specification is a growing challenge that confronts software developers and system engineers. This paper presents a technique for probing and fault injection of fault-tolerant distributed protocols. The proposed technique, called script-driven probing and fault injection, can be used for studying the behavior of distributed systems and for detecting design and implementation errors of fault-tolerant protocols. The focus of this work is on fault injection techniques that can be used to demonstrate three aspects of a target protocol: i) detection of design or implementation errors, ii) identification of violations of protocol specifications, and iii) insight into design decisions made by the implementers. The emphasis of our approach is on experimental techniques intended to identify specific "problems" in a protocol or its implementation rather than the evaluation of system dependability through statistical metrics such as fault coverage. To demonstrate the capabilities of this technique, the paper describes a probing and fault injection tool, called the PFI tool (Probe/Fault Injection Tool), and a summary of several extensive experiments that studied the behavior of two protocols: the transmission control protocol (TCP) and a group membership protocol (GMP).
[Protocols, system dependability, software developers, fault-tolerant distributed protocols, group membership protocol, formal specification, Fault diagnosis, design decisions, Fault tolerance, Fault tolerant systems, PFI tool, fault injection, protocols, protocol implementations, Probes, system engineers, script-driven probing, statistical metrics, Transmission Control Protocol, Fault detection, protocol specifications, Software systems, Systems engineering and theory, fault tolerant computing, software metrics]
Single connection emulation (SCE): an architecture for providing a reliable multicast transport service
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
We present a novel architecture for providing a reliable multicast transport service over existing protocol stacks. These protocol stacks ordinarily support reliable unicast transport layer connections over a network layer which is capable of providing an unreliable multicasting service. We propose the addition of a new single connection emulation (SCE) sublayer between the unicast transport layer and the multicast network layer. This added layer mimics the single destination network layer interface to the transport layer and interfaces with the multicast network layer to provide the necessary multicast functionality. The new architecture also enables interactions between applications and the SCE, thus allowing the applications to control the semantics of the reliable multicast connection. We discuss the design issues that need to be considered when such a sublayer is to be introduced. We also discuss an implementation of this new approach using the TCP/IP protocol stack and present some preliminary experimental results.
[Transport protocols, reliable multicast transport service, single connection emulation, internetworking, Multicast protocols, semantics, multicast functionality, TCP/IP protocol stack, protocol stacks, Unicast, transport protocols, Emulation, TCPIP, reliable unicast transport layer connections]
Causal separators for large-scale multicast communication
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
In recent years there has been a growing interest in developing communication systems that are able to deliver messages respecting potential causality. Unfortunately, causal delivery cannot be provided without costs: extra delays may be induced on message delivery or processes may be required to maintain and exchange records of causal relations. In this paper we present an extension to previous work on compression of causal information using knowledge about the topology of the communication structure. In order to make practical use of this result, we present a methodology to model the communication system. The technique exploits the physical structure of existing networks, in particular its hierarchical nature, to create a communication graph where causal separators match the underlying physical and administrative organization. We show that this approach can be applied to existing large-scale systems, providing the means for using topological timestamping with negligible overhead.
[Costs, Particle separators, communication system, large-scale multicast communication, causal separators, Multicast communication, distributed processing, Delay, topological timestamping, Network topology, causal delivery, delays, Large-scale systems]
Write caching in distributed file systems
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
Disk caches are employed in distributed file systems to avoid network accesses at clients and to compensate for the speed differential between main memory and disk at file servers. Because of concerns about volatility, however, write requests have typically not benefitted from the presence of caches. Instead, they have been processed with some sort of write-through or periodic write-back approach to ensure the integrity of the stored data. The introduction of reasonably priced non-volatile (NV) memories has prompted interest in the use of such memory for write caching, at the server and/or at the client. This paper describes an investigation through trace-driven simulation experiments of several approaches to write caching in distributed systems, with both volatile and non-volatile caches. The results support the findings of earlier work that suggests important differences between caching in the traditional single-level caching environment and caching in a two-level caching environment. While policies focusing on temporal locality perform well for a single-level caching system, or at the client of a two-level caching system, they may not be suitable for use at the server in a two-level caching system. This is because locality characteristics in the reference stream seen at the server in a two-level caching system may be destroyed by caching at the client with a NV write cache large enough to hold the client's working set of dirty blocks. Policies focusing on amortizing the cost of a disk seek operation over multiple write-back operations perform better at the server of a two-level caching system.
[Costs, locality characteristics, disk seek operation, temporal locality, distributed file systems, multiple write-back operations, File servers, cache storage, data integrity, write caching, trace-driven simulation, network accesses, File systems, Nonvolatile memory, network operating systems, file servers, distributed databases]
Mapping concurrently-communicating modules onto mesh multicomputers equipped with virtual channels
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
It is difficult to define and evaluate a meaningful performance metric when many packets are generated and exchanged concurrently in mesh-connected multicomputers equipped with wormhole switching and virtual channels. Thus, an approximate metric/cost function must be chosen so that when, task modules are mapped by optimizing this function, the actual performance of the mapping is also optimized. Several low-complexity cost functions are evaluated using the simulated annealing optimization process. The mappings found by optimizing these cost functions are then fed into a flit-level simulator to evaluate their actual performance. One particular cost function is found to be very effective.
[Measurement, flit-level simulator, task modules, low-complexity cost functions, Packet switching, multiprocessing systems, simulated annealing, concurrently-communicating modules, multiprocessor interconnection networks, virtual channels, simulated annealing optimization process, performance evaluation, wormhole switching, performance metric, mesh multicomputers, Simulated annealing, cost function, Cost function]
Selective broadcast data distribution systems
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
This paper describes a two tier architecture for high speed data distribution. The architecture consists of a database interface network which distributes information from a central database to a number of servers, and a user interface network which distributes information from the servers to the user terminals. The database interface network uses the selective broadcast technique to distribute data on a high speed channel. Data requested by users are filtered out by the servers and sent to the user terminals through the user interface network. The user interface network can be any conventional local area network for connecting the servers and the user terminals. A very tight upper bound on the mean response time of the system for uniform request distribution is first derived. This is followed by an approximate analysis for general request distributions. Simulation results and design examples showed that selective broadcast technique can provide an order of magnitude smaller response time under normal traffic conditions when compared to the nonselective broadcast technique such as the Datacycle system.
[two tier architecture, local area networks, user terminals, Delay, mean response time, Network servers, selective broadcast data distribution systems, general request distributions, Databases, servers, tight upper bound, Broadcasting, Traffic control, central database, selective broadcast technique, Local area networks, database interface network, Datacycle system, Upper bound, user interface network, local area network, simulation results, User interfaces, data communication, Joining processes, approximate analysis]
Analysis of resource lower bounds in real-time applications
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
Tasks in a real-time application usually have several stringent timing, resource, and communication requirements. Designing a distributed computing system which can meet all these requirements is a challenging problem. In this paper, we alleviate this problem by proposing a technique to determine a lower bound on the number of processors and resources required to meet the constraints of the application. We also extend the technique to estimate the cost of a system which meets all the application constraints. The proposed technique deals with most constraints found in real-time applications including deadlines, release times, resource requirements, precedence relationships, and non-zero communication times. It also derives these bounds for two different models of distributed systems.
[Costs, systems software, communication times, resource requirements, distributed processing, resource lower bounds, processors, Distributed computing, deadlines, processor scheduling, distributed computing system, release times, real-time applications, resource allocation, real-time systems, system monitoring, Timing, precedence relationships]
Dynamic techniques for minimizing the intrusive effect of monitoring actions
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
Programs written in distributed programming languages are often non-deterministic in nature and hence the execution of such programs is not only dependent upon the program input, but also on the timing of the execution. Thus, an attempt to monitor the run-time behavior of a non-deterministic program through code instrumentation, such as during debugging, can potentially alter the program's behavior. In this paper we present techniques for dynamically minimizing the intrusive effects of monitoring by attempting to ensure that the likelihoods of various outcomes of the non-deterministic events are the same for uninstrumented and instrumented programs. This goal is achieved by minimizing the intrusive effects of monitoring on message passing between processes located at different sites of a distributed system. Monitoring actions alter the message pool at the time of non-deterministic message selection and the order in which the messages arrive at a processor. The dynamic techniques presented in this paper minimize intrusion by restoring the message pool and message ordering.
[monitoring actions, program debugging, message passing, Instruments, dynamic techniques, run-time behavior, Debugging, message pool, distributed programming languages, Computer languages, Runtime, code instrumentation, Message passing, system monitoring, debugging, message ordering, Timing, intrusive effect minimisation, Monitoring]
A fast distributed modular algorithm for resource allocation
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
This paper concerns resource allocation in distributed message passing systems, i.e., the scheduling of accesses to system resources shared among many concurrent processes. Three different kinds of resource allocation problems with varying degrees of generality are considered: the dining philosophers problem, the drinking philosophers problem and the dynamic resource allocation problem. We present an efficient modular resource allocation algorithm that uses any arbitrary resource allocation algorithm as a subroutine. It improves the performance of the subroutine by letting each process wait only for its currently conflicting processes, and therefore, allows more concurrency. For appropriate choices of the subroutine, we obtain the fastest known resource allocation algorithms in terms of the worst case response time. Simulation studies were conducted which also indicate that our algorithms perform faster and require a smaller number of messages than other previously known algorithms on average, especially when resource contention among processes is high and the average time that a process remains in the critical region is large.
[message passing, resource contention, fast distributed modular algorithm, digital simulation, distributed message passing systems, drinking philosophers problem, system recovery, Delay, dining philosophers problem, Concurrent computing, simulation studies, dynamic resource allocation problem, Algorithms, resource allocation, performance, Message passing, concurrency control, scheduling, Resource management]
Constructing a configurable group RPC service
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
Current Remote Procedure Call (RPC) services implement a variety of semantics, with many of the differences related to how communication and server failures are handled. The list increases even more when considering group RPC, a variant of RPC often used for fault-tolerance where an invocation is sent to a group of servers rather than one. This paper presents an approach to constructing group RPC in which a single configurable system is used to build different variants of the service. The approach is based on implementing each property as a separate software module called a micro-protocol, and then configuring the micro-protocols needed to implement the desired service together using a software framework based on the x-kernel. The properties of point-to-point and group RPC are identified and classified, and the general execution model described. An example consisting of a modular implementation of a group RPC service is given to illustrate the approach. Dependency issues that restrict configurability are also addressed.
[micro-protocol, configurable system, fault-tolerance, dependency, group RPC, distributed processing, Fault tolerance, reconfigurable architectures, remote procedure calls, configurability, protocols, RPC service, configurable]
Real-time causal message ordering in multimedia systems
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
In multimedia systems, not only do messages that are sent to and received by multiple sites need to have a consistent order imposed by all sites, but cause and effect relations must be maintained. Causal ordering allows the cause and effect relations of messages to be maintained. This paper presents an algorithm that insures that multimedia data with real-time deadlines are delivered to the application layer in causal order. The algorithm is designed to insure that any message that arrives at a destination site before its deadline will be delivered to the application before the message expires. In addition, by focusing on a form of causal ordering violations caused by "the triangle inequality," this algorithm has a low overhead with respect to the amount of information that must be appended to each message.
[Real time systems, Algorithm design and analysis, real-time deadlines, Multimedia systems, causal ordering violations, real-time systems, real-time causal message ordering, triangle inequality, delta-causality, scheduling, multimedia systems, multimedia communication]
A multiversion concurrent object model for distributed and multiuser environments
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
In this paper, we describe the design and implementation of an extended concurrent object model for distributed and multiuser systems called the Multiversion Concurrent Object (MCO). In this model, an object executes multiple methods concurrently within itself, but is handled as a concurrent object. Each MCO makes a copy of its own state, called a version, for executing a method that is not interleaved with other similar copies during execution. The results of concurrent execution are merged at termination of each method. The MCO increases programmability, because it does not require any synchronization statements inside its methods. Using this model for shared objects, we can construct a system in which each user can work freely without being negatively impacted by others. Finally, a prototype implementation of MCO and performance evaluations are presented.
[multiuser environments, multiprocessing systems, multiprocessing programs, Prototypes, concurrent execution, distributed processing, performance evaluation, distributed systems, multiversion concurrent object model, programmability]
Integrating visualization support into distributed computing systems
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
Visualization and animation tools may become extremely important aids in the understanding, verification, and performance tuning of parallel computations. Presently, however, the use of visualization has had only a limited use for enhancing parallel computation. We hypothesize that one of the primary reasons for the limited use of visualization tools in parallel program development is the difficulty of acquiring the information necessary to drive the visual display. Our approach to this impediment focuses on integrating visualization support directly into a distributed computing system. Central to this integration is the addition of a logical clock that prevents the timestamps of events from violating causality. The implementation requires the "piggybacking" of a negligible amount of extra header information on system messages and the impact on performance is minimal. This results in a system that produces useful visualizations with no extra effort required by the applications programmer. Also integrated into the distributed system is support which simplifies the creation of programmer-defined, application-specific visualizations, unique to each new parallel program developed.
[Visualization, parallel computation, programmer-defined application-specific visualizations, program diagnostics, Drives, distributed processing, Displays, visualization support, piggybacking, Distributed computing, Programming profession, parallel programming, parallel program development, distributed computing system, Concurrent computing, distributed computing systems, Animation, software tools, Impedance, Clocks]
A thread-based interface for collective communication on ATM networks
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
This paper presents the results of an investigation of collective communication operations for distributed computing across asynchronous transfer mode (ATM) networks. Several collective operations have been implemented and studied on a three-switch ATM network testbed at Michigan State University. The methods use virtual topologies constructed from ATM virtual channels. A particular type of virtual topology is described that efficiently implements several collective operations through the use of hardware-supported ATM multicast channels. Performance measurements are presented that illustrate how a thread-based software design can take advantage of such underlying hardware features.
[Measurement, message passing, multicast channels, software design, virtual channels, collective communication, asynchronous transfer mode, Distributed computing, distributed computing, asynchronous transfer mode networks, performance measurements, Software design, Network topology, thread-based interface, hardware features, Hardware, virtual topologies, Asynchronous transfer mode, Testing, ATM networks]
Software tool evaluation methodology
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
The recent development of parallel and distributed computing software has introduced a variety of software tools that support several programming paradigms and languages. This variety of tools makes the selection of the best tool to run a given class of applications on a parallel or distributed system a non-trivial task that requires some investigation. We expect tool evaluation to receive more attention as the deployment and usage of distributed systems increases. In this paper, we present a multi-level evaluation methodology for parallel/distributed tools in which tools are evaluated from different perspectives. We apply our evaluation methodology to three message passing tools viz Express, p4, and PVM. The approach covers several important distributed systems platforms consisting of different computers (e.g., IBM-SP1, Alpha cluster, SUN workstations) interconnected by different types of networks (e.g., Ethernet, FDDI, ATM).
[distributed computing software, Ethernet networks, p4, PVM, Distributed computing, parallel programming, software tool evaluation methodology, Computer networks, Workstations, software tools, programming paradigms, multi-level evaluation methodology, software performance evaluation, message passing, IBM-SP1, FDDI, Express, Application software, Sun, distributed systems platforms, Alpha cluster, Parallel programming, Message passing, message passing tools, Ethernet, ATM, Software tools, SUN workstations, parallel computing software]
Server recovery using naturally replicated state: a case study
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
This paper describes design and preliminary measurements of a file server recovery scheme that uses naturally replicated state among clients. This scheme, implemented in the Calypso file system, is truly transparent to the user and avoids the overhead of explicit replication. A three-phase protocol reconstructs the server state either on a backup node (if disks are multi-ported) or on the rebooted server node. Measurements show that the recovery time is about 21 seconds for a busy 10-node cluster. However, the time to rebuild the distributed state is only about 1.5 seconds, and most of the recovery time is spent in replaying the write-ahead log of the underlying file system. Fortunately, the log redo time is bounded by the log size.
[write-ahead log, client-server systems, Protocols, Calypso file system, rebooted server node, File servers, Time measurement, three-phase protocol, system recovery, software fault tolerance, log redo time, File systems, server recovery, file servers, explicit replication, underlying file system, naturally replicated state, computer network reliability, protocols]
Comparing kernel-space and user-space communication protocols on Amoeba
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
Most distributed systems contain protocols for reliable communication, which are implemented either in the microkernel or in user space. In the latter case, the microkernel provides only low-level, unreliable primitives and the higher-level protocols are implemented as a library in user space. This approach is more flexible but potentially less efficient. We study the impact on performance of this choice for RPC and group communication protocols on Amoeba. An important goal in this paper is to look at overall system performance. For this purpose, we use several (communication-intensive) parallel applications written in Orca. We look at two implementations of Orca on Amoeba, one using Amoeba's kernel-space protocols and one using user-space protocols built on top of Amoeba's low-level FLIP protocol. The results show that comparable performance can be obtained with user-space protocols.
[Protocols, RPC, Orca, microkernel, group communication protocols, System performance, kernel-space communication protocols, higher-level protocols, FLIP protocol, remote procedure calls, distributed systems, Libraries, reliable communication, protocols, user-space communication protocols, user space, Amoeba]
Specification of a secured multi-server MMS protocol
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
We present a complete specification of a new architecture for the Manufacturing Message Specification (MMS) protocol. This architecture is based on a client/multi-server model adopted by the MMS standard. A client invokes a service request that implies the cooperation of many servers to execute the requested service and satisfies the client demand. This architecture permits the definition of distributed objects, each of them is composed of simple MMS objects distributed over many servers. The coordination of the MMS services execution across the distributed cooperated servers as well as the coherence of the distributed object are guaranteed by the services and the protocol offered by the ISO Commitment, Concurrency, and Recovery (CCR) application service element. The specification of the secured MMS multi-server protocol is introduced by a finite state machine model. This model is used as a base for a formal validation using the Calculus of Communicating Systems (CCS).
[Protocols, finite state machine model, distributed processing, Calculus, finite state machines, formal specification, Concurrent computing, formal verification, Manufacturing, Carbon capture and storage, protocols, computer integrated manufacturing, Manufacturing Message Specification protocol, ISO, ISO Commitment Concurrency and Recovery application service element, formal validation, calculus of communicating systems, Calculus of Communicating Systems, distributed object, secured multi-server MMS protocol, Automata, Coherence, distributed objects, distributed cooperated servers, CCR application service element]
Specifying weak sets
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
We present formal specifications of a new abstraction, weak sets, which can be used to alleviate high latencies when retrieving data from a wide-area information system like the World Wide Web. In the presence of failures, concurrency, and distribution, clients performing queries may observe behavior that is inconsistent with the stringent semantic requirements of mathematical sets. For example, an element retrieved and returned to the client may be subsequently deleted before the query terminates. We chose to specify formally the behavior of weak sets because we wanted to understand the varying degrees of inconsistency clients might be willing to tolerate and to understand the trade off between providing strong consistency guarantees and implementing weak sets efficiently. Our specification assertion language uses a novel construct that lets us model reachability explicitly; with it, we can distinguish between the existence of an object and its accessibility. The specifications were instrumental in understanding the design space, and we are currently implementing the most permissive of the specifications in several types of Unix systems.
[mathematical sets, Instruments, latencies, semantic requirements, abstraction, Unix systems, strong consistency, Information retrieval, World Wide Web, specification assertion language, weak sets, Formal specifications, distribution, formal specification, formal specifications, Delay, Information systems, concurrency, Concurrent computing, Web sites]
Coterie templates: a new quorum construction method
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
One approach to distributed mutual exclusion algorithms is the use of quorums. Quorum-based algorithms offer the advantage of protocol symmetry, spreading effort and responsibility uniformly across the distributed system. In this paper, we present an O(logn) algorithm to generate coterie templates of near-optimal O(n/sup 0.63/) size. Coterie templates are generic quorum structures that exhibit several desirable properties such as fault tolerance, symmetry and low storage cost. In addition, coteries can be instantiated from the template to reflect desirable network characteristics.
[Protocols, Costs, protocol symmetry, fault tolerance, performance evaluation, O(logn) algorithm, quorum construction method, network characteristics, Fault tolerance, coterie templates, distributed mutual exclusion algorithms, distributed algorithms, generic quorum structures, fault tolerant computing, protocols]
An efficient task allocation scheme for two-dimensional mesh-connected systems
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
Efficient allocation of proper size submeshes to incoming tasks in two-dimensional (2D) mesh-connected processors is very important for achieving the desired high performance. It also needs to guarantee the recognition of the free submeshes with minimum overhead. In this paper we present an efficient task allocation scheme for 2D meshes. By employing a new approach for searching the array, our scheme can find the available submesh without the scanning of the entire 2D array unlike earlier designs. As a result, our scheme can significantly reduce the task allocation time. Comprehensive computer simulation reveals that the average allocation time and waiting delay are much smaller than earlier schemes irrespective of the size of meshes. The hardware overhead is comparable to other schemes.
[two-dimensional mesh-connected systems, resource allocation, Computer simulation, Delay effects, mesh-connected processors, hardware overhead, multiprocessor interconnection networks, efficient task allocation scheme, Hardware, task allocation time, waiting delay, processor scheduling]
An efficient optimal reconfiguration algorithm for FBRNs
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
We study a new network architecture called FBRN (FDDI-based reconfigurable network). An FBRN consists of multiple FDDI token rings and has the ability to reconfigure itself in the event of network faults. Thus, an FBRN has the potential to provide high available bandwidth even in the presence of numerous faults. Realization of this potential depends crucially on the choice of reconfiguration algorithm. We design and analyze a distributed reconfiguration algorithm for FBRNs. Our algorithm is optimal in the sense that it always produces a configuration that has the maximum available bandwidth possible for the given fault pattern. This algorithm has polynomial time complexity and hence can be efficiently implemented. We evaluate the algorithm's performance in terms of the available bandwidth in the network as a function of the number of faults.
[Algorithm design and analysis, FDDI, reconfiguration algorithm, optimal reconfiguration algorithm, network architecture, Bandwidth, Polynomials, Token networks, token networks, multiple FDDI token rings, polynomial time complexity, FDDI-based reconfigurable network, computational complexity]
Schedulability-oriented replication of periodic tasks in distributed real-time systems
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
We consider the schedulability-oriented replication problem of a set of periodic real-time tasks where each task can be decomposed into several modules and intermodule communications. The objective is to find an allocation in which there exists a feasible schedule for the given task set. In this paper, we adopt a communication model where the replication of modules is not for the sake of fault tolerance but for increasing the degree of schedulability. To solve the problem, we develop a replication technique and embed the technique in a simulated annealing algorithm. Experimental results show that such replication may lead to a higher degree of schedulability and obtain a feasible solution.
[Fault tolerance, simulated annealing, distributed real-time systems, schedulability-oriented replication, intermodule communications, periodic real-time tasks, simulated annealing algorithm, real-time systems, Simulated annealing, distributed processing, processor scheduling, periodic tasks]
EVEREST: an event recognition testbed
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
Due to their fundamental nature, analyzing the behavior of distributed computations is a complex task. One approach is to monitor the system activity for occurrences of user defined events. As the computation executes, the monitoring system automatically collects and evaluates information pertaining to the defined events, and recognizes event occurrences. The need exists to study various approaches to event recognition in a context in which the strategies can be created and analyzed. This paper presents an EVEnt REcognition teSTbed (EVEREST) specifically designed to address these needs. Commands are provided by which the user can define the behavior to be recognized the configuration of the monitoring system and the strategies to be used in recognizing the event occurrences. Utilizing these directives, EVEREST recognizes occurrences of the defined events during the distributed program's execution. By providing flexible structuring and dynamic configuration of the monitoring system, multiple time view protocols and the separation of event and monitoring specification, EVEREST provides the capability of developing, testing and comparing various approaches to distributed event recognition.
[System testing, Protocols, program testing, Computerized monitoring, configuration, event occurrences, EVEREST, Distributed computing, user defined events, distributed event recognition, event recognition testbed, distributed computations, monitoring system, system monitoring, distributed program's execution]
An efficient distributed deadlock detection algorithm
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
We propose a distributed deadlock detection algorithm which detects cycles made of back edges in a distributed search tree. Our algorithm detects the deadlocks in which the initiator of the algorithm is directly or indirectly involved, while most of the proposed deadlock detection algorithms only detect the cycles in which the initiator is directly involved. Our algorithm is a reduced version of the one proposed previously, which resolves all deadlocks reachable from the initiator in a single execution by propagating the probe messages to the nodes in the search tree and having each node resolve deadlocks in its subtree with collected information on dependency between its subtree nodes. Our algorithm substantially reduces the message overhead of the previous algorithm. We compare the performance of our algorithm with others through extensive simulation. It is found that our algorithm detects deadlocks as fast as the previous algorithm with much fewer messages. The simulation results show that our algorithm outperforms other algorithms considerably in major performance measures.
[directed graphs, concurrency control, distributed search tree, System recovery, distributed processing, message overhead, tree searching, Detection algorithms, Probes, search problems, efficient distributed deadlock detection algorithm, back edges]
Performance evaluation of three logging schemes for a shared-nothing database server
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
The European Declarative System (EDS) is a high performance backend database server designed for a range of commercial mainframes. One major application domain of EDS is information processing for business and commercial environments. High performance is achieved by exploiting parallelism using a shared-nothing computer (up to 256 processors). Reliability is a crucial design issue for commercial and business information systems. Recovery control facilitates reliability and logging forms an important part of it. In general, logging is costly to implement. It is usually achieved at the expense of reduced system performance. Three logging schemes have been studied for EDS: (a) local discs-adopt a conventional approach by incorporating a local disc on each processor; (b) duplexing-arrange the processors in pairs, one for database operations and one for backup; and (c) cooperative logging-similar to duplexing except database and backup operations are performed on a single processor. The performance of these schemes for on-line transaction processing was evaluated and compared using the EDS behavioural simulator. The results of the evaluations are presented in this paper.
[transaction processing, performance evaluation, Transaction databases, Application software, database management systems, shared-nothing database server, European Declarative System, Information systems, Concurrent computing, online transaction processing, logging schemes, recovery control, duplexing, High performance computing, System performance, file servers, Information processing, Parallel processing, design issue, Business]
"Distributed Computing in 2010"
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
false
[Distributed computing]
Exploiting transaction semantics in multidatabase systems
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
Serializability is the traditionally accepted notion of correctness in most database systems. However, in a multidatabase system (MDBS) environment, where a number of pre-existing and autonomous database systems are integrated, requiring serializability could adversely affect the performance of the system. To enhance performance, one of the options is to relax the serializability requirement, and permit certain non-serializable executions. In this paper, we propose a powerful, yet simple mechanism, for specifying the set of non-serializable executions that are unacceptable in an MDBS environment. The undesirable interleavings among transactions are specified using regular expressions over transaction types. The mechanism facilitates the development of efficient graph-based schemes for ensuring that the concurrent execution of transactions meet the specifications. We analyze the complexities of the developed schemes and show that they are easily implementable in an MDBS environment.
[transaction processing, transaction semantics, performance, graph-based schemes, complexities, multidatabase systems, serializability, regular expressions, Interleaved codes, Database systems, database management systems, computational complexity]
Synthesis of protocol entities specifications from service specifications in a Petri net model with registers
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
In general, the services of a distributed system are provided by some cooperative protocol entities. The protocol entities must exchange some data values and synchronization messages in order to ensure the temporal ordering of the events which are described in a service specification of the distributed system. It is desirable that a correct protocol entity specification for each node can be derived automatically from a given service specification. In this paper, we propose an algorithm which synthesizes a correct protocol entity specification automatically from a service specification in a Petri Net model with Registers called PNR model. In our model, parallel events and selective operations can be described naturally. The control flow of a service specification must be described as a free-choice net in order to simplify the derivation algorithm, however, many practical systems can be described in this class. In our approach, since each protocol entity specification is also described in our PNR model, we can easily understand what events can be executed in parallel at each protocol entity.
[free-choice net, Protocols, Petri nets, parallel events, distributed processing, Control systems, protocol entity specification, selective operations, service specification, synchronization messages, PNR model, data values, Petri net model, cooperative protocol entities, distributed system services, Control system synthesis, Automatic control, registers, temporal ordering, protocols, protocol entities specifications, service specifications]
Distributed management by delegation
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
This paper introduces a novel approach to distributed computing based on delegation-agents, and describes its applications to decentralize network management. Delegation agents are programs that can be dispatched to remote processes, dynamically linked and executed under local or remote control. Unlike scripted agents, delegation agent programs may be written in arbitrary languages, interpreted or compiled. They can thus be more broadly applied to handle such tasks as real-time monitoring, analysis and control of network resources. Distributed management by delegation (MbD) uses this to manage remote elements and domains. MbD provides a paradigm for distributed, flexible, scalable and robust network management that overcomes the key limitations of current centralized management schemes.
[virtual reality, distributed processing, remote processes, decentralize network management, Distributed computing, distributed computing, real-time monitoring, computer network management, telecontrol, arbitrary languages, robust network management, distributed management by delegation, Robustness, centralized management schemes, Computer network management, Remote monitoring, remote elements]
Distributed join processing using bipartite graphs
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
Distributed query processing algorithms usually perform data reduction by using a semijoin program but the problem with these approaches is that they still require an explicit join of the reduced relations an the final phase. We introduce an efficient algorithm for join processing in distributed database systems that makes use of bipartite graphs in order to reduce data communication costs and local processing costs. The bipartite graphs represent the tuples that can be joined in two relations taking into account also the reduction state of the relations. This algorithm fully reduces the relations at each site. We then present a partitioning algorithm for response time optimization that takes into account the system configuration, i.e., the additional resources available. We also report on the results of a set of experiments that show that our algorithms outperform a number of the recently proposed methods for total processing time and response time minimization.
[Costs, Minimization methods, total processing time, system configuration, response time optimization, Partitioning algorithms, semijoin program, response time minimization, Delay, distributed database systems, query processing, data reduction, bipartite graphs, Query processing, distributed join processing, distributed databases, Database systems, Bipartite graph, query processing algorithms, Data communication, Time factors, data communication costs]
A practical technique for asynchronous transaction processing
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
Asynchronous transaction processing extends traditional on-line transaction processing (TP) to improve performance of distributed systems by alleviating the serializability (SR) bottleneck. For example, epsilon serializability (ESR) uses divergence control algorithms to allow more concurrency by permitting limited non-SR interleavings. In a distributed environment, ESR relaxes commit and abort dependencies among transactions, allowing transactions to commit asynchronously. A second example, chopping up transactions allows more concurrency by dividing transactions into smaller pieces and thus reduces resource holding time. Chopping transactions enforces no commit protocols among pieces from one original transaction, allowing each piece to commit asynchronously. We combine the benefits of ESR and chopping transactions by designing three new methods that chop transactions and run them under ESR. The practical applicability of our technique is enhanced by two factors: (1) chopping transactions does not require changes in existing TP systems, and (2) ESR support has already been prototyped on a commercial TP system.
[transaction processing, Paramagnetic resonance, Protocols, divergence control algorithms, Design methodology, performance evaluation, distributed processing, epsilon serializability, Concurrent computing, Strontium, performance, asynchronous transaction processing, Prototypes, serializability, Interleaved codes, distributed systems, protocols]
"Intelligent Agents in Distributed Systems"
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
false
[Intelligent agent]
General design of grid-based data replication schemes using graphs and a few rules
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
Grid-based data replication protocols have been proven to be extremely efficient due to the high availability and the low cost of read and write operations they offer. However, although each grid protocol uses logical grids, the semantics and interpreting algorithms vary from scheme to scheme. This has the disadvantage that switching from one protocol to another, results in a re-implementation of the new protocol. By designing a general framework which separates the policy from the mechanism this drawback can be overcome. The purpose of this paper is to present such a general framework for grid-based protocols. We show how to model various popular grid protocols as instances of the framework, thereby demonstrating that grid-based protocols which look quite different at first glance share a common concept of construction. Additionally, we explain how new grid-based protocols with particular properties can be designed by using the framework in a graphical way.
[Availability, logical grids, Protocols, Costs, replicated databases, grid-based data replication schemes, semantics, protocols, interpreting algorithms]
Distributed lock management for mobile transactions
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
We present a new lock management scheme which allows a read unlock for an item to be executed at any copy site of that item; the site may be different from the copy site on which the read lock is set. The scheme utilizes the replicated copies of data items to reduce the message costs incurred by the mobility of the transaction host. We demonstrate this idea in an optimistic locking algorithm called O2PL-MT (Optimistic Two Phase Locking for Mobile Transactions). Like its counterpart algorithm O2PL (Optimistic Two Phase Locking), O2PL-MT grants read locks immediately on demand and defers write locks until the commitment time. However, O2PL-MT requires the transmission of fewer messages than O2PL in a mobile environment in which data items are replicated.
[mobile transactions, Costs, read lock, replicated copies, message costs, concurrency control, distributed databases, read unlock, O2PL-MT, distributed lock management]
"How are we going to pay for this? Fee-for-service in distributed systems-research and policy issues"
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
With the increasing array of information and services being supported by distributed computing, we face a new challenge: How do we handle charges? Providers of information will want to receive "royalties\
[policy issues, computing services, distributed processing, DP management, distributed systems, Computer networks, Distributed computing, research, distributed computing]
MPI programming environment for IBM SP1/SP2
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
In this paper we discuss an implementation of the message passing interface standard (MPI) for the IBM Scalable Power PARALLEL 1 and 2 (SP1, SP2). Key to a reliable and efficient implementation of a message passing library on these machines is the careful design of a UNIX-Socket like layer in the user space with controlled access to the communication adapters and with adequate recovery and flow control. The performance of this implementation is at the same level as the IBM-proprietary message passing library (MPL). We also show that in the IBM SP1 and SP2 we achieve integrated tracing ability, where both system events, such as context switches and page fault etc., and MPI related activities are traced, with minimal overhead to the application program, thus presenting application programmers the trace of all the events that ultimately affect efficiency of a parallel program.
[Context, message passing, application program interfaces, Communication system control, communication adapters, Switches, flow control, message passing interface standard, Communication switching, Programming profession, parallel programming, adequate recovery, Programming environments, UNIX-Socket like layer, IBM Scalable Power PARALLEL 1, MPI programming environment, Message passing, page fault, Libraries, integrated tracing ability, parallel program, programming environments, IBM SP1/SP2, application programmers]
I-TCP: indirect TCP for mobile hosts
Proceedings of 15th International Conference on Distributed Computing Systems
None
1995
IP based solutions to accommodate mobile hosts within existing internetworks do not address the distinctive features of wireless mobile computing. IP-based transport protocols thus suffer from poor performance when a mobile host communicates with a host on the fixed network. This is caused by frequent disruptions in network layer connectivity due to i) mobility and ii) unreliable nature of the wireless link. We describe I-TCP, which is an indirect transport layer protocol for mobile hosts. I-TCP utilizes the resources of Mobility Support Routers (MSRs) to provide transport layer communication between mobile hosts and hosts on the fixed network. With I-TCP, the problems related to mobility and unreliability of wireless link are handled entirely within the wireless link; the TCP/IP software on the fixed hosts is not modified. Using I-TCP on our testbed, the throughput between a fixed host and a mobile host improved substantially in comparison to regular TCP.
[Transport protocols, internetworking, I-TCP, network layer connectivity, Mobile communication, Throughput, indirect TCP, internetworks, transport protocols, mobile hosts, TCPIP, Internet, wireless mobile computing, mobility support routers, Mobile computing, Testing, indirect transport layer protocol]
Reliable communication in cube-based multicomputers using safety vectors
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
Reliable communication in cube-based multicomputers (including disconnected ones) using the safety vector concept is studied in this paper. In the proposed approach each node in a cube-based multicomputer of dimension n is associated with a safety vector of n binary numbers which is an approximated measure of the number and distribution of faults in the neighborhood. The safety vector of each node in an n-dimensional hypercube can be easily calculated through n-1 rounds of information exchange among neighboring nodes. Optimal unicasting between two nodes is guaranteed if the kth bit of the safety vector of the source node is one, where k is the Hamming distance between the source and the destination. An extended deadlock-free unicasting using virtual channels is also introduced.
[deadlock-free unicasting, hypercube, Hamming distance, Routing, Reliability engineering, hypercube networks, cube-based multicomputer, cube-based multicomputers, Computer science, Fault tolerance, Casting, safety vectors, Bandwidth, System recovery, Traffic control, Hypercubes, fault tolerant computing, Safety, communication, unicasting]
Parallel simulation of mesh routing algorithms
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
Performance of a network depends primarily on the network topology, switching mechanism, flow control protocol and the underlying routing algorithm. While many routing algorithms have been proposed recently for direct networks, there is no time efficient technique to evaluate and compare all of them. A conventional routing algorithm simulation of a network on a uniprocessor takes unacceptably large computing time. The simulation can be made very time efficient by parallelizing it and running on a parallel test bed. This research is focussed on designing a parallel routing algorithm simulator for n-dimensional mesh connected networks with wormhole switching and virtual channel flow control. The research addresses partitioning mapping, synchronization issues, and implementation of various routing algorithms for 2-D and 3-D mesh architectures. Experimental results show that the parallel simulator can provide significant speedup compared do a uniprocessor environment.
[Algorithm design and analysis, parallel routing algorithm simulator, Computational modeling, Communication system control, multiprocessor interconnection networks, performance evaluation, Discrete event simulation, Partitioning algorithms, wormhole switching, network topology, routing algorithms, Network topology, performance, virtual channel flow, telecommunication network routing, mesh routing, flow control protocol, partitioning mapping, synchronization, Computer networks, Routing protocols, switching mechanism, Testing, Clocks]
Combined routing and scheduling of concurrent communication traffic in hypercube multicomputers
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
We propose and evaluate low-complexity, low-overhead schemes for distributed message scheduling and routing in binary hypercube multicomputers equipped with a hardware communication adapter at each node. The goal is to optimize the network performance not only for steady traffic flow, but also for concurrent bursty traffic. We comparatively evaluate the performance of different scheduling-coding combinations for several switching methods, such as message switching, circuit switching and virtual cut-through. The evaluation results have indicated that in case of heavy transient traffic, a partially-adaptive routing scheme, when combined with an appropriate message-scheduling policy, can outperform a fully-adaptive routing scheme.
[Multiprocessor interconnection networks, Laboratories, concurrent communication, Telecommunication traffic, hypercube multicomputers, hypercube networks, message-scheduling, circuit switching, Distributed computing, processor scheduling, Switching circuits, routing, Hypercubes, Hardware, message switching, fully-adaptive routing, virtual cut-through, performance evaluation, Routing, Communication switching, binary hypercube multicomputers, Processor scheduling, telecommunication network routing, distributed message scheduling, scheduling-coding combinations]
Integrating routing and resource reservation mechanisms in real-time multicast protocols
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
Multimedia applications must incorporate real-time task processing over networks. In most cases, there are many receivers for each data stream. Thus, a multicast resource reservation protocol for ensuring timing constraints over the network is required. In this paper, a real-time multicast protocol, called RtMP (Real-time Multicast Protocol) is presented. RtMP provides flexible connection establishment by integrating the multicast routing mechanism and the resource reservation mechanism by defining a generic interface between them. RtMP also provides a mechanism that guarantees service level of established connections according to the sender list. An implementation of RtMP on the real-time OS RT-Mach is also described along with performance evaluation. Finally, a comparison of RtMP with other real-time multicast protocols is given and its efficiency is shown.
[resource reservation mechanisms, resource reservation, multicast resource reservation protocol, multicast protocol, Laboratories, multimedia applications, Access protocols, multimedia systems, performance evaluation, distributed processing, real-time multicast protocols, Multicast protocols, Routing, Application software, Teleconferencing, routing, Unicast, real-time systems, Streaming media, Computer networks, Timing, protocols, multicast routing]
Multiplexing statistical real-time channels in a multi-access network
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
Given the client's traffic-generation characteristics and performance requirements, we propose a real-time communication scheme that provides delivery delay guarantees in a multiaccess local-area network (LAN). This scheme (i) reduces the link capacity that needs to be reserved to an average level as compared to the worst-case level required for deterministic performance guarantees, and (ii) preserves the ability of independent addition and deletion of real-time channels.
[Real time systems, real-time channels, Laboratories, Circuits, Telecommunication traffic, Ink, performance evaluation, local area networks, deterministic performance, local-area network, Intelligent networks, statistical real-time channels, multi-access network, multiplexing, worst-case level, Traffic control, LAN, multi-access systems, Resource management, link capacity, Local area networks]
Meeting delay requirements in computer networks with wormhole routing
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
We study high performance networks with wormhole routing and investigate their performance in terms of meeting message delay constraints. Traditional system uses unregulated greedy transmission control. This may result in unfairness of network access and unbounded packet blocking time, making it very difficult to efficiently support real-time applications. To overcome this problem, we propose a regulated transmission control method in which packet transmission at the source is regulated and hence unnecessary network contention is eliminated The regulated method is a generalization of the unregulated method and can be easily implemented in most of the commercially available networks.
[Real time systems, computer networks, performance evaluation, network contention, Routing, Control systems, Throughput, Application software, Computer science, Intelligent networks, wormhole routing, unbounded packet blocking, Surveillance, Prototypes, delay requirements, Computer networks, protocols, transmission control method, high performance, packet transmission]
Implementation of recoverable distributed shared memory by logging writes
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
Distributed shared memory, by avoiding the programming complexities of message passing, has become a convenient model to work with. But the benefits given by these systems can possibly be achieved only if the whole system behaves like a failure-free system. Many algorithms that have been proposed for implementing a reliable DSM require the processes to take check points whenever there is a data transfer, thus resulting in a heavy overhead during failure-free execution. We present an algorithm to provide recoverable DSM for sequential consistency where the checkpoint interval can be tailored to balance the cost of checkpointing versus the savings in recovery obtained by taking check points often. Unlike previous recovery techniques that use logging, both the logging and the message overheads are reduced. It can tolerate up to n faults, where n is the number of processes, and can be used in an environment where the cost of synchronizing the checkpoints is substantially high.
[Checkpointing, checkpointing, sequential consistency, Costs, message passing, distributed shared memory, recoverable, system recovery, Computer science, Parallel programming, Message passing, Fault tolerant systems, Bandwidth, distributed memory systems, data transfer, shared memory systems, fault tolerant computing, logging writes, Power engineering and energy, DSM]
How to recover efficiently and asynchronously when optimism fails
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
We propose a new algorithm for recovering asynchronously from failures in a distributed computation. Our algorithm is based on two novel concepts-a fault-tolerant vector clock to maintain causality information in spite of failures, and a history mechanism to detect orphan states and obsolete messages. These two mechanisms together with checkpointing and message-logging are used to restore the system to a consistent state after a failure of one or more processes. Our algorithm is completely asynchronous. It handles multiple failures, does not assume any message ordering, causes the minimum amount of rollback and restores the maximum recoverable state with low overhead. Earlier optimistic protocols lack one or more of the above properties.
[Checkpointing, checkpointing, rollback, Protocols, Costs, maximum recoverable state, distributed processing, multiple failures, distributed computation, History, Distributed computing, system recovery, causality information, history mechanism, Fault tolerance, optimistic protocols, Fault detection, fault tolerant computing, fault-tolerant vector clock, recovering asynchronously, message-logging, Clocks]
A low-overhead recovery technique using quasi-synchronous checkpointing
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
In this paper, we propose a quasi-synchronous checkpointing algorithm and a low-overhead recovery algorithm based on it. The checkpointing algorithm preserves process autonomy by allowing them to take checkpoints asynchronously and uses communication-induced checkpoint coordination for the progression of the recovery line which helps bound rollback propagation during a recovery. Thus, it has the easiness and low overhead of asynchronous checkpointing and the recovery time advantages of synchronous checkpointing. There is no extra message overhead involved during checkpointing and the additional checkpointing overhead is nominal. The algorithm ensures the existence of a recovery line consistent with the latest checkpoint of any process all the time. The recovery algorithm exploits this feature to restore the system to a state consistent with the latest checkpoint of a failed process. The recovery algorithm has no domino effect and a failed process needs only to rollback to its latest checkpoint and request the other processes to roll back to a consistent checkpoint. To avoid domino effect, it uses selective pessimistic message logging at the receiver end. The recovery is asynchronous for single process failure. Neither the recovery algorithm nor the checkpointing algorithm requires the channels to be FIFO. We do not use vector timestamps for determining dependency between checkpoints since vector timestamps generally result in high message overhead during failure-free operation.
[Checkpointing, Protocols, recovery algorithm, fault-tolerance, distributed processing, recovery, system recovery, rollback propagation, single process failure, failed process, Degradation, low-overhead recovery, quasi-synchronous checkpointing, fault tolerant computing]
A scalable technique for implementing multiple consistency levels for distributed objects
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
In large scale distributed systems, caching and replication could greatly speedup access and increase availability. Consistency of replicated state can be guaranteed by forcing operations to occur in the same order at all sites. However some applications can preserve correctness with weaker consistency requirements leading to better performance. We propose an object lifetime based mutual consistency detection mechanism that is used to implement multiple consistency levels. This mechanism provides scalable implementations because caching overheads at client nodes depend only on the accesses done at the node. A contribution of this paper is the separation of the the mutual consistency detection mechanism from the policy that decides the desired consistency guarantees. This allows multiple consistency levels to coexist, thus improving system performance through the use of weaker consistency levels when possible. Besides improving performance, the mechanism allows for a graceful weakening of consistency requirements when stronger requirements cannot be maintained, as in the case of disconnection that can be experienced in mobile environments. The mutual consistency detection mechanism also provides a uniform way of hoarding a mutually consistent set of objects during voluntary disconnection.
[large scale distributed systems, Mobile communication, cache storage, caching, performance improvement, scalable technique, System performance, data replication, Web pages, Collaboration, Object detection, distributed databases, distributed objects, system performance, consistency requirements, multiple consistency levels, Large-scale systems, Contracts]
Structured design of communication protocols
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
We propose a compositional technique to design multifunction protocols. The technique involves first designing the protocols performing the various functions separately and then combining them using a set of constraints. The constraints are used to specify the interactions between the component protocols. The interactions, for example, specify when a function has to be performed and whether two functions can be performed concurrently or not. We illustrate the use of our technique by designing several protocols. We give sufficient conditions to infer properties of the composite protocol from those of the component protocols.
[sufficient conditions, Sufficient conditions, formal verification, transport protocols, Access protocols, communication protocols, structured design, constraints, multifunction protocols]
Dynamic scheduling strategies for shared-memory multiprocessors
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
Efficiently scheduling parallel tasks on to the processors of a shared-memory multiprocessor is critical to achieving high performance. Given perfect information at compile-time, a static scheduling strategy can produce an assignment of tasks to processors that ideally balances the load among the processors while minimizing the run-time scheduling overhead and the average memory referencing delay. Since perfect information is seldom available, however, dynamic scheduling strategies distribute the task assignment function to the processors by having idle processors allocate work to themselves from a shared queue. While this approach can improve the load balancing compared to static scheduling, the time required to access the shared work queue adds directly to the overall execution time. To overlap the time required to dynamically schedule tasks with the execution of the tasks, we examine a class of self-adjusting dynamic scheduling (SADS) algorithms that centralizes the assignment of tasks to processors. These algorithms dedicate a single processor of the multiprocessor to perform a novel on-line branch-and-bound technique that dynamically computes partial schedules based on the loads of the other processors and the memory locality (affinity) of the tasks and the processors. Our simulation results show that this centralized scheduling outperforms self-scheduling algorithms even when using only a small number of processors.
[average memory referencing delay, load balancing, Heuristic algorithms, dynamic scheduling strategies, parallel tasks scheduling, Optimal scheduling, memory locality, processor scheduling, Multiprocessing systems, Runtime, resource allocation, self-adjusting dynamic scheduling algorithms, run-time scheduling, shared memory systems, Delay effects, task assignment function, shared-memory multiprocessors, Dynamic scheduling, Scheduling algorithm, branch-and-bound technique, Computer science, partial schedules, Processor scheduling, Load management, static scheduling strategy]
Performance comparison of process migration with remote process creation mechanisms in RHODOS
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
We claim that both remote process creation and process migration are efficient mechanisms to be used in the improvement or development of high performance computer systems. In particular we demonstrate that the claims made by some researchers that process migration is too heavy to be used to support dynamic load balancing are unsubstantiated. We support our claim by presenting these two mechanisms available in the RHODOS distributed operating system, comparing and contrasting these mechanisms and reporting on their performance.
[Costs, distributed processing, Dynamic scheduling, Mathematics, processor scheduling, process migration, RHODOS distributed operating system, Processor scheduling, resource allocation, High performance computing, Operating systems, network operating systems, high performance computer systems, Load management, remote procedure calls, operating systems (computers), remote process creation mechanisms, Computer networks, Workstations, Australia, dynamic load balancing]
Deadlock detection by fair reachability analysis: from cyclic to multi-cyclic protocols (and beyond?)
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
We generalize the technique of fair reachability analysis to multi-cyclic protocols modeled as networks of communicating finite state machines, where a number of cyclic protocols are interconnected in such a way that any two component cyclic protocols share at most one process and each channel in the protocol belongs to exactly one component cyclic protocol. By composing the fair reachability relations of the component cyclic protocols, we prove that the set of fair reachable states of a multi-cyclic protocol is exactly the set of reachable states that are of equal channel length with respect to each of its component cyclic protocols. As a result, each deadlock state is fair reachable, and deadlock detection is decidable for the class of multi-cyclic protocols whose fair reachable state spaces are finite. Under the assumption that the underlying communication topology of a protocol is strongly connected, we show that fair reachability analysis is inherently infeasible for logical correctness validation beyond multi-cyclic protocols.
[Protocols, state explosion, distributed processing, finite state machines, system recovery, deadlock detection, formal verification, communication topology, cyclic protocols, communicating finite state machines, reachability analysis, program diagnostics, logical correctness validation, memory protocols, Educational institutions, Explosions, State-space methods, Topology, Reachability analysis, Computer science, fair reachability analysis, Automata, System recovery, Computer errors, multi-cyclic protocols, relief strategies]
A framework for customizing coherence protocols of distributed file caches
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
In cooperative applications such as group CAD and group software development systems, multiple processes communicate with each other by sharing complex data structures consisting of nested structures and pointers. Although the sharing of complex data structures in the distributed environment is achieved through the technology of distributed shared memory, a single cache coherence protocol cannot efficiently serve various access patterns generated by cooperative applications. This paper describes a framework of protocol customization for the sharing of volatile and experiment data in cooperative applications. The major obstacle in user-level customization of protocols is that there are too many states and state transitions in an unabstracted protocol to enable average users to describe them. The protocol customization system (PCS) in this paper solves this problem by introducing a high-level model for protocol description that abstracts away non-determinism of messages, synchronization among hosts, and local paging actions. Consequently, users can define with brief descriptions cache coherence protocols adapted for particular applications. The ability of PCS to describe different kinds of protocols is examined in this paper and its run-time performance and memory usages are investigated.
[group software development systems, distributed shared memory, cache coherence, group CAD, nested structures, Access protocols, Programming, memory protocols, cooperative applications, Data structures, Electronic mail, Application software, Information science, distributed file caches, Memory management, coherence protocols, Abstracts, Coherence, distributed memory systems, shared memory systems, data structures, protocol customization, Personal communication networks, pointers]
Distance routing on series parallel networks
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
We consider the problem of routing messages on Series Parallel Graphs (SPGs) and we introduce a new technique called Distance Routing. This technique is based on the idea of encoding in the label of each node x some information about a shortest path from the source of the SPG to x, and from x to the terminal node of the SPG. We first compare shortest path Distance Routing and I-interval Routing Schemes on directed SPGs. We then show that Distance Routing can be used to route on bidirectional SPGs, where no general shortest path I-interval Routing Scheme can be applied. We also show the relevance of the study of the time complexity in the choice of a Compact Routing method.
[Costs, routing messages, Scholarships, network routing, graph theory, terminal node, multiprocessor interconnection networks, time complexity, Routing, Encoding, Topology, communication complexity, one-interval routing schemes, Computer science, shortest path distance routing, Boolean functions, Runtime, Labeling, series parallel networks]
Using projection aggregations to support scalability in distributed simulation
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
Distributed interactive simulation systems are growing to include well over 100,000 dynamic entities for applications such as multiplayer video games, military and industrial training, and collaborative engineering. In these applications, each host receives updates (such as position and orientation) from remote entities, models and renders the scene, and performs other tasks such as collision detection. The number of entities places a heavy burden on both the networking resources and computational resources available to the application. To address these limitations, some systems have aggregated information about groups of simulation entities according to their organizational structure or their location within the virtual world. However traditional aggregation techniques are inadequate because remote hosts need to access entities based on both their organization and their virtual world position. This paper describes projection aggregations, a technique for grouping entities by both their organization and location. Remote hosts use projections to control which entities are represented locally and at what level-of-detail. We describe how projection aggregations are implemented in a networked environment and demonstrate how they reduce network bandwidth and computational requirements. Finally, we argue that projection aggregations represent a general-purpose framework for representing all simulation entities, thereby supporting evolution of entity models.
[Military computing, projection aggregations, Scalability, Computational modeling, Computer simulation, distributed processing, collaborative engineering, digital simulation, distributed simulation, entity models, scalability, grouping entities, Computer science, Design engineering, multiplayer video games, computational resources, Layout, networking resources, Collaboration, Games, interactive systems, Computer networks]
Fault-tolerant file transmission by information dispersal algorithm in rotator graphs
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
A directed graph G=(V, E) is called the n-rotator graph if V={a/sub 1/a/sub 2//spl middot//spl middot//spl middot/a/sub n/|a/sub 1/a/sub 2//spl middot//spl middot//spl middot/a/sub n/ is a permutation of 1, 2, /spl middot//spl middot//spl middot/, n} and E={(a/sub 1/a/sub 2//spl middot//spl middot//spl middot/a/sub n/, b/sub 1/b/sub 2//spl middot//spl middot//spl middot/b/sub n/) I for some 2/spl les/i/spl les/n, b/sub 1/b/sub 2//spl middot//spl middot//spl middot/b/sub n/=a/sub 2//spl middot//spl middot//spl middot/a/sub i/a/sub 1/a/sub i+1//spl middot//spl middot//spl middot/a/sub n/}. We show that for any pair of distinct nodes in the n-rotator graph, we can construct n-1 disjoint paths, each with length less than 2n, connecting the two nodes. By using these disjoint paths and information dispersal algorithm (IDA) by M.O. Rabin (1989), we design a file transmission scheme and analyse its reliability.
[Algorithm design and analysis, multiprocessor interconnection networks, information dispersal algorithm, reliability, rotator graphs, Complexity theory, Synchronization, n-1 disjoint paths, fault-tolerant file transmission, Information analysis, Computer science, Fault tolerance, Network topology, directed graphs, directed graph, distinct nodes, fault tolerant computing, disjoint paths, computer network reliability, file transmission scheme, Telecommunication network reliability, Joining processes, Clocks]
Exception handling and resolution in distributed object-oriented systems
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
We address the problem of how to handle exceptions in distributed object-oriented systems. In a distributed computing environment exceptions may be raised simultaneously and thus need to be treated in a coordinated manner. We take two kinds of concurrency into account: 1) several objects are designed collectively and invoked concurrently to achieve a global goal, and 2) concurrent objects or object groups that are designed independently compete for the same system resources. We propose a new distributed algorithm for resolving concurrent exceptions and show that the algorithm works correctly even in complex nested situations, and is an improvement over previous proposals in that it requires only O(N/sup 2/) messages, and is fully object-oriented.
[distributed object-oriented systems, concurrent objects, exception handling, distributed processing, Mathematics, Distributed computing, system recovery, processor scheduling, Concurrent computing, resource allocation, object groups, system resources, Hardware, Communication networks, Distributed algorithms, object-oriented programming, distributed computing environment exceptions, Object oriented modeling, exception resolution, complex nested situations, Computer crashes, concurrency, distributed algorithm, Message passing, distributed algorithms, Error correction]
An embeddable and extendable language for large-scale programming on the Internet
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
This article addresses the problem of programming by combining already existing programs on the INTERNET. For this, an interpreter based language called Progress is presented. Progress makes it possible to use and combine programs and functionalities provided by various servers in a simple and elegant way, similar to what a UNIX shell does for locally available programs. Moreover the inherent parallelism, which is given due to the fact that programs may reside on different hosts, can be fully exploited in Progress. Other important features of Progress are that the language can be extended by new data types and that it is embeddable in other software systems. This article presents the main concepts and ideas of Progress and gives an overview of the language.
[Progress, distributed processing, Application software, Communication standards, Web and internet services, functionalities, embeddable extendable language, inherent parallelism, large-scale programming, interpreter based language, Software systems, remote procedure calls, Hardware, data types, Large-scale systems, Internet, Functional programming, Web server, Joining processes, parallel languages]
Robust distributed mutual exclusion
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
A token based algorithm for distributed mutual exclusion is presented. It uses a distributed counter to tolerate faults due to site failures and communication failures. This eliminates the need for expensive election protocols that are commonly employed in existing token-based algorithms. As in many existing fault-tolerant mutual exclusion algorithms, timeouts are used to detect failures. Our algorithm has the unique property that even if timeout periods are incorrectly assumed, the safety requirement of mutual exclusion is still guaranteed. It is, therefore, suitable for highly asynchronous distributed environments. Performance analysis shows that the algorithm is also efficient in terms of the average number of messages required per critical section, and the average time delay to enter the critical section. Finally, we show how to obtain a bounded algorithm from the basic unbounded algorithm.
[Algorithm design and analysis, client-server systems, Protocols, Costs, Delay effects, performance evaluation, timeouts, Counting circuits, average time delay, Computer science, Fault tolerance, token based algorithm, Fault detection, communication failures, Robustness, fault tolerant computing, highly asynchronous distributed environments, distributed counter, protocols, token networks, robust distributed mutual exclusion, performance analysis, Clocks, bounded algorithm]
Fast dynamic process migration
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
Dynamic process migration supports load sharing and processor fault tolerance. We present the new freeze free algorithm for process migration, which uses six techniques to: reduce process migration latency by an order of magnitude to 19.9 ms, effectively eliminate message freeze times, and to support processor fault tolerance. The freeze free algorithm resumes execution after the transfer of four items: the combined process control and execution state, the current code page, the current heap page, and the current code page. The algorithm effectively eliminates message freeze time by separating the process state from the communication state, and thus allows message processing to proceed in parallel with process migration. The algorithm eliminates old host residual dependencies by flushing old host resident, modified data; while the process executes in parallel on the new host. The paper analyzes the costs in both the process migration latency period and the cross-network demand paging operations, and identifies further cost reductions. This paper demonstrates that small overhead is needed for good load sharing system speedup by measuring the impact of increasing overhead an speedup.
[Costs, load sharing system speedup, load sharing, Resumes, Process control, distributed processing, communication state, Distributed computing, Delay, processor fault tolerance, Power measurement, resource allocation, process control, message freeze time, current code page, Computer networks, fault tolerant computing, freeze free algorithm, host residual dependencies, Workstations, fast dynamic process migration, Velocity measurement, Communication networks, current heap page]
Energy efficient filtering of nonuniform broadcast
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
In a wireless environment, information is broadcast on communication channels to clients using powerful, battery-operated palmtops. To conserve the usage of energy, the information to be broadcast must be organized so that the client can selectively tune in at the desirable portion of the broadcast. Most of the existing work focus on uniform broadcast. However very often, a small amount of information is more frequently accessed by a large number of clients while the remainder are less in demand. This nonuniform access pattern poses several new issues. In this paper we examine these issues and look at how a nonuniform broadcast can be organized for selective tuning by the clients. We propose several new indexing schemes to facilitate selective tuning. A performance study is conducted to study and demonstrate the effectiveness of the proposed schemes.
[telecommunication channels, Filtering, performance evaluation, Downlink, nonuniform broadcast, nonuniform access pattern, Batteries, Computer science, performance study, energy efficient filtering, Wireless networks, wireless environment, selective tuning, Broadcasting, battery-operated palmtops, Energy efficiency, Computer networks, indexing schemes, wireless LAN, Personal digital assistants, uniform broadcast, Mobile computing, communication channels]
Supporting a flexible parallel programming model on a network of workstations
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
We introduce a shared memory software prototype system for executing programs with nested parallelism on a network of workstations. This programming model exhibits a very convenient and natural programming style and provides functionality similar to a subset of Compositional C++. Such programming model is especially suitable for computations whose complexity and parallelism emerges only during their execution, as in divide and conquer problems. To both support and take advantage of the flexibility inherent in the programming model, we develop an architecture, which distributes both the shared memory management and the computation, removing bottlenecks inherent in centralization, thus also providing scalability and dependability. The system supports also dynamic load balancing, and fault tolerance-both transparently to the programmer. The prototype performs well using the realistic platforms of non-dedicated network of workstation. We describe encouraging performance experiments on a network in which some of the machines became slow unpredictably (to the application program). The system coped well with such dynamic behavior.
[parallel architectures, bottlenecks, Distributed computing, parallel programming, scalability, Concurrent computing, Computer architecture, Parallel processing, shared memory systems, divide and conquer, Workstations, Functional programming, shared memory management, Software prototyping, fault tolerance, dependability, workstations, Parallel programming, Memory management, network of workstations, parallel programming model, Software systems, Compositional C++, programming environments, dynamic load balancing]
Group routing without group routing tables
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
We present a group routing protocol for a network of processes. The task of the protocol is to route data messages to each member of a process group. To this end, a tree of processes is constructed in the network, ensuring each group member is included in the tree. To build this tree, the group routing protocol relies upon the unicast routing tables of each process. Thus, group routing is a composition of a unicast routing protocol, whose detailed behavior is unknown but its basic properties are given, and a protocol that builds a group tree based upon the unicast routing tables. The design of the group routing protocol is presented in three steps. First, a basic group routing protocol is presented and proven correct. Then, the protocol is refined twice, strengthening its properties with each refinement. The final protocol has the property of adapting the group tree to changes in the unicast routing tables without compromising the integrity of the group tree, even in the presence of unicast routing loops.
[group routing tables, Telecommunication traffic, unicast routing tables, group routing protocol, Videoconference, unicast routing loops, computer network management, protocol, Unicast, Network topology, transport protocols, data messages, Broadcasting, data communication, Routing protocols]
Communication compilation for unreliable networks
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
Parallel programs running on top of generic protocols (e.g. TCP) in a cluster of workstations often do not perform or scale as well as one would expect. One reason for this is that both the performance and scalability of parallel applications are highly dependent on the speed of communication, yet the generic protocols used to guarantee reliable message delivery add unnecessary overhead which degrades the performance of the parallel application. The main thesis we explore in this paper is that it is possible to use knowledge of application behavior to design protocols that are more efficient. In particular, we investigate automatic techniques for generating optimized application-specific network protocols for parallel applications running on unreliable networks. Our algorithms assume that the application communication can be represented by a context free grammar. Such algorithms form the basis for a communication compiler.
[Context, Protocols, Costs, Buffer storage, performance evaluation, parallel applications, Application software, reliable message, communication compiler, parallel programming, scalability, network protocols, Computer science, Degradation, performance, context free grammar, Parallel processing, context-free grammars, unreliable networks, fault tolerant computing, Workstations, Telecommunication network reliability, protocols, communication]
Sentries for the execution of concurrent programs
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
The sentry of a concurrent program P is a program that executes concurrently with P, periodically takes snapshots of P, and issues a warning if it detects that some snapshot does not satisfy a predefined predicate. The sentry is unique among snapshot-taking systems in its low-overhead. First, the shared storage between the observed program P and the sentry is linear in the number of P variables that are being observed. Second, the observed program P never waits for the sentry. Third, the mutual exclusion between the observed program and the sentry is achieved without using any special hardware or software constructs. In this paper, we present a family of two sentries. One sentry can be used for taking snapshots of scalar variables (and can check whether these snapshots satisfy a given propositional predicate), and the other sentry can be used for taking snapshots of complex variables such as arrays (and can check whether these snapshots satisfy a given first-order predicate). We briefly describe a system prototype for automatically generating sentries for any given concurrent program, and present some encouraging empirical results that we obtained from this prototype.
[system prototype, Costs, mutual exclusion, predefined predicate, Mathematics, concurrent programs, empirical results, parallel programming, shared storage, Computer science, Concurrent computing, snapshots, concurrent program, Prototypes, concurrency control, Ear, scalar variables, Hardware, Safety, complex variables]
A multi-level explicit rate control scheme for ABR traffic with heterogeneous service requirements
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
The Available-Bit-Rate (ABR) service that is being standardized by the ATM Forum dynamically determines the maximum transmission rate, so-called explicit rate, of a connection. A drawback of the ABR control scheme for calculating the explicit rates is that it tries to allocate the same bandwidth to all ABR connections regardless of the application type of the connection. In this study a multi-level explicit rate scheme is proposed that can allocate different explicit rates for different classes of connections. ABR traffic is controlled at three levels. At the topmost level, bandwidth is dynamically regulated between CBR, VER, and ABR traffic sources. At the next level, bandwidth is controlled between different classes of ABR traffic. At the lowest level bandwidth is distributed among connections belonging to the same ABR traffic class. The effectiveness of the proposed scheme is demonstrated in simulation experiments.
[telecommunication congestion control, Laboratories, bandwidth, Telecommunication traffic, Switches, Throughput, asynchronous transfer mode, heterogeneous service requirements, Computer science, multi-level explicit rate control scheme, Bandwidth, Traffic control, Available-Bit-Rate service, Communication system traffic control, Computer networks, ABR traffic, Asynchronous transfer mode, ATM networks]
Theoretical and empirical results on dynamic load balancing in an object-based distributed environment
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
We present theoretical and empirical investigations to explore the potential of dynamic load balancing for homogeneous systems. We introduce the object-based programming environment MOM. Then we define an own load balancing algorithm called Gating and compare it with other well known algorithms, theoretically, in a simulation and in the MOM system. The obtaining results are discussed.
[object-oriented programming, Object oriented modeling, Multitasking, MOM, processor scheduling, Programming environments, load balancing algorithm, Runtime, Message-oriented middleware, resource allocation, distributed algorithms, homogeneous systems, object-based programming environment, Parallel processing, Load management, object-based distributed environment, Hardware, Power system reliability, Object oriented programming, Gating, dynamic load balancing]
Aster: a framework for sound customization of distributed runtime systems
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
This paper introduces the Aster distributed configuration-based-programming system that is aimed at easing the development of emerging distributed applications having quality of service requirements. Our approach is based on high-level customization: given the specification of application requirements using the Aster interconnection language, a distributed runtime system, customized for meeting these requirements is built. So as to make the customization process sound, we propose a formal method that allows one to reason about specification matching of a customized distributed runtime system with the application's requirements.
[customized distributed runtime system, Aster interconnection language, Quality of service, distributed processing, distributed runtime system, Aster distributed configuration-based-programming system, high-level customization, Distributed computing, formal specification, parallel programming, specification matching, distributed runtime systems, Hardware, Dynamic programming, quality of service requirements, formal method, Multimedia systems, Multimedia computing, LAN interconnection, Application software, Programming profession, configuration management, software reusability, Software systems, computer aided software engineering, programming environments]
Strong interaction fairness via randomization
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
We present Multi, a symmetric, fully distributed, randomized algorithm that, with probability 1, schedules multiparty interactions in a strongly fair manner. To our knowledge, Multi is the first algorithm for strong interaction fairness to appear in the literature. Moreover, the expected time taken by Multi to establish an interaction is a constant not depending on the total number of processes in the system. In this sense, Multi guarantees real-time response. Multi makes no assumptions (other than boundedness) about the time it takes processes to communicate. It thus offers an appealing tonic to the impossibility results of Tsay&Bagrodia and Joung concerning strong interaction fairness in an environment, shared-memory or message-passing, in which processes are deterministic and the communication time is nonnegligible. Because strong interaction fairness is as strong a fairness condition that one might actually want to impose in practice, our results indicate that randomization may also prove fruitful for other notions of fairness lacking deterministic realizations and requiring real-time response.
[multiparty interactions, Taxonomy, fully distributed randomized algorithm, probability 1, Multi, real-time response, randomised algorithms, Computer science, Concurrent computing, Computer languages, strong interaction fairness, Councils, distributed algorithms, message-passing system, boundedness, shared-memory system]
An extended network scheduling model
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
In real-time communication, the rate monotonic scheduling has received much attention as a basis for scheduling messages in a local area network. Schedulability analysis in such a network has been addressed by some researchers. However, the existing analysis only take independent messages into account. In this paper we extend the analysis to accommodate a specific type of inter-dependent messages: request-response messages.
[Job shop scheduling, request-response messages, FDDI, Delay effects, rate monotonic scheduling, Access protocols, local area networks, real-time communication, Processor scheduling, schedulability analysis, real-time systems, extended network scheduling model, local area network, Bismuth, scheduling, Computer networks, Timing, Virtual manufacturing, Local area networks]
An adaptive load balancing algorithm for heterogeneous distributed systems with multiple task classes
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
We propose an adaptive load balancing algorithm for heterogeneous distributed systems. The algorithm intrinsically allows a batch of tasks to be relocated. The key of the algorithm is to transfer a suitable amount of processing demand from senders to receivers. This amount is determined dynamically during sender-receiver negotiations. Factors considered when this amount is determined include processing speeds of different nodes, the current load state of both sender and receiver, and the processing demands of tasks eligible for relocation. Composition of a task batch is modeled as a 0-1 Knapsack problem. We also propose a load state measurement scheme which is designed particularly for heterogeneous systems.
[Protocols, Heuristic algorithms, 0-1 Knapsack problem, Throughput, load state measurement scheme, Delay, Computer science, resource allocation, adaptive load balancing algorithm, heterogeneous distributed systems, multiple task classes, Message passing, Intrusion detection, operations research, Bandwidth, distributed databases, Load management, Particle measurements, sender-receiver negotiations, processing speeds]
Larchant: persistence by reachability in distributed shared memory through garbage collection
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
We consider a shared store based on distributed shared memory (DSM) supporting persistence by reachability (PBR) a very simple data sharing model for a distributed system. This DSM+PBR model is based on distributed garbage collection (GC). Within a general model for DSM+PBR, we specify a distributed GC algorithm that is efficient and scalable. Its main features are: (i) independent collection of memory subsets (even when replicated), (ii) orthogonal from coherence, (iii) asynchrony, and (iv) a simple heuristic to collect cycles avoiding extra I/O costs. We briefly describe our implementation and show some performance results.
[persistence by reachability, Costs, Navigation, distributed shared memory, Multimedia systems, Larchant, Multimedia databases, distributed system, File servers, Multimedia communication, garbage collection, storage management, Bioreactors, Memory management, Computer bugs, Coherence, distributed databases, data sharing model, performance results]
Group communication for upgrading distributed programs
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
Recently, large-scale distributed systems are being developed. Since it is expensive to newly develop the system, it is required to adopt the system in use to the changes of the user requirements and the environments. Therefore, it is essential to discuss how to upgrade the distributed programs. The system cannot be kept highly available by the conventional upgrading methods because multiple processes have to be suspended simultaneously. This paper discusses a new method which allows each process to invoke the upgrading procedure independently of the other processes. The key idea is that multiple versions of processes are allowed to co-exist temporarily. If the multiple versions of processes co-exist, such protocol errors as the unspecified receptions and the communication deadlocks can occur. Each pair of an old-version process and a new-version one is named a process group. The group communication protocol proposed in this paper supports message transmissions among the process groups. Moreover, the protocol errors can be detected and resolved by using checkpointing, timeout and rollback recovery.
[Checkpointing, checkpointing, Protocols, Laboratories, Application software, large-scale distributed systems, Distributed computing, system recovery, group communication, timeout, Information systems, communication deadlocks, message transmissions, transport protocols, System recovery, Computer errors, Computer networks, protocol errors, Large-scale systems, rollback recovery, upgrading distributed programs]
The DEC: processing scientific data over the Internet
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
We present the Distributed Batch Controller (DBC), a system built to support batch processing of large scientific datasets. The DBC implements a federation of autonomous workstation pools, which may be widely distributed. Individual batch jobs are executed using idle workstations in these pools. Input data are staged to the pool before processing begins. We describe the architecture and implementation of the DBC, and present the results of experiments in which it is used to perform image compression.
[data compression, DEC, Image retrieval, idle workstations, image compression, Control systems, Information retrieval, batch processing, Cache storage, Distributed computing, scientific data processing, Computer science, autonomous workstation pools, Satellites, Distributed Batch Controller, batch processing (computers), large scientific datasets, Distributed control, scientific information systems, Internet, Workstations, image coding]
Reservation-based totally ordered multicasting
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
We present an innovative totally ordered multicast protocol that is based on reservation of buffers at the destinations and intermediate network bridges, rather than on an acknowledgment strategy. By introducing timestamps into the packets and by using a buffer reservation retract scheme, our protocol not only produces a global total order of packets but also solves the deadlock and starvation problems to which reservation-based protocols are vulnerable. A discrete event simulation demonstrates the high throughput and low latency of the protocol.
[intermediate network bridges, buffer reservation retract scheme, multicast protocol, buffers, timestamps, Multicast protocols, Throughput, Discrete event simulation, global total order, Delay, Bridges, acknowledgment strategy, low latency, Bandwidth, System recovery, Streaming media, reservation-based totally ordered multicasting, Communication networks, Telecommunication network reliability, protocols, discrete event simulation]
Differential evaluation of continual queries
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
We define continual queries as a useful tool for monitoring of updated information. Continual queries are standing queries that monitor the source data and notify the users whenever new data matches the query. In addition to periodic refresh, continual queries include Epsilon Transaction concepts to allow users to specify query refresh based on the magnitude of updates. To support efficient processing of continual queries, we propose a differential re-evaluation algorithm (DRA), which exploits the structure and information contained in both the query expressions and the database update operations. The DRA design can be seen as a synthesis of previous research on differential files, incremental view maintenance, and active databases.
[database update operations, Power system management, differential evaluation, differential reevaluation algorithm, Epsilon Transaction concepts, World Wide Web, Transaction databases, differential files, Computer science, query processing, updated information, Query processing, Distributed databases, incremental view maintenance, Computer architecture, periodic refresh, Database systems, Internet, continual queries, Monitoring, active databases]
The performance value of shared network caches in clustered multiprocessor workstations
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
This paper evaluates the benefit of adding a shared cache to the network interface as a means of improving the performance of networked workstations configured as a distributed shared memory multiprocessor. A cache on the network interface offers the potential benefits of retaining evicted processor cache lines, providing implicit prefetching, and increasing intra-cluster sharing. Using simulation, eight parallel scientific applications were used to evaluate the performance impact of a shared network cache. In each case, we examined in detail the means by which processor cache misses were satisfied. For the applications studied, we found that the network cache offers substantial performance benefit when processor caches are too small to hold the application's primary working set, or when network contention limits application performance. The expected benefits of implicit prefetching and increased intra-cluster sharing did not contribute significantly to the performance enhancement of the network cache for most applications. Finally, the advantage afforded by the network cache diminishes as processor cache size increases and network contention decreases.
[Computer interfaces, Laboratories, cache storage, Network interfaces, Delay, network interfaces, Intelligent networks, Parallel processing, implicit prefetching, shared memory systems, Computer networks, Workstations, clustered multiprocessor workstations, shared network caches, Prefetching, Parallel machines, performance evaluation, performance value, networked workstations, intra-cluster sharing, network interface, processor cache lines, distributed memory systems, parallel scientific applications, performance impact, distributed shared memory multiprocessor]
A snapshot algorithm for distributed mobile systems
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
This paper considers distributed algorithms for distributed mobile systems. Many distributed algorithms have been designed for distributed systems consisting of static computers only. But most of them cannot be directly applied to mobile systems. This paper proposes a model of mobile systems. Management of movements of mobile hosts is abstracted in our model to simplify design of algorithms for mobile systems. This paper also defines the snapshot problem, one of the fundamental problems, on the model. The problem requires to find a strongly consistent configuration in which topological consistency is satisfied in addition to causal consistency. Furthermore, this paper presents a snapshot algorithm for mobile systems.
[Algorithm design and analysis, Portable computers, causal consistency, distributed mobile systems, Distributed computing, Computational complexity, strongly consistent configuration, Wireless communication, Information science, distributed algorithms, mobile hosts, topological consistency, Computer networks, Communications technology, static computers, Distributed algorithms, Mobile computing, portable computers, snapshot algorithm]
Specializing object-oriented RPC for functionality and performance
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
Remote procedure call (RPC) integrates distributed processing with conventional programming languages. However traditional RPC lacks support for forms of communication such as datagrams, multicast, and streams that fall outside the strict request-response model. Emerging applications such as Distributed Interactive Simulation (DIS) and Internet video require scalable, reliable, and efficient communication. Applications are often forced to meet these requirements by resorting to the error-prone ad-hoc message-based programming that characterized applications prior to the introduction of RPC. In this paper we describe an object-oriented RPC system that supports specialization for functionality and performance, allowing applications to modify and tune the RPC system to meet individual requirements. Our experiences with functional extensions to support reliable multicast and specializations to support streaming of performance-critical RPCs indicate that a wide range of communication semantics can be supported without resorting to ad-hoc messaging protocols.
[multiprocessing systems, object-oriented programming, multiprocessing programs, Object oriented modeling, Computational modeling, messaging protocols, Access protocols, Software performance, distributed processing, Multicast protocols, Internet video, object-oriented, Application software, object-oriented RPC, Computer science, Distributed processing, Computer languages, Distributed Interactive Simulation, communication semantics, remote procedure calls, Hardware, remote procedure call]
On programming with view synchrony
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
View synchrony has been proposed as a programming paradigm for developing reliable distributed applications. The paradigm is particularly attractive when the underlying computing system is asynchronous and prone to complex failure scenarios including partitions. View synchrony encourages a programming style where groups of processes cooperate closely in order to maintain some form of shared state among them. In this paper we examine the technical problems that arise in shared state management when programming applications using view synchrony. We identify three classes of problems corresponding to state transfer upon group joins, state recreation after total failures and state merging after partition unions. We argue that shared state problems are inherent to any implementation, and without explicit support, attempts to solve them may easily obscure much of the simplicity and elegance of view synchrony. Finally, we propose an extension to the traditional view synchrony model based on the notion of subviews that addresses the problems raised by shared state management.
[state merging, merging, reliable distributed applications, Delay effects, Merging, Broadcast technology, programming style, Reliability engineering, Computer crashes, Maintenance, state transfer, inference mechanisms, Distributed computing, Computer science, programming paradigm, distributed algorithms, partition unions, Internet, shared state management, state recreation, Distributed algorithms, programming, view synchrony]
On-line avoidance of the intrusive effects of monitoring on runtime scheduling decisions
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
Distributed programs are often instrumented for collecting information to assist in analyzing the behavior of an application. However, the act of monitoring a process introduces intrusive overhead that delays the times at which actions occur in a monitored execution in comparison to the times at which they occur in an unmonitored execution. These delays can alter the actions of individual processes and the behavior of the system as a whole. The overall behavior of process scheduling is determined by the choices a scheduler makes in terms of when particular processes will be given access to the processor. Monitoring introduced delays can result in different scheduling choices being made which can in turn lead 20 changes in the behavior of processes throughout the distributed system. This paper presents intrusion removal techniques which are designed to accommodate monitoring delays in an attempt to minimize the intrusion on the scheduling and execution behavior of the monitored computation.
[Instruments, Delay effects, distributed processing, Educational institutions, online avoidance, intrusive effects, Distributed computing, Information analysis, distributed programs, Runtime, Processor scheduling, resource allocation, Message passing, intrusion removal techniques, delays, process scheduling, execution behavior, application generators, system monitoring, Timing, runtime scheduling decisions, Monitoring]
A simulation-based evaluation of a disk I/O subsystem for a massively parallel computer: JUMP-1
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
JUMP-1 is a distributed shared-memory massively parallel computer and is composed of multiple clusters of inter-connected network called RDT (Recursive Diagonal Torus). Each cluster in JUMP-1 consists of 4 element processors, secondary cache memories, and 2 MBP (Memory Based Processor) for high-speed synchronization and communication among clusters. The I/O subsystem is connected to a cluster via a high-speed serial link called STAFF-Link. The I/O buffer memory is mapped onto the JUMP-1 global shared-memory to permit each I/O access operation as memory access. In this paper we describe evaluation of the fundamental performance of the disk I/O subsystem using event-driven simulation, and estimated performance with a Video On Demand (VOD) application.
[high-speed synchronization, high-speed serial link, Cache memory, video on demand application, event-driven simulation, input-output programs, cache storage, JUMP-1, Distributed computing, parallel machines, Research and development, Concurrent computing, interactive television, distributed shared-memory massively parallel computer, shared memory systems, Computer networks, Recursive Diagonal Torus, RDT, Computational modeling, Computer simulation, inter-connected network, performance evaluation, disk I/O subsystem, Application software, secondary cache memories, STAFF-Link, simulation-based evaluation, High performance computing, virtual machines, I/O buffer memory, Systems engineering and theory, Memory Based Processor]
Optimal deadlock detection in distributed systems based on locally constructed wait-for graphs
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
We present a new algorithm for detecting generalized deadlocks in distributed systems. Our algorithm incrementally constructs and reduces a wait-for graph (WFG) at an initiator process. This WFG is then searched for deadlock. The proposed algorithm has two primary advantages: First, it avoids sending messages along the edges of the global wait-for graph (WFG), thereby achieving a worst-case message complexity of 2n, where n is the number of processes in the WFG. Since information must be obtained from every process reachable from the initiator, this is optimal to within a constant factor. All the existing algorithms for the same problem construct a distributed snapshot of the WFG. As this involves sending messages along the edges of the WFG, the best available message complexity among these algorithms is 4e-2n+2l, which is O(n/sup 2/) in the worst case, where e and l are the number of edges and leaves in the WFG, respectively. Second, since the information about a detected deadlock is readily available at the initiator process, rather than distributed among different processes, it significantly simplifies the task of deadlock resolution, and helps to reduce system overhead associated with the resolution. The time complexity of our algorithm is also better than or equal to the existing algorithms.
[generalized deadlocks, Laboratories, NASA, distributed processing, distributed snapshot, time complexity, Topology, communication complexity, Sun, locally constructed wait-for graphs, optimal deadlock detection, deadlock resolution, Computer science, worst-case message complexity, network operating systems, concurrency control, System recovery, distributed systems, system overhead, Detection algorithms, Contracts, computational complexity]
Method-induced partitioning schemes for object-oriented databases
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
Object-oriented database systems are becoming popular and are being used in a large number of application domain. Many of these application domains are inherently distributed. The focus of this work is on articulating the concepts of method induced partitioning schemes in object-oriented databases by understanding and classifying the object behavior embodied by the methods. We provide a solution for supporting fragmentation transparency by using method transformation. Finally, we present guidelines for method induced partitioning in object-oriented databases.
[Encapsulation, application domain, Object oriented databases, Object oriented modeling, Instruction sets, method-induced partitioning schemes, object-oriented databases, object behavior, Relational databases, Spatial databases, Application software, method transformation, Guidelines, Computer science, fragmentation transparency, Database systems]
Atomic recovery units: failure atomicity for logical disks
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
Atomic recovery units (ARUs) are a mechanism that allows several logical disk operations to be executed as a single atomic unit with respect to failures. For example, ARUs can be used during file creation to update several pieces of file meta-data atomically. ARUs simplify systems, as they isolate issues of atomicity within the logical disk system, ARUs are designed as part of the Logical Disk (LD), which provides an interface to disk storage that separates file and disk management by using logical block numbers and block lists. This paper discusses the semantics of concurrent ARUs, as well as the concurrency control they require. A prototype implementation in a log-structured logical disk system is presented and evaluated. The performance evaluation shows that the run-time overhead to support concurrent ARUs is negligible for Read and Write operations, and small but pronounced for file creation (4.0%-7.2%) and deletion (17.9%-20.5%) which mainly manipulate meta-data. The low overhead (when averaged over file creation, writing, reading, and deletion) for concurrent ARUs shows that issues of atomicity can be successfully isolated within the disk system.
[block lists, Laboratories, Mathematics, semantics, Runtime, File systems, log-structured logical disk system, deletion, Prototypes, file creation, writing, logical disk system, magnetic disc storage, logical block numbers, failure atomicity, performance evaluation, Data structures, reading, Concurrency control, file management, run-time overhead, Computer science, disk management, atomic recovery units, Power system protection, concurrency control, logical disks, Writing, file organisation, file meta-data]
Dynamic resource migration for multiparty real-time communication
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
With long-lived multi-party connections, resource allocation subsystems in distributed real-time systems or communication networks must be aware of dynamically changing network load in order to reduce call-blocking probabilities. We describe a distributed mechanism to dynamically reallocate ("migrate") resources without adversely affecting the performance that established connections receive. In addition to allowing systems to dynamically adapt to load, this mechanism allows for distributed relaxation of resources (i.e. the adjustment of overallocation of resources due to conservative assumptions at connection establishment time) for multicast connections. We describe how dynamic resource migration is incorporated in the Tenet Scheme 2 protocols for multiparty real-time communication.
[Real time systems, Protocols, distributed real-time systems, distributed mechanism, communication networks, multicast connections, Quality of service, distributed processing, call-blocking probabilities, Application software, Tenet Scheme 2 protocols, Delay, Computer science, resource allocation, transport protocols, real-time systems, dynamic resource migration, Bandwidth, multiparty real-time communication, resource allocation subsystems, Computer networks, Workstations, Resource management]
Object migration in non-monolithic distributed applications
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
Object migration is usually applied to optimize distributed monolithic systems. We investigate the effects of using object migration in cooperative systems which consist of autonomous, independently developed components. We show that the use of migration policies which are set up with only one component in mind can have detrimental effects on the overall performance. To avoid this without changing the internal structure of the components, we introduce two novel approaches: transient placement and reduction of attachment-transitiveness. The effects of these modifications are evaluated by simulation.
[nonmonolithic distributed applications, object-oriented programming, Object oriented modeling, distributed processing, Counting circuits, autonomous independently developed components, Degradation, object migration, Fault tolerance, migration policies, Runtime, attachment-transitiveness, cooperative systems, data encapsulation, Manufacturing automation, transient placement]
Distributed application configuration
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
This paper presents Olan, a language and a run time support intended to facilitate the design, configuration and evolution of distributed applications made up of heterogeneous software components. Configuration covers two phases: for the application builder the identification of the software components and the description of their interconnections and communications; for the application administrator and maintainer the accurate use of system resources provided by the target environment, such as the placement of components on nodes. The main benefit of Olan is to provide a single unified description of distributed applications, adequate for construction, management, and evolution. The overall description is independent from the components' implementation, so that the configuration process, e.g. the production of different versions of an implementation, is decoupled from the programming process. The features of the configuration language and the functions of the runtime support system are illustrated through the example of a distributed teleconferencing application.
[Access control, Software maintenance, Programming, distributed processing, teleconferencing, distributed applications, Olan, application builder, Runtime, Workstations, software components, application administrator, parallel languages, heterogeneous software components, configuration language, distributed teleconferencing application, LAN interconnection, Application software, application maintainer, configuration management, Teleconferencing, distributed application configuration, configuration process, Communication system software, programming process, runtime support system, object-oriented languages, software reusability, Software systems]
A wait-free algorithm for optimistic programming: HOPE realized
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
Optimism is a powerful technique for avoiding latency by increasing concurrency. Optimistically assuming the results of one computation allows other computations to execute in parallel, even when they depend on the assumed result. Optimistic techniques can particularly benefit distributed systems because of the critical impact of communications latency. This paper reviews HOPE: our model of optimistic programming, and describes how optimism can enhance distributed program performance by avoiding remote communications delay. We then present the wait-free algorithm used to implement HOPE in a distributed environment.
[wait-free algorithm, optimistic techniques, distributed environment, distributed program performance, Distributed computing, Power system modeling, Delay, parallel programming, concurrency, Concurrent computing, Computer science, Power engineering computing, communications latency, Prototypes, concurrency control, Parallel processing, Writing, distributed systems, programming environments, HOPE, Power engineering and energy, optimistic programming]
Conservative garbage collection on distributed shared memory systems
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
In this paper we present the design and implementation of a conservative garbage collection algorithm for distributed shared memory (DSM) applications that use weakly-typed languages like C or C++, and evaluate its performance. In the absence of language support to identify references, our algorithm constructed a conservative approximation of the set of cross-node references based on local information only. It was also designed to tolerate memory inconsistency on DSM systems that use relaxed consistency protocols. These techniques enabled every node to perform garbage collections without communicating with others, effectively avoiding the high cost of cross-node communication in networks of workstations. We measured the performance of our garbage collector against explicit programmer management using three application programs. In two out of the three programs the performance of the GC version is within 15% of the explicit version. The results showed that the garbage collector has two effects on application programs. On one hand, it tends to reduce memory locality, increasing the communication cost; on the other hand, it may eliminate synchronization and memory accesses that would be incurred if memory were managed by the programmer reducing the communication cost.
[Algorithm design and analysis, Costs, Protocols, memory inconsistency, Automatic logic units, C language, relaxed consistency protocols, storage management, shared memory systems, Workstations, protocols, conservative garbage collection, performance evaluation, communication cost, C++ language, Application software, explicit programmer management, Programming profession, synchronisation, weakly-typed languages, Computer science, cross-node references, Memory management, distributed memory systems, distributed shared memory systems, Approximation algorithms]
Portable and scalable algorithms for irregular all-to-all communication
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
In this paper we develop portable and scalable algorithms for performing irregular all-to-all communication in High Performance Computing (HPC) systems. To minimize the communication latency, the algorithm reduces the total number of messages transmitted, reduces the variance of the lengths of these messages, and overlaps the communication with computation. The performance of the algorithm is characterized using a simple model of HPC systems. Our implementations are performed using the Message Passing Interface (MPI) standard and they can be ported to various HPC platforms. The performance of our algorithms is evaluated on CM5, T3D and SP2. The results show the effectiveness of the techniques as well as the interplay between the architectural features, the machine size, and the variance of message lengths. The experiences of our study can be applied in other HPC systems to optimize the performance of collective communication operations.
[Algorithm design and analysis, message passing, Scalability, T3D, portable algorithms, performance evaluation, high performance computing systems, CM5, communication latency, message passing interface standard, Delay, Concurrent computing, scalable algorithms, Power engineering computing, Message passing, High performance computing, transport protocols, SP2, irregular all-to-all communication, Large-scale systems, Workstations, Power engineering and energy]
Context management and its applications to distributed transactions
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
An emerging paradigm that handles multiple locii of control in a system allows multiple program threads to work on the same task, each thread to work on a different task, or a thread to work on multiple tasks for greater design flexibility or due to system constraints such as real-time demands and a high load on tasking. We use the definition of context to capture the notion of logical locus of control. The context of the work being currently executed must be identifiable uniquely by the application, the Resource Managers and the Transaction Manager because each context represents different work. In this paper we define context management by defining a local Context Manager and its user interface. We then show why the notion of context is required to solve the problems that arise in local and distributed transaction processing due to the emerging paradigm. We present solutions to these problems in transaction processing using the proposed context management.
[Real time systems, transaction processing, Protocols, Resource Managers, distributed processing, Context Manager, File servers, Transaction Manager, Yarn, Distributed computing, Delay, user interface, Operating systems, context management, User interfaces, Trademarks, System recovery, distributed transactions, multiple locii, system constraints, multiple program threads, real-time demands]
Query execution strategies for missing data in distributed heterogeneous object databases
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
The problem of missing data arises in the distributed heterogeneous object databases because of the missing attribute conflict and the existence of null values. A set of algorithms are provided in this paper for query processing when the predicates in global queries involve missing data. For providing more informative answers to the users, the maybe results due to missing data are presented in addition to the certain results. One algorithm is designed based on the centralized approach in which the data are sent to the same site for integration and then processing. On the other hand, for reducing the response time, the localized approaches evaluate the predicates in different component databases in parallel. The proposed algorithms are compared and discussed by the simulation result on the total execution time and response time.
[Costs, Object oriented databases, Computational modeling, Computer simulation, object-oriented databases, localized approaches, query execution strategies, Delay, missing data, Computer science, query processing, missing attribute conflict, simulation result, Query processing, response time, Distributed databases, total execution time, distributed databases, Computer networks, null values, Time factors, distributed heterogeneous object databases]
An efficient causal ordering algorithm for mobile computing environments
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
Causal message ordering is required for several distributed applications. In order to preserve causal ordering, only direct dependency information between messages with respect to the destination process(es) should be sent with each message. By eliminating other kinds of control information from the messages, the communication overheads can be significantly reduced. In this paper we present an algorithm that uses this knowledge to efficiently enforce causal ordering of messages. The proposed algorithm does not require any prior knowledge of the network or communication topology. As computation proceeds, it acquires knowledge of the logical communication topology and is capable of handling dynamically changing multicast communication groups. With regard to communication overheads, the algorithm is optimal for the broadcast communication case. Its energy efficiency and four bandwidth requirement make it suitable for mobile computing systems. We present a strategy that employs the algorithm for causally ordered multicasting of messages in mobile computing environments.
[Protocols, causal message ordering, logical communication topology, control information, Communication system control, distributed processing, mobile computing environments, Mobile communication, causal ordering algorithm, Distributed computing, destination process, Wireless communication, Multicast algorithms, Network topology, communication topology, Bandwidth, Broadcasting, direct dependency information, communication overheads, bandwidth requirement, Mobile computing, portable computers, multicast communication groups]
An adaptive job allocation method for multicomputer systems
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
The fragmentation problem in multicomputer systems reduces the system utilization and prohibits the systems from performing at their full capacity. In this paper, we propose a generic job allocation method for multicomputer systems based on job size reduction. We reduce the subsystem size requirement adaptively according to the availability of processors. The fragmentation problem is greatly alleviated by this approach. To ensure that the benefit of reducing fragmentation is not outweighed by the penalty of executing jobs on less number of processors, we restrict the number of times the size of a job can be reduced; hence the name restricted size reduction (RSR). Extensive simulations are conducted to validate the RSR method for hypercubes and mesh-based systems with different allocation algorithms. It is observed in both mesh and hypercube that by using the RSR method a simple algorithm can provide better performance than the more sophisticated allocation algorithms. We have also compared RSR method with the limit allocation that is based on a similar idea. Our method outperforms the limit allocation and provides better fairness to different size jobs. The performance gain, fairness, and low complexity makes the RSR method highly attractive.
[Availability, job size reduction, Performance gain, hypercube networks, Topology, hypercubes, Scheduling algorithm, multicomputer systems, restricted size reduction, adaptive job allocation method, Processor scheduling, resource allocation, High performance computing, System performance, Hypercubes, system utilization, mesh-based systems, computational complexity, generic job allocation method]
Distributed priority queues on hypercube architectures
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
We efficiently map a priority queue on the hypercube architecture in a load balanced manner, with no additional communication overhead. Two implementations for insert and deletemin operations are proposed on the single-port hypercube model. In a b-bandwidth, n-item priority queue in which every node contains b items in sorted order, the first implementation achieves optimal speed-up of O[min{log n, b(log n)/(log b+log log n)}] for inserting b pre-sorted items or deleting b smallest items, where b=O(n/sup 1/c/) with c>1. In particular, single insertion and deletion operations are cost-optimal and require O(log n/p+log p) time using O(log n/log log n) processors. The second implementation is more scalable since it uses a larger number of processors, and attains a 'nearly' optimal speed-up on the single-port hypercube. The insertion of log n pre-sorted items or the deletion of log n smallest items requires O(log log n)/sup 2/ time and O(log/sup 2/ n/log log n) processors. However, on the slightly more powerful pipelined hypercube model, we are able to reduce the time complexity to O(log log n) thus attaining optimal speed-up. To the best of our knowledge, our algorithms provide the first implementations of b-bandwidth distributed priority queues, which are load balanced and yet guarantee optimal speed-up.
[Tree data structures, Redundancy, performance evaluation, time complexity, Data structures, hypercube networks, insert, distributed priority queues, Sorting, hypercube architectures, resource allocation, Computer architecture, Binary trees, Hypercubes, Load management, Cost function, deletemin, Classification tree analysis, computational complexity]
Data mining for path traversal patterns in a web environment
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
In this paper, we explore a new data mining capability which involved mining path traversal patterns in a distributed information providing environment like world-wide-web. First, we convert the original sequence of log data into a set of maximal forward references and filter out the effect of some backward references which are mainly made for ease of traveling. Second, we derive algorithms to determine the frequent traversal patterns, i.e., large reference sequences, from the maximal forward references obtained. Two algorithms are devised for determining large reference sequences: one is based on some hashing and pruning techniques, and the other is further improved with the option of determining large reference sequences in batch so as to reduce the number of database scans required. Performance of these two methods is comparatively analyzed.
[log data sequence, pruning, data mining, hashing, mining path traversal patterns, Data mining, large reference sequences, Filters, maximal forward references, distributed databases, Marketing and sales, Performance analysis, Stock markets, path traversal patterns, database scans, information retrieval, Spatial databases, Transaction databases, Association rules, world-wide-web, Computer science, backward references, distributed information providing environment, web environment, Artificial intelligence]
Synchronization protocols in distributed real-time systems
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
In many distributed real-time systems, the workload can be modeled as a set of periodic tasks, each of which consists of a chain of subtasks executing on different processors. Synchronization protocols are used to govern the release of subtasks so that the precedence constraints among subtasks are satisfied and the schedulability of the resultant system is analyzable. Tasks have different worst-case and average end-to-end response times when different protocols are used. In this paper, we consider distributed real-time systems with independent, periodic tasks and fixed-priority scheduling algorithms. We propose three synchronization protocols and conduct simulation to compare their performance with respect to the two timing aspects.
[Real time systems, Protocols, distributed real-time systems, schedulability, simulation, distributed processing, Displays, Delay, periodic tasks, synchronization protocols, scheduling, protocols, Monitoring, fixed-priority scheduling algorithms, precedence constraints, Computational modeling, subtasks, performance evaluation, workload, Sun, synchronisation, Computer science, Processor scheduling, performance, real-time systems, Timing, timing aspects]
A tool for massively replicating Internet archives: design, implementation, and experience
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
This paper reports the design, implementation, and performance of a scalable and efficient tool to replicate Internet information services. Our tool targets replication degrees of tens of thousands of weakly-consistent replicas scattered throughout the Internet's thousands of autonomously administered domains. The main goal of our replication tool is to make existing replication algorithms scale in today's exponentially-growing, autonomously-managed internetworks.
[weakly-consistent replicas, Scattering, World Wide Web, Floods, massively replicating Internet archives, information services, autonomously-managed internetworks, Information science, Content addressable storage, Network topology, Databases, performance, Web and internet services, Internet, replication algorithms, Contracts, Propagation delay, replication degrees]
Evaluation of a distributed single address space operating system
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
In this paper we present an evaluation of Angel, a single address space operating system, originally designed in 1992, which has been developed over the past few years. The purpose of this work was to examine how we could simplify the kernel and service structure using the single address space as the enabling technology. We believe that in so doing, we could achieve far better performance and simpler extensibility compared to other micro-kernel architectures. We recently completed a prototype system based around the PC/486 architecture which has allowed us to evaluate our claims and postulations. An initial analysis of this prototype has been performed to examine the system's structure and performance characteristics. This demonstrates an architecture which is more flexible, faster and smaller than monolithic operating systems.
[operating system kernels, enabling technology, Angel, kernel, service structure, microkernel architectures, distributed single address space operating system, PC/486 architecture, Computer science, Operating systems, Space technology, Message passing, Microprocessors, prototype system, network operating systems, Prototypes, Computer architecture, Hardware, performance characteristics, Data communication, Kernel]
Semantics for parameter passing in a type-complete persistent RPC
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
Current RPC mechanisms for persistent languages are either pass by reference-in which case they do not scale-or pass by copy-in which case they duplicate objects and destroy sharing relationships. In this paper we argue that to build very large distributed persistent applications a compromise between these two mechanisms is needed. The ultimate goal of our research is to build a scalable persistent RPC while still maintaining object sharing type safety, type completeness and semantics that are readily understood by application programmers.
[object sharing type safety, parameter passing, semantics, very large distributed persistent applications, Information systems, type-complete persistent RPC, Computer science, Graphics, Computer languages, Hospitals, Databases, type completeness, Computer applications, distributed databases, persistent languages, Aging, remote procedure calls, Computer networks, Joining processes, application programmers]
An evaluation of the Amoeba group communication system
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
The Amoeba group communication system has two unique aspects: (1) it uses a sequencer-based protocol with negative acknowledgements for achieving a total order on all group messages; and (2) users choose the degree of fault tolerance they desire. This paper reports on our design decisions in retrospect, the performance of the Amoeba group system, and our experiences using the system. We conclude that sequencer-based group protocols achieve high performance (comparable to Amoeba's fast remote procedure call implementation), that the scalability of our sequencer-based protocols is limited by message processing time, and that the flexibility and modularity of user-level implementations of protocols is likely to outweigh the potential performance loss.
[Protocols, fault tolerance, Scalability, Laboratories, performance evaluation, Amoeba group communication system, sequencer-based group protocols, Computer science, Fault tolerance, transport protocols, Fault tolerant systems, negative acknowledgements, performance loss, sequencer-based protocol, Broadcasting, Performance loss, fault tolerant computing, Power system reliability, Telecommunication network reliability]
Reducing the cost for non-blocking in atomic commitment
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
Non-blocking atomic commitment protocols enable a decision (commit or abort) to be reached at every correct participant, despite the failure of others. The cost for non-blocking implies however (1) a high number of messages and communication steps required to reach commit, and (2) a complicated termination protocol needed in the case of failure suspicions. In this paper, we present a non-blocking protocol, called MDSPC (Modular and Decentralized Three Phase Commit), which enables to trade resiliency against efficiency. As conveyed by our performance measures, MDSPC is faster than existing non-blocking protocols, and in the case of a broadcast network and a reasonable resiliency rate (e.g. 2 or 3) is almost as efficient as the classical (blocking) 2PC. The termination protocol of MDSPC is encapsulated inside a majority consensus protocol. This modularity leads to a simple structure of MDSPC and enables a precise characterization of its liveness in an asynchronous system with an unreliable failure detector.
[transaction processing, atomic commitment, Costs, Protocols, Nominations and elections, performance evaluation, liveness, complicated termination protocol, Computer crashes, nonblocking protocol, unreliable failure detector, performance measures, Delay, transport protocols, majority consensus protocol, asynchronous system, Detectors, broadcast network, Broadcasting, protocols, termination protocol, Contracts]
Hardware supports for efficient barrier synchronization on 2-D mesh networks
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
In this paper, we consider a hardware scheme for supporting barrier synchronization on scalable systems with a 2D mesh network. Our design takes into account of the program execution path in such systems-from programming interfaces down to routers. The hardware router design will be based on the MPI-1 standard. A distributed algorithm is proposed to construct a collective synchronization tree (CS tree) from the nodes participating in the barrier based upon the CS tree, the status registers in the routers are set up and synchronization messages are transmitted along the paths set by the status registers. Performance evaluations show that our proposed method has better performance for barrier synchronization and is less sensitive to variations in group size and startup delay than previous approaches. However our scheme has the extra overhead of building the CS tree. Thus it is more suitable for parallel iterative computations, in which the same barrier is invoked repetitively.
[programming interfaces, Computer worms, performance evaluations, multiprocessor interconnection networks, scalable systems, Network interfaces, Delay, synchronization messages, Concurrent computing, Mesh networks, Hardware, Distributed algorithms, status registers, parallel iterative computations, performance evaluation, program execution, Application software, synchronisation, hardware supports, MPI-1 standard, Computer science, distributed algorithm, distributed algorithms, barrier synchronization, Binary trees, collective synchronization tree, 2-D mesh networks]
Monotone response-time derivation for distributed execution of rule-based programs
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
A key index of the performance of a rule-based program used in real-time monitoring and control is its response time. We first extend the definition of response time of an EQL rule-based program for distributed computation. To reduce the response time through distributed computation, we decompose an EQL program into disjoint modules. We then describe a tool which computes the response-times of finite-state EQL rule-based programs according to the imprecise computation paradigm, i.e., this tool always yields a range which is monotonically tightened as more time is spent in the computation. During the computation, a user can interrupt the analyzer and get both an intermediate result which is a guaranteed bound and a bound-quality factor which quantifies the tightness of this result. Our approach uses fast textual analysis to get initial bounds. It then performs a heuristic search by pruning the state-transition graph to improve the bound quality. A program-decomposition technique for reducing the search effort is also discussed. An analysis example on an EQL program involving 2/sup 59/ states is presented.
[distributed processing, state-transition graph, search effort, finite state machines, Distributed computing, Delay, Space vehicles, monotone response-time derivation, EQL rule-based program, heuristic programming, guaranteed bound, distributed execution, rule-based programs, logic programming, textual analysis, real-time monitoring and control, program-decomposition technique, Computerized monitoring, Knowledge based systems, Space shuttles, Dynamic scheduling, distributed computation, disjoint modules, Equations, finite-state EQL, heuristic search, Processor scheduling, response time, initial bounds, real-time systems, bound-quality factor, tightness, Timing, imprecise computation paradigm]
A distributed web server and its performance analysis on multiple platforms
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
A fundamental trend for servers in network-centric computing environments is to evolve from traditional database and transaction servers to information distribution and handling systems. In addition to documents written in the HyperText Markup Language (HTML), data stored in other forms can be retrieved through the Common Gateway Interface (CGI). A significant performance bottleneck is the initialization and setup phase for a CGI process to gain access to a backend server. In this paper, we describe the design and implementation of distributed Web server for CGI processes to acquire services efficiently. A Connection Manager Daemon (CMD) is developed to provide a number of cliettes, which are connected to backend servers to eliminate initialization costs for incoming requests. A Cache Manager is implemented to speedup response time in case of repeated requests. We also trace and monitor the Connection Manager Daemon as well as its clients using extended UTE (Unified Trace Environment) tools, and present its performance analysis and visualization. The platforms where we conduct this study include a single-node workstation, a cluster of workstations, and an IBM Scalable Parallel (SP) system.
[network servers, distributed web server, network-centric computing environments, visualization, Common Gateway Interface, hypermedia, Cache Manager, HTML, Distributed computing, Network servers, database, information distribution and handling systems, HyperText Markup Language, initialization, Computer networks, multiple platforms, Performance analysis, Workstations, Web server, performance bottleneck, client-server systems, performance evaluation, Information retrieval, IBM Scalable Parallel system, Connection Manager Daemon, Transaction databases, Markup languages, transport protocols, response time, performance analysis, transaction servers]
Totally ordered multicast in large-scale systems
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
Totally ordered multicast protocols have proved to be extremely useful in supporting fault-tolerant distributed applications. This paper compares the performance of the two main classes of protocols providing total order in large-scale systems (token-site and symmetric protocols) and proposes a new dynamic hybrid protocol that, when applied to systems where the topology/traffic patterns are not known a priori, offers a much lower latency than any of the previous classes of protocols in isolation.
[fault-tolerant distributed applications, Switches, Telecommunication traffic, performance evaluation, Multicast protocols, Throughput, Topology, symmetric protocols, Delay, large-scale systems, token-site, Fault tolerance, performance, totally ordered multicast, Fault tolerant systems, multicast protocols, fault tolerant computing, Large-scale systems, dynamic hybrid protocol, protocols, Local area networks]
Scaling heterogeneous databases and the design of Disco
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
Access to large numbers of data sources introduces new problems for users of heterogeneous distributed databases. End users and application programmers must deal with unavailable data sources. Database administrators must deal with incorporating new sources into the model. Database implementers must deal with the translation of queries between query languages and schemas. The Distributed Information Search COmponent (Disco) addresses these problems. Query processing semantics are developed to process queries over data sources which do not return answers. Data modeling techniques manage connections to data sources. The component interface to data sources flexibly handles different query languages and translates queries. This paper describes (a) the distributed mediator architecture of Disco, (b) its query processing semantics, (C) the data model and its modeling of data source connections, and (d) the interface to underlying data sources.
[distributed information search component, data model, Costs, heterogeneous databases, query languages, data modeling techniques, Database languages, query processing, Distributed databases, distributed databases, Database systems, data structures, data sources, Educational institutions, query processing semantics, Spatial databases, distributed mediator architecture, Programming profession, end users, Query processing, Disco, schemas, Data models, Catalogs, application programmers]
Language support for long-lived concurrent activities
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
Providing a general purpose programming environment that supports the definition of, and exercises control over, the flow of execution of long-running activities is highly beneficial for a variety of client/server distributed data-intensive applications. In this paper, we present a Transaction-Oriented Work-Flow Environment (TOWE) for the programming of long-lived activities through a set of class libraries. The TOWE is based on an amalgamation of object-oriented programming with distributed interprocess communication concepts. The concurrency abstractions provided by TOWE are objects, acting like processes, and involve an asynchronous, location-independent, mode of process invocation coupled with data-driven synchronization of processes.
[client-server systems, object-oriented programming, general purpose programming environment, Object oriented modeling, client/server distributed data-intensive applications, Control systems, distributed interprocess communication concepts, language support, class libraries, Application software, transaction-oriented work-flow environment, software libraries, Information systems, Programming environments, Concurrent computing, Computer science, concurrency abstractions, concurrency control, long-lived concurrent activities, Libraries, Australia, Object oriented programming, programming environments, data-driven synchronization]
Multi-dimensional locks with on-line allocation schemes
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
Synchronization is inherent to parallel computing and comprises many aspects. One aspect is the access control to shared data, especially important in MIMD shared-memory architectures. Several locking mechanisms have been proposed to perform this task. Most of these schemes rely on very little hardware support, such as atomic read-modify-write operations. Today, the amount of hardware required to implement a locking mechanism is not a major constraint and will be even less important in the future. This makes it interesting to investigate schemes with more flexibility, better performance, and improved ease of use, that rely more on hardware assisted circuits than previous suggestions. In this paper multi-dimensional locking schemes that allocate locks on-line are presented. A lock is assigned to the access control of a data structure only during the time this structure needs to be locked. Three specific schemes are proposed. Two offer the possibility of locking sub-structures within multi-dimensional arrays. All three schemes protect locked structures against accesses of misbehaving threads.
[Access control, atomic read-modify-write operations, Circuits, Laboratories, MIMD shared-memory architectures, data structure, performance evaluation, Data structures, misbehaving threads, Yarn, synchronisation, Databases, security of data, National electric code, Parallel processing, shared memory systems, synchronization, Hardware, access control, data structures, online allocation schemes, Protection, parallel computing, multidimensional locks]
Real-time scheduling using compact task graphs
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
The generation of a real-time schedule from a task precedence graph is complex and time consuming. In order to improve the efficiency of generating schedules, we propose a scheduling algorithm based upon the compact task graph (CTG) representation. In addition to precedence constraints, a CTG explicitly expresses the potential for interleaving the execution of tasks on a single processor and overlapping the execution of task on multiple processors. The CTG is compact since it expresses the potential for overlapping and interleaving without the generation of smaller tasks. If a task cannot be scheduled to meet its deadline through interleaving and overlapping, then selected tasks are split into smaller tasks which increases the feasibility, and hence the complexity, of scheduling the task. Thus, in effect our approach increases the complexity of scheduling a usual task graph only if it is essential. Our results demonstrate that a significant reduction in the time of scheduling is achieved using CTGs and our algorithms that exploit the CTGs. We also show how CTGs can be used for distributed on-line scheduling.
[complexity, Law, scheduling algorithm, Scheduling algorithm, processor scheduling, Computer science, Degradation, Processor scheduling, task precedence graph, real-time systems, compact task graphs, Parallel processing, Tin, Interleaved codes, Timing, Resource management, real-time scheduling, computational complexity]
Lock-based self-stabilizing distributed mutual exclusion algorithms
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
In 1974, Dijkstra introduced the notion of self-stabilization and presented a token circulation distributed mutual exclusion (DMX) protocol as the first self-stabilizing (SS) algorithm. Since then, many variations of SS DMX algorithms have been presented. Most, if not all, of these algorithms impose stronger assumptions on their execution environments than those provided by common distributed systems. Independently, non SS DMX algorithms have been studied extensively in the last 15 years. This paper presents two SS DMX algorithms that are based on existing non SS DMX algorithms: one is based on a link-locking algorithm and the other is on a node-locking algorithm. Our algorithms assume execution environments that are close to those provided by common distributed systems. Furthermore, they provide better synchronization delays than token circulation SS DMX algorithms. We have implemented our algorithms and tested them with various initial configurations.
[Context, synchronization delays, common distributed systems, Protocols, lock-based self-stabilizing distributed mutual exclusion algorithms, execution environments, link-locking algorithm, Delay, synchronisation, transport protocols, Fault tolerant systems, distributed algorithms, delays, token circulation distributed mutual exclusion protocol, Permission, Testing, SS DMX algorithms]
Fault-tolerant clock synchronization of large multicomputers via multistep interactive convergence
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
We present a fault-tolerant algorithm that internally synchronizes clocks in multicomputer systems employing not completely connected networks (NCCNs). The algorithm is referred to as multistep interactive convergence, and is locally implemented in each node by a time sewer process (TSP). The algorithm proceeds in rounds, and bases its operation on a logical mapping of the system's TSPs into an m-dimensional array. A TSP executes m steps per round, each step including a call to an interactive convergence procedure. Clock readings in step i are gathered only from TSPs sharing a row along dimension i of the array, which reduces the number of messages by orders of magnitude over a conventional interactive convergence algorithm. The algorithm can be used in systems of arbitrary topology, and provides the added benefit of increased locality of communication in regular NCCNs. These advantages can be combined with a variety of message staggering mechanisms to maintain network contention at a minimum. We characterize the maximum clock skew maximum clock drift, maximum clock discontinuity, and number of messages produced by the algorithm, and show that it tolerates arbitrary faults. A comparison with other algorithms is provided.
[interactive convergence procedure, Protocols, Costs, multiprocessor interconnection networks, not completely connected networks, network contention, fault-tolerant clock synchronization, Convergence, Fault tolerance, Network servers, message staggering mechanisms, Fault tolerant systems, multistep interactive convergence, Computer networks, protocols, interactive convergence algorithm, time sewer process, large multicomputers, maximum clock discontinuity, m-dimensional array, maximum clock skew maximum clock drift, Synchronization, synchronisation, logical mapping, fault tolerant computing, Logic arrays, Clocks]
A new token passing distributed mutual exclusion algorithm
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
Eliminating interference between concurrently executing activities through mutual exclusion is one of the most fundamental problems in computer systems. The problem of mutual exclusion in a distributed system is especially interesting owing to the lack of global knowledge in the presence of variable communication delays. In this paper, a new token-based distributed mutual exclusion algorithm is proposed. The algorithm incurs approximately three messages at high loads, irrespective of the number of nodes N in the system. At low loads, it requires approximately N messages. The paper also addresses failure recovery issues, such as token loss.
[client-server systems, Taxonomy, token passing distributed mutual exclusion algorithm, Communication system control, distributed system, Control systems, Telecommunication computing, Delay, Concurrent computing, Analytical models, Interference elimination, distributed algorithms, failure recovery issues, delays, System recovery, Performance analysis, token loss, protocols, communication delays, concurrently executing activities]
Verifiable transaction atomicity for electronic payment protocols
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
We study the transaction atomicity problem for designing electronic payment protocols in distributed systems. We observe that the techniques that are used to guarantee transaction atomicity in a database system are not robust enough to guarantee transaction atomicity in an electronic payment system, in which a set of dishonest or malicious participants may exhibit unpredictable behavior and cause arbitrary failures. We present a new concept-verifiable transaction atomicity-for designing electronic payment protocols. We give formal specifications to the verifiable atomic commitment problem. Then we design a robust electronic currency system to meet the specifications and achieve the verifiable transaction atomicity.
[transaction processing, EFTS, Protocols, Document handling, robust electronic currency system, formal specification, formal specifications, Network servers, electronic payment protocols, database system, verifiable transaction atomicity, Authentication, concurrency control, distributed databases, distributed systems, Robustness, Database systems, protocols]
An approach for constructing mobile applications using service proxies
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
In this paper, we propose software architecture using a notion of service proxies, that is a new framework for constructing applications in mobile computing environment. Our framework especially takes into account applications accessing services on Internet such as WWW browsers and MBONE conference tools. In our framework, an application is partitioned into two pieces, one piece runs on a mobile computer, and another piece runs on a stationary computer, where they are connected by wireless networks that may be replaced on the fly. The piece on a stationary computer is called a service proxy. The service proxy filters or caches data from servers before transmitting the data to the piece on a mobile computer. These two pieces are constructed by composing small objects whose composition can be dynamically reconfigured by adding or removing replaceable devices. The paper also presents a toolkit using our framework for building mobile applications easily.
[stationary computer, World Wide Web, wireless networks, Application software, MBONE conference tools, Network servers, software architecture, Filters, Software architecture, Wireless networks, Web and internet services, WWW browsers, toolkit, Computer applications, mobile applications, mobile computing environment, Computer networks, software engineering, Internet, wireless LAN, service proxies, Mobile computing, service proxy filters]
Making trust explicit in distributed commerce transactions
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
In a distributed environment where nodes are independently motivated, many transactions or commercial exchanges may be stymied due to a lack of trust between the participants. The addition of trusted intermediaries may facilitate some exchanges, but others are still problematic. We introduce a language for specifying these commercial exchange problems, and sequencing graphs, a formalism for determining whether a given exchange may occur We also present an algorithm for generating a feasible execution sequence of pairwise exchanges between parties (when it exists). Indemnities may be offered to facilitate previously infeasible transactions. We show when and how they enable commercial transactions.
[trusted intermediaries, NASA, cryptography, distributed environment, Electronic commerce, sequencing graphs, Concurrent computing, Computer science, distributed commerce transactions, Software libraries, Authentication, message authentication, Computer networks, Marketing and sales, financial data processing, Protection, commercial transactions, Business]
Hidden software capabilities
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
Software capabilities are a very convenient means to protect co-operating applications. They allow access rights to be dynamically exchanged between mutually suspicious interacting applications. However in all the proposed approaches, capabilities are made available at the programming language level, requiring application developers to wire protection definition in the application code, which is detrimental to both flexibility and reusability. We believe instead that capabilities should be hidden from the application programmer allowing protection definition and application code to be clearly separated. In this paper we propose a new protection model based on hidden software capabilities, in which protection definition is completely disjoined from the application code and described in an extended interface definition language (IDL). This allows to specify protection for existing modules and to easily change the protection policy of an application. This protection model can be integrated in a wide range of operating systems. We are currently implementing it in a single address space operating system based on distributed shared memory.
[hidden software capabilities, mutually suspicious interacting applications, distributed shared memory, co-operating applications, application programmer, application developers, operating system, software capabilities, extended interface definition language, Application software, Wire, Distributed computing, Programming profession, Computer languages, application code, security of data, Operating systems, Memory management, distributed memory systems, Permission, Protection, Kernel]
A semi-automated verification method for communication protocols modeled as 2-ECFSMs
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
Previously, we proposed a verification method via invariants for communication protocol modeled as 2-ECPSMs. In the proposed method, a human verifier describes an invariant of a given protocol in a disjunctive normal form, and a verification system shows safety or liveness based on the invariant. The tedious work on describing invariant formulae is the most significant shortcoming of the proposed method. This paper deals with a semi-automated derivation of invariant formulae for communication protocol modeled as 2-ECFSMs. In the method, the logical formula which holds on a subset of reachable states is automatically generated. Such a subset consists of states which are teachable by synchronous communication from the initial states and those which are reachable by sequences of sending transitions from synchronously reachable states. To obtain an invariant, a human verifier supplements several disjuncts for other part of reachability set. We conducted an experiment on deriving an invariant formula of a sample protocol extracted from the OSI session protocol. By the result, 636 conjunctive formulae were automatically derived and the conjunction of those formulae was shown to be an invariant of the sample protocol, i.e. the sample protocol was shown to be safe automatically.
[Context, Protocols, Data analysis, open systems, Petri nets, Humans, Communication system control, communication protocols, liveness, human verifier, Software safety, finite state machines, 2-ECFSMs, Turing machines, formal verification, disjunctive normal form, safety, Open systems, Communication channels, semi-automated verification method, OSI session protocol, protocols, invariant formula]
Network architecture for mobile and wireless ATM
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
There is an emergent interest in providing mobile users with ubiquitous wireless access to multimedia information. In this paper we present a network architecture and protocols to achieve this goal and describe their implementation in a prototype network called SWAN. Our network model assumes end to end ATM connectivity. Thus, the key question we address is how best to enhance ATM to support host mobility and wireless access.
[Virtual colonoscopy, Spine, ubiquitous wireless access, Access protocols, Multimedia computing, Mobile communication, Ubiquitous computing, asynchronous transfer mode, end to end ATM connectivity, land mobile radio, network architecture, multimedia information, transport protocols, Prototypes, SWAN, Computer architecture, host mobility, data communication, Computer networks, mobile users, wireless LAN, protocols, multimedia communication, Asynchronous transfer mode]
General connection rerouting method for various connection-oriented mobile communication networks
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
To date, various connection rerouting methods for connection-oriented mobile network services have been proposed. The previous methods, however, are limited to specific topologies or environments. In this paper, we propose the CI (Connection Information)-based rerouting widely applicable to various connection-oriented mobile network. This method requires neither a specific topology nor a complex connection, enables fast rerouting, takes a good trade-off between rerouting simplicity and route optimality, and can be extended easily.
[internetworking, fast rerouting, Mobile communication, Routing, asynchronous transfer mode, route optimality, land mobile radio, connection-oriented mobile communication networks, computer network management, Network topology, rerouting simplicity, Web and internet services, telecommunication network routing, digital communication, data communication, Computer networks, general connection rerouting method, Personal communication networks, IP networks, Mobile computing]
Route optimization and location updates for mobile hosts
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
Mobile hosts in a wireless network can move from one location to another while communicating with other hosts. A challenge is to provide seamless network access for mobile hosts and, at the same time, to retain compatibility with existing network protocols and applications. This paper addresses the issue of route optimization in IP mobility support that provides mobile handoff and "triangle" routing to mobile hosts through their home agents. We combine IP mobility support with hierarchical dynamic routing protocols OSPF and BGP. Existing mechanisms of authentication, incremental route propagation, and address aggregation can be used for efficient and secure propagation of location updates of mobile hosts. The geographical locality of consecutive mobile handoffs fits well with hierarchical dynamic routing protocols. No changes are required on fixed hosts or on routers that do not handle mobile hosts directly.
[mobility support, Home automation, Portable computers, internetworking, land mobile radio, home agents, location updates, Wireless networks, Mobile agents, mobile hosts, Routing protocols, Educational programs, hierarchical dynamic routing protocols, Educational technology, route optimization, BGP, incremental route propagation, network protocols, Computer science, address aggregation, computer network management, transport protocols, consecutive mobile handoffs, Authentication, telecommunication network routing, data communication, Mobile computing, OSPF]
To send or not to send: implementing deferred transmissions in a mobile host
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
As mobile hosts move, they encounter changing network characteristics. Characteristics such as bandwidth, reliability can change drastically when a mobile host moves from indoor to outdoor environment and vice versa. A mobile host can find itself in a lossy environment due to external conditions such as interference and fading. Existing end-to-end protocols are designed with an assumption that packet loss is a random and rare event. However due to mobility of hosts, there can be transient yet significant packet loss. In this paper we consider the effect of mobility on unreliable protocols such as UDP. We provide a mechanism by which unnecessary loss of packets can be avoided by allowing transmission only under "good" link conditions.
[Protocols, internetworking, reliability, end-to-end protocols, packet loss, Mobile communication, land mobile radio, Intelligent networks, mobile host, Bandwidth, Internetworking, Propagation losses, unreliable protocols, computer network reliability, Fading, UDP, bandwidth, Interference, Radio broadcasting, fading, deferred transmissions, Computer science, link conditions, changing network characteristics, transport protocols, lossy environment, data communication, interference]
A generic protocol for multipoint connections under link-state routing
Proceedings of 16th International Conference on Distributed Computing Systems
None
1996
We propose a protocol for the construction and maintenance of multipoint connections (MCs). The protocol is based on link-state routing: information regarding multipoint connections is broadcast to network switches, which perform all computations locally. The protocol is generic in that it can be used with MCs of different types, including symmetric MCs, receiver-only MCs, and asymmetric MCs. Results of a simulation study show that this generality can be achieved with negligible (in normal traffic periods) to moderate (in very busy periods) signaling overhead.
[telecommunication congestion control, internetworking, generic protocol, Switches, Telecommunication traffic, Multicast protocols, Distributed computing, telecommunication switching, multipoint connections, Computer science, signaling overhead, computer network management, switching networks, transport protocols, telecommunication network routing, Broadcasting, link-state routing, Routing protocols, Computer networks, network switches, IP networks, Asynchronous transfer mode]
Index Of Authors, Speakers, Moderators And Chairs
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
Presents an index of the authors whose papers are published in the conference.
[]
Distributed recovery with K-optimistic logging
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
Fault-tolerance techniques based on checkpointing and message logging have been increasingly used in real-world applications to reduce service downtime. Most industrial applications have chosen pessimistic logging because it allows fast and localized recovery. The price that they must pay, however, is the higher failure-free overhead. In this paper, we introduce the concept of K-optimistic logging where K is the degree of optimism that can be used to fine-tune the tradeoff between failure-free overhead and recovery efficiency. Traditional pessimistic logging and optimistic logging then become the two extremes in the entire spectrum spanned by K-optimistic logging. Our approach is to prove that only dependencies on those states that may be lost upon a failure need to be tracked on-line, and so transitive dependency tracking can be performed with a variable-size vector. The size of the vector piggybacked on a message then indicates the number of processes whose failures may revoke the message, and K corresponds to the system-imposed upper bound on the vector size.
[Checkpointing, checkpointing, pessimistic logging, Buffer storage, computer networks, recovery efficiency, Control systems, upper bound, Telecommunication control, optimistic logging, variable-size vector, system recovery, K-optimistic logging, message logging, Fault tolerance, Upper bound, Fault tolerant systems, localized recovery, Wood industry, transitive dependency tracking, fault tolerant computing, fault-tolerance techniques, distributed recovery, failure-free overhead]
Fast dissemination of link states using bounded sequence numbers with no periodic updates or age fields
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
Routing protocols based on the distribution of link-state information rely on sequence numbers to validate information that a router receives. A fundamental problem is to bound the sequence-number space. We propose a new sequence-number reset algorithm that needs neither periodic retransmissions nor age fields. It is based on a recursive query-response procedure and is designed to handle resource failures during operation. This new algorithm is applicable to routing protocols based on both flooding and selective distribution of link-state information. The correctness of the algorithm is verified in the context of selective dissemination of topology information, and its complexity analyzed. Because the reset algorithm does not use any aging, the distribution of new link-state information or the purging of old information is always done in a time proportional to the time it takes to traverse the network.
[Algorithm design and analysis, link states dissemination, sequence-number reset algorithm, selective distribution, periodic updates, flooding, Floods, Distributed computing, bounded sequence numbers, recursive query-response procedure, Computer science, Network topology, routing protocols, resource failures, Aging, Broadcasting, link-state information, Routing protocols, age fields, Internet, IP networks, protocols, computational complexity]
Distributed dynamic carrier allocations in mobile cellular networks: search vs. update
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
There are two approaches to distributed implementation of dynamic carrier allocation (DCA) strategies: the update approach and the search approach. We first investigate the fundamental differences between the two approaches: two simple and representative schemes, namely the basic update scheme and the basic search scheme, are presented and compared. We argue that the update approach is more appropriate than the search approach for mobile cellular networks. Then we propose an advanced update scheme that is more efficient than the basic update scheme in terms of message complexity and carrier acquisition delay. The advanced update scheme can support a group of DCA strategies which require resource planning.
[update approach, mobile cellular networks, Strategic planning, communication complexity, Distributed computing, Delay, Intelligent networks, Information science, Time division multiple access, basic update scheme, message complexity, resource allocation, distributed implementation, DCA strategies, Bandwidth, distributed dynamic carrier allocations, resource planning, Base stations, frequency division multiple access, TDMA, computer networks, advanced update scheme, search approach, carrier acquisition delay, Frequency division multiaccess, Land mobile radio cellular systems, time division multiple access, basic search scheme, FDMA multiplexing, cellular radio]
Group communication support for distributed collaboration systems
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
The Collaborative Computing Transport Layer (CCTL) is a communication substrate consisting of a suite of multiparty protocols, providing varying service qualities among process groups. CCTL explicitly supports distributed collaborative and multimedia applications. CCTL is based on a two-level group hierarchy. Logical interconnections among entities, called channels, define an efficient and light-weight group mechanism. Channels support a variety of service qualities such as reliability and message ordering. Related channels can also be combined to form sessions, heavy-weight groups which provide a default atomic multicast service. CCTL supports membership protocols tailored to the quality of service offered by a channel, including a relaxed form of virtual synchrony. In this paper, we present three membership protocols and compare and relate our implementations to alternatives. Our two-level architecture allows simple and efficient implementation of the membership protocols.
[Transport protocols, multiparty protocols, Quality of service, reliability, multimedia systems, virtual synchrony, distributed collaboration systems, Mathematics, Concurrent computing, membership protocols, groupware, process groups, message ordering, protocols, Collaborative Computing Transport Layer, Video sharing, default atomic multicast service, group communication support, Application software, two-level group hierarchy, Computer science, heavy-weight groups, Collaboration, Streaming media, Collaborative work, communication substrate, service qualities]
The hierarchical daisy architecture for causal delivery
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
We propose the hierarchical daisy architecture, which provides causal delivery of messages sent to any subset of processes. The architecture provides fault tolerance and maintains the amount of control information within a reasonable size. It divides processes into logical groups. Messages inside a logical group are sent directly, while messages that need to cross logical group boundaries are forwarded by servers. We prove the correctness of the daisy architecture and discuss possible optimizations.
[correctness proving, Protocols, message passing, Event detection, Stability, program verification, fault tolerance, Particle separators, parallel architectures, control information, Communication system control, Control systems, Added delay, distributed computing, Fault tolerance, logical group boundaries, Network topology, servers, causal delivery, Intersymbol interference, fault tolerant computing, hierarchical daisy architecture]
WebWave: globally load balanced fully distributed caching of hot published documents
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
Document publication service over such a large network as the Internet challenges us to harness available server and network resources to meet fast growing demand. We show that large scale dynamic caching can be employed to globally minimize server idle time, and hence maximize the aggregate server throughput of the whole service. To be efficient, scalable and robust, a successful caching mechanism must have three properties: (1) maximize the global throughput of the system; (2) find cache copies without recourse to a directory service, or to a discovery protocol; and (3) be completely distributed in the sense of operating only on the basis of local information. We develop a precise definition, which we call tree load balance (TLB), of what it means for a mechanism to satisfy these three goals. We present an algorithm that computes TLB offline, and a distributed protocol that induces a load distribution that converges quickly to a TLB one. Both algorithms place cache copies of immutable documents on the routing tree that connects the cached document's home server to its clients, thus enabling requests to stumble on cache copies en route to the home server.
[aggregate server throughput, Protocols, electronic publishing, Throughput, cache storage, caching mechanism, global throughput, Network servers, globally load balanced fully distributed caching, resource allocation, Web and internet services, hot published documents, Robustness, Large-scale systems, IP networks, cache copies, tree load balance, Web server, home server, routing tree, trees (mathematics), WebWave, load distribution, immutable documents, Mechanical factors, server idle time, distributed protocol, large scale dynamic caching, document publication service, Aggregates, distributed algorithms, network resources, Internet]
Design and implementation of an RSVP-based quality of service architecture for integrated services Internet
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
The paper presents the design and implementation of a quality of service architecture for the Internet. The architecture is based on the emerging standards for resource reservation in the Internet, namely the RSVP protocol and the associated service specifications defined by the Internet Engineering Task Force. Our architecture represents a major functional enhancement to the traditional sockets based communication subsystem, while preserving application programming interface and binary compatibility with existing applications. It is scalable and supports a variety of network interfaces ranging from legacy LAN interfaces, such as token ring and Ethernet, to high speed ATM interfaces. We also describe our initial experiences with the implementation of this architecture on the IBM AIX platform.
[Protocols, binary compatibility, application programming interface, Quality of service, legacy LAN interfaces, Network interfaces, network interfaces, Reduced instruction set computing, Web and internet services, RSVP based quality of service architecture, functional enhancement, high speed ATM interfaces, computer network reliability, protocols, integrated services Internet, resource reservation, token ring, sockets based communication subsystem, Rivers, Internet Engineering Task Force, RSVP protocol, Milling machines, telecommunication standards, IBM AIX platform, Sockets, Ethernet, Intserv networks, Internet, Resource management, service specifications]
Comprehensive distributed garbage collection by tracking causal dependencies of relevant mutator events
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
Comprehensive distributed garbage collection in object-oriented distributed systems has mostly been addressed via distributed versions of graph-tracing algorithms, a legacy of centralised garbage collection techniques. Two features jeopardise the scalability of these approaches: the bottleneck associated with having to reach a global consensus before any resource can actually be reclaimed; and the overhead of eager log-keeping. This paper describes an alternative approach to comprehensive distributed garbage collection that entails computing the vector-time characterising the causal history of some relevant events of the mutator processes computations. Knowing the causal histories of these events makes it possible to identify garbage objects that are not identifiable by means of per-site garbage collection alone. Computing the vector-times necessary to identify garbage is possible without the unbounded space overheads usually associated with dynamically reconstructing vector-times of arbitrary events of distributed computations. Our approach integrates a lazy log-keeping mechanism and therefore tackles both of the aforementioned stumbling blocks of distributed garbage collection.
[Scalability, graph theory, centralised garbage collection, History, Distributed computing, Yarn, scalability, storage management, distributed databases, bottleneck, Robustness, unbounded space overheads, object-oriented databases, Educational institutions, Hazards, causal dependency tracking, global consensus, causal histories, database theory, Computer science, object-oriented distributed database, distributed computations, distributed algorithms, Object detection, comprehensive distributed garbage collection, relevant mutator events, vector-time, graph-tracing algorithms, Resource management, log-keeping]
Client/server qualities: a basis for reliable distributed workflow management systems
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
In distributed workflow management systems (WfMSs) many workflow servers and application programs referenced within workflows have to work together cooperatively. We introduce a middleware service for WfMSs which offers qualities for client/server communication. These qualities characterize the behavior of clients and servers in case of failure and recovery. Our middleware service builds an abstraction from the concrete implementation of a client/server interaction and hides characteristics of underlying system services, e.g. RPC mechanisms and TP monitors. For this reason, our middleware service supports the implementation of reliable distributed WfMSs.
[Protocols, concrete implementation, Scalability, RPC mechanisms, Engines, TP monitors, Condition monitoring, underlying system services, reliable distributed WfMSs, application programs, client/server communication, client/server interaction, Workflow management software, Quality management, client-server systems, reliable distributed workflow management systems, workflow servers, abstraction, Application software, Middleware, software fault tolerance, Computer science, middleware service, client/server qualities, distributed algorithms, remote procedure calls, Concrete, office automation]
A quality of service based allocation and routing algorithm for distributed, heterogeneous real time systems
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
An increasing number of applications execute over a set of computing and communication resources and have end-to-end quality of service (QoS) requirements. Given a system and an application with specific flow structure and QoS requirements, this paper describes an integrated QoS-based allocation and routing algorithm that determines which computing and communication resources to utilize for this application. More specifically, the algorithm finds the best flow path for an application, subject to application flow and QoS constraints, so as to optimize system objectives (e.g., minimize total system utilization or balance the load). The paper also introduces a /spl Delta/-optimal algorithm that, relative to the optimal algorithm, significantly reduces the run-time of the algorithm with a minor degradation in the optimality of the solution. The /spl Delta/-optimal algorithm is guaranteed to produce a result that is at most /spl Delta/-units sub-optimal. The paper also analyzes the optimality and run time of the solution as a function of /spl Delta/ for two multimedia scenarios. Rules of thumb for setting /spl Delta/ are also given.
[Real time systems, load balancing, Quality of service, flow structure, distributed processing, flow path, multimedia computing, Distributed computing, Constraint optimization, Network servers, Runtime, optimisation, resource allocation, optimization, Computer networks, multimedia communication, optimal algorithm, application flow constraints, quality control, distributed heterogeneous real time systems, communication resources, Routing, quality of service, run-time, Application software, total system utilization, multimedia, routing algorithm, real-time systems, telecommunication network routing, allocation algorithm, Resource management, QoS requirements]
A reflective model for mobile software objects
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
Mobile software objects are autonomous computational entities that travel in large-scale and widely-distributed heterogeneous systems, and whose functionality can be attached to diverse computing environments. An object model that supports mobile objects should have special characteristics such as mutability of object's structure and semantics to facilitate adjustment to different environments, self-containment of objects to allow their migration as autonomous units, and extensive support for security. We discuss the requirements and design guidelines of such a model, and present MROM, a reflective model based on these guidelines. We also discuss MROM's implementation and present a component interoperability framework that was built on top of it, as an example application of the model.
[design guidelines, semantics, Security, Distributed computing, mutable reflective object model, Guidelines, component interoperability framework, security, Bandwidth, Cities and towns, Computer networks, Large-scale systems, mobile software objects, Java, Object oriented modeling, object self-containment, widely-distributed heterogeneous systems, MROM, large-scale systems, CORBA, software portability, autonomous computational entities, object model, Mobile computing, reflective model]
Comparison of two middleware data dissemination services in a wide-area distributed system
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
The paper provides an experimental comparison of two middleware data dissemination services: a distributed object based service, and a message based service. The paper compares these two services in the context of a common application: a wide area network collaboratory, namely the Upper Atmospheric Research Collaboratory (UARC). UARC is an example of an application that reliably streams data from a set of suppliers to a set of receivers. This comparison highlights the tradeoffs between ease of implementation and performance for a data streaming middleware service. By relying on a rigid language primitive, namely remote method invocation, the object based dissemination service gave up the control over its transport policies. In contrast, the lower level socket based service was specifically constructed to provide a flexible interface to its applications. This flexibility allowed the middleware to better support data delivery to a heterogeneous set of receivers. This is important in a wide area distributed system where hosts are connected together over a broad spectrum of network links. The paper provides a concrete example of the effects of high level design choices in the implementation of a wide area distributed system's communication middleware.
[distributed object based service, wide area networks, object based dissemination service, rigid language primitive, communication middleware, wide area distributed system, high level design choices, network links, Focusing, message based service, UARC, Upper Atmospheric Research Collaboratory, Biomedical imaging, Context-aware services, object-oriented programming, message passing, Instruments, information dissemination, geophysics computing, lower level socket based service, data streaming middleware service, Application software, Middleware, middleware data dissemination services, Collaboration, remote method invocation, Collaborative work, Concrete, Internet, wide area network collaboratory, data delivery]
Supporting dynamic space-sharing on clusters of non-dedicated workstations
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
Clusters of workstations are increasingly being viewed as a cost effective alternative to parallel supercomputers. However, resource management and scheduling on workstations clusters is complicated by the fact that the number of idle workstations available for executing parallel applications is constantly fluctuating. We present a case for scheduling parallel applications on non dedicated workstation clusters using dynamic space sharing, a policy under which the number of processors allocated to an application can be changed during its execution. We describe an approach that uses application level checkpointing and data repartitioning for supporting dynamic space sharing and for handling the dynamic reconfiguration triggered when failure or owner activity is detected on a workstation being used by a parallel application. The performance advantages of dynamic space sharing are quantified through a simulation study, and experimental results are presented for the overhead of dynamic reconfiguration of a grid oriented data parallel application using our approach.
[Checkpointing, resource management, parallel application scheduling, application level checkpointing, processor allocation, processor scheduling, dynamic space sharing, Concurrent computing, Fault tolerance, parallel supercomputers, resource allocation, data repartitioning, grid oriented data parallel application, dynamic reconfiguration, Workstations, parallel algorithms, idle workstations, Dynamic scheduling, Application software, non dedicated workstation clusters, Computer science, workstations, Processor scheduling, Fault detection, Resource management]
Distributed data mining of probabilistic knowledge
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
We present a distributed approach to data mining of a knowledge representation scheme known as Bayesian belief networks which are capable of dealing with uncertain knowledge. We make use of a machine learning paradigm and a distributed asynchronous search technique to achieve the task of distributed knowledge discovery from data. Our approach boasts a number of features, including dynamic load balancing and fault tolerance. Empirical experiments have been conducted to illustrate its feasibility, solving large scale Bayesian network discovery problems with multiple workstations.
[Knowledge engineering, multiple workstations, Data engineering, uncertainty handling, probabilistic knowledge, Data mining, Fault tolerance, uncertain knowledge, resource allocation, distributed knowledge discovery, Workstations, learning (artificial intelligence), knowledge representation scheme, distributed asynchronous search technique, fault tolerance, knowledge acquisition, Knowledge representation, software fault tolerance, machine learning paradigm, distributed approach, Bayesian methods, distributed algorithms, knowledge representation, distributed data mining, large scale Bayesian network discovery problems, Systems engineering and theory, Load management, Bayes methods, Research and development management, dynamic load balancing, Bayesian belief networks]
Total order multicast to multiple groups
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
We present a fault tolerant algorithm that ensures total order delivery of messages sent to multiple groups of processes. Our algorithm is a multiple group "genuine" multicast algorithm in the sense that: (1) any process can send a message to any set of process groups; and (2) only the sender and the receivers of a message take part in the algorithm needed to deliver the message. The correctness of our algorithm does not require reliable failure detectors, but requires causal order delivery of messages. This establishes a new and interesting link between causal order delivery and fault tolerance with unreliable failure detectors.
[message passing, program verification, fault tolerance, unreliable failure detectors, Liver, failure detectors, total order multicast, Computer crashes, software fault tolerance, Fault tolerance, Multicast algorithms, Fault detection, Fault tolerant systems, distributed algorithms, Detectors, multicast algorithm, Broadcasting, multiple process groups, algorithm correctness, multiple groups, causal order message delivery, fault tolerant algorithm, Contracts, total order delivery]
Distributed detection of generalized deadlocks
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
Fast and efficient detection of deadlocks remains an important problem in distributed operating systems. We present a distributed algorithm to detect generalized deadlocks in distributed systems. The algorithm performs reduction of a distributed wait-for-graph (WFG) to determine a deadlock. If sufficient information to decide the reducibility of a node is not available at that node, the algorithm attempts reduction later in a lazy manner. We prove the correctness of the algorithm. The algorithm has a message complexity of 2e messages and a worst case time complexity of 2d hops, where e is the number of edges and d is the diameter of the WFG. The algorithm is shown to perform better in both time and message complexity than the best known distributed algorithms to detect distributed generalized deadlocks. We conjecture that the algorithm is optimal in the number of messages and time delay, among distributed algorithms to detect generalized deadlocks.
[correctness proving, program verification, communication complexity, Distributed computing, distributed wait-for-graph, Sufficient conditions, Information science, message complexity, Operating systems, network operating systems, Distributed algorithms, Availability, generalized deadlocks, Delay effects, worst case time complexity, time delay, Topology, distributed algorithm, distributed deadlock detection, distributed algorithms, concurrency control, reducibility, System recovery, Resource management, distributed generalized deadlocks, distributed operating systems]
Load management in distributed video servers
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
We define and formulate various policies for load management in distributed video servers. We propose a predictive placement policy that determines the degree of replication necessary for popular videos using a cost based optimization procedure based on a priori predictions of expected subscriber requests. For scheduling requests, we propose an adaptive scheduling policy that compares the relative utilization of resources in a video server to determine an assignment of requests to replicas. To optimize storage utilization, we also devise methods for dereplication of videos based on changes in their popularities and in server usage patterns. Performance evaluations indicate that a load management procedure which uses a judicious combination of the different policies performs best for most server configurations. Advances in storage technologies are making high performance video servers a reality. These video servers are being deployed over emerging broadband networks to deliver a variety of interactive, digital video services to thousands of residential subscribers. To meet the scalability requirements in such large deployments, distributed video server architectures are being considered (M. Buddhikot and G. Parulkar, 1995). We propose various methods for load management that are targeted at improving the cost effectiveness of distributed video servers.
[network servers, Buffer storage, Scalability, Laboratories, Optimization methods, scalability requirements, predictive placement policy, broadband networks, expected subscriber requests, relative resource utilization, distributed video servers, a priori predictions, Network servers, resource allocation, interactive video, replication, popular videos, Independent component analysis, Scheduling, distributed video server architectures, Broadband communication, Milling machines, load management, digital video services, video equipment, scheduling requests, cost effectiveness, Load management, adaptive scheduling policy, residential subscribers, cost based optimization procedure]
Characterizing multicast orderings using concurrency control theory
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
Coordinating distributed executions is achieved by two widely used approaches: process groups and transactions. Typically, the two represent a trade-off in terms of the degrees of consistency and performance. By applying transaction concurrency control techniques to characterize and design process group multicast orderings, we aim to provide aspects of both ends of the trade-off. In particular, we propose a framework in which each message multicast is regarded as a transaction. Appropriate message ordering protocols are devised and shown to be correct using a variant of concurrency control theory. Also, we are able do incorporate certain aspects of application semantics for which existing process group approaches are inadequate. Finally, our framework provides a means to characterize the performance of orderings to allow a comparison of different ordering protocols.
[Process design, transaction processing, Protocols, message passing, multicast orderings, process group approaches, Liver, message ordering protocols, process group multicast orderings, Concurrency control, Transaction databases, History, distributed execution coordination, ordering protocols, message multicast, Computer science, Concurrent computing, concurrency control theory, transaction concurrency control techniques, application semantics, distributed algorithms, concurrency control, Marine vehicles]
A dynamic query scheduling framework for distributed and evolving information systems
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
The rapid growth of the wide area network technology has led to an increasing number of information sources available online. To ensure the query services to scale up with such dynamic open environments, an advanced distributed information system must provide adequate support for dynamic interconnection between information consumers and information producers, instead of just functioning as a static data delivery system. We develop a distributed query scheduling framework to demonstrate the feasibility and the benefit for supporting interoperability and dynamic information gathering across heterogeneous information sources, without relying on an integrated view predefined over the participating information sources. We outline the mechanisms developed for the main components of our distributed query scheduling framework, such as query routing and query execution planning services. We also provide a concrete example to illustrate the issues on how the information consumers' query requests are dynamically processed and linked to the heterogeneous information sources and how the query scheduling framework scales up as the number of information sources increases.
[open systems, Scalability, Distributed information systems, dynamic open environments, query scheduling framework, dynamic information gathering, Distributed computing, parallel programming, wide area network technology, heterogeneous information sources, Information systems, query processing, dynamic interconnection, High-speed networks, dynamic query scheduling framework, query requests, distributed databases, scheduling, query services, information systems, distributed information systems, evolving information systems, information consumers, query routing, distributed query scheduling framework, Dynamic scheduling, interoperability, Computer science, information producer, query execution planning services, Processor scheduling, Query processing, Concrete]
The Sage project: a new approach to software engineering for distributed applications
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
We describe the Sage project, a new approach to software engineering for (fault-tolerant) distributed applications. Sage uses the modal logic of knowledge and applies theoretical results detailing how processes learn facts about each other's state to derive the minimal communication graph for a wide range of coordination problems. The specification interface is controlled, yet expressive enough to capture important distributed coordination problems and weaker variants appropriate for wide-area applications. The resulting graphical display shows programmers which messages must be received. Sage allows users to experiment on the derived protocol by crashing processes, reordering events, losing messages, and partitioning the network. If a solution still exists, Sage regenerates the communication graph. This animates the effects of unpredictable system events on distributed applications, and separates the issues in testing a protocol's behavior in the face of failures, from the effects background system conditions can have on the testing procedure itself.
[message loss, System testing, Protocols, specification interface, graph theory, distributed coordination problems, Communication system control, Sage project, distributed processing, Displays, graphical display, modal logic, process crashing, formal logic, Fault tolerance, protocol, event reordering, software engineering, Logic, protocols, message passing, fault-tolerant distributed applications, testing, Application software, Programming profession, software fault tolerance, coordination problems, network partitioning, minimal communication graph, wide-area applications, unpredictable system events, Facial animation, Software engineering]
Maintaining strong cache consistency in the World-Wide Web
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
As the Web continues to explode in size, caching becomes increasingly important. With caching comes the problem of cache consistency. Conventional wisdom holds that strong cache consistency is too expensive for the Web, and weak consistency methods such as Time-To-Live (TTL) are most appropriate. The article compares three consistency approaches: adaptive TTL, polling-every-time, and invalidation, using prototype implementation and trace replay in a simulated environment. Our results show that invalidation generates less or a comparable amount of network traffic and server workload than adaptive TTL and has a slightly lower average client response time, while polling-every-time generates more network traffic and longer client response times. We show that, contrary to popular belief, strong cache consistency can be maintained for the Web with little or no extra cost than the current weak consistency approaches, and it should be maintained using an invalidation based protocol.
[Costs, average client response time, Telecommunication traffic, World Wide Web, cache storage, Delay, Network servers, server workload, Prototypes, invalidation based protocol, Time-To-Live, polling-every-time, simulated environment, Web server, Access protocols, trace replay, data integrity, Computer science, network traffic, strong cache consistency, World-Wide Web, adaptive TTL, Internet, Web sites, telecommunication traffic, weak consistency methods]
A framework for designing materialized views in data warehousing environment
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
Data warehouses are accessed by different queries with different frequencies. The portions of data accessed by a query can be treated as a view. When these views are related to each other and defined over overlapping portions of the base data, then it may be more efficient not to materialize all the views, but rather to materialize certain "shared views" from which the query results can be generated. We address some issues related to determining this set of shared views to be materialized in order to achieve the best combination of good performance and low maintenance, and provide an algorithm for achieving this goal.
[Algorithm design and analysis, query results, base data, shared views, Data warehouses, Educational institutions, materialized view design, software maintenance, overlapping portions, Delay, Computer science, query processing, Materials science and technology, Warehousing, very large databases, Distributed databases, distributed databases, Frequency, Australia, data warehousing environment]
An efficient logging scheme for recoverable distributed shared memory systems
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
The paper presents a new logging scheme for recoverable distributed shared memory systems. In previous schemes, the logging is performed whenever a new data item is accessed or written by a process. However, in the proposed scheme, only the data item accessed by multiple processes is logged when it is invalidated by the overwritten. Moreover, the logging is performed at one process responsible for that data item, unlike the other schemes in which every process accessing the data item performs the logging. As a result, the amount and the frequency of logging can be significantly reduced. The performance of the proposed scheme is analyzed using extensive simulation study and our new logging scheme shows superior performance in various system environments.
[Checkpointing, Computational modeling, recoverable distributed shared memory systems, Distributed computing, system recovery, simulation study, efficient logging scheme, Information science, Analytical models, data item, multiple processes, Fault tolerant systems, distributed memory systems, Frequency, shared memory systems, Computer networks, fault tolerant computing, Workstations, Performance analysis, system environments]
Dynamic Light-Weight Groups
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
The virtual synchrony model for group communication has proven to be a powerful paradigm for building distributed applications. In applications that use a large number of groups, significant performance gains can be attained if these groups share the resources required to provide virtual synchrony. A service that maps user groups onto instances of a virtually synchronous implementation is called a Light-Weight Group Service. This paper discusses the Light-Weight Group protocols in dynamic environments, where mappings cannot be defined a priori and may change over time. We show that it is possible to establish mappings that promote sharing and, at the same time, minimize interference. These mappings can be established in an automated manner using heuristics applied locally at each node. Experiments using an implementation in the Horus system show that significant performance improvements can be achieved with this approach.
[Light-Weight Group Service, Horus system, Protocols, virtual synchrony, File servers, virtual synchrony model, Application software, group communication, distributed applications, Computer science, dynamic environments, Network servers, performance gains, File systems, heuristics, Intersymbol interference, Detectors, Bandwidth, Power system reliability, computer network reliability, protocols]
Evaluating CORBA latency and scalability over high-speed ATM networks
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
We present two contributions to the study of CORBA performance over high-speed networks. First, we measure the latency of various types and sizes of two-way client requests using a pair of widely used implementations of CORBA-Orbix 2.1 and VisiBroker for C++ 2.0. Second, we use Orbix and VisiBroker to measure the scalability of CORBA servers in terms of the number of objects they can support efficiently. These experiments extend our previous work on CORBA performance for bandwidth-sensitive applications (such as satellite surveillance, medical imaging, and teleconferencing). Our results show that the latency for CORBA implementations is relatively high and server scalability is relatively low. Our latency experiments show that non-optimized internal buffering in CORBA implementations can cause substantial delay variance, which is unacceptable in many real-time or constrained-latency applications. Likewise our scalability experiments reveal that neither Orbix nor VisiBroker can handle a large number of objects in a single server process.
[nonoptimized internal buffering, Scalability, server scalability, asynchronous transfer mode, C language, constrained-latency applications, Distributed computing, teleconferencing, Delay, CORBA latency, VisiBroker, Network servers, High-speed networks, real-time, two-way client requests, delay variance, object-oriented methods, software performance evaluation, Orbix, client-server systems, C++, high-speed ATM networks, client servers, performance evaluation, Size measurement, bandwidth-sensitive applications, Application software, Computer science, satellite surveillance, Satellites, performance, Surveillance, real-time systems, object-oriented languages, CORBA scalability, medical imaging]
A framework for environment aware mobile applications
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
In mobile computing, factors such as add-on hardware components and heterogeneous networks result in an environment of changing resource constraints. An application in such a constrained environment must react to these changes so that available resources are properly utilized. In this paper, we propose a framework to build environment aware applications. The architecture is based on an event delivery mechanism that decouples event detection from delivery, giving the flexibility and extensibility that is necessary in a mobile computing environment. Information associated with the event is delivered as part of the event notification, while delivery latency is reduced by clever thread scheduling. We demonstrate the utility of this framework by structuring an environment aware networking subsystem around a prototype implementation. The performance of this implementation is competitive with current event delivery mechanisms such as the Unix signal.
[Unix, heterogeneous networks, Event detection, add-on hardware components, event delivery mechanism, Yarn, Delay, changing resource constraints, mobile computing, flexibility, resource allocation, network operating systems, Prototypes, Computer architecture, scheduling, Computer networks, Hardware, performance evaluation, environment aware mobile applications, extensibility, delivery latency, Processor scheduling, performance, prototype implementation, event detection, Computer applications, thread scheduling, event notification, Unix signal, wireless LAN, Mobile computing, portable computers]
Improving performance of TCP over wireless networks
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
Transmission Control Protocol (TCP) assumes a relatively reliable underlying network where most packet losses are due to congestion. In a wireless network, however, packet losses will occur more often due to unreliable wireless links than due to congestion. When using TCP over wireless links, each packet loss on the wireless link results in congestion control measures being invoked at the source. This causes severe performance degradation. In this paper, we study the effect of: burst errors on wireless links; packet size variation on the wired network; local error recovery by the base station; and explicit feedback by the base station, on the performance of TCP over wireless networks. It is shown that the performance of TCP is sensitive to the packet size, and that significant performance improvements are obtained if a good packet size is used. While local recovery by the base station using link-level retransmissions is found to improve performance, timeouts can still occur at the source, causing redundant packet retransmissions. We propose an explicit feedback mechanism, to prevent these timeouts during local recovery. Results indicate significant performance improvements when explicit feedback from the base station is used. A major advantage of our approaches over existing proposals is that no state maintenance is required at any intermediate host. Experiments are performed using the Network Simulator (NS) from Lawrence Berkeley Labs. The simulator has been extended to incorporate wireless link characteristics.
[TCP, Protocols, telecommunication congestion control, packet switching, timeouts, local recovery, digital simulation, Electronic mail, Network Simulator, Degradation, Wireless networks, Computer network reliability, Feedback, Propagation losses, packet losses, computer network reliability, base station, Base stations, local error recovery, network congestion, performance evaluation, wireless networks, Transmission Control Protocol, explicit feedback, packet size, Computer science, performance, transport protocols, redundant packet retransmissions, burst errors, link-level retransmissions, Internet, Lawrence Berkeley Lab, wireless LAN, packet size variation]
An optimistic quality-of-service provisioning scheme for cellular networks
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
We propose an optimistic quality-of-service (QoS) provisioning scheme for multimedia traffic in wireless networks. The scheme uses the standard classification of network traffic into two priority classes based on their bandwidth requirement and delay tolerance. We classify the users as local or departing based on their current coordinates in the cell. A call admission algorithm, a call management algorithm and a prediction based resource reservation algorithm which reserves the specified bandwidth (by borrowing channels from neighboring cells, if required) in the predicted destination cell(s) of a departing user are presented. A novel approach called bandwidth compaction is proposed which enables more efficient spectrum utilization. Simulation experiments show improvements of about 24% in the call blocking probability and 12% in the connection dropping probability over other bandwidth reserving schemes. An overall improvement of 24% in bandwidth utilization over other schemes not employing any bandwidth management technique, is also observed.
[Wireless LAN, Error analysis, telecommunication congestion control, Quality of service, Telecommunication traffic, call admission algorithm, call blocking probability, connection dropping probability, Compaction, telecommunication computing, departing user, Delay, multimedia traffic, Wireless networks, delay tolerance, Bandwidth, bandwidth management technique, Computer networks, optimistic quality-of-service provisioning scheme, computer network reliability, cellular networks, multimedia communication, call management algorithm, standard classification, quality control, bandwidth reserving schemes, wireless networks, bandwidth compaction, predicted destination cell, network traffic, Land mobile radio cellular systems, priority classes, prediction based resource reservation algorithm, bandwidth utilization, bandwidth requirement, spectrum utilization, cellular radio]
Extensible resource management for cluster computing
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
Advanced general purpose parallel systems should be able to support diverse applications with different resource requirements without compromising effectiveness and efficiency. We present a resource management model for cluster computing that allows multiple scheduling policies to co-exist dynamically. In particular, we have built Octopus, an extensible and distributed hierarchical scheduler that implements new space sharing, gang scheduling and load sharing strategies. A series of experiments performed on an IBM SP2 suggest that Octopus can effectively match application requirements to available resources, and improve the performance of a variety of parallel applications within a cluster.
[Performance evaluation, resource management model, cluster computing, Yarn, parallel machines, Delay, processor scheduling, application requirements, Concurrent computing, resource allocation, diverse applications, advanced general purpose parallel systems, load sharing strategies, parallel algorithms, resource requirements, distributed hierarchical scheduler, Dynamic scheduling, multiple scheduling policies, parallel applications, Application software, Octopus, Computer science, Processor scheduling, Time sharing computer systems, IBM SP2, space sharing, gang scheduling, Resource management, extensible resource management]
Two new quorum based algorithms for distributed mutual exclusion
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
Two novel suboptimal algorithms for mutual exclusion in distributed systems are presented. One is based on the modification of Maekawa's (1985) grid based quorum scheme. The size of quorums is approximately /spl radic/2/spl radic/N where N is the number of sites in a network, as compared to 2/spl radic/N of the original method. The method is simple and geometrically evident. The second one is based on the idea of difference sets in combinatorial theory. The resulting scheme is very close to optimal in terms of quorum size.
[Algorithm design and analysis, Costs, combinatorial mathematics, quorum size, combinatorial theory, distributed mutual exclusion, set theory, communication complexity, Distributed computing, network, suboptimal algorithms, optimisation, Search methods, distributed systems, Communication networks, quorum based algorithms, Testing, difference sets, Supercomputers, Computer science, distributed algorithms, concurrency control, grid based quorum scheme, Tin, geometry]
An architecture for post-development configuration management in a wide-area network
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
Few tools exist to address the post-development activities of configuring, releasing, installing, updating, reconfiguring, and even de-installing a software system. Certainly there is no unified approach for all of these activities, and none that can take full advantage of a wide-area network. The Software Dock represents an architecture for supporting post-development activities in such a setting. It is designed as a system of loosely-coupled, cooperating, distributed components that are bound together by a wide-area messaging and event system. In this paper we describe the Software Dock architecture and discuss the use of a prototype in deploying a complex system.
[Software Dock, wide area networks, wide-area messaging system, software prototyping, Laboratories, Software performance, software management, wide-area network, software installation, Intelligent networks, Engineering management, post-development configuration management, Computer architecture, software tools, software reconfiguration, Contracts, message passing, software maintenance, prototype, complex system, loosely-coupled cooperating distributed components, Computer science, configuration management, software release, Software systems, software updating, event system, Computer network management, Software engineering]
Scheduling algorithms for distributed Web servers
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
A distributed Web system, consisting of multiple servers for data retrieval and a Domain Name Server (DNS) for address resolution, can provide the scalability necessary to keep up with growing client demand at popular sites. However, balancing the requests among these atypical distributed servers opens interesting new challenges. Unlike traditional distributed systems in which a centralized scheduler has full control of the system, the DNS controls only a small fraction of the requests reaching the Web site. This makes it very difficult to avoid overloading situations among the multiple Web servers. We adapt traditional scheduling algorithms to the DNS, propose new policies, and examine their impact. Extensive simulation results show the advantage of using strategies that schedule requests on the basis of the origin of the clients and very limited state information, such as whether a server is overloaded or not. Conversely, algorithms that use detailed state information often exhibit the worst performance.
[data retrieval, network servers, client demand, DNS, Control systems, traditional scheduling algorithms, processor scheduling, scalability, Centralized control, Network servers, scheduling algorithms, resource allocation, request balancing, Bandwidth, atypical distributed servers, state information, Web server, Round robin, distributed Web system, multiple servers, information retrieval, Information retrieval, address resolution, Scheduling algorithm, distributed Web servers, distributed algorithms, Domain Name Server, multiple Web servers, simulation results, Load management, Internet, Acceleration, Web site]
A FIFO worst case analysis for a hard real-time distributed problem with consistency constraints
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
A solution for a hard real time scheduling problem in a distributed system is designed and proved. The constraints of our problem are first to preserve consistency even in the presence of concurrency and second to preserve the order of task releases, provided that task release times differ more than clock precision. That is achieved by FIFO based scheduling. The feasibility conditions resulting from the worst case response time analysis of each task set are given. The solution complexity is shown to be pseudo polynomial.
[Real time systems, Computer aided software engineering, worst case response time analysis, Humans, clock precision, distributed processing, distributed system, Delay, processor scheduling, pseudo polynomial solution complexity, Concurrent computing, hard real time distributed problem, FIFO based scheduling, FIFO worst case analysis, Polynomials, consistency constraints, data integrity, Synchronization, hard real time scheduling problem, concurrency, feasibility conditions, task set, task release order, real-time systems, concurrency control, task release times, Concrete, Time factors, Clocks, computational complexity]
Reliable support for a persistent distributed shared memory
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
This paper describes a reliable distributed storage server used to back segments located in a distributed virtual shared memory (DVSM). It focuses on the design of a generic logging support and gives hints about its implementation. The prototype developed within the AIX system is evaluated and exhibits good results. Using these results, some evolutions are proposed to improve performance.
[Availability, Software maintenance, Scalability, software reliability, virtual storage, Computer crashes, Information management, AIX system, Application software, reliable distributed storage server, Information systems, generic logging support, performance, Memory management, Prototypes, Management information systems, distributed databases, distributed virtual shared memory, persistent distributed shared memory, reliable support]
Building trust for distributed commerce transactions
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
The explosive growth of the Internet exposes unprecedented commercial opportunities over the network. The vast computer networks easily bring together customers and vendors who are physically distributed in different continents. Current research in electronic commerce mainly focuses on payment mechanisms. However, the global presence of customers and vendors makes it difficult to build trust among the parties involved in a transaction. We develop systematic mechanisms to address the trust problem in a network commerce environment. We first develop protocols for a single-customer single-vendor case where one customer buys goods from one vendor. Then we derive protocols for more complicated environments such as when a customer wants to buy goods from several vendors as part of a single transaction. Most of our protocols require some trusted third-party to intermediate transactions. However the workload and requirements placed on the trusted third-party vary from the whole goods and payment store-and-forward to very simple book-keepings of encryption keys. Our contributions also include an algorithm to find the optimal trusted agent.
[EFTS, transaction processing, payment mechanisms, business communication, encryption keys, optimal trusted agent, Electronic commerce, research, Fault tolerance, Network servers, customers, Cryptography, protocols, vendors, Business, electronic commerce, Instruments, trust problem, computer networks, Credit cards, marketing data processing, Cryptographic protocols, distributed commerce transactions, security of data, Authentication, Insurance, Internet, business data processing]
Concurrency control and view notification algorithms for collaborative replicated objects
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
This paper describes algorithms for implementing a high-level programming model for synchronous distributed groupware applications. In this model, several application data objects may be atomically updated, and these objects automatically maintain consistency with their replicas using an optimistic algorithm. Changes to these objects may be optimistically or pessimistically observed by view objects by taking consistent snapshots. The algorithms for both update propagation and view notification are based upon optimistic guess propagation principles, adapted for fast commit by using primary copy replication techniques. The main contribution of the paper is the synthesis of these two algorithmic techniques-guess propagation and primary copy replication-for implementing a framework that is easy to program and is well suited for the needs of groupware applications.
[Java, Collaborative software, Collaborative tools, high-level programming model, guess propagation, distributed processing, Concurrency control, Delay, view notification algorithms, synchronous distributed groupware, collaborative replicated objects, Collaboration, concurrency control, groupware, Collaborative work, Filling, Web sites, primary copy replication, Graphical user interfaces]
Messages versus messengers in distributed programming
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
Messengers are autonomous objects, each capable of navigating through the underlying network and performing various tasks at each node. Messenger applications are written using navigational commands rather than the send/receive primitives of conventional message-passing approaches. In this paper we contrast the two programming styles. The navigational style generally results in a smaller semantic gap between abstract algorithm descriptions and their actual implementations, which makes programs easier to construct, understand, and maintain. Other advantages of the navigational programming style include the ability to compute in unknown or dynamically changing network topologies.
[open systems, messenger applications, navigational programming style, changing network topologies, distributed processing, Organisms, Electronic mail, abstract algorithm descriptions, Distributed computing, parallel programming, Intelligent networks, navigational commands, program understanding, Computer networks, Skeleton, Dynamic programming, navigational style, distributed programming, tasks, message passing, object-oriented programming, Navigation, send receive primitives, Data structures, reverse engineering, software maintenance, Computer science, computer network, software portability, autonomous objects, security of data, messages, software maintenance5622807]
Workload characteristics for process migration and load balancing
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
Is process migration useful for load balancing? We present experimental results indicating that the answer to this question depends largely on the characteristics of the applied workload. Experiments with our Shiva system, which supports remote execution and process migration, show that only those CPU bound workloads which were generated using an unrealistic exponential distribution for execution times show improvements for dynamic load balancing. (We use the term 'dynamic' to indicate remote execution determined at and not prior to run time. The latter is known as 'static' load balancing.) Using a more realistic workload distribution and adding a number of short lived tasks prevents dynamic algorithms from working. Migration is only useful with heterogeneous workloads. We find the migration of executing tasks to remote data to be effective for balancing I/O bound workloads, and indicate the region of 'workload variable space' for which this migrate-to-data approach is useful.
[static load balancing, Heuristic algorithms, Exponential distribution, CPU bound workloads, Yarn, process migration, workload variable space, Shiva system, resource allocation, Vents, migrate-to-data approach, Distributed databases, distributed databases, dynamic algorithms, scheduling, Database systems, workload characteristics, remote execution, Web server, short lived tasks, unrealistic exponential distribution, Educational institutions, Transaction databases, realistic workload distribution, distributed algorithms, heterogeneous workloads, Load management, executing tasks, I/O bound workloads, dynamic load balancing]
Predictable network computing
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
Clusters of networked commercial, off the shelf (COTS) workstations are presently used for computation intensive tasks that were typically assigned to parallel computers in the past. However, it is hardly possible to predict the timing behavior of such systems or to give guarantees about execution times. We show how our SONiC (Shared Objects Net-interconnected Computer) system can control timing and partitioning of a workstation as a step towards a distributed real time system built from COTS components. SONiC provides a class based programming interface for creation of replicated shared objects of arbitrary, user defined sizes. Weak consistency protocols are employed to improve system performance. Our scheduling service ensures the requested interactive behavior of a workstation while simultaneously giving a specified number of CPU cycles to parallel tasks. Using offline scheduling methods we are able to implement real time guaranteed services on COTS workstations.
[Real time systems, networked commercial off the shelf workstations, Protocols, application program interfaces, COTS workstations, distributed real time system, Control systems, CPU cycles, Distributed computing, parallel programming, processor scheduling, Concurrent computing, timing behavior, Shared Objects Net-interconnected Computer, System performance, Computer networks, SONiC, Workstations, requested interactive behavior, computer networks, performance evaluation, replicated shared objects, computation intensive tasks, user defined sizes, workstations, predictable network computing, COTS components, class based programming interface, real-time systems, execution times, weak consistency protocols, system performance, Time sharing computer systems, scheduling service, Timing, real time guaranteed services, offline scheduling methods]
Determining the expected load of dynamic tree embeddings in hypercubes
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
The main contribution to use recurrence relations in analyzing the performance of a dynamic tree embedding algorithm in hypercubes. These recurrence relations characterize the expected load on each processor in a randomized tree embedding, and enable us to evaluate expected loads numerically and analytically. As a matter of fact, our methodology does not depend on the hypercube topology, and can be applied to study dynamic tree growing in other networks.
[Algorithm design and analysis, Heuristic algorithms, parallel architectures, hypercube networks, Mathematics, probabilistic analysis, hypercubes, Concurrent computing, dynamic tree growing, random walk, Tree graphs, resource allocation, numerical analysis, Hypercubes, Computer networks, randomized tree embedding, Performance analysis, software performance evaluation, expected load, dynamic tree embeddings, trees (mathematics), probability, topology, performance ratio, Computer science, recurrence relations, Binary trees]
Fragment reconstruction: providing global cache coherence in a transactional storage system
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
Cooperative caching is a promising technique to avoid the increasingly formidable disk bottleneck problem in distributed storage systems; it reduces the number of disk accesses by servicing client cache misses from the caches of other clients. However, existing cooperative caching techniques do not provide adequate support for fine grained sharing. We describe a new storage system architecture, split caching, and a new cache coherence protocol, fragment reconstruction, that combine cooperative caching with efficient support for fine grained sharing and transactions. We also present the results of performance studies that show that our scheme introduces little overhead over the basic cooperative caching mechanism and provides better performance when there is fine grained sharing.
[transaction processing, Costs, transactional storage system, Cooperative caching, Scalability, parallel architectures, Laboratories, client cache misses, cooperative caching mechanism, Cache storage, cache storage, distributed storage systems, cooperative caching techniques, Delay, Network servers, fine grained sharing, performance studies, Local area networks, client-server systems, disk accesses, global cache coherence, fragment reconstruction, Transaction databases, disk bottleneck problem, Computer science, split caching, memory architecture, storage system architecture, cache coherence protocol, cooperative caching]
Indexed sequential data broadcasting in wireless mobile computing
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
Energy saving is one of the most important issues in wireless mobile computing. Among others, one viable approach to achieving energy saving is to use an indexed data organization to broadcast data over wireless channels to mobile units. We explore the issue of indexing data with skewed access for sequential broadcasting in wireless mobile computing. We propose methods to build index trees based on access frequencies of data records. To minimize the average cost of index probes, we consider two cases: one for fixed index fanouts and the other for variant index fanouts, and devise algorithms to construct index trees for both cases. We show that the cost of index probes can be minimized not only by employing an imbalanced index tree that is designed in accordance with data access skew, but also by exploiting variant fanouts for index nodes.
[radio broadcasting, Energy consumption, Costs, Batteries, index trees, fixed index fanouts, indexed sequential data broadcasting, access frequencies, indexed data organization, variant index fanouts, Broadcasting, Computer networks, wireless channels, Personal digital assistants, Probes, data records, indexing, skewed access, trees (mathematics), computer networks, data access skew, Indexes, energy saving, index probes, Central Processing Unit, wireless mobile computing, Mobile computing, mobile units, cellular radio]
Multi-threading and remote latency in software DSMs
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
This paper evaluates the use of per-node multi-threading to hide remote memory and synchronization latencies in a software DSM. As with hardware systems, multi-threading in software systems can be used to reduce the costs of remote requests by switching threads when the current thread blocks. We added multi-threading to the CVM software DSM and evaluated its impact on performance for a suite of common shared memory programs. Multi-threading resulted in speed improvements of at least 17% in three of the seven applications in our suite, and lesser improvements in the other applications. However, we found that: good performance is not always achievable transparently for non-trivial applications; multi-threading can negatively interact with DSM operations; multi-threading decreases cache and TLB locality; and any multi-threading speedup is dependent on available work.
[TLB locality, Costs, nontrivial applications, Software performance, shared memory programs, Wire, remote latency, Yarn, Delay, remote memory, Physics computing, CVM software, shared memory systems, Hardware, per-node multithreading, software performance evaluation, remote requests, costs, message passing, cache, thread switching, Application software, synchronization latency, synchronisation, software distributed shared memory system, hardware systems, speed improvement, performance, multiprogramming, distributed memory systems, Software systems, Frequency]
Exploiting temporal and spatial constraints on distributed shared objects
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
Gigabit network technologies have made it possible to combine workstations into a distributed, massively-parallel computer system. Middleware, such as distributed shared objects (DSO), attempts to improve programmability of such systems, by providing globally accessible 'object' abstractions. Researchers have developed consistency protocols for replicated 'memory' objects. These protocols are well suited to scientific applications but less suited to multimedia or groupware applications. We address the state sharing needs of complex distributed applications with: high-frequency symmetric accesses to shared objects; unpredictable and limited locality of accesses; dynamically changing sharing behavior; and potential data races. We show that a DSO system exploiting application-level temporal and spatial constraints on shared objects can outperform shared object protocols which do not exploit application-level constraints. We compare our S(emantic) DSO against entry consistency using a sample application having the four properties mentioned above.
[distributed shared objects, distributed massively-parallel computer system, replicated memory objects, temporal constraints, dynamically changing sharing behavior, spatial constraints, scientific applications, multimedia computing, Distributed computing, programmability, Concurrent computing, groupware, shared memory systems, Computer networks, Workstations, protocols, state sharing needs, middleware, globally accessible object abstractions, message passing, high-frequency symmetric access, Access protocols, Educational institutions, Application software, Middleware, consistency protocols, workstations, entry consistency, multimedia, Message passing, distributed memory systems, Collaborative work, gigabit network technologies, data races]
Access control in wide-area networks
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
Access control involves maintaining information about which users can access system resources and ensuring that access is restricted to authorized users. In wide-area networks such as the Internet, implementing access control is difficult, since resources may be replicated, the task of managing access rights may be distributed among multiple sites, and events such as host failures, host recoveries, and network partitions must be dealt with. This paper explores the problem of access control in such an environment, and in particular the inherent tradeoff between security, availability, and performance. Techniques for dealing with access control in the presence of partitions are presented and used as the basis for an algorithm that allows application control over these tradeoffs.
[Access control, wide area networks, application control, Control systems, availability, host recoveries, Intelligent networks, security, File systems, Databases, multiple sites, authorisation, Permission, system resources, access control, computer network reliability, IP networks, Availability, access rights, replicated resources, performance evaluation, Partitioning algorithms, authorized users, performance, Authentication, network partitions, Internet, host failures]
Layered analytic performance modelling of a distributed database system
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
Very few analytic models have been reported for distributed database systems, perhaps because of complex relationships of the different kinds of resources in them. Layered queueing models seem to be a natural framework for these systems, capable of modelling all the different features which are important for performance (e.g. devices, communications, multithreaded processes, locking). To demonstrate the suitability of the layered framework, a previous queueing study of the CARAT distributed testbed has been recast as a layered model. Whereas the queueing model bears no obvious resemblance to the database system, the layered model directly reflects its architecture. The layered model predictions have about the same accuracy as the queueing model.
[layered queueing models, queueing theory, analytic models, queueing study, Peer to peer computing, layered analytic performance modelling, layered model predictions, Predictive models, Data engineering, CARAT distributed testbed, layered framework, Delay, software fault tolerance, multithreaded processes, Network servers, Computer architecture, distributed databases, Systems engineering and theory, Database systems, Performance analysis, queueing model, software performance evaluation, Testing, distributed database system]
Hierarchical, adaptive cache consistency in a page server OODBMS
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
Due to its simplicity and communication efficiency many client-server object-oriented database systems are based on the basic page server architecture-pages serve as their smallest unit of data transfer, client caching, and concurrency control. In an earlier paper, we showed how to extend this architecture to permit object-level callback locking, and we showed through simulations that significant performance gains can be expected. In the current paper we report on our experiences from implementing this approach in the context of the SHORE system, which supports a generalized peer-servers architecture. In addition to discussing some of the stickier implementation details, we also explain how our callback algorithm was extended to support the hierarchical locking approach used in SHORE. Finally, we present performance measurements obtained by running SHORE on an IBM SP2 machine.
[Communication system control, Control systems, cache storage, Adaptive control, Environmental management, page server OODBMS, adaptive cache consistency, Database systems, software performance evaluation, client-server systems, Object oriented databases, Object oriented modeling, generalized peer-servers architecture, object-oriented databases, client caching, client-server object-oriented database systems, Concurrency control, Transaction databases, Programmable control, performance measurements, object-level callback locking, concurrency control, SHORE system, data transfer, IBM SP2 machine, communication efficiency]
Synthesizing protocol specifications from service specifications in timed extended finite state machines
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
We propose a specification model and present a method to algorithmically derive a protocol specification from a service specification based on the model. Unlike the previous models based on finite state machines, the proposed model can explicitly express concurrency, synchronization, and timing requirements such as delays and timeouts. We assume that there exists a reliable communication channel between say two protocol entities and the maximum delay for each channel is bounded by a positive constant. Because of the variable nature of the communication delays along with the time constraints associated with events, no protocol specification can fully simulate the service specification. The proposed method derives a protocol specification that is optimal in the sense that it provides the largest possible subset of the service specification under the communication delay constraints. We also give a method to derive a sub specification from a service specification and a maximum communication delay of each channel such that the sub specification, but no superset of it, can be simulated by the derived protocol specification.
[Protocols, time constraints, timeouts, finite state machines, formal specification, maximum delay, protocol specification synthesis, communication delay constraints, protocols, Delay effects, timing, Educational institutions, timing requirements, synchronisation, concurrency, Computer science, timed extended finite state machines, reliable communication channel, Automata, delays, Communication channels, synchronization, Timing, Error correction, Time factors, communication delays, Clocks, service specifications]
A fault-tolerant object service on CORBA
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
There are more and more COSS (Common Object Service Specifications) on CORBA (Common Object Request Broker Architecture) announced by the OMG (Object Management Group), but no common specification about fault-tolerance exists. We propose a "warm stand-by" replication approach. When an object (primary object) is invoked, it will invoke a secondary object, and the primary object will log the messages and checkpoint the state to the secondary object periodically. If the primary object fails, the secondary object can take over by way of a client executing a few operations to change the secondary object's mode to primary. Following the style of COSS, we define four interfaces and provide class implementations that can help programmers write programs with fault-tolerant capability. The whole model has been implemented on Orbix, which is a full implementation of CORBA specification.
[checkpoint, distributed processing, Environmental management, formal specification, Embedded software, Fault tolerance, Information science, COSS, Common Object Service Specifications, Fault tolerant systems, replication approach, Software standards, object-oriented methods, fault-tolerant object service, object oriented programming, Object oriented programming, Orbix, interfaces, Object oriented modeling, Object Management Group, OMG, specification, Application software, message, Programming profession, software fault tolerance, CORBA, secondary object, Common Object Request Broker Architecture, class implementations, primary object]
Online replication of shared variables
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
A linearizable implementation of read/write shared variables in a general distributed system is provided. The proposed system adapts the distribution of the replicas to the pattern of read/write requests, and uses distributed hierarchical indexing to locate replicas.
[Costs, message passing, replicated databases, indexing, graph theory, linearizable implementation, distributed system, replica distribution, Programming profession, replica location, database theory, Computer science, Fault tolerance, read write shared variables, Tree graphs, Message passing, Atomic layer deposition, shared variable online replication, distributed hierarchical indexing, Load management, shared memory systems, Computer networks, Indexing]
Effect of virtual circuit rerouting on application performance
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
One of the key problems in extending ATM protocols to support host mobility is designing virtual circuit rerouting protocols to maintain continual network connectivity to a mobile host. Ideally, VC rerouting must be done fast enough so as to cause minimal disruption to applications, running on mobile hosts. In this paper, we evaluate the impact of several virtual circuit rerouting strategies on application performance. Our results show that using simple and fast rerouting policies results in good performance for both data and real-time traffic.
[Virtual colonoscopy, Wireless application protocol, application disruption, Circuits, virtual circuit rerouting, Telecommunication traffic, Mobile communication, asynchronous transfer mode, Delay, application performance, mobile host, Wireless networks, mobile hosts, Traffic control, host mobility, protocols, wireless network, VC rerouting, software performance evaluation, continual network connectivity, Access protocols, performance evaluation, telecommunication network routing, real-time systems, Intserv networks, ATM protocols, wireless LAN, real-time traffic]
Certification reports: supporting transactions in wireless systems
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
The emergence of small portable computers and the advances in wireless networking have made mobile computing today a reality. Information systems and databases are among the applications that make mobile computing attractive. While the topic of querying data in wireless and mobile systems has received a lot of attention, techniques to efficiently update data in these systems while providing transaction semantics are not fully developed. We present a novel protocol that uses the broadcast facility to help mobile units do some of the work of verifying if the transactions being run by them need to be aborted. Only when the mobile unit cannot detect any conflict is the server involved in completing the verification. Of course, if the transaction can commit, the server will install the valves in the central database and notify the mobile units (again, using the broadcast channel). The protocol uses a modified version of optimistic control. We study the performance of the protocol by means of a detailed simulation.
[transaction processing, radio broadcasting, Protocols, Portable computers, transaction semantics, program verification, certification reports, mobile systems, broadcast channel, Valves, small portable computers, Information systems, broadcast facility, query processing, transaction support, mobile computing, distributed databases, Broadcasting, Computer networks, protocols, optimistic control, verification, wireless systems, computer networks, data querying, data updating, Transaction databases, Application software, Certification, mobile communication, wireless networking, Mobile computing, portable computers]
Race analysis of traces of asynchronous message-passing programs
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
An execution of a message-passing program is nondeterministic if message races exist. In this paper, a formal definition of a message race for asynchronous communication is presented. The trace of an execution of a message-passing program is a sequence of send and receive events. For a receive event r in a trace T, its race set is the set of messages in T that have a race with the message received at r and can be received at r during some possible executions of the same program with the same input. A race analysis algorithm analyzes a trace to determine the race set for each receive event in the trace. Three race analysis algorithms are given for three different types of sequences of send and receive events. It is shown that these race analysis algorithms can be used to solve a number of problems in testing and debugging message-passing programs.
[Algorithm design and analysis, program debugging, message passing, race set, program testing, program diagnostics, Debugging, trace analysis, asynchronous communication, nondeterministic execution, message races, send event, Computer science, receive event, Asynchronous communication, Fault detection, race analysis, Testing, asynchronous message-passing programs]
Connection admission control for hard real-time communication in ATM networks
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
Connection Admission Control (CAC) as needed in ATM networks to provide Quality of Service (QoS) guarantees real-time connections. In this paper, we present a CAC scheme based on (1) a bit-stream traffic model to describe traffic generation patterns of CBR/VBR connections and traffic distortions within a network, and (2) worst-case queueing analysis to obtain hard cell queueing delay bounds. The proposed CAC scheme can be used for the establishment of hard real-time connections in ATM networks with conventional static priority FIFO queueing switches. The effectiveness of the scheme is illustrated by applying it to an ATM-based Real-Time Plant Control Network (RTPCN) currently under development by the Mitsubishi Electric Corporation. The CAC scheme presented in the paper can also be extended to set up soft real-time connections in ATM networks.
[Real time systems, telecommunication congestion control, CAC, Communication system control, computer networks, Switches, queueing theory5622792, asynchronous transfer mode, hard real-time communication, access protocols, Connection Admission Control, Delay, Quality of Service, Intelligent networks, worst-case queueing analysis, hard cell queueing delay bounds, Admission control, real-time systems, Bandwidth, Traffic control, Communication system traffic control, soft real-time connections, Asynchronous transfer mode, ATM networks]
Efficient load balancing in interconnected LANs using group communication
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
The paper investigates the use of group communication to improve the efficiency of load balancing in interconnected LANs. Conventional load balancing techniques usually assume point-to-point connections among the computers and typically work on a single LAN only. This may waste network bandwidth and lengthen the load balancing time in LANs that support group communication such as the Ethernet. To tackle this problem a two level algorithm is proposed. At the LAN level, all computers in a LAN maintain loading information in the entire LAN using a globally ordered group channel. The workload in the different LANs is balanced by moving load from the overloaded LANs to the underloaded ones. It is proved that the proposed algorithm converges to the state of global balance geometrically. Experimental results show that the algorithm reduces load balancing time and network utilization significantly, compared to the single level algorithm without group communication.
[loading information, Ethernet networks, network utilization, local area networks, globally ordered group channel, resource allocation, load balancing techniques, Bandwidth, Broadcasting, point-to-point connections, Computer networks, IP networks, Local area networks, global balance, LAN interconnection, network bandwidth, group communication, Computer science, two level algorithm, distributed algorithms, efficient load balancing, load balancing time, Load management, Internet, interconnected LANs]
A dynamic probe strategy for quorum systems
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
A quorum system is a collection of sets called quorums. Any two quorums in a quorum system must have at least one element in common. Quorum systems can be used for many different applications in a distributed system. A quorum system is available, if all elements in at least one quorum ore operational. The elements are probed to determine if they are available. A probe strategy is used to determine the order in which the elements are probed. This paper studies the average case probe complexity, which is defined as the expected number of probes required to determine if a given quorum system is available. Then, two new probe strategies are presented and analyzed. The first strategy is a generalizations of the alternating color strategy, and the second strategy is based on a measure of the relative importance of each element in the system.
[Protocols, wide area networks, Nominations and elections, alternating color strategy, dynamic probe strategy, distributed system, probe strategy, communication complexity, average case probe complexity, quorum systems, Fault tolerant systems, Performance analysis, computer network reliability, Wool, protocols, Probes, distributed processing5622778, computational complexity]
A structured channel borrowing scheme for dynamic load balancing in cellular networks
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
We propose an efficient dynamic load balancing scheme in cellular networks for managing a teletraffic hot spot in which channel demand exceeds a certain threshold. A hot spot, depicted as a stack of hexagonal 'ring' of cells, is classified as complete if all cells within it are hot. The rings containing only cold cells outside the hot spot are called 'peripheral rings'. Our load balancing scheme migrates channels through a structured borrowing mechanism from the cold cells within the 'rings' or 'peripheral rings' to the hot cells in the hot spot. For the more general case of an incomplete hot spot, a cold cell is further classified as cold safe, cold semi-safe or cold unsafe, and a demand graph is constructed from the channel demand of each hot cell from its adjacent cells in the next outer ring. The channel borrowing algorithm works on the demand graph in a bottom up fashion, satisfying the demands of the cells in each subsequent inner ring. Markov chain models are developed for a hot cell and detailed simulation experiments are conducted to evaluate the performance of our load balancing scheme. Comparison with an existing load balancing strategy under moderate and heavy teletraffic conditions, shows a performance improvement of 12% in terms of call blockade by our load balancing scheme.
[Markov chain models, graph theory, channel demand, demand graph, structured channel borrowing scheme, channel borrowing algorithm, Electronic mail, Distributed computing, structured borrowing mechanism, Concurrent computing, Intelligent networks, resource allocation, teletraffic hot spot management, hexagonal ring, Computer networks, cellular networks, Base stations, peripheral rings, computer networks, cold cells, simulation experiments, Land mobile radio cellular systems, Markov processes, Load management, Frequency, Computer network management, load balancing scheme, telecommunication traffic, dynamic load balancing, cellular radio]
Centralized failure injection for distributed, fault-tolerant protocol testing
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
We describe a centralized approach to testing that distributed fault-tolerant protocols satisfy their safety and timeliness specifications in the presence of the very failures they are designed to tolerate. CESIUM is a testing environment based on the centralized simulation of distributed executions and failures. Processes are run in a single address space while providing the appearance of a truly distributed execution. The human tester can force the occurrence of arbitrary failures and security attacks. The implementations under test are not instrumented for testing purposes, and their source codes need not be available. We prove that CESIUM can execute exactly the set of runs feasible in the real distributed system being simulated. We also show that there are safety and timeliness properties in the specifications of many existing distributed protocols that cannot be tested in practical distributed systems. All of these properties can, however, be accurately tested by CESIUM without introducing any perturbation in test experiments.
[System testing, testing environment, Protocols, Humans, centralized simulation, distributed protocol testing, Security, formal specification, truly distributed execution, specifications, Delay, centralized failure injection, fault-tolerant protocol testing, Fault tolerance, safety, CESIUM, arbitrary failures, Safety, timeliness specifications, protocols, security attacks, Instruments, Vehicle crash testing, performance evaluation, fault tolerant computing, single address space, Clocks]
Encapsulating mobile objects
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
This paper describes a technique to effectively isolate mobile objects or processes that execute downloaded, potentially suspicious programs. It relies on wish lists, trust lists and capability lists. Wish lists are carried along with programs or mobile objects and denote the resources requested by the program to do what it claims to do. Wish lists are transformed into capability lists when downloaded programs are started. Trust lists reside on stations and are used to determine which members of wish lists are taken over into capability lists. The capability lists are enforced during the execution of programs. All lists are symbolic to enable their interpretation in heterogeneous environments. The paper describes the technique, its integration in a Linux environment and first experiences.
[distributed processing, Security, downloaded programs, Filters, mobile object encapsulation, Mobile agents, suspicious programs, trust lists, distributed systems, Workstations, object oriented programming, Protection, Linux environment, Java, object-oriented programming, capability lists, data security, heterogeneous environments, program execution, Application software, Computer science, software portability, security of data, Linux, data encapsulation, Mobile computing, wish lists, symbolic lists]
Load profiling: a methodology for scheduling real-time tasks in a distributed system
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
Traditionally, the goal of load management protocols for distributed systems has been to ensure that nodes are equally loaded. We show that for real time systems, load balancing is not desirable since it results in the available bandwidth being distributed equally amongst all nodes-in effect making all nodes in the system capable of offering almost the same bandwidth (e.g., in cycles per second) to incoming tasks. We show that this "one size fits all" practice leads to a higher rate of missed deadlines as incoming tasks may be denied service because they require bandwidth that cannot be granted at any single node-while plenty of fragmented bandwidth is collectively available in the system. We propose a new load profiling strategy that allows the nodes of a distributed system to be unequally loaded so as to maximize the chances of finding a node that would satisfy the computational needs of incoming real time tasks. The performance of the proposed protocol is evaluated via simulation, and is contrasted to other dynamic scheduling protocols for real time distributed systems.
[Real time systems, fragmented bandwidth, Protocols, multiprocessing systems, real time task scheduling, load balancing, Computational modeling, incoming real time tasks, Control systems, Dynamic scheduling, load profiling strategy, load profiling, Distributed computing, dynamic scheduling protocols, processor scheduling, Computer science, Processor scheduling, resource allocation, real-time systems, Bandwidth, Load management, load management protocols, real time distributed systems, missed deadlines]
Deadline assignment in distributed hard real-time systems with relaxed locality constraints
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
In a real time system, tasks are constrained by global end to end deadlines. In order to cater for high task schedulability, these deadlines must be distributed over component subtasks in an intelligent way. Existing methods for automatic distribution of end to end deadlines are all based on the assumption that task assignments are entirely known beforehand. This assumption is not necessarily valid for large real time systems. Furthermore, most task assignment strategies require information on deadlines in order to make good assignments, thus forming a circular dependency between deadline distribution and task assignment. We present a heuristic approach that performs deadline distribution prior to task assignment. The deadline distribution problem is presented in the context of large distributed hard real time systems with relaxed locality constraints, where schedulability analysis must be performed offline, and only a subset of the tasks are constrained by predetermined assignments to specific processors. Using experimental results we identify drawbacks of previously proposed techniques, and then show that our solution provides significantly better performance for a large variety of system configurations.
[Real time systems, Software maintenance, relaxed locality constraints, distributed processing, Sensor systems, distributed hard real time systems, circular dependency, end to end deadlines, Distributed computing, processor scheduling, automatic distribution, heuristic programming, task assignment strategies, Parallel processing, large real time systems, Performance analysis, task assignment, Software reusability, deadline distribution, heuristic approach, Application software, component subtasks, Computer science, task assignments, Processor scheduling, deadline assignment, schedulability analysis, real-time systems, high task schedulability, global end to end deadlines]
Characterization of message ordering specifications and protocols
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
We study the problem of determining which message ordering specifications can be implemented in a distributed system. Further, if a specification can be implemented, we give a technique to determine whether it can be implemented by tagging information with user messages or if it requires control messages. To specify the message ordering, we use a novel method called forbidden predicates. All existing message ordering guarantees such as FIFO, flush channels, causal ordering, and logically synchronous ordering, (as well as many new message orderings) can be concisely specified using forbidden predicates. We then present an algorithm that determines from the forbidden predicate the type of protocol needed to implement that specification.
[user messages, Protocols, message passing, message ordering specifications, distributed system, Synchronous motors, message ordering guarantees, FIFO, Distributed computing, formal specification, parallel programming, forbidden predicates, control messages, flush channels, Tagging, causal ordering, protocols, Distributed algorithms, Marine vehicles, logically synchronous ordering]
Secure reliable multicast protocols in a WAN
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
A secure reliable multicast protocol enables a process to send a message to a group of recipients such that all honest destinations receive the same message, despite the malicious efforts of fewer than a third of them, including the sender. This has been shown to be a useful tool in building secure distributed services, albeit with a cost that typically grows linearly with the size of the system. For very large networks, for which such a cost may be too prohibitive, we present two approaches for bringing the cost down: First, we show a protocol whose cost is on the order of the number of tolerated failures. Secondly, we show how relaxing the consistency requirement to a selected probability level of guarantee can bring down the associated cost to a constant.
[Wide area networks, Costs, consistency requirement, WAN, Multicast protocols, secure reliable multicast protocols, communication complexity, secure distributed services, Delay, probability level, Computational efficiency, Internet, Digital signatures]
Distributed connection management for real-time communication over wormhole-routed networks
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
Wormhole networks provide a very-high-speed communication medium that is well suited for a large number of bandwidth demanding applications. Unfortunately, the lack of buffering in the switches causes blocked packets to transiently monopolize arbitrary large portions of the network, thus making it difficult to give real-time guarantees when connections contend for communication links. We present an efficient, fully distributed admission control scheme for time-critical connections over wormhole networks. In comparison to a corresponding centralized scheme, such a distributed scheme offers advantages in terms of scalability and easy integration into hybrid networks. We compare this scheme to a centralized algorithm in a suite of simulation experiments and show that its performance is encouraging.
[Protocols, Laboratories, Switches, communication links, Routing, access protocols, Application software, real-time communication, Communication switching, scalability, wormhole-routed networks, Computer science, distributed admission control, distributed connection management, admission control scheme, Admission control, Bandwidth, time-critical connections, hybrid networks, Computer network management]
Aggressive release consistency for software distributed shared memory
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
As a software-based distributed shared memory (DSM) system is especially sensitive to the traffic amount over the network, we propose a new software DSM model. The model postpones the enforcement of data coherence at the time of the first shared memory access after an acquire, instead of at the time of the acquire like the lazy release consistency (LRC) model. This leads to an aggressive implementation of release consistency and thus a reduced number of messages transferred over the network when compared with LRC. Our model is evaluated on the basis of the TreadMarks framework using three applications, where TreadMarks is a software DSM implementation following LRC. The experimental results on a network of workstations indicate that our model leads to fewer messages transmitted across the network than LRC, by over 16% for one application and over 12% for the other two.
[Telecommunication traffic, local area networks, Distributed computing, software DSM model, Traffic control, Lead, shared memory systems, Hardware, Computer networks, Workstations, software performance evaluation, message passing, lazy release consistency model, TreadMarks framework, shared memory access, data integrity, Application software, software distributed shared memory system, network traffic, data coherence, network of workstations, Coherence, distributed memory systems, Time sharing computer systems, aggressive release consistency]
Connection-oriented communications for real-time applications in FDDI-ATM-FDDI heterogeneous networks
Proceedings of 17th International Conference on Distributed Computing Systems
None
1997
We study connection-oriented service in an FDDI-ATM-FDDI heterogeneous network for real-time applications. We design and analyze an algorithm for connection admission control (CAC) for such a network. Upon a request of connection establishment, the CAC determines if the worst case delays of the requesting and existing connections can be satisfied given the available network resources. If so, the CAC allocates appropriate network resources to the requesting connection. The process of allocating resources for homogeneous networks (e.g. FDDI-only or ATM-only) may not be applied directly to a heterogeneous network environment (e.g. FDDI-ATM-FDDI network) because heterogeneity adds more complexity to the process. Hence resource allocation in a heterogeneous network needs more careful analysis than its homogeneous counterpart. We propose a CAC algorithm that will, by proper parameter tuning, allocate sufficient but not excessive network resources to the requesting connection in an FDDI-ATM-FDDI network. We show that the system can achieve satisfactory performance with this CAC algorithm. Our approach is compatible with current network standards and hence can be readily used in practical systems.
[network standards, telecommunication congestion control, network resource allocation, asynchronous transfer mode, local area networks, Multiaccess communication, Delay, Intelligent networks, Network servers, real-time applications, resource allocation, Communication networks, Local area networks, connection-oriented service, FDDI, performance evaluation, connection admission control, parameter tuning, CAC algorithm, connection-oriented communications, telecommunication standards, Computer science, homogeneous networks, performance, Admission control, FDDI-ATM-FDDI heterogeneous networks, real-time systems, delays, local area network, Resource management, worst case delays]
Granularity control for distributed execution of logic programs
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
Distributed execution of logic programs requires a match of granularity between a program and the distributed multiprocessor it runs on to exploit its potential for performance fully. This paper presents methods to control the granularity of tasks on distributed heterogeneous processors effectively. It considers the characteristics of such platforms and relates the amount of local computation with the significant communication overheads by introducing the notion of a collection of parallel tasks. The experimental results indicate that the proposed controls can model all kinds of predicates (recursive, mutually recursive etc.) satisfactorily and improve the performance of various forms of parallelism (AND, OR, combinations).
[logic programs, Costs, Communication system control, distributed processing, distributed heterogeneous processors, Proposals, Engines, parallel programming, Information analysis, distributed multiprocessor, distributed execution, Parallel processing, Automatic control, logic programming, communication overheads, Performance analysis, parallel tasks, software performance evaluation, multiprocessing systems, local computation, Logic programming, predicates, performance, Distributed control, granularity control]
Some economics of market-based distributed scheduling
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
Market mechanisms solve distributed scheduling problems by allocating the scheduled resources according to market prices. We model distributed scheduling as a discrete resource allocation problem, and demonstrate the applicability of economic analysis to this framework. Drawing on results from the literature, we discuss the existence of equilibrium prices for some general classes of scheduling problems, and the quality of equilibrium solutions. We then present two auction protocols for implementing solutions, and analyze their computational and economic properties.
[auction protocols, computational properties, Design methodology, Communication system control, Access protocols, market prices, distributed processing, equilibrium prices, market-based distributed scheduling, Distributed computing, discrete resource allocation problem, Information analysis, market mechanisms, optimisation, economics, Processor scheduling, resource allocation, scheduling, economic cybernetics, IP networks, Resource management]
Flexible batching and consistency mechanisms for building interactive groupware applications
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
The paper presents our state sharing support for building object oriented interactive groupware in wide area distributed environments. We motivate and present an asynchronous model for updating replicated state, which supports atomicity of updates across multiple shared objects. Coupled with our flexible marshalling framework, this model allows existing application data structure classes to be easily extended and made shareable. To solve the problem of replica consistency we use a novel combination of three mechanisms: (1) global locks, (2) detection of incorrect update ordering, and (3) cloning a subset of the shared objects and state reinitialization. Finally, to reduce network load due to fine grain user interaction, we provide a framework for application specified event batching called Late Event Modification.
[wide area networks, network load, object oriented interactive groupware, multiple shared objects, updates, replica consistency, Delay, application specified event batching, Late Event Modification, groupware, interactive systems, global locks, data structures, flexible batching, consistency mechanisms, interactive groupware applications, application data structure classes, Java, state sharing support, object-oriented programming, asynchronous model, replicated databases, atomicity, incorrect update ordering, Collaborative software, Object oriented modeling, Cloning, replicated state, shared objects, fine grain user interaction, Concurrency control, data integrity, state reinitialization, Collaboration, concurrency control, Object detection, wide area distributed environments, Collaborative work, Internet, flexible marshalling framework, cloning]
LBF: a performance metric for program reorganization
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
We introduce a new performance metric, called Load Balancing Factor (LBF), to assist programmers with evaluating different tuning alternatives. The LBF metric differs from traditional performance metrics since it is intended to measure the performance implications of a specific tuning alternative rather than quantifying where time is spent in the current version of the program. A second unique aspect of the metric is that it provides guidance about moving work within a distributed or parallel program rather than reducing it. A variation of the LBF metric can also be used to predict the performance impact of changing the underlying network. The LBF metric can be computed incrementally and online during the execution of the program to be tuned. We also present a case study that shows that our metric can predict the actual performance gains accurately for a test suite of six programs.
[Measurement, US Department of Energy, Computational modeling, Debugging, Load Balancing Factor, distributed processing, Educational institutions, program reorganization, Programming profession, parallel programming, case study, Computer science, performance metric, computer network, LBF metric, resource allocation, Integrated circuit testing, NIST, Load management, distributed program, parallel program, program tuning, software performance evaluation, software metrics]
Coordinated exception handling in distributed object systems: from model to system implementation
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
Exception handling in concurrent and distributed programs is a difficult task though it is often necessary. In many cases traditional exception mechanisms for sequential programs are no longer appropriate. One major difficulty is that the process of handling an exception may need to involve multiple concurrent components that are cooperating in pursuit of some global goal. Another complication is that several exceptions may be raised concurrently in different nodes of a distributed environment. Existing proposals and actual concurrent languages either ignore these difficulties or only cope with a limited form of them. The paper attempts a general solution, developed especially for distributed object systems, starting from a conceptual model, together with algorithms for coordinating concurrent components and resolving multiple exceptions, through to an actual system implementation. An industrial production cell is chosen as a case study to demonstrate the usefulness of the proposed model and algorithms. A system that supports coordinated atomic actions and exception resolution is implemented in distributed Ada 95 and examined through several performance-related experiments.
[distributed object systems, algorithms, Communication system control, exception handling, Complexity theory, Proposals, concurrent programs, Distributed computing, parallel processing, Concurrent computing, Production, coordinated atomic actions, distributed Ada 95, Distributed algorithms, coordinated exception handling, object-oriented programming, Object oriented modeling, industrial production cell, exception resolution, global goal, multiple concurrent components, coordinating concurrent components, system implementation, conceptual model, Error correction, performance-related experiments]
Distributed predicate detection in a faulty environment
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
There has been very little research in distributed predicate detection for faulty, asynchronous environments. We define a class of predicates called set decreasing predicates which can be detected in such an environment. We introduce a set of failure detectors called infinitely often accurate detectors which are implementable in asynchronous systems. Based on these failure detectors we present an algorithm to detect conjunction of local predicates and send-monotonic channel predicates. Since perfect failure detection is impossible in an asynchronous system, we cannot guarantee that our detection algorithm will not have false detections. However, if the predicate ever holds then it is guaranteed to be detected.
[program diagnostics, distributed predicate detection, Process control, failure detectors, distributed processing, Electrical fault detection, State-space methods, software fault tolerance, research, Condition monitoring, set decreasing predicates, Fault detection, faulty asynchronous environments, Detectors, local predicates, Detection algorithms, send-monotonic channel predicates, failure detection]
Dynamic load distribution using anti-tasks and load state vectors
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
We propose a new load distribution (LD) algorithm which is based on anti-tasks and load state vectors. Anti-tasks are composite agents which travel around a distributed system to facilitate the pairing up of task senders and receivers, as well as the collection and dissemination of load information. Time-stamped load information of processing nodes is stored in load state vectors which, when used together with anti-tasks, encourage mutual sharing of load information among processing nodes. Anti-tasks, which make use of load state vectors to decide their travelling paths, are spontaneously directed towards processing nodes having high transient workload, thus allowing their surplus work-load to be relocated quickly.
[task receivers, Costs, Heuristic algorithms, task senders, load information dissemination, time-stamped load information, Performance gain, distributed processing, distributed system, Distributed computing, Delay, anti-tasks, Computer science, load state vectors, Runtime, dynamic load distribution, resource allocation, composite agents, Bandwidth, Distributed control, Local area networks, software performance evaluation]
CLIQUES: a new approach to group key agreement
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
The paper considers the problem of key agreement in a group setting with highly dynamic group member population. A protocol suite, called CLIQUES, is developed by extending the well known Diffie-Hellman key agreement method to support dynamic group operations. Constituent protocols are provably secure and efficient.
[Access control, Peer to peer computing, Laboratories, Access protocols, dynamic group operations, Multicast protocols, Security, group setting, CLIQUES, highly dynamic group member population, Diffie-Hellman key agreement method, Privacy, protocol suite, security of data, Authentication, DH-HEMTs, groupware, constituent protocols, Cryptography, protocols, group key agreement]
Performance comparison of architectures for client-server interactions in CORBA
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
Middleware promotes interoperability as well as provides transparent location of servers in heterogeneous client-server environments. Although a number of benefits are provided by middleware, careful consideration of system architecture is required to achieve high performance. Based on implementation and measurements made on a network of workstations running a commercial CORBA compliant ORB called ORBeline the paper is concerned with the impact of client-agent-server interaction architecture on performance. The paper reports on the relative performances of three interaction architectures under different workload conditions. In particular the impact of inter-node delays, message size, and request service times on the latency and scalability attributes of these architectures is analyzed. A method called agent cloning and how it can be used for improving system performance are described.
[open systems, latency, workstation network, inter-node delays, local area networks, agent cloning, Distributed computing, Delay, scalability, Network servers, Computer architecture, commercial CORBA compliant ORB, Computer networks, request service times, software performance evaluation, middleware, client-server systems, Client-server systems, Object oriented modeling, transparent server location, message size, ORBeline, interoperability, client-agent-server interaction architecture, Application software, Middleware, heterogeneous client-server environments, workload conditions, delays, system architecture, Systems engineering and theory, performance comparison]
NFS/M: an open platform mobile file system
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
With the advancement of wireless networks and mobile computing, there is an increasing need to build a mobile file system that can perform efficiently and correctly for accessing online information. Previous system research on mobile file systems is based on some experimental platforms. We describe the design and implementation of a mobile file system on an open platform, the Linux kernel, and at the same time, our mobile file system is compatible with the popular NFS 2.0 protocol. We formally define the file semantics of our mobile file system, which we called NFS/M. We also specify the conditions of object conflict as well as our conflict resolution algorithms. NFS/M supports client side caching, data prefetching, file system service during the disconnected mode, data reintegration and conflict resolution on various file system objects. Since the NFS/M is based on an open platform, it serves as a basic building block for developing future mobile computing applications.
[Portable computers, open systems, Linux kernel, object conflict, NFS 2.0 protocol, data prefetching, cache storage, Distributed computing, Network servers, mobile computing, File systems, online information access, network operating systems, open platform, file semantics, client side caching, Computer networks, Communication networks, protocols, Availability, operating system kernels, data reintegration, conflict resolution algorithms, wireless networks, NFS/M, Computer applications, Communication channels, wireless LAN, mobile file system, Mobile computing]
Signalling adaptors between distributed systems and telecommunication networks
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
Conventional telecommunication software components are mainly developed using message passing as the underlying communication mechanisms. Recent standards on modern distributed systems, exemplified by CORBA and DCE, however, have converged to a new programming paradigm based on the client/server architecture, object-oriented model, and remote procedure calls (RPCs). In the near future, we envision the coexistence of conventional and CORBA or DCE-based telecommunication software components. As a result, protocol converters between the network components using a message-oriented protocol and the ones using RPCs have become an important issue. This paper addresses how to interwork these two different communication paradigms using a signalling adaptor (SA), which is a protocol converter for enabling seamless communications between message-based systems and object-oriented, RPC-based systems.
[Protocols, Costs, object-oriented model, Distributed computing, Intelligent networks, Network servers, protocol converters, signalling adaptors, telecommunication networks, distributed systems, message-based systems, Computer networks, object-oriented methods, protocols, Signal design, client-server systems, message passing, Object oriented modeling, DCE, client server architecture, Telecommunication standards, message-oriented protocol, standards, CORBA, telecommunication software components, Communication industry, remote procedure calls]
Consistency conditions for multi-object distributed operations
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
The traditional distributed shared memory (DSM) model provides atomicity at levels of read and write on single objects. Therefore, multi-object operations such as double compare and swap, and atomic m-register assignment cannot be efficiently expressed in this model. We extend the traditional DSM model to allow operations to span multiple objects. We show that memory consistency conditions such as sequential consistency and linearizability can be extended to this general model. We also provide algorithms to implement these consistency conditions in a distributed system.
[sequential consistency, double compare, Costs, message passing, distributed shared memory, atomicity, atomic m-register assignment, Read-write memory, multi-object distributed operations, distributed system, data integrity, Transaction databases, Programming profession, linearizability, distributed algorithms, Coherence, distributed memory systems, shared memory systems, Hardware, memory consistency conditions, Testing]
Real-time primary-backup (RTPB) replication with temporal consistency guarantees
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
The paper presents a real-time primary-backup (RTPB) replication scheme for supporting fault-tolerant real-time applications. It formally defines two types of temporal consistency, namely external temporal consistency and inter-object temporal consistency. By introducing a key concept called phase variance, the authors are able to build temporal consistency models and derive necessary and sufficient conditions that can be used as the basis for update and transmission scheduling that achieve temporal consistency guarantees. Furthermore, they prove that the term phase variance used in the models can be bounded under various scheduling algorithms, namely EDF, rate monotonic and distance-constrained scheduling. A RTPB implementation was developed within the x-kernel architecture on the MK 7.2 microkernel and the results of a detailed performance evaluation is also discussed.
[Airplanes, update scheduling, temporal consistency models, Laboratories, EDF scheduling, real-time primary-backup replication, external temporal consistency, fault-tolerant real-time applications, MK 7.2 microkernel, x-kernel architecture, processor scheduling, Condition monitoring, Fault tolerance, scheduling algorithms, Fault tolerant systems, software performance evaluation, inter-object temporal consistency, object-oriented programming, rate monotonic scheduling, Buildings, performance evaluation, Transaction databases, distance-constrained scheduling, Application software, phase variance, Scheduling algorithm, software fault tolerance, real-time systems, concurrency control, Acceleration, temporal consistency guarantees, transmission scheduling]
Competitive sourcing for Internet commerce
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
In electronic commerce on the Internet, a customer can choose among several competitive suppliers, but because of the nature of the Internet, the reliability and trustworthiness of suppliers may vary significantly. The customer's goal is to maximize its utility, by minimizing the expense required to fulfil its request, and maximizing its probability of success by some deadline. To this end, the customer creates a request strategy, describing which suppliers to contact under what conditions. We describe models for representing request strategies complete with supplier reliabilities, delivery timeliness profiles, and customer deadlines. We also develop decision procedures for selecting request strategies that maximize expected utility under certain scenarios, and more efficient heuristics that approximate the optimal solution.
[decision theory, delivery timeliness profiles, business communication, Electronic commerce, Environmental economics, competitive sourcing, heuristics, IP networks, Business, electronic commerce, request strategy, Internet commerce, Europe, probability, marketing data processing, optimal solution, competitive suppliers, supplier reliability, Computer science, Utility theory, Layout, decision procedures, Internet, Time factors, business data processing, customer deadlines]
Wide-area Nile: a case study of a wide-area data-parallel application
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
The Nile system is a distributed environment for running very large, data-intensive applications across a network of commodity workstations. These applications process data from elementary particle collisions, generated by the Cornell Electron Storage Ring, and are used by physicists of the CLEO experiment. The applications have a simple data-parallel structure, and so Nile executes them using as much parallelism as is available. Nile currently runs at any single site. It is being used by alpha testers and is scheduled for beta release in March 1998. We describe how we are adapting this local-area Nile system to allow for wide-area, multiple site interactions. In particular, we consider the two problems of scaling and of fault tolerance.
[data-intensive applications, Computer aided software engineering, wide area networks, high energy physics instrumentation computing, workstation network, local area networks, distributed environment, parallel processing, CLEO experiment, Fault tolerance, Electron beams, Cornell Electron Storage Ring, multiple site interactions, Detectors, Nile system, Testing, fault tolerance, wide area data parallel application, elementary particle collisions, Application software, Physics, case study, Processor scheduling, Collaboration, local area network, fault tolerant computing, Resource management, wide area network]
Robust state sharing for wide area distributed applications
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
We present the Mocha wide area computing infrastructure we are developing. Mocha provides support for robust shared objects on heterogeneous platforms, and utilizes advanced distributed shared memory techniques for maintaining consistency of shared objects that are replicated at multiple nodes to improve performance. In addition, our system handles failures that we feel will be common in wide area environments. We have used an approach that makes use of multiple communication protocols to improve the efficiency of shared object state transfers in Mocha. We also provide an empirical evaluation of our prototype implementation for local area, wide area, and home service networks and present a sample home service application that has been programmed with the system.
[Protocols, wide area networks, distributed shared memory, World Wide Web, Electrical capacitance tomography, replicated objects, wide area computing infrastructure, Prototypes, shared memory systems, Robustness, Computer networks, Workstations, protocols, robust shared objects, Java, robust state sharing, communication protocols, performance evaluation, Educational institutions, data integrity, wide area distributed applications, Mocha, system failures, performance, heterogeneous platforms, home service networks, local area network, distributed memory systems, fault tolerant computing, shared object state transfers, Web sites, wide area network]
A framework for dependability driven software integration
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
The integration of system and SW functions for efficiency, performance and especially dependability is of interest from a research and system design perspective. We propose a framework for directing the process of integration of SW functions, with the objective of designing and maintaining desired dependability attributes of the system over the integration process. Rules of composition for integrated functions, and measures to quantify the goodness of dependable system integration are also addressed.
[Navigation, software reliability, Sensor phenomena and characterization, system design, Throughput, Displays, Information management, Security, Guidelines, software integration, Fault tolerance, performance, software dependability, Timing, Collision avoidance, software performance evaluation]
A multithreaded message-passing system for high performance distributed computing applications
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
NYNET (ATM wide area network testbed in New York state) Communication System (NCS) is a multithreaded message passing system developed at Syracase University that provides high performance and flexible communication services over asynchronous transfer mode (ATM) based high performance distributed computing (HPDC) environments. NCS capitalizes on thread based programming model to overlap computations and communications, and develop a dynamic message passing environment with separate data and control paths. This leads to a flexible and adaptive message passing environment that can support multiple flow control, error control, and multicasting algorithms. We provide an overview of the NCS architecture and present how NCS point to point communication services are implemented. We also analyze the overhead incurred by using multithreading and compare the performance of NCS point to point communication primitives with those of other message passing systems such as p4, PVM, and MPI. Benchmarking results indicate that NCS shows comparable performance to other systems for small message sizes but outperforms other systems for large message sizes.
[System testing, NCS architecture, wide area networks, p4, Communication system control, PVM, MPI, NCS point to point communication services, asynchronous transfer mode, flexible communication services, Distributed computing, Yarn, parallel programming, network interfaces, adaptive message passing environment, error control, multithreaded message passing system, dynamic message passing environment, Dynamic programming, Wide area networks, message passing, Computational modeling, message size, NYNET, ATM wide area network testbed in New York state, high performance distributed computing environments, high performance distributed computing applications, Programmable control, multiple flow control, Message passing, multicasting algorithms, thread based programming model, Asynchronous transfer mode]
Optimal channel allocation for data dissemination in mobile computing environments
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
We discuss the wireless channel allocation problem for data dissemination in mobile computing systems. Methods for accessing data through broadcast and on-demand channels are described. We provide analytical models and cost formulae for the exclusive broadcast channels and the exclusive on-demand channels and propose a dynamic channel allocation algorithm for optimizing system performance. Our performance evaluation shows that dynamic channel allocation significantly improves system performance and the channel allocation algorithm gives us the optimal solution for various system parameter settings.
[telecommunication channels, wireless channel allocation problem, Heuristic algorithms, Broadcast technology, mobile computing environments, analytical models, on-demand channels, Analytical models, System performance, Bandwidth, Broadcasting, optimal channel allocation, Cost function, dynamic channel allocation algorithm, performance evaluation, data dissemination, Processor scheduling, data access, Channel allocation, data communication, wireless LAN, broadcast channels, cost formulae, Mobile computing, portable computers]
A suite of database replication protocols based on group communication primitives
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
This paper proposes a family of replication protocols based on group communication in order to address some of the concerns expressed by database designers regarding existing replication solutions. Due to these concerns, current database systems allow inconsistencies and often resort to centralized approaches, thereby reducing some of the key advantages provided by replication. The protocols presented in this paper take advantage of the semantics of group communication and use related isolation guarantees to eliminate the possibility of deadlocks, reduce the message overhead, and increase performance. A simulation study shows the feasibility of the approach and the flexibility with which different types of bottlenecks can be circumvented.
[Protocols, Communication systems, database replication protocols, bottlenecks, Electronic mail, Delay, simulation study, Information systems, database design, Distributed databases, Database systems, deadlocks, software performance evaluation, Availability, message passing, replicated databases, memory protocols, data integrity, Transaction databases, group communication, database inconsistencies, performance, concurrency control, System recovery, message overhead]
Methodologies for distributed information retrieval
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
Text collections have traditionally been located at a single site and managed as a monolithic whole. However, it is now common for a collection to be spread over several hosts and for these hosts to be geographically separated. The authors examine several alternative approaches to distributed text retrieval. They report on their experience with a full implementation of these methods, and give retrieval efficiency and retrieval effectiveness results for collections distributed over both a local area network and a wide area network. They conclude that, compared to monolithic systems, distributed information retrieval systems can be fast and effective, but that they are not efficient.
[Wide area networks, wide area networks, information retrieval, geographically separated hosts, Information retrieval, Magnetic heads, local area networks, Distributed computing, information retrieval system evaluation, distributed information retrieval methodologies, Computer science, text collections, retrieval effectiveness, Software libraries, distributed text retrieval, retrieval efficiency, Distributed databases, local area network, Database systems, Australia, Local area networks, wide area network]
Khazana: an infrastructure for building distributed services
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
Essentially all distributed systems, applications, and services at some level boil down to the problem of managing distributed shared state. Unfortunately, while the problem of managing distributed shared state is shared by many applications, there is no common means of managing the data-every application devises its own solution. We have developed Khazana, a distributed service exporting the abstraction of a distributed persistent globally shared store that applications can use to store their shared state. Khazana is responsible for performing many of the common operations needed by distributed applications, including replication, consistency management, fault recovery, access control and location management. Using Khazana as a form of middleware, distributed applications can be quickly developed from corresponding uniprocessor applications through the insertion of Khazana data access and synchronization operations.
[Access control, data management, distributed processing, Information management, system recovery, distributed applications, storage management, Distributed databases, authorisation, distributed systems, access control, middleware, replication, client-server systems, location management, Collaborative software, Khazana, persistent globally shared store, Application software, consistency management, uniprocessor applications, Middleware, Programming profession, software fault tolerance, synchronisation, Computer science, fault recovery, Collaboration, Collaborative work, synchronization, distributed shared state management, distributed services]
Reflective data sharing in managing Internet databases
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
The popularity of the Internet has generated an explosion in the number of accessible information sources. In addition, recent advances in wide-area networking have led to a push for a logically-unified, yet physically distributed, information repository accessible through the Internet. The architecture of data resources and sources should be scalable and able to accommodate hundreds and thousands of databases. These are called Internet databases. This has to be achieved against a backdrop of accommodating database autonomy and bridging their heterogeneity with reasonable overhead. In addition, manual and evolution management needs a careful design as ad hoc management is clearly intractable in a highly dynamic environment as the Internet. Therefore, there is a need for supporting a self-adjustable model of evolution. We propose an architecture that has the ability to reflect upon its own state and to decide when to evolve. The use of flexible constructs like service links, coalitions, and co-databases provide a dynamic and portable model for eliciting data sharing. Further, we propose an implementation model using CORBA and Web related technologies.
[Scalability, data resources, co-databases, World Wide Web, Environmental management, Intelligent networks, Databases, service links, Prototypes, Management information systems, distributed databases, distributed information repository, object-oriented methods, IP networks, client-server systems, self-adjustable model, Explosions, coalitions, CORBA, information sources, reflective data sharing, Internet databases, Internet, data handling, Australia, wide area network, data sharing]
Quarterware for middleware
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
We make two observations about communications middleware: first, most middleware are similar, the differences are in their interfaces and optimizations; second, neither a fixed set of abstractions nor a fixed implementation of a set of abstractions is likely to be sufficient and well-performing for all applications. Based on these observations, we present Quarterware, a customizable middleware architecture. It abstracts basic middleware functionality, and admits application specific specializations and extensions. We demonstrate its flexibility by deriving implementations for core facilities of CORBA, RMI, and MPI. The performance results show that the derived implementations equal or exceed the performance of corresponding native versions. These results suggest that customizing middleware on a per-application basis is an effective approach for building robust, high-performance applications.
[Quarterware, MPI, Electronic mail, Design optimization, Network servers, optimization, Abstracts, middleware functionality, Robustness, object-oriented methods, Web server, Assembly, software performance evaluation, client-server systems, message passing, customizable middleware architecture, Buildings, Middleware, Message Passing Interface, abstractions, RMI, Computer science, CORBA, Remote Method Invocation, application specific specialization, high-performance applications]
On improving reachability analysis for verifying progress properties of networks of CFSMs
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
State explosion is well-known to be the principle limitation in protocol verification. In this paper, leaping reachability analysis (LRA) is advocated as an incremental improvement of a verification technique called simultaneous reachability analysis (SRA) to tackle state explosion. SRA is a relief strategy for the verification of progress properties of protocols modeled as networks of communicating finite state machines (CFSMs) without any topological or structural constraints. The improvement is a uniform and property-driven relief strategy which proves to be adequate for detecting all deadlocks, all non-executable transitions, all unspecified receptions and all buffer overflows in a protocol specified in the CFSM model. Experiments show that LRA can largely relieve the state explosion problem by reducing the amount of storage space and execution time required for verification.
[protocol verification, Protocols, Buffer storage, state explosion, unspecified receptions, finite state machines, deadlock detection, formal verification, storage space, leaping reachability analysis, communicating finite state machines, protocols, progress properties, reachability analysis, CFSM, buffer overflows, Explosions, State-space methods, Topology, Reachability analysis, Information technology, simultaneous reachability analysis, property-driven relief strategy, Automata, concurrency control, nonexecutable transitions, execution time, System recovery, Buffer overflow]
Efficient ordered broadcasting in reliable CSMA/CD networks
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
Ordered broadcast is a communication paradigm which requires that all the nodes in the network receive a message, and that the nodes receive the different messages in the same order. Ordered broadcast is particularly useful for supporting fault tolerance in distributed systems. We present a protocol for ordered broadcasts in a CSMA/CD network that uses the high reliability and collision detection properties of these networks in a novel manner. The control of the protocol is distributed, and even if the sender of a message fails after transmitting a message which is missed by some nodes, the ordered broadcast property is satisfied. The protocol has a small overhead, but requires both hardware and software support for implementation.
[collision detection, Protocols, Ethernet networks, message passing, fault tolerance, CSMA/CD networks, reliability, local area networks, Multiaccess communication, Delay, broadcasting, ordered broadcasting, Intelligent networks, Fault tolerance, protocol, Computer network reliability, Broadcasting, distributed systems, Hardware, communication paradigm, computer network reliability, Telecommunication network reliability, carrier sense multiple access]
Group communication protocol for real-time applications
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
In distributed applications, a group of multiple processes cooperate by exchanging messages. It is critical to support the group of application processes with enough quality of service (QoS) including the ordered delivery of messages. The delay time and the message loss ratio are significant QoS parameters. In Internet applications, the delay time and the loss ratio are significantly different in different communication channels. The authors define a novel causality named /spl Delta/*-causality among the messages to hold in the world-wide environment. They discuss how to transmit messages to the destination processes and how to resolve message loss and delay supporting the /spl Delta/*-causality given the requirements of delay time and message loss ratio.
[Protocols, message passing, telecommunication channels, message exchange, message transmission, Internet applications, multiple processes cooperation, /spl Delta/*-causality, quality of service, losses, group communication protocol, distributed applications, real-time applications, ordered message delivery, delay time, real-time systems, delays, message loss ratio, Internet, destination processes, protocols, communication channels, world-wide environment]
Using leases to support server-driven consistency in large-scale systems
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
The paper introduces volume leases as a mechanism for providing cache consistency for large scale, geographically distributed networks. Volume leases are a variation of leases, which were originally designed for distributed file systems. Using trace driven simulation, we compare two new algorithms against four existing cache consistency algorithms and show that our new algorithms provide strong consistency while maintaining scalability and fault tolerance. For a trace based workload of Web accesses, we find that volumes can reduce message traffic at servers by 40% compared to a standard lease algorithm, and that volumes can considerably reduce the peak load at servers when popular objects are modified.
[Protocols, peak load, File servers, cache storage, Electrical capacitance tomography, Delay, scalability, Network servers, cache consistency algorithms, distributed databases, Computer networks, Large-scale systems, cache consistency, Web server, Robots, volume leases, Wide area networks, client-server systems, fault tolerance, popular objects, large scale geographically distributed networks, information retrieval, trace based workload, server driven consistency, data integrity, Web accesses, message traffic, large scale systems, Internet]
Addressing false causality while detecting predicates in distributed programs
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
The partial-order model of distributed computations based on the happened before relation has been criticized for allowing false causality between events. Our strong causality model addresses this problem by allowing multiple local threads of control. This paper addresses the predicate detection problem for the class of weak conjunctive predicates in the strong causality model. We show that, in general, the problem is NP-complete. However, an efficient solution is demonstrated for a useful sub-case. Further, this solution can be used to achieve an exponential reduction in time for solving the general problem. Our predicate detection algorithms can be applied to distributed debugging when processes have independent events, as in multi-threaded processes.
[program debugging, Event detection, distributed processing, NP-complete, Distributed computing, Yarn, parallel programming, Concurrent computing, false causality, weak conjunctive predicates, partial-order model, Testing, Educational programs, Computational modeling, Debugging, multi-threaded processes, distributed programs, strong causality model, predicate detection problem, Processor scheduling, distributed computations, distributed debugging, multiple local threads of control, Detection algorithms, computational complexity]
Design and implementation of a distributed X-multiplexor
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
Application sharing is an important aspect of collaborative computing. One mechanism for sharing applications is a multiplexor, a tool that broadcasts input/output messages from a single client to multiple displays. We describe the design and implementation of a multiplexor for X windows that supports a heterogeneous environment and works effectively across both local and wide area networks. This X-multiplexor is composed of two distinct components, the Collaborative Computing Frameworks X-Multiplexor (CCFX) which acts as a pseudo server for the client, and the Collaborative Computing Session Manager (CCSM) which renders the client images and supplies the user interface to the system. In effect, CCSM acts as a distributed window manager, ensuring that all participants have the same view of the shared clients. This two headed approach offers significant gains in flexibility and resource usage. CCFX and CCSM can communicate using any reliable transport protocol. This can result in significant reduction of bandwidth, decreased latency, and avoids some TCP/IP communication required by the X protocol. In addition, this architecture facilitates the creation of a virtual X environment. Using this virtual environment, instead of an actual X server's environment, and then translating the virtual properties to a specific server's properties, allows for more robust support of heterogeneous systems.
[Computer interfaces, Transport protocols, distributed window manager, wide area networks, application sharing, reliable transport protocol, Collaborative Computing Frameworks X-Multiplexor, Displays, local area networks, user interfaces, Collaborative Computing Session Manager, heterogeneous systems support, TCP/IP communication, Network servers, Bandwidth, groupware, Broadcasting, heterogeneous environment, virtual X environment, distributed X-multiplexor, Wide area networks, client-server systems, input/output messages, collaborative computing, two headed approach, user interface, resource usage, multiplexing, pseudo server, virtual environment, User interfaces, Collaborative work, Rendering (computer graphics)]
A mechanism for establishing policies for electronic commerce
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
The paper introduces a mechanism for establishing policies for electronic commerce in a unified and secure manner. A commercial policy can be viewed as the embodiment of a contract between the principals involved in a certain type of commercial activity, and it may be concerned with such issues as: ensuring that a payment for services is refunded under specified circumstances; preventing certificates representing e-cash from being duplicated; ensuring that credit card numbers are used only for the transaction they are intended for; and, for certain socially sensitive transactions like the purchase of drugs, ensuring auditability by proper authorities. Our mechanism is based on a previously published concept of law governed interaction. It makes a strict separation between the formal statement of a policy, which we call a "law," and the enforcement of this law, which is carried out by a set of policy independent trusted controllers. A new policy under this scheme is created basically by formulating its law, and can be easily deployed throughout a distributed system. This mechanism enables a single agent to engage in several different activities, subject to disparate policies. Two example policies are discussed in detail: one ensures refundability of payment under certain circumstances; the other provides for payment by means of non copyable tickets.
[Drugs, transaction processing, formal statement, Protocols, commercial policy, Subscriptions, business communication, distributed system, commercial activity, Electronic commerce, commerce, contracts, electronic commerce policies, e-cash, Contracts, Business, client-server systems, law governed interaction, contract, Credit cards, policy independent trusted controllers, credit card numbers, socially sensitive transactions, Computer science, payment refundability, non copyable tickets, security of data, certificates, Internet, auditability, Merchandise]
HADES: a middleware support for distributed safety-critical real-time applications
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
Most distributed safety critical real time systems designed in the past have been specialized to meet the particular requirements of the application domain to which they were targeted. This approach led to specific, inflexible, dedicated and non reusable solutions, often based on specialized hardware. The paper presents an overview of HADES, which provides a set of flexible tools built on top of off the shelf hardware, and designed to help in the construction of a panel of distributed safety critical real time applications. In order for HADES to support the execution of the widest range of applications, we have followed a rigorous methodology based on: (i) the separation of services dedicated to a specific application domain (scheduling policy) from services providing a range of robustness properties common to a large spectrum of application domains (e.g. task dispatching, fault detection, clock synchronization, monitoring); (ii) the provision of a precise cost information induced by all these services in order to increase the accuracy of the application feasibility test.
[Real time systems, application domain, application domains, specialized hardware, clock synchronization, middleware support, non reusable solutions, safety-critical software, fault detection, HADES, cost information, rigorous methodology, task dispatching, scheduling, Hardware, Robustness, Safety, Monitoring, robustness properties, client-server systems, Synchronization, Middleware, Highly Available Distributed Embedded System, Fault detection, scheduling policy, real-time systems, application feasibility test, Dispatching, off the shelf hardware, distributed safety critical real time applications, Clocks, distributed safety critical real time systems]
A protocol and correctness proofs for real-time high-performance broadcast networks
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
Novel real time applications require high performance real time distributed systems, and therefore high performance real time networks. We examine a Hard Real Time Distributed Multiaccess problem which arises with such application problems. We present a solution, based on broadcast LANs or busses, such as Gigabit Ethernets or busses internal to ATM nodes, associated with a deterministic Ethernet-like protocol called CSMA/Deadline Driven Collision Resolution. We give an analysis of balanced m ary tree algorithms which are used by CSMA/DDCR, and derive feasibility conditions for the HRTDM problem.
[Real time systems, Algorithm design and analysis, high performance real time distributed systems, busses, Ethernet networks, correctness proofs, deterministic Ethernet-like protocol, Gigabit Ethernets, Switches, balanced m ary tree algorithms, HRTDM problem, local area networks, CSMA/Deadline Driven Collision Resolution, high performance real time networks, CSMA/DDCR, formal verification, real time high performance broadcast networks, ATM nodes, Broadcasting, real time applications, multi-access systems, protocols, trees (mathematics), Access protocols, Application software, broadcasting, feasibility conditions, Upper bound, Hard Real Time Distributed Multiaccess problem, broadcast LANs, real-time systems, Systems engineering and theory, Asynchronous transfer mode, carrier sense multiple access]
Experience with secure multi-processing in Java
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
As the Java/sup TM/ platform is the preferred environment for the deployment of network computers, it is appealing to run multiple applications on a single Java enabled desktop. We experimented with using the Java platform as a multiprocessing, multi user environment. Although the Java Virtual Machine (JVM) is not inherently a single application design, we have found that the implementation of the Java Development Kit (JDK) often implicitly assumes that the JVM runs exactly one application at any one time. We report on the limitations we encountered and propose improvements to several aspects of the Java technology architecture, including its security features. We have implemented all the proposed changes in a prototype based on an in-house beta version of JDK 1.2. Our prototype uses a Bourne shell like command line tool to launch multiple applications (such as Appletviewer) within one JVM.
[Access control, Java Development Kit, JVM, security features, Java enabled desktop, multi user environment, Security, JDK, Java technology architecture, Prototypes, multi-access systems, Virtual prototyping, Protection, in-house beta version, Java, object-oriented programming, multiprocessing systems, network computers, Bourne shell like command line tool, Java platform, Virtual machining, Appletviewer, Application software, multiple applications, Sun, Java Virtual Machine, Computer science, security of data, virtual machines, object-oriented languages, secure multiprocessing]
A feedback based scheme for improving TCP performance in ad-hoc wireless networks
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
Ad-hoc networks are completely wireless networks of mobile hosts, in which the topology rapidly changes due to the movement of mobile hosts. This frequent topology may lead to sudden packet losses and delays. Transport protocols like TCP have been built mainly for reliable, fixed networks. Hence, when used in ad-hoc networks, TCP misinterprets this loss as congestion and invokes congestion control. This leads to unnecessary retransmissions and loss of throughput. To overcome this problem, a feedback scheme is proposed, so that the source can distinguish between route failure and network congestion. When a route is disrupted, the source is sent a route failure notification (RFN) packet, allowing it to freeze its timers and stop sending packets. When the route is re-established, the source is informed through a route re-establishment notification (RRN) packet, upon which it resumes by unfreezing timers and continuing packet transmissions. The simulated performance of TCP on ad-hoc networks with and without feedback is compared and reported. It is observed that in the event of route failures, as the route re-establishment time increases, the use of feedback provides significant gains in throughput as well as savings in unnecessary packet transmissions. Several further enhancements and directions for future work are also sketched.
[Transport protocols, feedback scheme, telecommunication congestion control, route re-establishment notification, Throughput, Electronic mail, Delay, Network topology, Wireless networks, Feedback, mobile hosts, route failure notification packet, packet losses, computer network reliability, throughput, TCP performance, topology, network congestion, performance evaluation, congestion control, Ad hoc networks, ad-hoc wireless networks, Computer science, transport protocols, delays, wireless LAN, route failure, Mobile computing, portable computers]
Agents negotiating for load balancing of electricity use
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
Emerging technologies allowing two-way communication between utility companies and their customers, as well as with smart equipment, are changing the rules of the energy market. Deregulation makes it even more demanding for utility companies to create new business processes for mutual benefit. Dynamic load management of the power grid is essential to make better and more cost-effective use of electricity production capabilities, and to increase customer satisfaction. The compositional development method DESIRE has been used to analyse, design, implement and verify a multi-agent system capable of negotiation for load management.
[smart equipment, Energy consumption, Production systems, Protocols, Costs, load balancing, electricity use, cost-effective, Companies, business processes, Mathematics, Uniform resource locators, customer satisfaction, deregulation, compositional development method, cooperative systems, dynamic load management, two-way communication, multi-agent system, software agents, power engineering computing, load management, Computer science, utility companies, Consumer behavior, agent negotiation, DESIRE, Load management, electricity supply industry]
Adapting to bandwidth variations in wide-area data combination
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
Efficient data combination over wide area networks is hard as these networks have large variations in available bandwidth. We examine the utility of changing the location of combination operations as a technique to adapt to variations in network bandwidth. We try to answer the following questions. First, does relocation of operators provide a significant performance improvement? Second, is online relocation useful or does a one-time positioning at start-up time provide most if not all the benefits? If online relocation is useful, how frequently should it be done and is global knowledge of network performance required or can local knowledge and local relocation of operators be sufficient? Fourth, does the effectiveness of operator relocation depend on the ordering of the combination operations. That is, are certain ways of ordering more amenable to adaptation than others? Finally, how do the results change as the number of data sources changes?.
[data sources, Costs, wide area networks, performance evaluation, Educational institutions, performance improvement, Delay, online relocation, Computer science, operator relocation, network performance, Bandwidth, NIST, data communication, Computer networks, bandwidth variations, wide-area data combination, combination operations]
Fast, optimized Sun RPC using automatic program specialization
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
Fast remote procedure call (RPC) is a major concern for distributed systems. Many studies aimed at efficient RPC consist of either new implementations of the RPC paradigm or manual optimization of critical sections of the code. This paper presents an experiment that achieves automatic optimization of an existing, commercial RPC implementation, namely the Sun RPC. The optimized Sun RPC is obtained by using an automatic program specializer. It runs up to 1.5 times faster than the original Sun RPC. Close examination of the specialized code does not reveal further optimization opportunities which would lead to significant improvements without major manual restructuring. The contributions of this work are: the optimized code is safely produced by an automatic tool and thus does not entail any additional maintenance; to the best of our knowledge this is the first successful specialization of mature, commercial, representative system code; and the optimized Sun RPC runs significantly faster than the original code.
[Transport protocols, optimising compilers, Performance gain, Telecommunications, software maintenance, Sun, Programming profession, program optimization, Computer science, partial evaluation (compilers), Operating systems, experiment, network operating systems, Production, remote procedure calls, distributed systems, partial evaluation, Hardware, Safety, software tools, Sun RPC, remote procedure call, software performance evaluation, automatic program specialization]
Structuring remote object systems for mobile hosts with intermittent connectivity
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
In distributed object systems, remote method invocation attempts to abstract communication between different machines to the level of object method invocation. Link failure in such systems often appears as a failed method invocation, which must be explicitly handled outside the remote object infrastructure. This usually implies that techniques to handle the effects of link failure are embedded within application functionality. The resulting application can become unnecessarily specialized to a particular environment, restricting the situations where it could be used. We explore the possibility of cleanly decoupling policies for handling a link failure from the application. With this goal in mind, we present extensions to the Java RMI system in the context of a distributed system with mobile hosts. An intermittent communication link is typical in such systems, and applications use techniques such as data caching or queuing of method calls to alleviate the effects. We show how such techniques and policies can be implemented generically so that not only does the application become oblivious of them, but also policies can be chosen dynamically depending on a particular situation.
[distributed object systems, intermittent communication link, Laboratories, Java RMI, distributed processing, distributed system, Mobile communication, cache storage, intermittent connectivity, Delay, Guidelines, remote object systems, data caching, mobile hosts, National electric code, Computer architecture, Robustness, Context, Java, object-oriented programming, Application software, link failure, software fault tolerance, object method invocation, remote method invocation, method call queueing, object-oriented languages, remote procedure calls]
Flexible protocol composition in BAST
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
The paper presents BAST, an object-oriented library of reliable distributed protocols. The authors show how BAST can be used to build fault-tolerant distributed applications, and how new protocols can be added to it. They discuss some distributed protocol design issues and the way these issues are circumvented in BAST. They briefly describe their Smalltalk and Java implementations of BAST, together with some performance results.
[Protocols, distributed processing, distributed protocol design, Java implementation, software libraries, Fault tolerance, Runtime, Prototypes, Broadcasting, Libraries, protocols, software performance evaluation, object-oriented library, Java, object-oriented programming, Smalltalk implementation, flexible protocol composition, Programming profession, software fault tolerance, reliable distributed protocols, fault-tolerant distributed application building, performance, Message passing, Telecommunication network reliability, BAST]
Fault-tolerant adaptive and minimal routing in mesh-connected multicomputers using extended safety levels
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
The minimal routing problem in mesh-connected multicomputers with faulty blocks is studied. Two dimensional (2D) meshes are used to illustrate the approach. A sufficient condition for minimal routing in 2D meshes with faulty blocks is proposed. Unlike many other models that assume all the nodes know global fault distribution, our approach is based on the concept of an extended safety level which is a special form of limited fault information. Fault information is distributed to a limited number of nodes while it is still sufficient to support minimal routing. We study the existence of minimal paths at a given source node, limited distribution of fault information, and minimal routing itself. The proposed approach is also adaptive which allows all messages to use any minimal path. Our approach is the first attempt to address adaptive and minimal routing in 2D meshes with faulty blocks using limited fault information.
[extended safety levels, Costs, parallel architectures, multiprocessor interconnection networks, fault-tolerant adaptive routing, Fault tolerance, mesh-connected multicomputers, Network topology, 2D meshes, Hypercubes, Routing protocols, minimal routing, Safety, Communication networks, multiprocessing systems, message passing, network routing, two dimensional meshes, global fault distribution, Communication switching, Computer science, faulty blocks, fault tolerant computing, minimal paths, Telecommunication network reliability]
Toward transparent selective sequential consistency in distributed shared memory systems
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
This paper proposes a transparent selective sequential consistency approach to distributed shared memory (DSM) systems. First, three basic techniques-time selection, processor selection, and data selection-are analyzed for improving the performance of strictly sequential consistency DSM systems, and a transparent approach to achieving these selections is proposed. Then, this paper focuses on the protocols and techniques devised to achieve transparent data selection, including a novel selective lazy/eager updates propagation protocol for propagating updates on shared data objects, and the critical region updated pages set scheme to automatically detect the associations between shared data objects and synchronization objects. The proposed approach is able to offer the same potential performance advantages as the entry consistency model or the scope consistency model, but it imposes no extra burden to programmers and never fails to execute programs correctly. The devised protocols and techniques have been implemented and experimented with in the context of the TreadMarks DSM system. Performance results have shown that for many applications, our transparent data selection approach outperforms the lazy release consistency model using a lazy or eager updates propagation protocol.
[Protocols, data selection, TreadMarks, Distributed computing, shared data objects, shared memory systems, DSM systems, eager updates propagation protocol, processor selection, Performance analysis, transparent data selection, selective lazy propagation protocol, transparent selective sequential consistency, scope consistency model, performance evaluation, memory protocols, data integrity, Sun, Information technology, Programming profession, synchronisation, Parallel programming, performance, Message passing, Object detection, distributed memory systems, distributed shared memory systems, time selection, critical region updated pages set scheme, entry consistency model, Australia, synchronization objects]
Dynamic load balancing in geographically distributed heterogeneous Web servers
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
With ever increasing Web traffic, a distributed multi server Web site can provide scalability and flexibility to cope with growing client demands. Load balancing algorithms to spread the requests across multiple Web servers are crucial to achieve the scalability. Various domain name server (DNS) based schedulers have been proposed in the literature, mainly for multiple homogeneous servers. The presence of heterogeneous Web servers not only increases the complexity of the DNS scheduling problem, but also makes previously proposed algorithms for homogeneous distributed systems not directly applicable. This leads us to propose new policies, cabled adaptive TTL algorithms, that take into account both the uneven distribution of client request rates and heterogeneity of Web servers to adaptively set the time-to-live (TTL) value for each address mapping request. Extensive simulation results show that these strategies are robust and effective in balancing load among geographically distributed heterogeneous Web servers.
[Scalability, World Wide Web, homogeneous distributed systems, client request rates, processor scheduling, Centralized control, Uniform resource locators, resource allocation, heterogeneous Web servers, Web server, client-server systems, multiple homogeneous servers, load balancing algorithms, domain name server based schedulers, Dynamic scheduling, Routing, Parallel architectures, Scheduling algorithm, address mapping request, time-to-live value, DNS scheduling problem, multiple Web servers, Load management, geographically distributed heterogeneous Web servers, distributed multi server Web site, adaptive TTL algorithms, Internet, client demands, Web traffic, dynamic load balancing]
Using broadcast primitives in replicated databases
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
We explore the use of different variants of broadcast protocols for managing replicated databases. Starting with the simplest broadcast primitive, the reliable broadcast protocol, we show how it can be used to ensure correct transaction execution. The protocol is simple, and has several advantages, including prevention of deadlocks. However, it requires a two-phase commitment protocol for ensuring correctness. We then develop a second protocol that uses causal broadcast and avoids the overhead of two-phase commit by exploiting the causal delivery properties of the broadcast primitives to implicitly collect the relevant information used in two-phase commit. Finally, we present a protocol that employs atomic broadcast and completely eliminates the need for acknowledgements during transaction commitment.
[transaction processing, deadlock prevention, Protocols, replicated databases, software reliability, memory protocols, causal broadcast, transaction execution, Transaction databases, Electronic mail, atomic broadcast, Computer science, causal delivery properties, reliable broadcast protocol, IEEE services, USA Councils, broadcast primitives, concurrency control, Broadcasting, System recovery, Permission, Advertising, two-phase commitment protocol, transaction commitment]
Globally distributed computation over the Internet-the POPCORN project
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
The POPCORN project provides an infrastructure for globally distributed computation over the whole Internet. It provides any programmer connected to the Internet with a single huge virtual parallel computer composed of all processors on the Internet which care to participate at any given moment. The system provides a market-based mechanism of trade in CPU time to motivate processors to provide their CPU cycles for other peoples' computations. Selling CPU time is as easy as visiting a certain Web site with a Java-enabled browser. Buying CPU time is done by writing a parallel program, using our programming paradigm (and libraries). This paradigm was designed to fit the situation of global computation. A third entity in our system is a market for CPU time, which is where buyers and sellers meet and trade. The system has been implemented and may be visited and used on our Web site: http://www.cs.huji.ac.il/-popcorn.
[CPU time, market-based mechanism, globally distributed computation, POPCORN project, Distributed computing, parallel machines, parallel programming, software libraries, Java-enabled browser, virtual machines, virtual parallel computer, object-oriented languages, Internet, Web site]
Distributed virtual malls on the World Wide Web
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
Virtual malls allow consumers to shop and purchase products on the World Wide Web from multiple stores. The paper presents a virtual mall in which stores may be distributed across multiple Web sites. Stores participate in the virtual mall by communicating with a mall coordinator. The virtual mall allows shoppers to perform actions across multiple stores simultaneously such as viewing product availability. Multiple purchases across different stores can be coordinated using multi-phase commits. The mall coordinator can authenticate clients on all stores participating in the virtual mall while only requiring clients to provide authentication information once. State information is preserved using dynamic argument embedding which is compatible with all browsers and servers supporting HTTP and is less obtrusive than cookies. The distributed virtual mall concept and infrastructure can be applied to other distributed electronic commerce applications on the Web.
[multi-phase commits, dynamic argument embedding, World Wide Web, product purchasing, Electronic commerce, multiple stores, Uniform resource locators, distributed virtual malls, multiple purchases, product availability viewing, client authentication information, servers, Search engines, state information, Web server, retail data processing, shopping, Availability, home shopping, HTTP, Application software, browsers, Authentication, Web pages, message authentication, distributed electronic commerce applications, mall coordinator, Internet, Web sites, business data processing]
Timestamps for programs using messages and shared variables
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
Algorithms for vector timestamps have been developed to determine the "happened before" relations between events of an execution of a message passing program. Many message passing programs contain variables shared by multiple processes (including threads). Such programs need to have vector timestamps for send, receive, read and write events. We define two "happened-before" relations, called strong happened-before (SHB) and weak happened-before (WHB), between events of an execution involving send, receive, read and write statements. We then present two timestamp assignment algorithms, one for SHB and the other for WHB, and show how to use such timestamps to determine the SHB or WHB relation between any two events of an execution involving send, receive, read and write statements. For a program containing n processes, the size of a vector timestamp for SHB or WHB is n, regardless of the number of shared variables in the program. Finally, we show how to apply WHB timestamps to perform race analysis for programs using messages and shared variables.
[message passing, strong happened-before, happened-before relations, Debugging, shared variables, temporal logic, write events, WHB relation, Electronic switching systems, vector timestamps, message passing program, Yarn, Hip, Read only memory, parallel programming, happened before relations, Message passing, multiple processes, WHB timestamps, SHB relation, race analysis, timestamp assignment algorithms, weak happened-before, Testing]
Improving data freshness in lazy master schemes
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
Many distributed database applications need to replicate data to improve data availability and query response time. The two phase commit protocol guarantees mutual consistency of replicated data but does not provide good performance. Lazy replication has been used as an alternative solution in several types of applications such as online financial transactions and telecommunication systems. In this case, mutual consistency is relaxed and the concept of freshness is used to measure the deviation between replica copies. We propose two update propagation strategies that improve freshness. Both of them use immediate propagation: updates to a primary copy are propagated towards a slave node as soon as they are detected at the master node without waiting for the commitment of the update transaction. We study the behavior of our strategies and show that they improve data freshness with respect to the deferred approach.
[Real time systems, transaction processing, Protocols, online financial transactions, lazy master schemes, Master-slave, lazy replication, Delay, query processing, Distributed databases, data availability, software performance evaluation, two phase commit protocol, Availability, distributed database, replica copies, replicated databases, update propagation strategies, Interconnected systems, Data warehouses, memory protocols, data freshness, data integrity, Transaction databases, Exchange rates, replicated database, performance, query response time, telecommunication systems, replicated data consistency]
Self-organizing cooperative WWW caching
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
A distributed, self-organizing cooperative caching mechanism for WWW is proposed. The servers maintain a cooperative cache without a centralized server, adapt themselves to the change of network speed and server addition and failure, and need little manual administration. A performance study shows that the proposed mechanism improves the user's response time by 34.0% over conventional proxies.
[self-organizing cooperative caching, client-server systems, Costs, Cooperative caching, Delay effects, Laboratories, system failure, World Wide Web, cache storage, network speed, Information systems, self-organising storage, distributed cache, Network servers, performance study, World Wide Web caching, response time, Computer architecture, distributed memory systems, client server systems, Internet, Web server, software performance evaluation]
System support for partition-aware network applications
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
Network applications and services need to be environment-aware in order to meet non-functional requirements in increasingly dynamic contexts. We consider partition awareness as an instance of environment awareness in network applications that need to be reliable and self-managing. Partition-aware applications dynamically reconfigure themselves and adjust the quality of their services in response to partitioning and merging of networks. As such, they can automatically adapt to changes in the environment so as to remain available in multiple partitions without blocking, albeit with reduced or degraded functionality. We propose a system layer consisting of group membership and reliable multicast services that provides systematic support for partition-aware application development. We illustrate the effectiveness of the proposed interface by solving several problems that represent different classes of realistic network applications.
[Context-aware services, service quality, merging, Merging, software reliability, computer networks, Context awareness, distributed processing, reliable multicast services, reliable applications, group membership, Application software, system support, Hip, application development, Degradation, Computer science, partition-aware network applications, nonfunctional requirements, environment-aware applications, self-managing applications, Collaborative work, Telecommunication network reliability, Communication networks]
A protocol for removing communication intrusion in monitored distributed systems
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
The development of a distributed application that exhibits both desired functionality as well as performance is a complex task. Therefore, the construction of monitoring tools to assist in the development of complex distributed applications is of great practical significance. For example, monitoring tools can be used to observe the behavior and fine tune the performance. One of the fundamental problems that must be addressed is to ensure that the monitoring tool is able to report the true behavior of an application, that is, it is able to monitor the application non-intrusively. We identify the intrusive effects of monitoring on communication in an arbitrary point-to-point network and present the enhanced communication protocol for on-line communication intrusion removal. Our experiments demonstrate that the intrinsic overhead of a distributed system integrated with online intrusion removal techniques is 0.43%, 98.3% of intrusion on the accumulated number of events and 97.2% of intrusion on the sequence of event occurrences can be removed successfully.
[monitored distributed systems, Protocols, message passing, Instruments, experiments, distributed processing, Educational institutions, Application software, distributed application, Scheduling algorithm, monitoring tools, Runtime, protocol, Processor scheduling, performance, arbitrary point-to-point network, online communication intrusion removal, Load management, system monitoring, communication intrusion, Hardware, protocols, Monitoring, software performance evaluation]
Self-stabilization with global rooted synchronizers
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
We propose a self-stabilizing synchronization technique, called the global rooted synchronization, that synchronizes processors in a tree network. This synchronizer converts a synchronous protocol for tree networks into a self-stabilizing version. The synchronizer requires only O(1) memory (other than the memory needed to maintain the tree) at each node regardless of the size of the network, stabilizes in O(h) time, where h is the height of the tree, and does not invoice any global operations. Applications of this technique are presented.
[Algorithm design and analysis, Protocols, synchronous protocol, memory, trees (mathematics), global rooted synchronization, software fault tolerance, Counting circuits, synchronisation, processor synchronization, Computer science, global operations, Upper bound, self-stabilizing synchronization, distributed algorithms, Broadcasting, Robustness, tree network, Distributed algorithms]
Decentralized micropayment consolidation
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
We propose a novel protocol for aggregating micropayments in a networked environment. The protocol is based on the idea of debt consolidation and is fully decentralized. We propose client server and serverless versions of the protocol. We also analyze the mathematical properties of the protocol. Finally, we show how basic cryptographic techniques can be used to support the operation of the protocol in an untrusted environment.
[client-server systems, Costs, debt consolidation, computer networks, untrusted environment, decentralized micropayment consolidation, World Wide Web, Routing, cryptography, mathematical properties, Electronic mail, cryptographic techniques, Application software, Electronic commerce, networked environment, Cryptographic protocols, Computer science, serverless version, client server version, Computer networks, financial data processing, Cryptography, protocols]
A statistical method for time synchronization of computer clocks with precisely frequency-synchronized oscillators
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
With the growing of computer networks, continuous media communication and processing are expected in distributed systems. Continuous media communication and processing require both time and frequency synchronization between computer clocks. The authors propose a statistical time synchronization method for computer clocks that have precisely frequency-synchronized oscillators. This method not only improves time synchronization accuracy but also prevents degradation of the frequency stability of the precise oscillators when errors in the measured time offset between computer clocks caused by network traffic possess a Gaussian distribution. The improvement of time synchronization accuracy is achieved by estimating the confidence interval of the measured time offsets between the computer clocks. The prevention of frequency stability degradation is achieved by testing the difference between the latest mean and the mean at the last time adjustment for deciding the need for time correction. Evaluation by simulation of changes in the time offset in an ISDN clock synchronization system showed that this method achieves accurate time and stable frequency synchronization.
[confidence interval estimation, measured time offset errors, ISDN clock synchronization system, simulation, Gaussian distribution, Frequency measurement, frequency stability degradation prevention, Degradation, statistical method, ISDN, time synchronization accuracy, distributed systems, Computer networks, time adjustment difference testing, continuous media communication, oscillators, frequency stability, Statistical analysis, Stability, time synchronization, precisely frequency-synchronized oscillators, computer networks, Time measurement, continuous media processing, time correction, Oscillators, synchronisation, clocks, network traffic, Computer errors, statistical analysis, computer clocks, Frequency synchronization, stable frequency synchronization, errors, telecommunication traffic, Clocks]
Detectors and correctors: a theory of fault-tolerance components
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
Two primitive components, namely detectors and correctors, provide a basis for achieving the different types of fault tolerance properties required in computing systems. We develop the theory of these primitive tolerance components, characterizing precisely their role in achieving the different types of fault tolerance. Also, we illustrate how they can be used to formulate extant design methods and argue that they sometimes offer the potential for better designs than those obtained from extant methods.
[Design methodology, program diagnostics, correctors, distributed processing, detectors, software fault tolerance, Fault tolerance, Information science, fault tolerance components, Fault detection, Voting, Fault tolerant systems, composition, fault environment, Detectors, extant design methods, Error correction codes, Safety, Testing, primitive tolerance components]
A bandwidth-sensitive update scheduling method for Internet push
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
We present the BUS (Bandwidth-sensitive Update Scheduling) method for WWW push proxies that actively sets different update schedules for various WWW push channels. Based on open protocols describing push objects, BUS optimizes the currency of the objects it delivers to the clients under the constraint of the available bandwidth. As a result, push traffic overflow is minimized with adequate gateway bandwidth reserved for critical non-push traffic. The BUS method also provides a mechanism to monitor client interest and conducts a dynamic proxy update for given channels if the client requests for these channels increase suddenly. In contrast to fixed schedule approaches, this dynamic update mechanism can better capture the dynamic changes (such as the financial activities) that are of interest to many clients.
[dynamic proxy update, Content based retrieval, dynamic update mechanism, Liver, client interest, open protocols, gateway bandwidth, World Wide Web, Electrical capacitance tomography, push objects, client requests, financial activities, Bandwidth, BUS method, scheduling, push traffic overflow, protocols, Web server, bandwidth-sensitive update scheduling method, client-server systems, update schedules, WWW push proxies, Rivers, Application software, Milling machines, dynamic changes, WWW push channels, fixed schedule approaches, critical non-push traffic, Internet, Internet push]
A framework for consistent, replicated Web objects
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
Despite the extensive use of caching techniques, the Web is overloaded. While the caching techniques currently used help some, it would be better to use different caching and replication strategies for different Web pages, depending on their characteristics. We propose a framework in which such strategies can be devised independently per Web document. A Web document is constructed as a worldwide, scalable distributed Web object. Depending on the coherence requirements for that document, the most appropriate caching or replication strategy can subsequently be implemented and encapsulated by the Web object. Coherence requirements are formulated from two different perspectives: that of the Web object, and that of clients using the Web object. We have developed a prototype in Java to demonstrate the feasibility of implementing different strategies for different Web objects.
[document handling, Java, client-server systems, Protocols, object-oriented programming, Scalability, coherence requirements, Web document, replication strategy, cache storage, Electrical capacitance tomography, replication strategies, Web pages, Prototypes, Bandwidth, object-oriented languages, caching techniques, Internet, worldwide scalable distributed Web object, Web server, consistent replicated Web objects]
Improved lightpath (wavelength) routing in large WDM networks
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
We address the problem of efficient circuit switching in wide area networks. The solution provided is based on finding optimal routes for lightpaths and semilightpaths. A lightpath is a fully optical transmission path, while a semilightpath is a transmission path constructed by chaining several lightpaths together, using wavelength conversion at their junctions. The problem thus is to find an optimal lightpath/semilightpath in the network in terms of the cost of wavelength conversion and the cost of using the wavelengths on links. We first present fast, efficient algorithms both for the general problem and for a natural restricted version. The new algorithms outperform earlier work, providing time improvements amounting to an almost linear time factor in most cases. Also, all our algorithms can be implemented on the network in a distributed way.
[wavelength division multiplexing, Biomedical optical imaging, Costs, wide area networks, graph theory, optical networks, WDM networks, Optical fiber networks, Mathematics, circuit switching, Wavelength routing, lightpath routing, wavelength conversion, Intelligent networks, Bandwidth, optical communication, wavelength routing, Optical wavelength conversion, switching theory, optical transmission path, distributed algorithms, telecommunication network routing, optimal routes, semilightpaths, High speed optical techniques]
An adaptive protocol for implementing causally consistent distributed services
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
Distributed services that are accessed by widely distributed clients are becoming common place. Such services cannot be provided at the needed level of performance and availability without replicating the service at multiple nodes, and without allowing a relatively weak level of consistency among replicated copies of the state of a service. This paper explores causally consistent distributed services when multiple related services are replicated to meet performance and availability requirements. This consistency criterion is particularly well suited for some distributed services (e.g., cooperative document sharing), and it is attractive because of the efficient implementations allowed by it.
[Availability, consistency criterion, client-server systems, Costs, multiple nodes, Access protocols, widely distributed clients, distributed processing, Educational institutions, multiple related services, Electrical capacitance tomography, availability, Distributed computing, Delay, causally consistent distributed services, performance, Ores, adaptive protocol, Pattern matching, Contracts, software performance evaluation, replicated service]
A scalable scheduling algorithm for real-time distributed systems
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
Scheduling real time tasks in a distributed memory multiprocessor is characterized as sequencing a set of tasks and assigning them to processors of the architecture. Real time systems research has extensively investigated the sequencing dimension of the scheduling problem by extending uniprocessor scheduling techniques to more complex architectures. We introduce a technique that uses an assignment oriented representation to dynamically schedule real time tasks on the processors of the system. The technique we propose, automatically controls and allocates the scheduling time, in order to minimize deadline violation of real time tasks, due to the scheduling overhead. We evaluate this technique in the context of scheduling real time transactions in a distributed database application which we implemented on an Intel Paragon distributed memory multiprocessor. In this implementation, we compared the performance of our algorithm with another dynamic algorithm that uses a sequence oriented representation. The results show interesting performance trade-offs among the candidate algorithms and validate our conjectures about scalability performance limitations of sequence oriented representations. The results also show the effect of the mechanisms that our technique uses to control and allocate scheduling time.
[Real time systems, transaction processing, Heuristic algorithms, deadline violation, sequencing dimension, scheduling problem, assignment oriented representation, resource allocation, sequence oriented representation, distributed memory multiprocessor, distributed databases, scheduling, Intel Paragon distributed memory multiprocessor, dynamic algorithm, real time distributed systems, real time tasks, scalability performance limitations, real time systems research, uniprocessor scheduling techniques, scheduling time allocation, complex architectures, scheduling overhead, Scheduling algorithm, real time transactions, distributed database application, sequence oriented representations, scalable scheduling algorithm, Processor scheduling, real-time systems, distributed memory systems, Problem-solving, Time factors]
A delay-optimal quorum-based mutual exclusion scheme with fault-tolerance capability
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
The performance of a mutual exclusion algorithm is measured by the number of messages exchanged per critical section execution and the delay between successive executions of the critical section. There is a message complexity and synchronization delay trade-off in mutual exclusion algorithms. Lamport's (1978) algorithm and Ricart and Agrawal's (1981) algorithm both have a synchronization delay of T, but their message complexity is O(N). Maekawa's (1985) algorithm reduces message complexity to O(/spl radic/N); however, it increases the synchronization delay to 2T. After Maekawa's algorithm, many quorum-based mutual exclusion algorithms have been proposed to reduce message complexity or increase the resiliency to site and communication link failures. Since these algorithms are Maekawa-type algorithms, they also suffer from long synchronization delay 2T. We propose a delay-optimal quorum-based mutual exclusion algorithm which reduces the synchronization delay to T and still has the low message complexity O(K) (K is the size of the quorum, which can be as low as log N). A correctness proof and detailed performance analysis are provided.
[Costs, message exchange, Merging, communication complexity, communication link failures, Fault tolerance, Information science, message complexity, resource allocation, Fault tolerant systems, Performance analysis, delay optimal scheme, fault tolerance, Delay effects, quorum based mutual exclusion scheme, Synchronization, Sun, correctness proof, synchronisation, Computer science, distributed algorithms, algorithm performance, concurrency control, delays, fault tolerant computing, synchronization delay]
Trust metrics, models and protocols for electronic commerce transactions
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
The paper introduces the notion of quantifiable trust for electronic commerce. It describes metrics and models for the measurement of trust variables and fuzzy verification of transactions. Trust metrics help preserve system availability by determining risk on transactions. Furthermore, when several entities are involved in electronic transactions, previously know techniques are applied for trust propagation. Malicious transacting entities may try to illegitimately gain access to private trust information. Suitable protocols are developed to minimize breach of privacy and incorporate a non repudiable context using cryptographic techniques.
[transaction processing, Protocols, fuzzy set theory, malicious transacting entities, business communication, private trust information, cryptographic techniques, Electronic commerce, commerce, Postal services, Privacy, formal verification, electronic commerce transactions, Cryptography, protocols, non repudiable context, Business, fuzzy verification, Identity-based encryption, trust metrics, trust variables, trust propagation, quantifiable trust, electronic transactions, security of data, Sockets, Authentication, system availability, data privacy, Internet]
Lightweight transactions on networks of workstations
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
Although transactions have been a valuable abstraction of atomicity, persistency, and recoverability, they have not been widely used in programming environments today, mostly because of their high overheads that have been driven by the low performance of magnetic disks. A major challenge in transaction-based systems is to remove the magnetic disk from the critical path of transaction management. We present PERSEAS, a transaction library for main memory databases that decouples the performance of transactions from the magnetic disk speed. Our system is based on a layer of reliable main memory that provides fast and recoverable storage of data. We have implemented our system as a user-level library on top of the Windows NT operating system in a network of workstations connected with the SCI interconnection network. Our experimental results suggest that PERSEAS achieves performance that is orders of magnitude better than traditional recoverable main memory systems.
[transaction processing, Windows NT, Multiprocessor interconnection networks, lightweight transactions, main memory databases, local area networks, network operating systems, persistency, distributed databases, Libraries, Hardware, magnetic disks, PERSEAS, Workstations, Mirrors, SCI interconnection network, software performance evaluation, networks of workstations, magnetic disc storage, atomicity, recoverability, operating system, transaction management, Computer crashes, Transaction databases, transaction library, Computer science, performance, Power system reliability, user-level library, Personal communication networks, programming environments]
Low-overhead protocols for fault-tolerant file sharing
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
We quantify the adverse effect of file sharing on the performance of reliable distributed applications. We demonstrate that file sharing incurs significant overhead, which is likely to triple over the next five years. We present a novel approach that eliminates this overhead. Our approach: tracks causal dependencies resulting from file sharing using determinants; efficiently replicates the determinants in the volatile memory of agents to ensure their availability during recovery; and reproduces during recovery the interactions with the file server as well as the file data lost in a failure. Our approach allows agents to exchange files directly without first saving the files on disks at the server. As a consequence, the costs of supporting file sharing and message passing in a reliable distributed application become virtually identical. The result is a simple, uniform approach, which can provide low-overhead fault tolerance to applications in which communication is performed through message passing, file sharing, or a combination of the two.
[Checkpointing, Costs, message passing, low overhead protocols, reliable distributed applications, Peer to peer computing, software reliability, Access protocols, File servers, recovery, system recovery, causal dependencies, Read only memory, Delay, software fault tolerance, application performance, Fault tolerance, Message passing, file servers, file server, fault-tolerant file sharing, protocols, software performance evaluation]
Flexible exception handling in the OPERA process support system
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
Exceptions are one of the most pervasive problems in process support systems. In installations expected to handle a large number of processes, having exceptions is bound to be a normal occurrence. Any programming tool intended for large, complex applications has to face this problem. However, current process support systems, despite their orientation towards complex, distributed, and heterogeneous applications, provide almost no support for exception handling. This paper shows how flexible mechanisms for failure handling are incorporated into the OPERA process support system using a combination of programming language concepts and transaction processing techniques. The resulting mechanisms allow the construction of fault-tolerant workflow processes in a transparent and flexible way while ensuring reusability of workflow components.
[transaction processing, Humans, flexible exception handling, exception handling, distributed processing, Personnel, Environmental management, OPERA, distributed applications, Information systems, Databases, failure handling, Fault tolerant systems, programming language, workflow component reuse, software tools, Logic programming, programming tool, Logic design, software fault tolerance, fault-tolerant workflow processes, Programming environments, large complex applications, Computer languages, process support system, heterogeneous applications]
Low-cost checkpointing with mutable checkpoints in mobile computing systems
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
Mobile computing raises many new issues, such as lack of stable storage, low bandwidth of wireless channel, high mobility, and limited battery life. These new issues make traditional checkpointing algorithms unsuitable. We introduce the concept of mutable checkpoint, which is neither a tentative checkpoint nor a permanent checkpoint. Mutable checkpoints can be saved anywhere; e.g., the memory or local disk of MHs. In this way, taking a mutable checkpoint avoids the overhead of transferring a large amount of data to the stable storage in MSS over the wireless network. Based on mutable checkpoints, our non-blocking algorithm avoids the avalanche effect, minimizes the number of synchronization messages and forces only a minimum number of processes to take their checkpoints on the stable storage.
[Checkpointing, Algorithm design and analysis, high mobility, message exchange, Batteries, Distributed computing, system recovery, nonblocking algorithm, synchronization messages, Information science, Wireless networks, Bandwidth, Computer networks, avalanche effect, mutable checkpoints, wireless network, message passing, low-cost checkpointing, Routing, limited battery life, mobile computing systems, synchronisation, fault tolerant computing, low bandwidth, stable storage, wireless LAN, Mobile computing]
A language for specifying the composition of reliable distributed applications
Proceedings. 18th International Conference on Distributed Computing Systems
None
1998
This paper describes the design of a scripting language aimed at expressing task (unit of computation) composition and inter-task dependencies of distributed applications whose execution could span arbitrary large durations. This work is motivated by the observation that an increasingly large number of distributed applications are constructed by composing them out of existing applications and are executed in an heterogeneous environment. The resulting applications can be very complex in structure, containing many notification and dataflow dependencies between their constituent applications. The language enables applications to be structured with the properties of modularity, interoperability, dynamic reconfigurability and fault-tolerance.
[reliable distributed applications, open systems, scripting language, Buildings, software reliability, distributed processing, interoperability, Distributed computing, formal specification, dynamic reconfigurability, software fault tolerance, Fault tolerance, Microwave integrated circuits, authoring languages, modularity, specification languages, inter-task dependencies, heterogeneous environment, dataflow dependencies, task composition, specification language]
HiFi: a new monitoring architecture for distributed systems management
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
With the increasing complexity of large scale distributed (LSD) systems, an efficient monitoring mechanism has become an essential service for improving the performance and reliability of such complex applications. The paper presents a scalable, dynamic, flexible and nonintrusive monitoring architecture for managing large scale distributed (LSD) systems. This architecture, which is referred to as the HiFi monitoring system, detects and classifies interesting primitive and composite events and performs either a corrective or steering action. When appropriate, information is also disseminated to management applications, such as reactive control tools. The outlined solution offers improvements over related works by supporting new monitoring techniques such as hierarchical filtering based monitoring and filter incarnation that improve the monitoring scalability and dynamism which are required for managing large scale distributed systems. The HiFi monitoring system has been implemented and used at the Old Dominion University for monitoring and steering Interactive Remote Instruction (IRI) which is a large scale distributed multimedia system for distance learning.
[Event detection, steering action, distributed processing, composite events, Filters, filter incarnation, monitoring techniques, Computer architecture, Aging, interactive systems, nonintrusive monitoring architecture, Large-scale systems, large scale distributed systems management, HiFi monitoring system, Interactive Remote Instruction, hierarchical filtering based monitoring, Filtering, Computerized monitoring, reactive control tools, large scale distributed multimedia system, monitoring scalability, monitoring mechanism, distance learning, Computer science, management applications, Collaboration, monitoring architecture, system monitoring, computer aided instruction, Remote monitoring]
A dynamic object replication and migration protocol for an Internet hosting service
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
The paper proposes a protocol suite for dynamic replication and migration of Internet objects. It consists of an algorithm for deciding on the number and location of object replicas and an algorithm for distributing requests among currently available replicas. Our approach attempts to place replicas in the vicinity of a majority of requests, while ensuring at the same time that no servers are overloaded. The request distribution algorithm uses the same simple mechanism to take into account both server proximity and load, without actually knowing the latter. The replica placement algorithm executes autonomously on each node, without the knowledge of other object replicas in the system. The proposed algorithms rely on the information available in databases maintained by Internet routers. A simulation study using synthetic workloads and the network backbone of UUNET, one of the largest Internet service providers, shows that the proposed protocol is effective in eliminating hot spots and achieves a significant reduction in backbone traffic and server response time at the expense of creating only a small number of extra replicas.
[Protocols, Scalability, server proximity, Spine, synthetic workloads, Internet service provider, object migration protocol, UUNET, Electrical capacitance tomography, backbone traffic, server response time, Delay, replica placement algorithm, simulation study, Microwave integrated circuits, Space technology, Web and internet services, request distribution algorithm, protocols, Web server, distributed object management, client-server systems, Internet routers, Internet hosting service, network backbone, dynamic object replication, Computer science, protocol suite, Internet objects, Internet]
Fault tolerant video on demand services
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
This paper describes a highly available distributed video on demand (VoD) service which is inherently fault tolerant. The VoD service is provided by multiple servers that reside at different sites. New servers may be brought up "on the fly" to alleviate the load on other servers. When a server crashes it is replaced by another server in a transparent way; the clients are unaware of the change of service provider. In test runs of our VoD service prototype, such transitions are not noticeable to a human observer who uses the service. Our VoD service uses a sophisticated flow control mechanism and supports adjustment of the video quality to client capabilities. It does not assume any proprietary network technology: it uses commodity hardware and publicly available network technologies (e.g., TCP/IP, ATM). Our service may run on any machine connected to the Internet. The service exploits a group communication system as a building block for high availability. The utilization of group communication greatly simplifies the service design.
[Video on demand, Humans, Communication system control, video servers, Fault tolerance, resource allocation, Web and internet services, video on demand, flow control mechanism, Prototypes, distributed VoD, TCPIP, commodity hardware, Hardware, client-server systems, multiple servers, client server system, Vehicle crash testing, video quality, Computer crashes, server crash, group communication, fault tolerant video on demand, fault tolerant computing, Internet]
Run-time detection in parallel and distributed systems: application to safety-critical systems
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
There is growing interest in run-time detection as parallel and distributed systems grow larger and more complex. This work targets run-time analysis of complex, interactive scientific applications for purposes of attaining scalability improvements with respect to the amount and complexity of the data transmitted, transformed, and shared among different application components. Such improvements are derived from using database techniques to manipulate data streams. Namely, by imposing a relational model on the data streams, constraints on the stream may be expressed as database queries evaluated against the data events comprising the stream. The application in the paper is to a safety-critical system. The paper also presents a tool, dQUOB, Dynamic QUery OBjects, which: (1) offers the means for dynamic creation of queries and for their application to large data streams; (2) permits implementation and runtime use of multiple "query optimization" techniques; and (3) supports dynamic reoptimization of queries based on streams' dynamic behavior.
[data events, run time detection, multiple query optimization techniques, dynamic behavior, data streams, safety-critical software, distributed processing, Read only memory, dynamic reoptimization, Information analysis, query processing, database techniques, Runtime, System performance, interactive systems, distributed systems, runtime use, Performance analysis, Safety, natural sciences computing, interactive scientific applications, database queries, Dynamic QUery OBjects, dynamic creation, Educational institutions, Time measurement, Hazards, run-time analysis, relational databases, Phase detection, parallel systems, safety-critical systems, scalability improvements, relational model, dQUOB]
Replicated directory service for weakly consistent distributed caches
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
Relais is a replicated directory service that improves Web caching within an organization. Relais connects a distributed set of caches and mirrors, providing the abstraction of a single consistent, shared cache. Relais is based on a replication protocol that exploits the semantics of user requests to guarantee cache coherence. The protocol also exploits the semantics of origin servers to optimize bandwidth requirements. At the entry point for a client, Relais maintains a directory of the contents of each Relais data provider; this minimizes look-up latency, by making it a local operation. The cost of maintaining the directories consistent is minimized thanks to a weak consistency protocol. Relais has been prototyped on top of Squid; it has been in daily use for over a year.
[Costs, cache coherence, weakly consistent distributed caches, local operation, File servers, cache storage, Personnel, Delay, Network servers, Asynchronous communication, Squid, Prototypes, groupware, replicated directory service, minimized look-up latency, single consistent shared cache, Mirrors, IP networks, protocols, Web server, Relais, client-server systems, user request semantics, replicated databases, mirrors, replication protocol, abstraction, information networks, Web caching, origin server semantics, weak consistency protocol, optimized bandwidth requirements, client entry point]
Adapting distributed applications using extensible networks
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
Active networks have been proposed to allow the dynamic extension of network behavior by downloading application-specific protocols (ASPs) into network routers. We demonstrate the feasibility of the use of ASPs in an active network for the adaptation of distributed software components. We have implemented three examples which show that ASPs can be used to easily extend distributed applications, and furthermore, that such adaptation can be safe, portable and efficient. Safety and efficiency is obtained by implementing the ASPs in PLAN-P, a domain-specific language and run-time system for active networking. The presented examples illustrate three different applications: audio broadcasting with bandwidth adaptation in routers; an extensible HTTP server with load-balancing facilities; and a multipoint MPEG server derived from a point-to-point server.
[active networks, Protocols, load balancing, distributed software components, high level languages, distributed processing, application-specific protocols, distributed applications, Postal services, multipoint MPEG server, resource allocation, bandwidth adaptation, Computer architecture, Bandwidth, run-time system, audio broadcasting, Hardware, Safety, DSL, protocols, HTTP server, Web server, extensible networks, network routers, Application specific processors, Application software, domain-specific language, telecommunication network routing, point-to-point server, PLAN-P]
ETE: a customizable approach to measuring end-to-end response times and their components in distributed systems
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
Detecting and resolving performance problems in distributed systems often requires measurements of end-to-end ("finger tip to eyeball") response times. Existing approaches embed transaction definitions in instrumentation codes. As a result, service providers (e.g., ISPs) cannot tailor transaction definitions to the usage patterns of their customers. We propose a new approach-ETE (end-to-end)-in which transaction definitions are externalized so that they can be customized. This is accomplished by having instrumentation generate events (not transactions) and employing a separate component-the transaction generator-that uses external definitions of transactions to construct response time measurements from event streams. ETE provides measurements of both end-to-end response times and their components. The latter reflect delays for services within distributed systems (e.g., name resolution service). We have used ETE to measure response times for Web transactions, terminal emulators, and Lotus Notes.
[transaction processing, distributed processing, customizable approach, Distributed computing, Delay, transaction generator, Degradation, response time measurements, Web and internet services, distributed systems, transaction definitions, usage patterns, external definitions, Instruments, Web transactions, performance evaluation, performance problems, Time measurement, Milling machines, ISPs, end-to-end response time measurement, instrumentation codes, event streams, terminal emulators, Web pages, time measurement, service providers, ETE, name resolution service, Explosives, Time factors, Lotus Notes]
Imprecise calendars: an approach to scheduling computational grids
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
We describe imprecise calendars, a way to organize and schedule clusters of nodes in a computation grid. Imprecise calendars permit the easy and efficient sharing of resources between different clusters of computers that are part of a computational grid. In addition, they can be used to provide specific time reservations for applications. We describe the algorithms and policies for manipulation of imprecise calendars. We also include a series of simulation studies that compare our approach to previous batch scheduling systems for both a single cluster and collection of clusters up to over 3000 nodes.
[workstation clusters, Identity-based encryption, imprecise calendars, Calendars, Parallel machines, Educational institutions, Supercomputers, Application software, processor scheduling, computational grids, Computer science, Processor scheduling, resource allocation, batch scheduling systems, Computer applications, Grid computing, clusters]
Active correlation tracking
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
We describe methods of identifying and exploiting sharing patterns in multi-threaded DSM applications. Active correlation tracking is used to determine the affinity, or amount of sharing, in pairs of threads. Thread affinities are combined to create correlation maps, which summarize sharing between all pairs of threads in the application. Correlation maps can be used in two ways. First, they can be used as an aid for performance tuning, Second, they can be used to estimate the impact on communication requirements of reconfiguring running applications through thread migration. Thread migration provides a way of tuning applications for which sharing information is not known a priori, and a means of adapting to dynamic algorithms or environments. We show that i) accurate thread affinities can be obtained without multiple rounds of migration, ii) thread affinities lead to good approximations of application communication requirements, iii) simple heuristics can use thread affinities to efficiently approximate optimal mappings of threads to nodes, and iv) good placement is essential for high performance.
[Identity-based encryption, thread migration, multi-threading, performance tuning, optimal mappings, application communication requirements, Educational institutions, Application software, Yarn, communication requirements, Computer science, simple heuristics, active correlation tracking, dynamic algorithms, distributed shared memory systems, Density functional theory, Software systems, Workstations, thread affinities, correlation maps]
Stateful group communication services
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
Reliable group multicasts provide a nice abstraction for communicating data reliably among group members and have been used for a variety of applications. In this paper we present Corona, a group communication service for building collaboration tools and reliable data dissemination services in Web-based environments, where clients connect independently of other clients and are not necessarily connected to the group multicast services all the time. The key features of Corona are: (1) the shared state of a group consists of a set of objects shared collectively among group members; (2) Corona supports multiple state transfer policies to accommodate clients with different needs and resources; (3) the communication service provides the current group state or state updates to new clients even when other clients are not available; (4) the service supports persistent groups that tolerate client failures and leaves. We show that the overhead incurred by the multicast service in managing each group's shared state has little impact on the latency seen by the clients or the server throughput. We also show that the multicast service does not have to be aware of the client-specific semantics of the objects in the group's state.
[client-specific semantics, current group state, multicast service, clients, latency, server throughput, Throughput, Electrical capacitance tomography, Delay, Web-based environments, collaboration tools, Fault tolerance, client failure tolerance, multiple state transfer policies, groupware, Broadcasting, group state updates, Corona, client leave tolerance, client-server systems, Collaborative tools, abstraction, stateful group communication services, software fault tolerance, reliable data dissemination services, persistent groups, reliable group multicasts, Intersymbol interference, Collaboration, Collaborative work, data communication, Internet]
Fast and fair mutual exclusion for shared memory systems
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
Two fast mutual exclusion algorithms using read-modify-write and atomic read/write registers are presented. The first one uses both compare&swap and fetch&store; the second uses only fetch&store. Fetch&store are more commonly available than compare&swap. It is impossible to obtain better algorithms if "time" is measured by counting remote memory references. We were able to maintain the same level of performance with or without the support of compare&swap. However, fairness is degraded from 1-bounded bypass to lockout freedom without the support.
[Algorithm design and analysis, Atomic measurements, fairness, Protocols, Spine, mutual exclusion algorithms, read-modify-write registers, Read-write memory, atomic read write registers, fetch and store, 1-bounded bypass, Registers, compare and swap, Multiprocessing systems, Computer science, Degradation, lockout freedom, resource allocation, performance, Councils, remote memory references, distributed algorithms, shared memory systems]
An Advanced Communication Toolkit for implementing the Broker pattern
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
The Broker pattern is a powerful solution when building middleware communication systems. Existing toolkits, such as BAST, GTS, and ACE, although useful, are insufficient to implement the Broker pattern architecture. These systems concentrate on wrappers for communication protocols, and on implementing auxiliary communication patterns, but address only some aspects of object communication. In this work we demonstrate how the Broker pattern can be easily implemented by using an Advanced Communication Toolkit (ACT). ACT model defines four layers according to the increasing degree of abstraction of exchanged information. The resulting systems are highly customizable, extensible, portable, and can communicate at any of the four layers independently. ACT supports various high-level communication protocols (e.g., HTTP, IIOP, SMTP) and can be used to implement Broker-based systems such as OMG CORBA, Java RMI, or Microsoft DCOM.
[high-level communication protocols, Java RMI, Electronic switching systems, Electrical capacitance tomography, OMG CORBA, Reactive power, ACT model, Computer architecture, wrappers, protocols, distributed object management, object communication, Java, client-server systems, communication protocols, Microsoft DCOM, Computer languages, software portability, middleware communication systems, Communication industry, Automata, software reusability, Software systems, Computer industry, remote procedure calls, Advanced Communication Toolkit, Broker pattern]
Search space reduction in QoS routing
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
To provide real time service, integrated networks require the underlying routing algorithm to be able to find low cost paths that satisfy given Quality of Service (QoS) constraints. The problem of constrained shortest (least cost) path routing is known to be NP hard, and some heuristics have been proposed to find a near optimal solution. However, these heuristics either impose relationships among the link metrics to reduce the complexity of the problem which may limit the general applicability of the heuristic, or are too costly in terms of execution time to be applicable to large networks. We focus on solving the delay constrained minimum cost path problem, and present a fast algorithm to find a near optimal solution. This algorithm, called DCCR (Delay-Cost-Constrained Routing), is a variant of the k-shortest path algorithm. DCCR uses a new adaptive path weight function, together with an additional constraint imposed on the path cost, to restrict the search space. Thus, DCCR can return a near optimal solution in a very short time. Furthermore, we use the method proposed by D. Blokh and G. Gutin (1995) to further reduce the search space by using a tighter bound on path cost. This makes our algorithm more accurate and even faster. We call this improved algorithm SSR+DCCR (Search Space Reduction+DCCR). Through extensive simulations, we confirm that SSR+DCCR performs very well compared to the optimal but very expensive solution.
[Costs, Quality of service, Distributed computing, search space reduction, delay constrained minimum cost path problem, Constraint optimization, Intelligent networks, NP hard, path cost, Polynomials, QoS routing, adaptive path weight function, search space, search problems, near optimal solution, Delay effects, real time service, computer networks, k-shortest path algorithm, Routing, Educational institutions, constrained shortest path routing, quality of service, low cost paths, Search Space Reduction+DCCR, routing algorithm, SSR+DCCR, real-time systems, Iterative algorithms, fast algorithm, minimisation, Delay-Cost-Constrained Routing, Quality of Service constraints, computational complexity]
PASS-a service for efficient large scale dissemination of time varying data using CORBA
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
A common class of wide-area distributed applications remotely collect time-varying data and send it to consumers around the network. Some examples of these include network management, stock ticker data and event logs. The environment in which these applications must operate often dictates the schemes for disseminating the data between the writers and the readers. If the transport channel can be optimized to match the application's behavior patterns and the network resource constraints, sufficient improvements in application-level quality of service (QoS) can be achieved. The PASS (Piecewise Asynchronous Sample Service) system addresses this problem by using a flexible system of interconnected servers. PASS servers are distributed geographically around the network and are connected to readers and writers using the CORBA protocol. The forwarding policies used by the servers and the server interconnections can be customized for each application. Thus, PASS acts like an application-level multicast service with variable forwarding policies. PASS has been used to disseminate the up/down status of a large number of devices to a network management system. The PASS forwarding policy used very little network bandwidth while responding to failures in half a network round-trip time.
[time-varying systems, network servers, device up/down status, wide area networks, application-level service quality, telecommunication services, event logs, Electronic switching systems, Electrical capacitance tomography, Personnel, time-varying data, application-level multicast service, transport channel optimization, failure response, Bandwidth, multicast communication, network resource constraints, application behavior patterns, stock ticker data, Large-scale systems, Piecewise Asynchronous Sample Service, customizable forwarding policies, Contracts, Monitoring, distributed object management, geographically distributed interconnected servers, information dissemination, network round-trip time, CORBA protocol, Educational institutions, wide-area distributed applications, quality of service, network bandwidth, Middleware, network management, computer network management, inter-computer links, PASS, remote method invocation, distributed objects, remote data collection, efficient large-scale data dissemination, Pattern matching]
Efficient collective communication in distributed heterogeneous systems
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
The Information Power Grid (IPG) is emerging as an infrastructure that will enable distributed applications-such as videoconferencing and distributed interactive simulation-to seamlessly integrate collections of heterogeneous workstations, multiprocessors, and mobile nodes over heterogeneous wide-area networks. This paper introduces a framework for developing efficient collective communication schedules in such systems. Our framework consists of analytical models of the heterogeneous system, scheduling algorithms for the collective communication pattern, and performance evaluation mechanisms. We show that previous models, which considered node heterogeneity but ignored network heterogeneity, can lead to solutions which are worse than the optimal by an unbounded factor. We then introduce an enhanced communication model, and develop three heuristic algorithms for the broadcast and multicast patterns. The completion time of the schedule is chosen as the performance metric. The heuristic algorithms are FEF (Fastest Edge First), ECEF (Earliest Completing Edge First), and ECEF with look-ahead. For small system sizes, we find the optimal solution using exhaustive search. Our simulation experiments indicate that the performance of our heuristic algorithms is close to optimal. For performance evaluation of larger systems, we have also developed a simple lower bound on the completion time. Our heuristic algorithms achieve significant performance improvements over previous approaches.
[Measurement, wide area networks, Heuristic algorithms, enhanced communication model, Mobile communication, Power grids, completion time, heterogeneous wide-area networks, analytical models, processor scheduling, performance metric, Analytical models, scheduling algorithms, distributed interactive simulation, distributed heterogeneous systems, Broadcasting, Workstations, videoconferencing, multiprocessors, mobile nodes, multiprocessing systems, Fastest Edge First, heuristic algorithms, Information Power Grid, performance evaluation, optimal solution, Scheduling algorithm, workstations, efficient collective communication schedules, Teleconferencing, broadcast patterns, Multicast algorithms, multicast patterns, virtual machines, Earliest Completing Edge First, performance evaluation mechanisms, exhaustive search]
An efficient multicast protocol for content-based publish-subscribe systems
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
The publish/subscribe (or pub/sub) paradigm is an increasingly popular model for interconnecting applications in a distributed environment. Many existing pub/sub systems are based on pre-defined subjects, and hence are able to exploit multicast technologies to provide scalability and availability. An emerging alternative to subject-based systems, known as content-based systems, allow information consumers to request events based on the content of published events. This model is considerably more flexible than subject-based pub/sub. However, it was previously not known how to efficiently multicast published events to interested content-based subscribers within a large and geographically distributed network of broker (or router) machines. We develop and evaluate a novel and efficient distributed algorithm for this purpose, called -link matching". Link matching performs just enough computation at each node to determine the subset of links to which an event should be forwarded. We show via simulations that: link matching yields higher throughput than flooding when subscriptions are selective; and the overall CPU utilization of link matching is comparable to that of centralized matching.
[Wide area networks, client-server systems, multicast protocol, content-based subscribers, Multicast protocols, distributed environment, Electrical capacitance tomography, Rivers, Table lookup, Floods, content-based publish-subscribe systems, scalability, Milling machines, Matched filters, distributed algorithm, Multicast algorithms, CPU utilization, distributed algorithms, Publish-subscribe, multicast communication, link matching, protocols]
Site selection for real-time client request handling
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
In a conventional client-server database system (CS-DBS), a transaction and its requisite data have to be colocated at a single site for the operation to proceed. This has traditionally been achieved by moving either the data or the transaction. Today, the availability of powerful workstations and high-bandwidth networking options has led users to expect real-time guarantees about the completion times of their tasks. So as to offer such guarantees in a CS-DBS, a transaction should be processed by any means that allows it to meet its deadline. In this paper we explore the option of moving both transactions and data to the most promising sites for successful completion. We propose a load-sharing mechanism that oversees the shipment of data and transactions in order to increase the efficiency of a client-server cluster. Additionally, we make use of the concept of grouped locks to schedule the movement of data objects in the cluster in a more efficient manner. An experimental evaluation shows that the use of our load-sharing algorithm provides a considerable improvement in the real-time processing efficiency of a CS-DBS even in the presence of very high volumes of update transactions.
[Real time systems, workstation clusters, transaction processing, client-server systems, real-time guarantees, client-server database system, grouped locks, Power system management, Satellite broadcasting, real-time client request handling, high-bandwidth networking options, Financial management, Throughput, load-sharing mechanism, Electrical capacitance tomography, Application software, Environmental management, client-server cluster, real-time processing efficiency, Information science, resource allocation, real-time systems, distributed databases, Database systems]
DBRpc: a highly adaptable protocol for reliable DSM systems
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
This paper evaluates the Dynamic Boundary Restricted protocol with phase-checking (DBRpc). DBRpc is a highly-adaptable extension of DBR that can respond to changes in the network environment and application workload. DBRpc is suitable for reliable shared memory systems that require rapid and complete response to site failures and phase changes. In this paper, we compare DBRpc with traditional protocols as well as variants of the BR/DBR class of protocols. We find that DBRpc offers unequaled adaptability and availability while having competitive operation costs.
[Availability, Protocols, Costs, competitive operation costs, Read-write memory, site failures, network environment, phase changes, adaptability, availability, software fault tolerance, reliable shared memory systems, transport protocols, Failure analysis, application workload, distributed shared memory systems, DBRpc, highly adaptable protocol, Distributed Bragg reflectors, Dynamic Boundary Restricted protocol with phase-checking]
A property-based clustering approach for the CORBA Trading Service
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
The CORBA Trading Service is an object service advertiser for heterogeneous distributed computing environments. Current approaches for the design and implementation of such a CORBA service do not deal with some of the major problems of searching for service offers in large-scale distributed systems, namely performance and scalability problems. This paper proposes an appropriate approach for clustering service offers based on the service properties, in order to enhance the efficiency of the trading service. The proposed approach clusters service offers within a hierarchy of contexts by specialisation of property sets. Performance results of the proposed clustering approach are discussed, and the benefits of including clustering of properties with the CORBA Trading Service are shown.
[Context-aware services, CORBA Trading Service, efficiency, Object oriented databases, Scalability, Heuristic algorithms, object service advertiser, performance problems, Partitioning algorithms, advertising, large-scale distributed systems, Distributed computing, Computer science, service property set specialization, scalability problems, performance, Clustering algorithms, service offer clustering, heterogeneous distributed computing environments, service offer searching, property-based clustering approach, Large-scale systems, Australia, distributed object management]
Causally ordered multicast: the conservative approach
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
Process group toolkits provide methods to structure a system as a set of groups of cooperating processes, to detect process failures, and to order events (by ordering messages). Such tools have a performance cost for applications, particularly when a system is built using a large number of overlapping groups. We built an event-driven simulation to study performance of causally ordered message delivery in large systems composed of overlapping groups. Our studies, the first ever of multiple group systems, reveal some conditions under which the delays can be very large: two orders of magnitude greater than when delays are not required. Further, in a large system these delays can lead to increased system burstiness which limits system scalability. These results suggest that a system supporting multiple overlapping groups needs to be carefully designed and the system should often provide users with control over when to apply ordering guarantees.
[Out of order, Transport protocols, Costs, event-driven simulation, ordering guarantees, Delay, system burstiness, process failure detection, Fault tolerant systems, multicast communication, system scalability, protocols, cooperating process groups, message passing, causally ordered message delivery, performance evaluation, Multicast protocols, overlapping groups, large systems, conservative approach, Multicast algorithms, performance, delay, Intersymbol interference, delays, causally ordered multicast, virtual machines, performance cost, process group toolkits, event ordering]
Design and performance evaluation of a Java-based multicast browser tool
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
This paper presents a case study in the use of reliable multicasting in Web-based multi-party applications. To carry out this study, we have designed and implemented WEBCLASS, a multicast browser tool written in Java. In WEBCLASS, all the actions of a "master" Web browser are mimicked on a set of client browsers. Monitoring of the master browser is performed by a set of threads, which use a reliable multicast protocol to disseminate state information and Web resources to programs that control the client browsers. The architecture and operation of the main components of WEBCLASS are described, and experimental results of a performance study are presented.
[Java, client-server systems, Java-based multicast browser tool, WEBCLASS, Communication system control, Multicast communication, performance evaluation, Application software, Computer science, Negative feedback, transport protocols, Distributed databases, Web-based multi-party applications, multicast communication, online front-ends, groupware, Streaming media, Computer networks, computer aided instruction, Internet, Monitoring, client browsers]
Progressive construction of consistent global checkpoints
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
A checkpoint pattern is an abstraction of the computation performed by a distributed application. A progressive view of this abstraction is formed by a sequence of consistent global checkpoints that may have occurred in this order during the execution of the application. Considering pairs of checkpoints, we have determined that a checkpoint must be observed before another in a progressive view if the former Z-precedes the latter. Based on the Z-precedence and characteristics of the checkpoint pattern, we propose original algorithms for the progressive construction of consistent global checkpoints. We demonstrate that the Z-precedence between a pair of checkpoints is a much simpler way to express the existence of a zigzag path connecting them, and we discuss other advantages of our relation.
[Checkpointing, program debugging, Protocols, checkpoint pattern, Debugging, distributed processing, progressive construction, consistent global checkpoints, distributed application, system recovery, software fault tolerance, Z-precedence, zigzag path, system monitoring, Joining processes, Monitoring, computation abstraction, algorithm]
The Swarm scalable storage system
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
Swarm is a storage system that provides scalable, reliable, and cost-effective data storage. Swarm is based on storage servers, rather than file servers; the storage servers are optimized for cost-performance and aggregated to provide high-performance data access. Swarm uses a striped log abstraction to store data on the storage servers. This abstraction simplifies storage allocation, improves file access performance, balances server loads, provides fault-tolerance through computed redundancy, and simplifies crash recovery. We have developed a Swarm prototype using a cluster of Linux-based personal computers as the storage servers and clients; the clients access the servers via the Swarm-based Sting file system. Our performance measurements show that a single Swarm client can write to two storage servers at 3.0 MB/s, while four clients can write to eight servers at 16.0 MB/s.
[storage allocation, Costs, network servers, crash recovery, storage clients, fault-tolerance, File servers, Control systems, local area networks, system recovery, Network servers, Fault tolerance, File systems, computed redundancy, scalable data storage, Prototypes, Clustering algorithms, striped log abstraction, cost performance, Swarm scalable storage system, redundancy, client-server systems, high-performance data access, Linux-based personal computer cluster, Access protocols, information retrieval, abstraction, file access performance, server load balancing, Computer science, reliable data storage, cost-effective data storage, performance measurements, storage servers, Swarm-based Sting file system, fault tolerant computing]
(h, k)-arbiters for h-out of-k mutual exclusion problem
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
h-out of-k mutual exclusion is a generalization of 1-mutual exclusion problem, where there are k limits of shared resources and each process requests h(1/spl les/h/spl les/k) units at the same time. Though the k-arbiter has been shown to be a quorum-based solution to this problem, quorums in the k-arbiter are much larger than these in the 1-coterie for 1-mutual exclusion. Thus, the algorithm based on the k-arbiter needs many messages. This paper defines two (h, k)-arbiters for h-out of-k mutual exclusion: a uniform (h, k)-arbiter and a (k+1)-cube (h, k)-arbiter. The quorums in each (h, k)-arbiter are not larger than the ones in the corresponding k-arbiter; consequently using the (h, k)-arbiters is more efficient than using the k-arbiters. Uniform (h, k)-arbiter is an optimal generalization of the majority coterie for 1-mutual exclusion. (k+1)-cube (h, k)-arbiter is a quasi-optimal generalization of square grid coterie for 1-mutual exclusion.
[(h, mutual exclusion problem, optimal generalization, Video sharing, distributed processing, Partitioning algorithms, k)-arbiters, Printers, majority coterie, resource allocation, quorum-based solution, Bandwidth, Permission, System recovery, shared resources, messages, square grid coterie, Resource management, Distributed algorithms]
Beyond the black box: event-based inter-process communication in process support systems
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
We describe the concept and implementation of an inter-process communication facility based on the exchange of events between concurrently running processes. In contrast to previous approaches based on shared data stored in a common database, our approach has the advantage of being platform independent and providing straightforward support for distribution. In addition, we also explore the problem of process atomicity and consistency when events are revoked due to the abort of processes. We present a family of recovery protocols that allow to control the effects of aborted processes.
[process atomicity, process support systems, distributed processing, Iron, Electrical capacitance tomography, Transaction databases, system recovery, process consistency, recovery protocols, Engines, Technology management, database, Prototypes, National electric code, distributed databases, shared data, concurrent processes, protocols, Usability, platform independent, Monitoring, event-based inter-process communication]
Possession System: middleware for adaptive multiuser applications in a mobile environment
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
Describes the design and implementation of a middleware system named the Possession System. The system is designed based on the "possession model\
[Adaptive systems, unpredictable changes, soul, user interaction, adaptive multi-user applications, body, mobile computing, networked sensors, multi-access systems, Possession System, behavioral changes, middleware, Pervasive computing, client-server systems, application components, distributed devices, mobile environment, Sensor systems and applications, Ubiquitous computing, device configuration, Middleware, adaptive systems, component migration, abstractions, system events, available computing resources, distributed multi-user applications, ubiquitous computing environments, possession model, Mobile computing, consistent user view, geographic location]
Optimal dynamic location update for PCS networks
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
The movement based dynamic location update scheme is studied. An analytical model is applied to formulate the costs of location update and paging in the movement based location update scheme. The problem of minimizing the total cost is formulated as an optimization problem that finds the optimal threshold in the movement based location update scheme. We prove that the total cost function is a convex function of the threshold. Based on the structure of the optimal solution, an efficient algorithm is proposed to find the optimal threshold directly. Furthermore, the proposed algorithm is applied to study the effects of changing important parameters of mobility and calling patterns numerically.
[PCS networks, optimization problem, personal communication networks, convex function, paging, movement based dynamic location update scheme, analytical model, optimal threshold, mobile communication systems, telecommunication computing, personal communication service networks, calling patterns, Personal communication networks, minimisation, cost minimization, total cost function]
Self-stabilizing neighborhood synchronizer in tree networks
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
Proposes a self-stabilizing synchronization technique, called the Neighborhood Synchronizer (/spl Nscr//spl Sscr/), that synchronizes nodes with their neighbors in a tree network. The /spl Nscr//spl Sscr/ scheme has an extremely small memory requirement-only one bit per processor. Algorithm /spl Nscr//spl Sscr/ is inherently self-stabilizing. We apply our synchronizer to design a broadcasting algorithm /spl Bscr//spl Ascr/ in a tree network. Algorithm /spl Bscr//spl Ascr/ is also inherently self-stabilizing and needs only 2h+2m-1 rounds to broadcast m messages, where h is the height of the tree.
[self-stabilizing synchronization technique, tree networks, Protocols, node synchronization, Neighborhood Synchronizer, Color, self-adjusting systems, memory requirement, message broadcasting, synchronisation, broadcasting, Computer science, Intelligent networks, Casting, broadcasting algorithm, switching networks, Feedback, distributed algorithms, Broadcasting, stability]
Striping and buffer caching for software RAID file systems in workstation clusters
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
A software RAID file system is defined as a system that distributes data redundantly across an array of disks attached to each of the workstations connected to a high-speed network. This configuration provides higher throughput and availability compared to conventional file systems. In this paper, we consider two specific issues, namely the distribution of data among the cluster (i.e. striping) and buffer caching for such an environment. Through simulation studies, we compare the performance of various striping methods and show that, for effective striping in software RAID file systems, we must take advantage of its flexible nature. Furthermore, for buffer caching, we show that conventional caching schemes that have been developed for distributed systems are insufficient, and that the ExODP (Exclusively Old Data and Parity) scheme, which is presented in this paper, overcomes the limitations of the previously proposed schemes.
[workstation clusters, Costs, simulation, Throughput, World Wide Web, File servers, availability, Concurrent computing, File systems, Parallel processing, software RAID file systems, Workstations, Computer science education, throughput, disk array, software performance evaluation, high-speed network, redundant data distribution, buffer storage, exclusively old data and parity, buffer caching, RAID, ExODP, performance, striping, distributed algorithms, Software systems]
Scalable processing of read-only transactions in broadcast push
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
Recently, push-based delivery has attracted considerable attention as a means of disseminating information to large client populations in both wired and wireless settings. We address the problem of ensuring the consistency and currency of client read-only transactions in the presence of updates. To this end, additional control information is broadcast. A suite of methods is proposed that vary in the complexity and volume of the control information transmitted and subsequently differ in response times, degrees of concurrency, and space and processing overheads. The proposed methods are combined with caching to improve query latency. The relative advantages of each method are demonstrated through both simulation results and qualitative arguments. Read-only transactions are processed locally at the client without contacting the server and thus the proposed approaches are scalable, i.e., their performance is independent of the number of clients.
[transaction processing, Satellite broadcasting, simulation, Electronic switching systems, cache storage, query latency, consistency, caching, Delay, Concurrent computing, query processing, Network servers, push-based delivery, distributed databases, Monitoring, client-server systems, information dissemination, broadcast push, Information retrieval, Magnetic heads, concurrency, Computer science, read-only transaction processing, Land mobile radio cellular systems, response time, client server systems, scalable processing]
System support for dynamic layout of distributed applications
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
Dynamic application layout is the capability to move the components of a distributed program among different hosts during the execution of the application. This capability is essential for large-scale applications since it allows to adapt them to changes in resource availability which are common in wide-area environments. The FarGo system introduces a model for programming the layout of distributed applications separately from their basic logic, by attaching relocation semantics to inter-component references, and by using a built-in monitoring support for making relocation decisions. Dynamic layout policies are encoded within the application using a special API or externally using a high-level scripting language. This paper presents the design of the runtime environment that realizes the model.
[FarGo system, application program interfaces, resource availability, high-level scripting language, relocation semantics, Power engineering computing, resource allocation, authoring languages, mobile objects, dynamic distributed application layout, Cities and towns, software components, runtime environment, Monitoring, distributed programming, Availability, Runtime environment, Java, Logic programming, inter-component references, Power system modeling, large-scale applications, wide-area environments, Systems engineering and theory, system monitoring, API, Power engineering and energy]
NAP: practical fault-tolerance for itinerant computations
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
One use of mobile agents is support for itinerant computation (D. Chess et al., 1995). An itinerant computation is a program that moves from host to host in a network. Which hosts the program visits is determined by the program. The program can have a pre-defined itinerary or can dynamically compute the next host to visit as it visits each successive host; it can visit the same host repeatedly or it can even create multiple concurrent copies of itself on a single host. Itinerant computations are susceptible to processor failures, communications failures, and crashes due to program bugs. NAP is a protocol for supporting fault tolerance in itinerant computations. It employs a form of failure detection and recovery, and it generalizes the primary backup approach to a new computational model. The guarantees offered by NAP as well as an implementation for NAP in TACOMA are discussed.
[Protocols, practical fault tolerance, processor failures, multiple concurrent copies, system recovery, Jacobian matrices, Fault tolerance, mobile computing, Mobile agents, primary backup approach, mobile agents, Computer networks, communications failures, protocols, Local area networks, compuational model, TACOMA, Computer crashes, itinerant computations, Programming profession, Computer science, Computer bugs, fault tolerant computing, NAP, failure detection]
Using hysteresis to reduce adaptation cost of a dynamic quorum assignment
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
One classic technique for coordinating distributed computations is to require each processor to get permission for certain actions from some quorum of processors, such that every processor's quorum overlaps every other processor's quorum. A dynamic quorum assignment allows the processor-to-quorum mapping to adapt during system execution, to improve performance and availability when processors fail or change load. This work considers how the run-time quorum adaptation cost is impacted by the selection of a quorum mapping function. An effective quorum mapping function exhibits not only desirable quorum size and load properties, but also a type of hysteresis that minimizes the changes made to the processor-to-quorum mapping whenever the mapping is recomputed. A new quorum mapping function called MEMRING is given that exhibits hysteresis by identifying quorums that are similar to previously chosen quorums. This behavior reduces the needed number of modifications to dynamic quorum assignment data structures, and can consequently reduce the amount of interprocessor communication needed for distributed control of quorum adaptation. The expected cost of distributed quorum adaptation using MEMRING is shown to be less than the expected cost of using other quorum mapping functions that have similar quorum size and load properties.
[Protocols, load properties, MEMRING, quorum size, dynamic quorum assignment, quorum mapping function, adaptation cost, availability, Distributed computing, processor scheduling, hysteresis, Runtime, interprocessor communication, Permission, Cost function, data structures, Safety, expected cost, Hysteresis, Availability, distributed control, run-time quorum adaptation cost, Data structures, processor-to-quorum mapping, distributed computations, quorum adaptation, Distributed control, system execution]
Static and dynamic mapping of media assets on a network of distributed multimedia information servers
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
This paper presents principles and algorithms for the management of a network of media servers. Such a server network allows the online delivery of broadband multimedia data, e.g. audio and video streams, to a large number of widely distributed clients. Thus, the implementation of large scale distributed broadband media information services is possible if such networks can be handled efficiently. In the paper two new combinatorial optimization problems are defined that have to be solved for an efficient management of the distributed media servers connected by a high-bandwidth communication network. The algorithms consider the case of a static mapping of media assets onto the server network as well as the dynamic migration of media assets on such a network. The first scenario is used in the case that all media assets are loaded for the first time onto the server network. The second scenario is applied dynamically during run-time of an information service taking the varying user access pattern and changing media content of the server network into account. In this paper the algorithms are presented and their efficiency is demonstrated using some benchmark instances. The algorithms are part of a larger distributed system that allows the management of a network of distributed media servers. The principles of this system are presented as well as some applications that have been developed and are supported by this distributed server management system (DSMS).
[client-server systems, high-bandwidth communication network, multimedia servers, user access pattern, Multimedia systems, client server system, Mathematics, run-time, distributed server management system, Information systems, network management, Computer science, Graphics, Network servers, Reactive power, combinatorial optimization, computer network management, optimisation, broadband multimedia data, Bandwidth, Streaming media, media servers, Large-scale systems, distributed multimedia information servers]
Interpreting stale load information
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
In this paper we examine the problem of balancing load in a large-scale distributed system when information about server loads may be stale. It is well known that sending each request to the machine with the apparent lowest load can behave badly in such systems, yet this technique is common in practice. Other systems use round-robin or random selection algorithms that entirely ignore load information or that only use a small subset of the load information. Rather than risk extremely bad performance on one hand or ignore the chance to use load information to improve performance on the other, we develop strategies that interpret load information based on its age. Through simulation, we examine several simple algorithms that use such load interpretation strategies under a range of workloads. Our experiments suggest that by properly interpreting load information, systems can (1) match the performance of the most aggressive algorithms when load information is fresh relative to the job arrival rate, (2) outperform the best of the other algorithms we examine by as much as 60% when information is moderately old, (3) significantly outperform random load distribution when information is older still, and (4) avoid pathological behavior even when information is extremely old.
[client-server systems, job arrival rate, Engineering profession, pathological behavior, load interpretation strategies, Sun, Network servers, Pathology, resource allocation, Operating systems, server loads, stale load information, Load management, Large-scale systems, Web server, Round robin, load information, large-scale distributed system, Load modeling, random load distribution]
Effective complexity reduction for optimal scheduling of distributed real-time applications
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
The application of optimal search strategies to scheduling for distributed real-time systems is, in general, plagued by an inherent computational complexity. This has effectively prevented the integration of strategies such as branch-and-bound (B&B) in scheduling frameworks and tools used in practice today. To show that optimal scheduling is, in fact, a viable alternative for many real-time scheduling scenarios, we propose an approach that can reduce the average search complexity to levels comparable with that of a polynomial-time heuristic. Our approach is based on making intelligent choices in the selection of strategies for search tree vertex traversal and task deadline assignment. More specifically, we conjecture that effective complexity reduction is achieved by (i) traversing vertices in the search tree in a depth-first fashion and (ii) assigning local task deadlines that are non-overlapping fractions of the application end-to-end deadline. Through an extensive experimental study, we find that our approach contribute to reducing the average search complexity by several orders of magnitude for a frequently-used class of distributed real-time applications.
[Real time systems, optimal scheduling, Optimal scheduling, task deadline assignment, search tree vertex traversal, Application software, tree searching, Computational complexity, Distributed computing, Intelligent robots, processor scheduling, inherent computational complexity, Automotive engineering, real-time scheduling scenarios, Processor scheduling, resource allocation, branch-and-bound, real-time systems, distributed real-time applications, Polynomials, effective complexity reduction, Intelligent systems, end-to-end deadline, computational complexity]
Proxy cache coherency and replacement-towards a more complete picture
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
This work studies the interaction of Web proxy cache coherency and replacement policies using trace-driven simulations. We specifically examine the relative importance of each type of policy in affecting the overall costs, the potential of incorporating coherency issues in cache replacement and the inclusion of additional factors such as frequency of resource use in replacement and coherency policies. The results show that the cache replacement policy in use is the primary cost determinant for relatively small caches, while the cache coherency policy is the determinant for larger caches. Incorporating cache coherency issues in cache replacement policies yields little improvement in overall performance. The use of access frequency in cache replacement, along with temporal locality and size information, results in a simple and better performing policy than found in previously published work. Combining this new replacement policy with the best piggyback-based cache coherency policy results in a 4.5% decrease in costs and 89% reduction in staleness ratio when compared to policy combinations in current use. Preliminary work indicates that cache replacement and coherency policies continue to affect costs in the presence of HTTP protocol enhancements such as persistent connections.
[proxy cache coherency, access frequency, Costs, Protocols, temporal locality, persistent connections, Telecommunication traffic, staleness ratio, Electronic switching systems, cache storage, piggyback-based cache coherency policy, Electrical capacitance tomography, primary cost determinant, Delay, trace-driven simulations, Network servers, proxy cache replacement, cache replacement policy, coherency policies, concurrency control, HTTP protocol enhancements, size information, Page description languages, Internet]
On classes of problems in asynchronous distributed systems with process crashes
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
This paper is on classes of problems encountered in asynchronous distributed systems in which processes can crash but links are reliable. The hardness of a problem is defined with respect to the difficulty to solve it despite failures: a problem is easy if it can be solved in presence of failures, otherwise it is hard. Three classes of problems are defined: F, NF and NFC. F is the class of easy problems, namely, those that can be solved in presence of failures (e.g., reliable broadcast). The class NF includes harder problems, namely, the ones that can be solved in a non-faulty system (e.g., consensus). The class NFC (NF-complete) is a subset of NF that includes the problems that are the most difficult to solve in presence of failures. It is shown that the terminating reliable broadcast problem, the non-blocking atomic commitment problem and the construction of a perfect failure detector (problem P) are equivalent problems and belong to NFC. Moreover the consensus problem is not in NFC. The paper presents a general reduction protocol that reduces any problem of NF to P. This shows that P is a problem that lies at the core of distributed fault-tolerance.
[Costs, distributed processing, terminating reliable broadcast problem, Computer crashes, Electrical capacitance tomography, Noise measurement, asynchronous distributed systems, distributed fault-tolerance, software fault tolerance, reliable links, consensus, perfect failure detector, Fault detection, Detectors, nonblocking atomic commitment problem, Polynomials, process crashes, reliable broadcast, protocols]
Providing support for survivable CORBA applications with the Immune system
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
The Immune system aims to provide survivability to CORBA applications, enabling them to continue to operate despite malicious attacks, accidents or faults. Every object within the CORBA application is actively replicated by the Immune system, with majority voting applied on incoming invocations and responses to each replica of the object. Secure multicast protocols are employed to enable the majority voting to be effective, even when processors within the network and objects within the application become corrupted.
[telecommunication security, incoming invocations, malicious attacks, corrupted processors, majority voting, Electrical capacitance tomography, Security, Distributed computing, CORBA applications, secure multicast protocols, Voting, multicast communication, computer network reliability, protocols, Web server, Protection, distributed object management, Immune system, fault tolerance, replicated databases, object-oriented databases, actively replicated objects, Multicast protocols, Application software, accidents, survivability, Accidents]
On providing quality-of-service control for core-based multicast routing
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
In this paper, we develop efficient admission control tests for member join/leave and its associated state refresh and update procedures for receiver-initiated core-based multicast routing, e.g., core based tree (CBT) protocol, to allow construction of QoS-capable multicast trees, while making the minimum possible impact on the existing infrastructure. Specifically, we (i) derive sufficient conditions for a multicast tree to maintain its QoS; (ii) devise effective admission tests to verify whether or not a group member may join the multicast tree at adequate QoS, while not violating existing QoS guarantees to other on-tree members; and (iii) identify the minimum set of state information required for the admission tests and develop a soft state refresh and update procedure. Finally, we validate the effectiveness of the proposed mechanism by incorporating it into the CBT protocol, and evaluate it via event-driven simulations in terms of the probability of join requests being rejected, message overhead, and scalability.
[telecommunication congestion control, Scalability, quality-of-service control, Quality of service, Predictive models, QoS-capable multicast trees, Discrete event simulation, receiver-initiated core-based multicast routing, Delay, soft state refresh procedure, scalability, Web and internet services, multicast communication, Routing protocols, state information, protocols, member leave, join request rejection probability, Testing, event-driven simulations, core based tree protocol, trees (mathematics), Multicast protocols, quality of service, soft state update procedure, member join, Admission control, telecommunication network routing, admission control tests, on-tree members, message overhead, telecommunication traffic]
A compiler-based approach to design and engineering of complex real-time systems
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
We present a compiler-based approach to the design and engineering of complex real-time systems. The systems are built from computation, communication and I/O subsystems, based on different extant real-time models and schemes. We have defined a high-level system specification language, called Real-Time System Markup Language (RTSML), for integration of model-specific subsystems. We have developed a prototype extensible compiler from RTSML to a Constraint Logic Programming (CLP) language. Model-specific compiler modules can be added that generate CLP code for subsystems based on the corresponding real-time models and schemes, and their integration with other subsystems of a complex real-time system being specified. We specify an example system consisting of dozens of communicating real-time tasks that share real-time computation and communication resources, and map it to two problem solving approaches. The first one is the conventional CLP search, while the second one is suitable for system reconfiguration when system parameters change dynamically. We compare the two approaches and present their timings for the example system. Based on the results obtained, and potentials of the software technologies used in our compiler-based approach, it appears promising for large-scale complex real-time systems.
[Real time systems, high-level system specification language, program compilers, Design engineering, Program processors, Prototypes, specification languages, RTSML, Large-scale systems, constraint handling, model-specific subsystems, distributed programming, Logic programming, extant real-time models, communication resources, problem solving, Specification languages, compiler-based approach, large-scale complex real-time systems, model-specific compiler modules, Markup languages, real-time systems, Real-Time System Markup Language, Timing, Problem-solving, complex real-time systems, system reconfiguration]
Redirection algorithms for load sharing in distributed Web-server systems
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
Replication of information among multiple World Wide Web servers is necessary to support high request rates to popular Web sites. A clustered Web server organization is preferable to multiple independent mirrored servers because it maintains a single interface to the users and has the potential to be more scalable, fault-tolerant and better load-balanced. In this paper, we propose a Web cluster architecture in which the Domain Name System (DNS) server, which dispatches the user requests among the servers through the URL name to the IP address mapping mechanism, is integrated with a redirection request mechanism based on HTTP. This should alleviate the side-effect of caching the IP address mapping at intermediate name servers. We compare many alternative mechanisms, including synchronous vs. asynchronous activation and centralized vs. distributed decisions on redirection. Moreover, we analyze the reassignment of entire domains or individual client requests, different types of status information and different server selection policies for redirecting requests. Our results show that the combination of centralized and distributed dispatching policies allows the Web server cluster to handle high load skews in the WWW environment.
[redirection request mechanism, status information, World Wide Web, domain reassignment, Web site request rates, Domain Name System, caching, scalability, IP address mapping mechanism, Information analysis, asynchronous activation, Uniform resource locators, Web cluster architecture, Fault tolerance, resource allocation, URL name, Clustering algorithms, file servers, clustered Web server organization, intermediate name servers, synchronous activation, Web server, load skew, information resources, replicated information, centralized decisions, load sharing, fault tolerance, Service oriented architecture, server selection policies, HTTP, client request reassignment, user interface, redirection algorithms, dispatching policies, World Wide Web servers, transport protocols, distributed algorithms, distributed Web server systems, user request dispatching, Dispatching, load balance, distributed decisions, Web sites, mirrored servers]
Reducing message overhead in TMR systems
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
Traditional TMR protocols assume either single, reliable voters for each triple-modular redundant unit (TMRU) or triplicated voters (one for each processor) for each TMRU. In the first case a voter is a single point of failure for the system. In the second case, many physical messages must be sent across the communication network for each logical data item. We examine some protocols which attempt to maintain the functionality of the triplicated voter TMR protocol while reducing the number of physical messages required by one third. Possible solutions are examined to the many issues that result from this reduction in communication. Three different reduced-communication triple-modular redundant (RTMR) protocols are considered, each of which makes different assumptions about the nature of the underlying computation.
[Protocols, Delay systems, single reliable voters, logical data item, TMR protocols, reduced-communication triple-modular redundant protocols, TMR systems, Hardware, redundancy, Communication networks, protocols, message passing, communication network, Redundancy, Electrical fault detection, Circuit faults, message overhead reduction, physical messages, triplicated voters, failure, triple-modular redundant unit, fault tolerant computing, Error correction, Timing, Telecommunication network reliability]
Efficient kernel support of fine-grained protection domains for mobile code
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
Mobile code is an emerging paradigm of distributed computing. It roams over a network, is linked with an application, and runs as a part of an application. In the case of Web browsers, it is commonplace to download a mobile code, called a plug-in, from a truly open network such as the Internet. Owing to the anonymity of an open network, the mobile code may be malicious; thus, it is important to protect local computing resources from attacks by malicious code. We have developed a kernel that supports fine-grained protection domains that preclude mobile code from making unauthorized accesses to the local resources. The developed scheme provides a novel mechanism, called a multi-protection page table, of virtual memory for creating fine-grained protection domains. The multi-protection page table enables efficient cross-domain calls, whereas it provides protection. Experimental results show that the developed scheme incurs only a 5.9% execution overhead even if cross domain calls occur 30000 times per second.
[Switches, virtual memory, efficient cross-domain calls, Distributed computing, distributed computing, Information science, Network servers, mobile code, Computer networks, Libraries, anonymity, IP networks, Web browsers, Kernel, Protection, distributed programming, paged storage, plug-in, multi-protection page table, open network, unauthorized access, local computing resource protection, efficient kernel support, security of data, online front-ends, execution overhead, Internet, fine-grained protection domains, malicious code attacks, Mobile computing]
Distributed deadlock detection and resolution based on hardware clocks
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
Inexpensive but accurate hardware clocks are now commonplace on many systems. A clock synchronization protocol can keep the collection of clocks for a group of networked systems roughly synchronized without the expenditure of a great deal of processor time or network bandwidth. As long as the bounded skew between clocks is taken into account, rough real time can provide an intuitive and valuable mechanism for providing a notion of order in a distributed system. This paper presents a straightforward token-based protocol for the detection of distributed deadlock under the single resource model. It uses clock values as token time stamps to ensure that exactly one process in a deadlock cycle, the process that completed the cycle, detects the deadlock and aborts, breaking the deadlock. The clock-valued time stamps also ensure that no false deadlocks are detected, without additional protocol to eliminate obsolete tokens. Arguments for the correctness of the protocol are developed.
[Protocols, clock synchronization protocol, Drives, deadlock resolution, processor time, Cities and towns, Hardware, protocols, real time, bounded skew, Receivers, hardware clocks, Concurrency control, Synchronization, network bandwidth, synchronisation, Computer science, clocks, distributed deadlock detection, concurrency control, real-time systems, token-based protocol, System recovery, single resource model, Clocks]
Ambassadors: structured object mobility in worldwide distributed systems
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
In many distributed systems applications deployed on the worldwide scale, it is latency, rather than bandwidth, that is the primacy determinant of performance. This paper describes Ambassadors, a communication technique using mobile Java objects within an RPC/RMI-like communication structure. Ambassadors minimise the aggregate latency of sequences of inter-dependent remote operations by migration of code to the vicinity of the server to execute those operations. Furthermore, because Ambassadors migrate within an RPC/RMI-like structure communication has well defined failure semantics, an important characteristic in supporting effective software engineering of distributed systems.
[Decision support systems, Java, object-oriented programming, code migration, RPC, bandwidth, latency, mobile Java objects, Ambassadors, structured object mobility, communication technique, RMI, performance, remote operations, failure semantics, remote procedure calls, distributed systems, software engineering, distributed programming, software performance evaluation]
Incorporating transaction semantics to reduce reprocessing overhead in replicated mobile data applications
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
Update anywhere-anytime-anyway transactional replication has unstable behavior as the workload scales up. To reduce this problem, a two-tier replication algorithm is proposed in (Gray et al., 1996) that allows mobile applications to propose tentative transactions that are later applied to a master copy. However it can suffer from heavy reprocessing overhead in many circumstances. We present the method of merging histories instead of reprocessing to reduce the overhead of two-tier replication. The basic idea is when a mobile node connects to the base nodes merging the tentative history into the base history so that substantial work of tentative transactions could be saved. As a result, a set of undesirable transactions (denoted B) have to be backed out to resolve the conflicts between the two histories. Desirable transactions that are affected directly or indirectly, by the transactions in B complicate the process of backing out B. We present a family of novel rewriting algorithms for the purpose of backing out B. By incorporating transaction semantics, our rewriting methods are strictly better at saving desirable tentative transactions than the traditional reads-from transitive-closure based approach. In most cases our rewriting methods are better at saving desirable tentative transactions than an approach which is based only on commutativity.
[transaction processing, history merging, merging, transaction semantics, rewriting algorithms, replicated databases, Merging, Government, update anywhere-anytime-anyway, Electrical capacitance tomography, Transaction databases, reprocessing overhead, History, mobile databases, Read only memory, Delay, Information systems, Content addressable storage, transactional replication, commutativity, System recovery, replicated mobile data applications, two-tier replication algorithm, reads-from transitive-closure]
Mobile agent programming in Ajanta
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
The paper gives an overview of Ajanta, a Java based system for mobile agent programming. We outline the Ajanta architecture, and discuss the basic elements that comprise an agent based application. Ajanta's programming environment is defined in terms of a set of primitive operations for agent creation, dispatch, migration and remote control. Agents can access server resources using a proxy based access control mechanism. We describe a scheme for agent migration based on the composition of some basic migration patterns which incorporate exception handling mechanisms. Finally, we present two agent based distributed applications implemented using the Ajanta system. One is a middleware which supports file sharing over the Internet and the other is a distributed calendar manager.
[exception handling, Network servers, programming environment, mobile computing, Mobile agents, agent migration, agent based application, Computer networks, mobile agent programming, primitive operations, Protection, distributed programming, agent based distributed applications, middleware, file sharing, distributed calendar manager, Java, client-server systems, server resources, Identity-based encryption, remote control, proxy based access control mechanism, Ajanta architecture, agent creation, Application software, Computer science, basic migration patterns, Java based system, Internet, exception handling mechanisms, Remote monitoring]
Trust vs. threats: recovery and survival in electronic commerce
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
The paper analyzes threats and attacks in the Internet commerce world and suggests schemes to detect the attacks when they occur, prevent further loss once an attack is detected, and provides remedial corrective actions so as to enable victims of commerce-related attacks to resume conducting business transactions. Some commerce based transaction recovery mechanisms are suggested to recover from losses these attacks may have caused. Suitable cryptographic primitives and protocols that realize an anonymous complaint and an anonymous receipt are developed to provide trust and security against e-commerce related attacks.
[Internet commerce world, business communication, e-commerce related attacks, Electronic commerce, Security, system recovery, Read only memory, business transactions, security, Safety, Cryptography, protocols, Contracts, Business, electronic commerce, Resumes, cryptographic primitives, commerce based transaction recovery mechanisms, cryptography, remedial corrective actions, anonymous receipt, Cryptographic protocols, anonymous complaint, Internet, commerce-related attacks]
Initial synchronization of TDMA communication in distributed real-time systems
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
This paper discusses the startup phase of a TDMA protocol intended for safety-critical real-time systems using a broadcast bus. The protocol contains sender id in each message, and nodes send messages of equal size in a fixed order. A single channel media is used and data and synchronization information must therefore share the same channel. Synchronization is challenging, since clocks must be synchronized to guarantee successful transmissions, and successful transmissions rely on synchronized clocks. We describe three different start-up algorithms and evaluate them with respect to complexity, resynchronization time and performance in the presence of transient faults.
[Real time systems, synchronization information, TDMA protocol, Protocols, distributed real-time systems, Broadcast technology, safety-critical software, distributed processing, single channel media, Control systems, Delay, Time division multiple access, Embedded system, embedded systems, Broadcasting, broadcast bus, start-up algorithms, Synchronization, TDMA communication, transient faults, time division multiple access, safety-critical real-time systems, resynchronization time, synchronized clocks, Clocks, computational complexity]
Uniform timing of a multi-cast service
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
Presents a new architecture for a clock synchronization protocol based on multi-cast communication. The protocol implements the gradual tuning of the clocks, in order to automatically compensate the systematic drift and increase the time between re-synchronizations. Hosts participating in the same service are grouped into a cohort: the protocol self-stabilizes into an optimal routine where all members of a cohort are periodically synchronized with the same messages. New members dynamically joining the cohort perturb this routine, which is spontaneously and eventually restored.
[Protocols, perturbation, broadcast communication, Postal services, optimal routine, Network servers, multi-cast service, Databases, Measurement standards, gradual tuning, multicast communication, spontaneous restoration, protocols, Web server, stability, multimedia communication, Availability, virtual clock, tuning, distributed multimedia applications, systematic drift compensation, timing, host cohort, multi-cast communication, self-adjusting systems, Synchronization, synchronisation, broadcasting, clocks, distributed timing, distributed algorithms, messages, compensation, Timing, periodic synchronization, self-stabilizing protocol, uniform timing, clock synchronization protocol architecture, resynchronization time, Clocks]
The inter-group router approach to scalable group composition
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
This paper examines the problem of building scalable, fault-tolerant distributed systems from collections of communicating process groups, while maintaining well-defined end-to-end delivery semantics. We propose a new architecture which supports modular group composition by providing a distinction between intra-group and inter-group communication. With this architecture, multiple group communication protocols and end-to-end delivery semantics can be used in a single system. These features reduce the complexity of ordering messages in a group composition, resulting in enhanced scalability. Finally we present simulation results comparing the performance of a group composition using our architecture to that of a single process group.
[Protocols, Scalability, simulation, intra-group communication, Multicast communication, distributed processing, Electrical capacitance tomography, communication complexity, multiple group communication protocols, scalable fault-tolerant distributed systems, reconfigurable architectures, communicating process groups, message ordering, Large-scale systems, inter-group communication, Argon, protocols, Monitoring, inter-group router approach, architecture, Availability, network routing, scalable group composition, Computer science, performance, High performance computing, modular group composition, virtual machines, fault tolerant computing, end-to-end delivery semantics]
Load balancing and hot spot relief for hash routing among a collection of proxy caches
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
Hash routing partitions the entire URL space among a collection of cooperating proxy caches. Each partition is assigned to a cache server. Duplication of cache contents is eliminated. Client requests to a cache server for non-assigned partition objects are forwarded to proper sibling caches. As a result, the load level of the cache servers can be quite unbalanced. We examine an adaptable controlled replication (ACR) of non-assigned partition objects in each cache server to reduce the load imbalance and relieve the problem of hot-spot references. Trace-driven simulations are conducted to study the effectiveness of ACR. The results show that: (1) access skew exists, and the load of the cache servers tends to be unbalanced in hash routing; (2) with a relatively small amount of ACR, say 10% of the cache size, significant improvements in load balance can be achieved; and (3) ACR provides a very effective remedy for load imbalance due to hot-spot references.
[nonassigned partition objects, load balancing, hot-spot references, World Wide Web, cache storage, Delay, Uniform resource locators, Network servers, resource allocation, cooperating proxy caches, cache servers, Routing protocols, URL space partitioning, IP networks, Web server, information resources, replicated databases, Computational modeling, adaptable controlled replication, adaptive control, hash routing, trace-driven simulations, Aggregates, distributed algorithms, telecommunication network routing, access skew, Load management, client request forwarding, Internet, naming services]
Processing transactions over optimistic atomic broadcast protocols
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
Atomic broadcast primitives allow fault-tolerant cooperation between sites in a distributed system. Unfortunately, the delay incurred before a message can be delivered makes it difficult to implement high performance, scalable applications on top of atomic broadcast primitives. A new approach has been proposed which, based on optimistic assumptions about the communication system, reduces the average delay for message delivery. We develop this idea further and present a replicated database architecture that employs the new atomic broadcast primitive in such a way that the coordination phase of the atomic broadcast is fully overlapped with the execution of transactions, providing high performance without relaxing transaction correctness.
[transaction processing, Protocols, replicated databases, replicated database architecture, Scalability, Laboratories, optimistic atomic broadcast protocols, scalable applications, memory protocols, distributed system, World Wide Web, fault-tolerant cooperation, software fault tolerance, Information systems, Postal services, Computer science, delay, Operating systems, Fault tolerant systems, message delivery, Broadcasting, high performance, software performance evaluation]
Exposing application alternatives
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
We present the design of an interface to allow applications to export tuning alternatives to a higher-level system. By exposing different parameters that can be changed at runtime, applications can be made to adapt to changes in their execution environment due to other programs, or the addition or deletion of nodes, communication links, etc. An integral part of this interface is that an application not only exposes its options, but also the resource utilization of each option and the effect that the option will have on the application's performance. We discuss how these options can be evaluated to tune the overall performance of a collection of applications in the system. Finally, we show preliminary results from a database application that is automatically reconfigured by the system from query shipping to data shipping based on the number of active clients.
[application program interfaces, communication links, Environmental management, Distributed computing, application performance, database application, query processing, Runtime, Software architecture, resource allocation, Physics computing, distributed databases, Large-scale systems, resource utilization, software performance evaluation, data shipping, active clients, application interface design, Harmony, Educational institutions, runtime, Application software, Computer science, query shipping, Resource management]
Mockingbird: flexible stub compilation from pairs of declarations
Proceedings. 19th IEEE International Conference on Distributed Computing Systems
None
1999
Mockingbird is a prototype tool for developing interlanguage and distributed applications. It compiles stubs from pairs of interface declarations, allowing existing data types to be reused on both sides of every interface. Other multilanguage stub compilers impose data types on the application, complicating development. Mockingbird supports C/C++, Java, and CORBA IDL, and can be extended to other languages. Mockingbird can generate stubs that convert between types whose structural equivalence would be missed by other tools. We show that this kind of tool improves programming productivity, and describe, in detail, Mockingbird's design and implementation.
[Computer interfaces, Mockingbird, C language, program compilers, distributed applications, Read only memory, Program processors, Prototypes, data types, software tools, distributed object management, Productivity, Java, flexible stub compilation, Identity-based encryption, programming productivity, interlanguage applications, LAN interconnection, C++ language, Programming profession, declarations, Computer languages, software tool, application generators, CORBA IDL]
A content placement and management system for distributed Web-server systems
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Clusters of commodity computers are becoming an increasingly popular approach for building cost-effective high-performance Internet servers. However, how to place and manage content in such a distributed and complex system is becoming a challenging problem. In particular, such distributed servers tend to be more heterogeneous, and this heterogeneity will further increase the management burden. This paper describes the motivation, design, implementation and performance of a content placement and management system for a heterogeneous distributed Web server.
[Content management, search engines, commodity computer clusters, World Wide Web, content placement system, file servers, distributed databases, distributed Web server systems, Internet, server heterogeneity, software performance evaluation, content management system, high-performance Internet servers]
Active files: a mechanism for integrating legacy applications into distributed systems
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Despite increasingly distributed Internet information sources with diverse storage formats and access-control constraints, most of the end applications (e.g. filters and media players) that view and manipulate data from these sources operate against a traditional file-based interface. These legacy applications need to be rewritten to access remote sources, or need to rely upon ad-hoc intermediary applications that aggregate the data into a passive file before executing the legacy application. This paper presents a simple, elegant, programmable method for allowing natural integration of legacy applications into distributed system infrastructures, The approach, called "active files\
[Data privacy, logical proxy, Passive filters, storage formats, Information filtering, semantics, data aggregation, access filter, distributed system infrastructures, distributed Internet information sources, programmable method, ad-hoc intermediary applications, Distributed databases, distributed databases, authorisation, Information filters, software engineering, application rewriting, sentinel process, data viewing, legacy applications, data manipulation, local file, Explosions, Application software, Computer science, access control constraints, Microsoft Windows NT, Aggregates, remote sources, integrated software, multiple information source encapsulation, file organisation, Internet, active files, file-based interface, transparent file integration]
An open real-time environment for parallel and distributed systems
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Most computer-based systems have hard real-time constraints. Schedulers in complex systems must be designed to manage a set of applications developed and deployed independently. We study an open real-time environment architecture for distributed systems where real-time applications may run concurrently with non-real-time applications. The architecture uses a two-level scheduling scheme. Each application is assigned a sporadic server to schedule the processes in the application. All sporadic servers are then scheduled by a system-wide fixed priority scheduler. Using the proposed open environment architecture, all hard real-time applications are guaranteed to have their reserved CPU utilization in order to meet all their deadlines. The guarantee is independent of the behaviors of all other applications in the same system. We present the schedulability analysis methods on systems with or without shared memory.
[Real time systems, open systems, open real-time environment, distributed processing, hard real-time constraints, Production facilities, two-level scheduling scheme, Distributed computing, parallel processing, sporadic server, Concurrent computing, Computer architecture, Automatic control, scheduling, shared memory systems, distributed systems, Kernel, Application software, system-wide fixed priority scheduler, Computer science, parallel systems, Processor scheduling, CPU utilization, schedulability analysis, real-time systems, shared memory]
An adaptive, perception-driven error spreading scheme in continuous media streaming
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
For transmission of continuous media (CM) streams such as audio and video over the Internet, a critical issue is that periodic network overloads cause bursty packet losses. Studies on human perception of audio and video streams have shown that bursty losses have the most annoying affects. Hence, addressing this issue is critical for multimedia applications such as Internet telephony, videoconferencing, distance learning, etc. Classical error handling schemes like retransmission and forward error recovery have the undesirable effects of (a) introducing timing variations, which is unacceptable for isochronous traffic, and (b) using up valuable bandwidth, potentially exacerbating the problem. This paper introduces a new concept called error spreading, which is a transformation technique that takes an input sequence of packets (from an audio or video stream) and permutes them before transmission. The packets are then un-permuted at the receiver before delivery to the application. The purpose is to spread out bursty network errors, in order to achieve better perceptual quality of the transmitted stream. Analysis has been done to determine the provable lower bound on bursty errors tolerable by this class of protocols. An algorithm to generate the optimal permutation for a given network loss rate is presented. While our previous work had focused on streams with no inter-frame dependencies, e.g. MJPEG encoded video, in this paper the technique is generalized to streams with inter-frame dependencies, e.g. MPEG encoded video.
[Protocols, transformation technique, multimedia applications, inter-frame dependencies, Humans, multimedia systems, video, continuous media streaming, adaptive perception-driven error spreading scheme, error spreading, Bandwidth, Propagation losses, audio, IP networks, videoconferencing, video signal processing, periodic network overloads, optimal permutation, input packet sequence, network loss rate, quality of service, distance learning, bursty packet losses, Teleconferencing, Computer aided instruction, audio signal processing, Streaming media, MPEG encoded video, Timing, Internet, perceptual quality, Internet telephony, errors]
Atomic broadcast in asynchronous crash-recovery distributed systems
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Atomic broadcast is a fundamental problem of distributed systems: it states that messages must be delivered in the same order to their destination processes. This paper describes a solution to this problem in asynchronous distributed systems in which processes can crash and recover. A consensus-based solution to atomic broadcast problem has been designed by Chandra and Toueg (1996) for asynchronous distributed systems where crashed processes do nor recover. Although our solution is based on different algorithmic principles, it follows the same approach: it transforms any consensus protocol suited to the crash-recovery model into an atomic broadcast protocol suited to the same model. We show that atomic broadcast can be implemented without requiring any additional log operations in excess of those required by the consensus. The paper also discusses how additional log operations can improve the protocol in terms of faster recovery and better throughput.
[Protocols, Identity-based encryption, consensus-based solution, distributed processing, Throughput, Computer crashes, system recovery, atomic broadcast, Microwave integrated circuits, Fault tolerance, Fault tolerant systems, crash-recovery model, Ash, message delivery, Broadcasting, log operations, fault tolerant computing, algorithmic principles, asynchronous crash-recovery distributed systems]
Aggregate TCP congestion control using multiple network probing
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Extensive research in TCP's congestion control mechanism has resulted in an effective algorithm that gives fairly precise estimates on the available bandwidth on a given network path (J. Postel, 1981; V. Jacobson, 1988). However, most past efforts focused on enhancing the accuracy and robustness of the path bandwidth estimation algorithms for individual TCP connections. Relatively fewer attempts have been made to further improve data transport efficiency by sharing path bandwidth information among concurrent TCP connections with the same sources and destinations. The paper proposes an aggregate TCP-based congestion control algorithm (ATCP) that allows individual TCP connections to reach their fair shares of the available network path bandwidth more quickly, while still observing TCP's congestion control semantics. In addition, the proposed algorithm is guaranteed to perform no worse than current TCP congestion control algorithm in all cases, and is designed to be implemented in a way that is completely transparent to both ends of a TCP connection. ATCP is particularly useful for TCP connections that are short-lived and yet have a long round-trip delay, such as Web page transfers using HTTP 1.0. Our trace-driven simulation study shows that the aggregate congestion control algorithm can reduce the normalized transaction latency by a factor of up to 2, compared to standard TCP.
[multiple network probing, normalized transaction latency, telecommunication congestion control, Throughput, path bandwidth information, network path, telecommunication computing, Delay, aggregate TCP congestion control, network path bandwidth, Bandwidth, TCPIP, individual TCP connections, round-trip delay, State estimation, Probes, trace-driven simulation study, Web page transfers, Transmission Control Protocol, aggregate TCP-based congestion control algorithm, data transport efficiency, Computer science, bandwidth allocation, concurrent TCP connections, Aggregates, transport protocols, Web pages, path bandwidth estimation algorithms, Timing, congestion control semantics, HTTP 1, ATCP]
Popularity-aware greedy dual-size Web proxy caching algorithms
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Web caching aims at reducing network traffic, server load and user-perceived retrieval delays by replicating popular content on proxy caches that are strategically placed within the network. While key to effective cache utilization, popularity information (e.g. relative access frequencies of objects requested through a proxy) is seldom incorporated directly in cache replacement algorithms. Rather other properties of the request stream (e.g. temporal locality and content size), which are easier to capture in an online fashion, are used to indirectly infer popularity information, and hence drive cache replacement policies. Recent studies suggest that the correlation between these secondary properties and popularity is weakening due in part to the prevalence of efficient client and proxy caches. This trend points to the need for proxy cache replacement algorithms that directly capture popularity information. We present an on-line algorithm that effectively captures and maintains an accurate popularity profile of Web objects requested through a caching proxy. We propose a novel cache replacement policy that uses such information to generalize the well-known greedy dual-size algorithm, and show the superiority of our proposed algorithm by comparing it to a host of recently-proposed and widely-used algorithms using extensive trace-driven simulations and a variety of performance metrics.
[Measurement, Content based retrieval, cache replacement algorithms, Telecommunication traffic, cache storage, Delay, Network servers, Traffic control, popularity-aware Web proxy caching, cache utilization, software performance evaluation, information resources, client-server systems, greedy dual-size Web proxy caching, online algorithm, Read-write memory, server load, trace-driven simulation, Computer science, Bridges, network traffic, client cache, proxy cache replacement, performance metrics, user-perceived retrieval delays, Frequency, Internet, software metrics]
A relaxed mutual exclusion problem with application to channel allocation in mobile cellular networks
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Distributed channel allocation is a fundamental resource management problem in mobile cellular networks. It has a flavor of distributed mutual exclusion but is not exactly a mutual exclusion problem (because a channel may be reused in different cells). However it is still not clear what is the relationship between the two problems. We establish the exact relationship between the two. Specifically, we introduce the problem of relaxed mutual exclusion to model the problem of distributed channel allocation. We develop a general algorithm that guarantees relaxed mutual exclusion for a single resource, prove a necessary and sufficient condition for the information structure, and address the issues that arise in relaxed mutual exclusion, including deadlock resolution, dealing with multiple resources, and design of efficient information structure.
[resource management, Base stations, mobile cellular networks, Interference, distributed mutual exclusion, relaxed mutual exclusion problem, Electronic switching systems, Application software, Hip, deadlock resolution, Intelligent networks, Information science, distributed channel allocation, mobile communication, resource allocation, Land mobile radio cellular systems, Channel allocation, Resource management, channel allocation, protocols, information structure]
MobileSpaces: a framework for building adaptive distributed applications using a hierarchical mobile agent system
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
This paper presents a new framework for constructing mobile agents. The framework introduces the notion of agent hierarchy and inter-agent migration and thus allows a group of mobile agents to be dynamically assembled into a single mobile agent. It provides a powerful method to construct a distributed application, in particular a large-scale mobile application. To demonstrate how to exploit our framework, we construct an extensible and portable mobile agent system based on the framework. The system is implemented as a collection of mobile agents and thus can dynamically change and evolve its functions by migrating agents that offer the functions. Also, mobile agent-based applications running on the system can naturally inherit the extensibility and adaptability of the system.
[Buildings, large-scale mobile application, Programming, Control systems, hierarchical mobile agent system, Application software, adaptive distributed application building, software agents, adaptive systems, extensible mobile agent system, portable mobile agent system, Mobile agents, Robustness, MobileSpaces, Large-scale systems, Personal digital assistants, agent hierarchy, Assembly, Mobile computing, distributed programming, inter-agent migration]
Performance analysis of dynamic location updation strategies for mobile users
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
This paper focuses on the important issue of strategies for tracking users in a mobile communication network. Since the users are mobile, the network must keep track of where the users are, a prerequisite for routing calls to the users. We assume that the mobile users periodically inform the network of their present location. Now, it is very important to decide an efficient location updation strategy that will optimize the overhead in terms of usage of radio spectrum and power, as well as the search cost for a mobile when a call is to be set up. Static location updation strategies often result in poor performance. Three dynamic location updation strategies have been considered, namely, the time-based strategy, the movement-based strategy and the distance-based strategy. A memoryless movement pattern on a two-dimensional mesh topology of cells has been studied. Analytical models have been developed to compare the performance of the strategies. Simulation studies also have been made for performance evaluation.
[mobile communication network, movement-based strategy, simulation, Mobile communication, user tracking, Databases, Cost function, search cost, Computer networks, Performance analysis, mobile users, time-based strategy, performance evaluation, Routing, Microelectronics, distance-based strategy, Cellular networks, two-dimensional mesh topology, mobile communication, radio spectrum, call routing, memoryless movement pattern, dynamic location updation strategies, Personal communication networks, wireless LAN, Mobile computing, performance analysis]
Precluding useless events for on-line global predicate detections
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Detecting global predicates is an important task in testing and debugging distributed programs. In this paper, we propose an approach that effectively precludes useless events for global predicate detection, facilitating the process of an independent online checking routine. To identify more useless events than a simple causality-check method can do, our method tracks and maintains the precedence information of event intervals as a graph. To reduce the potentially expensive space and time costs as the graph expands, we propose an effective scheme to prune the graph. The performance of our method is analyzed and evaluated by simulations. The result shows that our approach outperforms conventional approaches in terms of the number of useless events found.
[program debugging, Costs, Event detection, program testing, graph theory, distributed program debugging, Electronic switching systems, precedence information, distributed program testing, online global predicate detection, event intervals, distributed programming, software performance evaluation, Testing, graph pruning, useless events, Debugging, performance evaluation, Explosions, causality check, simulations, Computer science, time cost, space cost, online operation, Communication channels, independent online checking routine, graph expansion, Clocks]
Scalable QoS guaranteed communication services for real-time applications
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
We propose an approach to flow-unaware admission control which is a combination with an aggregate packet forwarding scheme, improving scalability of networks while guaranteeing end-to-end deadlines for real-time applications. We achieve this by using an off-line delay computation and verification step, which allows to reduce the overhead at admission control while keeping admission probability and resource utilization high. Our evaluation data show that our system's admission probabilities are very close to those of significantly more expensive flow-aware approaches. At the same time, the admission control overhead during flow establishment is very low. Our results therefore support the claim from the DS architecture literature that scalability can be achieved through flow aggregation without sacrificing resource utilization and with significant reduction in run time overhead.
[internetworking, Delay, Cost accounting, Read only memory, scalability, real-time applications, run time overhead, Web and internet services, QoS guaranteed communication services, scheduling, resource utilization, probability, quality of service, offline delay computation, Application software, flow-unaware admission control, Computer science, aggregate packet forwarding, Aggregates, Admission control, real-time systems, end-to-end deadlines, Intserv networks, Resource management, admission probability]
Adaptive data delivery in wireless communication environments
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
The combination of broadcast and on-demand data delivery services is an economic way to build a highly scalable wireless information system with limited bandwidth. The use of data broadcasting should be adaptive so that the system response time can always be minimised. A traditional approach requires the development of a system response time equation in order to find the optimal solution. However, obtaining such an equation is not always possible. We observe that by maintaining a certain level of on-demand request arrival rate, a close approximation to the optimal solution can be obtained. Using this approach, a real-time adaptive data delivery algorithm is developed. Our algorithm does not require the access information of the data items to be known exactly, which is needed normally for this kind of optimization problems. A simple and low overhead bit vector mechanism is able to capture the relative popularities of the data items. With this information, our algorithm can give a performance comparable to the ideal case in which the access information for each data item is known exactly.
[broadcast, bit vector mechanism, wireless communication environments, mobile radio, limited bandwidth, adaptive data delivery, on-demand request arrival rate, on-demand data delivery services, highly scalable wireless information system, Wireless communication, system response time equation, mobile computing, data broadcasting, real-time systems, data communication, real-time adaptive data delivery algorithm, broadcast channels, optimization problems]
Quartz: a QoS architecture for open systems
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
This paper describes an architecture that provides support for quality of service (QoS) specification and enforcement in heterogeneous distributed computing systems. The Quartz QoS architecture has been designed to overcome various limitations of previous QoS architectures that have constrained their use in heterogeneous systems. These limitations include dependencies on specific platforms and the fact that their functionality is often limited by design to one particular area of application. Quartz is able to accommodate differences among diverse computing platforms and areas of application by adopting a flexible and extensible platform-independent design, which allows its internal components to be rearranged dynamically in order to adapt the architecture to the surrounding environment. Further significant problems found in other QoS architectures, such as the lack of flexibility and expressiveness in the specification of QoS requirements and limited support for resource adaptation, are also addressed by Quartz. This paper describes the motivations for and design of Quartz in detail, presents a prototype implementation of Quartz and an analysis of its design based on experience with a number of applications that use this prototype.
[platform-independent design, QoS architecture, Protocols, Identity-based encryption, open systems, computer networks, Quality of service, Ice, quality of service, Application software, Quartz, Middleware, Distributed computing, quality of service architecture, Prototypes, Open systems, Computer architecture, heterogeneous distributed computing systems, resource adaptation]
Understanding replication in databases and distributed systems
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Replication is an area of interest to both distributed systems and databases. The solutions developed from these two perspectives are conceptually similar but differ in many aspects: model, assumptions, mechanisms, guarantees provided, and implementation. In this paper, we provide an abstract and "neutral" framework to compare replication techniques from both communities. The framework has been designed to emphasize the role played by different mechanisms and to facilitate comparisons. The paper describes the replication techniques used in both communities, compares them, and points out ways in which they can be integrated to arrive to better, more robust replication protocols.
[replication, databases, Protocols, replicated databases, replication protocols, Laboratories, Mechanical factors, Transaction databases, Information systems, Fault tolerant systems, Distributed databases, Broadcasting, distributed systems, Space exploration, protocols, Contracts]
On low-cost error containment and recovery methods for guarded software upgrading
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
To assure dependable onboard evolution, we have developed a methodology called guarded software upgrading (GSU). We focus on a low-cost approach to error containment and recovery for GSU. To ensure low development cost, we exploit inherent system resource redundancies as the fault tolerance means. In order to mitigate the effect of residual software faults at low performance cost, we take a crucial step in devising error containment and recovery methods by introducing the confidence-driven notion. This notion complements the message-driven (or communication-induced) approach employed by a number of existing checkpointing protocols for tolerating hardware faults. In particular, we discriminate between the individual software components with respect to our confidence in their reliability and keep track of changes of our confidence (due to knowledge about potential process state contamination) in particular processes. This, in turn, enables the individual processes in the spaceborne distributed system to make decisions locally at run-time, on whether to establish a checkpoint upon message passing and whether to roll back or roll forward during error recovery. The resulting message-driven confidence-driven approach enables cost-effective checkpointing and cascading-rollback free recovery.
[Checkpointing, system recovery methods, checkpointing, Costs, Protocols, spaceborne distributed system, software reliability, Software performance, system recovery, error containment, Runtime, error recovery, Fault tolerant systems, residual software faults, message-driven approach, aerospace computing, spaceborne computing systems, Hardware, cascading-rollback free recovery, protocols, software components, software performance evaluation, system resource redundancies, message passing, fault tolerance, Redundancy, low-cost error containment, run-time, software maintenance, Contamination, software fault tolerance, performance, Message passing, confidence-driven notion, guarded software upgrading]
Design and analysis of clusters with single I/O space
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Support of single system image (SSI) services is the main approach that enables better utilization of PC/workstation clusters. Some SSI services can be easily built with the support of other low-level, elementary, SSI services. In this paper, we describe a single I/O space architecture for achieving a SSI at the I/O subsystem level. Furthermore, we demonstrate how the single I/O space can facilitate the development of other key SSI services. Typical SSI services which can benefit from the single I/O space include single file hierarchy, single memory space, checkpointing systems and single process space with process migration facilities. Benchmark performance results show that our design achieves both performance and storage size scalabilities that are essential to building I/O-intensive clusters.
[Checkpointing, workstation clusters, single file hierarchy, Scalability, benchmark performance, device drivers, Distributed computing, single I/O space architecture, Concurrent computing, File systems, I/O-intensive clusters, Computer architecture, Parallel processing, Workstations, single process space, Availability, checkpointing systems, single system image services, PC clusters, Buildings, performance evaluation, process migration facilities, performance scalability, single memory space, storage size scalability]
An efficient cache maintenance scheme for mobile environment
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
We present a new cache maintenance scheme, called AS, suitable for the wireless mobile environment. Our scheme integrates the mobility management scheme of Mobile IP with cache maintenance scheme used in the CODA file system. As opposed to broadcasting invalidation report schemes, AS supports arbitrary disconnection patterns and uses less wireless bandwidth. We present analytical and simulation results to show the superiority of our caching scheme.
[CODA file system, Protocols, Mobile IP, mobile environment, File servers, Electronic switching systems, cache storage, Mobile radio mobility management, Chemical technology, disconnection patterns, Computer science, Network servers, mobile computing, AS, File systems, transport protocols, Bandwidth, Broadcasting, mobility management scheme, fault tolerant computing, Internet, cache maintenance scheme, wireless mobile environment]
Scheduling heuristics for data requests in an oversubscribed network with priorities and deadlines
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Providing up-to-date input to users' applications is an important data management problem for a distributed computing environment, where each data storage location and intermediate node may have specific data available, storage limitations, and communication links available. Sites in the network request data items and each request has an associated deadline and priority. This work concentrates on solving a basic version of the data staging problem in which all parameter values for the communication system and the data request information represent the best known information collected so far and stay fixed throughout the scheduling process. The network is assumed to be oversubscribed and not all requests for data items can be satisfied. A mathematical model for the basic data staging problem is given. Then, three multiple-source shortest-path algorithm based heuristics for finding a near-optimal schedule of the communication steps for staging the data are presented. Each heuristic can be used with each of four cost criteria developed. Thus, twelve implementations are examined. In addition, two different weightings for the relative importance of different priority levels are considered. The performance of the proposed heuristics is evaluated and compared by simulations. The proposed heuristics are shown to perform well with respect to upper and lower bounds. Furthermore, the heuristics and a complex cost criterion allow more highest priority messages to be received than a simple-cost-based heuristic that schedules all highest priority messages first.
[Costs, Military computing, data management, data staging problem, Satellite broadcasting, simulation, distributed processing, communication links, Mathematics, scheduling heuristics, Environmental management, deadlines, distributed computing environment, processor scheduling, cost criteria, near-optimal schedule, Intelligent networks, priorities, data storage location, multiple-source shortest-path algorithm, Web server, Contracts, military computing, computer networks, up-to-date input, data requests, Application software, Couplings, virtual machines, oversubscribed network, storage limitations, mathematical model, intermediate node]
An asymptotically multi-layered decentralized consensus protocol with an initiator
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
A decentralized consensus protocol refers to a process for all nodes in a distributed system to collect the information/status from every other node and reach a consensus among them. Two classes of decentralized consensus protocols have been studied before: the one without an initiator and the one with an initiator. While the one without an initiator has been well studied in the literature: it is noted that prior protocols with an initiator mainly relied upon the one without an initiator and thus did not fully exploit the intrinsic properties of having an initiator. By exploiting the concept of multi-layered execution, we develop in this paper an efficient multi-layered decentralized consensus protocol for a distributed system with an initiator. By adapting itself to the number of nodes in the system, the proposed protocol can determine a proper layer for execution and reach the consensus in the minimal numbers of message steps while incurring a much smaller number of messages than required by prior works. It is shown that the decentralized consensus protocols developed in this paper for the case of having an initiator significantly outperform prior schemes.
[Protocols, message steps, multi-layered execution, initiator, distributed processing, distributed system, asymptotically multi-layered decentralized consensus protocol, protocols]
System mechanisms for partial rollback of mobile agent execution
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Mobile agent technology has been proposed for various fault-sensitive application areas, including electronic commerce, systems management and active messaging. Recently proposed protocols providing the exactly-once execution of mobile agents allow the usage of mobile agents in these application areas. Based on these protocols, a mechanism for the application-initiated partial rollback of the agent execution is presented. The rollback mechanism uses compensation operations to roll back the effects of the agent execution on the resources and uses a mixture of physical logging and compensation operations to roll back the state of the agent. The introduction of different types of compensation operations allows performance improvements during the agent rollback.
[compensation operations, electronic messaging, application-initiated partial rollback, partial rollback, Security, Electronic commerce, mobile agent technology, Reactive power, Technology management, mobile computing, Mobile agents, systems management, Permission, Logic, protocols, Business, electronic commerce, rollback mechanism, object-oriented programming, exactly-once execution, Access protocols, physical logging, system mechanisms, fault-sensitive application areas, agent rollback, software agents, mobile agent execution, active messaging, Resource management]
Detecting and representing relevant Web deltas using Web join
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
We show how to detect and represent Web deltas, i.e., changes in Web information, that are relevant to a user's query in the context of our Web warehousing system called WHOWEDA (Warehouse of Web Data). In WHOWEDA, Web information are materialized views stored in Web tables and can be manipulated and analyzed using a set of Web algebraic operators. We present a mechanism to detect relevant Web deltas using Web join and outer Web join. We show how to represent these changes using delta Web tables.
[information resources, Web algebraic operators, Web warehousing system, Cardiac disease, information retrieval, outer Web join, Cancer drugs, Information systems, Computer science, query processing, Web deltas, delta Web tables, Databases, Acquired immune deficiency syndrome, Warehousing, WHOWEDA, Web pages, materialized views, Diabetes, Internet, Probes, data warehouses, Web join]
Multimedia service configuration and reservation in heterogeneous environments
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Widely deployed multimedia services are expected to accommodate clients in a highly heterogeneous environment. Clients of a multimedia service may vary greatly in processing and communication capabilities. In addition, due to workload, location, and service time differences, the availability of end-to-end resources between a client and a server may also vary. Current solutions tend to focus on either the qualitative heterogeneity (in client and resource types) or the quantitative heterogeneity (in resource availability) problem. We present a framework for dynamic end-to-end multimedia service configuration and reservation: an integrated solution to both aspects of the heterogeneity problem. Service configuration is responsible for choosing appropriate service components to compose a customized service deliver to each client; while service reservation is responsible for reserving the end-to-end resources in a coordinated manner and providing the best possible quality within the chosen configuration. We have implemented a prototype of this framework as part of the 2K operating system and tested it by building a proof-of-concept video streaming service on top of it. Our experiments show the soundness of this framework.
[Availability, System testing, client-server systems, Engineering profession, wide area networks, heterogeneous environments, service time, multimedia service reservation, Digital cameras, multimedia service configuration, Computer science, multimedia services, video streaming service, Operating systems, Surveillance, Prototypes, end-to-end resources, Streaming media, heterogeneous environment, multimedia service, service reservation, Page description languages, dynamic end-to-end multimedia service configuration, multimedia communication]
Secure group communication in asynchronous networks with failures: integration and experiments
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
The increasing popularity and diversity of collaborative applications prompts a need for highly secure and reliable communication platforms for dynamic peer groups. Security mechanisms for such groups tend to be both expensive and complex and their integration with reliable group communication services presents a formidable challenge, This paper discusses some important integration issues, reports on our implementation experience and provides experimental results. Our approach utilizes distributed group key management developed by the Cliques project. We enhance it to handle processor and network faults (under a fail-stop or crash-and-recover model) and asynchronous membership events (such as joins, leaves, merges and network partitions). Our approach leverages the strong properties provided by the Spread group communication system, such as message ordering, clean failure semantics and a membership service. The result of this work is a secure group communications layer and an API that provide the application programmer with both standard group communication services and flexible security services.
[application program interfaces, collaborative applications, secure group communications layer, clean failure semantics, Spread group communication system, Electrical capacitance tomography, Tellurium, Intelligent networks, Cliques project, groupware, distributed group key management, message ordering, National security, Marine vehicles, processor faults, network faults, secure group communication, failures, computer networks, security mechanisms, dynamic peer groups, Application software, Hip, Programming profession, membership service, Computer science, security of data, Collaboration, fault tolerant computing, API, reliable communication platforms, asynchronous networks]
Computing global functions in asynchronous distributed systems prone to process crashes
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Global data is a vector with one entry per process. Each entry must be filled with an appropriate value provided by the corresponding process. Several distributed computing problems amount to compute a function on global data. This paper proposes a protocol to solve such problems in the context of asynchronous distributed systems where processes may fail by crashing. The main problem that has to be solved lies in computing the global data and in providing each non-crashed process with a copy of it, despite the possible crash of some processes. To be consistent, the global data must contain (at least) all the values provided by the processes that do not crash. This defines the global data computation (GDC) problem. To solve this problem, processes execute a sequence of asynchronous rounds during which they construct (in a decentralized way) the value of the global data, and eventually each process gets a copy of it. To cope with process crashes, the protocol uses a perfect failure detector. The proposed protocol has been designed to be time-efficient. It allows early decisions. Let t be the maximum number of processes that may crash (t<n where n is the total number of processes) and f be the actual number of process crashes (f/spl les/t). In the worst case, the protocol terminates in min(2f+2,t+1) rounds. Moreover the protocol does not require processes to exchange information on their perception of crashes. The message size depends only on the size of the global data.
[Protocols, message size, distributed processing, time-efficient, Computer crashes, Electrical capacitance tomography, asynchronous distributed systems, Distributed computing, system recovery, software fault tolerance, global function computing, protocol, global data computation, perfect failure detector, Voting, Detectors, Ash, process crashes, vector, protocols, global data]
Contract type sequencing for reallocative negotiation
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
The capability to reallocate items-e.g. tasks, securities, bandwidth slices, MW hours of electricity, and collectibles-is a key feature in automated negotiation. Especially when agents have preferences over combinations of items, this is highly nontrivial. Marginal cost based reallocation leads to an anytime algorithm where every agent's utility increases monotonically over time. Different contract types head toward different locally optimal task allocations, and contracts from a recently introduced comprehensive contract type, OCSM-contracts, head toward the global optimum. Reaching it can take an impractically long time, so it is important to trade off solution quality against negotiation time. To construct negotiation protocols that lead to the best achievable allocations in a bounded amount of time, we compared sequences of four contract types. Original, cluster, swap, and multiagent contracts. The experiments show that it is profitable to use multiple contract types in the sequence: significantly better solutions are reached, and faster, than if only one contract type is used. However, the best sequences only include original and cluster contracts. Swap and multiagent contracts lead to bad local optima quickly. Interestingly, the number of contracts using any given contract type does not always decrease over time: contracts play the role of enabling further contracts.
[Protocols, multi-agent systems, Engineering profession, Transportation, negotiation time, contract type sequencing, solution quality, Electronic commerce, contracts, multiagent contracts, negotiation support systems, Computer science, cluster contracts, automated negotiation, swap contracts, Bandwidth, reallocative negotiation, Cost function, Electricity supply industry, Computer security, Contracts, original contracts]
Mobile Cache Protocol: a dynamic object relocation protocol for wide area networks
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
This paper proposes a relocation protocol of a replicated object in a widely distributed object system called Mobile Cache Protocol (MCP). MCP has four basic features: it creates a replicated object and allocates it on an intermediate site to reduce the latency of object retrieval in a wide area network; it supports both client and server oriented updates to achieve object consistency; it supports reservation of replication of an object; and several clients can share a replicated object to enhance storage efficiency. The protocol is summarized and implementation and various experiments of the protocol are described. Though it is designed for a distributed object based system, MCP also can be applied to dynamic Web caching.
[Wide area networks, information resources, client-server systems, Protocols, wide area networks, replicated databases, memory protocols, cache storage, distributed object system, Mobile Cache Protocol, dynamic Web caching, dynamic object relocation protocol, client server systems, replicated object, object retrieval, object consistency, MCP, distributed programming, distributed object management]
MPEG-4 support to multiuser virtual environments
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
The few existing virtual reality distributed systems integrated to multimedia suffer from technical limitations on the number of users supported and on the presentation quality of the scenes. These limitations are due, among other things, to the slow progress of the VRML language to deal with multimedia integration. Today, the MPEG-4 standard is one of the most attractive alternative technologies to the development of multiuser virtual environments integrated to multimedia. The paper evaluates the current status of multimedia integration to virtual environments in the WWW and proposes a tool called SVRT-MM (Shared Virtual Reality Tool Integrated with Multimedia) that extends an existing MPEG-4 player to support multiuser. The article intends to be a contribution to the ongoing developments of the MPEG-4 standard related to multiuser support in 3D environments.
[virtual reality, multimedia integration, Avatars, MPEG-4 support, virtual reality distributed systems, Shared Virtual Reality Tool Integrated with Multimedia, multimedia systems, World Wide Web, 3D environments, MPEG 4 Standard, technical limitations, Virtual reality, SVRT-MM, multiuser virtual environments, multi-access systems, Large-scale systems, Standards development, VRML language, information resources, Virtual environment, MPEG-4 player, telecommunication standards, Computer science, multiuser support, Layout, WWW, Web sites, presentation quality, MPEG-4 standard]
Static and adaptive data replication algorithms for fast information access in large distributed systems
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Creating replicas of frequently accessed objects across a read-intensive network can result in large bandwidth savings which, in turn, can lead to reduction in user response time. On the contrary, data replication in the presence of writes incurs extra cost due to multiple updates. The set of sites at which an object is replicated constitutes its replication scheme. Finding an optimal replication scheme that minimizes the amount of network traffic given read and write frequencies for various objects, is NP-complete in general. We propose two heuristics to deal with this problem for static read and write patterns. The first is a simple and fast greedy heuristic that yields good solutions when the system is predominantly read-oriented. The second is a genetic algorithm that through an efficient exploration of the solution space provides better solutions for cases where the greedy heuristic does not perform well. We also propose an extended genetic algorithm that rapidly adapts to the dynamically changing characteristics such as the frequency of reads and writes for particular objects.
[Costs, Communication system control, fast information access, NP-complete, static data replication algorithms, read-intensive network, Constraint optimization, genetic algorithm, heuristic programming, heuristics, Broadcasting, user response time, Marine vehicles, replicated databases, bandwidth, genetic algorithms, large distributed systems, Equations, frequently accessed object replicas, database theory, greedy heuristic, Computer science, adaptive data replication algorithms, network traffic, read patterns, distributed algorithms, data replication, Writing, write patterns]
Scheduling using genetic algorithms
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Considers the scheduling of mixed task- and data-parallel modules comprising computation and communication operations. The program generation starts with a specification of the maximum degree of task- and data-parallelism of the method to be implemented. In several derivation steps, the degree of parallelism is adapted to a specific distributed memory machine. We present a scheduling derivation step based on the genetic algorithm paradigm. The scheduling takes not only decisions on the execution order (independent modules can be executed consecutively by all processors available or concurrently by independent groups of processors) but also on appropriate data distributions and task implementation versions. We demonstrate the efficiency of the algorithm by an example from numerical analysis.
[execution order, Costs, Scientific computing, mathematics computing, parallel programming, Genetic algorithms, distributed memory machine, Runtime, numerical analysis, Parallel processing, scheduling, computation operations, data-parallelism, efficiency, task-parallelism, consecutive execution, parallel program module scheduling, concurrent execution, Parallel machines, genetic algorithms, Scheduling algorithm, program generation, communication operations, Processor scheduling, Numerical analysis, data distributions, task implementation versions, distributed memory systems, Signal processing, subroutines]
HydraNet-FT: network support for dependable services
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
With the Internet increasingly being used as the access medium for a variety of critical services, there is a growing need to provide fault-tolerant (FT) services over internetworks, in a completely client-transparent fashion. We present HydraNet-FT, an infrastructure to dynamically replicate services across an internetwork and have the replicas provide a single fault-tolerant service access point to clients. HydraNet-FT uses the TCP communication protocol with a few modifications on the server side to allow one-to-many message delivery from a client to service replicas and many-to-one message delivery from the replicas to the client. A communication channel between the replicas provides atomicity and message ordering. A low-latency failure estimator is used to detect failures of servers in the system and to initiate fail-over mechanisms. An implementation and measurements on a local testbed show that the overhead of our scheme is reasonably small.
[Protocols, HydraNet-FT, internetworking, server failure detection, Multimedia communication, Read only memory, Fault tolerance, Network servers, internetworks, Web and internet services, client-transparent service provision, network support, Broadcasting, many-to-one message delivery, message ordering, computer network reliability, Web server, one-to-many message delivery, replica communication channel, Availability, client-server systems, fault tolerance, dynamic service replication, atomicity, overhead, service access point, TCP communication protocol, server-side modifications, low-latency failure estimator, Content addressable storage, fail-over mechanism initiation, transport protocols, service replicas, Internet, dependable services, fault-tolerant services]
Task assignment with unknown duration
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
We consider a distributed server system and ask which policy should be used for assigning tasks to hosts. In our server tasks are not preemptible. Also, the task's service demand is not known a priori. We are particularly concerned with the case where the workload is heavy-tailed, as is characteristic of many empirically measured computer workloads. We analyze several natural task assignment policies and propose a new one TAGS (Task Assignment based on Guessing Size). The TAGS algorithm is counterintuitive in many respects, including load unbalancing, non-work-conserving and fairness. We find that under heavy-tailed workloads, TAGS can outperform all task assignment policies known to us by several orders of magnitude with respect to both mean response time and mean slowdown, provided the system load is not too high.
[fairness, client-server systems, Costs, service demand, Technical Activities Guide -TAG, Independent component analysis, Switches, distributed server system, mean slowdown, Delay, Counting circuits, unknown duration task assignment, computer workloads, Computer science, mean response time, non-work conserving, resource allocation, load unbalancing, heavy-tailed workload, Tail, Particle measurements, Task Assignment based on Guessing Size, TAGS algorithm, software performance evaluation]
Deadlock analysis of client/server programs
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Deadlocks are a common type of fault in distributed programs. To detect deadlocks in a distributed program P, one approach is to construct the reachability graph (RG) of P, which contains all possible states of P, and analyze the RG to detect deadlocks. Since the size of RG(P) is an exponential function of the number of processes in P, the use of RG for deadlock detection has limited success. In this paper, we show an efficient technique for deadlock analysis of client/server programs. We present a theory of deadlock analysis of client/server LTS systems, in which a server or client is represented as a labeled transition system (LTS). For a client/server LTS system, we define its client/server reachability graph (CSRG), which has its size being a polynomial function of the number of clients. We show that the use of CSRG not only significantly reduces the effort for deadlock analysis but also provides a basis for proving freedom from deadlock for any number of clients.
[client-server systems, reachability analysis, polynomial function, Resumes, labeled transition system, fault, Roentgenium, Reachability analysis, system recovery, client/server programs, reachability graph, Computer science, deadlock analysis, Runtime, System recovery, Polynomials, distributed program, distributed programming, Testing]
Highly concurrent shared storage
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Switched system-area networks enable thousands of storage devices to be shared and directly accessed by end hosts, promising databases and file systems highly scalable, reliable storage. In such systems, hosts perform access tasks (read and write) and management tasks (storage migration and reconstruction of data on failed devices.) Each task translates into multiple phases of low-level device I/Os, so that concurrent host tasks accessing shared devices can corrupt redundancy codes and cause hosts to read inconsistent data. Concurrent control protocols that scale to large system sizes are required in order to coordinate on-line storage management and access tasks. In this paper we identify, the tasks that storage controllers must perform, and propose an approach which allows these tasks to be composed from basic operations-called base storage transactions (BSTs)-such that correctness requires only the serializability of the BSTs and not of the parent tasks. We present highly scalable distributed protocols which exploit storage technology trends and BST properties to achieve serializability while coming within a few percent of ideal performance.
[Protocols, Costs, Laboratories, highly scalable distributed protocols, on-line storage management, distributed processing, Throughput, input-output programs, storage management, Atomic layer deposition, Employment, concurrent host tasks, management tasks, serializability, switched system-area networks, highly scalable reliable storage, protocols, highly concurrent shared storage, databases, low-level device I/Os, Redundancy, storage controllers, Binary search trees, file systems, Concurrency control, concurrent control protocols, access tasks, storage devices, concurrency control, Writing, base storage transactions, end hosts]
Partitionable light-weight groups
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Group communication, providing virtual synchrony semantics, is a powerful paradigm for building distributed applications. For applications that require a large number of groups, significant performance gains can be attained if these groups share the resources required to provide virtual synchrony. A service that maps multiple user groups onto a small number of instances of a virtually synchronous implementation is called a Light-Weight Group Service. The paper describes the design of a light-weight group service able to operate in partitionable networks. Partitions pose challenges to the design of this service, in particular because inconsistent mapping decisions can be made when the system is partitioned. The paper focuses on the design of reconciliation mechanisms needed when a partition is healed.
[Availability, virtual synchrony semantics, Light-Weight Group Service, partitionable light-weight groups, Protocols, partitionable networks, reconciliation mechanisms, Electrical capacitance tomography, group communication, virtually synchronous implementation, distributed application development, performance gains, Databases, resource allocation, Collaboration, resource sharing, Detectors, Bandwidth, groupware, Collaborative work, multiple user groups, inconsistent mapping decisions, naming services, distributed programming]
On request forwarding for dynamic Web caching hierarchies
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
We propose a Web caching scheme, based on the caching neighborhood protocol, featuring dynamic caching hierarchies as its underlying infrastructure. Dynamic Web caching hierarchies consist of proxy servers building hierarchies on a per request basis, in contrast to static Web caching hierarchies that comprise proxy servers preconfigured into hierarchies. Concerns of overheads and efficiency in forwarding requests individually have driven conventional Web caching schemes to use static Web caching hierarchies. Nevertheless, we show that a Web caching scheme featuring dynamic caching hierarchies can be both efficient and effective in request forwarding.
[information resources, Protocols, network servers, request forwarding, dynamic Web caching hierarchies, cache storage, Computer numerical control, Delay, Guidelines, Information science, caching neighborhood protocol, proxy servers, protocols]
Boosting the performance of NOW-based shared memory multiprocessors through directory hints
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Directory hints help a node in a NOW-based shared memory multiprocessor to keep track of where valid copies of a memory block may reside. With this information the node can fetch the block directly from those nodes on a read miss. In this way the number of network transactions to serve the miss may be reduced and the expensive directory lookup operation may be removed from the critical path. We discuss the issues involved in implementing the directory hint scheme on a NOW-based shared memory multiprocessor and examine one such implementation, which employs a small and fast cache to store the hints. Our simulation results show that the directory hint scheme can effectively reduce the read stall time. Also its performance is very competitive compared with a more expensive implementation which uses a large level-three cache. A drawback of the scheme is that it will introduce extra network traffic. We believe that the state-of-the-art interconnection networks, such as those built upon the SCI Spider (Galles, 1996) and the Intel Cavallino (Carbonaro and Verhoorn, 1996) chips, provide the opportunity to make the directory hint scheme feasible even with the slower network such as the one built by Myrinet switches (N.J.B et al., 1996).
[workstation clusters, Art, Multiprocessor interconnection networks, simulation, directory hints, cache storage, interconnection networks, Delay, Read only memory, directory lookup operation, table lookup, network transactions, shared memory systems, read stall time, Workstations, workstation networks, cache, Memory architecture, performance evaluation, Boosting, shared memory multiprocessors, Myrinet switches, Hip, Computer science, network traffic, performance, memory block, Memory management, Intel Cavallino, NOW, telecommunication traffic, SCI Spider]
The effect of nogood learning in distributed constraint satisfaction
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
We present resolvent-based learning as a new nogood learning method for a distributed constraint satisfaction algorithm. This method is based on a look-back technique in constraint satisfaction algorithms and can efficiently make effective nogoods. We combine the method with the asynchronous weak-commitment search algorithm (AWC) and evaluate the performance of the resultant algorithm on distributed 3-coloring problems and distributed 3SAT problems. As a result, we found that the resolvent-based learning works well compared to previous learning methods for distributed constraint satisfaction algorithms. We also found that the AWC with the resolvent-based learning is able to find a solution with fewer cycles than the distributed breakout algorithm, which was known to be the most efficient algorithm (in terms of cycles) for solving distributed constraint satisfaction problems.
[Costs, Multiagent systems, multi-agent systems, Laboratories, asynchronous weak-commitment search algorithm, performance evaluation, problem solving, nogood learning, Learning systems, distributed breakout algorithm, resolvent-based learning, distributed 3SAT problems, distributed algorithms, look-back technique, distributed constraint satisfaction algorithm, distributed 3-coloring problems, Resource management, constraint handling, learning (artificial intelligence), Artificial intelligence, Distributed algorithms, search problems, software performance evaluation, Disk recording]
Agent migration between incompatible agent platforms
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Several general purpose agent platforms exist, for example, Voyager, Jade, and Grasshopper, each of which provides an environment for building and executing software agents. Unfortunately, the platforms are usually incompatible with each other. Thus, agents built for one platform cannot be used in another platform, nor can they interact with agents in other platforms. Some effort has been put into standardizing agent communication and migration in FIPA and in OMG, but these standards are not yet supported by most of the existing platforms. Therefore, we should find some other ways to allow interaction between agents in different platforms. We show that it is possible to make platform independent agents that are able to migrate between incompatible platforms. We also describe how messages can be delivered to agents in other platforms, and show how to build platform independent service agents that are used via method calls. The ideas have been tested in practice with Voyager, Jade, and Grasshopper platforms.
[application program interfaces, Voyager, Communication standards, platform independent agents, Operating systems, FIPA, agent migration, agent communication, Software agents, agent interaction, Testing, Identity-based encryption, message passing, object-oriented programming, Grasshopper, general purpose agent platforms, OMG, method calls, software agents, Programming profession, incompatible agent platforms, Computer science, standards, Computer languages, message delivery, service agents, Writing, Jade]
An asynchronous recovery scheme based on optimistic message logging for mobile computing systems
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
This paper presents an asynchronous recovery scheme to provide fault-tolerance for mobile computing systems. The proposed scheme is based on optimistic message logging, since the checkpointing-only schemes are not suitable for the mobile environment in which unreliable mobile hosts and fragile network connection may hinder any kind of coordination for checkpointing and recovery. Also, in order to reduce the overhead imposed on mobile hosts, mobile support stations take charge of logging and dependency tracking, and mobile hosts maintain only a small amount of information for mobility tracking. As a result, truly asynchronous recovery for mobile systems can be achieved with little overhead.
[Checkpointing, mobility tracking, message passing, fault-tolerance, unreliable mobile hosts, optimistic message logging, checkpointing-only schemes, Distributed computing, system recovery, asynchronous recovery scheme, Space stations, Computer science, mobile computing, Wireless networks, Fault tolerant systems, fragile network connection, dependency tracking, Bandwidth, logging, Computer networks, fault tolerant computing, wireless LAN, Frequency synchronization, Mobile computing]
Consistency requirements of distributed shared memory for Dijkstra's mutual exclusion algorithm
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
As is well known any algorithm correct in an asynchronous shared memory setting (physically shared memory) can be directly applied in distributed shared memory (DSM) systems provided that the latter guarantees strong consistency (atomic or sequential) of replicas. Generally however, in DSM systems, weaker consistency models (causal, processor, PRAM, etc.) are often considered to improve the performance. A weakening consistency model may however imply the incorrectness of the algorithm. So we face a consistency requirement problem, the problem of finding the weakest consistency model of DSM that is sufficient and necessary for algorithm correctness. We consider a reliable DSM environment, and present a complex consistency model comprising three elementary models: sequential consistency, coherence and PRAM consistency. This complex model is then applied to Dijsktra's (1965) algorithm for mutual exclusion of n processes, one of the first solutions to a fundamental problem in both centralised and distributed operating systems. In the resulting algorithm, coherence and PRAM consistency are associated with some write operations performed at shared memory locations. As concurrent execution of write operations with weaker consistency models is more efficient when compared to the execution of strongly consistent operations, the proposed solution reduces synchronisation delay (mutual exclusion overhead) and thereby increases system throughput. The presented model is proven to be sufficient for algorithm correctness. Moreover, the algorithm is shown to be optimal in the sense that further relaxation of any write operations semantics violates progress (liveness) or safety of the algorithm.
[sequential consistency, distributed shared memory, coherence, PRAM consistency, performance evaluation, synchronisation delay, Phase change random access memory, concurrency theory, write operations, Delay, consistency requirement problem, performance, Operating systems, Memory management, network operating systems, concurrency control, weakest consistency model, system throughput, distributed shared memory systems, fault tolerant computing, algorithm correctness, mutual exclusion algorithm, distributed operating systems]
Achieving per-flow weighted rate fairness in a core stateless network
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Corelite is a quality of service architecture that provides weighted max-min fairness for rate among flows in a network without maintaining any per-flow state in the core routers. There are three key mechanisms that work in concert to achieve the service model of Corelite: the introduction of markers in a packet flow by the edge routers to reflect the normalized rate of the flow; weighted fair marker feedback at the core routers upon incipient congestion detection; and linear increase/multiplicative decrease based rate adaptation of packet flows at the edge routers in response to marker feedback.
[State feedback, per-flow weighted rate fairness, packet switching, Quality of service, Electrical capacitance tomography, core stateless network, Intelligent networks, Fluid flow measurement, edge routers, Web and internet services, flow rate adaptation, incipient congestion detection, IP networks, Corelite, quality of service, Diffserv networks, quality of service architecture, telecommunication network routing, weighted fair marker feedback, Ear, packet flow, Intserv networks, Internet, markers, weighted max-min fairness]
Graceful quorum reconfiguration in a robust emulation of shared memory
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Providing shared-memory abstraction in message-passing systems often simplifies the development of distributed algorithms and allows for the reuse of shared-memory algorithms in the message-passing setting. A robust emulation of atomic single-writer/multi-reader registers in message-passing systems was developed by Attiya, Bar-Noy and Dolev (1995). This emulation was extended by Lynch and Shvartsman (1997) to multi-writer/multi-reader registers using reconfigurable quorum systems. In this work we present a new atomic multi-writer/multi-reader register service that includes a fault-tolerant reconfiguration service. This new emulation has a substantially improved performance and fault-tolerance characteristics. We introduce the concept of intermediate quorum configurations and show how they can be used by readers/writers during reconfiguration. The result is that the quorum reconfigurations are graceful: readers and writers no longer "busy-wait" during reconfigurations, bur are able to complete their operations. An additional advance is that the reconfigurer is eliminated as the single point of failure. When the reconfigurer fails, readers and writers continue using intermediate configurations. In finite executions, read and write operations terminate in bounded time using a bounded number of messages (the bounds depend on the "currency" of the configuration at the invoker of the operation). Finally, the service places no restrictions on the installed quorum configuration: a previously installed quorum system can be replaced by an arbitrary new quorum system.
[message passing, Engineering profession, Laboratories, robust emulation, intermediate quorum configurations, Registers, reconfigurable quorum systems, message-passing systems, Computer science, Fault tolerance, atomic multi-writer/multi-reader register service, Message passing, graceful quorum reconfiguration, fault-tolerant reconfiguration service, Emulation, distributed algorithms, Automata, shared-memory abstraction, shared memory systems, finite executions, Robustness, fault tolerant computing, Contracts]
Coherence-based coordinated checkpointing for software distributed shared memory systems
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Fault-tolerant techniques that can cope with system failures in software distributed shared memory (SDSM) are essential for creating productive and highly available parallel computing environments on clusters of workstations. We propose a new, efficient coordinated checkpointing technique, called coherence-based coordinated checkpointing (CCC), for SDSM. Our CCC minimizes both the checkpointing overhead during failure-free execution and the cost of recovery from failures by leveraging existing coherence information maintained by SDSM. In the presence of system failures, it allows SDSM to recover from the most recent checkpoint, saving the re-computation time. We have performed experiments on a cluster of eight Sun Ultra-5 workstations, comparing our CCC technique against both simple coordinated checkpointing (SCC) and incremental coordinated checkpointing (ICC) techniques by actually implementing these techniques in TreadMarks, a stare-of-the-art SDSM system. The experimental results demonstrate that our CCC technique consistently outperforms both SCC and ICC techniques. In particular our technique increases the execution time slightly by 0.5% to 4% for a 2-minute checkpointing interval during failure-free execution, while SCC and ICC techniques result in the execution time overhead of 4% to 100% and 3% to 64%, respectively for the same checkpointing interval.
[Checkpointing, workstation clusters, fault-tolerant techniques, Protocols, Costs, failure recovery, experiments, coherence-based coordinated checkpointing, TreadMarks, Distributed computing, system recovery, Read only memory, Sun Ultra-5 workstations, Space technology, Workstations, Protection, parallel computing, software performance evaluation, incremental coordinated checkpointing, software distributed shared memory systems, system failures, software fault tolerance, simple coordinated checkpointing, Parallel programming, execution time, distributed shared memory systems, Software systems]
Distributed Java Virtual Machine for message passing architectures
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
This paper introduces a distributed shared memory Java Virtual Machine architecture. This project is targeted for any distributed message-passing architecture, and specifically for networks of workstations. The whole system is implemented in user space which offers portability and flexibility. The memory consistency is provided by one of four protocols implementing release consistency. The novelty of the consistency protocols presented is that access faults are avoided by replicating objects ahead-of-time where necessary. The relative performance of these protocols is evaluated for three benchmark applications. Our experimental results indicate that, in the majority of cases, update protocols outperform invalidate protocols.
[workstation clusters, access faults, invalidate protocols, distributed shared memory Java Virtual Machine architecture, Electrical capacitance tomography, message passing architectures, portability, flexibility, Computer architecture, Workstations, workstation networks, release consistency, update protocols, protocols, benchmark applications, Java, Runtime environment, message passing, memory consistency, Access protocols, object replication, Virtual machining, Computer science, Message passing, Memory management, distributed shared memory systems, user space]
On supporting weakly-connected browsing in a mobile Web environment
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
A mobile environment is weakly-connected, characterized by low communication bandwidth and poor connectivity. The conventional paradigm for surfing mobile Web documents is ineffective since portions of a document could be corrupted during transmission and it is expensive to re-transmit the whole document. It is important that the high content-bearing portions should be transmitted successfully so that a mobile client could at least obtain a high level content and determine if the corrupted portions need to be retransmitted. We have proposed a multi-resolution transmission paradigm which allows higher content-bearing portions of a Web document to be transmitted, by partitioning it into multiple organizational units and associating an information content with each unit. The client can explore the higher content-bearing portion earlier and terminate browsing an irrelevant document sooner. We extend our previous work and propose a fault-tolerant multi-resolution transmission scheme which allows units of higher information content to be recovered from transmission error. The client can obtain an overall content of a Web document and either terminate the transmission of the remaining portions or decide if the corrupted portions need to be retransmitted. We demonstrate its feasibility with a prototype and with simulation results.
[low communication bandwidth, mobile client, Humans, simulation, Mobile communication, multi-resolution transmission paradigm, weakly-connected browsing, poor connectivity, Fault tolerance, fault-tolerant scheme, Bandwidth, Search engines, Virtual prototyping, mobile Web environment, document browsing, information resources, document handling, client-server systems, Navigation, information retrieval, Computer science, information content, XML, Web pages, fault tolerant computing, Internet]
On the burstiness of the TCP congestion-control mechanism in a distributed computing system
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Several studies in network traffic characterization have concluded that network traffic is self-similar and therefore not readily amenable to statistical multiplexing in a distributed computing system. This paper examines the effects of the TCP protocol stack on network traffic via an experimental study on the different implementations of TCP. We show that even when aggregate application traffic smooths out as more applications' traffic are multiplexed, TCP introduces burstiness into the aggregate traffic load, reducing network performance when statistical multiplexing is used within the network gateways.
[telecommunication congestion control, Laboratories, Telecommunication traffic, Quality of service, Electrical capacitance tomography, Distributed computing, distributed computing system, Intelligent networks, network performance, Traffic control, statistical multiplexing, Computer networks, burstiness, Peer to peer computing, computer networks, TCP congestion-control mechanism, TCP protocol stack, aggregate application traffic, network traffic, network traffic characterization, network gateways, Aggregates, transport protocols, multiplexing, telecommunication traffic]
A semantic broadcast scheme for a mobile environment based on dynamic chunking
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Data broadcast is an effective approach to disseminate information from a database server to numerous mobile clients in a mobile environment. Since a broadcast session contains only a subset of the database items, a client might not be able to obtain all its items from the broadcast and is forced to request additional ones from the server on demand. We describe a semantic-based broadcast approach which attaches a semantic description to each broadcast unit, called a chunk, which is a cluster of data items. This allows a client to determine if a query can be answered entirely using a broadcast as well as defining the precise nature of the remaining items in the form of a "supplementary" query. Chunks could be of different sizes and are hierarchically organized. We propose a heuristic to schedule the broadcast order of the chunks to improve the tuning time, access time, and a new metric called a data affinity index. The performances are evaluated via experiments based on a simulation model.
[Performance evaluation, client-server systems, database server, semantic-based broadcast approach, mobile environment, Scheduling, database management systems, Sun, Delay, Information systems, mobile computing, Databases, mobile clients, data affinity index, data broadcast, file servers, Bandwidth, Pricing, Broadcasting, semantic broadcast scheme, dynamic chunking, Mobile computing]
A client-server approach to virtually synchronous group multicast: specifications and algorithms
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
This paper presents a formal design for a novel group multicast service that provides virtually synchronous semantics in asynchronous fault-prone environments. The design employs a client-server architecture in which group membership is maintained not by every process but only by dedicated membership servers, while virtually synchronous group multicast is implemented by service end-points running at the clients. Specifically, the paper defines service semantics for the client-server interface, that is, for the group membership service. The paper then specifies virtually synchronous semantics for the new group multicast service, as a collection of commonly used safety and liveness properties. Finally, the paper presents new algorithms that use the defined group membership service to implement the specified properties. The algorithm that provides the complete virtually synchronous semantics executes in a single message round in parallel with the membership service's agreement on views, and is therefore more efficient than previously suggested algorithms providing such semantics.
[Algorithm design and analysis, Communication systems, virtually synchronous semantics, group membership, formal specification, group multicast service, Uniform resource locators, asynchronous fault-prone environments, service end-points, group membership service, liveness properties, groupware, multicast communication, Safety, client-server architecture, client-server systems, single message round, dedicated membership servers, specification, Maintenance, Power system modeling, Computer science, Multicast algorithms, Interleaved codes, formal design, fault tolerant computing, Power system reliability, safety properties]
A new network layer protocol with routing address and tables auto-configuration mechanism
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
We propose a network layer protocol denoted ACRP (Auto-Configurated Routing Protocol) equipped with a function that automatically configures routing addresses and routing tables for a large scale network system made up of multiple connected physical networks. The goals of the work presented, are to enable adding and deleting physical networks during practical use and designing the protocol in such a way that the auto-configuration is finished within a few minutes. By simulating ACRP, we have succeeded in determining the algorithm structures for the routing tables protocol, that satisfy these requirements in a network with approximately 400000 nodes. This work will contribute to the improvement of expandability and maintainability of control LANs for intelligent buildings, etc.
[Auto-Configurated Routing Protocol, intelligent buildings, control LANs, Communication system control, Control systems, routing tables protocol, algorithm structures, multiple connected physical networks, Network topology, routing addresses, large scale network system, Routing protocols, Large-scale systems, protocols, tables auto-configuration mechanism, Monitoring, controller area networks, Intelligent control, Buildings, Lighting control, Intelligent structures, network layer protocol, configuration management, telecommunication network routing, routing address, ACRP]
Spout: a transparent distributed execution engine for Java applets
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
The advent of executable contents such as Java applets exposes WWW users to a new class of attacks that were not possible before. Serious security breach incidents due to implementation bugs arose repeatedly in the past several years. Without a provably correct implementation of Java's security architecture specification, it is difficult to make any conclusive statements about the security characteristic of current Java virtual machines. The Spout project takes an alternative approach to address Java's security problems. Rather than attempt a provably secure implementation, we aim to confine the damages of malicious Java applets to selective machines, thus protecting resources behind an organization's firewall from attacks by malicious or buggy applets. Spout is essentially a distributed Java execution engine that transparently decouples the processing of an incoming applet's application logic from that of the graphical user interface (GUI), such that the only part of an applet that is actually running on the requesting user's host is the harmless GUI code. A unique feature of the Spout architecture that does not exist in other similar systems, is that it is completely transparent to and does not require any modifications to WWW browsers or class libraries on the end hosts. This paper describes the design, implementation, and performance measurements of the first Spout prototype, which also incorporates run-time resource monitoring mechanisms to counter denial-of-service attacks.
[implementation bugs, buggy applets, graphical user interfaces, World Wide Web, class libraries, Security, Engines, malicious applets, software architecture, Java virtual machines, mobile code, Libraries, Logic, Java applets, Web browsers, run-time resource monitoring, Protection, distributed programming, distributed object management, transparent distributed execution engine, Graphical user interfaces, information resources, Java, object-oriented programming, graphical user interface, Virtual machining, security architecture specification, performance measurements, security of data, Computer bugs, firewall, Spout, denial-of-service attacks, security breach]
Scheduling with global information in distributed systems
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Buffered coscheduling is a distributed scheduling methodology for time-sharing communicating processes in a distributed system, e.g., PC cluster. The principle mechanisms involved in this methodology are communication buffering and strobing. With communication buffering, communication generated by each processor is buffered and performed at the end of regular intervals (or time slices) to amortize communication and scheduling overhead. This regular communication structure is then leveraged by introducing a strobing mechanism which performs a total exchange of information at the end of each time slice. Thus, a distributed system can rely on this global information to more efficiently schedule communicating processes rather than rely on isolated or implicit information gathered from local events between processors. We describe how buffered coscheduling is implemented in the context of our SMART simulator. We then present performance measurements for two synthetic workloads and demonstrate the effectiveness of buffered coscheduling under different computational granularities, context-switch times and time-slice granularities.
[Measurement, communication buffering, time-sharing systems, Laboratories, digital simulation, Read only memory, time-slice granularities, strobing, resource allocation, Operating systems, network operating systems, time-sharing communicating processes, scheduling, distributed systems, global information, computational granularities, software performance evaluation, buffered coscheduling, Identity-based encryption, Computational modeling, SMART simulator, context-switch times, Scheduling algorithm, performance measurements, Processor scheduling, distributed scheduling methodology, Time sharing computer systems, Context modeling, distributed operating systems]
A dynamic distributed video on demand service
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
The use of Internet services has become part of everyday life. The need for more advanced applications is increasing and solutions have to be provided in order to meet it. Multimedia information is becoming more popular. The connection bandwidth is not yet ready to cope with rising expectations for applications such as video on demand (VoD). We propose a way to implement such a service over a limited bandwidth/best effort Internet based network. Our proposal consists of the implementation of algorithms and the introduction of quality of service features on a network such as the Internet that inherently does not support such features. We propose two techniques for the implementation of an Internet based video on demand service. The first deals with the distribution of video titles on several video servers and is called the Disk Manipulation Algorithm (DMA). The second gives a dynamic nature to the service, imposes a virtual routing procedure to the system and is called the Virtual Routing Algorithm (VRA).
[Internet services, Disk Manipulation Algorithm, Video on demand, Ethernet networks, Cable TV, bandwidth, Routing, Electrical capacitance tomography, quality of service, video servers, multimedia information, High-speed networks, Web and internet services, video on demand, virtual routing, Bandwidth, video titles, Internet, dynamic distributed video on demand service, Virtual Routing Algorithm, IP networks, multimedia communication, Asynchronous transfer mode]
Efficient RMI: dynamic specialization of object serialization
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
This paper describes a novel approach to object serialization in remote method invocation (RMI). Object serialization transforms objects' representations between heterogeneous platforms. Efficient serialization is primary concern in RMI because the conventional approaches incur large runtime overheads. The approach described specializes a serializing routine dynamically according to a receiver's platform, and this routine converts the sender's in-memory representations of objects directly into the receiver's in-memory representations. This approach simplifies the process of RMI: the receiver can access the passed objects immediately without any data copies and data conversions. A new platform can join the existing community of senders and receivers because a specialized routine for the platform is generated as needed. Experimental results show that significant performance gains are obtained by this approach. The prototype implementation of this approach was 1.9-3.0 times faster than Sun XDR, and the time needed for generating a specialized routine was only 0.6 msec.
[Java, object-oriented programming, receiver platform, open systems, in-memory representations, Performance gain, serializing routine, Sun, object serialization, Computer science, Information science, Computer languages, Runtime, Data conversion, Operating systems, heterogeneous platforms, Prototypes, remote method invocation, dynamic specialization, remote procedure calls]
Deterministic optimal routing for two heterogeneous parallel servers
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
The paper describes the characteristics of an optimal routing that assigns each arriving packet to one of two heterogeneous parallel servers. The characteristics are derived from numerical solutions to a routing problem, which finds an optimal routing that minimizes the average packet delay under the condition that the input traffic is completely deterministic. Four characteristics are presented: (1) under light or moderate traffic, the average packet delay of the optimal routing is almost the same as that of the join the shortest delay (JSD) policy; (2) as traffic becomes heavier, the optimal routing more often uses the fix queue based on size (FS) policy; (3) under heavy traffic, the optimal routing assigns small packets to a slower server; (4) as the ratio of the two servers' service rates C/sub 1//C/sub 2/ (C/sub 1//spl les/C/sub 2/) decreases, under lighter traffic the optimal routing uses the FS policy. A new adaptive routing policy based on these four characteristics outperforms the JSD policy.
[numerical solutions, adaptive routing policy, deterministic optimal routing, join the shortest delay, moderate traffic, packet switching, computer networks, service rates, Routing, FS policy, optimisation, routing problem, fix queue, telecommunication network routing, file servers, heavy traffic, heterogeneous parallel servers, JSD policy, input traffic, average packet delay, telecommunication traffic]
PARK: a paused-and-run k-stream multimedia synchronization control scheme
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
In the paper, we propose the pause-and-run approach for k-stream (PARK) multimedia presentations over the Internet to achieve reliable transmission of continuous media. The main characteristics of the PARK approach are as follows: (i) to achieve reliable transmission of continuous media, PARK adopts TCP instead of UDP; (ii) since the slow-start scheme is embedded in TCP, a novel flow adaptation scheme is proposed to reduce the overhead of the network and end hosts. The server adapts its transmission rates to the buffer situation of the client and prevents the client's buffers from overflow and underflow as much as possible; (iii) with the provision of multiple-stream synchronization and multi-level adaptation control, the client achieves smooth multimedia presentations and achieves graceful presentation degradation when the resources are insufficient.
[TCP, telecommunication congestion control, multiple-stream synchronization, Laboratories, multimedia systems, Reliability engineering, Delay, multi-level adaptation control, PARK, Degradation, Network servers, graceful presentation degradation, paused-and-run k-stream multimedia synchronization control scheme, flow adaptation scheme, Computer network reliability, IP networks, slow-start scheme, transmission rates, reliable continuous media transmission, client buffers, synchronisation, Computer science, transport protocols, k-stream multimedia presentations, Streaming media, Internet]
Paradigm-oriented distributed computing using mobile agents
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
We describe the implementation underlying an environment for distributed computing that uses the concept of well-known paradigms. The main advantage of paradigm oriented distributed computing is that the user only needs to specify application-specific sequential code, while the underlying infrastructure takes care of the parallelization and distribution. The main features of the proposed approach, called PODC, which differentiate it from other approaches, are the following: (1) it is intended for loosely-coupled network environments, not specialized multiprocessors; (2) it is based on an infrastructure of mobile agents; (3) it supports programming in C, rather than a functional or special-purpose language, and (4) it provides a Web based interactive graphics interface through which programs are constructed, invoked, and monitored. The three paradigms presently supported in PODC are the bag-of-tasks, the branch-and-bound and genetic programming. We describe their implementation and performance within the mobile agent based PODC environment.
[graphical user interfaces, bag-of-tasks, Web based interactive graphics interface, Distributed computing, Read only memory, mobile computing, Mobile agents, Genetic programming, Parallel processing, paradigm oriented distributed computing, PODC, Monitoring, distributed programming, information resources, Identity-based encryption, genetic programming, mobile agent infrastructure, genetic algorithms, tree searching, application-specific sequential code, parallelization, Programming profession, Computer science, loosely-coupled network environments, branch-and-bound, Computer applications, C programming, mobile agent based PODC]
TabSum: a flexible and dynamic table summarization approach
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Many diverse small devices, such as smart phones and personal digital assistants, are being deployed to access the Internet. Small devices typically have limited display and processing capabilities. It is thus difficult to effectively present a large table of information on these different devices such that the table can be easily browsed by the users. We present the design and prototype implementation of TabSum, a flexible and dynamic table summarization approach to reducing tables into smaller but still meaningful representations for various devices. TabSum supports easy browsing of a large table by various devices and provides personalization by taking into account both device capabilities and user preferences. A multi-source multi-layered specification methodology is proposed to describe summarization rules preferred by various sources. On request, a set of table reduction rules are dynamically applied to the rows and columns of a table to reduce its size. The approach is flexible and dynamic. It summarizes both numeric and non-numeric data types.
[browsing, dynamic table summarization, utility programs, Displays, smart phones, user interfaces, report generators, TabSum, personalization, multisource multilayered specification methodology, Prototypes, Internet, Personal digital assistants, Smart phones, personal digital assistants]
An application-transparent, platform-independent approach to rollback-recovery for mobile agent systems
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
This paper proposes a new approach to rollback-recovery for mobile agent systems, and describes its implementation in the MESSENGERS mobile agents system. The used checkpointing method allows the implementation of a space and time efficient, user-transparent rollback-recovery in heterogeneous distributed environments. Together with an efficient non-blocking system snapshot algorithm this checkpointing method is an attractive choice for implementing a rollback-recovery mechanism in a mobile agent system, because it exploits features specific to such systems during the recovery. This paper also presents an optimization technique, called concurrent checkpointing, that increases the effectiveness of the proposed rollback-recovery mechanism.
[Checkpointing, workstation clusters, checkpointing, concurrent checkpointing, system recovery, Concurrent computing, optimisation, Mobile agents, optimization, Computer networks, MESSENGERS, Workstations, platform-independent approach, rollback recovery, distributed programming, Microcomputers, mobile agent systems, Application software, software agents, software fault tolerance, Computer science, High performance computing, heterogeneous distributed environments, application-transparent approach, Internet, nonblocking system, snapshot algorithm]
Suez: a cluster-based scalable real-time packet router
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Suez is a high-performance real-time packet router that supports fast best-effort packet routing and scalable QoS-guaranteed packet scheduling, and is built on a hardware platform consisting of a cluster of commodity PCs connected by a gigabit/sec system area network. The major goal of the Suez project is to demonstrate that the PC cluster architecture can be as cost-effective a platform for high-performance network packet routing as for parallel computing. Suez features a cache-conscious routing-table search algorithm that exploits CPU caching hardware for fast lookup by treating IP addresses directly as virtual addresses. To scale the number of real-time connections supportable with the link speed, Suez implements a fixed-granularity fluid fair queuing (FGFFQ) algorithm that completely eliminates the per-packet sorting overhead associated with conventional weighted fair queuing algorithms. This paper presents the architectural features of Suez, and reports the performance measurements of the Linux-based Suez prototype, which is built on four Pentium-II 400 MHz machines and Myrinet.
[Real time systems, Measurement, workstation clusters, CPU caching hardware, IP addresses, Myrinet, fixed-granularity fluid fair queuing algorithm, high-performance real-time packet router, processor scheduling, Linux-based Suez prototype, Clustering algorithms, fast best-effort packet routing, Computer architecture, Parallel processing, hardware platform, Hardware, virtual addresses, scalable QoS-guaranteed packet scheduling, cache-conscious routing-table search algorithm, fast lookup, cluster-based scalable real-time packet router, virtual storage, weighted fair queuing algorithms, performance evaluation, Routing, Pentium-II machines, quality of service, Scheduling algorithm, Sorting, PC cluster, performance measurements, 400 MHz, real-time connections, real-time systems, telecommunication network routing, system area network, Personal communication networks]
A client-server oriented algorithm for virtually synchronous group membership in WANs
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
We describe a novel scalable group membership service designed explicitly for wide area networks. Our membership service is scalable in the number of groups supported, in the number of members in each group, and in the topology each group spans. Our service also supplies the hooks needed to provide clients with full virtual synchrony semantics. Our service attains, on average, a low message overhead by agreeing on membership within a single message round. Furthermore, our service avoids notifying the application of obsolete membership views when the network is unstable, yet it converges when the network has stabilized.
[Wide area networks, Algorithm design and analysis, client-server systems, Change detection algorithms, wide area networks, virtually synchronous group membership, full virtual synchrony semantics, topology, WANs, Maintenance, scalable group membership service, Computer science, Design engineering, Network topology, low message overhead, Fault tolerant systems, multicast communication, groupware, Interleaved codes, client-server oriented algorithm, message round, Contracts]
A security architecture for mobile agents in Ajanta
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
A mobile agent represents a computation that can autonomously migrate in a network to perform tasks on behalf of its creator. This paper describes the security architecture of Ajanta, a Java-based system for mobile agent programming. This architecture provides mechanisms to protect server resources from malicious agents, agent data from tampering by malicious servers, and to protect the system infrastructure itself. An agent can carry three kinds of protected data: read-only objects, objects visible only to specific servers, and a secure append-only list of objects. Agents' access to server resources is controlled using a proxy-based mechanism. A generic authentication protocol is used for all sensitive client-server interactions. Ajanta also supports communication between remote agents using RMI, which can be controlled by the servers' security policies.
[append-only object list, malicious agents, Communication system control, Ajanta, Network servers, Mobile agents, Computer architecture, Computer networks, mobile agent programming, Protection, distributed programming, distributed object management, Java, client-server systems, server resources, object-oriented programming, Data security, Access protocols, client-server interactions, malicious servers, remote agent communication, read-only objects, RMI, security of data, Java-based system, security architecture, Authentication, remote procedure calls, generic authentication protocol, proxy-based mechanism]
Improving distributed workload performance by sharing both CPU and memory resources
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
We develop and examine job migration policies by considering effective usage of global memory in addition to CPU load sharing in distributed systems. When a node is identified for lacking sufficient memory space to serve jobs, one or more jobs of the node will be migrated to remote nodes with low memory allocations. If the memory space is sufficiently large the jobs will be scheduled by a CPU-based load sharing policy. Following the principle of sharing both CPU and memory resources, we present several load sharing alternatives. Out objective is to reduce the number of page faults caused by unbalanced memory allocations for jobs among distributed nodes, so that overall performance of a distributed system can be significantly improved. We have conducted trace-driven simulations to compare CPU-based load sharing policies with our policies. We show that our load sharing policies not only improve performance of memory bound jobs, but also maintain the same load sharing quality as the CPU-based policies for CPU-bound jobs. Regarding remote execution and preemptive migration strategies, our experiments indicate that a strategy selection in load sharing is dependent on the amount of memory demand of jobs-remote execution is more effective for memory-bound jobs, and preemptive migration is more effective for CPU-bound jobs. Our CPU memory-based policy using either high performance or high throughput approach and using the remote execution strategy performs the best for both CPU-bound and memory-bound jobs.
[storage allocation, Throughput, preemptive migration, Degradation, resource allocation, Space technology, Operating systems, scheduling, distributed systems, remote execution, memory resources, software performance evaluation, job migration policies, memory bound jobs, load sharing, distributed workload performance, Nominations and elections, page faults, performance evaluation, Educational institutions, Application software, Sun, high throughput, Computer science, trace-driven simulations, CPU resources, virtual machines, Load management, memory allocations]
A query propagation approach to improve CORBA Trading Service scalability
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Existing CORBA traders, at least most of them, support the core functions of the OMG specification of the Trading Service. We believe that this is useful and can help potential users to use such a service in heterogeneous environments. However we also believe that this is not sufficient because such environments deal with dynamic information and often require scalability (e.g. stock market applications). The CORBA Trading Service uses static information recorded in different components of a trading graph, which reduces its ability to deal with dynamic environments. This paper proposes solutions to the issue of type management and query routing in the context of CORBA Trading Service to improve the scalability and the quality of the results returned by the core trader functions. We propose a query routing mechanism that uses dynamic information recorded within different traders. Some of this information, such as hit factor, is calculated based on the number of offers a remote trader can address and their relative hops away. Finally, we demonstrate that the proposed approach has led to better performance for the core CORBA trader functions.
[Context-aware services, Scalability, query routing, OMG specification, heterogeneous environments, query propagation, Electronic switching systems, Electrical capacitance tomography, trading graph, Software development management, Computer science, Query processing, dynamic information, CORBA trading service scalability, Computer architecture, hit factor, stock market applications, static information, Australia, distributed object management, CORBA traders, Quality management, type management]
A multi-version approach to conflict resolution in distributed groupware systems
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Groupware systems are a special class of distributed computing systems which support human-computer-human interaction. Real-time collaborative graphics editors allow a group of users to view and edit the same graphics document at the same time from geographically dispersed sites connected by communication networks. Resolving conflict access to shared objects is one of the core issues in the design of this type of system. This paper proposes a novel distributed multi-version approach to conflict resolution. This approach aims to preserve the work concurrently produced by multiple users in the face of conflicts, and to minimize the number of object versions for accommodating combined effects of conflicting and compatible operations. Major technical contributions of this work include a formal specification of a unique combined effect for any group of conflicting and compatible operations, a distributed algorithm for incremental creation of multiple object versions, and a consistent object identification scheme for multi-version and multireplica graphics editing systems. All algorithms and schemes preserved in this paper have been used in the GRACE (Graphics Collaborative Editing) system implemented in Java.
[human-computer-human interaction, distributed processing, object versions, Electrical capacitance tomography, Distributed computing, formal specification, Delay, consistent object identification scheme, multi-version graphics editing systems, groupware, Graphics Collaborative Editing system, GRACE system, geographically dispersed sites, Java, Collaborative software, communication networks, shared objects, Sun, Information technology, multireplica graphics editing systems, distributed computing systems, multiple users, distributed algorithm, computer graphics, distributed groupware systems, Collaboration, real-time systems, Collaborative work, Internet, real-time collaborative graphics editors, Australia, multi-version conflict resolution]
Critical bandwidth allocation techniques for stored video delivery across best-effort networks
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
We propose two new techniques for the delivery of compressed prerecorded video streams across best-effort networks like the Internet. Current approaches for the delivery of stored video across best-effort networks typically alter the quality of the video frames, the frame rate delivered to the user, or a combination of both. By using network feedback, these algorithms continually adjust the video quality to fit within the available network resources. These approaches, however do not take advantage of the a priori information available from stored video streams, namely the frame sizes that the movie consists of. We show how monitoring the a priori information and actively monitoring a client-side buffer can help smooth the video frame rate delivered to the user, providing a more consistent quality of video.
[frame sizes, multimedia systems, client-side buffer, stored video streams, compressed prerecorded video streams, a priori information, frame rate, Bandwidth, Video compression, Motion pictures, Computer networks, IP networks, video frames, data compression, client-server systems, Smoothing methods, best-effort networks, Video sharing, Prefetching, quality control, critical bandwidth allocation techniques, video quality, video coding, video frame rate, network feedback, bandwidth allocation, stored video delivery, Channel allocation, Streaming media, network resources, Internet]
Neural nets based predictive prefetching to tolerate WWW latency
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
With the explosive growth of WWW applications on the Internet, users are experiencing access delays more often than ever. Recent studies showed that prefetching could alleviate the WWW latency to a larger extent than caching. Existing prefetching methods are mostly based on URL graphs. They use the graphical nature of hypertext links to determine the possible paths through a hypertext system. While they have been demonstrated effective in prefetching of documents that are often accessed, they are incapable of pre-retrieving documents whose URLs had never been accessed. We propose a context-specific prefetching technique to overcome the limitation. It relies on keywords in anchor texts of URLs to characterize user access patterns and on neural networks over the keyword set to predict future requests. It features a self-learning capability and good adaptivity to the change of user surfing interest. The technique was implemented in a SmartNewsReader system and cross-examined in a daily browsing of MSNBC and CNN news sites. The experimental results showed an achievement of approximately 60% hit ratio due to prefetching. Of the prefetched documents, less than 30% was undesired.
[keywords, hypermedia, neural networks, World Wide Web, Etching, History, Delay, Uniform resource locators, query processing, storage management, access delays, WWW latency, predictive prefetching, self-learning capability, Prefetching, context-specific prefetching technique, anchor texts, Application software, WWW applications, user access patterns, hypertext links, Neural networks, SmartNewsReader system, Explosives, Internet, user surfing interest, neural nets]
Self-stabilizing network orientation algorithms in arbitrary rooted networks
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
We present the first deterministic self-stabilizing network orientation algorithms. We present three protocols for arbitrary and asynchronous networks. All the protocols set up a chordal sense of direction in the network. The protocols are self-stabilizing, meaning that starting from an arbitrary state, the protocols are guaranteed to reach a state, in which all edge labels (assigned to the links) are valid (meaning, they satisfy the specification of the orientation problem).
[Protocols, fault tolerance, arbitrary rooted networks, Routing, deterministic algorithms, self-stabilizing network orientation algorithm, Read only memory, Computer science, Intelligent networks, Network topology, chordal sense of direction, Fault tolerant systems, distributed algorithms, Hypercubes, Computer networks, computer network reliability, Labeling, protocols, asynchronous networks]
The effect of false-name declarations in mechanism design: towards collective decision making on the Internet
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
The purpose of this paper is to analyze a collective decision making problem in an open, dynamic environment, such as the Internet. More specifically, we study a class of mechanism design problems where the designer of a mechanism cannot completely identify the participants (agents) of the mechanism. A typical example of such a situation is Internet auctions. The main contributions of this paper are as follows. We develop a formal model of a mechanism design problem in which false-name declarations are possible, and prove that the revelation principle still holds in this model. When false-name declarations and hiding are possible, we show that there exists no auction protocol that achieves Pareto efficient allocations in a dominant strategy equilibrium for all cases. We show a sufficient condition where the Clarke mechanism is robust against false-name declarations (the concavity of the maximal total utility of agents).
[Protocols, Clarke mechanism, mechanism design, Electronic mail, Cost accounting, Uniform resource locators, Sufficient conditions, hiding, protocols, open dynamic environment, electronic commerce, Discussion forums, Pareto efficient allocations, Decision making, revelation principle, false-name declarations, group decision support systems, Bridges, Computer science, economics, formal model, collective decision making, dominant strategy equilibrium, Internet auctions, Internet]
Dynamic adaptive file management in a local area network
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
In light of advances in processor and networking technology, especially the emergence of network attached disks, the traditional client-server architecture of file systems has become suboptimal for many computation/data intensive applications. We introduce a revised architecture for file management employing network attached storage: the dynamic file server environment (Dynamo). Dynamo introduces two main architectural innovations: to provide high scalability, the file management functions are mainly performed cooperatively by the clients in the system. Furthermore, data is transferred directly to the client's cache from network-attached disks, thus avoiding copies from a disk to the server buffer and then over the network to the client. Secondly, Dynamo uses a cooperative cache management which employs a decentralized lottery-based page replacement strategy. We show via performance benchmarks run on the Dynamo system and simulation results how this architecture increases the system's adaptability, scalability and cost performance.
[dynamic file server environment, dynamic adaptive file management, Scalability, simulation, Magnetohydrodynamic power generation, File servers, local area networks, cache storage, Environmental management, File systems, network attached disks, network operating systems, file servers, Disaster management, Computer architecture, distributed databases, Computer networks, client-server architecture, Local area networks, software performance evaluation, client-server systems, performance benchmarks, decentralized lottery-based page replacement, Dynamo, local area network, file organisation, Computer network management, high scalability, cooperative cache management, data intensive applications]
A framework to protect mobile agents by using reference states
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
To protect mobile agents from attacks by their execution environments, or hosts, one class of protection mechanisms uses "reference states" to detect modification attacks. Reference states are agent states that have been produced by non-attacking or reference hosts. This paper examines this class of mechanisms and presents the bandwidth of the achieved protection. First, the notion of reference states is introduced. This notion allows to define a protection scheme that can be used to realize a whole class of mechanisms to protect mobile agents. To do so, after an initial analysis of already existing approaches, the abstract features of these approaches are extracted. A discussion examines the strengths and weaknesses of the general protection scheme, and a framework is presented that allows an agent programmer to choose an appropriate protection level using this scheme. An example illustrates the usage of the framework and its overhead.
[object-oriented programming, Data security, bandwidth, data security, Humans, Control systems, reference states, software agents, Programming profession, modification attacks, mobile agent protection framework, security of data, Software protection, Mobile agents, Bandwidth, Hardware, reference hosts, agent programming, Cryptography, distributed programming, distributed object management]
Striping doesn't scale: how to achieve scalability for continuous media servers with replication
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Multimedia applications place high demands for QoS, performance, and reliability on storage servers and communication networks. These, often stringent requirements, make design of cost-effective and scalable continuous media (CM) servers difficult. In particular, the choice of data placement techniques can have a significant effect on the scalability of the CM server and its ability to utilize resources efficiently. In the recent past, a great deal of work has focused on "wide" data striping. Another approach to dealing with load imbalance problems is replication. The appropriate compromise between the degree of striping and the degree of replication is key do the design of scalable CM servers. Thus, the main focus of the paper is a study of scalability characteristics of CM servers as a function of tradeoffs between striping and replication.
[load imbalance problems, Scalability, multimedia applications, Quality of service, scalability, Degradation, Network servers, resource allocation, QoS, scalable CM servers, Bandwidth, Large-scale systems, Communication networks, replication, scalability characteristics, scalable continuous media servers, multimedia servers, data placement techniques, communication networks, Maintenance, quality of service, continuous media servers, stringent requirements, storage servers, Performance loss, data handling, Telecommunication network reliability, data striping]
Index structures of user profiles for efficient Web page filtering services
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Searching information from the WWW efficiently and effectively has become a very important issue. We apply the information filtering concept to finding good matches of Web pages to what the user needs. Furthermore, we propose four new methods for indexing user information needs and show their efficiency in comparison with two well-known approaches.
[information resources, search engines, indexing, Keyword search, information retrieval, World Wide Web, information filtering, Information filtering, user information needs, Data mining, Web page filtering services, Computer science, Distributed processing, user profiles, Web pages, Search engines, Information filters, index structures, Metasearch, Internet, information needs]
Dynamic replica allocation using database migration in broadband networks
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Dynamic relocation of databases through networks, or database migration, will soon become a powerful standard database operation effectively using recent advances in broadband networks. In our previous work, a transaction processing method has been proposed in distributed database environments, based on database migration operations. Generally, database replication is an effective technique to improve transaction processing throughput in conventional systems. We propose a replica management method for use with database migration. Database migration is used at the beginning of a transaction to dynamically relocate database replicas. The performance of the proposed method is evaluated using simulation results.
[storage allocation, transaction processing, distributed database, replicated databases, simulation, database migration, Multimedia databases, performance evaluation, Data engineering, broadband networks, Broadband communication, Transaction databases, database replication, dynamic replica allocation, Narrowband, Information systems, transaction processing method, Intelligent networks, Distributed databases, dynamic database relocation, software performance evaluation, Power engineering and energy, Propagation delay, replica management method]
Prediction-capable data compression algorithms for improving transmission efficiency on distributed systems
Proceedings 20th IEEE International Conference on Distributed Computing Systems
None
2000
Network bandwidth is a limited and precious resource in distributed computing environments. Insufficient bandwidth will severely degrade the performance of a distributed computing task in exchanging massive amounts of data among the networked hosts. A feasible solution to save bandwidth is to incorporate data compression during transmission. However blind, or unconditional, compression may only result in waste of CPU power and even slow down the overall network transfer rate, if the data to be transmitted are hard to compress. We present a prediction-capable lossless data compression algorithm to address this problem. By adapting to the compression speed of a host CPU, current system load, and network speed, our algorithm can accurately estimate the compression time of each data block given, and decide whether it should be compressed or not. Experimental results indicate that our prediction mechanism is both efficient and effective, achieving 93% of prediction accuracy at the cost of only 3.2% of the execution time of unconditional compression.
[data compression, Protocols, Costs, data block, Data compression, computer networks, performance evaluation, Electronic switching systems, prediction-capable data compression algorithms, transmission efficiency, network bandwidth, distributed computing, lossless data compression algorithm, Bandwidth, Pressing, Prediction algorithms, distributed systems, Computer networks, Central Processing Unit, Internet, compression time]
Geometric spanners for wireless ad hoc networks
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
We propose a new geometric spanner, for wireless ad hoc networks, which can be constructed efficiently in a distributed manner. It combines the connected dominating set and the local Delaunay graph to form the backbone of a wireless network. This new spanner has the following attractive properties: (1) the backbone is a planar graph; (2) the node degree of the backbone is bounded from above by a positive constant; (3) it is a spanner both for hops and length; moreover, we show that, given any two nodes u and /spl upsi/, there is a path connecting them in the backbone such that its length is no more than 6 times that of the shortest path and the number of links is no more than 3 times that of the shortest path; (4) it can be constructed locally and is easy to maintain when the nodes move around; and (5) we show that the computation cost of each node is at most O(d log d), where d is its l-hop neighbors in the original unit disk graph, and the communication cost of each node is bounded by a constant. Simulation results are also presented for studying its practical performance.
[Spine, graph theory, simulation, wireless ad hoc networks, planar graph, Floods, communication complexity, computation cost, Mobile ad hoc networks, Information geometry, Network topology, Wireless networks, Clustering algorithms, connected dominating set, local Delaunay graph, backbone, 1-hop neighbors, unit disk graph, geometric spanner, length, Routing, Ad hoc networks, communication cost, Computer science, node degree, mobile communication, performance, hops]
Invariant consistency: a mechanism for inter-process ordering in distributed shared memory systems
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
We propose the notion of invariant consistency that allows programmers to specify inter-process ordering requirements. In our approach, we allow a programmer to label a program and provide an ordering specification. In particular, we associate a counter count/sub l/ with each label l that counts the number of times the operation labeled l has been executed. The ordering specification is given by an invariant on these counters.
[data consistency protocol, Multiprocessor interconnection networks, Access protocols, data integrity, Distributed computing, Programming profession, Multiprocessing systems, Counting circuits, Concurrent computing, invariant consistency, Sufficient conditions, Message passing, inter-process ordering, distributed shared memory systems, protocols, Contracts, distributed programming]
A fully distributed framework for cost-sensitive data mining
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
We propose a fully distributed system (as compared to centralized and partially distributed systems) for cost-sensitive data mining. Experimental results have shown that this approach achieves higher accuracy than both the centralized and partially distributed learning methods, however, it incurs much less training time, neither communication nor computation overhead.
[cost-sensitive data mining, cost-sensitive learning, data mining, probability, Switches, Relational databases, fully distributed learning, Credit cards, Rivers, relational database, Data mining, Distributed computing, Milling machines, Computer science, Learning systems, fraud, Machine learning, decision trees, distributed databases, fully distributed framework, learning (artificial intelligence), training time]
Non-blocking transactional mobile agent execution
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Mobile agents are computer programs that act autonomously on behalf of a user and travel through a network of heterogeneous machines. To enable mobile agent technology for e-business, transaction support needs to be provided, in particular execution atomicity. Execution atomicity ensures that either all operations of the agent succeed, or none at all, and needs to be ensured also in the face of infrastructure failures. We distinguish between blocking and non-blocking solutions for transactional mobile agents, i.e., mobile agents, that execute as a transaction. The approach presented in the paper is non-blocking. A non-blocking transactional mobile agent execution has the important advantage, that it can make progress despite failures.
[transaction processing, computer programs, Laboratories, Throughput, Computer crashes, logical execution environment, Distributed computing, software agents, e-business, software fault tolerance, transaction support, Mobile agents, heterogeneous machines, nonblocking transactional mobile agent execution, Computer networks, Books, Logic, execution atomicity, infrastructure failures, electronic commerce]
Dynamic replica control based on fairly assigned variation of data with weak consistency for loosely coupled distributed systems
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
This paper proposes a replica control method based on a fairly assigned variation of numerical data that has weak consistency for loosely coupled systems managed or used by different organizations. This method dynamically distributes the variation of numerical data to replicas according to their demands while achieving fairness among them. By assigning the variation, a replica can determine the possibility that processed update transactions will be aborted and can notify a client of the possibility even when network partitioning happens. In addition, fairly, assigning the variation of data to replicas enables the disadvantage among replicas caused by asynchronous update to be balanced among replicas. Fairness control for assigning the variation of data is performed by averaging the demands in the variation that are requested by the replicas. Simulation showed that our system can achieve extremely high fairness while processing update transactions at the maximum rate.
[Availability, fairness, replicated databases, Scalability, Computational modeling, Humans, weak consistency, Control systems, replica control method, data integrity, transactions, loosely coupled systems, Delay, replicated database systems, Telegraphy, numerical data, update transactions, asynchronous update, concurrency control, Telephony, Database systems, Computer networks]
Distributing MPEG movies over the Internet using programmable networks
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Distributing video over the Internet is an increasingly important application. Nevertheless, the real-time and high bandwidth requirements of video make video distribution over today's Internet a challenge. Adaptive approaches can be used to respond to changes in bandwidth availability while limiting the effect of such changes on perceptual quality and resource consumption. Nevertheless, most existing adaptation mechanisms have limited scalability and do not effectively exploit the heterogeneity of the Internet. In this paper, we describe the design and implementation of a MPEG video broadcasting service based on active networks. In an active network, routers can be programmed to make routing decisions based on local conditions. Because decisions are made locally, adaptation reacts rapidly to changing conditions and is unaffected by conditions elsewhere in the network. Programmability allows the adaptation policy to be tuned to the structure of the transmitted data, and to the properties of local clients. We use the PLAN-P domain-specific language for programming active routers; this language provides high-level abstractions and safety guarantees that allow complex protocols to be developed rapidly and reliably. Our experiments show that our approach to video distribution permits the decoding of up to 9 times as many frames in a heavily loaded network as distribution using standard routers.
[active networks, routing decisions, complex protocols, Scalability, programmable networks, heavily loaded network, adaptation policy, Multimedia communication, MPEG video broadcasting service, video servers, programmability, PLAN-P domain-specific language, real-time high bandwidth requirements, Bandwidth, Broadcasting, Motion pictures, high-level abstractions, safety guarantees, IP networks, protocols, routers, Availability, client-server systems, MPEG movie distribution, Routing, local conditions, decoding, Domain specific languages, broadcasting, local clients, telecommunication network routing, Internet]
Query optimization to meet performance targets for wide area applications
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Recent technology advances have enabled mediated query processing with Internet accessible WebSources. A characteristic of WebSources is that their access costs exhibit transient behavior These costs depend on the network and server workloads, which are often affected by, the time of day,, day, etc. Given transient behavior, an appropriate performance target (PT) for a noisy, environment will correspond to "at least X percentage of queries will have a latency of less than T units of time". In this paper we propose an optimizer strategy that is sensitive to the objective of meeting such performance targets (PT). For each query plan, a PT sensitive optimizer uses both the expected value of the cost distribution of the plan, as well as the expected delay, of the plan. We validate our strategy using a simulation based study of the optimizers behavior. We also experimentally validate the optimizer using traces of access costs for real WebSources.
[mediated query processing, query plan, performance target, Educational institutions, query evaluation, Delay, query processing, Network servers, Telegraphy, Network topology, Query processing, Working environment noise, remote relation, Cost function, WebSources, Internet, Web server, Internet accessible]
Reasoning about joint administration of access policies for coalition resources
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
We argue that joint administration of access policies for a dynamic coalition formed by autonomous domains requires that these domains set up a coalition authority that distributes attribute certificates authorizing access to policy objects (e.g., ACLs). Control over the issuance of such certificates is retained by member domains separately holding shares of the joint coalition authority's private key with which they sign the attribute certificates. Hence, any (proper) subset of the member domains need not be trusted to protect the private key. However, application servers that implement joint administration of access policies based on attribute certificates must trust all the signers of those certificates, namely all member domains of the coalition. To capture these trust relations we extend existing access control logics and show that the extensions are sound. To reason about joint administration of access policies, we illustrate an authorization protocol in our logic for accessing policy objects using threshold attribute certificates.
[Access control, access policies, coalition resources, File servers, Authorization, formal logic, public key cryptography, authorisation, application servers, Logic, protocols, Web server, Protection, coalition authority, authorization protocol, joint administration, client-server systems, Access protocols, policy objects, Diseases, threshold attribute certificates, Public key, autonomous domains, Resource management, access control logics]
Accelerating Internet streaming media delivery using network-aware partial caching
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Internet streaming applications are affected by adverse network conditions such as high packet loss rates and long delays. This paper aims at mitigating such effects by leveraging the availability of client-side caching proxies. We present a novel caching architecture and associated cache management algorithms that turn edge caches into accelerators of streaming media delivery. A salient feature of our caching algorithms is that they allow partial caching of streaming media objects and joint delivery of content from caches and origin servers. The caching algorithms we propose are both network-aware and stream-aware; they take into account the popularity of streaming media objects, their bit-rate requirements, and the available bandwidth between clients and servers. Using realistic models of Internet bandwidth derived from proxy cache logs and measured over real Internet paths, we have conducted simulations to evaluate the performance of various cache management alternatives. Our experiments demonstrate that network-aware caching algorithms can significantly reduce service delay and improve overall stream quality. Our experiments also show that partial caching is particularly effective when bandwidth variability is not very high.
[caching architecture, origin servers, stream-aware algorithms, Quality of service, cache storage, client-side caching proxies, multimedia computing, cache management algorithms, Network servers, proxy cache logs, Bandwidth, IP networks, Web server, multimedia communication, client-server systems, multimedia servers, bit-rate requirements, Internet streaming media delivery acceleration, network-aware partial caching, high packet loss rates, performance evaluation, available bandwidth, Application software, network-aware algorithms, simulations, Computer science, edge caches, long delays, Streaming media, Internet bandwidth, Internet, Acceleration]
OS support for P2P programming: a case for TPS
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Just as the remote procedure call (RPC) turned out to be a very effective OS abstraction in building client-server applications over LANs, type-based publish-subscribe (TPS) can be viewed as a high-level candidate abstraction for building peer-to-peer (P2P) applications over WANs. This paper relates our preliminary, though positive, experience of implementing and using TPS over JXTA, which can be viewed as the P2P counterpart to sockets. We show that, at least for P2P applications with the Java type model, TPS provides a high-level programming support that ensures type safety and encapsulation, without hampering the decoupled nature of these applications. Furthermore, the loss of flexibility (inherent to the use of any high level abstraction) and the performance overhead, are negligible with respect to the simplicity gained by using TPS.
[Encapsulation, encapsulation, Computer aided software engineering, Protocols, high-level programming support, type-based publish-subscribe, WANs, high-level candidate abstraction, type safety, Operating systems, network operating systems, peer-to-peer programming, JXTA, Libraries, Safety, distributed programming, Java, OS support, Peer to peer computing, information dissemination, Java type model, Sockets, Internet, information needs, data encapsulation]
The transition from asynchronous to synchronous system operation: an approach for distributed fault-tolerant systems
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Immediately after power-up, synchronous distributed systems need some time until essential timing properties, which are required to operate correctly, are established. We say that synchronous systems are initially in asynchronous operation. In this paper, we present an algorithm and architectural guidelines that assure the transition from asynchronous to synchronous operation within a bounded duration even in case of failures.
[Real time systems, failures, timing, distributed fault-tolerant systems, synchronous distributed systems, Distributed computing, Delay, architectural guidelines, Guidelines, Upper bound, bounded duration, timing properties, asynchronous to synchronous system operation transition, Fault tolerant systems, distributed algorithms, Computer architecture, fault tolerant computing, Timing, Clocks, Testing, algorithm]
An optimal strategy for anonymous communication protocols
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
For many Internet applications, the ability to protect the identity of participants in a distributed applications is critical. For such applications, a number of anonymous communication systems have been realized over the recent years. The effectiveness of these systems relies greatly on the way messages are routed among the participants. (We call this the route selection strategy.) In this paper we describe how to select routes so as to maximize the ability of the anonymous communication systems to protect anonymity To measure this ability, we define a metric (anonymity degree), and we design and evaluate an optimal route selection strategy that maximizes the anonymity degree of a system. Our analytical and experimental data shows that the anonymity degree may not always monotonically increase as the length of communication paths increase. We also found that variable path-length strategies perform better than fixed-length strategies.
[sender anonymity, Protocols, Data analysis, Internet applications, privacy, Electronic mail, Application software, Communication system security, distributed applications, Computer science, Privacy, security, anonymous communication systems, data privacy, route selection strategy, Internet, protocols, Protection, Electronic voting]
SYN-dog: sniffing SYN flooding sources
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Presents a simple and robust mechanism called SYN-dog to sniff SYN flooding sources. We install SYN-dog as a software agent at leaf routers that connect stub networks to the Internet. The statelessness and low computation overhead of SYN-dog make itself immune to any flooding attacks. The core mechanism of SYN-dog is based on the protocol behavior of TCP SYN-SYN/ACK pairs, and is an instance of the sequential change detection. To make SYN-dog insensitive to site and access pattern, a non-parametric cumulative sum (CUSUM) method is applied, thus making SYN-dog much more generally applicable and its deployment much easier. Due to its proximity to the flooding sources, SYN-dog can trace the flooding sources without resorting to expensive IP traceback.
[telecommunication security, Laboratories, leaf routers, Floods, SYN flooding sources, Computer crime, CUSUM method, TCPIP, Robustness, Software agents, IP networks, Availability, client-server systems, stub networks, statelessness, software agent, nonparametric cumulative sum, Access protocols, software agents, SYN-dog, low computation overhead, security of data, transport protocols, telecommunication network routing, protocol behavior, flooding attacks, Internet, statistical analysis, sequential change detection]
On the performance of group key agreement protocols
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Group key agreement (GKA) is a fundamental building block for securing peer group communication systems (GCS). Several group key agreement protocols were proposed in the past, all assuming an underlying group communication infrastructure. This paper presents a performance evaluation of 5 notable GKA protocols integrated with a reliable group communication system (Spread). They are: centralized group key distribution (CKD), Burmester-Desmedt (BD), Steer et al. (STR), group Diffie-Hellman GDH) and tree-based group Diffie-Hellman (TGDH).. We present concrete results obtained in experiments on local- and wide-area networks. Our analysis of these results offers insights into their relative scalability and practicality.
[Wide area networks, System testing, Protocols, Costs, group Diffie-Hellman agreement, performance evaluation, local area networks, Distributed computing, peer group communication systems, reliable group communication system, centralized group key distribution, tree-based group Diffie-Hellman agreement, Linux, group key agreement protocols, Concrete, Personal communication networks, Telecommunication network reliability, protocols, Local area networks]
Scalable and efficient update dissemination for distributed interactive applications
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Distributed interactive applications such as multiplayer games will become increasingly popular in wide area distributed systems. To provide the response time desired by users despite high and unpredictable communication latency in such systems, shared objects will be replicated or cached by clients that participate in the applications. Any updates to the shared objects will have to be disseminated to clients that actually use the objects to maintain consistency. We address the problem of efficient and scalable update dissemination in an environment where client interests can change dynamically and the number of multicast channels available for update dissemination is limited. We present a heuristic based algorithm that can group objects and clients in a way that it handles limited bandwidth resources. We show that our algorithm can produce better results than several algorithms that have been developed in the past for update dissemination.
[Heuristic algorithms, multiplayer games, client interests, shared object replication, Distributed computing, consistency, Delay, distributed interactive applications, scalable efficient update dissemination, Unicast, computer games, Bandwidth, Virtual reality, groupware, interactive systems, distributed object management, information dissemination, multicast channels, heuristic based algorithm, shared objects, Educational institutions, communication latency, wide area distributed systems, limited bandwidth resources, Multicast algorithms, response time, Games, Internet, shared object caching]
On-demand multicasting in ad-hoc networks:comparing AODV and ODMRP
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Multicasting can efficiently support a wide variety of applications that are characterized by a close degree of collaboration, typical for many mobile ad-hoc network (MANET) applications currently envisioned. To deal with the specific characteristics of MANETs, new multicast protocols that operate in an on-demand manner are being proposed and investigated. Our results show that a tree-based on-demand protocol is not necessarily the best choice. A mesh-based protocol can outperform tree-based protocols, due to the availability of alternative paths, which allow multicast datagrams to be delivered to all or most multicast receivers even if links fail.
[mobile radio, on-demand multicast routing protocol, Multicast protocols, tree-based on-demand protocol, Ad hoc networks, multicast datagrams, ad-hoc on-demand distance vector routing protocol, Mobile ad hoc networks, ad-hoc network, Intelligent networks, packet radio networks, mobile computing, MANET, Unicast, Network topology, on-demand multicasting, telecommunication network routing, Bidirectional control, Computer applications, multicast communication, Broadcasting, Routing protocols, protocols, mesh-based protocol]
ControlWare: a middleware architecture for feedback control of software performance
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Attainment of software performance assurances in open, largely unpredictable environments has recently become an important focus for real-time research. Unlike closed embedded systems, many contemporary distributed real-time applications operate in environments where offered load and available resources suffer considerable random fluctuations, thereby complicating the performance assurance problem. Feedback control theory has recently been identified as a promising analytic foundation for controlling performance of such unpredictable, poorly modeled software systems, the same way other engineering disciplines have used this theory for physical process control. In this paper we describe the design and implementation of ControlWare, a middleware QoS-control architecture based on control theory, motivated by the needs of performance-assured Internet services. It offers a new type of guarantees we call convergence guarantees that lie between hard and probabilistic guarantees. The efficacy of the architecture in achieving its QoS goals under realistic load conditions is demonstrated in the context of web server and proxy QoS management.
[Real time systems, Internet services, client-server systems, Fluctuations, Control system analysis, Software performance, Feedback control, quality of service, Application software, Middleware, convergence guarantees, ControlWare, Embedded system, QoS-control, distributed real-time applications, Computer architecture, Performance analysis, software performance assurances, middleware architecture, distributed object management]
A practical approach for 'zero' downtime in an operational information system
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
An operational information system (OIS) supports a real-time view of an organization's information critical to its logistical business operations. A central component of an OIS is an engine that integrates data events captured from distributed, remote sources in order to derive meaningful real-time views of current operations. This event derivation engine (EDE) continuously updates these views and also publishes them to a potentially large number of remote subscribers. The paper first describes a sample OIS and EDE in the context of an airline's operations. It then defines the performance and availability requirements to be met by this system, specifically focusing on the EDE component. One particular requirement for the EDE is that subscribers to its output events should not experience downtime due to EDE failures, crashes or increased processing loads. Toward this end, we develop and evaluate a practical technique for masking failures and for hiding the costs of recovery from EDE subscribers. This technique utilizes redundant EDEs that coordinate view replicas with a relaxed synchronous fault tolerance protocol. A combination of pre- and post-buffering of replicas is used to attain a solution that offers low response times (i.e., 'zero' downtime) while also preventing system failures in the presence of deterministic faults like 'ill-formed' messages. Parallelism realized via a cluster machine and application-specific techniques for reducing synchronization across replicas are used to scale a 'zero' downtime EDE to support the large number of subscribers it must service.
[Real time systems, workstation clusters, Delta Air Lines, deterministic faults, zero downtime, pre-buffering, parallelism, real-time view, crashes, system recovery, Engines, Delay, Information systems, availability requirements, distributed remote sources, Computer architecture, Parallel processing, Hardware, event derivation engine, continuous view updates, information systems, view replicas, logistical business operations, Availability, performance requirements, failures, cluster machine, subscribers, application-specific techniques, airline operations, synchronization reduction, data event integration, Airports, redundant event derivation engine, Computer crashes, increased processing loads, post-buffering, software fault tolerance, travel industry, operational information system, ill-formed messages, relaxed synchronous fault tolerance protocol]
Key trees and the security of interval multicast
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
A key tree is a distributed data structure of security keys that can be used by a group of users. In this paper we describe how any user in the group can use the different keys in the key tree to securely multicast data to different subgroups within the group. The cost of securely multicasting data to a subgroup whose users are "consecutive" is O(log n) encryptions, where n is the total number of users in the group. The cost of securely multicasting data to an arbitrary subgroup is O(n/2) encryptions. However this cost can be reduced to one encryption by introducing an additional key tree to the group.
[Tree data structures, Costs, key trees, Data security, distributed data structure, Multicast protocols, cryptography, arbitrary subgroup, Distributed computing, security of data, interval multicast security, multicast communication, Broadcasting, encryptions, System software, tree data structures, Cryptography, Computer security]
Distributed bandwidth broker for QoS multicast traffic
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
The differentiated services model is the emerging standard to provide Quality-of-Service (QoS) support for multimedia applications in the future Internet. This model involves bandwidth broker agents performing admission control and network configuration functionalities. A great deal of effort has been recently devoted to investigate viable approaches to the implementation of mechanisms that automatically perform the bandwidth broker functions, yet no standard policy has been proposed so far. In this paper we propose a distributed measurement-based protocol that performs admission control functionalities for multicast traffic in diff-serv networks. The protocol supports dynamic changes of the multicast group membership, operates on-demand, and supports the premium service. We prove that the proposed protocol performs an effective and efficient admission control function.
[Performance evaluation, differentiated services model, Phase measurement, multimedia applications, bandwidth broker agents, Quality of service, Length measurement, admission control, network configuration, Web and internet services, distributed bandwidth broker, Bandwidth, multicast communication, Traffic control, Communication system traffic control, multimedia communication, distributed measurement based protocol, Multicast protocols, quality of service, diff-serv networks, premium service, QoS multicast traffic, transport protocols, Admission control, dynamic multicast group membership changes, Internet, telecommunication traffic, on-demand operation]
Design of adaptive and reliable mobile agent communication protocols
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
This paper presents a mailbox-based scheme for designing flexible and adaptive message delivery protocols in mobile agent (MA) systems. The scheme associates each mobile agent with a mailbox while allowing the decoupling between them, i.e., a mobile agent can migrate to a new site without bringing its mailbox. By separating the concerns of locating the mailbox of a mobile agent and delivering a message to the agent, we obtain a large space of protocol design with flexibility. Using a three-dimensional model based on the scheme, we have developed a taxonomy of MA communication protocols, which not only covers, as special cases, several known MA message delivery protocols, but also allows for the design of new ones well suited for various application requirements. We describe such an efficient and adaptive protocol derived front the model. The protocol guarantees reliable delivery of messages to mobile agents. We analyze the design trade-offs and performance of the protocol, using an analytic model as well as extensive simulation experiments.
[Algorithm design and analysis, Target tracking, Wireless application protocol, Laboratories, Taxonomy, Mobile communication, mobile agent systems, software agents, three-dimensional model, Computer science, Design engineering, security of data, Mobile agents, reliable mobile agent communication protocols, adaptive message delivery protocols, Performance analysis, analytic model, protocols, mailbox-based scheme]
Incremental replication for mobility support in OBIWAN
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
The need for sharing is well known in a large number of distributed collaborative applications. These applications are difficult to develop for wide area (possibly mobile) networks because of slow and unreliable connections. For this purpose, we developed a platform called OBIWAN that: i) allows the application to decide, in run-time, the mechanism by which objects should be invoked, remote method invocation or invocation on a local replica, ii) allows incremental replication of large object graphs, iii) allows the creation of dynamic clusters of data, and iv) provides hooks for the application programmer to implement a set of application specific properties such as relaxed transactional support or updates dissemination. These mechanisms allow an application to deal with situations that frequently occur in a (mobile) wide-area network, such as disconnections and slow links: i) as long as objects needed by an application (or by an agent) are colocated, there is no need to be connected to the network, and ii) it is possible to replace, in run-time, remote by local invocations on replicas, thus improving the performance and adaptability of applications. The prototype is developed in Java, is very small and simple to use, the performance results are very encouraging, and existing applications can be easily modified to take advantage of OBIWAN.
[Java, Portable computers, wide area networks, application program interfaces, replicated databases, distributed collaborative applications, mobile environment, Mechanical factors, incremental replication, cooperative distributed applications, Programming profession, local replica, Runtime, mobile computing, Collaboration, Prototypes, remote method invocation, Collaborative work, remote procedure calls, Internet, Personal digital assistants, OBIWAN, distributed invocation]
ADAPTLOAD: effective balancing in clustered web servers under transient load conditions
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
We focus on adaptive policies for load balancing in clustered web servers, based on the size distribution of the requested documents. The proposed scheduling policy, ADAPTLOAD, adapts its balancing parameters on-the-fly, according to changes in the behavior of the customer population such as fluctuations in the intensity of arrivals or document popularity. Detailed performance comparisons via simulation using traces from the 1998 World Cup show that ADAPTLOAD is robust as it consistently outperforms traditional load balancing policies, especially under conditions of transient overload.
[workstation clusters, client-server systems, Fluctuations, load balancing, load sharing, Exponential distribution, Scalability, ADAPTLOAD, adaptive policy, Educational institutions, clustered web servers, Sun, Computer science, Processor scheduling, resource allocation, scheduling policy, Load management, Robustness, Internet, Web server, transient overload]
Cooperative computing for distributed embedded systems
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
The next generation of computing systems will be embedded, in a virtually unbounded number, and dynamically connected. The current software, network architectures, and their associated programming models are not suitable for this scenario. This paper presents a distributed computing model, Cooperative Computing, and the Smart Messages architecture for programming large networks of embedded systems. In Cooperative Computing, distributed applications are dynamic collections of migratory execution units, called Smart Messages, working to achieve a common goal. Virtually any user-defined distributed application can be implemented using our model. We present preliminary results for our prototype implementation as well as simulation results for two previously proposed applications for sensor networks, Directed Diffusion and SPIN, implemented using Smart Messages.
[Embedded computing, Computational modeling, distributed processing, Smart Messages architecture, Application software, network architectures, Distributed computing, distributed applications, Intelligent sensors, cooperative computing, Embedded system, migratory execution units, embedded systems, Computer architecture, Computer applications, computing systems, distributed computing model, Computer networks, embedding, Virtual prototyping, embedded]
dRBAC: distributed role-based access control for dynamic coalition environments
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
distributed role-based access control (dRBAC) is a scalable, decentralized trust-management and access-control mechanism for systems that span multiple administrative domains. dRBAC utilizes PKI identities to define trust domains, roles to define controlled activities, and role delegation across domains to represent permissions to these activities. The mapping of controlled actions to roles enables their namespaces to serve as policy roots. dRBAC distinguishes itself from previous approaches by providing three features: (1) third-party delegation of roles from outside a domain's namespace, relying upon an explicit delegation of assignment; (2) modulation of transferred permissions using scalar valued attributes associated with roles; and (3) continuous monitoring of trust relationships over long-lived interactions. The paper describes the dRBAC model and its scalable implementation using a graph approach to credential discovery and validation.
[Access control, graph approach, dynamic coalition environments, third-party delegation, distributed processing, policy roots, credential discovery, Control systems, trust domains, scalar valued attributes, controlled activities, scalable decentralized access control mechanism, Authorization, trust relationships, dRBAC, long-lived interactions, Web and internet services, authorisation, Permission, IP networks, Protection, continuous monitoring, Computerized monitoring, scalable decentralized trust-management mechanism, Computer science, distributed role-based access control, PKI identities, multiple administrative domains, role delegation, credential validation, Feeds, namespaces, transferred permissions]
RICA: a receiver-initiated approach for channel-adaptive on-demand routing in ad hoc mobile computing networks
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
To support truly peer-to-peer applications in ad hoc wireless mobile computing networks, a judicious and efficient ad hoc routing protocol is needed. Much research has been done on designing ad hoc routing protocols and some well known protocols are also being implemented in practical situations. However, one major drawback in existing state-of-the-art protocols, such as the AODV routing protocol, is that the time-varying nature of the wireless channels among the mobile terminals is ignored, let alone exploited. This can be a severe design shortcoming because the varying channel quality can lead to very poor overall route quality, in turn result in low data throughput. In this paper, by using a previously proposed adaptive channel coding and modulation scheme which allows a mobile terminal to dynamically adjust the data throughput via changing the amount of error protection incorporated, we devise a new receiver-initiated algorithm for ad hoc routing that dynamically changes the routes according to the channel conditions. Extensive simulation results indicate that our proposed protocol are more efficient in that shorter delays and higher rates are achieved.
[Peer to peer computing, Wireless application protocol, modulation, Throughput, Channel coding, Delay, channel-adaptive on-demand routing, peer-to-peer, adaptive channel coding, mobile computing, computer network management, RICA, telecommunication network routing, routing protocols, Computer networks, Routing protocols, protocols, ad hoc networks, Protection, Mobile computing, Modulation coding]
Convergence refinement
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Refinement tools such as compilers do not necessarily preserve fault-tolerance. That is, given a fault-tolerant program in a high-level language as input, the output of a compiler in a lower-level language will not necessarily be fault-tolerant. We identify a type of refinement, namely "convergence refinement\
[Algorithm design and analysis, finite automata, stabilizing implementations, stabilization, refinement calculus, 3-state stabilizing token-ring system, high-level language, wrapper, fault-tolerant program, program compilers, compilers, Convergence, Fault diagnosis, Fault tolerance, Information science, Program processors, graybox design, Fault tolerant systems, convergence refinement, stability, client-server systems, High level languages, software fault tolerance, Transformers, formal design, Concrete, refinement tools, lower-level language]
A pluggable service-to-service communication mechanism for VNA architecture
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
This paper proposes a middleware for home networks, called Virtual Networked Appliance (VNA) architecture, in which the service description method and the Service to Service (S2S) communication mechanism are separated in an orthogonal way. Through the separation, VNA architecture solved the following two problems of existing middleware technologies: aspect violation and middleware fragmentation. In this paper, we first clarify the two problems and their relationship. Then, we describe the proposed middleware architecture as a solution from the viewpoint of the overall configuration and the S2S communication mechanism.
[client-server systems, Home automation, middleware fragmentation, Access protocols, service description method, S2S communication mechanism, Middleware, Programming profession, Home computing, Home appliances, VNA architecture, virtual networked appliance architecture, home networks, Computer architecture, pluggable service-to-service communication mechanism, aspect violation, Computer networks, Timing, protocols, Usability, service to service communication mechanism, distributed object management, middleware]
Clustering algorithms for content-based publication-subscription systems
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
We consider efficient communication schemes based on both network-supported and application-level multicast techniques for content-based publication-subscription systems. We show that the communication costs depend heavily on the network configurations, distribution of publications and subscriptions. We devise new algorithms and adapt existing partitional data clustering algorithms. These algorithms can be used to determine multicast groups with as much commonality as possible, based on the totality of subscribers' interests. They perform well in the context of highly heterogeneous subscriptions, and they also scale well. An efficiency of 60% to 80% with respect to the ideal solution can be achieved with a small number of multicast groups (less than 100 in our experiments). Some of these same concepts can be applied to match publications to subscribers in real-time, and also to determine dynamically whether to unicast, multicast or broadcast information about the events over the network to the matched subscribers. We demonstrate the quality of our algorithms via simulation experiments.
[broadcast, Costs, electronic publishing, partitional data clustering algorithms, Subscriptions, simulation, content-based publication-subscription systems, Electronic mail, Discrete event simulation, unicast, network-supported multicast techniques, Unicast, Clustering algorithms, multicast communication, communication costs, Broadcasting, Stock markets, efficient communication schemes, information dissemination, application-level multicast techniques, Partitioning algorithms, subscription distribution, content-based retrieval, Multicast algorithms, network configurations, pattern clustering, publication distribution, multicast groups]
Reclaiming space from duplicate files in a serverless distributed file system
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
The Farsite distributed file system provides availability by replicating each file onto multiple desktop computers. Since this replication consumes significant storage space, it is important to reclaim used space where possible. Measurement of over 500 desktop file systems shows that nearly half of all consumed space is occupied by duplicate files. We present a mechanism to reclaim space from this incidental duplication to make it available for controlled file replication. Our mechanism includes: (1) convergent encryption, which enables duplicate files to be coalesced into the space of a single file, even if the files are encrypted with different users' keys; and (2) SALAD, a Self-Arranging Lossy Associative Database for aggregating file content and location information in a decentralized, scalable, fault-tolerant manner. Large-scale simulation experiments show that the duplicate-file coalescing system is scalable, highly effective, and fault-tolerant.
[desktop file systems, location information, Farsite, File servers, availability, Distributed computing, self-organising storage, Fault diagnosis, storage management, SALAD, storage space reclamation, File systems, Databases, network operating systems, decentralized scalable system, Large-scale systems, Cryptography, file content aggregation, Availability, controlled file replication, duplicate files, replicated databases, fault-tolerant system, large-scale simulation, Extraterrestrial measurements, cryptography, Self-Arranging Lossy Associative Database, Secure storage, software fault tolerance, duplicate-file coalescing system, serverless distributed file system, content-addressable storage, convergent encryption]
D-Stampede: distributed programming system for ubiquitous computing
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
We focus on an important problem in ubiquitous computing, namely, programming support for the distributed heterogeneous computing elements that make up this environment. We address the interactive, dynamic, and stream-oriented nature of this application class and develop appropriate computational abstractions in the D-Stampede distributed programming system. The key features of D-Stampede include indexing data streams temporally, correlating different data streams temporally, performing automatic distributed garbage collection of unnecessary stream data, supporting high performance by exploiting hardware parallelism where available, supporting platform and language heterogeneity, and dealing with application level dynamism. We discuss the features of D-Stampede, the programming ease it affords, and its performance.
[Drives, temporal data stream correlation, programming support, application level dynamism, ubiquitous computing, Distributed computing, storage management, interactive application class, Space technology, computational abstractions, Parallel processing, Hardware, Dynamic programming, temporal data stream indexing, high performance, language heterogeneity, distributed programming, Pervasive computing, automatic distributed garbage collection, Ubiquitous computing, Educational institutions, stream-oriented application class, D-Stampede distributed programming system, Middleware, dynamic application class, platform heterogeneity, hardware parallelism, distributed heterogeneous computing elements]
Design of a policy-driven middleware for secure distributed collaboration
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
We present the execution model of policy-driven middleware for building secure distributed collaboration systems from their high level specifications. Our specification model supports nested collaboration activities, and uses role-based security policies and event count based coordination specification. From the specifications of a collaboration environment, appropriate policy modules are derived for enforcing security and coordination requirements. Policy-driven distributed middleware provides services to users to join roles in an activity, perform role specific operations, or create new activities. We describe the design challenges for middleware and present the run-time structures and protocols supported by it for creating activities, roles, and objects.
[Real time systems, secure distributed collaboration systems, Runtime environment, runtime structures, Protocols, Collaborative software, role-based security policies, Buildings, high level specifications, event count based coordination specification, Middleware, formal specification, Computer science, security of data, groupware, nested collaboration activities, Collaborative work, policy modules, protocols, National security, Online Communities/Technical Collaboration, distributed object management, policy-driven middleware]
Cost-effective switching fabrics with distributed control for scalable routers
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
This paper deals with scalable switching fabrics for high-performance routers with large numbers of ports for connecting external links operating at various speeds to arrive at aggregate rates up to multi-terabits per second. The proposed switching fabrics employ no centralized scheduling and consist of small routing units (RUs), which are interconnected by multistage-based connecting components (CCs) in accordance with grid structures, with routing decisions made by RUs and CCs individually in a simple, distributed manner. They are referred to as grid-oriented, multistage-connected RUs, dubbed GRM. With distributed routing, GMR enjoys good scalability and low hardware complexity. It is found, based on our extensive simulation, that GMR outperforms not only their crossbar counterparts for small sizes, but also their compatible designs aiming at large sized construction (built from multiple stages of small crossbars), despite its lower hardware complexity. Two types of chips are sufficient to permit any sized construction; one for RUs and another for CCs. The proposed switching fabrics are cost-effective, readily suitable for scalable routers.
[Packet switching, Scalability, distributed control, performance evaluation, distributed processing, Routing, scalable switching, multistage-based connecting components, Distributed computing, scalability, computer network management, Aggregates, routing units, telecommunication network routing, Distributed control, Fabrics, Hardware, scalable routers, Internet, Joining processes, Asynchronous transfer mode]
Routing indices for peer-to-peer systems
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Finding information in a peer-to-peer system currently requires either a costly and vulnerable central index, or flooding the network with queries. We introduce the concept of routing indices (RIs), which allow nodes to forward queries to neighbors that are more likely to have answers. If a node cannot answer a query, it forwards the query to a subset of its neighbors, based on its local RI, rather than by selecting neighbors at random or by flooding the network by forwarding the query to all neighbors. We present three RI schemes: the compound, the hop-count, and the exponential routing indices. We evaluate their performance via simulations, and find that RIs can improve performance by one or two orders of magnitude vs. a flooding-based system, and by up to 100% vs. a random forwarding system. We also discuss the tradeoffs between the different RI schemes and highlight the effects of key design variables on system performance.
[Costs, Peer to peer computing, random forwarding system, Routing, Distributed computing, routing indices, query processing, compound routing index, flooding-based system, hop-count routing index, Computer hacking, exponential routing index, System performance, peer-to-peer systems, telecommunication network routing, distributed databases, Search engines, Robustness, Web search]
Power-aware prefetch in mobile environments
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Most of the prefetch techniques used in the current cache management schemes do not consider the power constraints of the mobile clients and other factors such as the size of the data items, the data access rate, and the data update rate. We address these issues by proposing a power-aware prefetch scheme, called the value-based adaptive prefetch (VAP) scheme. The VAP scheme defines a value function which can optimize the prefetch cost to achieve better performance. Also, VAP dynamically adjusts the number of prefetches based on the current energy level to prolong the system running time. As stretch is widely adopted as a performance metric for variable-size data requests, we show by analysis that the proposed algorithm can indeed achieve the optimal performance in terms of stretch when power consumption is considered. Simulation results demonstrate that our algorithm significantly outperforms existing prefetching algorithms under various scenarios.
[Measurement, Energy consumption, power constraints, cache storage, power consumption, cache management, performance metric, mobile computing, data access rate, Bandwidth, parameter estimation, Energy states, Performance analysis, power-aware prefetch, mobile environments, client-server systems, Prefetching, Computational modeling, optimal performance, value-based adaptive prefetch scheme, data update rate, Handheld computers, mobile clients, variable-size data requests, Energy management, Mobile computing]
Universal interaction with networked home appliances
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
We propose universal interaction for networked home appliances, which is a simple mechanism to fill the gap between traditional user interface systems and advanced user interaction devices. Our system enables us to control appliances in a uniform way at any place, and the system allows us to select suitable input and output devices according to our preferences and situations. Also, the devices can be changed dynamically according to a user's preferences.
[home automation, advanced user interaction devices, Home appliances, client-server systems, universal interaction, networked home appliances, mobile computing, graphical user interfaces, traditional user interface systems, ubiquitous computing, domestic appliances]
A self-stabilizing protocol for pipelined PIF in tree networks
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Self-stabilization is a promising paradigm for achieving fault-tolerance of distributed systems. A self-stabilizing protocol can converge to its intended behavior even when it starts from any system configuration, and, thus, can tolerate any type and any number of transient faults. The PIF (propagation of information with feedback) scheme in a tree network allows the root process to broadcast its information to all other processes and to collect their responses. Many distributed systems utilize the PIF scheme as a fundamental communication scheme. This paper first formalizes the pipelined PIF in tree networks, and proposes a self-stabilizing protocol for the pipelined PIF. The protocol applies the PIF to a sequence of information in a pipelined fashion. The protocol has stabilizing time of O(h) (where h is the height of the tree network). After stabilization, it completes each PIF in O(h) asynchronous rounds and has throughput of O(1). Moreover, the protocol achieves fault-containment: for a complete binary tree network, its expected stabilizing time from 1-faulty configurations is O(1).
[Protocols, pipelined PIF, Throughput, information broadcast, communication complexity, Distributed computing, communication scheme, Convergence, Intelligent networks, Fault tolerant systems, Feedback, pipelined propagation of information with feedback, Broadcasting, distributed systems, throughput, complete binary tree network, protocols, stability, stabilizing time, fault tolerance, root process, trees (mathematics), asynchronous rounds, Pipeline processing, Binary trees, fault containment, fault tolerant computing, tree network, pipeline processing, self-stabilizing protocol]
Unification of replication and transaction processing in three-tier architectures
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
In this paper we describe a software infrastructure that unifies replication and transaction processing in three-tier architectures and, thus, provides high availability and fault tolerance for enterprise applications. The infrastructure is based on the Fault Tolerant CORBA and CORBA Object Transaction Service standards, and works with commercial-off-the-shelf application servers and database systems. The infrastructure replicates the application servers to protect the business logic processing. In addition, it replicates the transaction coordinator which renders the two-phase commit protocol non-blocking and, thus, avoids potentially long service disruptions caused by coordinator failure. The infrastructure handles the interactions between the application servers and the database servers through replicated gateways that prevent duplicate requests from reaching the database servers. The infrastructure implements client-side automatic failover mechanisms, which guarantees that clients know the outcome of the requests that they have made. The infrastructure starts the transactions at the application servers, and retries aborted transactions, caused by process or communication failures, automatically on the behalf of the clients.
[Availability, transaction processing, replication, network servers, replicated databases, fault tolerance, software infrastructure, business logic processing, Transaction databases, Application software, CORBA, Fault tolerance, Fault tolerant systems, Computer architecture, database servers, Database systems, fault tolerant computing, application servers, Logic, Protection, distributed object management, Business]
On peer-to-peer media streaming
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
In this paper, we study a peer-to-peer media streaming system with the following characteristics: (1) its streaming capacity grows dynamically; (2) peers do not exhibit server-like behavior; (3) peers are heterogeneous in their bandwidth contribution; and (4) each streaming session may involve multiple supplying peers. Based on these characteristics, we investigate two problems: (1) how to assign media data to multiple supplying peers in one streaming session and (2) how to quickly amplify the system's total streaming capacity. Our solution to the first problem is an optimal media data assignment algorithm OTS/sub p2p/, which results in minimum buffering delay in the consequent streaming session. Our solution to the second problem is a distributed differentiated admission control protocol DAC/sub p2p/. By differentiating between requesting peers with different outbound bandwidth, DAC/sub p2p/ achieves fast system capacity amplification; benefits all requesting peers in admission rate, waiting time, and buffering delay; and creates an incentive for peers to offer their truly available out-bound bandwidth.
[Protocols, buffer storage, waiting time, Peer to peer computing, Delay effects, distributed differentiated admission control protocol, Nonhomogeneous media, peer-to-peer media streaming system, Distributed computing, multiple supplying peers, electronic data interchange, minimum buffering delay, streaming capacity, requesting peers, Admission control, outbound bandwidth, Bandwidth, optimal media data assignment algorithm, admission rate, Streaming media, protocols, Joining processes, bandwidth contribution]
The complexity of adding failsafe fault-tolerance
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
In this paper, we focus our attention on the problem of automating the addition of failsafe fault-tolerance where fault-tolerance is added to an existing (fault-intolerant) program. A failsafe fault-tolerant program satisfies its specification (including safety and liveness) in the absence of faults. And, in the presence of faults, it satisfies its safety specification. We present a somewhat unexpected result that, in general, the problem of adding failsafe fault-tolerance in distributed programs is NP-hard. Towards this end, we reduce the 3-SAT problem to the problem of adding failsafe fault-tolerance. We also identify a class of specifications, monotonic specifications and a class of programs, monotonic programs. Given a (positive) monotonic specification and a (negative) monotonic program, we show that failsafe fault-tolerance can be added in polynomial time. We note that the monotonicity restrictions are met for commonly encountered problems such as Byzantine agreement, distributed consensus, and atomic commitment. Finally, we argue that the restrictions on the specifications and programs are necessary to add failsafe fault-tolerance in polynomial time; we prove that if only one of these conditions is satisfied, the addition of failsafe fault-tolerance is still NP-hard.
[Algorithm design and analysis, complexity, formal specification, Fault diagnosis, Fault tolerance, automation, Fault tolerant systems, Polynomials, polynomial time, Safety, Contracts, distributed programming, distributed consensus, failsafe fault tolerance addition, atomic commitment, Automation, Engineering profession, Byzantine agreement, monotonic programs, safety specification, 3-SAT problem, software fault tolerance, distributed programs, Computer science, NP-hard problem, monotonic specifications, computational complexity]
Architecture of a large-scale location service
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Location-aware services are a promising way of exploiting the special possibilities created by ubiquitous mobile devices and wireless communication. Advanced location-aware applications will require highly accurate information about the geographic location of mobile objects and functionality that goes beyond simply querying the user's position, for example determining all mobile objects inside a certain geographic area. In this paper, we propose a generic large-scale location service, which has been designed with the goal of managing the highly dynamic location information for a large number of mobile objects, thus providing a common infrastructure that can be employed by location-aware applications. We propose a hierarchical distributed architecture, which can efficiently process these queries in a scalable way. To be able to deal with the frequent updates and queries resulting from highly dynamic location information, we propose a data storage component, which makes use of a main memory database.
[storage allocation, large-scale location service architecture, Memory, Spatial databases, common infrastructure, Information management, Distributed computing, Nearest neighbor searches, wireless communication, Network servers, mobile objects, hierarchical distributed architecture, data storage component, Prototypes, computer architecture, Computer architecture, Database systems, Large-scale systems, main memory database, location-aware services, geographic location]
Scheduling real-time data items in multiple channels and multiple receivers environments
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
The problem of transmitting data items with timing constraint in the multiple broadcast channels where clients equipped with multiple receivers is studied. In our approach, the data items are divided into two sets, the broadcast data set and the on-demand data set. The data items in the broadcast data set is periodically broadcast on the broadcast channels while the data items in the on-demand data set are transmitted when they are requested. The server offline allocates the data items in the broadcast data set to the multiple broadcast channels. The clients retrieved data items from the broadcast channels are guaranteed to receive them within the timing constraint. Once the desired data item is not included in the broadcast channels, clients make requests associated with deadlines to the server. By using the bandwidth remaining for the on-demand mode, the server broadcasts the requested data items in an online fashion. The intention of our work is to serve as many requests as possible.
[radio data systems, decision theory, clients, Roads, timing constraint, multiple channels, database management systems, Fault tolerance, Databases, Bandwidth, Broadcasting, scheduling, broadcast scheduling, multiple receivers, wireless data delivery model, client-server systems, fault tolerance, Information retrieval, Scheduling algorithm, Computer science, broadcast data set, Processor scheduling, on-demand data set, real-time systems, real-time data items, Timing, broadcast channels]
Dynamic QoS-aware multimedia service configuration in ubiquitous computing environments
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Ubiquitous computing promotes the proliferation of various stationary, embedded and mobile devices interconnected by heterogeneous networks. It leads to a highly dynamic distributed system with many devices and services coming and going frequently. Many emerging distributed multimedia applications are being deployed in such a computing environment. In order to make the experience for a user truly seamless and to provide soft performance guarantees, we must meet the following challenges: (1) users should be able to perform tasks continuously, despite changes of resources, devices and locations; (2) users should be able to efficiently utilize all accessible resources within runtime environments to receive the best possible Quality-of-Service (QoS). In this paper, we propose an integrated QoS-aware service configuration model to address the above problems. The configuration model includes two tiers: (1) service composition tier, which is responsible for choosing and composing current available service components appropriately and coordinating arbitrary interactions between them to achieve the user's objectives; and (2) service distribution tier which is responsible for dividing an application into several partitions and distributing them to different available devices appropriately. Our initial experimental results based on both prototype and simulations show the soundness of our model and algorithms.
[Pervasive computing, Runtime environment, heterogeneous networks, Quality-of-Service, NASA, Multimedia computing, Quality of service, distributed processing, Ubiquitous computing, service configuration model, quality of service, multimedia computing, ubiquitous computing, Distributed computing, Intelligent networks, QoS, Computer networks, Contracts]
Migratory TCP: connection migration for service continuity in the Internet
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Today's Internet services are commonly built over TCP, the standard Internet connection-oriented reliable transport protocol. The endpoint naming scheme of TCP, based on network layer (IP) addresses, creates an implicit binding between a service and the IP address of a server providing it, throughout the lifetime of a client connection. This makes a TCP client prone to all adverse conditions that may affect the server endpoint or the internetwork in between, after the connection is established: congestion or failure in the network, server overloaded, failed or under DoS attack. Studies that quantify the effects of network stability and route availability demonstrate that connectivity failures can significantly impact Internet services. As a result, although highly available servers can be deployed, sustaining continuous service remains a problem. We propose cooperative service model, in which a pool of similar servers, possibly geographically distributed across the Internet, cooperate in sustaining a service by migration of client connections within the pool. The control traffic between servers, needed to support migrated connections, can be carried either over the Internet or over a private network. From client's viewpoint, at any point during the lifetime of its service session, the remote endpoint of its connection may transparently migrate between servers.
[Transport protocols, Availability, network stability, Stability, Computer crime, Network servers, transport protocols, Web and internet services, service continuity, TCPIP, route availability, TCP migration, Communication system traffic control, connection migration, Internet, IP networks, Web server]
Improving the scalability of fault-tolerant database clusters
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Replication has become a central element in modem information systems playing a dual role: increase availability and enhance scalability. Unfortunately, most existing protocols increase availability at the cost of scalability; This paper presents architecture, implementation and performance of a middleware based replication tool that provides both availability and better scalability than existing systems. Main characteristics are the usage of specialized broadcast primitives and efficient data propagation.
[Availability, client-server systems, Protocols, Costs, Scalability, middleware based replication tool, Middleware, scalability, Information systems, fault-tolerant database clusters, Fault tolerance, Databases, Broadcasting, Modems, fault tolerant computing, information systems, protocols]
Dynamic data broadcasting with traffic awareness
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Data dissemination has significantly served as a scalable data delivery mechanism in wireless networks. However, even though the broadcast traffic has the nature of dynamic changes, most previous research efforts were elaborated upon the premise of static workloads and access patterns without having proper traffic awareness. In this paper, we address the existence of client impatience and accordingly devise an online traffic awareness mechanism based on a novel selective deferment and reflection (SDR) technique to estimate the dynamic workloads and access patterns in a granularity of a broadcast cycle. In comparison with prior probing and feedback approaches, our design is of practical usefulness in that it has low complexity and is light-weight without performance degradation. With various dynamic traffic scenarios, the experimental results show that with an increasing/decreasing workload, the real access frequency distribution is bounded by two specific estimated distributions. This fact in turn suggests us to employ a trigonometric tuning method to further enhance the estimation. In addition, we examine that the mean difference between the estimated access frequency distribution and the real one is very small, consequently indicating the feasibility and reliability of our proposed data broadcast mechanism with traffic awareness.
[Energy consumption, client-server systems, message passing, selective deferment reflection technique, dynamic workloads, Telecommunication traffic, File servers, data dissemination, access patterns, Frequency estimation, dynamic data broadcasting, Electronic mail, client impatience, Degradation, mobile computing, traffic awareness, Wireless networks, Feedback, Bandwidth, Broadcasting, Internet, trigonometric tuning, wireless network, telecommunication traffic]
A working-set approach to reduce the download-execution time of mobile programs
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
The on-demand downloading of application code over a wireless network is an emerging paradigm for the distributed environment of mobile computing devices. In this environment, a user must wait while a mobile computing device fetches the application code over the network prior to execution. To reduce this downloading latency, we have developed a download approach based on working sets, and have designed and implemented a working-set generator that defines working sets by focusing on the intervals during which a program refers to junctions, variables, and constants. The program invocation times with this approach, simulated using trace information obtained from actual program behavior on Linux, have been compared with those of all-at-once, page, class, and symbol download approaches. This simulation showed that the working-set approach can reduce the download-execution time compared to that with the four other approaches.
[Java, downloading latency, Computational modeling, Distributed computing, Delay, program invocation times, concurrency, working sets, mobile computing, Wireless networks, Linux, downloading, Bandwidth, Computer networks, IP networks, wireless network, Personal digital assistants, Mobile computing, distributed programming]
A new document placement scheme for cooperative caching on the Internet
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Most existing work on cooperative caching has been-focused on serving misses collaboratively. Very few have studied the effect of cooperation on document placement schemes and its potential enhancements on cache hit ratio and latency reduction. In this paper we propose a new document placement scheme, called the Expiration Age based scheme (EA scheme), which takes into account the contentions at individual caches in order to limit the replication of documents within a cache group and increase document hit ratio. The main idea of this new scheme is to view the aggregate disk space of the cache group as a global resource of the group, and uses the concept of cache expiration age to measure the contention of individual caches. The decision of whether to cache a document at a proxy is made collectively, among the caches that already have a copy of this document. The experiments show that the EA scheme yields higher hit rates and better response times compared to the existing document placement schemes used in most of the caching proxies.
[latency reduction, Protocols, Cooperative caching, document placement schemes, Internet Cache Protocol, Telecommunication traffic, information retrieval, Educational institutions, Distributed computing, cache hit ratio, Delay, EA scheme, storage management, Aggregates, ICP, Collaborative work, Internet, protocols, cooperative caching, expiration age]
The totem redundant ring protocol
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Group communication protocols greatly simplify the design of fault-tolerant distributed systems. Most of those protocols focus on node redundancy rather than on network redundancy. The totem redundant ring protocol allows the use of multiple redundant local-area networks. The partial or total failure of a network remains transparent to the application processes. The distributed system remains operational while an administrator reacts to an alarm raised by the totem redundant ring protocol. The user can choose between active, passive and active-passive replication of the network.
[multiple redundant local-area networks, passive replication, Protocols, Redundancy, Switches, active replication, local area networks, Application software, group communication protocols, Communication switching, Fault tolerant systems, totem redundant ring protocol, Broadcasting, active-passive replication, fault tolerant computing, redundancy, Telecommunication network reliability, protocols, fault-tolerant distributed systems, Local area networks, Contracts]
Balancing performance, energy, and quality in pervasive computing
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
We describe Spectra, a remote execution system for battery-powered clients used in pervasive computing. Spectra enables applications to combine the mobility of small devices with the greater processing power of static compute servers. Spectra is self-tuning: it monitors both application resource usage and the availability of resources in the environment, and dynamically determines how and where to execute application components. In making this determination, Spectra balances the competing goals of performance, energy conservation, and application quality. We have validated Spectra's approach on the Compaq Itsy v2.2 and IBM ThinkPad 560X using a speech recognizer a document preparation system, and a natural language translator. Our results confirm that Spectra almost always selects the best execution plan, and that its few suboptimal choices are very close to optimal.
[Pervasive computing, Availability, Compaq Itsy v2.2, client-server systems, Costs, battery powered, Natural languages, remote execution system, Spectra, Batteries, pervasive computing, Distributed computing, Network servers, mobile computing, Energy conservation, document preparation system, Speech recognition, natural language translator, Computer networks]
Extending RMI to support dynamic reconfiguration of distributed systems
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
By using dynamic reconfiguration, the flexibility, adaptability, and extensibility of distributed systems are enhanced. Treating interactions among components during dynamic reconfiguration is still one of the most crucial problems in this research area. This challenge should be met by middleware design and development. However, current standard middleware offers little support for dynamic reconfiguration of distributed systems. In this paper, we describe an extended Java RMI that supports efficiently dynamic reconfiguration of distributed systems. This extended Java RMI can automatically monitor and manipulate invocations between components during dynamic reconfiguration. Moreover, the extended Java RMI can automatically switch invocations from remote to local and vice versa.
[Java, application program interfaces, Computerized monitoring, extended Java RMI, Switches, Vehicle dynamics, Middleware, Distributed computing, Telematics, remote procedure calls, distributed systems, dynamic reconfiguration, component interactions, Personal digital assistants, Web server, Remote monitoring, middleware]
Routing through the mist: privacy preserving communication in ubiquitous computing environments
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Ubiquitous computing is poised to revolutionize the way we compute and interact with each other. However, unless privacy concerns are taken into account early in the design process, we will end up creating a very effective distributed surveillance system, which would be a dream come true for electronic stalkers and "big brothers". We present a protocol, which preserves the privacy of users and keeps their communication anonymous. In effect, we create a "mist" that conceals users from the system and other users. Yet, users will still be able to enjoy seamless interaction with services and other entities that wander within the ubiquitous computing environment.
[Pervasive computing, Process design, computer networks, Access protocols, Routing, Ubiquitous computing, privacy, anonymous communication, ubiquitous computing, Distributed computing, Intelligent sensors, Computer science, Privacy, mobile computing, security, protocol, security of data, Physics computing, telecommunication network routing, Mist Routers, data privacy, protocols, authentication]
Adaptive and virtual reconfigurations for effective dynamic job scheduling in cluster systems
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
In a cluster system with dynamic load sharing support, a job submission or migration to a workstation is determined by the availability of CPU and memory resources of the workstation at the time. In such a system, a small number of running jobs with unexpectedly large memory allocation requirements may significantly increase the queuing delay times of the rest of jobs with normal memory requirements, slowing down executions of individual jobs and decreasing the system throughput. We call this phenomenon as the job blocking problem because the big jobs block the execution pace of majority jobs in the cluster. We propose a software method incorporating with dynamic load sharing, which adaptively reserves a small set of workstations through virtual cluster reconfiguration to provide special services to the jobs demanding large memory allocations. This policy implies the principle of shortest-remaining-processing-time policy. As soon as the blocking problem is resolved by the reconfiguration, the system will adaptively switch back to the normal load sharing state. We present three contributions in this study. (1) the conditions to cause the job blocking problem; (2) the adaptive software method in a dynamic load sharing system; and (3) trace-driven simulations. We show that our method can effectively improve the cluster computing performance by quickly resolving the job blocking problem. The effectiveness and performance insights are also analytically verified.
[Availability, workstation clusters, virtual reality, adaptive reconfigurations, Optimal scheduling, Switches, Dynamic scheduling, Educational institutions, workstation, Job design, Delay, processor scheduling, dynamic load sharing system, Computer science, Processor scheduling, resource allocation, Workstations, job blocking problem, cluster system, memory resources, job scheduling, virtual reconfigurations]
Agent chaining: an approach to dynamic mobile agent planning
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
The distributed agent concept has become a new computing paradigm in Internet distributed computing, including mobile computing. Mobile agent planning is one of the most important techniques for completing a given task efficiently. The static planning technique may not be the best approach in real network environments. For better performance, it is necessary that mobile agents be more sensitive to the network conditions. We propose a dynamic planning algorithm, named n-ary agent chaining, which is based on static mobile agent planning. Mobile agents can change their itinerary dynamically according to current network status using the proposed algorithm. The proposed algorithm also takes into account the locality of target nodes on the network. Thus, with a properly chosen locality factor it can adapt to realistic network situations. Agents reproduced from the original one, named cloned agents, process the unprocessed nodes in the proposed algorithm. Since the turn-around time can be calculated mathematically with known network statistics before launching the agents, the proposed algorithm is suitable for agent problem domains with deadline constraints.
[multi-agent systems, Heuristic algorithms, Telecommunication traffic, Distributed computing, planning (artificial intelligence), mobile computing, dynamic mobile agent planning, cloned agents, Mobile agents, Distributed databases, distributed databases, turn-around time, Fluctuations, static planning technique, n-ary agent chaining, deadline constraints, Information retrieval, distributed agent concept, Internet distributed computing, agent reproduction technique, software agents, Computer science, locality factor, Internet, realistic network situations, Mobile computing]
Dining philosophers that tolerate malicious crashes
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
We present a solution to the problem of dining philosophers. Our solution tolerates malicious crashes. In a malicious crash the failed process behaves arbitrarily for a finite time and then ceases all operation undetectably to other processes. The tolerance of our solution is achieved by the combination of stabilization and crash failure locality. Stabilization allows our program to recover from an arbitrary state. Crash failure locality ensures that only a limited number of processes are affected by a process crash. The crash failure locality of our solution is optimal. Finally, we argue that the malicious crash fault model and its extensions are worthy of further study as they admit tolerances that are not achieved under stronger fault models and are unnecessary under weaker fault models.
[malicious crash tolerance, stabilization, Computer crashes, program recovery, Distributed computing, system recovery, software fault tolerance, dining philosophers problem, failed process, fault models, resource allocation, concurrency control, stability, crash failure locality]
Bidding for storage space in a peer-to-peer data preservation system
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Digital archives protect important data collections from failures by making multiple copies at other archives, so that there are always several good copies of a collection. In a cooperative replication network sites "trade" space, so that each site contributes storage resources to the system and uses storage resources at other sites. Here, we examine bid trading: a mechanism where sites conduct auctions to determine who to trade with. A local site wishing to make a copy of a collection announces how much remote space is needed, and accepts bids for how much of its own space the local site must "pay" to acquire that remote space. We examine the best policies for determining when to call auctions and how much to bid, as well as the effects of "maverick" sites that attempt to subvert the bidding system. Simulations of auction and trading sessions indicate that bid trading can allow sites to achieve higher reliability than the alternative: a system where sites trade equal amounts of space without bidding.
[storage allocation, data collection protection, failures, Peer to peer computing, Decision making, Government, remote space, reliability, maverick sites, digital archives, simulations, Computer science, bid trading, storage management, records management, peer-to-peer data preservation system, cooperative replication network, multiple copies, Protection, storage space bidding, auctions]
Impact of network density on data aggregation in wireless sensor networks
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
In-network data aggregation is essential for wireless sensor networks where energy resources are limited. In a previously proposed data dissemination scheme (directed diffusion with opportunistic aggregation), data is opportunistically aggregated at intermediate nodes on a low-latency tree. In this paper, we explore and evaluate greedy aggregation, a novel approach that adjusts aggregation points to increase the amount of path sharing, reducing energy consumption. Our preliminary results suggest that, under investigated scenarios, greedy aggregation can achieve up to 45% energy savings over opportunistic aggregation in high-density networks without adversely impacting latency or robustness.
[greedy aggregation, Energy consumption, Costs, radio data systems, wireless sensor networks, latency, robustness, path sharing, data aggregation, aggregation points, Delay, radio access networks, Intelligent networks, Wireless sensor networks, Energy resources, network density, Intersymbol interference, data dissemination scheme, energy resources, Robustness, Energy efficiency, Large-scale systems]
Partial database replication using epidemic communication
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Data replication in distributed databases has been investigated extensively with the hope that it will improve performance, reliability, and availability. However the growth of the Internet has shown us that current replica management do not work well when the replicas are connected by an unreliable network, subject to congestion and dynamic topology changes. In this paper we present a replica update protocol that handles an adaptive partial replication scheme on such a network.
[Availability, dynamic topology, Protocols, replicated databases, reliability, performance evaluation, Transaction databases, availability, Distributed computing, epidemic communication, Computer science, adaptive partial replication scheme, Network servers, Network topology, Distributed databases, replica update protocol, distributed databases, Internet, IP networks, protocols, Joining processes, congestion, partial database replication]
Conditional messaging: extending reliable messaging with application conditions
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Standard messaging middleware guarantees the delivery of messages to intermediary destinations like message queues, but does not guarantee the receipt or the processing of a message by final recipients. Conditional messaging is an extension to standard messaging middleware that addresses this shortcoming by allowing an application to define, monitor, and evaluate various conditions on messages, such as time constraints on the receipt or the processing of a message by a set of final recipients. In this paper, we introduce the notion of conditional messaging, and present the design and implementation of a flexible and reliable system that supports conditional messaging for use in Java 2 Enterprise Edition and message queuing environments. Our solution uniquely shifts the responsibilities for implementing the management of conditions on messages from the application to the middleware. We further discuss the grouping of multiple conditional messages into atomic units-of-work, which can also integrate requests to transactional resources like distributed objects using object middleware. Conditional messaging serves to implement various kinds of backward dependencies for distributed object transactions that integrate messaging.
[transaction processing, time constraints, electronic messaging, messaging middleware, Distributed computing, conditional messaging, Centralized control, Condition monitoring, Databases, USA Councils, reliable messaging, transactional resources, multiple conditional message grouping, atomic units of work, message processing, distributed object management, Context-aware services, Java, Java 2 Enterprise Edition, Calendars, Middleware, message receipt, message queuing environments, object middleware, distributed objects, application conditions, backward dependencies]
Formally verified Byzantine agreement in presence of link faults
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
This paper shows that deterministic consensus in synchronous distributed systems with link faults is possible, despite the impossibility result of Gray (1978). Instead of using randomization, we overcome this impossibility by moderately restricting the inconsistency that link faults may cause system-wide. Relying upon a novel hybrid fault model that provides different classes of faults for both nodes and links, we provide a formally verified proof that the m+1-round Byzantine agreement algorithm OMH (Lincoln and Rushby (1993)) requires n > 2f/sub l//sup s/ + f/sub l//sup r/ + f/sub l//sup ra/ + 2(f/sub a/ + f/sub s/) + f/sub o/ + f/sub m/ + m nodes for transparently masking at most f/sub l//sup s/ broadcast and f/sub l//sup r/ receive link faults (including at most f/sub l//sup ra/ arbitrary ones) per node in each round, in addition to at most f/sub a/, f/sub s/, f/sub o/, f/sub m/ arbitrary, symmetric, omission, and manifest node faults, provided that m /spl ges/ f/sub a/ + f/sub o/ + 1. Our approach to modeling link faults is justified by a number of theoretical results, which include tight lower bounds for the required number of nodes and an analysis of the assumption coverage in systems where links fail independently with some probability p.
[Algorithm design and analysis, formally verified proof, Laboratories, Byzantine agreement algorithm, hybrid fault model, deterministic consensus, assumption coverage, formal verification, Fault tolerant systems, Failure analysis, Broadcasting, Distributed algorithms, Automation, Field buses, probability, synchronous distributed systems, OMH, deterministic algorithms, software fault tolerance, lower bounds, Computer science, link faults, receive link faults, distributed algorithms, fault-tolerant distributed systems, Formal verification, broadcast faults]
Towards a distributed platform for resource-constrained devices
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Many visions of the future predict a world with pervasive computing, where computing services and resources permeate the environment. In these visions, people will want to execute a service on any available device without worrying about whether the service has been tailored for the device. We believe that it will be difficult to create services that can execute well on the wide variety of devices that are being developed because of problems with diversity and resource constraints. We believe that these problems can be greatly reduced by using an ad-hoc distributed platform to transparently off-load portions of a service from a resource-constrained device to a nearby server. We implemented a preliminary prototype and emulator to study this approach. Our experiments show the beneficial use of nearby resources to relieve both memory and processing constraints, when it is appropriate to do so. We believe that this approach will reduce the burden on developers by masking more device details.
[Pervasive computing, Computer vision, Java, client-server systems, server, resource constraints, Government, distributed processing, Displays, off loading, Distributed computing, resource-constrained devices, resource allocation, Memory management, processing constraint, Prototypes, AIDE, memory constraint, partitioning, distributed platform, Personal digital assistants, Mobile computing]
Snap-stabilizing PIF algorithm in arbitrary networks
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
We present the first snap-stabilizing propagation of information with feedback (PIF) protocol in arbitrary networks. A snap-stabilizing protocol, starting from any arbitrary initial system configuration, always behaves according to its specification. Our protocol is distributed, deterministic, and does not use a pre-constructed spanning tree.
[Algorithm design and analysis, Protocols, specification, snap-stabilizing PIF algorithm, distributed protocol, Distributed computing, deterministic protocol, Computer science, Intelligent networks, Fault tolerance, Tree graphs, Fault detection, Feedback, distributed algorithms, snap-stabilizing propagation of information with feedback protocol, Broadcasting, arbitrary initial system configuration, fault tolerant computing, protocols, arbitrary networks]
Process migration: a generalized approach using a virtualizing operating system
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Process migration has been used to perform specialized tasks, such as load sharing and checkpoint/restarting long running applications. Implementation typically consists of modifications to existing applications and the creation of specialized support systems, which limit the applicability of the methodology. Off the shelf applications have not benefited from process migration technologies, mainly due to the lack of an effective generalized methodology and facility. The benefits of process migration include mobility, checkpointing, relocation, scheduling and on the fly maintenance. This paper shows how regular shrink-wrapped applications can be migrated. The approach to migration is to virtualize the application by injecting functionality into running applications and operating systems. Using this scheme, we separate the physical resource bindings of the application and replace it with virtual bindings. This technique is referred to as virtualization. We have developed a virtualizing operating system (vOS), residing on top of Windows 2000 that injects stock applications with the virtualizing software. It coordinates activities across multiple platforms providing new functionality to the existing applications. The vOS makes it possible to build communities of systems that cooperate to run applications and share resources non-intrusively while retaining application binary compatibility.
[Checkpointing, checkpointing, on the fly maintenance, restarting, mobility, application program interfaces, Communities, virtual bindings, Distributed computing, system recovery, Application virtualization, Concurrent computing, process migration, Operating systems, network operating systems, scheduling, Libraries, multiple platforms, application binary compatibility, Kernel, virtualizing operating system, load sharing, specialized support systems, stock application injection, relocation, Application software, Windows 2000, Computer science, software portability, virtualizing software]
Improving search in peer-to-peer networks
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Peer-to-peer systems have emerged as a popular way to share huge volumes of data. The usability of these systems depends on effective techniques to find and retrieve data; however current techniques used in existing P2P systems are often very inefficient. We present three techniques for efficient search in P2P systems. We present the design of these techniques, and then evaluate them using a combination of analysis and experiments over Gnutella, the largest open P2P system in operation. We show that while our techniques maintain the same quality of results as currently used techniques, they use up to 5 times fewer resources. In addition, we designed our techniques to be simple, so that they can be easily incorporated into existing systems for immediate impact.
[client-server systems, Costs, message passing, open systems, Peer to peer computing, Multimedia systems, peer-to-peer networks, information retrieval, efficient search, Information retrieval, open system, Computer science, query processing, Intelligent networks, Network topology, peer-to-peer systems, Fault tolerant systems, Bandwidth, P2P systems, Gnutella, Usability, search problems]
From total order to database replication
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
This paper presents in detail an efficient and provably correct algorithm for database replication over partitionable networks. Our algorithm avoids the need for end-to-end acknowledgments for each action while supporting network partitions and merges and allowing dynamic instantiation of new replicas. One round of end-to-end acknowledgments is required only upon a membership change event such as a network partition. New actions may be introduced to the system at any point, not only while in a primary component. We show how performance can be further improved for applications that allow relaxation of consistency requirements. We provide experimental results that demonstrate the efficiency of our approach.
[Availability, network partition, provably correct algorithm, replicated databases, partitionable networks, dynamic instantiation, Throughput, Computer crashes, Partitioning algorithms, Transaction databases, database replication, Distributed computing, Engines, Delay, Computer science, end-to-end acknowledgments, Prototypes, consistency requirements, protocols, total order replication]
Overlook: scalable name service on an overlay network
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
This paper indicates that a scalable fault-tolerant name service can be provided utilizing an overlay network and that such a name service can scale along a number of dimensions: it can be sized to support a large number of clients, it can allow large numbers of concurrent lookups on the same name or sets of names, and it can provide name lookup latencies measured in seconds. Furthermore, it can enable updates to be made pervasively visible in times typically measured in seconds for update rates of up to hundreds per second. We explain how many of these scaling properties for the name service are obtained by reusing some of the same mechanisms that allowed the underlying overlay network to scale. Finally, we observe that the overlay network is sensitive to bandwidth and CPU limitations.
[client-server systems, Adaptive systems, scaling, Peer to peer computing, Scalability, concurrent lookups, overlay network, Routing, Size measurement, name lookup latencies, Mechanical factors, Delay, fault-tolerant name service, table lookup, Web and internet services, Fault tolerant systems, update rates, Bandwidth, fault tolerant computing, Internet, scalable name service, Overlook]
Anthill: a framework for the development of agent-based peer-to-peer systems
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Recent peer-to-peer (P2P) systems are characterized by decentralized control, large scale and extreme dynamism of their operating environment. As such, they can be seen as instances of complex adaptive systems (CAS) typically found in biological and social sciences. We describe Anthill, a framework to support the design, implementation and evaluation of P2P applications based on ideas such as multi-agent and evolutionary programming borrowed from CAS. An Anthill system consists of a dynamic network of peer nodes; societies of adaptive agents travel through this network, interacting with nodes and cooperating with other agents in order to solve complex problems. Anthill can be used to construct different classes of P2P services that exhibit resilience, adaptation and self-organization properties. We also describe preliminary experiences with Anthill in implementing a file sharing application.
[Adaptive systems, multi-agent systems, complex adaptive systems, application program interfaces, self-organization, distributed processing, adaptation, Biology, Distributed computing, Centralized control, agent-based peer-to-peer systems, adaptive agents, Grid computing, Large-scale systems, societies, resilience, file sharing, evolutionary programming, Peer to peer computing, peer nodes, self-adjusting systems, dynamic network, evolutionary computation, extreme dynamism, Content addressable storage, decentralized control, P2P services, Anthill, Distributed control, P2P systems, Resource management]
An extensible and scalable Content Adaptation Pipeline architecture to support heterogeneous clients
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
The importance of middleware and content adaptation has previously been demonstrated for pervasive use of Web-based applications. In this paper we propose a modular extensible, and scalable middleware component called the Content Adaptation Pipeline that performs content adaptation on arbitrarily complex data types not limited to text and graphic images. Furthermore, the architecture can be used as part of many client-server applications, not just Web browsers. In our work we leverage the XML language as a uniform means to describe all the elements in our architecture, including the client device and user profiles, the data characteristics, the transcoding operations performed on the data, and the resultant adapted data. We illustrate the flexibility of our architecture to support new data types and adaptation operations by first showing its use with data from a real-world medical application and then extending its capabilities to handle animated graphics and also real-time streaming RTP data. Finally, we demonstrate scalability in our architecture by executing the Content Adaptation Pipeline over a distributed set of servers running an efficient protocol.
[Content Adaptation Pipeline, Scalability, Pipelines, Medical services, Web-based applications, pipeline architecture, medical application, client device, heterogeneous clients, computer animation, protocol, data characteristics, data structures, protocols, Web browsers, multimedia communication, real-time streaming RTP data, hypermedia markup languages, Biomedical equipment, client-server systems, Service oriented architecture, Transcoding, pervasive applications, Middleware, Graphics, user profiles, modular extensible component, XML, real-time systems, online front-ends, Animation, arbitrarily complex data types, client-server applications, transcoding operations, Internet, pipeline processing, scalable middleware component, medical computing, animated graphics]
Dynamic behaviour of the distributed tree quorum algorithm
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Mechanisms that ensure mutual access, replication and resilience to failures in large distributed systems can be based on quorum consensus. Effectiveness and scalability of the method selected are crucial. The aim of this paper is to present a message/time cost analysis of a distributed algorithm based on the tree quorum, which needs no global information, and has message complexity independent of the load. Results obtained for the different identifications of the requests (a requester's identification, timestamp, and group priority) illustrate the flexibility of the method The algorithm supports priority, the feature important for current middleware technologies.
[Algorithm design and analysis, Costs, Scalability, quorum consensus, requester identification, tree quorum, communication complexity, Delay, scalability, Information analysis, message complexity, tree data structures, Distributed algorithms, timestamp, middleware, Availability, replication, client-server systems, large distributed systems, Middleware, software fault tolerance, Resilience, synchronisation, Computer science, distributed algorithm, distributed algorithms, failure resilience, dynamic behaviour, mutual access, message time cost analysis, group priority, synchronization delay]
Group-based management of distributed file caches
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
We describe a way to manage distributed file system caches based upon groups of files that are accessed together. We use file access patterns to automatically construct dynamic groupings of files and then manage our cache by fetching groups, rather than single files. We present experimental results, based on trace-driven workloads, demonstrating that grouping improves cache performance. At the file system client, grouping can reduce LRU demand fetches by 50 to 60%. At the server cache hit rate improvements are much more pronounced, but vary widely (20 to over 1200%) depending upon the capacity of intervening caches. Our treatment includes information theoretic results that justify our approach to file grouping.
[file groups, cache performance, server, automatic dynamic groupings, distributed file system caches, Predictive models, Performance gain, trace-driven workloads, file system client, cache storage, information theoretic results, LRU demand fetches, Delay, storage management, Accuracy, File systems, System performance, network operating systems, file servers, software performance evaluation, client-server systems, Prefetching, file access patterns, cache hit rate improvements, Statistics, Computer science, Timing, group-based management]
Fast Collect in the absence of contention
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
We present a generic module, called Fast Collect. Fast Collect is an implementation of single-writer multi-reader (SWMR) shared-memory in an asynchronous system in which a processor updates its cell and then reads in any order all the other cells. Our simple implementation of Fast Collect uses some multiwriter multi-reader (MWMR) variables and one local Boolean per processor, such that eventually, in the absence of contention, i.e. if only a single processor repeatedly performs collect, the amortized cost per each collect is a constant. With the example of Disk Paxos we show how Fast Collect can be used as a building block in wait-free algorithms.
[Costs, multiwriter multi-reader variables, Adaptive algorithm, single-writer multi-reader shared memory, Software algorithms, Mathematics, Steady-state, Disk Paxos, SWMR shared memory, Concurrent computing, Computer science, wait-free algorithms, storage management, processor cell update, Computer bugs, distributed algorithms, Fast Collect, asynchronous system, MWMR variables, Chromium, shared memory systems, Distributed algorithms, generic module]
Version stamps-decentralized version vectors
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
Version vectors and their variants play a central role in update tracking in optimistic distributed systems. Existing mechanisms for a variable number of participants use a mapping from identities to integers, and rely on some form of global configuration or distributed naming protocol to assign unique identifiers to each participant. These approaches are incompatible with replica creation under arbitrary partitions, a typical mode of operation in mobile or poorly connected environments. We present an update tracking mechanism that overcomes this limitation; it departs from the traditional mapping and avoids the use of integer counters, while providing all the functionality of version vectors in what concerns version tracking.
[mobile environments, Protocols, poorly connected environments, Synchronization, Distributed computing, Counting circuits, clocks, mobile computing, Message passing, decentralized version vectors, version tracking, optimistic distributed systems, Mobile computing, version stamps, update tracking, Clocks]
Timestamping messages in synchronous computations
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
We present a method of timestamping messages and events in synchronous computations that capture the order relationship with vectors of size less than or equal to the size of the vertex cover of the communication topology of the system. Our method is fundamentally different from the techniques of Fidge (1989) and Mattern (1989). The timestamps in our method do not use one component per process but still guarantee that the order relationship is captured accurately. Our algorithm is online and only requires piggybacking of timestamps on program messages. It is applicable to all programs that either use programming languages based on synchronous communication such as CSP or use synchronous remote procedure calls.
[Visualization, message passing, vertex cover, graph theory, Debugging, Topology, piggybacking, Distributed computing, Programming profession, Concurrent computing, order relationship, Asynchronous communication, Fault tolerance, messages timestamping, synchronous computations, communication topology, remote procedure calls, program messages, Remote monitoring, synchronous communication, Clocks, synchronous remote procedure calls]
A reliable multicast algorithm for mobile ad hoc networks
Proceedings 22nd International Conference on Distributed Computing Systems
None
2002
A reliable multicast algorithm, called RMA, for mobile ad hoc networks is presented that is based on a new cost criterion, called link lifetime, for determining the optimal path between a pair of nodes. The algorithm has the characteristics of using an undirected graph for its routing operations rather than a fixed structure like a tree or a mesh. Previously proposed routing metrics for mobile ad hoc networks were designed for use in wired environments, where link stability is not a concern. We propose a new metric, called the lifetime, which is more appropriate for mobile ad hoc networks. The lifetime metric is dependent on the predicted future life of the link under consideration. We developed a simulator for the mobile ad hoc networks, which is portable and scalable to a large number of nodes. Using the simulator, we carried out a simulation study to analyze the effectiveness of the routing metrics and the performance of the proposed reliable multicast algorithm. The simulation results show that the lifetime metric helps achieve better performance in mobile ad hoc environments than the hop count metric.
[graph theory, link lifetime, routing operations, Mobile ad hoc networks, Analytical models, packet radio networks, mobile computing, Tree graphs, mobile ad hoc networks, predicted future life, Bandwidth, multicast communication, Cost function, Performance analysis, undirected graph, cost criterion, mobile radio, Stability, reliable multicast algorithm, simulator, RMA, Routing, Ad hoc networks, routing metrics, Multicast algorithms, telecommunication network routing, lifetime metric]
A command and control support system using CORBA
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
A C/sup 4/I (command, control, computers, communication and intelligence) support system spans a large variety of requirements and usually serves many users with diverse needs. Also, in order to properly display information to the decision-makers in a timely way, it must integrate data from other systems, not necessarily built with the same technology. The Operations Theater Surveillance System (SATO), presented in this paper, was designed in 1998 for the Mercury project, which is developing the Brazilian Navy's new C/sup 2/S (Command and Control Support) system. This paper, as an experience report, focuses on the software constructs used in the system, with an architectural perspective. SATO displays and manages all the information presented to the users. In addition, as Mercury's main subsystem, it lays the foundation for integrating other subsystems and legacy systems. Data enters Mercury from all crisis control centers (CCCs) distributed throughout the country.
[Operations Theater Surveillance System, Military computing, Communication system control, C/sup 4/I support system, Control systems, user needs, subsystems, Command and control systems, Network servers, software architecture, information display, Software architecture, crisis control centers, command and control systems, distributed systems, software constructs, surveillance, Intelligent systems, distributed object management, naval engineering computing, Brazilian Navy, SATO, CORBA, Computer displays, command and control support system, Surveillance, decision-making, legacy systems, Distributed control, data integration, Mercury project]
Support for speculative update propagation and mobility in Deno
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
This paper presents the replication framework of Deno, an object replication system specifically designed for mobile and weakly-connected environments. Deno uses weighted voting for availability and pair-wise, epidemic information flow for flexibility. This combination allows the protocols to operate with less than full connectivity, to easily adapt to changes in group membership, and to make few assumptions about the underlying network topology. Deno has been implemented and runs on top of Linux and Win32 platforms. We use the Deno prototype to characterize the performance of two versions of Deno's protocol. The first version enables globally serializable execution of update transactions. The second supports a weaker consistency level that still guarantees transactionally-consistent access to replicated data. We demonstrate that the incremental cost of providing global serializability is low, and that speculative dissemination of updates can significantly improve commit performance.
[Unix, weighted voting, Costs, mobility, Deno, object replication system, weakly-connected environments, group membership, Environmental management, Network topology, Voting, Prototypes, speculative update propagation, Win32 platforms, Large-scale systems, protocols, Availability, commit performance, replication framework, Access protocols, network topology, Computer science, performance, Linux, epidemic information flow, operating systems (computers), globally serializable execution]
Distributed network simulations using the dynamic simulation backplane
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Presents an approach for creating distributed, component-based simulations of communication networks by interconnecting models of sub-networks drawn from different network simulation packages. This approach supports the rapid construction of simulations for large networks by reusing existing models and software, and fast execution using parallel discrete event simulation techniques. A dynamic simulation backplane is proposed that provides a common format and protocol for message exchange, and services for transmitting data and synchronizing heterogeneous network simulation engines. In order to achieve plug-and-play interoperability, the backplane uses existing network communication standards and dynamically negotiates among the participant simulators to define a minimal subset of required information that each simulator must supply, as well as other optional information. The backplane then automatically creates a message format that can be understood by all participating simulators and dynamically creates the content of each message by using callbacks to the simulation engines. We describe our approach to interoperability as well as an implementation of the backplane. We present results that demonstrate the proper operation of the backplane by distributing a network simulation between two different simulation packages, ns2 and GloMoSim. Performance results show that the overhead for the creation of the dynamic messages is minimal. Although this work is specific to network simulations, we believe our methodology and approach can be used to achieve interoperability in other distributed computing applications as well.
[model reuse, Protocols, open systems, GloMoSim, callbacks, dynamic simulation backplane, distributed component-based simulations, Discrete event simulation, heterogeneous network simulation engine synchronization, ns2, Distributed computing, telecommunication computing, distributed network simulations, fast execution, Engines, message format creation, telecommunication networks, network simulation packages, software packages, Computer networks, discrete event simulation, Contracts, software performance evaluation, data transmission services, message passing, software reuse, Computational modeling, communication networks, message exchange protocol, overhead, Educational institutions, minimal required information subset, dynamic message contents creation, network communication standards, Backplanes, Packaging, distributed computing applications, plug-and-play interoperability, subroutines, interconnected sub-network models, parallel discrete event simulation techniques, dynamic negotiation]
Generalized role-based access control
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Generalized Role-Based Access Control (GRBAC) is a new paradigm for creating and maintaining rich access control policies. GRBAC leverages and extends the power of traditional role based access control (RBAC) by incorporating subject roles, object roles and environment roles into access control decisions. Subject roles are like traditional RBAC roles: they abstract the security-relevant characteristics of subjects into categories that can be used in defining a security policy. Similarly, object roles abstract the various properties of objects, such as object type (e.g., text, JPEG, executable) or sensitivity level (e.g., classified, top secret) into categories. Environment roles capture environmental information, such as time of day or system load so it can be used to mediate access control. Together, these three types of roles offer flexibility and expressive power, as well as a degree of usability not found in current access control models.
[Access control, transaction processing, access control decisions, distributed processing, rich access control policies, JPEG, expressive power, environmental information, sensitivity level, object type, authorisation, GRBAC, Permission, access control, security policy, generalized role based access control, object roles, Educational institutions, Mechanical factors, Power system security, Power system modeling, environment roles, security-relevant characteristics, RBAC, access control models, Information security, subject roles, Usability]
Performance tuning of distributed applications with CoSMoS
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Performance monitoring of distributed software executing on workstation cluster systems is a complex task because of the multitude of factors that can affect performance in these environments. For performance analysis to be done effectively, all runtime events which have an impact on performance have to be captured. We introduce an integrated monitoring system that is capable of recording performance data on the application level, the operating system level, and the network level. It is thus able to provide complete coverage of all subsystems relevant to application performance. One of the key features of the proposed monitoring system is its ability to correlate performance data with application source code, pointing the developer to the portions of his code that may be time-critical. The system thus supports a cyclic process of optimizing performance in multiple iterations of analysis and refinement. This method allows performance bottlenecks to be tracked down to source code instruction level.
[workstation clusters, performance monitoring, Software performance, time-critical software, distributed applications, application performance, Runtime, Operating systems, network operating systems, Workstations, Performance analysis, software performance evaluation, Availability, application source code, Computerized monitoring, performance tuning, runtime events, operating system, performance bottlenecks, Application software, Communication system software, source code instruction level, integrated monitoring system, system monitoring, Software tools, CoSMoS]
Object distribution with local information
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
We investigate the problem of distributing communicating objects across wide-area environments. Our goals are to balance load, minimize network communication, and use resources efficiently. However, applications running in such environments are often dynamic and highly unpredictable. Furthermore, synchronous communication is usually too expensive to be used in disseminating load information. We therefore investigate policies that use local information to approximate desired global behaviors. Our results with Java applications show that simple, local approaches are surprisingly effective in capturing load information and object relationships, and in making migration and clustering decisions based on profiled information.
[Costs, wide area networks, load balancing, network communication, Java applications, object relationships, resource allocation, Parallel processing, object distribution, Computer networks, Large-scale systems, communicating object distribution, profiled information, load information, distributed object management, synchronous communication, Testing, Java, Educational institutions, local information, Application software, clustering decisions, Computer science, global behaviors, wide-area environments, Metacomputing, local approaches]
An exercise in proving self-stabilization through Lyapunov functions
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Self-stabilizing distributed algorithms exhibit interesting analogies with the stabilizing feedback systems that are used in various engineering domains. In this paper, we show, with E.W. Dijkstra's (1974) token-ring algorithm for mutual exclusion serving as an example, that methodologies from control theory, e.g. Lyapunov's "second method\
[Algorithm design and analysis, Lyapunov's 2nd method, control theory, functions, mutual exclusion, Control systems, Mechanical factors, self-stabilizing distributed algorithms, Mechanical engineering, Computer science, feedback, Reactive power, Lyapunov functions, Feedback, distributed algorithms, stabilizing feedback systems, engineering domains, token-ring algorithm, Token networks, Control theory, token networks, Distributed algorithms, stability, Lyapunov methods]
An application-oriented approach for distributed system modeling and simulation
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Complexity of applications operating in a network environment has been considerably increased, since numerous architectural models, such as the client/server model and its extensions, have lately emerged. When dealing with distributed applications, network modeling is not so demanding and modeling solutions for widely used network components are already adopted by commercial tools. We introduce a modeling approach for distributed systems, putting the emphasis on distributed applications. This approach enables the analytical description of applications on the basis of predetermined, high-level operations (or actions) which can be customized to conform to specific architectural models. Operations are ultimately expressed in terms of primitive actions. Through this multi-layer decomposition scheme, in-depth analysis of application mechanisms is promoted. The modeling approach is oriented towards performance evaluation through simulation and a simulation tool has been constructed for this purpose. Modeling examples and a case study for a distributed database banking system are also presented.
[Geography, distributed database, client server model, Object oriented modeling, Banking, distributed system modeling, distributed processing, performance evaluation, Electronic mail, distributed applications, Aerospace control, application-oriented approach, banking system, Distributed databases, Management information systems, virtual machines, distributed databases, Performance analysis, Mathematical model, Informatics, distributed system simulation, multi-layer decomposition scheme, bank data processing]
On the minimal characterization of the rollback-dependency trackability property
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Checkpoint and communication patterns that enforce rollback-dependency trackability (RDT) have only online trackable checkpoint dependencies and allow efficient solutions to the determination of consistent global checkpoints. Baldoni, Helary and Raynal (1998) have explored RDT at the message level, in which checkpoint dependencies are represented by zigzag paths. They have presented many characterizations of RDT and conjectured that a certain communication pattern characterizes the minimal set of zigzag paths that must be tested online by a checkpointing protocol in order to enforce RDT. The contributions of this work are: a proof that their conjecture is false; a minimal characterization of RDT; and introduction of an original approach to analyze RDT checkpointing protocols.
[Checkpointing, Protocols, fault tolerance, Laboratories, communication patterns, consistent global checkpoints, checkpointing protocol, system recovery, software fault tolerance, Information systems, Information analysis, zigzag paths, Fault tolerance, online trackable checkpoint dependencies, High performance computing, distributed algorithms, message level, rollback-dependency trackability property, protocols, Pattern analysis, Distributed algorithms, Testing]
Maintaining mutual consistency for cached Web objects
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Existing Web proxy caches employ cache consistency mechanisms to ensure that locally cached data is consistent with that at the server. We argue that techniques for maintaining consistency of individual objects are not sufficient; a proxy should employ additional mechanisms to ensure that related Web objects are mutually consistent with one another. We formally define the notion of mutual consistency and the semantics provided by a mutual consistency mechanism to end users. We then present techniques for maintaining mutual consistency in the temporal and value domains. A novel aspect of our techniques is that they can adapt to the variations in the rate of change of the source data, resulting in judicious use of proxy and network resources. We evaluate our approaches using real-world Web traces and show that: (i) careful tuning can result in substantial savings in the network overhead incurred without any substantial loss in fidelity, of the consistency guarantees, and (ii) the incremental cost of providing mutual consistency guarantees over mechanisms to provide individual consistency guarantees is small.
[cache consistency mechanisms, incremental cost, Costs, Electromagnetic compatibility, HTML, cache storage, semantics, Delay, mutual consistency mechanism, consistency guarantees, mutual consistency guarantees, file servers, individual objects, Web server, Stock markets, Web proxy caches, information resources, Engineering profession, data integrity, value domains, real-world Web traces, Computer science, network overhead, end users, mutual consistency, network resources, Frequency, related Web objects, source data, Internet, locally cached data, cached Web objects]
JR: flexible distributed programming in an extended Java
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Java provides a clean object-oriented programming model and allows for inherently system-independent programs. Unfortunately, Java has a limited concurrency model, providing only threads and remote method invocation (RMI). The JR programming language extends Java to provide a rich concurrency model. JR provides dynamic remote virtual machine creation, dynamic remote object creation, remote method invocation, asynchronous communication, rendezvous, and dynamic process creation. JR programs are written in an extended Java and then translated into standard Java programs. The JR run-time support system is also written in standard Java. This paper describes the JR programming language and its implementation. Some initial measurements of the performance of the implementation are also included.
[object-oriented programming model, application program interfaces, extended Java, asynchronous communication, JR, Yarn, Concurrent computing, Strontium, dynamic remote object creation, dynamic process creation, Dynamic programming, Object oriented programming, distributed programming, dynamic remote virtual machine creation, Java, object-oriented programming, Object oriented modeling, JR programming language, Virtual machining, run-time support system, Computer science, flexible distributed programming, Computer languages, performance, inherently system-independent programs, concurrency model, concurrency control, remote method invocation, object-oriented languages, remote procedure calls]
Adaptive beacon placement
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Beacon placement strongly affects the quality of spatial localization, a critical service for context-aware applications in wireless sensor networks; yet this aspect of localization has received little attention. Fixed beacon placement approaches such as uniform and very dense placement are not always viable and will be inadequate in very noisy environments in which sensor networks may be expected to operate (with high terrain and propagation uncertainties). We motivate the need for empirically adaptive beacon placement and outline a general approach based on exploration and instrumentation of the terrain conditions by a mobile human or robot agent. We design, evaluate and analyze three novel adaptive beacon placement algorithms using this approach for localization based on RF-proximity. In our evaluation, we find that beacon density rather than noise level has a more significant impact on beacon placement algorithms. Our beacon placement algorithms are applicable to a low (beacon) density regime of operation. Noise makes moderate density regimes more improvable.
[Algorithm design and analysis, Context-aware services, Uncertainty, wireless sensor networks, Instruments, RF-proximity, Humans, noisy environments, distributed sensors, Mobile robots, Noise level, Wireless sensor networks, mobile communication, Working environment noise, spatial localization, robot agent, context-aware applications, Robot sensing systems, terrain conditions, adaptive beacon placement]
Cost effective mobile agent planning for distributed information retrieval
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
The number of agents and the execution time are two significant performance factors in mobile agent planning (MAP). Fewer agents cause lower network traffic and consume less bandwidth. Regardless of the number of agents used, the execution time for a task must be kept minimal, which means that use of the minimal number of agents must not impact on the execution time unfavorably. As the population of the mobile agent application domain grows, the importance of these two factors also increases. After a careful review of these two factors, we propose two heuristic algorithms for finding the minimal number of traveling agents for retrieving information from a distributed computing environment, while keeping the latency minimal. Although agent planning, specifically MAP, is quite similar to the famous traveling salesman problem (TSP), agent planning has a different objective function from that of TSP. TSP deals with the optimal total routing cost, while MAP attempts to minimize the execution time to complete tasks of information retrieval. In this paper, we suggest two cost-effective MAP algorithms, BYKY1 (Baek-Yeo-Kim-Yeom 1) and BYKY2, which can be used in distributed information retrieval systems to find the factors mentioned above. At the end of each algorithm, 2OPT, a well-known TSP algorithm, is called to optimize each agent's local routing path. Experimental results show that BYKY2 produces near-optimal performance. These algorithms are more realistic and applicable directly to the problem domains than those of previous works.
[minimal latency, near-optimal performance, Costs, multi-agent systems, Heuristic algorithms, Telecommunication traffic, optimal total routing cost, BYKY2 algorithm, mobile agent planning, travelling salesman problem, performance factors, Distributed computing, Delay, execution time minimization, travelling salesman problems, planning (artificial intelligence), mobile computing, Mobile agents, distributed agent system, Bandwidth, distributed databases, distributed information retrieval systems, 2OPT algorithm, distributed programming, software performance evaluation, objective function, cost-effectiveness, heuristic algorithms, bandwidth, information retrieval, Traveling salesman problems, Information retrieval, Routing, BYKY1 algorithm, network traffic, minimum travelling agent number, information retrieval systems, local routing path optimization, minimisation]
MAGE: a distributed programming model
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Writing distributed programs is difficult. To ease this task, we introduce a new programming abstraction which we call a mobility attribute. Mobility attributes provide a syntax that describes the mobility semantics of program components. Programmers attach mobility attributes to program components to dynamically control the placement of these components within the network. Mobility attributes intercept component invocations and decide whether and where to move a component before the component executes. This allows the programmer to improve her program's runtime efficiency by colocating components and resources. We present MAGE, an object oriented distributed system, that supports mobility attributes and illustrates their utility.
[Costs, programming abstraction, Logic programming, component invocations, Object oriented modeling, program components, Laboratories, distributed programming model, mobility semantics, object oriented distributed system, Distributed computing, Programming profession, Computer science, MAGE, mobility attribute, Computer architecture, Writing, syntax, runtime efficiency, Large-scale systems, distributed programming, software performance evaluation, distributed object management]
A heuristic for dynamic bandwidth allocation with preemption and degradation for prioritized requests
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Bandwidth allocation is a fundamental problem in communication networks. The problem of bandwidth allocation is further intensified when the requested bandwidth exceeds the available unused bandwidth and so not all requests can be completely served. This research examines on-line bandwidth allocation, where the decision for acceptance or rejection of the request has to be made when future requests and their arrival statistics are not known. A request can be defined as a flow of information from a source to a destination with a certain amount of bandwidth, a priority level, a utility function that is based on the bandwidth received and a worth that is based on the utility function and the priority level. The goal of this research is to develop a scheduling heuristic for an overloaded system that attempts to schedule the requests such that the sum of the worths of the requests satisfied in a fixed interval of rime is the maximum. The scheduling heuristic can preempt or degrade already-scheduled requests. Three different types of utility functions (step, linear, and concave) are examined. Other parameters being considered include network loading and the relative weights of the different priority levels. The heuristic variations developed are shown to perform well compared to a complete sharing policy and an upper bound.
[communication networks, Quality of service, heuristic, digital simulation, degradation, utility function, priority level, Statistics, dynamic bandwidth allocation, arrival statistics, Degradation, bandwidth allocation, Intelligent networks, Upper bound, prioritized requests, telecommunication networks, Bandwidth, Channel allocation, Internet, Communication networks, Resource management, preemption]
Maximizing speedup through performance prediction for distributed shared memory systems
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Parallel applications executing on large-sized parallel systems achieve better speedup than on small-sized systems. However, because of the design and implementation of the distributed shared memory (DSM) system, there are some instances where large system size gives no further performance improvement over small system size. It is important to determine what system size will result in the maximum speedup while all kinds of applications are running on DSM systems. In this paper, we describe the design and implementation of the performance prediction mechanism in our DSM system, Proteus, which supports node reconfiguration to adjust the system size at run time. We adopt a simple computation model and combine it with run-time information to predict the performance under different system sizes. With this mechanism, it is possible to provide timely prediction results to adjust the size of the underlying system and thus maximize speedup.
[Protocols, Scalability, run-time information, Predictive models, performance evaluation, speedup maximization, parallel applications, Application software, Yarn, Concurrent computing, performance prediction, parallel systems, Proteus, Runtime, Multithreading, reconfigurable architectures, node reconfiguration, Parallel processing, distributed shared memory systems, Load management, system size adjustment]
Distributed query processing in the Internet: exploring relation replication and network characteristics
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
We introduce the concept of network graph for distributed query processing. Semijoins and joins are termed contributive replicated semijoins and contributive replicated joins, respectively, when they are interleaved into a join sequence to reduce the amount of data transmission cost required in a network with replicated relations. Our solution procedure consists of three consecutive steps, namely relation selection, join sequence scheduling and merge processing. A simulator is developed to evaluate the performance of algorithms devised. Our results show that the approach of interleaving a join sequence with contributive replicated semijoins/joins is not only efficient in its execution but also effective in reducing the total amount of data transmission cost required to process distributed queries.
[contributive replicated semijoins, Costs, relational algebra, merge processing, distributed query processing, Relational databases, relation replication, Electronic mail, network graph, query processing, Intelligent networks, scheduling, Database systems, IP networks, Data communication, software performance evaluation, data transmission cost, replicated databases, performance evaluation, join sequence scheduling, Query processing, Interleaved codes, contributive replicated joins, Internet, relation selection]
Design and evaluation of redistribution strategies for wide-area commodity distribution
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
The proliferation of e-commerce has enabled a new set of applications that allow globally distributed purchasing of commodities such as books, CDs, travel tickets, etc., over the Internet. These commodities can be represented online by tokens, which can be distributed among servers to enhance the performance and availability of such applications. There are two fundamental approaches for distributing such tokens-partitioning and replication. Partitioning-based approaches eliminate the need for tight quorum synchronization required by replication-based approaches. The effectiveness of partitioning, however, relies on token redistribution techniques that allow dynamic migration of tokens to where they are needed. We propose pair-wise token redistribution strategies to support applications that involve wide-area commodity distribution. Using a detailed simulation model and real Internet message traces, we investigate the performance of our redistribution strategies and a previously proposed replication based scheme. Our results reveal that, for the types of applications and environment we address, partitioning-based approaches perform superior primarily due to their ability to provide higher server autonomy.
[pair-wise token redistribution, globally distributed commodity purchasing, Distribution strategy, Scalability, simulation, Educational institutions, World Wide Web, token distribution, message traces, Delay, application performance, dynamic token migration, synchronisation, server autonomy, Marketing and sales, e-commerce, Internet, Books, wide-area commodity distribution, Web server, software performance evaluation, tight quorum synchronization, Merchandise, electronic commerce]
On detecting global predicates in distributed computations
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Monitoring of global predicates is a fundamental problem in asynchronous distributed systems. This problem arises in various contexts, such as design, testing and debugging, and fault tolerance of distributed programs. In this paper, we establish that the problem of determining whether there exists a consistent cut of a computation that satisfies a predicate in k-CNF (k/spl ges/2), in which no two clauses contain variables from the same process, is NP-complete in general. A polynomial-time algorithm to find the consistent cut, if it exists, that satisfies the predicate for special cases is provided. We also give algorithms (albeit exponential) that can be used to achieve an exponential reduction in time over existing techniques for solving the general version. Furthermore, we present an algorithm to determine whether there exists a consistent cut of a computation for which the sum x/sub 1/+x/sub 2/+/spl middot//spl middot//spl middot/+x/sub n/ exactly equals some constant k, where each x/sub i/ is an integer variable on a process p/sub i/ such that it is incremented or decremented by at most one at each step. As a corollary, any symmetric global predicate on Boolean variables, such as absence of simple majority and exclusive-OR of local predicates, can now be detected. Additionally, the problem is proved to be NP-complete if each x/sub i/ can be changed by an arbitrary amount at each step. Our results solve the previously open problems in predicate detection proposed by V.K. Garg (1997) and bridge the wide gap between the known tractability and intractability results that have existed until now.
[k-conjunctive normal form, global predicate monitoring, computability, systems design, asynchronous distributed systems, Distributed computing, tractability, Fault tolerance, Fault tolerant systems, Boolean variables, debugging, Polynomials, polynomial-time algorithm, distributed programming, Testing, symmetric global predicate, fault tolerance, exponential algorithms, Computerized monitoring, software testing, intractability, Debugging, NP-complete problem, distributed programs, Bridges, distributed computations, Fault detection, System recovery, integer variables, consistent cut, local predicates, global predicate detection, computational complexity]
Interagent communication and synchronization in DaAgent
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
This paper describes the design and implementation of interagent communication and synchronization models in the DaAgent mobile agent-based computing system. Eight models of interagent communication and synchronization are proposed. The paper evaluates these models based on their utility, performance, and applicability in an Internet computing environment.
[multi-agent systems, interagent communication models, DaAgent mobile agent-based computing system, Mobile communication, Electronic commerce, Power system modeling, software agents, synchronisation, Computer science, Power measurement, interagent synchronization models, USA Councils, Mobile agents, Prototypes, Internet computing environment, Internet, Mobile computing]
Revisiting hierarchical quorum systems
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
In distributed systems, it is often necessary to provide coordination among the multiple concurrent processes. Quorum systems provide a decentralized approach to provide such coordination that is resilient to node and communication link failures. Quorum systems are highly available and may be used to balance the load among the elements of the system. In this paper, we propose a modification to the hierarchical grid quorum system that leads to a smaller quorum size and better availability and load. We also propose a new hierarchical quorum construction based on the organization of elements in a triangular shape that presents better average quorum size, availability and load than other highly-available systems with almost optimal load.
[Protocols, Costs, Shape, load balancing, quorum size, distributed processing, availability, communication link failures, Network servers, resource allocation, Permission, distributed systems, decentralized approach, resilience, Availability, concurrency theory, hierarchical grid, Computer crashes, Partitioning algorithms, hierarchical systems, concurrent process coordination, triangular shape, System recovery, hierarchical quorum systems, fault tolerant computing, node failures]
Robust TCP congestion recovery
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Presents a new robust TCP (Transmission Control Protocol) congestion recovery scheme to (1) handle bursty packet losses while preserving the self-clocking capability; (2) detect a TCP connection's new equilibrium during congestion recovery, thus improving both link utilization and effective throughput; and (3) make the TCP behavior during congestion recovery very close to that during congestion avoidance, thus "extending" the performance model for congestion avoidance to that for TCP loss recovery. Furthermore, the new recovery scheme requires only a slight modification to the sender side of TCP implementation, thus making it widely deployable. The performance of the proposed scheme is evaluated for scenarios with many TCP flows under the drop-tail and RED (random early detection) gateways in the presence of bursty packet losses. The evaluation results show that the new scheme achieves at least as great a performance improvement as TCP SACK (Selective ACKnowledgments) and consistently outperforms TCP New-Reno. Moreover, its steady-state TCP behavior is close to the ideal TCP congestion behavior. Since the proposed scheme does not require selective acknowledgments nor receiver modifications, its implementation is much simpler than TCP SACK.
[Protocols, telecommunication congestion control, self-clocking capability, Laboratories, packet switching, performance model, TCP New-Reno, Throughput, World Wide Web, selective acknowledgments, Electronic mail, losses, system recovery, RED gateway, drop-tail gateway, random early detection, Robustness, Large-scale systems, throughput, IP networks, TCP connection equilibrium detection, robust TCP congestion recovery, TCP SACK, TCP flows, performance evaluation, Transmission Control Protocol, link utilization, sender-side modification, steady-state behavior, bursty packet losses, congestion avoidance, transport protocols, Performance loss, Internet, TCP loss recovery, widely deployable recovery scheme]
Optimistic active replication
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Replication is a powerful technique for increasing availability of a distributed service. Algorithms for replicating distributed services do however face a dilemma: they should be: efficient (low latency); while ensuring consistency of the replicas, which are two contradictory goals. The paper concentrates on active replication, where all the replicas handle the clients' requests. Active replication is usually implemented using the atomic broadcast primitive. To be efficient, some atomic broadcast algorithms deliberately sacrifice consistency, if inconsistency is likely to occur with a low probability. We present an algorithm that handles replication efficiently in most scenarios, while preventing inconsistencies. The originality of the algorithm is to take the client-server interaction into account, while traditional solutions consider atomic broadcast as a black box.
[optimistic active replication, Fault tolerance, client-server systems, distributed service availability, client requests, Fault tolerant systems, Broadcasting, replica consistency, Delay, software fault tolerance, client-server system, atomic broadcast]
A fully automated object extraction system for the World Wide Web
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
This paper presents a fully automated object extraction system Omini. A distinct feature of Omini is the suite of algorithms and the automatically learned information extraction rules for discovering and extracting objects from dynamic Web pages or static Web pages that contain multiple object instances. We evaluated the system using more than 2,000 Web pages over 40 sites. It achieves 100% precision (returns only correct objects) and excellent recall (between 99% and 98%, with very few significant objects left out). The object boundary identification algorithms are fast, about 0.1 second per page with a simple optimization.
[information resources, object extraction system, Automation, search engines, information extraction rules, static Web pages, system evaluation, dynamic Web pages, information retrieval, Educational institutions, World Wide Web, HTML, Omini, Data mining, Programming profession, object boundary identification algorithms, optimization, Web pages, Search engines, Writing, Explosives, Internet, Web sites]
Modeling and analyzing real-time CORBA and supervision and control framework and applications
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
We advocate the need to exploit formal methods in the development of critical applications on top of RT-CORBA, a recently defined real-time extension of CORBA. We illustrate our approach using the TRIO formal notation. First, we provide a model of the core features of RT CORBA and of the real-time event service. Then we formalize the requirements of a simple application for supervision and control, and we outline the object architecture of its implementation based on the RT-CORBA platform. Finally we show how the above model (RT-CORBA and service plus application objects) can be employed in the proof that the application requirements are actually fulfilled.
[Real time systems, object-oriented programming, supervision and control framework, Object oriented modeling, Quality of service, Control systems, Application software, Yarn, application requirements, TRIO formal notation, Operating systems, real-time systems, object architecture, Open systems, formal methods, real-time extension, real-time event service, Safety, Energy management, distributed object management, real-time CORBA]
Random, Ephemeral Transaction Identifiers in dynamic sensor networks
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Recent advances in miniaturization and low-cost, low-power design have led to active research in large-scale, highly distributed systems of small, wireless, low-power unattended sensors and actuators. We explore the use of Random, Ephemeral TRansaction Identifiers (RETRI) in such systems, and contrast it with the typical design philosophy of using static identifiers in roles such as node addressing or efficient data naming. Instead of using statically assigned identifiers that are guaranteed to be unique, nodes randomly select probabilistically unique identifiers for each new transaction. We show how this randomized scheme can significantly improve the system's energy efficiency in contexts where that efficiency is paramount, such as energy-constrained wireless sensor networks. Benefits are realized if the typical data size is small compared to the size of an identifier, and the number of transactions seen by an individual node is small compared to the number of nodes that exist in the entire system. Our scheme is designed to scale well: identifier sizes grow with a system's density not its overall size. We quantify these benefits using an analytic model that predicts our scheme's efficiency. We also describe an implementation as applied to packet fragmentation and an experiment that validates our model.
[Actuators, wireless sensor networks, dynamic sensor networks, Sensor phenomena and characterization, distributed processing, Sensor systems, Temperature sensors, actuators, experiment, low-cost low-power design, probabilistically unique identifiers, energy efficiency, Acoustic sensors, Broadcasting, distributed systems, Large-scale systems, RETRI, Random Ephemeral Transaction Identifiers, miniaturization, packet fragmentation, distributed sensors, Power system modeling, wireless low-power unattended sensors, Computer science, node addressing, Wireless sensor networks, data naming]
Applications of probabilistic quorums to iterative algorithms
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Presents a definition of a read-write register that sometimes returns out-of-date values, shows that the definition is implemented by the probabilistic quorum algorithm of D. Malkhi et al. (1997), and shows how to program with such registers using the framework of A. U/spl uml/resin and M. Dubois (1990). Consequently, existing iterative algorithms for an interesting class of problems (including finding shortest paths, constraint satisfaction and transitive closure) converge with high probability if executed in a system in which the shared data is implemented with registers satisfying the new definition. Furthermore, the algorithms in this framework inherit positive attributes concerning load and availability from the underlying register implementation. A monotone version of the new register definition is specified and implemented; it can provide improved expected convergence time and message complexity for iterative algorithms.
[Algorithm design and analysis, iterative methods, convergence time, inheritance, Registers, communication complexity, Convergence, convergence probability, message complexity, constraint satisfaction, availability attributes, load attributes, Polynomials, transitive closure, programming, iterative algorithms, Availability, probability, Data structures, Computer crashes, convergence of numerical methods, out-of-date values, Application software, randomised algorithms, Computer science, read-write register, monotone register definition, shortest paths, shared data, probabilistic quorum algorithm, Iterative algorithms]
Appia, a flexible protocol kernel supporting multiple coordinated channels
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Distributed applications are becoming increasingly complex, often requiring the simultaneous use of several communication channels with different qualities-of-service. This paper presents the Appia system, a protocol kernel that supports applications requiring multiple coordinated channels. Appia offers a clean and elegant way for the application to express inter-channel constraints, such as, for instance, that all channels should provide consistent information about the failures of remote nodes. These constraints can be implemented as protocol layers that can be dynamically combined with other protocol layers.
[Protocols, Virtual environment, Computational modeling, Computer simulation, protocol layers, flexible protocol kernel, distributed processing, Application software, Distributed computing, distributed applications, inter-channel constraints, Appia, Runtime, Streaming media, remote node failure, Collaborative work, protocols, multiple coordinated channels, consistent information, Kernel, communication channels]
A secure access control mechanism against Internet crackers
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Internet servers are always in danger of being "highjacked" by various attacks, like the buffer overflow attack. We propose a process cleaning technique for making an access control mechanism secure against hijacking. To minimize damage in cases where the full control of the servers is stolen, access restrictions must be imposed on the servers. However, designing a secure access control mechanism is not easy, because that mechanism itself can be a security hole. Process cleaning prevents malicious code injected by a cracker from illegally removing access restrictions from a hijacked server. In this paper, we describe the access control mechanism of our Compacto operating system using process cleaning. According to the results of our experiments, process cleaning can be implemented with acceptable performance overheads.
[Access control, telecommunication security, Internet crackers, Internet servers, File servers, Electronic mail, damage minimization, server control hijacking, Operating systems, network operating systems, file servers, authorisation, computer crime, Web server, Protection, process cleaning technique, Cleaning, Image restoration, buffer overflow attack, performance overhead, computer network management, malicious code injection, secure access control mechanism, Compacto operating system, Internet, Buffer overflow, access restrictions]
Adaptive approaches to relieving broadcast storms in a wireless multihop mobile ad hoc network
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
In a multihop mobile ad hoc network, broadcasting is an elementary operation to support many applications. In (Ni et al., 1999), it is shown that naively broadcasting by flooding may cause serious redundancy, contention, and collision in the network, which we refer to as the broadcast storm problem. Several threshold-based schemes are shown to perform better than flooding in (Ni et al., 1999). However, how to choose thresholds also poses a dilemma between reachability and efficiency under different host densities. We propose several adaptive schemes, which can dynamically adjust thresholds based on local connectivity information. Simulation results show that these adaptive schemes can offer better reachability as well as efficiency as compared to the results in (Ni et al., 1999).
[network collision, network flooding, network contention, Mobile communication, Routing, adaptive schemes, Relays, Delay, broadcast storms, Mobile ad hoc networks, Computer science, Intelligent networks, Storms, mobile communication, telecommunication network routing, threshold-based schemes, Spread spectrum communication, Broadcasting, redundancy, local connectivity information, wireless multihop mobile ad hoc network]
Selective checkpointing and rollbacks in multithreaded distributed systems
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Modern distributed systems are often multithreaded and object-oriented in their design. They require efficient techniques to checkpoint and restore their state for improving fault-tolerance properties. The traditional process-based techniques of distributed checkpointing and rollback algorithms suffer from the problem of false dependencies, which makes them very rigid and inefficient for use with modern systems. In this paper, we develop protocols that can selectively checkpoint (and rollback) some threads of a distributed system while leaving others untouched, and yet ensuring the consistency of state resulting from such a partial rollback.
[Checkpointing, selective rollbacks, Protocols, process-based techniques, distributed checkpointing, partial rollback, state consistency, Yarn, system recovery, Design engineering, Fault tolerance, Fault tolerant systems, object-oriented design, Modems, protocols, selective checkpointing, multi-threading, fault tolerance, multithreaded distributed systems, Programming profession, software fault tolerance, Computer science, Multithreading, false dependencies, state restoration]
A framework for modeling agent-oriented software
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
With the increasing importance of complex software systems in the software industry, the need for using agent technologies to develop large-scale commercial and industrial software systems is growing rapidly. Such systems are complex, and there is a pressing need for system modeling techniques to support reliable, maintainable and extensible design. G-nets are a type of Petri net defined to support the modeling of a system as a set of independent and loosely-coupled modules. In this paper, we first introduce an extension of G-nets - the agent-based G-net - as a generic model for agent design. Then, to progress from an agent-based design model to an agent-oriented model, new mechanisms to support inheritance modeling are introduced. To illustrate our formal modeling technique for multi-agent systems, an example of an agent family in electronic commerce is provided.
[multi-agent systems, agent family, Petri nets, Formal languages, Switches, inheritance, formal specification, large-scale software systems development, Pressing, agent design, complex software systems, Large-scale systems, electronic commerce, inheritance modelling, Multiagent systems, programming theory, Object oriented modeling, formal modelling technique, agent-based G-nets, reliable maintainable extensible design, Maintenance, industrial software systems, software agents, large-scale systems, agent-oriented software modelling framework, commercial software systems, Software systems, Computer industry, subroutines, independent loosely-coupled modules]
Dynamic load sharing with unknown memory demands in clusters
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
A compute farm is a pool of clustered workstations to provide high performance computing services for CPU-intensive, memory-intensive, and I/O active jobs in a batch mode. Existing load sharing schemes with memory considerations assume jobs' memory demand sizes are known in advance or predictable based on users' hints. This assumption can greatly simplify the designs and implementations of load sharing schemes, but is not desirable in practice. In order to address this concern, we present three new results and contributions in this study. Conducting Linux kernel instrumentation, we have collected different types of workload execution traces to quantitatively characterize job interactions, and modeled page fault behavior as a function of the overloaded memory sizes and the amount of jobs' I/O activities. Based on experimental results and collected dynamic system information, we have built a simulation model which accurately emulates the memory system operations and job migrations with virtual memory considerations. We have proposed a memory-centric load sharing scheme and its variations to effectively process dynamic memory, allocation demands, aiming at minimizing execution time of each individual job by dynamically migrating and remotely submitting jobs to eliminate or reduce page faults and to reduce the queuing time for CPU services. Conducting trace-driven simulations, we have examined these load sharing policies to show their effectiveness.
[workstation clusters, Unix, storage allocation, batch mode, trace driven simulation, Linux kernel, virtual memory, resource allocation, memory-intensive, Workstations, job interactions, Kernel, Monitoring, operating system kernels, experimental results, Electric breakdown, Instruments, Computational modeling, input output active jobs, unknown memory demands, virtual storage, Educational institutions, workload execution traces, queuing time, Application software, dynamic load sharing, Computer science, CPU-intensive, Linux, page fault behavior, execution time, high performance computing]
Self-stabilizing PIF algorithm in arbitrary rooted networks
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
We present a deterministic distributed Propagation of Information with Feedback (PIF) protocol in arbitrary rooted networks. The proposed algorithm does not use a preconstructed spanning tree. The protocol is self-stabilizing, meaning that starting from an arbitrary state (in response to an arbitrary perturbation modifying the memory state), it is guaranteed to behave according to its specification. Every PIF wave initiated by the root inherently creates a tree in the graph. So, the tree is dynamically created according to the progress of the PIF wave. This allows our PIF algorithm to take advantage of the relative speed of different components of the network. The proposed algorithm can be easily used to implement any self-stabilizing system which requires a (self-stabilizing) wave protocol running on an arbitrary network.
[Algorithm design and analysis, Protocols, fault tolerance, arbitrary rooted networks, trees (mathematics), deterministic distributed PIF protocol, tree, self-stabilizing wave protocol, Distributed computing, deterministic algorithms, Computer science, Intelligent networks, Fault tolerance, Propagation of Information with Feedback protocol, Tree graphs, Fault detection, Feedback, distributed algorithms, self-stabilizing PIF algorithm, Broadcasting, protocols]
Distributed admission control for anycast flows with QoS requirements
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
We study a distributed admission control (DAC) procedure for anycast flows with QoS requirements. We focus on algorithms that perform destination selection, which is critical in anycast. Several algorithms are proposed. These algorithms differ from each other in their dependence on system status information. We also address the issue of resource reservation and re-trial control in the DAC procedure. Performance data obtained by mathematical analysis and computer simulation show that in terms of admission probabilities, DAC systems that are based on local status information can perform closely to those that utilize global and dynamic status information. We note that the latter is much more expensive and difficult to realize.
[resource reservation, Scalability, Computer simulation, computer networks, digital simulation, admission probabilities, quality of service, mathematical analysis, Centralized control, Information analysis, resource re-trial control, Computer science, distributed admission control, Unicast, destination selection, Admission control, distributed algorithms, Distributed control, computer simulation, Computer networks, Performance analysis, system status information, anycast flows]
OSU-MAC: a new, real-time medium access control protocol for wireless WANs with asymmetric wireless links
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
In this paper, we document our design of a MAC protocol, called OSU-MAC, subject to the physical layer characteristics and constraints of a narrow-band wireless modem testbed currently being built at the Ohio State University. The narrow-band wireless modem testbed is expected to support both real-time (bus location tracking) and non-real-time (regular) data applications. A number of techniques are proposed to support QoS imposed by the real-time applications, to deal with the asymmetry on the forward and reverse channels and the half-duplex transmission constraint imposed by the physical layer, and to enhance the error control capability of OSU-MAC. We also present simulation results to demonstrate the key functional characteristics of OSU-MAC.
[Real time systems, real-time medium access control protocol, wide area networks, Wireless application protocol, Quality of service, wireless modem testbed, access protocols, constraints, physical layer characteristics, Narrowband, wireless WANs, error control capability, QoS, Modems, key functional characteristics, Testing, Wide area networks, OSU-MAC, asymmetric wireless links, Access protocols, Physical layer, half-duplex transmission constraint, quality of service, simulation results, Media Access Protocol, wireless LAN]
Pushing the limits of multicast in ad hoc networks
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Focuses on the requirements of "better than best effort" (high hop-by-hop delivery guarantee) broadcasting in highly dynamic mobile multi-hop ad-hoc networks (MANETs). Our work is motivated by mission-critical applications, such as disaster relief and military operations. This class of applications is characterized by: (1) high delivery guarantee requirements, even in the presence of high mobility, and (2) a broadcast style of communication, where all nodes are receivers. Extensive simulations conducted on two different platforms show that, as node speeds, network traffic load and number of senders increase, the performance of existing multicast protocols (exemplified by ODMRP and MAODV) degrades in terms of packet delivery and overhead. In contrast, simple flooding, while clearly not a panacea, performs comparatively well and shows promise as a foundation for more specialized protocols for highly dynamic MANETs of the future.
[sender numbers, Telecommunication traffic, receiver nodes, hop-by-hop delivery guarantee, network traffic load, disaster relief operations, Intelligent networks, multicast protocol performance, packet delivery, MAODV, Spread spectrum communication, multicast communication, mission-critical applications, Broadcasting, Traffic control, Routing protocols, Robustness, protocols, delivery guarantee requirements, Computational modeling, military operations, overhead, node speeds, performance evaluation, Multicast protocols, Ad hoc networks, flooding, broadcasting, simulations, ODMRP, mobile communication, dynamic MANET, specialized protocols, mobile multi-hop ad-hoc networks]
Combining generality and practicality in a conit-based continuous consistency model for wide-area replication
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Replication is a key approach to scaling wide-area applications. However, the overhead associated with large-scale replication quickly becomes prohibitive across wide-area networks. One effective approach to addressing this limitation is to allow applications to dynamically trade reduced consistency for increased performance and availability. Although extensive study has been performed on relaxed consistency models in traditional replicated databases, none of the models can simultaneously achieve the following two typically conflicting requirements imposed by wide-area applications: generality (capturing application-specific consistency semantics) and practicality (enabling efficient application-independent consistency protocols to be designed and providing natural ways to express application semantics). We propose a conit-based continuous consistency model designed to simultaneously achieve generality and practicality. Our conit theory provides generality, where application-specific consistency requirements are exported as conits. Practicality is achieved by using a simple, spanning set of metrics for conit consistency and by using a per-write weight specification. We demonstrate the generality of our model through representative wide-area applications and by showing that a number of existing models can be expressed as instances of our model. Our efficient, application-independent consistency protocols and prototype implementation verify its practicality.
[wide area networks, wide-area replication, large-scale replication, consistency, conit-based continuous consistency model, Databases, wide-area networks, Prototypes, Hafnium, Large-scale systems, software performance evaluation, replicated databases, Access protocols, Sensor systems and applications, data integrity, Application software, application-specific consistency semantics, Programming profession, Computer science, per-write weight specification, performance, generality, practicality, Collaboration, application-independent consistency protocols, metrics]
Differentiated caching services; a control-theoretical approach
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
The increasing diversity of Internet appliances calls for an architecture for performance differentiation on information access. The World Wide Web is the dominant interface for information access today. Web proxy caching is the key performance accelerator in the Web infrastructure. While many research efforts have addressed performance differentiation in the network and on Web servers, providing multiple levels of service in proxy caches has received much less attention. This paper has two main contributions. First, we describe, implement and evaluate an architecture for differentiated content caching services as a key element of the Internet infrastructure. Second, we describe a control-theoretical approach that lays well-understood theoretical foundations for resource management to achieve performance differentiation in proxy caches. We describe our experiences with implementing the differentiated caching services scheme in Squid, a popular proxy cache used by many Internet service providers today. Experimental studies and analyses prove that differentiated caching services provide significant better performance to the premium content classes.
[performance accelerator, resource management, control theory, proxy caches, World Wide Web, cache storage, Delay, Information analysis, Home appliances, resource allocation, Squid, Web and internet services, Web proxy caching, file servers, information access, Computer architecture, Workstations, multiple service levels, Web server, telecommunication control, performance evaluation, performance differentiation, Internet service providers, Computer science, Internet infrastructure, Internet appliances, system architecture, premium content classes, differentiated caching services, Internet, Web sites, Acceleration]
An analytical study of opportunistic lease renewal
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
We present opportunistic renewal, a lease management protocol designed to keep distributed file systems or distributed shared memories consistent in the presence of a network partition or other computer failures. Our treatment includes an analytical model of the protocol that compares performance with existing lease protocols and quantifies improvements. In addition, this analytical model provides the structure to understand message overhead and availability trade-offs when selecting lease parameters. We include results demonstrating that opportunistic renewal substantially reduces the network overhead associated with lease renewal. As a corollary, opportunistic renewal can reduce the lease length at any given network overhead; e.g., by a factor of 50 at 1% network overhead. Lower overhead makes leasing less intrusive and shorter lease periods allow a system to recover from failure more quickly.
[network partition, opportunistic lease renewal, Protocols, distributed file systems, failure recovery, memory protocols, Application software, system recovery, software fault tolerance, network overhead, Analytical models, Network servers, computer failure, File systems, distributed shared memories, Operating systems, network operating systems, distributed shared memory systems, Computer networks, message overhead, Large-scale systems, Internet, Computer network management, lease management protocol]
The effects of inter-packet spacing on the delivery of multimedia content
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Streaming multimedia content with UDP has become increasingly popular over distributed systems such as the Internet. However, because UDP does not possess any congestion control mechanism and most best-effort traffic is served by the congestion-controlled TCP, UDP flows steal bandwidth from TCP to the point that TCP flows can starve for network resources. Furthermore, such applications may cause the Internet infrastructure to eventually suffer from congestion collapse because UDP traffic does not self-regulate itself. To address this problem, next-generation Internet routers will implement active queue management schemes to punish malicious traffic, e.g. non-adaptive UDP flows, and to the improve the performance of congestion-controlled traffic, e.g. TCP flows. The arrival of such routers will cripple the performance of today's UDP-based multimedia applications. So, in this paper, we introduce the notion of inter-packet spacing with control feedback to enable these UDP-based applications to perform well in the next-generation Internet while being adaptive and self-regulating. When compared with traditional UDP-based multimedia streaming, we illustrate that our counter-intuitive inter-packet spacing scheme with control feedback can reduce packet loss by 90% without adversely affecting the delivered throughput.
[adaptive self-regulating applications, telecommunication congestion control, network protocol, packet switching, nonadaptive UDP flows, packet loss, Throughput, Adaptive control, delivered throughput, Feedback, multimedia content delivery, UDP-based multimedia applications, Bandwidth, active queue management schemes, Traffic control, distributed systems, Communication system traffic control, inter-packet spacing, rate-adjusting congestion control, multimedia communication, congestion collapse, Internet routers, next-generation Internet, Multimedia systems, UDP traffic, TCP flows, multimedia content streaming, congestion-controlled traffic performance, Programmable control, Internet infrastructure, transport protocols, control feedback, network resource starvation, Streaming media, malicious traffic, Internet]
Fault-tolerant static scheduling for real-time distributed embedded systems
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
We present a heuristic for producing automatically a distributed fault-tolerant schedule of a given data-flow algorithm onto a given distributed architecture. The faults considered are processor failures, with a fail-silent behavior. Fault-tolerance is achieved with the software redundancy of computations and the time redundancy of data-dependencies.
[Real time systems, Actuators, distributed fault-tolerant schedule, real-time distributed embedded systems, processor failures, Distributed computing, distributed architecture, processor scheduling, Automotive engineering, software redundancy, Fault tolerant systems, Embedded system, embedded systems, fault-tolerant static scheduling, redundancy, data dependencies, computations, Redundancy, heuristic, Scheduling algorithm, time redundancy, Processor scheduling, fail-silent behavior, data flow computing, fault tolerant computing, data-flow algorithm, Consumer electronics]
Modular composition of redundancy management protocols in distributed systems: an outlook on simplifying protocol level formal specification and verification
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
In recent years, formal methods (FMs) have been extensively used for the verification and validation (V&V) of dependable distributed protocols. In our studies utilizing FMs for V&V, we have observed that a number of protocols providing for distributed and dependable services can often be formulated using a small set of basic functional primitives or their variations. Thus, from the formal viewpoint, the objective of this paper is to introduce techniques, utilizing concepts of category theory, that could effectively identify and reuse basic formal modules in order to simplify formal specification and verification for a spectrum of protocols.
[Protocols, distributed processing, formal specification, dependable distributed protocol validation, Guidelines, protocol-level formal verification, formal verification, functional primitives, formal module identification, distributed systems, redundancy, computer network reliability, protocol-level formal specification, Virtual prototyping, protocols, Flexible manufacturing systems, Testing, Redundancy, Reliability theory, redundancy management protocols, simplification, State-space methods, Formal specifications, modular composition, formal module reuse, formal methods, category theory, Error correction, dependable services]
Optimal hash routing for Web proxies
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Hash routing is an effective approach for coordinating a collection of Web proxies. In this paper, we present a comprehensive analytical model for hash routing which takes into consideration a number of factors: the original request distribution, the object allocation strategy, the speeds of the proxies and cache hit ratios. Based on this model, the optimal hash routing problems for static and dynamic client configurations are investigated. Two schemes, OBJ-OPT (OBJect OPTimization) and OBJ/DNS-OPT (OBJect and Domain Name Server OPTimization), are proposed to reduce the response times of Web requests. OBJ-OPT optimizes object allocation under a static client configuration, and OBJ/DNS-OPT optimizes both object and DNS allocations under a dynamic client configuration. Extensive trace-driven simulations have been conducted to evaluate the proposed schemes. The results show that they significantly outperform the intuitive scheme based only on the speeds of the proxies.
[object allocation optimization, HTML, cache storage, World Wide Web proxies, analytical model, Web request response times, telecommunication computing, Delay, Uniform resource locators, optimisation, resource allocation, cache hit ratios, OBJ-OPT scheme, optimal hash routing, Web server, discrete event simulation, information resources, client-server systems, request distribution, proxy speed, Image retrieval, domain name server allocation optimization, Routing, Computer science, trace-driven simulations, OBJ/DNS-OPT scheme, performance, telecommunication network routing, dynamic client configuration, static client configuration, file organisation, Explosives, object allocation strategy, Internet, Web sites]
Exploring robustness in group key agreement
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Secure group communication is crucial for building distributed applications that work in dynamic environments and communicate over unsecured networks (e.g. the Internet). Key agreement is a critical part of providing security services for group communication systems. Most of the current contributory key agreement protocols are not designed to tolerate failures and membership changes during execution. In particular, nested or cascaded group membership events (such as partitions) are not accommodated. We present the first robust contributory key agreement protocols, resilient to any sequence of events while preserving the group communication membership and ordering guarantees.
[unsecured networ, Protocols, contributory key agreement protocols, distributed processing, ordering guarantees, Communication system security, distributed applications, dynamic environments, Network servers, Intelligent networks, Robustness, cascaded group membership events, group communication membership, protocols, security services, National security, secure group communication, partitions, Buildings, robustness, Computer science, security of data, robust contributory key agreement protocols, group communication systems, Explosives, fault tolerant computing, Internet, group key agreement]
On slicing a distributed computation
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
We introduce the notion of a slice of a distributed computation. A slice of a distributed computation with respect to a global predicate is a computation which captures those and only those consistent cuts of the original computation which satisfy the global predicate. We show that a slice exists for a global predicate iff the predicate is a regular predicate. We then give an efficient algorithm for computing the slice and show applications of slicing to testing and debugging of distributed programs.
[Software testing, Software maintenance, Educational programs, System testing, program debugging, distributed computation slicing, program testing, Lattices, Debugging, global predicate, regular predicate, Distributed computing, Programming profession, distributed programs, Fault tolerant systems, program slicing, distributed programming]
Towards communication-sensitive load balancing
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Recent studies have shown that if the overhead associated with process migration is not high when compared to the estimated lifetime of processes, migrating isolated processes to underutilized workstations can improve system performance. However, if tightly-coupled processes from groups of communicating processes or other existing dependencies are separated, system performance can decrease due to increased communication cost. Current load balancers do not make effective use of communication dependencies in their algorithms. We have implemented a communication-sensitive load balancer for DUNES (Distributed UNix ExtenSion)-a user-level distributed operating system-that uses the run-time communication pattern between processes when balancing load. We examine communication-sensitive load balancing under different workload conditions and compare its performance against a load balancer that is not communication-aware. Our results show that a communication-sensitive load balancer performs substantially better than a load balancer that is not communication-aware under workloads that have a "good mix" of CPU-bound and I/O-bound processes.
[Unix, workstation clusters, Costs, run-time communication pattern, tightly-coupled processes, Intelligent networks, communicating processes, resource allocation, System performance, communication-sensitive load balancing, network operating systems, Lifetime estimation, Computer networks, Workstations, underutilized workstations, Distributed Unix Extension, estimated process lifetime, isolated processes, process migration overhead, I/O-bound processes, performance evaluation, communication cost, CPU-bound processes, Statistics, DUNES, workload conditions, High performance computing, noncommunication-aware load balancer, Life estimation, system performance, communication dependencies, Load management, user-level distributed operating system]
Enforcing perfect failure detection
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Perfect failure detectors can correctly decide whether a computer is crashed. However it is impossible to implement a perfect failure detector in purely asynchronous systems. We show how to enforce perfect failure detection in timed distributed systems with hardware watchdogs. The two main system model assumptions are: each computer can measure time intervals with a known maximum error; and each computer has a watchdog that crashes the computer unless the watchdog is periodically updated. We have implemented a system that satisfies both assumptions using a combination of off-the-shelf software and hardware.
[distributed processing, Computer crashes, Time measurement, system recovery, off-the-shelf software, perfect failure detection, Heart beat, Fault detection, hardware watchdogs, Fault tolerant systems, off-the-shelf hardware, Detectors, Computer errors, Hardware, fault tolerant computing, Error correction, purely asynchronous systems, timed distributed systems, time intervals, Clocks]
A hierarchical cluster algorithm for dynamic, centralized timestamps
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Partial-order data structures used in distributed-system observation tools typically use vector timestamps to efficiently determine event precedence. Unfortunately all current dynamic vector-timestamp algorithms either require a vector of size equal to the number of processes in the computation or require a graph search operation to determine event precedence. This fundamentally limits the scalability of such observation systems. In this paper we present an algorithm for hierarchical, clustered vector time-stamps. We present results for a variety of computation environments that demonstrate such timestamps can reduce space consumption by more than an order-of-magnitude over Fidge/Mattern timestamps while still providing acceptable time bounds for computing timestamps and determining event precedence.
[Costs, Heuristic algorithms, Scalability, hierarchical cluster algorithm, partial-order data structures, event precedence, graph search operation, time bounds, distributed processing, Data structures, Control systems, vector timestamps, Distributed computing, scalability, Computer science, clustered vector time-stamps, dynamic vector-timestamp algorithms, Clustering algorithms, Data visualization, data structures, distributed-system observation tools, centralized timestamps, Monitoring]
A transparent network handover for nomadic CORBA users
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
This paper introduces a generic disconnection handling and network handover mechanism for nomadic CORBA applications. The disconnection handling provides a transparent or explicit mode. In the explicit mode, a special class provided by the user or application is informed if a disconnection occurs and can take appropriate actions. The handover mechanism supports an active handover, where the handover decision is based on application and user needs as well as a passive handover which is needed if after a disconnection the original network is not available. Both mechanisms provide a solution for the lost reply problem occurring if the mobile node is disconnected before the reply to an outstanding request has arrived. The presented mechanisms are integrated into the generic proxy platform /spl Pi//sup 2/. /spl Pi//sup 2/ transparent encapsulates the wireless link and splits the connection between the client on the mobile node and the server into separate parts. /spl Pi//sup 2/ can be transparently integrated into existing distributed systems without changes to clients or servers using a generic interface and dynamic type determination.
[generic disconnection handling, Technological innovation, Portable computers, transparent network handover, nomadic CORBA users, generic interface, Electronic mail, Batteries, Middleware, Wireless communication, generic proxy platform, Computer displays, Computer architecture, Telematics, dynamic type determination, Computer networks, distributed object management]
Mobile transactional agents
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Mobile agents is an important enabling technology for certain types of real world applications such as e-commerce and workflows. While the potential benefits are appealing, this technology is not being used by many applications. This is largely attributed to the lack of components such as transactions and their integration with de-facto technologies. This work focuses on bridging this gap and devising an extended transactional model for mobile agents. The nature of mobile agents for autonomous processing and mobility introduces numerous challenges with respect to transactions which are originally, addressed in our work. First, given that, unstructured, execution scripts for agents, it is necessary to isolate side-effects that occur between two successive migrations to later be able to commit or abort them locally according to transaction semantics maintained across multiple hosts. Second, intra-transaction parallelism requires synchronization among multiple autonomous agents to join their processing and to rollback a transaction. Third, recovery from failures at a specific host, may necessitate maintaining a global state of a transaction across all the agent's destinations to determine how to proceed (e.g., repeat activities upon recovery). This paper presents a comprehensive transaction model for mobile agents and its deployment in the context of Java-based mobile agents. The overall complexity of transaction management is handled using a division of labor between an object-oriented programming model, protocols and a concurrency control mechanism. Specifically, the programming model maintains separation of concerns between mobility, transactions and application logic in order to cope with the aforementioned challenges.
[object-oriented programming model, complexity, enabling technology, Laboratories, workflows, real world applications, Information systems, programming model, Mobile agents, mobile agents, Robustness, e-commerce, protocols, mobile transactional agents, Object oriented programming, Java, object-oriented programming, Logic programming, Object oriented modeling, Concurrency control, software agents, application logic, concurrency control, Systems engineering and theory, synchronization, Java-based mobile agents]
A general resource allocation synchronization problem
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
We introduce a new synchronization problem called GRASP. We show that this problem is very general, in that it can provide solutions with strong properties to a wide make of previously-studied and new problems. We present a shared-memory solution to this problem that is based on a new solution to the dining philosophers problem with constant failure locality. We use the powerful tool of wait-free transactions to simplify our solution without restricting concurrency.
[Concurrent computing, general resource allocation synchronization, Engineering profession, resource allocation, Laboratories, wait-free transactions, shared-memory solution, Resource management, Sun, GRASP, synchronisation, dining philosophers problem]
Fast reconciliations in fluid replication
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Mobile users can increasingly depend on high speed connectivity. Despite this, using distributed file services across the wide area is painful. Fast approaches sacrifice one or more of safety, visibility, and consistency in the name of performance. Instead, we propose fluid replication, the ability to create replicas where and when needed. These replicas, called WayStations, maintain consistency with home servers through periodic reconciliations. Two techniques make reconciliation fast; this is crucial to the success of fluid replication. First, we defer propagation of updates, and only invalidate files during a reconciliation. Second, rather than depend on operation logs, we provide the subtrees in which all updates have occurred. These subtrees, named by their least common ancestors, or LCAs, can be constructed incrementally, and reduce the burden of checking serializability during a reconciliation. While these techniques provide better performance, they are not without risk. Bulk invalidation can lead to false sharing, optimistic updates are subject to conflict, and deferred updates may cause performance problems if they are needed elsewhere. To address these concerns, we performed a trace-based evaluation of our algorithms.
[Performance evaluation, Costs, Ethernet networks, distributed file services, Network servers, mobile computing, File systems, Wireless networks, serializability, high speed connectivity, Safety, mobile users, trace-based evaluation, software performance evaluation, replicated databases, Peer to peer computing, subtrees, performance evaluation, Computer science, fluid replication, replicated database, bulk invalidation, update propagation, WayStations, Mobile computing, wide area network]
Backoff protocols for distributed mutual exclusion and ordering
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Presents a simple and efficient protocol for mutual exclusion in synchronous message-passing distributed systems subject to failures. Our protocol borrows design principles from prior work in backoff protocols for multiple access channels such as the Ethernet. Our protocol is adaptive in that the expected amortized system response time - informally, the average time a process waits before entering the critical section - is a function only of the number of clients currently contending and is independent of the maximum number of processes that might contend. In particular, in the contention-free case, a process can enter the critical section after only one round-trip message delay. We use this protocol to derive a protocol for ordering operations on a replicated object in an asynchronous distributed system subject to failures. This protocol is always safe, is probabilistically live during periods of stability and is suitable for deployment in practical systems.
[Disruption tolerant networking, Ethernet networks, probabilistically live protocol, distributed mutual exclusion, asynchronous distributed system, contention-free case, local area networks, access protocols, contending client number, round-trip message delay, amortized system response time, Fault tolerance, average process waiting time, safety, multi-access systems, multiple access channels, message passing, Stability, failures, Delay effects, stable periods, backoff protocols, Access protocols, Interference, synchronous message-passing distributed systems, critical section, Computer science, Upper bound, Fault detection, delays, Ethernet, replicated object, ordering operations]
A lattice based framework of shared memory consistency models
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
A memory consistency model specifies certain aspects of the behavior of a memory system. Stronger consistency models are easier for programmers to use, but provide less flexibility for optimizing the memory implementation. More relaxed consistency models are just the opposite. The goal of our work is to develop a framework that captures the relationships among existing models, and maps the territory of possible models that have not yet been discovered. This work is based on the idea of orthogonal consistency properties. hypothesize that all consistency models can be represented by different combinations of a few primitive properties. The work comes from examining the PRAM, cache, processor, and causal consistency models. Processor is a combination of PRAM and cache, and causal is a combination of PRAM plus an additional requirement. These factors suggest an underlying structure to the models.
[cache, Lattices, causal consistency models, Read-write memory, Phase change random access memory, concurrency theory, shared memory consistency models, cache storage, PRAM, processor, Programming profession, Computer science, lattice based framework, Databases, orthogonal consistency properties, shared memory systems]
Anonymous Gossip: improving multicast reliability in mobile ad-hoc networks
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
In recent years, a number of applications of ad-hoc networks have been proposed. Many of them are based on the availability of a robust and reliable multicast protocol. We address the issue of reliability and propose a scalable method to improve packet delivery of multicast routing protocols and decrease the variation in the number of packets received by different nodes. The proposed protocol works in two phases. In the first phase, any suitable protocol is used to multicast a message to the group, while in the second concurrent phase, the gossip protocol tries to recover lost messages. Our proposed gossip protocol is called Anonymous Gossip (AG) since nodes need not know the other group members for gossip to be successful. This is extremely desirable for mobile nodes, that have limited resources, and where the knowledge of group membership is difficult to obtain. As a first step, anonymous gossip is implemented over MAODV without much overhead and its performance is studied. Simulations show that the packet delivery of MAODV is significantly improved and the variation in number of packets delivered is decreased.
[group membership, gossip protocol, Intelligent networks, multicast routing protocols, mobile ad-hoc networks, Computer network reliability, packet delivery, MAODV, multicast communication, telecommunication network reliability, Routing protocols, Robustness, protocols, Availability, Peer to peer computing, Anonymous Gossip, Multicast protocols, Ad hoc networks, Application software, multicast reliability, simulations, Computer science, mobile communication, telecommunication network routing, lost message recovery]
An application of parameter estimation to route discovery by on-demand routing protocols
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
To discover a route to a peer node, an on-demand routing protocol may initiate a flood-search procedure known as route discovery. By selecting the correct query radius, the number of packet transmissions required for route discovery can be minimized. This paper presents methods to estimate the geographic radius (R/sub G/) and the number of currently active pairs of communicating nodes (P) in a mobile ad-hoc network. The methods are entirely distributed and incur little communication overhead. Network nodes can apply the estimated parameters to predict the probability mass function (PMF) of the route discovery hop distance. An accurate prediction of the PMF aids the selection of an appropriate query radius for the route discovery process. A computationally lightweight procedure to select an appropriate query radius, based only on an estimate of P, is also proposed. Simulation results show that this procedure facilitates a sensible tradeoff between the route request packet overhead and the route reply delay.
[Parameter estimation, simulation, query radius, communication overhead, Delay, Mobile ad hoc networks, Network topology, packet transmission minimization, geographic radius, Broadcasting, parameter estimation, probability mass function, hop distance, route reply delay, Routing protocols, peer node, Contracts, Peer to peer computing, probability, active communicating node pairs, Roentgenium, mobile ad-hoc network, transport protocols, distributed methods, telecommunication network routing, delays, route discovery, Information processing, on-demand routing protocols, flood-search procedure, route request packet overhead]
Open metadata formats: efficient XML-based communication for heterogeneous distributed systems
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
The definition and translation of metadata is incorporated in all systems that exchange structured data. We observe that the manipulation of this metadata can be decomposed into three separate steps: discovery, binding of program objects to the metadata, and marshaling of data to and from wire formats. We have designed a method of representing message formats in XML, using data types that are available in the XML schema specification. We have implemented a tool called xml2wire that uses such metadata and exploits this decomposition in order to provide flexible metadata definition facilities for an efficient binary communications mechanism. We also observe that the use of xml2wire makes possible such flexibility without intolerable performance effects.
[metadata definition, open systems, Design methodology, metadata manipulation, XML schema specification, message format representation, distributed processing, Wire, Distributed computing, data marshaling, flexibility, heterogeneous distributed systems, xml2wire, wire formats, metadata translation, structured data exchange, data discovery, Libraries, open metadata formats, data types, Manufacturing, hypermedia markup languages, meta data, XML-based communication, binary communications mechanism, abstract data types, Data structures, Educational institutions, program object binding, Chemicals, electronic data interchange, performance, XML, Collaboration]
A traveling salesman mobility model and its location tracking in PCS networks
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
This paper considers the location tracking problem in PCS networks. How a solution to this problem performs in fact highly, depends on the mobility patterns of users. In this paper we propose a new traveling salesman mobility (TSM) model, in the hope of catching the mobility patterns of a large group of users. The TSM model is characterized by features of "stop-or-move\
[GSM, PCS networks, Base stations, personal communication networks, Educational technology, Traveling salesman problems, location tracking, mobility patterns, Paging strategies, Information analysis, location tracking strategy, simulations, Computer science, travelling salesman problems, Intelligent networks, Traffic control, traveling salesman mobility model, Personal communication networks]
Adaptive parameter collection in dynamic distributed environments
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Cost-effectively collecting distributed state information is a challenging problem. There is perhaps no single perfect solution since different distributed application environments pose different requirements from the information collection process. Knowledge of the environment, in terms of traffic conditions, load models etc., plays a key role in determining tradeoffs between accuracy (needed to ensure quality-of-service requirements) and cost-effectiveness. We develop an adaptive information collection algorithm that utilizes network traffic knowledge characterized using a time series model. The algorithm utilizes an information collection architecture consisting of a directory service integrated into the middleware layer with monitoring modules distributed across the network. The cost-effectiveness of the proposed information collection algorithm proposed is verified in simulations over diverse network traffic patterns, i.e. Internet WAN (TCP), MPEG (multimedia) and Web access traffic traces. Our results show that the proposed adaptive information collection algorithm compensates for inaccuracies in network traffic predictions in a cost-effective manner.
[Protocols, Telecommunication traffic, time series model, Network servers, Traffic control, Communication system traffic control, Monitoring, quality-of-service, Load modeling, middleware, client-server systems, cost-effectiveness, MPEG, computer networks, time series, quality of service, Application software, directory service, adaptive parameter collection, Computer science, distributed state information, network traffic, Load management, Web access traffic traces, Internet, dynamic distributed environments, telecommunication traffic]
MVSS: Multi-View Storage System
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Presents MVSS, a storage system for active storage devices. MVSS offers a single framework for supporting various services at the device level. It provides a flexible interface for associating services to a file through multiple views of the file. Similar to views of a database in a multi-view database system, views in MVSS are generated dynamically and are not stored on physical storage devices. MVSS represents each view of an underlying file through a separate entry in the file system namespace. MVSS separates the deployment of services from file system implementations and thus allows services to be migrated to the storage devices. The paper presents the design of MVSS and shows how different services can be supported in MVSS at the device level. To illustrate our approach, we implemented a prototype system on PCs running Linux. We present results from the prototype implementation to demonstrate the effectiveness of our approach.
[Unix, Costs, Scalability, flexible interface, microcomputer applications, active storage devices, file system namespace, file system implementations, Network servers, File systems, device-level service support, Prototypes, Computer networks, Database systems, Cryptography, Multi-View Storage System, file-service association, service deployment, storage media, virtual storage, personal computers, MVSS, Linux, prototype system, service migration, Personal communication networks, dynamically generated views]
Adaptive protocols for agent migration
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
A framework for building adaptive network protocols for agent migration over a network is presented. A key idea of the framework is to introduce mobile agents as first-class objects. That is, the framework allows network protocols for agent migration to be naturally implemented within mobile agents and then dynamically deployed at network nodes by migrating the agents that carry the protocols. A prototype implementation was built on a hierarchical mobile agent system, and several practical protocols for agent migration were designed and implemented. These protocols can have significant contributions to active network technology as well as mobile agent technology.
[Protocols, Adaptive systems, Navigation, first-class objects, Electronic mail, hierarchical mobile agent system, Electronic commerce, software agents, network nodes, Mobile agents, adaptive network protocols, active network technology, mobile agents, agent migration, Computer networks, Cryptography, Data communication, protocols, Mobile computing, distributed programming]
Efficient generalized deadlock detection and resolution in distributed systems
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Presents a distributed algorithm for detecting generalized deadlocks in distributed systems. The algorithm constructs a distributed spanning tree through the propagation of probes and receiving replies from those probes. The initiator of the algorithm builds a local wait-for graph to determine the existence of deadlock. A scheme for encoding the path information from the initiator to each process is developed so that ancestor-descendant relationships between the tree nodes are not explicitly sent to the initiator but are inferred at the initiator. The advantages of the proposed algorithm are: (1) all deadlocks reachable from the initiator are resolved, whereas current algorithms detect deadlock only if the initiator is in deadlock; (2) deadlock resolution is simplified without additional message transmission, due to the availability of dependency relations among processes at the initiator; and (3) a unique property of the algorithm is that it handles concurrent algorithm executions and prevents duplicate deadlock detection which may cause false deadlock resolution, whereas most deadlock detection algorithms ignore this issue and deal with a single execution of the algorithm. In addition, our scheme provides a solution to the problem of G. Bracha et al.'s (1987) algorithm that may not detect a deadlock if the lower-priority execution is simply discarded. Our algorithm performs better than or comparably to the current best algorithms in terms of both message and time complexities.
[ancestor-descendant relationship, local wait-for graph, path information encoding, Distributed computing, reachable deadlocks, Postal services, deadlock resolution, concurrent algorithm executions, message complexity, Tree graphs, distributed systems, Computer science education, Distributed algorithms, Probes, algorithm initiator, dependency relations, trees (mathematics), tree nodes, time complexity, probe propagation, Encoding, low-priority execution, Bracha algorithm, generalized deadlock detection, distributed algorithm, distributed spanning tree, distributed algorithms, algorithm performance, concurrency control, System recovery, Systems engineering and theory, Detection algorithms, computational complexity, duplicate deadlock detection]
Dynamic migration algorithms for distributed object systems
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Complex distributed object systems require dynamic migration algorithms that allocate and reallocate objects to respond to changes in the load or in the availability of the resources. We present the Cooling and Hot-Spot migration algorithms that reallocate objects when the load on a processor is high or when the latency of a task is high. The algorithms have been implemented as a feedback loop in the Eternal Resource Management System where information obtained from monitoring the behavior of the objects and the usage of the processors' resources is used to dynamically balance the load on the processors and improve the latency of the tasks. The cost of moving an object is justified by amortization over many method invocations, and constrains the rate at which objects are moved. The experimental results show that our algorithms guarantee steady flow of operation for the tasks and gracefully migrate objects from the processors when processor overloads and high task latencies are detected.
[distributed object systems, Costs, load balancing, Heuristic algorithms, Delay, Cooling migration algorithm, Feedback loop, resource allocation, System performance, high task latencies, Eternal Resource Management System, Monitoring, distributed object management, dynamic migration algorithms, Availability, experimental results, Cooling, Hot-Spot migration algorithm, method invocations, processor overload, distributed algorithms, Object detection, object allocation, Resource management, processor load, task latency]
Unifying stabilization and termination in message-passing systems
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
We dispel the myth that it is impossible for any stabilizing message passing program to be terminating. We identify fixpoint-symmetry as a necessary condition for a message passing stabilizing program to be terminating. Our results do confirm that a number of well-known input-output problems (e.g., leader election and consensus) do not admit a terminating and stabilizing solution. On the flip side, they show that reactive problems such as mutual exclusion and reliable-transmission do admit such solutions. We go on to present stabilizing and terminating programs for both problems. Also, we describe a way to add termination to a stabilizing program, and demonstrate it in the context of our design of a solution to the reliable-transmission problem.
[Context, message passing, program verification, stabilization, software reliability, Nominations and elections, mutual exclusion, input-output problems, Computer crashes, Mathematics, stabilizing message passing program, Computer science, Information science, reliable transmission, program termination, fixpoint symmetry]
Comparing and contrasting adaptive middleware support in wide-area and embedded distributed object applications
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
The Quality Objects (QuO) middleware is a set of extensions to standard distributed object computing middleware that is used to control and adapt the quality of service in a number of distributed application environments, from wide-area to embedded distributed applications. This paper compares and contrasts the characteristics of key use cases and the variations in QuO implementations that have emerged to support them. We present these variations in the context of several actual applications being developed using the QuO middleware.
[Real time systems, wide area networks, QuO middleware, Quality of service, Distributed computing, Network servers, use cases, Tail, Skeleton, distributed object management, service quality adaptation, Embedded computing, Java, client-server systems, telecommunication control, embedded distributed object applications, Ice, quality of service, Middleware, adaptive systems, distributed object computing, service quality control, distributed application environments, Quality Objects middleware, adaptive middleware support, wide-area distributed object applications]
Performance analysis of the General Packet Radio Service
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Presents an efficient and accurate analytical model for the radio interface of the General Packet Radio Service (GPRS) in a GSM network. The model is utilized for investigating how many packet data channels should be allocated for GPRS under a given amount of traffic in order to guarantee appropriate quality of service. The presented model constitutes a continuous-time Markov chain. The Markov model represents the sharing of radio channels by circuit-switched GSM connections and packet-switched GPRS sessions under a dynamic channel allocation scheme. In contrast to previous work, the Markov model explicitly represents the mobility of users by taking into account arrivals of new GSM and GPRS users as well as handovers from neighboring cells. To validate the simplifications necessary for making the Markov model amenable to numerical solution, we provide a comparison of the results of the Markov model with a detailed simulator on the IP level.
[Ground penetrating radar, radio channel sharing, radio interface, General Packet Radio Service, user arrivals, Circuits, packet switching, Telecommunication traffic, Quality of service, circuit switching, analytical model, service quality guarantee, Analytical models, packet radio networks, Packet radio networks, Traffic control, circuit-switched GSM connections, Performance analysis, channel allocation, continuous time systems, simplification validation, packet-switched GPRS sessions, GSM, continuous-time Markov chain, dynamic channel allocation scheme, performance evaluation, quality of service, packet data channel allocation, numerical solution, IP-level simulator, user mobility, GSM network, Channel allocation, Markov processes, communication traffic, performance analysis, neighboring cell handovers, cellular radio]
RAD: a compile-time solution to buffer overflow attacks
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Buffer overflow attack can inflict upon almost arbitrary programs and is one of the most common vulnerabilities that can seriously compromise the security of a network-attached computer system. This paper presents a compiler-based solution to the notorious buffer overflow attack problem. Using this solution, users can prevent attackers from compromising their systems by changing the return address to execute injected code, which is the most common method used in buffer overflow attacks. Return address defender (RAD) is a simple compiler patch that automatically creates a safe area to store a copy of return addresses and automatically adds protection code into applications that it compiles to defend programs against buffer overflow attacks. Using it to protect a program does not need to modify the source code of the protected programs. Moreover, RAD does not change the layout of stack frames, so binary code it generated is compatible with existing libraries and other object files. Empirical performance measurements on a fully operational RAD prototype show that programs protected by RAD only experience a factor of between 1.01 to 1.31 slow-down. In this paper we present the principle of buffer overflow attacks, a taxonomy of defense methods, the implementation details of RAD, and the performance analysis of the RAD prototype.
[Measurement, arbitrary programs, Taxonomy, return address defender, network-attached computer system, program compilers, Program processors, security, security of data, buffer overflow attacks, compiler patch, RAD, Prototypes, Binary codes, compile-time solution, Computer networks, Libraries, Buffer overflow, binary code, Protection, Computer security, stack frames]
Developing and refining an adaptive token-passing strategy
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Token rotation algorithms play an important role in distributed computing, to support such activities as mutual exclusion, round-robin scheduling, group membership and group communication protocols. Ring-based protocols maximize throughput in busy systems but can incur a linear (in the number of processors) delay when a processor needs to obtain a token to perform an operation. This paper synthesizes new algorithmic techniques for improving the performance (responsiveness) of logical ring protocols. The parameterized technique presents the safety properties of ring protocols and maintains high throughput in busy systems, while reducing the delay in lightly loaded systems from a linear to a logarithmic function in the number of processors. The development in this paper is done using term rewriting systems, where our parameterized protocol is developed in a series of safety-preserving refinements of a basic specification.
[logarithmic function, mutual exclusion, distributed processing, Throughput, group membership, busy systems, communication complexity, group communication protocols, Delay, processor scheduling, distributed computing, Design optimization, responsiveness, parameterized protocol, lightly loaded systems, safety, Safety, protocols, Round robin, rewriting systems, adaptive token-passing strategy, throughput maximization, Access protocols, performance evaluation, logical ring protocols, processor number, Scheduling algorithm, adaptive systems, term rewriting systems, Computer science, round-robin scheduling, Processor scheduling, delay, performance, ring-based protocols, delays, Ear, safety-preserving specification refinements, token rotation algorithms, safety properties]
Tight space self-stabilizing uniform l-mutual exclusion
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
A self-stabilizing algorithm, regardless of the initial system state, converges in finite time to a set of states that satisfy a legitimacy predicate without the need for explicit exception handler of backward recovery. The l-mutual exclusion is a generalization of the fundamental problem of mutual exclusion: the system has to guarantee the fair sharing of a resource that can be used by l processors simultaneously. We present a space efficient solution to the l-mutual exclusion problem that performs on uniform unidirectional ring networks and that is self-stabilizing. Our solution improves the space complexity of previously known approaches by a factor of min(n/sup 2//spl times/log(n), 1/l/spl times/log/sup l-1/ (n)), while retaining none of their drawbacks in terms of system hypothesis (we support unfair scheduler and ensure strong correctness) or specification verification (we guarantee high level 2-mutual exclusion). When l is fixed, the space complexity at each node is constant in average, making our approach suitable for scalable systems.
[uniform unidirectional ring networks, distributed processing, Distributed computing, strong correctness, system hypothesis, Processor scheduling, explicit exception handler, tight space self-stabilizing uniform l-mutual exclusion, backward recovery, Robustness, fault tolerant computing, fair sharing, space complexity, computational complexity, specification verification]
Transactions on partially replicated data based on reliable and atomic multicasts
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
We focus on a partial data replication model in which each object is made fault-tolerant by a process group. We propose a replication control protocol that provides transaction properties based both on reliable and atomic multicast primitives and a two phase locking protocol. Furthermore, we propose two deadlock prevention rules to obtain two variants of the base protocol, one that favors the execution of transaction write operations over read operations and another one that does the contrary, favoring read over write. The latter variant ensures that read-only transactions are never aborted. Both protocols ensure that all transactions are not aborted by deadlock prevention rules.
[transaction processing, replicated data transactions, replication control protocol, deadlock prevention rules, Costs, replicated databases, fault-tolerance, Communication system control, Access protocols, memory protocols, reliable atomic multicast, Multicast protocols, Control systems, Transaction databases, read-only transactions, software fault tolerance, partial data replication model, two phase locking protocol, transaction write operations, Fault tolerance, concurrency control, multicast communication, System recovery, Broadcasting]
Multiprocessor preprocessing algorithms for uniprocessor on-line scheduling
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
H. Chetto and M. Chetto (1989) presented an algorithm for the online admission control and run-time scheduling of aperiodic real-time jobs in preemptive uniprocessor environments that are executing systems of periodic hard real-time tasks. This algorithm requires a significant degree of preprocessing of the system of periodic tasks - in general, this preprocessing takes a time that is exponential in the representation of the periodic task system. In this paper, we develop techniques for speeding up the preprocessing phase of the Chetto & Chetto algorithm, by adapting it for implementation in parallel environments. We validate the effectiveness of our parallelization both by theoretical results and through simulation experiments.
[Real time systems, periodic task system representation, periodic hard real-time tasks, telecommunication congestion control, simulation, uniprocessor online scheduling, online admission control, processor scheduling, Concurrent computing, Runtime, run-time scheduling, Communication system traffic control, parallel algorithms, multiprocessing programs, Computational modeling, exponential time complexity, multiprocessor preprocessing algorithms, parallel implementation, preemptive uniprocessor environments, parallelization, Scheduling algorithm, Processor scheduling, online operation, Admission control, Character generation, real-time systems, aperiodic real-time jobs, Timing, computational complexity]
LIME: a middleware for physical and logical mobility
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
LIME is a middleware supporting the development of applications that exhibit physical mobility of hosts, logical mobility of agents, or both. LIME adapts a coordination perspective inspired by work on the Linda model. The context for computation, represented in Linda by a globally accessible, persistent tuple space, is represented in LIME by transient sharing of the tuple spaces carried by each individual mobile unit. Linda tuple spaces are also extended with a notion of location and with the ability to react to a given state. The hypothesis underlying our work is that the resulting model provides a minimalist set of abstractions that enable rapid and dependable development of mobile applications. In this paper, we illustrate the model underlying LIME, present its current design and implementation, and discuss initial lessons learned in developing applications that involve physical mobility.
[client-server systems, physical mobility, hosts, Drives, Mobile communication, Data structures, Virtual machining, Application software, Middleware, Programming profession, agents, Computer science, logical mobility, coordination perspective, Investments, LIME, Linda tuple spaces, Fabrics, persistent tuple space, distributed object management, middleware]
A protocol design of communication state transfer for distributed computing
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
This paper presents the design of a communication state transfer protocol to support process migration in a dynamic, distributed computing environment. In our design, processes in distributed computation communicate one another via message passing and are migration-enabled. Due to mobility, mechanisms to maintain reliability and correctness of data communication are needed. Following an event-based approach, Such mechanisms are derived to handle various communication situations when a process migrates. These mechanisms collectively preserve the semantics of the communication and support efficient communication state transfer.
[Protocols, message passing, Snow, Maintenance, semantics, mobility mechanisms, Distributed computing, Sun, dynamic distributed computing environment, data communication reliability, communication state transfer, Computer science, process migration, Message passing, migration-enabled processes, event-based approach, Load management, Large-scale systems, Data communication, protocols, communication state transfer protocol design, distributed programming, data communication correctness]
The home model and competitive algorithms for load balancing in a computing cluster
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Most implementations of a computing cluster (CC) use greedy-based heuristics to perform load balancing. In some cases, this is in contrast to theoretical results about the performance of online load balancing algorithms. We define the home model in order to better reflect the architecture of a CC. In this new theoretical model, we assume a realistic cluster structure in which every job has a "home" machine which it prefers to be executed on, e.g. due to I/O considerations or because it was created there. We develop several online algorithms for load balancing in this model. We first provide a theoretical worst-case analysis, showing that our algorithms achieve better competitive ratios and perform less reassignments than algorithms for the unrelated machines model, which is the best existing theoretical model to describe such clusters. We then present an empirical average-case performance analysis by means of simulations. We show that the performance of our algorithms is consistently better than that of several existing load balancing methods, e.g. the greedy and the opportunity cost methods, especially in a dynamic and changing CC environment.
[workstation clusters, Costs, Computational modeling, computing cluster, simulation, competitive algorithms, worst-case analysis, home model, Home computing, Computer science, Bridges, resource allocation, greedy-based heuristics, distributed algorithms, algorithm performance, Clustering algorithms, Computer architecture, Load management, Computer networks, Communication networks, online load balancing algorithms]
Constructing adaptive software in distributed systems
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Adaptive software that can react to changes in the execution environment or user requirements by switching algorithms at run time is powerful yet difficult to implement, especially in distributed systems. This paper describes a software architecture for constructing such adaptive software and a graceful adaptive protocol that allows adaptations to be made in a coordinated manner across hosts transparently to the application. A realization of the architecture based on Cactus, a system for constructing highly configurable distributed services and protocols, is also presented. The approach is illustrated by outlining examples of adaptive components from a group communication service.
[Protocols, runtime reactive software, adaptive components, execution environment changes, Software performance, changed user requirements, Security, adaptive software construction, software architecture, Software architecture, configurable distributed services, Computer architecture, Cactus, distributed systems, graceful adaptive protocol, protocols, distributed programming, Buildings, group communication service, Application software, configurable protocols, Middleware, adaptive systems, Computer science, coordinated adaptations, Software systems, computer aided software engineering]
Dynamic database management for PCS networks
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
This paper presents a dynamic database management method for location management of personal communications service (PCS) networks. The proposed method provides dynamics copies of user location information in the nearest home location register (HLR) database, which allows mobile users to access the system efficiently.
[PCS networks, Costs, personal communication networks, location management, personal communications service, Registers, database management systems, telecommunication computing, Delay, mobile user access, Databases, mobile communication, dynamics copies, home location register database, Bandwidth, Telephony, Cities and towns, Computer networks, dynamic database management method, Personal communication networks, Computer network management, user location information]
On-line realignment of clients in networked databases
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Upper bounds on the scalability of client-server databases (CSDs) persist due to long delays experienced at the servers' queues. To this end, we have proposed a three-tier architecture that exploits similarities in object access behavior demonstrated by clients. Clients are statically grouped into logical clusters and object requests can be then served in an "internal to cluster" fashion. This is achieved with the introduction of an intermediate directory tier. Good client clustering yields more scalable CSD configurations as it minimizes the number of inter-cluster data accesses. We introduce the problem of client realignment in light of changing client localities and propose an online reclustering framework to address it. Online reclustering facilitates adaptive reconfiguration and redistribution of sites. The core of our proposal is a change detection approach that uses meta-data extracted from observed clients' access patterns. We evaluate the impact of employing a multi-featured change detection scheme in the three-tier CSD architecture and experimentally investigate its performance and trade-offs involved.
[Scalability, change detection approach, inter-cluster data access, object access behavior, meta-data, Proposals, Data mining, client-server databases, networked databases, Delay, scalability, Intelligent networks, Information science, Network servers, software architecture, three-tier architecture, Databases, online reclustering framework, experiment, Computer architecture, distributed databases, software performance evaluation, client-server systems, meta data, online client realignment, intermediate directory tier, Upper bound, client clustering, delays]
Shared state consistency for time-sensitive distributed applications
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Distributed applications that share a dynamically changing state are increasingly being deployed in wide-area environments. Such applications must access the state in a consistent manner, but the consistency requirements vary significantly from other systems. For example, shared memory models, such as sequential consistency, focus on the ordering of operations, and the same level of consistency is provided to each process. In interactive distributed applications, the timeliness of updates becoming effective could be an extremely important consistency requirement, and it could be different across different users. We propose a system that provides both non-timed and time-sensitive read and write operations for dynamic shared state. For example, a timed read can be used by a process to read a recently written value, whereas a timed write can make a new value available to all readers within a certain amount of time. We develop a consistency model that precisely defines the semantics of timed and non-tinted read and write operations. A protocol that implements this model is also presented. We also describe an implementation and some performance measurements.
[sequential consistency, Protocols, shared state consistency, distributed processing, shared memory models, update timeliness, recently written value, time-sensitive operations, write operations, semantics, caching, interactive distributed applications, consistent state access, protocols, operation ordering, Pervasive computing, read operations, Delay effects, nontimed operations, implementation, concurrency theory, Application software, dynamic shared state, Home computing, performance measurements, wide-area environments, concurrency control, dynamically changing state, time-sensitive distributed applications, consistency protocol]
Endpoint admission control: network based approach
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Proposes a network-based endpoint admission control system for scalable QoS-guaranteed real-time communication services. This system is based on a sink tree-based resource management strategy and is particularly well-suited for differentiated services-based architectures. By performing the admission decision at the endpoints, the flow setup latency and the signaling overhead are kept to a minimum. In addition, the proposed system integrates routing and resource reservation along the routes, and therefore displays higher admission probability and better link resource utilization. This approach achieves a low overall admission control overhead because much of the delay computation is done during system configuration, and so resources can effectively be pre-allocated before run time. We investigate a number of resource-sharing approaches that allow resources to be efficiently re-allocated at run time with minimized additional overhead. We provide simulation experiments that illustrate the benefits of using sink tree-based resource management for resource pre-allocation and for routing, both with and without resource sharing.
[Real time systems, sink tree-based resource management strategy, telecommunication congestion control, Scalability, simulation, signaling overhead, routing, link resource utilization, resource allocation, differentiated services-based architectures, Bandwidth, flow setup latency, resource reservation, minimized additional overhead, service quality, Delay effects, Computational modeling, trees (mathematics), delay computation, telecommunication signalling, system configuration, network-based endpoint admission control system, Routing, quality of service, scalable QoS-guaranteed real-time communication services, Computer science, resource pre-allocation, Admission control, real-time systems, telecommunication network routing, delays, resource sharing, Intserv networks, Resource management, admission probability]
Robust double auction protocol against false-name bids
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Internet auctions have become an integral part of electronic commerce (EC) and a promising field for applying agent technologies. Although the Internet provides an excellent infrastructure for large-scale auctions, we must consider the possibility of a new type of cheating, i.e., a bidder trying to profit from submitting several bids under fictitious names (false-name bids). Double auctions are an important subclass of auction protocols that permit multiple buyers and sellers to bid to exchange a good, and have been widely used in stock, bond, and foreign exchange markets. If there exists no false-name bid, a double auction protocol called PMD protocol has proven to be dominant-strategy incentive compatible. On the other hand, if we consider the possibility of false-name bids, the PMD protocol is no longer dominant-strategy incentive compatible. We develop a new double auction protocol called the Threshold Price Double auction (TPD) protocol, which is dominant strategy incentive compatible even if participants can submit false-name bids. The characteristics of the TPD protocol is that the number of trades and prices of exchange are controlled by the threshold price. Simulation results show that this protocol can achieve a social surplus that is very close to being Pareto efficient.
[Protocols, Laboratories, robust double auction protocol, simulation, Pareto efficient, Electronic commerce, Application software, cheating, Uniform resource locators, dominant-strategy incentive, agent technology, fraud, PMD protocol, false-name bids, Internet auctions, Robustness, Internet, Large-scale systems, Threshold Price Double auction protocol, protocols, Bonding, Business, electronic commerce]
Design and implementation of a composable reflective middleware framework
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
With the evolution of the global information infrastructure, service providers will need to provide effective and adaptive resource management mechanisms that can serve more concurrent clients and deal with applications that exhibit quality-of-service (QoS) requirements. Flexible, scalable and customizable middleware can be used as an enabling technology for next-generation systems that adhere to the QoS requirements of applications that execute in highly dynamic distributed environments. To enable application-aware resource management, we are developing a customizable and composable middleware framework called CompOSE|Q (Composable Open Software Environment with QoS), based on a reflective meta-model. In this paper, we describe the architecture and runtime environment for CompOSE|Q and briefly assess the performance overhead of the additional flexibility. We also illustrate how flexible communication mechanisms can be supported efficiently in the CompOSE|Q framework.
[enabling technology, Quality of service, Software safety, Distributed computing, Intelligent transportation systems, software architecture, customizable middleware framework, Composable Open Software Environment with QoS, CompOSE|Q, global information infrastructure, service quality requirements, runtime environment, software performance evaluation, application-aware resource management, Runtime environment, client-server systems, adaptive resource management mechanisms, information service provision, Access protocols, reflective meta-model, quality of service, Application software, Middleware, Computer science, performance overhead, composable reflective middleware framework, system architecture, flexible scalable customizable middleware, concurrent clients, next-generation systems, Resource management, dynamic distributed environments, flexible communication mechanisms]
Placement of read-write Web proxies on the Internet
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
This paper investigates the optimal placement of proxies of a Web server on the Internet. With the consideration of both read and write operations to the data on the Web server. First, we study the problem of optimal placement of k proxies in a system to minimize the total access cost to the Web server. Then, for unknown number of proxies, we find the optimal number of proxies required in the system. The problems are formulated using a dynamic programming method and optimal solutions are obtained. Intensive simulations have been conducted to evaluate the performance of the proposed algorithms, and to demonstrate the relationship between the number of proxies required in the system and the read-write ratio. This work can significantly alleviate the Web access traffic on the Internet and improve the performance of the Web server.
[information resources, simulation, Telecommunication traffic, dynamic programming, performance evaluation, read-write ratio, Delay, Web access traffic, Computer science, Network servers, optimal read-write Web proxy placement, total access cost minimization, file servers, virtual machines, Traffic control, Cost function, Frequency, Internet, Dynamic programming, Web server, software performance evaluation, telecommunication traffic, dynamic programming method]
A real-time system for tele-surgery
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Describes a real-time system supporting a tele-surgery application based on the Zeus/sup TM/ system. The application involves performing minimally invasive surgical procedures remotely - i.e. without the surgeon being present in the same room with the patient. Because there is human life at stake, the underlying real-time system must be robust, fail-safe and resilient to communications problems. This paper focuses primarily on the approach taken in communicating between the surgeon-side and patient-side subsystems. In particular, techniques for dealing with the challenges of real-time communications are discussed, e.g. bit errors, packet errors and synchronization. In addition to the control actions and feedback data, several serial data streams are multiplexed, transmitted between one side and the other, demultiplexed, and delivered by the real-time system. These streams have a fundamentally different character from the control actions and feedback values used with the robots. Both types of real-time streams are sent over the same communications link.
[Real time systems, communications link, robust fail-safe resilient system, telerobotics, Humans, Communication system control, medical robotics, tele-surgery, surgeon-side subsystem, bit errors, serial data streams, Delay, Arteries, communications problems, Microsurgery, patient-side subsystem, demultiplexing, real-time system, Robots, telemedicine, packet errors, Application software, Surges, feedback data, Minimally invasive surgery, Zeus system, multiplexing, real-time systems, data transmission, feedback values, data communication, synchronization, remote minimally invasive surgical procedures, control actions, surgery]
A dynamic heuristic broadcasting protocol for video-on-demand
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Most existing distribution protocols for video-on-demand are tailored for a specific range of video access rates and perform poorly beyond that range. We present a dynamic heuristic broadcasting protocol that performs as well as stream tapping with unlimited extra tapping at low video access rates and has the same average bandwidth requirements as the best existing broadcasting protocols at high video access rates. We also show how our protocol can handle compressed video and adapt itself to the individual bandwidth requirements of each video.
[Video sharing, Access protocols, Multicast protocols, Multimedia communication, Proposals, compressed video, broadcasting, dynamic heuristic broadcasting protocol, Computer science, unlimited extra tapping, bandwidth requirements, performance, transport protocols, video on demand, Bandwidth, video access rates, Broadcasting, Streaming media, Video compression, video-on-demand, adaptive protocol, stream tapping, distribution protocols]
Token based group mutual exclusion for asynchronous rings
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
We propose a group mutual exclusion algorithm for unidirectional rings. Our algorithm does not require the processes to have any id. Moreover, processes maintain no special data structures to implement any queues. The space requirement of processes depends only on the number of shared resources, and is equal to 4/spl times/log(m+1)+2 bits. The size of messages is 2/spl times/log(m+1) bits only. Every resource request generates O(n/sup 2/) messages in the worst case, but zero messages in the best case.
[message size, Access protocols, Quality of service, resource request, Data structures, concurrency theory, Synchronization, Concurrent computing, Computer science, Message passing, token based group mutual exclusion, Web and internet services, algorithm theory, unidirectional rings, asynchronous rings, Web server, space requirement, Clocks]
Availability study of dynamic voting algorithms
Proceedings 21st International Conference on Distributed Computing Systems
None
2001
Fault-tolerant distributed systems often select a primary component to allow a subset of the processes to function when failures occur. The dynamic voting paradigm defines rules for selecting the primary component adaptively: when a partition occurs, if a majority of the previous primary component is connected, a new and possibly smaller primary component is chosen. Several studies have shown that dynamic voting leads to more available solutions than other paradigms for maintaining a primary component. However, these studies have assumed that every attempt made by the algorithm to form a new primary component terminates successfully. Unfortunately, in real systems, this is not always the case: a change in connectivity can interrupt the algorithm while it is still attempting to form a new primary component; in such cases, algorithms may block until the processes can resolve the outcome of the interrupted attempt. This paper uses simulations to evaluate the effect of interruptions on the availability of dynamic voting algorithm. We study four dynamic voting algorithms and identify two important characteristics that impact an algorithm's availability in runs with frequent connectivity changes. First, we show that the number of processes that need to be present in order to resolve past attempts impacts the availability, especially during long runs with numerous connectivity changes. Second, we show that the number of communication rounds exchanged in an algorithm plays a significant role in the availability achieved, especially in the degradation of availability as connectivity changes become more frequent.
[Heuristic algorithms, Oils, Aerodynamics, Partitioning algorithms, availability, algorithm interruption, software fault tolerance, simulations, Computer science, Degradation, Telegraphy, partition, communication rounds, Voting, Fault tolerant systems, distributed algorithms, dynamic voting algorithms, primary component selection, algorithm termination, fault-tolerant distributed systems, connectivity changes, Contracts, process failures]
Fragmentation based D-MAC protocol in wireless ad hoc networks
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Directional antennas have been recently suggested to be used in wireless ad hoc networks to reduce interference outside the antenna direction and to increase spatial reuse of wireless channels. Several MAC schemes that exploit directional antennas have been proposed, among which the two D-MAC schemes proposed in [5] may have received the most attention. In this paper, we carefully analyze how the second D-MAC scheme proposed in [5] operates in the hidden/exposed terminal scenarios and show that it not only introduces new collisions, but also reduces spatial reuse to some extent. To remedy these problems, we propose an enhanced version of the second D-MAC scheme, called fragmentation based D-MAC (FD-MAC), to reduce collision, to achieve better spatial reuse (than the second D-MAC scheme), and to improve fairness. Through ns-2 simulation, we demonstrate the effectiveness of FD-MAC, and in particular, we show that in complex scenarios such as multihop forwarding and mesh topologies, FD-MAC significantly outperforms DCF with RTS/CTS and the two D-MAC schemes, both in terms of throughput and fairness.
[telecommunication congestion control, Wireless application protocol, fragmentation based D-MAC protocol, Ad hoc networks, multihop forwarding, access protocols, media access control, mesh topology, Distributed computing, Intelligent networks, directive antennas, wireless ad hoc network, interference reduction, directional antenna, IEEE standards, ad hoc networks, wireless LAN]
A generic distributed broadcast scheme in ad hoc wireless networks
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
We propose a generic framework for distributed broadcasting in ad hoc wireless networks. The approach is based on selecting a small subset of hosts (also called nodes) to form a forward node set to carry out a broadcast process. The status of each node, forwarding or non-forwarding, is determined either by itself (self-pruning) or by other nodes (neighbor-designating). Node status can be determined at different snapshots of network state along time (called views) without causing problems in broadcast coverage. A sufficient condition, called coverage condition, is given for a node to take the non-forward status. Such a condition can be easily checked locally around the node. Several existing broadcast algorithms can be viewed as special cases of the generic framework with k-hop neighborhood information. A comprehensive comparison among existing algorithms is conducted. Simulation results show that new algorithms, which are more efficient than existing ones, can be derived from the generic framework. This work is an extension to an early work in which only self-pruning methods are discussed [16].
[distributed broadcasting, self-pruning, Routing, Ad hoc networks, Floods, Computer science, Wireless communication, ad hoc wireless network, Intelligent networks, Sufficient conditions, distributed algorithm, Network topology, Wireless networks, distributed algorithms, telecommunication network routing, Broadcasting, ad hoc networks, wireless LAN]
Enhancing the fault-tolerance of nonmasking programs
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
In this paper we focus on automated techniques to enhance the fault-tolerance of a nonmasking fault-tolerant program to masking. A masking program continually satisfies its specification even if faults occur. By contrast, a nonmasking program merely guarantees that after faults stop occurring, the program recovers to states from where it continually satisfies its specification. Until the recovery is complete, however a nonmasking program can violate its (safety) specification. Thus, the problem of enhancing fault-tolerance from nonmasking to masking requires that safety be added and recovery be preserved. We focus on this enhancement problem for high atomicity programs-where each process can read all variables-and for distributed programs-where restrictions are imposed on what processes can read and write. We present a sound and complete algorithm for high atomicity programs and a sound algorithm for distributed programs. We also argue that our algorithms are simpler than previous algorithms, where masking fault-tolerance is added to a fault-intolerant program. Hence, these algorithms can partially reap the benefits of automation when the cost of adding masking fault-tolerance to a fault-intolerant program is high. To illustrate these algorithms, we show how the masking fault-tolerant programs for triple modular redundancy and Byzantine agreement can be obtained by enhancing the fault-tolerance of the corresponding nonmasking versions. We also discuss how the derivation of these programs is simplified when we begin with a nonmasking fault-tolerant program.
[Automation, Costs, Engineering profession, nonmasking program, Byzantine agreement, fault-tolerance, triple modular redundancy, Laboratories, Redundancy, program synthesis, formal specification, Computer science, Fault tolerance, distributed algorithm, Fault tolerant systems, distributed algorithms, fault tolerant computing, Safety, distributed programming, atomicity program, Software engineering, computational complexity]
A generic framework for indulgent consensus
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Consensus is a fundamental distributed agreement problem that has to be solved when one has to design or implement reliable applications. As consensus cannot be solved in pure asynchronous distributed systems, those systems have to be equipped with appropriate oracles to circumvent the impossibility. Several oracles (unreliable failure detector leader capability, random number generator) have been proposed, and consensus protocols based on such ad hoc oracles have been designed This paper presents a generic consensus framework that can be instantiated with any oracle, or combination of oracles, that satisfies a set of properties. This generic framework provides indulgent consensus protocols that are particularly simple and efficient both in well-behaved runs (i.e., when there are no failures), and in stable runs (i.e., when there is no failure during the execution although some processes can be initially crashed). In those runs, the protocols terminate in two communication steps (which is optimal). Indulgence means that the resulting protocol never violates its safety property even when the underlying oracle behaves arbitrarily. Interestingly, the protocol can also allow processes to decide in one communication step in some specific configurations.
[Context, Heart, Protocols, oracle, asynchronous distributed system, Computer crashes, Distributed computing, system recovery, indulgent consensus protocol, Computer science, distributed algorithms, Detectors, System recovery, fault tolerant computing, Safety, Random number generation]
The hash history approach for reconciling mutual inconsistency
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
We introduce the hash history mechanism for capturing dependencies among distributed replicas. Hash histories, consisting of a directed graph of version hashes, are independent of the number of active nodes but dependent on the rate and number of modifications. We present the basic hash history scheme and discuss mechanisms for trimming the history over time. We simulate the efficacy of hash histories on several large CVS traces. Our results highlight a useful property of the hash history: the ability to recognize when two different non-commutative operations produce the same output, thereby reducing false conflicts and increasing the rate of convergence. We call these events coincidental equalities and demonstrate that their recognition can greatly reduce the time to global convergence.
[Availability, replicated databases, Peer to peer computing, Merging, cryptography, History, Synchronization, Distributed computing, hash history approach, Convergence, version hash, coincidental equality, distributed replicas, directed graphs, Intrusion detection, directed graph, Aging, reconciling mutual inconsistency, Clocks]
Elastic vector time
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
In recent years there has been an increasing demand to build "soft" real-time applications on top of asynchronous distributed systems. Designing and implementing such applications is a non-trivial task and application designers are often faced with the need to circumvent impossibility results. In this paper we discuss how to ensure that actions are executed in the correct order even in the face of failures. We propose a novel time base and a new synchronization mechanism for the design of distributed "soft" real-time applications. We demonstrate (1) how this time base can be used to enforce an externally consistent ordering, and (2) how it permits to circumvent impossibility results by sketching how to solve the leader election and perfect failure detection problem.
[Real time systems, Delay effects, Nominations and elections, Communication system control, asynchronous distributed system, distributed processing, Control systems, failure detection problem, time base, Synchronization, failure analysis, synchronisation, real-time application, Upper bound, synchronization mechanism, Distributed control, Safety, elastic vector time, Clocks]
Pragmatic type interoperability
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Providing type interoperability consists in ensuring that, even if written by different programmers, possibly in different languages and running on different platforms, types that are supposed to represent the same software module are indeed treated as one single type. This form of interoperability is crucial in modern distributed programming. We present a pragmatic approach to deal with type interoperability in a dynamic and distributed environment. Our approach is based on an optimistic transport protocol, specific serialization mechanisms and a set of implicit type conformance rules. We experiment the approach over the .NET platform which we indirectly evaluate.
[Transport protocols, Java, object-oriented programming, open systems, Laboratories, pragmatic type interoperability, Application software, Simple object access protocol, implicit type conformance rule, Programming profession, Computer languages, transport protocol, software module, Operating systems, Linux, transport protocols, network operating systems, Hardware, network operating system, distributed programming]
SPEED: a stateless protocol for real-time communication in sensor networks
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
In this paper, we present a real-time communication protocol for sensor networks, called SPEED. The protocol provides three types of real-time communication services, namely, real-time unicast, real-time area-multicast and real-time area-anycast. SPEED is specifically tailored to be a stateless, localized algorithm with minimal control overhead End-to-end soft real-time communication is achieved by maintaining a desired delivery speed across the sensor network through a novel combination of feedback control and non-deterministic geographic forwarding. SPEED is a highly efficient and scalable protocol for sensor networks where the resources of each node are scarce. Theoretical analysis, simulation experiments and a real implementation on Berkeley motes are provided to validate our claims.
[Base stations, wireless sensor networks, real-time unicast, real-time area-multicast, Communication system control, feedback control, sensor networks, real-time area-anycast, Feedback control, quality of service, stateless protocol, real-time communication, Intelligent sensors, Computer science, SPEED, Intelligent networks, Analytical models, Unicast, Berkeley motes, routing protocols, multicast communication, nondeterministic geographic forwarding, Routing protocols, Large-scale systems]
Remote belief: preserving volition for loosely coupled processes
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Knowledge has proven to be a useful and fundamental formalism for reasoning about distributed systems. The application of this formalism, however entails a loss of volition on the part of processes about which something is known. This loss of volition is often not appropriate in loosely coupled distributed systems. In this paper we generalize the formal characterization of knowledge into one of belief. Belief has the advantage of allowing processes to maintain volition. We examine some of the similarities and surprising differences between knowledge and belief. We also present some examples of distributed applications that are more conveniently characterized with belief rather than knowledge.
[message passing, distributed system reasoning, loosely coupled process, Educational institutions, Mathematics, belief maintenance, Application software, Distributed computing, Game theory, Environmental economics, Computer science, formal logic, Information science, knowledge representation, Artificial intelligence, Context modeling]
Updates in highly unreliable, replicated peer-to-peer systems
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
This paper studies the problem of updates in decentralised and self-organising P2P systems in which peers have low online probabilities and only local knowledge. The update strategy we propose for this environment is based on a hybrid push/pull rumor spreading algorithm and provides a fully decentralised, efficient and robust communication scheme which offers probabilistic guarantees rather than ensuring strict consistency. We describe a generic analytical model to investigate the utility of our hybrid update propagation scheme from the perspective of communication overhead.
[replicated databases, Peer to peer computing, online probability, Project management, Calendars, distributed processing, Floods, communication complexity, replicated peer-to-peer systems, communication overhead, Delay, update propagation scheme, Analytical models, Multicast algorithms, hybrid push/pull rumor spreading algorithm, Robustness, Internet, Books, Business]
Synchronous Consensus for dependent process failures
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
We present a new abstraction to replace the t of n assumption used in designing fault-tolerant algorithms. This abstraction models dependent process failures yet it is as simple to use as the t of n assumption. To illustrate this abstraction, we consider Consensus for synchronous systems with both crash and arbitrary process failures. By considering failure correlations, we are able to reduce latency and enable the solution of Consensus for system configurations in which it is not possible when forced to use algorithms designed under the t of n assumption. We show that, in general, the number of rounds required in the worst case when assuming crash failures is different from the number of rounds required when assuming arbitrary failures. This is in contrast with the traditional result under the t of n assumption.
[Algorithm design and analysis, process failure, synchronous system, Drives, Computer crashes, failure correlation, system recovery, synchronous consensus, Delay, synchronisation, Computer science, Fault tolerance, Design engineering, fault-tolerant algorithm, system crash, distributed algorithm, Fault tolerant systems, distributed algorithms, Failure analysis, fault tolerant computing, Distributed algorithms]
Analysis of task assignment with cycle stealing under central queue
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
We consider the problem of task assignment in a distributed server system, where short jobs are separated from long jobs, but short jobs may be run in the long job partition if it is idle (cycle stealing). Jobs are assumed to be nonpreemptible, where short and long jobs have generally distributed service requirements, and arrivals are Poisson. We consider two variants of this problem: a central queue model and an immediate dispatch model. This paper presents the first analysis of cycle stealing under the central-queue model. (Cycle stealing under the immediate dispatch model is analyzed in [9]). The analysis uses a technique which we refer to as busy period transitions. Results show that cycle stealing can reduce mean response time for short jobs by orders of magnitude, while long jobs are only slightly penalized. Furthermore using a central queue yields significant performance improvement over immediate dispatch, both from the perspective of the benefit to short jobs and the penalty to long jobs.
[client-server systems, queueing theory, Engineering profession, Exponential distribution, cycle stealing, distributed server system, Telecommunication computing, task analysis, Distributed computing, Power system modeling, Delay, processor scheduling, busy period transition, Computer science, resource allocation, Stochastic systems, Markov processes, long job, Performance analysis, task assignment, central queue model, Queueing analysis, short job, dispatch model]
Integrated access control and intrusion detection for Web Servers
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Current intrusion detection systems work in isolation front access control for the application the systems aim to protect. The lack of coordination and inter-operation between these components prevents detecting and responding to ongoing attacks in real time, before they cause damage. To address this, we apply dynamic authorization techniques to support fine-grained access control and application level intrusion detection and response capabilities. This paper describes our experience with integration of the Generic Authorization and Access Control API (GAA-API) to provide dynamic intrusion detection and response for the Apache Web Server The GAA-API is a generic interface which may be used to enable such dynamic authorization and intrusion response capabilities for many applications.
[Access control, application program interfaces, Government, integrated access control, intrusion detection, Computer crime, Authorization, authorization, Network servers, Apache Web server, Operating systems, application program interface, Intrusion detection, authorisation, API, Internet, Web server, Protection, Clocks]
Scalable resource allocation for multi-processor QoS optimization
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
We present scalable QoS optimization algorithms for allocating resources to tasks in a multi-processor environment. Given a set of tasks, each of which is capable of running at one of several different QoS levels, the algorithms can select a QoS operating point, the number of replicas for fault-tolerance and the processors on which to run the replicas so as to maximize overall system QoS. The algorithms are extensions of Q-RAM (QoS-based Resource Allocation Model) [5] and fix two deficiencies with the basic algorithm. The first is that the existing algorithm is weak in making resource trade-off decisions such as to which processor to map a task. The second was that it was not scalable to very large numbers of resources such as in a large multi-processor system. In this paper we present two new algorithms which significantly enhance the ability of Q-RAM to make resource tradeoff decisions. We also introduce a hierarchical decomposition scheme which enables QoS optimization to be performed on problems with thousands of resources and thousands of tasks.
[multiprocessing systems, quality of service, Distributed computing, processor scheduling, distributed algorithm, optimisation, resource allocation, QoS optimization, distributed algorithms, multiprocessing system, real-time systems, Resource management, real-time system]
Protecting BGP routes to top level DNS servers
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
The Domain Name System (DNS) is an essential part of the Internet infrastructure and provides fundamental services, such as translating host names into IP addresses for Internet communication. The DNS is vulnerable to a number of potential faults and attacks. In particular, false routing announcements can deny access to the DNS service or redirect DNS queries to a malicious impostor Due to the hierarchical DNS design, a single fault or attack against the routes to any of the top level DNS servers can disrupt Internet services to millions of users. In this paper we propose a path-filtering approach to protect the routes to the critical top level DNS servers. Our approach exploits the high degree of redundancy in top level DNS servers and also exploits the observation that popular destinations, including top level DNS servers, are well connected via stable routes. Our path-filter restricts the potential top level DNS server route changes to be within a set of established paths. Heuristics derived from routing operations are used to adjust the potential routes overtime. We tested our path-filtering design against BGP routing logs and the results show that the design can effectively ensure correct routes to top level DNS servers without impacting DNS service availability.
[IEEE news, network routing, fault-tolerance, Redundancy, Routing, IP address, Domain Name System, Network servers, BGP path filtering, security of data, Current measurement, Web and internet services, DNS infrastructure protection, domain name system, fault tolerant computing, route hijacking, Internet, IP networks, Protection, Web server, Testing]
NRMI: natural and efficient middleware
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
We present NRMI: a drop-in replacement of Java RMI that offers call-by-copy-restore semantics for arbitrary linked data structures, in addition to regular call-by-copy semantics. Call-by-copy-restore middleware is more natural to use than traditional call-by-copy RPC mechanisms, enabling distributed calls to behave much like local calls. We discuss in depth the effect of calling semantics for middleware, describe how call-by-copy-restore can be implemented efficiently, and show examples of Java programs where NRMI is more convenient than regular Java RMI.
[Tree data structures, Java, distributed call, NRMI, Java RMI, Data structures, Educational institutions, Middleware, Distributed computing, Tree graphs, call-by-copy RPC mechanism, call-by-copy semantics, remote procedure calls, call-by-copy-restore semantics, Velocity measurement, middleware]
Compiler scheduling of mobile agents for minimizing overheads
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Mobile code carried by a mobile agent can automatically travel to several data sources in order to complete a designated program. Traditionally, most mobile agent systems [7][8][13] need explicit involvement of the programmer to designate migration and computation schedule of the agent. In this paper, we study the compiler-supported agent scheduling to optimize either the number of the migrations or the amount of data transfer. Two approaches are proposed and evaluated in our experiments, i.e. the static and dynamic scheduling algorithms. The first algorithm works totally offline. After converting the program control flow, graph (CFG) to program dependency graph (PDG), the schedule is worked out. On the other hand, in order to dynamically schedule the agent when it reaches predicate (control flow) nodes, our dynamic scheduling algorithm generates the motion schedule incrementally. Finally, our results show good improvement over unoptimized agent code both in terms of data transfer sizes and number of agent migrations.
[data sources, optimising compilers, dynamic scheduling algorithm, Heuristic algorithms, flow graphs, Dynamic scheduling, Educational institutions, static scheduling algorithm, Distributed computing, program dependency graph, Scheduling algorithm, overhead minimization, Program processors, Processor scheduling, control flow graph, Mobile agents, Distributed databases, mobile agents, scheduling, data transfer, mobile code, compiler scheduling, Mobile computing, distributed programming]
On energy efficiency and network connectivity of mobile ad hoc networks
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
In mobile ad hoc networks, it is often more important to optimize for energy efficiency than throughput. In this paper we investigate the effect of transmit range on energy efficiency of packet transmissions. We determine a common range for all nodes such that the average energy expenditure per received packet is minimized. In the first part of this paper, we consider stationary networks. We show that energy efficiency depends on various system parameters that includes path loss exponent of the channel, energy dissipation model and network offered load. In particular, when the path loss exponent is large, energy efficiency decreases when the transmit range increases. Hence, the network should be operated at the critical range that just maintains network connectivity. However, when the path loss exponent is small, operating at the critical range yields inferior throughput and energy efficiency. Our results show that energy efficiency is intimately connected to network connectivity. Three network connectivity regimes are identified as the transmit range of all nodes increases. In the second part, we examine the effect of node mobility on energy efficiency. We show that at normal offered load, an optimal transmit range exists such that energy efficiency is maximized The optimal range turns out to be insensitive to node mobility, and is much larger than the critical range. We show that the energy expenditure can be reduced by 15% to 73% in different mobility scenarios, if the network is operated at the optimal range.
[system parameters, Throughput, Ad hoc networks, Information technology, packet transmissions, Mobile ad hoc networks, node mobility, mobile computing, optimisation, stationary networks, Power control, Energy dissipation, mobile ad hoc networks, network connectivity, Bidirectional control, energy efficiency, energy conservation, Energy efficiency, Computer networks, ad hoc networks, Power engineering and energy, energy dissipation model]
Smart dust-hardware limits to wireless sensor networks
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
With the MEMS revolution in full swing, microsensors are now following manufacturing curves that are at least related to Moore's Law. Combined with the push for low power communication and computation coming from the handheld community, this enables an exciting new field of low-cost, high-performance wireless sensor networks. This talk will address some of the practical and physical limits associated with miniaturization of wireless sensor networks.
[wireless sensor networks, microsensors, Micromechanical devices, Moore Law, Wireless sensor networks, Handheld computers, Biographies, Moore's Law, smart dust, Hardware, Computer networks, Manufacturing, Optical sensors, power communication, Microsensors]
Trading replication consistency for performance and availability: an adaptive approach
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Replication system is one of the most fundamental building blocks of wide-area applications. Due to the inevitable dependencies on wide-area communication, trade-off between performance, availability and replication consistency is often a necessity. While a number of proposals have been made to provide a tunable consistency bound between strong and weak extremes, many of them rely on a statically specified enforcement across replicas. This approach, while easy to implement, neglects the dynamic contexts within which replicas are operating, delivering sub-optimal performance and/or system availability. In this paper we analyze the problem of optimal performance/availability for a given consistency level under heterogeneous workload and network condition. We prove several optimization rules for different goals. Based on these results, we developed an adaptive update window protocol in which consistency enforcement across replicas is self-tuned to achieve optimal performance/availability. A prototype system, FRACS, is built and evaluated in this paper. The experiment results demonstrate significant advantages of adaptation over static approach for a variety of workloads.
[Availability, network condition, Protocols, replication system, replicated databases, wide area networks, heterogeneous workload, Adaptation model, window protocol, data integrity, Application software, Proposals, Electronic commerce, Delay, suboptimal performance, optimisation, prototype system, distributed algorithms, consistency level, Prototypes, Collaborative work, system availability, Performance analysis, wide-area application]
Monitoring and characterization of component-based systems with global causality capture
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Current software development techniques and tools lack the capability to characterize function call chains in multithreaded and distributed applications built upon component technologies like CORBA, COM and J2EE. The root cause is that causal linkage information necessary to trace end-to-end call chains is private to each vendor's runtime and often unavailable for logging or analysis. We propose and demonstrate a mechanism for maintaining and correlating global causality information of component-based applications, and using this information to expose and characterize function call chains and their associated behaviors in such multithreaded and distributed applications. Our approach relies on a global virtual tunnel facilitated by the instrumented stubs and skeletons. This tunnel maintains and correlates causal information throughout the end-to-end call chains spanning threads, processes and processors. As a result, monitoring data captured locally can be correlated and system-wide propagation of timing latency and CPU utilization becomes perceivable.
[object-oriented programming, multi-threading, Instruments, Programming, global casuality capture, Application software, Yarn, virtual tunnel, Information analysis, function call chain, Couplings, Runtime, system monitoring, remote procedure calls, Skeleton, component-based system, multithreading, Timing, Monitoring, distributed object management]
Mutual anonymity protocols for hybrid peer-to-peer systems
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
In a hybrid peer-to-peer (P2P) system, some operations are intentionally centralized, such as indexing of peers' files. We present several protocols to achieve mutual communication anonymity between an information requester and a provider in a hybrid P2P information-sharing environment with trusted index servers such that neither the requester, nor the provider can identify each other and no other peers can identify the two communicating parties with certainty. Some existing protocols provide solutions to achieve mutual anonymity in pure P2P systems without any trusted central controls. Compared with two representative protocols, our proposed mutual anonymity protocols improve efficiency by utilizing trusted third parties and aiming at both reliability and low-cost. We show that with some limited central support, our protocols can accomplish the goals of anonymity, efficiency, and reliability. We have evaluated our techniques in a browser-sharing environment. We show that the average increase in response time caused by our protocols is trivial, and these protocols show advantages over existing protocols in a hybrid P2P system.
[information-sharing, Protocols, Peer to peer computing, Laboratories, browser-sharing environment, distributed processing, Educational institutions, Control systems, information requester, information provider, Delay, Centralized control, Computer science, peer files, hybrid peer-to-peer systems, database indexing, Resists, mutual anonymity protocols, protocols, index servers, Indexing]
SmartNode: achieving 802.11 MAC interoperability in power-efficient ad hoc networks with dynamic range adjustments
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
The standard CSMA/CA based IEEE 802.11 protocol assumes that each node uses a certain fixed (or maximum) transmission power for the transmission of each packet. However, a MAC protocol with power adjustments can have significant benefits towards better power conservation and higher system throughput through better spatial reuse of spectrum. In this work, we propose a power efficient MAC layer algorithm, SmartNode, that is compatible with the basic RTS-CTS-DATA-ACK MAC protocol defined in IEEE 802.11. Our algorithm uses the minimum required power level for the transmission of data packets, which requires special handling for the exchange of control packets. Compared with previously proposed power-controlled MAC protocols, our algorithm does not require multiple data channels at the physical layer so that it is able to inter-operate with regular nodes running the existing IEEE 802.11 MAC protocol. Through extensive performance evaluations, we have demonstrated that our proposed algorithm is effective in a power-controlled ad hoc network - it is able to increase system throughput while conserving power with dynamic power adjustments.
[spatial reuse, control packets, open systems, multiple data channels, Dynamic range, Throughput, access protocols, Multiaccess communication, Mobile ad hoc networks, Intelligent networks, dynamic range adjustments, Spread spectrum communication, Power system relaying, performance evaluation, Physical layer, Ad hoc networks, interoperability, standard CSMA, IEEE 802.11 protocol, smartnode, power conservation, Media Access Protocol, power-efficient ad hoc networks, data packets, ad hoc networks, wireless LAN, MAC protocols]
Evaluating distributed checkpointing protocols
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
This paper presents an objective measure, called overhead ratio, for evaluating distributed checkpointing protocols. This measure extends previous evaluation schemes by incorporating several additional parameters that are inherent in distributed environments. In particular, we take into account the rollback propagation of the protocol, which impacts the length of the recovery process, and therefore the expected program run-time in executions that involve failures and recoveries. The paper also analyzes several known protocols and compares their overhead ratio.
[Checkpointing, Protocols, Costs, Debugging, distributed processing, distributed environment, Application software, Distributed computing, system recovery, Computer science, Coordinate measuring machines, Runtime, Fault tolerant systems, protocols, overhead ratio, distributed checkpointing protocol evaluation]
Dynamic module replacement in distributed protocols
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Dynamic module replacement - the ability to hot swap a component's implementation at runtime - is fundamental to supporting evolutionary change in long-lived and highly-available systems. Most existing solutions require special-purpose middleware or depend on research languages with limited support for mainstream software development. We present a language-neutral technique for dynamic module replacement using Service Facilities (Serfs) - a pattern-based design strategy for decoupling runtime dependencies. We demonstrate the sufficiency of Serfs with respect to a litmus test of criteria for module replacement. Next, we extend the traditional scope of module replacement to encompass the domain of modules for distributed protocols. We conclude by applying the Serf strategy to illustrate dynamic replacement of mutual exclusion protocols in modules for distributed resource allocation.
[Protocols, distributed protocols, Programming, Production facilities, Information science, distributed resource allocation, Runtime, resource allocation, software engineering, mutual exclusion protocols, protocols, Testing, middleware, Java, object-oriented programming, software development, language-neutral technique, Serf strategy, Banking, research languages, Middleware, dynamic module replacement, pattern-based design strategy, Resource management, service facilities]
Decision-support workload characteristics on a clustered database server from the OS perspective
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
A range of database services are being offered on clusters of workstations today to meet the demanding needs of applications with voluminous datasets, high computational and I/O requirements and a large number of users. The underlying database engine runs on cost-effective off-the-shelf hardware and software components that may not really be tailored/tuned for these applications. At the same time, many of these databases have legacy codes that may not be easy to modulate based on the evolving capabilities and limitations of clusters. An indepth understanding of the interaction between these database engines and the underlying operating system (OS) can identify a set of characteristics that would be extremely valuable for future research on systems support for these environments. To our knowledge, there is no prior work that has embarked on such a characterization for a clustered database server. Using IBM DB2 Universal Database (UDB) Extended Enterprise Edition (EEE) V7.2 Trial version and TPC-H like/sup 1/ decision support queries, this paper studies numerous issues by evaluating performance on an off-the-shelf Pentium/Linux cluster connected by Myrinet. These include detailed performance profiles of all kernel activities, as well as qualitative and quantitative insights on the interaction between the database engine and the operating system.
[workstation clusters, operating system kernels, Linux cluster, OS, operating system, Application software, database management systems, clustered database server, decision support systems, workstation cluster, Computer science, OS kernel activity, Image databases, Operating systems, Linux, decision-support workload characteristics, network operating systems, Search engines, Hardware, Workstations, Kernel, Modulation coding, IBM DB2 universal database]
Performance optimizations for group key management schemes
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Recently, many group key management approaches based on the use of logical key trees have been proposed to address the issue of scalable group rekeying that is needed to support secure communications for large and dynamic groups. In this paper, we present two optimizations for logical key tree organizations that utilize information about the characteristics of group members to further reduce the overhead of group rekeying. First, we propose a partitioned key tree organization that exploits the temporal patterns of group member joins and departures to reduce the overhead of rekeying. Using an analytic model, we show that our optimization can achieve up to 31.4% reduction in key server bandwidth overhead over the unoptimized scheme. Second, we propose an approach under which the key tree is organized based on the loss probabilities of group members. Our analysis shows this optimization can reduce the rekeying overhead by up to 12.1%.
[telecommunication security, group key management, Data security, logical key tree, Multicast protocols, Computer science, Teleconferencing, Multicast algorithms, optimisation, public key cryptography, Web and internet services, optimization, Management information systems, Bandwidth, multicast communication, Cryptography, IP networks]
The conundrum of distributed computing
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
We have been engineering distributed computing architectures according to roughly the same formulas for quite a number of years; yet each time, the end result has had greater complexity. Modern distributed computing architectures have an enormous number of protocols, layers, configuration parameters, APIs, etc. While we seem to be proceeding in the right direction in functionality, the growth in complexity is perplexing and has the potential to slow progress in research and in commercial use. I'll discuss this topic and propose areas of research that are relevant to resolving the issue.
[Protocols, application program interfaces, distributed processing, Marketing management, Distributed computing, Middleware, Computer science, Biographies, File systems, Computer architecture, distributed computing architecture, API, System buses, protocols, configuration parameter, Business]
Effective delay control for high rate heterogeneous real-time flows
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
This paper presents a new method to control the delay performance for high rate heterogeneous real-time traffic flows based on a novel traffic control algorithm which is a generalization of traditional (/spl sigma/, /spl rho/) regulator. Our new control algorithm operates like the traditional regulator under the normal loading situation, but provides more regulation for the high rate (heavy load condition) of the traffic. For a set of heterogenous real-time traffic flows R we can show that D/sub r/(R) /spl les/ D(R) where D/sub r/(R) and D(R) are the worst-case delay bounds with our new control algorithm and that with (/spl sigma/, /spl rho/) regulator respectively. More specifically, we develop a set of formula that can be used to set the parameters in our new traffic controller so that the worst case delay bound is minimized by streaming the traffic flow. We can prove that there exists a minimum (average) input rate p* such that D/sub r/(R) = D(R) for /spl rho/ /spl les/ /spl rho/* and D/sub r/(R) < D(R) for /spl rho/ > /spl rho/*. Using the extended regulator can effectively control the delay when the average heterogeneous traffic rate is high. The issues are particularly useful for Integrated Services where a flow may over claim its share of resource and for Differentiated Services where a class of traffic flows may possess very high rates.
[Regulators, traffic control algorithm, telecommunication congestion control, Delay effects, heterogeneous real-time traffic flow, distributed processing, delay control, Mathematics, quality of service, Information technology, High performance computing, Councils, real-time systems, delays, Bandwidth, Traffic control, controllers, regulator, Communication system traffic control, Intserv networks, telecommunication traffic]
Responsive security for stored data
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
We present the design of a distributed store that offers various levels of security guarantees while tolerating a limited number of nodes that are compromised by an adversary. The store uses secret sharing schemes to offer security guarantees namely availability, confidentiality and integrity. However, a pure secret sharing scheme could suffer from performance problems and high access costs. We integrate secret sharing with replication for better performance and to keep access costs low. The tradeoffs involved between availability and access cost on one hand and confidentiality and integrity on the other are analyzed. Our system differs from traditional approaches such as state machine or quorum based replication that have been developed to tolerate Byzantine failures. Unlike such systems, we augment replication with secret sharing and demonstrate that such a hybrid scheme offers additional flexibility that is not possible with replication alone.
[Availability, Byzantine fault tolerance, Costs, Protocols, distributed storage, Data security, data security, secure storage, Educational institutions, data integrity, data confidentiality, Application software, Distributed computing, Fault tolerance, storage management, security of data, Information security, data availability, secret sharing scheme, fault tolerant computing, Cryptography]
Building topology-aware overlays using global soft-state
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Distributed hash table (DHT) based overlay networks offer an administration-free and fault-tolerant storage space that maps "keys" to "values". For these systems to function efficiently, their structures must fit that of the underlying network. Existing techniques for discovering network proximity information, such as landmark clustering and expanding-ring search are either inaccurate or expensive. The lack of global proximity information in overlay construction and maintenance can result in bad proximity approximation or excessive communication. To address these problems, we propose the following: (1) Combining landmark clustering and round-trip time (RTT) measurements to generate proximity information, achieving both efficiency and accuracy. (2) Controlled placement of global proximity information on the system itself as soft-state, such that nodes can independently access relevant information efficiently. (3) Publish/subscribe functionality that allows nodes to subscribe to the relevant soft-state and get notified as the state changes necessitate overlay restructuring.
[topology-aware overlay network, Communication system control, landmark clustering, distributed processing, Control systems, table lookup, Fault tolerance, Network topology, Fault tolerant systems, fault-tolerant storage space, round-trip time measurement, Buildings, expanding-ring search, global soft-state, Routing, Extraterrestrial measurements, Time measurement, network topology, global proximity information, distributed hash table, file organisation, network proximity information, fault tolerant computing, Internet, content-addressable storage]
PARM : power aware reconfigurable middleware
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
In distributed environments, generic middleware services (e.g. caching, location management etc.) are widely used to satisfy application needs in a cost-effective manner. Such middleware services consume system resources such as storage, computation and communication and can be sources of significant power overheads when executed on low-power devices. We present a distributed middleware framework (PARM), that is inherently power-aware and reconfigures itself to adapt to diminishing power levels of low-power devices. In this paper, we i) determine whether a reconfigurable component-based middleware framework can be utilized to achieve energy gains in low-power devices, while preserving the semantics of the middleware services, ii) present and evaluate a graph theoretic approach for dynamically determining middleware component reconfigurations and ascertaining the optimal frequency at which the restructuring should occur, for maximal energy gains at the device. We use extensive profiling to chart the energy usage patterns of middleware components and applications, and use the profiled data to drive our reconfiguration decisions. Our simulation results demonstrate that our framework is able to save 5% to 35% of energy depending on the nature and class of applications and middleware components used.
[Energy consumption, Power system management, Optimization methods, PARM, distributed environment, Application software, optimal frequency, Middleware, Environmental management, power aware reconfigurable middleware, Computer science, Network servers, reconfigurable architectures, power overhead, low-power devices, Frequency, system resources, reconfigurable component-based middleware framework, Energy management, distributed object management, middleware]
An integrated approach to modeling and analysis of embedded real-time systems based on Timed Petri Nets
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
In computer-based control systems, embedded software is taking over what mechanical and dedicated electronic systems used to do, that is, to engage and control the physical world, interacting directly with sensors and actuators. Therefore, software running on a digital processor is tightly-coupled with its surrounding physical environment. We propose an integrated approach based on Timed Petri-Nets for modeling and analysis of embedded real-time systems where real-time scheduling behavior of the controller software is explicitly represented at the model-level, together with the physical environment that it interacts with. This enables the designer to have an integrated view of the entire system while analyzing the system and making design decisions. We also describe a syntax-directed, automated translation procedure from Timed Petri-Nets to Timed Automata, thus enabling the use of model checkers such as UPPAAL for analysis purposes. We consider the railroad crossing problem as an application example, and evaluate alternatives for controller implementation on either single-processor or distributed multi-processor platforms based on the integrated approach.
[Real time systems, Embedded computing, Actuators, multiprocessing systems, embedded real-time system, Petri nets, automated translation procedure, distributed multiprocessor scheduling, Control systems, UPPAAL model checker, Sensor systems, parallel machines, processor scheduling, Embedded software, digital processor, Mechanical sensors, formal verification, embedded software, Physics computing, embedded systems, Automatic control, controller software, timed automata]
Improving application throughput with enterprise JavaBeans Caching
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
We present the design of an Enterprise JavaBeans (EJBs) caching architecture, and show that EJB caching can greatly improve application throughput. Throughput is improved because data serving is offloaded from the database server to the cache-enabled application server. An important feature of our architecture is that the caching function is transparent to applications that use it. The cache-enabled application server uses the same (EJB) programming model, and the same transactional semantics, as provided by non-caching architectures.
[Java, database server, Scalability, EJB caching architecture, Relational databases, Throughput, Spatial databases, cache storage, enterprise JavaBeans caching architecture, Transaction databases, Security, Concurrent computing, cache-enabled application server, Runtime, transactional semantic, application throughput, Logic, data serving, distributed object management]
Mobile agent rendezvous in a ring
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
In the rendezvous search problem, two mobile agents must move along the n nodes of a network so as to minimize the time required to meet or rendezvous. When the mobile agents are identical and the network is anonymous, however, the resulting symmetry can make the problem impossible to solve. Symmetry is typically broken by having the mobile agents run either a randomized algorithm or different deterministic algorithms. We investigate the use of identical tokens to break symmetry so that the two mobile agents can run the same deterministic algorithm. After deriving the explicit conditions under which identical tokens can be used to break symmetry on the n node ring, we derive the lower and upper bounds for the time and memory complexity of the rendezvous search problem with various parameter sets. While these results suggest a possible tradeoff between the mobile agents' memory and the time complexity of the rendezvous search problem, we prove that this tradeoff is limited.
[Algorithm design and analysis, memory complexity, time complexity, Search problems, Mathematics, randomized algorithm, deterministic algorithms, randomised algorithms, Computer science, Fault diagnosis, Intelligent networks, Upper bound, rendezvous search problem, Mobile agents, mobile agent, Intrusion detection, mobile agents, search problems, Clocks, computational complexity]
Deliver multimedia streams with flexible QoS via a multicast DAG
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
In this paper we investigate the problem of delivering real time multimedia streams in a MAN environment. Most research in this area presumes the scenario where all packets of a stream are transmitted along a single path from the sender to a receiver In this paper, we propose a new application layer multicast scheme, namely ROME, to better fulfill heterogeneous QoS requirements from various receivers simultaneously by setting up multiple paths. Instead of enforcing a multicast tree structure, a multicast DAG (Directed Acyclic Graph) is constructed in ROME, where multiple nodes in the multicast group may share the responsibility to deliver the stream to a node. To achieve this each stream is partitioned into a set of stream shares. A QoS oriented share assignment technique is proposed to distribute stream shares progressively among delivery paths to maximize the number of fulfilled clients while minimizing the overall bandwidth usage. Moreover, both the delivery paths and the stream partition can be adapted easily on the fly to accommodate multicast group membership change during the real time delivery. Last but not least, we evaluate our scheme via simulations and show that the proposed scheme can fulfill a larger number of clients with higher QoS requirements than existing approaches under the same network capacity.
[Tree data structures, metropolitan area network, Quality of service, real time multimedia stream, directed acyclic graph, Multicast protocols, quality of service, DAG, Multimedia communication, application layer multicast protocol, Computer science, Unicast, QoS, directed graphs, metropolitan area networks, multicast protocols, telecommunication network routing, Bandwidth, Streaming media, Video compression, Internet, multimedia communication]
VirtualWire: a fault injection and analysis tool for network protocols
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
The prevailing practice for testing protocol implementations is direct code instrumentation to trigger specific states in the code. This leaves very little scope for reuse of the test cases. In this paper, we present the design, implementation, and evaluation of VirtualWire, a network fault injection and analysis system designed to facilitate the process of testing network protocol implementations. VirtualWire injects user-specified network faults and matches network events against anticipated responses based on high-level specifications written in a declarative scripting language. With VirtualWire, testing requires no code instrumentation and fault specifications can be reused across versions of a protocol implementation. We illustrate the effectiveness of VirtualWire with examples drawn from testing Linux's TCP implementation and a real-time Ethernet protocol called Rether. In each case, 10 to 20 lines of script is sufficient to specify the test scenario. VirtualWire is completely transparent to the protocols under test, and additional overhead in protocol processing latency it introduces is below 10% of the normal.
[fault specification, System testing, Ethernet networks, Linux TCP implementation testing, Instruments, network protocol, Laboratories, Rether, Access protocols, local area networks, Ethernet protocol, formal specification, Delay, Computer science, Filters, formal verification, code instrumentation, transport protocols, fault tolerant computing, Kernel, Formal verification, VirtualWire]
PEAS: a robust energy conserving protocol for long-lived sensor networks
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
In this paper we present PEAS, a robust energy-conserving protocol that can build long-lived, resilient sensor networks using a very large number of small sensors with short battery lifetime. PEAS extends the network lifetime by maintaining a necessary set of working nodes and turning off redundant ones. PEAS operations are based on individual node's observation of the local environment and do not require any node to maintain per neighbor node state. PEAS performance possesses a high degree of robustness in the presence of both node power depletions and unexpected failures. Our simulations and analysis show that PEAS can maintain an adequate working node density in the face of up to 38% node failures, and it can maintain roughly a constant overhead level under various deployment conditions ranging from sparse to very dense node deployment by using less than 1% of total energy consumption. As a result, PEAS can extend a sensor network's functioning time in linear proportion to the deployed sensor population.
[power depletions, Protocols, PEAS, wireless sensor networks, network protocol, energy conserving protocol, distributed processing, Sensor systems, sensor networks, Distributed computing, Robustness, protocols, node failures, energy consumption]
Efficient distributed shared state for heterogeneous machine architectures
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
InterWeave is a distributed middleware system that supports the sharing of strongly typed, pointer-rich data structures across heterogeneous platforms. As a complement to RPC-based systems such as CORBA, .NET and Java RMI, InterWeave allows processes to access shared data using ordinary reads and writes. Experience indicates that InterWeave-style sharing facilitates the rapid development of distributed applications, and enhances performance through transparent caching of state. In this paper we focus on the aspects of InterWeave specifically designed to accommodate heterogeneous machine architectures. Beyond the traditional challenges of message-passing in heterogeneous systems, InterWeave (1) identifies and tracks data changes in the face of relaxed coherence models, (2) employs a wire format that captures not only data but also diffs in a machine and language-independent form, and (3) swizzles pointers to maintain long-lived (cross-call) address transparency. To support these operations, InterWeave maintains an extensive set Of metadata structures, and employs a variety of performance optimizations. Experimental results show that InterWeave achieves performance comparable to that of RPC parameter passing when transmitting previously uncached data. When updating data that have already been cached, InterWeave's use of platform-independent diffs allows it to significantly outperform the straightforward use of RPC.
[heterogeneous machine architecture, Java RMI, data structure, Wire, Optimization, distributed shared state, metadata structure, Computer architecture, data structures, RPC-based system, IP networks, distributed object management, middleware, Java, meta data, message passing, .NET technology, Data structures, InterWeave-style sharing facility, Middleware, Computer science, CORBA, Coherence, distributed shared memory systems, remote procedure calls, Internet, performance optimization, distributed middleware system]
Cross-feature analysis for detecting ad-hoc routing anomalies
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
With the proliferation of wireless devices, mobile ad-hoc networking (MANET) has become a very exciting and important technology. However, MANET is more vulnerable than wired networking. Existing security mechanisms designed for wired networks have to be redesigned in this new environment. In this paper, we discuss the problem of intrusion detection in MANET. The focus of our research is on techniques for automatically constructing anomaly detection models that are capable of detecting new (or unseen) attacks. We introduce a new data mining method that performs "cross-feature analysis" to capture the inter-feature correlation patterns in normal traffic. These patterns can be used as normal profiles to detect deviation (or anomalies) caused by attacks. We have implemented our method on a few well known ad-hoc routing protocols, namely, Dynamic Source Routing (DSR) and Ad-hoc On-Demand Distance Vector (AODV), and have conducted extensive experiments on the ns-2 simulator. The results show that the anomaly detection models automatically computed using our data mining method can effectively, detect anomalies caused by typical routing intrusions.
[telecommunication security, Data security, data mining, routing protocol, Educational institutions, intrusion detection, Data mining, Communication system security, Mobile ad hoc networks, mobile computing, MANET, mobile ad-hoc networking, cross-feature analysis, Intrusion detection, routing protocols, ad hoc routing anomaly detection model, ad-hoc on-demand distance vector, Routing protocols, Computer networks, Cryptography, ad hoc networks, dynamic source routing, Mobile computing]
Weakly-connected dominating sets and sparse spanners in wireless ad hoc networks
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
A set S is dominating if each node in the graph G = (V, E) is either in S or adjacent to at least one of the nodes in S. The subgraph weakly induced by S is the graph G' = (V, E') such that each edge in E' has at least one end point in S. The set S is a weakly-connected dominating set (WCDS) of G if S is dominating and G' is connected G' is a sparse spanner if it has linear edges. In this paper, we present two distributed algorithms for finding a WCDS in O(n) time. The first algorithm has an approximation ratio of 5, and requires O(n log n) messages. The second algorithm has a larger approximation ratio, but it requires only O(n) messages. The graph G' generated by the second algorithm forms a sparse spanner with a topological dilation of 3, and a geometric dilation of 6.
[Spine, graph theory, sparse spanner, topology, Routing, Ad hoc networks, set theory, communication complexity, Mobile ad hoc networks, Computer science, Intelligent networks, weakly-connected dominating set, distributed algorithm, message complexity, mobile computing, wireless ad hoc network, distributed algorithms, Linear approximation, Broadcasting, Approximation algorithms, ad hoc networks, Distributed algorithms]
New algorithms for content-based publication-subscription systems
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
This paper introduces new algorithms specifically designed for content-based publication-subscription systems. These algorithms can be used to determine multicast groups with as much commonality as possible, based on the totality of subscribers' interests. The algorithms are based oil concepts borrowed from the literature on spatial databases and clustering. These algorithms perform well in the context of highly heterogeneous subscriptions, and they also scale well. Based on concepts borrowed from the spatial database literature, we develop an algorithm to match publications to subscribers in real-time. We also investigate the benefits of dynamically determining whether to unicast, multicast or broadcast information about the events over the network to the matched subscribers. We call this the distribution method problem. Some of these same concepts can be applied to match publications to subscribers in real-time, and also to determine dynamically whether to unicast, multicast or broadcast information about the events over the network to the matched subscribers. We demonstrate the quality of our algorithms via a number of realistic simulation experiments.
[visual databases, distributed processing, content-based publication-subscription systems, Distributed computing, content management, spatial databases, multicast group, broadcast information, real-time systems, heterogeneous subscription, multicast communication, statistical analysis, real-time system, computational complexity, distribution method problem]
A context-aware data management system for ubiquitous computing applications
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
One of the factors that differentiates ubiquitous computing from traditional distributed computing is context. Context, such as time, location, and situation, allows a system to adapt to the current surroundings in order to facilitate the use of the computational environment. In this paper, we present a file system for ubiquitous computing applications that is context-aware. Context is used to support the types of applications and devices that are found in ubiquitous computing spaces. Novel features of the system include how the view of data adapts to the activity being currently performed and how user data is imported into the local environment. Our system is evaluated as part of a ubiquitous computing infrastructure deployed in a seminar room to investigate issues of performance, scalability, and usability.
[file servers, Conference management, distributed processing, Ubiquitous computing, file system, ubiquitous computing, database management systems, context-aware data management system, Distributed computing, distributed computing]
Efficiently distributing component-based applications across wide-area environments
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Distributed deployment of network applications in wide-area environments has proven effective for improving enduser experience. Another trend is the use of component frameworks for building network services. Their component-based nature makes such applications natural candidates for distributed deployment, but it is unclear if the design patterns underlying component frameworks also enable efficient service distribution. In this paper, we investigate the application design rules and accompanying system-level support essential to a beneficial and efficient service distribution process. Our study targets the widely used Java 2 Enterprise Edition (J2EE) component platform and Java Pet Store, a sample component-based e-commerce application. Our results present strong experimental evidence that component-based applications can be efficiently distributed in wide-area environments using a small set of generally applicable design rules for orchestrating interactions and managing component state. We additionally discuss enforcement of these rules, and their automated implementation by container frameworks.
[Java, object-oriented programming, wide area networks, J2EE component platform, Buildings, Containers, Data processing, service distribution process, wide-area network, Application software, Environmental management, application design rule, component-based e-commerce application, Computer science, Web and internet services, IP networks, Positron emission tomography, middleware]
Partial lookup services
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Lookup services are used in many Internet applications to translate a key (e.g., a file name) into an associated set of entries (e.g., the location of file copies). The key lookups can often be satisfied by returning just a few entries instead of the entire set. However, current implementations of lookup services do not take advantage of this usage pattern. In this paper, we formalize the notion of a partial lookup service that explicitly supports returning a subset of the entries per lookup. We present four schemes for building a partial lookup service, and propose various metrics for evaluating the schemes. We show that a partial lookup service may have significant advantages over conventional ones in terms of space usage, fairness, fault tolerance, and other factors.
[partial lookup service, Costs, fault tolerance, Peer to peer computing, Internet applications, Application software, Sun, Computer science, table lookup, space usage, Network servers, Fault tolerance, Operating systems, Web and internet services, Intrusion detection, data structures]
Collision avoidance in single-channel ad hoc networks using directional antennas
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Three collision-avoidance protocols are analyzed that use omni-directional packet reception together with omni-directional transmissions, directional transmissions, or a combination of both. A simple model is introduced to analyze the performance of these collision avoidance protocols in multi-hop networks with arbitrary topologies. The numerical results of this analysis show that collision avoidance using a narrow antenna beamwidth for the transmission of all control and data packets achieves the highest throughput among the three collision avoidance schemes considered. Simulation experiments of the popular IEEE 802.11 MAC protocol and its variants based on directional transmissions and omni-directional packet reception validate the results predicted in the analysis. The results further show that narrow-beamwidth transmissions can also reduce the average delay experienced by nodes. It is concluded that the advantage of spatial reuse achieved by narrow-beamwidth transmissions outweighs that of conservative collision avoidance schemes featured by the omnidirectional transmission of some control packets. This is due to the fact that the latter requires far more stringent coordination of nodes with their neighbors and hidden terminals, which can lead to much more channel resource wasted due to nodes' excessive waiting time.
[collision avoidance, omnidirectional transmission, spatial reuse, Protocols, telecommunication channels, telecommunication congestion control, Transmitting antennas, Throughput, access protocols, Analytical models, Network topology, directive antennas, multihop network, Directional antennas, Spread spectrum communication, omnidirectional packet reception, data packet, narrow-beamwidth transmission, directional antenna, Performance analysis, directional transmissions, computer networks, IEEE 802.11 MAC protocol, Ad hoc networks, channel resource, single-channel ad hoc network, ad hoc networks, Collision avoidance]
Scalable service differentiation in a shared storage cache
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Motivated by the need to enable easier data sharing and curb rising storage management costs, storage systems are becoming increasingly consolidated and thereby shared by a large number of users and applications. In such environments, service differentiation becomes increasingly important. Since caching is a fundamental and pervasive technique employed to improve the performance of storage systems, providing differentiated services from a storage cache is a crucial component of the entire end-to-end QoS solution. In this paper we discuss a QoS architecture for a shared storage proxy cache which can provide long-term hit rate assurances to competing classes. The proposed architecture consists of three components: (a) per-class feedback controllers that track the performance of each class, (b) a fairness controller that allocates excess resources fairly in the case when all goals are met, and (c) a contention resolver that decides cache allocation in the case when at least one class does not meet its target hit rate. We compare the performance of various feedback per-class controllers, and provide guidelines for designing QoS mechanisms for such a dynamic environment.
[QoS architecture, contention re-solver, Target tracking, Costs, Control systems, per-class feedback controller, Cache storage, cache storage, quality of service, ubiquitous computing, Adaptive control, Environmental management, shared storage, Guidelines, fairness controller, resource allocation, Feedback, Outsourcing, Resource management, hit rate]
A path information caching and aggregation approach to traffic source identification
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Probabilistic packet marking (PPM) is a technique designed to identify packet traffic sources with low storage and processing overhead on network routers. In most previous PPM approaches, individual path messages carry only partial path information. These methods are susceptible to "path falsification" attacks, which greatly reduce their effectiveness. This work proposes a path-falsification-attack free PPM algorithm called Path Information Caching and Aggregation (PICA) that records paths of packet streams in fix-length path messages, thus eliminating the need of path reconstruction at the receiver end. Besides, by using a router's forwarding table to decompose packet volume, this semi-stateful method is more accurate in traffic volume report. It also supports both a packet rate-based path message generation algorithm and a redundant path message suppression mechanism to further eliminate path messages with the same destination. Finally, PICA protects PICA routers from being attacked by faked path messages. We have performed a trace-driven simulation study on the proposed PICA algorithm and compared its effectiveness with IETF's iTrace scheme by varying the sampling probability, the number of attack sources, and attack traffic rate. Compared to iTrace, the PICA algorithm reduces the total number of path messages required by a factor of more than 2, while reporting traffic volume more accurately.
[redundant path message suppression mechanism, probability, Telecommunication traffic, Master-slave, cache storage, path information caching and aggregation, PICA network routers, Computer crime, packet traffic source identification, Computer science, Network servers, probabilistic packet marking, message authentication, telecommunication network routing, path message generation algorithm, lETFs iTrace scheme, Traffic control, Sampling methods, IP networks, packet sampling, Protection, Web server, telecommunication traffic]
Human design: wearable computers for human networking
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Computer technology has mostly focused either on the isolated individual, or has treated the person as a clueless extra wandering in a computer-controlled environment. Researchers seem to have forgotten that people are social animals, and that the quality of their lives is defined by their roles in human organizations. Instead of inventing technology for the individual as an isolated entity, why not invent systems that support people's organizational roles? Or even invent new types of organizations? My colleagues and I are inventing technology that can potentially produce organizations that are more creative and efficient, and that better support the individual. Using wearable computers that actively analyze face-to-face interactions within the workplace we can extract conversational features, identify participants, define context, and determine content. By aggregating this information, high-potential collaborations and expertise within the organization can be identified, information movement and decisionmaking can be augmented, and social networks reinforced. Examples using this technology to initiate productive connections are shown, and privacy concerns are addressed.
[Social network services, Humans, ergonomics, computer technology, Data mining, intranets, computer-controlled environment, human networking, wearable computers, Privacy, human organization, Animals, Wearable computers, Employment, Isolation technology, Feature extraction, Collaborative work, organizational role, organisational aspects, wearable computer]
Method partitioning - runtime customization of pervasive programs without design-time application knowledge
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Method Partitioning is a dynamic technique for customizing performance-critical message-based interactions between program components, at runtime and without the need for design-time application knowledge. The technique partitions program units that implement message handling, with low costs and high levels of flexibility. It consists of (a) static analysis of a message handling method to produce candidate partitioning plans for the method, (b) cost models for evaluating the cost/benefits of different partitioning plans, (c) a Remote Continuation mechanism that "connects" the distributed parts of a partitioned method at runtime, and (d) Runtime Profiling and Reconfiguration which monitors actual costs of candidate plans and dynamically selects "best" plans from candidates. Experiments with prototypical implementation of Method Partitioning in the JECho distributed event system demonstrate significant performance improvements for both communication-bound and compute-intensive applications, with both applications having dynamic factors that are not predictable at design time.
[Costs, design-time application knowledge, Optimization methods, runtime customization, Telecommunication traffic, program component, ubiquitous computing, Distributed computing, cost model, candidate partitioning plan, Network servers, Runtime, USA Councils, Jecho distributed event system, Mobile agents, cost-benefit analysis, pervasive programs, message passing, program diagnostics, static analysis, remote continuation mechanism, method partitioning, Application software, runtime profiling, Computer science, message handling, compute-intensive application, message-based interaction, communication-bound application]
Self-stabilizing smoothing and counting
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
A smoothing network is a distributed data structure that accepts tokens on input wires and routes them to output wires. It ensures that however imbalanced the traffic on input wires, the numbers of tokens emitted on output wires are approximately balanced. Prior work on smoothing networks always assumed that such networks were properly initialized. In a real distributed system, however, network switches may be rebooted or replaced dynamically, and it may not be practical to determine the correct initial state for the new switch. Prior analyses do not work under these new assumptions. This paper makes the following contributions. First, we show that some well-known 1-smoothing networks, known as counting networks, when started in an arbitrary initial state (perhaps chosen by an adversary), remain remarkably smooth, degrading from 1-smooth to log(n)-smooth, where n is the number of input/output wires. Second, we show that the same networks can be made eventually 1-smooth by "piggy-backing" a small amount of additional information on messages when (and only when) trouble is detected.
[Smoothing methods, distributed data structure, Switches, Telecommunication traffic, distributed processing, Data structures, Distributed computing, smoothing network, Computer science, Degradation, Network servers, Wires, data structures, network switches, counting networks, stability, computational complexity]
Obstruction-free synchronization: double-ended queues as an example
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
We introduce obstruction-freedom, a new nonblocking property for shared data structure implementations. This property is strong enough to avoid the problems associated with locks, but it is weaker than previous nonblocking properties-specifically lock-freedom and wait-freedom-allowing greater flexibility in the design of efficient implementations. Obstruction-freedom admits substantially simpler implementations, and we believe that in practice it provides the benefits of wait-free and lock-free implementations. To illustrate the benefits of obstruction-freedom, we present two obstruction-free CAS-based implementations of double-ended queues (deques); the first is implemented on a linear array, the second on a circular array. To our knowledge, all previous nonblocking deque implementations are based on unrealistic assumptions about hardware support for synchronization, have restricted functionality, or have operations that interfere with operations at the opposite end of the deque even when the deque has many elements in it. Our obstruction-free implementations have none of these drawbacks, and thus suggest that it is much easier to design obstruction-free implementations than lock-free and wait-free ones. We also briefly discuss other obstruction-free data structures and operations that we have implemented.
[wait-free implementation, Laboratories, obstruction-free synchronization, Drives, Data structures, lock-free implementation, nonblocking property, Yarn, Sun, Delay, synchronisation, Computer science, storage management, Content addressable storage, shared data structure implementation, Hardware, data structures, double-ended queues, Software engineering]
Performance guarantees for cluster-based internet services
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
As web-based transactions become an essential element of everyday corporate and commerce activities, it becomes increasingly important that the performance of web-based services be predictable and guaranteed even in the presence of wildly fluctuating input loads. In this paper, we propose a general implementation framework to provide quality of service (QoS) guarantee for cluster-based Internet services, such as E-commerce or directory service. We describe the design, implementation, and evaluation of a web request distribution system called Gage, which can provide every subscriber with distinct guarantee on the number of generic web requests that are serviced per second regardless of the total input loads at run time. Gage is one of the first systems that can support QoS guarantee involving multiple system resources, i.e., CPU, disk, and network. The frontend request distribution server of Gage distributes incoming requests among a cluster of back-end web server nodes so as to maintain per-subscriber QoS guarantee and load balance among the back-end servers. Each back-end web server node includes a Gage module, which performs distributed TCP splicing and detailed resource usage accounting. Performance evaluation of the fully operational Gage prototype demonstrates that the proposed architecture can indeed provide the guaranteed request throughput for different classes of web accesses, even in the presence of excessive input loads. The additional performance overhead associated with QoS support in Gage is merely 3.06%.
[workstation clusters, Quality of service, CPU, Resource virtualization, Uniform resource locators, Network servers, resource allocation, QoS, Web and internet services, Computer architecture, e-commerce, Web server, Business, electronic commerce, Splicing, performance evaluation, Web based transaction, quality of service, directory service, Web request distribution system, distributed TCP splicing, Computer science, transport protocols, cluster-based Internet service, Gage, load balance, Internet]
A graph-theoretical analysis of multicast authentication
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Message authentication is considered as a serious bottleneck to multicast security, particular for stream-type of traffic. The technique of hash chaining/signature amortization has been proposed in many schemes for stream authentication, with or without multicast settings. However, none of them is optimal. They either have a large packet overhead or are not robust to packet loss. Some even have a large receiver delay or require a large receiver buffer size. These schemes are constructed by trial-and-error methods. There lack tools to evaluate and compare their performances. There is no systematic way to construct these schemes either In this paper, we introduce the notion of dependence-graphs which links these hash-chained authentication schemes to the well-known graph theory, and provides an effective analytical tool. Many important metrics of a hash-chained authentication scheme can be readily and easily determined from its dependence-graph. As well, a dependence-graph demonstrates design tradeoff and provides insights into optimizing hash-chained schemes.
[Performance evaluation, telecommunication security, graph theory, Educational institutions, Graph theory, Delay, hash-chained multicast authentication, Strontium, message authentication, multicast communication, Robustness, Internet, Computer security, Digital signatures, Message authentication, dependence-graph]
Neighbor table construction and update in a dynamic peer-to-peer network
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
In a system proposed by Plaxton, Rajaraman and Richa (PRR), the expected cost of accessing a replicated object was proved to be asymptotically optimal for a static set of nodes and pre-existence of consistent and optimal neighbor tables in nodes [9]. To implement PRR's hypercube routing scheme in a dynamic, distributed environment, such as the Internet, various protocols are needed (for node joining, leaving, table optimization, and failure recovery). In this paper we first present a conceptual foundation, called C-set trees, for protocol design and reasoning about consistency. We then present the detailed specification of a join protocol. In our protocol, only nodes that are joining need to keep extra state information about the join process. We present a rigorous proof that the join protocol generates consistent neighbor tables for an arbitrary number of concurrent joins. The crux of our proof is based upon induction on a C-set tree. Our join protocol can also be used for building consistent neighbor tables for a set of nodes at network initialization time. Lastly, we present both analytic and simulation results on the communication cost of a join in our protocol.
[PRR hypercube routing scheme, Peer to peer computing, Induction generators, trees (mathematics), Access protocols, distributed processing, distributed environment, optimal neighbor table construction, Intelligent networks, Analytical models, C-set trees, Fault tolerant systems, routing protocols, Hypercubes, Cost function, Routing protocols, Internet, protocol design, dynamic peer-to-peer network]
Rate-monotonic scheduling on uniform multiprocessors
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Each processor in a uniform multiprocessor machine is characterized by a speed or computing capacity, with the interpretation that a job executing on a processor with speed s for t time units completes (s /spl times/ t) units of execution. The scheduling of systems of periodic tasks on uniform multiprocessor platforms using the rate-monotonic scheduling algorithm is considered here. A simple, sufficient test is presented for determining whether a given periodic task system will be successfully scheduled by algorithm upon a particular uniform multiprocessor platform-this test generalizes earlier results concerning rate-monotonic scheduling upon identical multiprocessor platforms.
[multiprocessing systems, Processor scheduling, parallel machine, embedded systems, rate-monotonic scheduling algorithm, static priority, Distributed computing, parallel machines, processor scheduling, multiprocessor scheduling, periodic tasks]
An integrated resource sharing policy for multimedia storage servers based on network-attached disks
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
In this paper we propose using the network-attached disk (NAD) architecture to design highly scalable and cost-effective multimedia-on-demand (MOD) servers. In order to ensure enhanced performance, we propose two schemes, called distributed interval caching (DIC) and multi-objective scheduling (MOS). The DIC scheme utilizes the on-disk buffers for caching intervals between successive streams, while the MOS scheme improves resource sharing by scheduling requests for service intelligently based on four predefined criteria. We then integrate the two schemes and study the overall performance benefits through extensive simulation. We also study the effectiveness of the proposed DIC scheme by developing an analytical model that estimates the performance limit of DIC The results demonstrate that the integrated policy works very well in increasing the number of customers that can be serviced concurrently while decreasing their waiting times, and that the performance improvements scale with the number of disks in the server.
[multimedia-on-demand server, multimedia servers, network-attached disk architecture, File servers, cache storage, Multimedia communication, processor scheduling, Computer science, on-disk buffer, Network servers, Design engineering, distributed interval caching, multiobjective scheduling, resource allocation, video on demand, resource sharing, Computer architecture, Bandwidth, Broadcasting, scheduling, Motion pictures, Resource management]
TCP-PR: TCP for persistent packet reordering
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Most standard implementations of TCP perform poorly when packets are reordered. In this paper, we propose a new version of TCP that maintains high throughput when reordering occurs and yet, when packet reordering does not occur is friendly to other versions of TCP. The proposed TCP variant, or TCP-PR, does not rely on duplicate acknowledgments to detect a packet loss. Instead, timers are maintained to keep track of how long ago a packet was transmitted. In case the corresponding acknowledgment has not yet arrived and the elapsed time since the packet was sent is larger than a given threshold, the packet is assumed lost. Because TCP-PR does not rely on duplicate acknowledgments, packet reordering (including out-of-order acknowledgments) has no effect on TCP-PR performance. Through extensive simulations, we show that TCP-PR performs consistently better than existing mechanisms that try to make TCP more robust to packet reordering. When the case that packets are not reordered, we verify that TCP-PR maintains the same throughput as typical implementations of TCP (specifically, TCP-SACK) and shares network resources fairly.
[TCP, transport protocols, packet loss detection, packet switching, persistent packet reordering, Distributed computing]
Group membership and wide-area master-worker computations
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Group communications systems have been designed to provide an infrastructure for fault-tolerance in distributed systems, including wide-area systems. In our work on master-worker computation for GriPhyN, which is a large project in the area of the computational grid, we asked the question should we build our wide-area master-worker computation using wide-area group communications? This paper explains why we decided doing so was not a good idea.
[Context, client-server systems, Protocols, wide area networks, fault tolerance, Scalability, fault-tolerance, grid computing, Distributed computing, master-worker computation, Computer science, Jacobian matrices, group communications system, computational grid, Fault tolerant systems, Grid computing, distributed systems, wide area systems, Computer networks, Local area networks]
PeerCQ: a decentralized and self-configuring peer-to-peer information monitoring system
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
PeerCQ is a totally decentralized system that performs information monitoring tasks over a network of peers with heterogeneous capabilities. It uses Continual Queries (CQs) as its primitives to express information-monitoring requests. A primary objective of the PeerCQ system is to build a decentralized Internet scale distributed information-monitoring system, which is highly scalable, self-configurable and supports efficient and robust way of processing CQs. This paper describes the basic architecture of the PeerCQ system and focuses on the mechanisms used for service partitioning at the P2P protocol layer. A set of initial experiments is reported, demonstrating the sensitiveness of the PeerCQ approach to large scale P2P information monitoring and the effectiveness of the PeerCQ service-partitioning algorithms with respect to load balancing and system utilization.
[continual query, Protocols, open systems, load balancing, Peer to peer computing, File servers, Routing, Educational institutions, peer-to-peer information monitoring system, PeerCQ architecture, decentralized system, resource allocation, Robustness, Computer networks, Internet, Large-scale systems, PeerCQ service-partitioning algorithm, system utilization, Monitoring]
Software fault tolerance of distributed programs using computation slicing
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Writing correct distributed programs is hard. In spite of extensive testing and debugging, software faults persist even in commercial grade software. Many distributed systems, especially those employed in safety-critical environments, should be able to operate properly even in the presence of software faults. Monitoring the execution of a distributed system, and, on detecting a fault, initiating the appropriate corrective action is an important way to tolerate such faults. This gives rise to the predicate detection problem which involves finding a consistent cut of a distributed computation, if it exists, that satisfies the given global predicate. Detecting a predicate in a computation is, however, an NP-complete problem. To ameliorate the associated combinatorial explosion problem, we introduce the notion of computation slice in our earlier papers [5, 10]. Intuitively, slice is a concise representation of those consistent cuts that satisfy a certain condition. To detect a predicate, rather than searching the state-space of the computation, it is much more efficient to search the state-space of the slice. In this paper we provide efficient algorithms to compute the slice for several classes of predicates. Our experimental results demonstrate that slicing can lead to an exponential improvement over existing techniques in terms of lime and space.
[Software testing, program debugging, software debugging, partial-order method, computation slicing, software testing, predicate detection, search-space pruning, software-fault tolerance, Explosions, Software safety, NP-complete problem, Distributed computing, Software debugging, software fault tolerance, Fault tolerance, Fault detection, distributed algorithms, Writing, distributed program, program slicing, Monitoring, computational complexity]
Impact of data compression on energy consumption of wireless-networked handheld devices
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
We investigate the use of data compression to reduce the battery consumed by handheld devices when downloading data from proxy servers over a wireless LAN. To make a careful trade-off between the communication energy and the overhead to perform decompression, we experiment with three universal lossless compression schemes, using a popular handheld device in a wireless LAN environment and we find interesting facts. The results show that, from the battery-saving perspective, the gzip compression software (based on LZ77) to be far superior to bzip2 (based on BWT) and compress (based on LZW). We then present an energy model to estimate the energy consumption for the compressed downloading. With this model, we further reduce the energy cost of gzip by interleaving communication with computation and by using a block-by-block selective scheme based on the compression factor of each block. We also use a threshold file size below which the file is not to be compressed before transferring.
[Energy consumption, Wireless LAN, data compression, Costs, Data compression, data compression downloading, Batteries, battery-saving, Network servers, Handheld computers, gzip compression software, handheld device, proxy servers, Interleaved codes, Computer networks, wireless LAN, Web server, energy consumption, mobile handsets, wireless-network]
QoS-assured service composition in managed service overlay networks
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Many value-added and content delivery services are being offered via service level agreements (SLAs). These services can be interconnected to form a service overlay network (SON) over the Internet. Service composition in SON has emerged as a cost-effective approach to quickly creating new services. Previous research has addressed the reliability, adaptability, and compatibility issues for composed services. However little has been done to manage generic quality-of-service (QoS) provisioning for composed services, based on the SLA contracts of individual services. In this paper we present QUEST a QoS assUred composEable Service infrasTructure, to address the problem. QUEST framework provides: (1) initial service composition, which can compose a qualified service path under multiple QoS constraints (e.g., response time, availability). If multiple qualified service paths exist, QUEST chooses the best one according to the load balancing metric; and (2) dynamic service composition, which can dynamically recompose the service path to quickly recover from service outages and QoS violations. Different from the previous work, QUEST can simultaneously achieve QoS assurances and good load balancing in SON.
[QUEST framework, Protocols, Peer to peer computing, content delivery service, NASA, QoS-assured service composition, Quality of service, dynamic service composition, SLA, quality of service, load balancing metric, service level agreement, Intelligent networks, computer network management, resource allocation, service overlay network, Web and internet services, value-added service, Load management, Internet, Computer network management, Contracts, Portals]
On effectiveness of link padding for statistical traffic analysis attacks
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Traffic analysis attacks aim at deriving mission critical information from the analysis of the traffic transmitted over a network. Countermeasures for such attacks are usually realized by properly "padding" the payload traffic so that the statistics of the overall traffic become significantly different from that of the payload traffic. In this paper, we propose a analytical framework for traffic analysis attacks based on statistical pattern recognition techniques. We study the effectiveness of countermeasures for traffic analysis attacks within our proposed framework. Two basic countermeasure strategies are (a) to pad the traffic with constant interarrival times of packets (CIT) or (b) to pad the traffic with variable interarrival times (VIT). Our experiments show that CIT countermeasures fail when the adversary uses sample variance or sample entropy of packet interarrival times for statistical analysis. On the other hand, VIT countermeasures are effective regardless of which sample statistics are used by the adversary. These observations are validated by analysis of detection rates based on sample distributions of packet interarrival times.
[telecommunication security, Statistical analysis, payload traffic, variable interarrival time, Mission critical systems, statistical traffic analysis attacks, statistical pattern recognition, Telecommunication traffic, Entropy, Pattern recognition, Statistics, Information analysis, Statistical distributions, packet interarrival time, constant interarrival time, statistical analysis, Pattern analysis, telecommunication traffic, Payloads, pattern recognition]
Enabling snap-stabilization
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
A snap-stabilizing protocol guarantees that the system always behaves according to its specification provided some processor initiated the protocol. We present how to snap-stabilize some important protocols, like Leader Election, Reset, Snapshot, and Termination Detection. We use a Snap-stabilizing Propagation of Information with Feedback protocol for arbitrary networks as the key module in the above transformation process. Finally, we design a universal transformer to provide a snap-stabilizing version of any protocol (which can be self-stabilized with the transformer of [15]).
[Process design, Algorithm design and analysis, termination detection protocol, Protocols, multiprocessing systems, leader election protocol, snapshot protocol, Nominations and elections, Distributed computing, formal specification, Computer science, feedback protocol, Feedback, Fault tolerant systems, reset protocol, Broadcasting, snap-stabilizing protocol, fault tolerant computing, protocols, arbitrary networks, stability, Fuzzy systems]
Scalable distributed concurrency services for hierarchical locking
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Middleware components are becoming increasingly important as applications share computational resources in distributed environments. One of the main challenges in such environments is to achieve scalability of concurrency control. Existing concurrency protocols lack scalability. Scalability enables resource sharing and computing with distributed objects in systems with a large number of nodes. We have designed and implemented a novel, scalable and filly decentralized middleware concurrency control protocol. Our experiments on a Linux cluster indicate that an average number of three messages is required per lock request on a system with as many as 120, which is a logarithmic asymptote. At the same time, the response time for the requests scales linearly with the increase in concurrency level. A comparison to another scalable concurrency protocol shows that our protocol results in significantly superior asymptotic savings in message overhead and response time for large number of nodes. While our approach follows the specification of general CORBA concurrency services for large-scale data and object repositories, the principles are applicable to any distributed concurrency services and transaction models. The results of this work impact scalability for distributed computing facilities ranging from embedded computing with distributed objects over peer-to-peer computing environments to arbitrating accesses in very large database environments.
[workstation clusters, Protocols, Scalability, embedded computing, distributed environment, Distributed computing, Delay, very large database environment, Concurrent computing, resource allocation, very large databases, distributed object management, middleware, Linux cluster, Peer to peer computing, peer-to-peer computing environment, Concurrency control, Middleware, CORBA concurrency service, middleware concurrency control protocol, concurrency control, resource sharing, Computer applications, Resource management, scalable distributed concurrency service]
Efficient content-based event dispatching in the presence of topological reconfiguration
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Distributed content-based publish-subscribe middleware provides the decoupling, flexibility, expressiveness, and scalability required by highly dynamic distributed applications, e.g., mobile ones. Nevertheless, the available systems exploiting a distributed event dispatcher are unable to rearrange dynamically their behavior to adapt to changes in the topology of the dispatching infrastructure. In this work, we first define a strawman solution based on ideas proposed (but never precisely characterized) in existing work. We then analyze this solution and achieve a deeper understanding of how the event dispatching information is reconfigured. Based on this analysis, we modify the strawman approach to reduce its overhead. Simulations show that the reduction is significant (up to 50%), and yet the algorithm is resilient to concurrent reconfigurations.
[topological reconfiguration, Scalability, Fluid dynamics, Subscriptions, distributed content-based publish-subscribe middleware, dynamic distributed application, Middleware, Distributed computing, Information analysis, strawman approach, Network topology, distributed event dispatcher, event dispatching information, Publish-subscribe, Computer architecture, Dispatching, middleware]
Monotonicity and partial results protection for mobile agents
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Remotely executing mobile code introduces a plethora of security problems. This paper examines the "external agent replay" attack, identifies the notion of one-way program state transitions, describes the use of monotonic variables as a practical method for detecting these attacks, examines the more general problem state modification attacks, and introduces the use of "results verification vectors" to protect agents from attack.
[Engineering profession, security problem, Mobile communication, cryptography, Security, Distributed computing, Delay, Cryptographic protocols, formal verification, Mobile agents, mobile agent, mobile agents, remotely executing mobile code, remote procedure calls, Autonomous agents, partial results protection, Protection, one-way program state transitions, monotonic variables]
Optimal distribution tree for Internet streaming media
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Internet radio and television stations require significant bandwidth to support delivery of high quality audio and video streams to a large number of receivers. IP multicast is an appropriate delivery model for these applications. However, widespread deployment of IP multicast on the Internet is unlikely in the near future. An alternative is to build a multicast tree in the application layer Previous studies have addressed tree construction in the application layer However most of them focus on reducing delay. Few systems have been designed to achieve a high throughput for bandwidth-intensive applications. In this paper we present a distributed algorithm to build an application-layer tree. We prove that our algorithm finds a tree such that the average incoming rate of receivers in the tree is maximized (under certain network model assumptions). We also describe protocols that implement the algorithm. For implementation on the Internet, there is a tradeoff between the overhead of available bandwidth measurements and fast convergence to the optimal tree. This tradeoff can be controlled by tuning some parameters in our protocols. Our protocols are also designed to maintain a small number, O(log n), of soft states per node to adapt to network changes and node failures.
[TV, multicast protocol, topology, Access protocols, optimal distribution tree, Throughput, Proposals, multimedia computing, Delay, bandwidth allocation, distributed algorithm, multimedia, Tree graphs, distributed algorithms, multicast protocols, Bandwidth, application-layer tree, Streaming media, Internet, Distributed algorithms, bandwidth measurement]
Experiences with monitoring OSPF on a regional service provider network
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
This paper presents the results from a detailed, experimental study of OSPF an intra-domain routing protocol, running on a mid-size regional Internet service provider. Using multiple, distributed probes running custom monitoring tools, we collected continuous protocol information for a full year. We use this data to analyze the health of the network including the amount, source, duration and periodicity of routing instability. We found that information from external routing protocols produces significant levels of instability within OSPF. We also examine the evolution of the routing topology over time, showing that short term changes are incremental and that the long term trend shows constant change. Finally, we present a set of detailed investigations into several large scale anomalies. These anomalies demonstrate the significant impact external routing protocols have on OSPF. In addition, they highlight the need for new network management tools that can incorporate information from routing protocols.
[Data analysis, Computerized monitoring, Telecommunication traffic, regional Internet service provider network, OSPF monitoring, computer network management, network management tool, Network topology, Web and internet services, routing protocols, Production, Routing protocols, Large-scale systems, Internet, Probes, intra-domain routing protocol, Testing]
Efficient self-stabilizing algorithms for tree networks
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Many proposed self-stabilizing algorithms require an exponential number of moves before stabilizing on a global solution, including some rooting algorithms for tree networks [1, 2, 3]. These results are vastly improved upon in [6] with tree rooting algorithms that require only O(n/sup 3/ + n/sup 2//spl middot/c/sub h/) moves, where n is the number of nodes in the network and c/sub h/ is the highest initial value of a variable. In the current paper, we describe a new set of tree rooting algorithms that brings the complexity down to O(n/sup 2/) moves. This not only reduces the first term by an order of magnitude, but also reduces the second term by an unbounded factor We further show a generic mapping that can be used to instantiate an efficient self-stabilizing tree algorithm from any traditional sequential tree algorithm that makes a single bottom-up pass through a rooted tree. The new generic mapping improves on the complexity of the technique presented in [8].
[Algorithm design and analysis, tree networks, Heuristic algorithms, Government, graph theory, Nominations and elections, distributed processing, tree rooting algorithms, self-stabilizing algorithms, Tree graphs, sequential tree algorithm, Chromium, Dynamic programming, tree data structures, Informatics, computational complexity]
Local-spin mutual exclusion using fetch-and-/spl phi/ primitives
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
We present a generic fetch-and-/spl phi/-based local-spin mutual exclusion algorithm with O(1) time complexity under the RMR (remote-memory-reference) measure. This algorithm is "generic" in the sense that it can be implemented using any fetch-and-/spl phi/ primitive of rank 2N, where N is the number of processes. The rank of a fetch-and-/spl phi/ primitive expresses the extent to which processes may "order themselves" using that primitive. By using an arbitration tree, a /spl otimes/(log/sub r/ N) algorithm can be constructed using any primitive of rank r, where 2 /spl les/ r < N. For primitives that meet a certain additional condition, we present a O(log N/log log N) algorithm, which is time-optimal for certain primitives of constant rank.
[generic fetch, arbitration tree, local-spin mutual exclusion algorithm, time complexity, shared memory systems, remote-memory-reference measure, Distributed computing, computational complexity]
A case for grid computing on virtual machines
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
We advocate a novel approach to grid computing that is based on a combination of "classic" operating system level virtual machines (VMs) and middleware mechanisms to manage VMs in a distributed environment. The abstraction is that of dynamically instantiated and mobile VMs that are a combination of traditional OS processes (the VM monitors) and files (the VM state). We give qualitative arguments that justify our approach in terms of security, isolation, customization, legacy support and resource control, and we show quantitative results that demonstrate the feasibility of our approach front a performance perspective. Finally, we describe the middleware challenges implied by the approach and an architecture for grid computing using virtual machines.
[Computer aided software engineering, legacy support, grid computing, qualitative argument, middleware mechanism, resource control, Virtual machining, operating system level, distributed environment, Security, Middleware, Distributed computing, Environmental management, Voice mail, resource allocation, Operating systems, virtual machines, Grid computing, operating systems (computers), Virtual manufacturing, middleware]
Proceedings 23rd International Conference on Distributed Computing Systems
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
The following topics are dealt with: self-stabilizing systems; sensor networks and energy management; peer-to-peer services; distributed algorithms; multimedia streaming; secure data sharing; quality of service; Internet protocols; middleware communication services; fault-tolerant systems; ubiquitous computing; network security; real-time systems; Web servers; formal methods; ad hoc network protocols; wide area distributed computing; mobile agents; and wireless access control.
[Transport protocols, wireless sensor networks, open systems, multimedia systems, sensor networks, ubiquitous computing, secure data sharing, multimedia streaming, mobile agents, protocols, Distributed algorithms, middleware, Multimedia systems, Data security, network security, Web servers, quality of service, security of data, transport protocols, distributed algorithms, peer-to-peer services, real-time systems, Open systems, Internet, ad hoc networks, self-stabilising system, middleware communication, fault tolerant systems]
Energy-efficient real-time scheduling in IEEE 802.11 wireless LANs
23rd International Conference on Distributed Computing Systems, 2003. Proceedings.
None
2003
Energy constraint has become an important factor in the design of MAC protocols. This paper addresses the issue of providing deterministic timing guarantees in the wireless LAN while minimizing energy consumption of the wireless nodes. We present a centralized energy efficient algorithm, Scheduled Contention Free Burst (S-CFB), which is built upon the recently proposed Hybrid Coordination Function (HCF) in the IEEE 802.11 standard. Two mechanisms are proposed: 1) bundling message transmissions into multiple contention free bursts in order to reduce control overhead; 2) allowing wireless nodes to sleep by avoiding preemption whenever possible. Our performance stud), shows that SCFB meets the timing requirement for real-time messages, allows wireless nodes to switch to idle state for a longer period of time, and reduces the number of control frames required to maintain contention free transmission.
[Wireless LAN, Energy consumption, message transmission, IEEE 802.11 standard, Switches, energy constraint, Transceivers, access protocols, Bandwidth, scheduling, energy-efficient algorithm, IEEE standards, real-time scheduling, energy consumption, Local area networks, scheduled contention free burst, Job shop scheduling, Sleep, real-time systems, wireless nodes, Energy efficiency, Timing, wireless LAN, minimisation, hybrid coordination function, MAC protocols]
Energy efficient data collection in distributed sensor environments
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Sensors are typically deployed to gather data about the physical world and its artifacts for a variety of purposes that range from environment monitoring, control, to data analysis. Since sensors are resource constrained, often sensor data is collected into a sensor database that resides at (more powerful) servers. A natural tradeoff exists between the sensor resources (bandwidth, energy) consumed and the quality of data collected at the server. Blindly transmitting sensor updates at a fixed periodicity to the server results in a suboptimal solution due to the differences in stability of sensor values and due to the varying application needs that impose different quality requirements across sensors. We propose adaptive data collection mechanisms for sensor environments that adjusts to these variations while at the same time optimizing the energy consumption of sensors. Our experimental results show significant energy savings compared to the naive approach to data collection.
[Energy consumption, sensor database, Data analysis, distributed sensor environments, data analysis, Sensor phenomena and characterization, Sensor systems, distributed sensors, Power system modeling, Distributed computing, data collection, Computer science, Network servers, resource allocation, Distributed databases, distributed databases, Energy efficiency]
EnviroTrack: towards an environmental computing paradigm for distributed sensor networks
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Distributed sensor networks are quickly gaining recognition as viable embedded computing platforms. Current techniques for programming sensor networks are cumbersome, inflexible, and low-level. We introduce EnviroTrack, an object-based distributed middleware system that raises the level of programming abstraction by providing a convenient and powerful interface to the application developer geared towards tracking the physical environment. EnviroTrack is novel in its seamless integration of objects that live in physical time and space into the computational environment of the application. Performance results demonstrate the ability of the middleware to track realistic targets.
[Embedded computing, environmental computing paradigm, Target tracking, wireless sensor networks, Sensor systems, quality of service, EnviroTrack, Distributed computing, Middleware, tracking, distributed sensor networks, object-based distributed middleware system, Vehicles, Wireless sensor networks, Physics computing, Computer networks, Monitoring, distributed programming, middleware]
A clock synchronization algorithm for multi-hop wireless ad hoc networks
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
In multihop wireless ad hoc networks, it is important that all mobile hosts are synchronized. Synchronization is necessary for power management and for frequency hopping spread spectrum (FHSS) operations. IEEE 802.11 standards specify a clock synchronization protocol but this protocol suffers from the scalability problem due to its inefficiency contention mechanism. We propose an automatic self-time-correcting procedure (ASP) to achieve clock synchronization in a multihop environment. Our ASP has two features. Firstly, a faster host has higher priority to send its timing information out than a slower one. Secondly, after collecting enough timing information, a slower host can synchronize to the faster one by self-correcting its timer periodically (which makes it becoming a faster host). Simulation results show that our ASP decreases 60% the average maximum clock drift as compared to the IEEE 802.11 and reduces 99% the number of asynchronism in a large-scale multihop wireless ad hoc networks.
[frequency hopping spread spectrum, Protocols, wireless sensor networks, Scalability, clock synchronization algorithm, multihop wireless ad hoc networks, Application specific processors, Mobile ad hoc networks, synchronisation, automatic self-time-correcting procedure, clocks, mobile computing, protocol, mobile hosts, IEEE 802.11 standards, Spread spectrum communication, Timing, Large-scale systems, IEEE standards, ad hoc networks, protocols, Frequency synchronization, Energy management, Clocks]
Providing statistical delay guarantees in wireless networks
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
We study the delay performance of policed traffic to provide real-time guarantees over wireless networks. A number of models have been presented in the literature to describe wireless (radio or optical) networks in terms of the wireless channel and the underlying error control mechanisms. We describe a general framework to incorporate such models into delay guarantee computations for real-time traffic. Static-priority scheduling is considered, and two different admission control mechanisms are used to achieve the trade-off between resource utilization and admission overhead.
[radio links, statistical delay guarantee computation, telecommunication congestion control, static-priority scheduling, wireless networks, error control mechanisms, Delay, Intelligent networks, Wireless networks, Admission control, admission control mechanisms, Optical attenuators, real-time systems, Forward error correction, Traffic control, optical fibre networks, Communication system traffic control, wireless channel, Error correction, Automatic repeat request, telecommunication traffic, real-time traffic]
Subscription summarization: a new paradigm for efficient publish/subscribe systems
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
We contribute a new paradigm for publish/subscribe systems. It is centered on the novel notion of subscription summarization. We first present the summarization structures for a broker's subscriptions and accompanying algorithms, which operate on the summary structures to match incoming events to the brokers with relevant subscriptions and for the maintenance of subscriptions in the face of updates. Second, we present novel algorithms for efficiently propagating subscription summaries to brokers. Finally, we present a novel algorithm for the efficient distributed processing of incoming events, utilizing the propagated subscription summaries to route the events to brokers with matched subscriptions. We study the performance of our contributions, comparing them against a baseline approach and against corresponding techniques employed in a well-known event-based distributed system. Our results show the significant performance gains introduced for both the subscription propagation and distributed event processing tasks.
[event-based distributed system, Subscriptions, subscription summarization, Performance gain, distributed processing, Information retrieval, Data structures, Routing, broker subscriptions, information industry, Distributed computing, subscription propagation, distributed event processing tasks, Distributed processing, Computer displays, Computer architecture, data structures, FETs, publish-subscribe systems]
Robust emulations of shared memory in a crash-recovery model
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
A shared memory abstraction can be robustly emulated over an asynchronous message passing system where any process can fail by crashing and possibly recover (crash-recovery model), by having (a) the processes exchange messages to synchronize their read and write operations and (b) log key information on their local stable storage. This paper extends the existing atomicity consistency criterion defined for multiwriter/multireader shared memory in a crash-stop model, by providing two new criteria for the crash-recovery model. We introduce lower bounds on the log-complexity for each of the two corresponding types of robust shared memory emulations. We demonstrate that our lower bounds are tight by providing algorithms that match them. Besides being optimal, these algorithms have the same message and time complexity as their most efficient counterpart we know of in the crash-stop model.
[message passing, Laboratories, Read-write memory, robust shared memory emulations, Computer crashes, multireader shared memory, communication complexity, system recovery, synchronisation, Fault tolerance, asynchronous message passing system, message complexity, Message passing, Emulation, crash-recovery model, distributed shared memory systems, Robustness, Hardware, Workstations, Distributed algorithms]
Azim: direction based service using azimuth based position estimation
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Here, we propose a system named Azim that provides service based on both location and direction, which uses position estimation method based on azimuth data. In this system, a user's position is estimated by having the user point to and measure azimuths of several markers or objects whose positions are already known. Because the measurements are naturally associated with some degree of error, the user's position is calculated as a probability distribution. Since both the user's position and azimuth data are obtained in this method, both sources of data are used to realize more advanced services such as identifying the object pointed to by a user. The proposed system utilizes a wireless LAN for supporting these advanced services. Finally, a prototype system was implemented using a direction sensor that combines a magnetic compass and accelerometer, and we exemplify the usefulness of our approach through an experiment.
[Wireless LAN, probability, azimuth-based position estimation, Sensor systems, Probability distribution, direction-based service, direction sensor, Global Positioning System, Information science, Magnetic field measurement, Azimuth, Magnetic sensors, magnetic compass, Prototypes, accelerometer, Position measurement, wireless LAN]
A reliable mobile agent communication protocol
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
We first propose a generic framework for the design of mobile agent communication protocols. The framework uses a flexible and adaptive mailbox-based scheme that associates each mobile agent with a mailbox while allowing the decoupling between them. This flexible approach allows us to design a variety of protocols which can be made adaptive to specific applications. Based on the framework, we derive a new protocol which possesses good characteristics such as efficiency and adaptability. To improve reliability, we implement the protocol with a fault tolerant architecture that consists of two levels of message passing primitives. Simulation results show that our protocol can effectively handle both network and host failures while keeping the communication cost low.
[Protocols, message passing, host failure, Mobile communication, Reliability engineering, communication complexity, Distributed computing, network failure, Computer science, Fault tolerance, Design engineering, Message passing, transport protocols, Mobile agents, mobile agents, fault tolerant architecture, mobile agent communication protocol, fault tolerant computing, Telecommunication network reliability, mailbox-based scheme]
A new QoS point coordination function for multimedia wireless LANs
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
We propose a new MAC protocol, named Q-PCF (quality-of-service PCF), which provides QoS guarantees to real-time multimedia applications for WLANs. Specifically, Q-PCF has the following attractive features. First, it supports multiple priority levels and guarantees that high-priority stations always join the polling list earlier than low-priority stations. Second, it provides fast reservation scheme such that real-time stations can get on the polling list in bounded time. Third, it employs dynamic bandwidth allocation scheme to support CBR/VBR transportation and provide per-flow probabilistic performance assurances. Forth, it adopts the novel mobile-assisted admission control technique such that the access point can admit as many newly flows as possible, while not violating admitted flows' guarantees. Simulation results do confirm that Q-PCF achieves high goodput and low frame delay dropped rate. Last but not least, we believe that the Q-PCF protocol can be easily applied to the current IEEE 802.11 products without major modifications.
[Wireless LAN, IEEE 802.11 standard, Quality of service, access protocols, Delay, mobile-assisted admission control technique, VBR transportation, dynamic bandwidth allocation scheme, multiple priority levels, Bandwidth, IEEE standards, real-time multimedia applications, Local area networks, multimedia communication, Q-PCF protocol, WLAN, quality of service, Application software, Computer science, bandwidth allocation, Channel allocation, MAC protocol, Computer industry, Media Access Protocol, CBR transportation, wireless LAN]
Epidemic algorithms for reliable content-based publish-subscribe: an evaluation
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Distributed content-based publish-subscribe middleware is emerging as a promising answer to the demands of modern distributed computing. Nevertheless, currently available systems usually do not provide reliability guarantees. This hampers their use in dynamic and unreliable scenarios, notably including mobile ones. We evaluate the effectiveness of an approach based on epidemic algorithms. Three algorithms we originally proposed in [P. Costa et al., (2003)] are thoroughly compared and evaluated through simulation in challenging unreliable settings. The results show that our use of epidemic algorithms improves significantly event delivery, is scalable, and introduces only limited overhead.
[epidemic algorithms, Scalability, Peer to peer computing, content-based publish-subscribe, Subscriptions, Routing, Distributed computing, Middleware, distributed computing, Network servers, mobile computing, Network topology, Publish-subscribe, Dispatching, middleware, computational complexity]
A proximity-based dynamic path shortening scheme for ubiquitous ad hoc networks
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
This paper describes the design, implementation, and evaluation of a proximity-based dynamic path shortening scheme, called DPS. In DPS, active route paths adapt dynamically to node mobility based on the "local" link quality estimation at each own node, without exchanging periodic control packets such as Hello packets. Each node monitors its own local link quality only when receiving packets and estimates whether to enter the "proximity "of the neighbor node to shorten active paths in a distributed manner. Simulation results of DPS in several scenarios of various node mobility and traffic flows reveal that adding DPS to DSR and AODV (conventional prominent on-demand ad hoc routing protocols) significantly reduces the end-to-end packet latency up to 50-percent and also the number of routing packets up to 70-percent over DSR, particularly in heavy traffic cases. We also demonstrate the more simulation results obtained by using our two novel mobility models which generate realistic node mobility: random orientation mobility and random escape mobility models. Finally, simple performance experiments using DPS implementation on FreeBSD OS demonstrate that DPS shortens active routes in the order of milliseconds (about 5 ms).
[telecommunication links, Hello packets, random escape mobility models, Ad hoc networks, active route paths, random orientation mobility, periodic control packets, Distributed computing, mobile computing, ubiquitous ad hoc networks, local link quality estimation, on-demand ad hoc routing protocols, routing protocols, FreeBSD OS, proximity-based dynamic path shortening scheme, ad hoc networks, telecommunication traffic]
ACT: an adaptive CORBA template to support unanticipated adaptation
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
We propose an Adaptive CORBA Template (ACT), which enables run-time improvements to CORBA applications in response to unanticipated changes in either their functional requirements or their execution environments. ACT enhances CORBA applications by transparently weaving adaptive code into their object request brokers (ORBs) at run time. The woven code intercepts and adapts the requests, replies, and exceptions that pass through the ORBs. Specifically, ACT can be used to develop an object-oriented framework in any language that supports dynamic loading of code and can be applied to any CORBA ORB that supports portable interceptors. Moreover, ACT can be used to support interoperation among otherwise incompatible adaptive CORBA frameworks. To evaluate the performance and functionality of ACT, we implemented a prototype in Java. Our experimental results show that the overhead introduced by the ACT infrastructure is negligible, while the adaptations offered are highly flexible.
[Java, Quality of service, Application software, Middleware, Distributed computing, Adaptive coding, Computer science, Fault tolerance, Runtime, object request brokers, object-oriented framework, Adaptive CORBA Template, distributed object management, CORBA application, Software engineering, middleware]
Client clustering for traffic and location estimation
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Resource management mechanisms for large-scale, globally distributed network services need to assign groups of clients to servers according to network location and expected load generated by these clients. Current proposals address network location and traffic modeling separately. We develop a novel clustering technique that addresses both network proximity and traffic modeling. Our approach combines techniques from network-aware clustering, location inference, and spatial analysis. We conduct a large, measurement-based study to identify and evaluate Web traffic clusters. Our study links millions of Web transactions collected from two world-wide sporting event Websites, with millions of network delay measurements to thousands of Internet address clusters. Because our techniques are equally applicable to other traffic types, they are useful in a variety of wide-area distributed computing optimizations, and Internet modeling and simulation scenarios.
[workstation clusters, resource management, client-server systems, distributed network services, Computational modeling, location estimation, Telecommunication traffic, Proposals, Distributed computing, distributed computing, Network servers, resource allocation, client clustering, Traffic control, Internet modeling, Large-scale systems, Internet, Resource management, IP networks, Web traffic, telecommunication traffic]
Spatial programming using smart messages: design and implementation
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Spatial programming (SP) is a space-aware programming model for outdoor distributed embedded systems. Central to SP are the concepts of space and spatial reference, which provide applications with a virtual resource naming in networks of embedded systems. A network resource is referenced using its expected physical location and properties. Together with other SP features, such as reference consistency and access timeout, they help programmers cope with highly dynamic network configurations in a network-transparent fashion. We present the SP design and its implementation using smart messages, a lightweight software architecture similar to mobile agents, that we developed for networks of embedded systems. We also describe the implementation and evaluation of a simple SP application over a testbed consisting of HP iPAQs running Linux and equipped with 802.11 cards for wireless communication. The experimental results indicate that SP is a viable programming model for outdoor distributed computing.
[spatial programming, distributed processing, Distributed computing, distributed computing, wireless communication, software architecture, mobile communication, Linux, mobile agent, embedded systems, HP iPAQ, outdoor distributed embedded system, 802.11 card, smart message]
A service-curve model with loss and a multiplexing problem
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
We propose a new service model based on service curves, which has a loss aspect. In this model, instead of forcing all the packets to meet their deadlines assigned via a service curve, we allow some packets to be dropped. Specifically, the new model is based on guaranteeing at least a certain fraction of the all packets to meet their deadlines assigned via a service curve. The proposed model is composable. We find a necessary and sufficient condition to employ for an efficient connection admission control at a multiplexer to deliver the services according to the new model, via a graph-theoretic approach. A scheduling algorithm to deliver the services as specified by the new service model, at a multiplexer, also exists.
[telecommunication congestion control, multiplexing problem, packet switching, packet loss, connection admission control, quality of service, real-time communication, Distributed computing, graph-theoretic approach, processor scheduling, task sheduling, service-curve model, multiplexing, service guarantees]
Overlay multicast trees of minimal delay
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Overlay multicast (or application-level multicast) has become an increasingly popular alternative to IP-supported multicast. End nodes participating in overlay multicast can form a directed tree rooted at the source using existing unicast links. For each receiving node there is always only one incoming link. Very often, nodes can support no more than a fixed number of outgoing links due to bandwidth constraints. Here, we describe an algorithm for constructing a multicast tree with the objective of minimizing the maximum communication delay (i.e. the longest path in the tree), while satisfying degree constraints at nodes. We show that the algorithm is a constant-factor approximation algorithm. We further prove that the algorithm is asymptotically optimal if the communicating nodes can be mapped into Euclidean space such that the nodes are uniformly distributed in a convex region. We evaluate the performance of the algorithm using randomly generated configurations of up to 5,000,000 nodes.
[Peer to peer computing, trees (mathematics), Delay, Multicast algorithms, Tree graphs, Unicast, application-level multicast tree, directed graphs, Bandwidth, multicast communication, overlay multicast tree, Approximation algorithms, IP-supported multicast tree, directed tree, Internet, IP networks, Global communication]
End-to-end utilization control in distributed real-time systems
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
An increasing number of distributed real-time systems face the critical challenge of providing end-to-end quality of service (QoS) guarantees in open and unpredictable environments. In particular, such systems often need to guarantee the CPU utilization on multiple processors in order to achieve overload protection and meet end-to-end deadlines while task execution times are unpredictable. While the recently developed feedback control real-time scheduling algorithms have shown promise, they cannot handle the common end-to-end task model in distributed systems where each task is comprised of a chain of subtasks distributed on multiple processors. We present the end-to-end utilization control (EUCON) algorithm that features a distributed feedback loop that dynamically enforces desired CPU utilization bounds on multiple processors based on online performance measurements EUCON is based on a model predictive control approach that models the utilization control problem on a distributed platform as a multivariable constrained optimization problem. A multiinput-multioutput model predictive controller is designed based on a difference equation model that describes the dynamic behavior of distributed real-time systems. Both control theoretic analysis and simulations demonstrate that EUCON can provide robust utilization guarantees even when task execution times deviate from the estimation or vary significantly at run-time.
[Real time systems, Measurement, model predictive control, open systems, end-to-end utilization control, Quality of service, predictive controller, Predictive models, Control systems, Feedback control, processor scheduling, difference equations, distributed real-time system, optimisation, resource allocation, real-time scheduling algorithm, online performance measurement, embedded system, Protection, multiprocessing systems, Distributed feedback devices, difference equation model, multivariable constrained optimization problem, quality of service, distributed feedback loop, Scheduling algorithm, MIMO systems, CPU utilization, real-time systems, Distributed control, predictive control]
An analysis for differentiated services in IEEE 802.11 and IEEE 802.11e wireless LANs
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
We study backoff-based priority schemes for IEEE 802.11 and the emerging IEEE 802.11e standard by differentiating the minimum backoff window size, the backoff window-increasing factor, and the retransmission limit. An analytical model is proposed to derive saturation throughputs, saturation delays, and frame dropping probabilities of different priority classes for all proposed priority schemes. Simulations are conducted to validate analytical results. The proposed priority schemes can be easily implemented, and results are beneficial in designing good priority parameters.
[Wireless LAN, differentiated services, Ethernet networks, telecommunication congestion control, IEEE 802.11 standard, Quality of service, Throughput, Delay, Centralized control, Analytical models, backoff-based priority schemes, Media Access Protocol, IEEE standards, IEEE 802.11e standard, IP networks, wireless LAN, frame dropping probability, Local area networks, carrier sense multiple access]
sFlow: towards resource-efficient and agile service federation in service overlay networks
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Existing research work towards the composition of complex federated services has assumed that service requests and deliveries flow through a particular service path or tree. Here, we extend such a service model to a directed acyclic graph, allowing services to be delivered via parallel paths and interleaved with each other. Such an assumption of the service flow model has apparently introduced complexities towards the development of a distributed algorithm to federate existing services, as well as the provisioning of the required quality in the most resource-efficient fashion. To this end, we propose sFlow, a fully distributed algorithm to be executed on all service nodes, such that the federated service flow graph is resource efficient, performs well, and meets the demands of service consumers.
[federated service flow graph, Peer to peer computing, flow graphs, Transcoding, directed acyclic graph, quality of service, Flow graphs, Intelligent networks, distributed algorithm, Multicast algorithms, Tree graphs, service overlay network, distributed algorithms, telecommunication networks, Clustering algorithms, sFlow, Streaming media, agile service federation, Computer networks, Distributed algorithms]
SLIC: a selfish link-based incentive mechanism for unstructured peer-to-peer networks
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Most peer-to-peer (P2P) systems assume that all peers are cooperating for the benefit of the community. However in practice, there is a significant portion of peers who leech resources from the system without contributing any in return. We propose a simple selfish link-based incentive (SLIC) mechanism for unstructured P2P file sharing systems to create an incentive structure where in exchange for better service, peers are encouraged to share more data, give more capacity to handle other peers' queries, and establish more connections to improve the P2P overlay network. Our SLIC algorithm does not require nodes to be altruistic and does not rely on third parties to provide accurate information about other peers. We demonstrate, through simulation, that SLIC's locally selfish and greedy approach is sufficient for the system to evolve into a "good" state.
[Access control, Economics, unstructured peer-to-peer network, Peer to peer computing, computer networks, Control systems, Mechanical factors, Computer crime, Sun, Distributed computing, Filters, selfish link-based incentive mechanism, file sharing system, distributed algorithms]
A distributed implementation of sequential consistency with multi-object operations
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Sequential consistency is a consistency criterion for concurrent objects stating that the execution of a multiprocess program is correct if it could have been produced by executing the program on a mono-processor system, preserving the order of the operations of each individual process. Several protocols implementing sequential consistency on top of asynchronous distributed systems have been proposed. They assume that the processes access the shared objects through basic read and write operations. We consider the case where the processes can invoke multiobject operations which can read or write several objects in a single operation atomically. It proposes a particularly simple protocol that guarantees sequentially consistent executions in such a context. The previous sequential consistency protocols, in addition to considering only unary operations, assume either full replication or a central manager storing copies of all the objects. In contrast, the proposed protocol has the noteworthy feature that each object has a separate manager. Interestingly, this provides the protocol with a versatility dimension that allows deriving simple protocols providing sequential consistency or atomic consistency when each operation is on a single object.
[sequential consistency, message passing, multiprocessing programs, concurrent objects, Access protocols, data integrity, NP-complete problem, asynchronous distributed systems, Distributed computing, Computer science, Message passing, multiprocess program, Memory management, concurrency control, multiobject operations, shared memory systems, protocols, object manager, shared object memory abstraction]
Loop-free routing using a dense label set in wireless networks
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
We present a new class of on-demand routing protocols called split label routing (SLR). The protocols guarantee loop-freedom at every instant by ensuring that node labels are always in topological order, and thus induce a directed acyclic graph (DAG). The novel feature of SLR is that it uses a dense ordinal set with a strict partial order to label nodes. For any two labels there is always some label in between them. This allows SLR to "insert" a node in to an existing DAG, without the need to relabel predecessors. SLR inherently provides multiple paths to destinations. We present a practical, finitely dense implementation that uses a destination-controlled sequence number. The sequence number functions as a reset to node ordering when no more label splits are possible. The sequence number is changed only by the destination. Simulations show that our proposed protocol outperforms existing state-of-the-art on-demand routing protocols.
[Base stations, Portable computers, directed acyclic graph, loop-free routing, wireless networks, Ad hoc networks, split label routing, Relays, Mobile ad hoc networks, Land mobile radio, Intelligent networks, dense label set, Wireless networks, directed graphs, routing protocols, Radio network, on-demand routing protocols, Routing protocols, ad hoc networks]
Linguistic support for distributed programming abstractions
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
We contribute to addressing context of Java and the type-based publish/subscribe (TPS) abstraction, an object-oriented variant of the publish/subscribe paradigm. We present an experience that compares implementations of TPS in (1) a variant of Java we designed to inherently support TPS, (2) standard Java, and (3) Java augmented with genericity. We derive from our implementation experience general observations on what features a programming language should support in order to enable a satisfactory library implementation of TPS, and finally, also alternative abstractions. In particular, we (re-) insist here on the importance of providing genericity and reflective features in the language, and point out the very fact that current efforts towards providing such features are still insufficient.
[Java, object-oriented programming, Laboratories, type-based publish-subscribe abstraction, programming language linguistics, programming language semantics, Middleware, Sun, Programming profession, Computer languages, Microwave integrated circuits, remote procedure calls, Libraries, Safety, Dynamic programming, distributed programming abstraction, distributed programming, middleware]
PIC: practical Internet coordinates for distance estimation
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
We introduce PIC, a practical coordinate-based mechanism to estimate Internet network distance (i.e., round-trip delay or network hops). Network distance estimation is important in many applications; for example, network-aware overlay construction and server selection. There are several proposals for distance estimation in the Internet but they all suffer from problems that limit their benefit. Most rely on a small set of infrastructure nodes that are a single point of failure and limit scalability. Others use sets of peers to compute coordinates but these coordinates can be arbitrarily wrong if one of these peers is malicious. While it may be reasonable to secure a small set of infrastructure nodes, it is unreasonable to secure all peers. PIC addresses these problems: it does not rely on infrastructure nodes and it can compute accurate coordinates even when some peers are malicious. We present PIC's design, experimental evaluation, and an application to network-aware overlay construction and maintenance.
[telecommunication security, Economic indicators, Internet coordinates, Peer to peer computing, Scalability, Delay estimation, Proposals, network-aware overlay construction, Network servers, network distance estimation, network maintenance, Internet, IP networks, Probes, Web server]
Enhancing real-time CORBA via real-time Java features
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
End-to-end middleware predictability is essential to support quality of service (QoS) capabilities needed by distributed real-time and embedded (DRE) applications. Real-time CORBA is a middleware standard that allows DRE applications to allocate, schedule, and control the QoS of CPU, memory, and networking resources. Existing real-time CORBA solutions are implemented in C++, which is generally more complicated and error-prone to program than Java. The real-time specification for Java (RTSJ) provides extensions that enable Java to be used for developing DRE systems. Real-time CORBA does not currently leverage key RTSJ features, such as scoped memory and real-time threads. Thus, integration of real-time CORBA and RTSJ is essential to ensure the predictability required for Java-based DRE applications. We provide the following contributions to the study of middleware for DRE applications. First we analyze the architecture of ZEN, our implementation of real-time CORBA, identifying sources for the application of RTSJ features. Second, we describe how RTSJ features, such as scoped memory and real-time threads, can be associated with key ORB components to enhance the predictability of DRE applications using realtime CORBA and the RTSJ. Third, we perform preliminary qualitative and quantitative analysis of predictability enhancements arising from our application of RTSJ features. Our results show that use of RTSJ features can considerably improve the predictability of DRE applications written using Real-time CORBA and real-time Java.
[Real time systems, Java, real-time specification, distributed real-time embedded application, Quality of service, C++ language, quality of service, real-time Java, Application software, Middleware, Yarn, formal specification, Communication standards, Computer science, Processor scheduling, Memory management, embedded systems, qualitative analysis, quantitative analysis, distributed object management, real-time CORBA, middleware]
A study of BGP path vector route looping behavior
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Measurements have shown evidences of inter-domain packet forwarding loops in the Internet, but the exact cause of these loops remains unclear. As one of the efforts in identifying the causes, this paper examines how transient loops can be created at the inter-domain level via BGP, and what are the major factors that contribute to duration of the routing loops. As a path-vector routing protocol, BGP messages list the entire AS path to each destination and the path information enables each node to detect, thus break, arbitrarily long routing loops involving itself. However, delays due to physical constrains and protocol mechanisms slow down routing updates propagation and the routing information inconsistencies among the nodes lead to loop formation during convergence. We show that the duration of transient BGP loops match closely to BGP's routing convergence time and the looping duration is linearly proportional to BGP's minimum route advertisement interval timer (MRAI) value. We also examine four BGP routing convergence enhancements and show that two enhancements effective in speeding up routing convergence are also effective in reducing routing loops.
[Algorithm design and analysis, minimum route advertisement interval timer, packet switching, Telecommunication traffic, inter-domain packet forwarding loops, network topology, Convergence, Forward contracts, Intersymbol interference, routing protocols, Traffic control, Routing protocols, Internet, Velocity measurement, BGP path vector route, Propagation delay]
Group digital assistant: combined or shared PDA screen
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
We propose a new concept of a combined and a shared screen for new small devices called GDA (group digital assistant). In GDA, the screens of two PDAs can be treated just as one screen in combination view mode and can be shared in sharing view mode. GDA uses a wireless LAN and Bluetooth for radio communication. We have developed application software for an idea generation support system on GDA and experimented in cooperative work for evaluation. We found that functions of GDA were effective in actual cooperative work. The concept of GDA can permit usage of a cellular phone and a PDA in the next generation.
[Wireless LAN, Bluetooth, group digital assistant, Application software, PDA, Middleware, radio communication, Computer displays, Cellular phones, groupware, Collaborative work, Systems engineering and theory, cellular phone, notebook computers, wireless LAN, idea generation support system, Personal digital assistants, Radio communication, cellular radio]
Nash equilibria in parallel downloading with multiple clients
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Recently, the scheme of parallel downloading has been proposed as a novel approach to expedite the reception of a large file from the Internet. Experiments with a single client have shown that the client can improve its performance significantly by using the scheme. Simulations and experiments with multiple clients using the scheme have been conducted in [Gkantsidis, C et al., (2003), Koo, S et al., (2003)] to investigate the impact that this technique might have on the network if it is widely adopted. Contrast to the methodology used in [Gkantsidis, C et al., (2003), Koo, S et al., (2003)], we formulate parallel downloading as a noncooperative game. Within this framework, we present a characterization of the traffic configuration at Nash equilibrium in a general network, and analyze its properties in a specific network. We also establish the dynamic convergence to equilibrium from an initial nonequilibrium state for a specific network. Finally, we investigate the efficiency of Nash equilibrium from the point of view of the clients and the system respectively, i.e., downloading latencies perceived by individual clients and total latencies over all connections. We find that although the traffic configuration at Nash equilibrium is optimal from the point of view of the clients, it may be bad from the point of view of the system.
[Chaos, client-server systems, computer networks, Telecommunication traffic, game theory, Nash equilibrium, Encoding, traffic configuration, parallel processing, Delay, Convergence, Computer science, Employment, multiple clients view, parallel downloading, Internet, Web server, noncooperative game, telecommunication traffic]
Ensuring content and intention consistency in real-time group editors
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Real-time group editors allow distributed users to work on local replicas of a shared document simultaneously to achieve high responsiveness and free interaction. Operational transformation (OT) is the standard method for consistency maintenance in state-of-the-art group editors. It is potentially able to achieve content consistency (convergence) as well as intention consistency (so that the converged content is what the users intend), while traditional concurrency control methods such as locking and serialization often cannot. However, existing OT algorithms are often not able to really guarantee consistency due to important algorithmic flaws that have been there for fourteen years. We present a novel state difference based transformation (SDT) algorithm to solve the problem. Our result also reveals that the standard priority schemes to break ties in distributed systems should be used with more caution.
[transaction processing, Protocols, operational transformation, Collaborative software, Concurrency control, data integrity, Delay, real-time group editors, Convergence, Computer science, distributed users, content consistency, Collaboration, concurrency control, state difference based transformation, groupware, Collaborative work, Computer networks, intention consistency, IP networks]
PROP: a scalable and reliable P2P assisted proxy streaming system
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
The demand of delivering streaming media content in the Internet has become increasingly high for scientific, educational, and commercial applications. Three representative technologies have been developed for this purpose, each of which has its merits and serious limitations. Infrastructure-based CDNs with dedicated network bandwidths and powerful media replicas can provide high quality streaming services but at a high cost. Server-based proxies are cost-effective but not scalable due to the limited proxy capacity and its centralized control. Client-based P2P networks are scalable but do not guarantee high quality streaming service due to the transient nature of peers. To address these limitations, we present a novel and efficient design of a scalable and reliable media proxy system supported by P2P networks. This system is called PROP abbreviated from our technical theme of "collaborating and coordinating PROxy and its P2P clients". Our objective is to address both scalability and reliability issues of streaming media delivery in a cost-effective way. In the PROP system, the clients' machines in an intranet are self-organized into a structured P2P system to provide a large media storage and to actively participate in the streaming media delivery, where the proxy is also embedded as an important member to ensure quality of streaming service. The coordination and collaboration in the system are efficiently conducted by our P2P management structure and replacement policies. We have comparatively evaluated our system by trace-driven simulations with synthetic workloads and with a real-life workload trace extracted from the media server logs in an enterprise network. The results show that our design significantly improves the quality of media streaming and the system scalability.
[paged storage, Costs, Scalability, Quality of service, multimedia streaming system, replacement policy, Cache storage, cache storage, content management, P2P networks, Computer science, Network servers, resource allocation, media contents, Bandwidth, Streaming media, Internet, Web server, proxy caching approach]
Notification-based QoS control protocol for multimedia group communication in high-speed networks
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
In group communications, multiple processes first establish a group and then each process sends a message to multiple processes while receiving messages from multiple processes in the group. In addition, messages are required to be causally/totally delivered to each process. Due to the limited computation and communication resource, processes cannot send and receive as messages as the processes would like. We newly propose a notification-based data transmission procedure with two-phase slow start (TPSS) to efficiently exchange multimedia messages in a group so as to satisfy QoS requirement. In TPSS, the transmission rate of a process is increased by transmitting redundant data so that no data is lost even if some packets are lost.
[high-speed network, Peer to peer computing, multimedia group communication, Communication system control, Quality of service, Multicast protocols, notification-based QoS control protocol, quality of service, Multimedia communication, data transmission procedure, Centralized control, Intelligent networks, High-speed networks, Bandwidth, groupware, data communication, Data communication, protocols, multimedia communication]
Canon in G major: designing DHTs with hierarchical structure
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Distributed hash tables have been proposed as flat, nonhierarchical structures, in contrast to most scalable distributed systems of the past. We show how to construct hierarchical DHTs while retaining the homogeneity of load and functionality offered by flat designs. Our generic construction, Canon, offers the same routing state vs. routing hops trade-off provided by standard DHT designs. The advantages of Canon include (but are not limited to) (a) fault isolation, (b) efficient caching and effective bandwidth usage for multicast, (c) adaptation to the underlying physical network, (d) hierarchical storage of content, and (e) hierarchical access control. Canon can be applied to many different proposed DHTs to construct their Canonical versions. We show how four different DHTs - Chord, Symphony, CAN and Kademlia - can be converted into their Canonical versions that we call Crescendo, Cacophony, Can-Can and Kandy respectively.
[Access control, Kandy, Scalability, Merging, bandwidth usage, cache storage, Cacophony, Canon, File systems, Symphony, Bandwidth, distributed databases, Canonical version, data structures, Large-scale systems, Chord, Crescendo, Buildings, hierarchical structure, Routing, fault isolation, bandwidth allocation, hierarchical access control, distributed hash table, Kademlia, Internet, Artificial intelligence]
Stork: making data placement a first class citizen in the grid
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Todays scientific applications have huge data requirements which continue to increase drastically every year. These data are generally accessed by many users from all across the the globe. This implies a major necessity to move huge amounts of data around wide area networks to complete the computation cycle, which brings with it the problem of efficient and reliable data placement. The current approach to solve this problem of data placement is either doing it manually, or employing simple scripts which do not have any automation or fault tolerance capabilities. Our goal is to make data placement activities first class citizens in the Grid just like the computational jobs. They will be queued, scheduled, monitored, managed, and even check-pointed. More importantly, it will be made sure that they complete successfully and without any human interaction. We also believe that data placement jobs should be treated differently from computational jobs, since they may have different semantics and different characteristics. For this purpose, we have developed Stork, a scheduler for data placement activities in the grid.
[Wide area networks, Automation, wide area networks, fault tolerance, fault tolerance capability, grid computing, Collision mitigation, first class citizen, Biomedical measurements, Application software, Fault tolerance, Processor scheduling, automation, data placement, grid, Grid computing, Computer networks, Monitoring, wide area network]
Coordinating adaptations in distributed systems
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Distributed applications may use sophisticated runtime adaptation strategies to meet their performance or quality-of-service goals. Coordinating an adaptation that involves multiple processes can require complex communication or synchronization, in addition to communication in the base application. We propose conceptually simple high-level directives and a sophisticated runtime algorithm for coordinating adaptation automatically and transparently in distributed applications. The coordination directives specify when to adapt, in terms of the relative computational progress of each relevant process. The coordination algorithm relies on simple compiler transformations to track the progress of the processes, and performs the adaptive changes locally and asynchronously at each process. Measurements of the runtime overhead of the automatic coordination algorithm for two adaptive applications (a parallel PDE solver and a distributed video tracking code) show that the overhead is less than 1% of execution time for both these codes, even with relatively frequent adaptations, and does not grow significantly with the number of coordinating processes.
[Quality of service, parallel PDE solver, Time measurement, quality of service, compiler transformations, Middleware, Videoconference, program compilers, automatic coordination algorithm, synchronisation, Runtime, Program processors, distributed video tracking code, Processor scheduling, runtime algorithm, quality-of-service goals, Grid computing, Robustness, Computer networks, runtime adaptation strategies, coordination directives, middleware]
Dining philosophers with crash locality 1
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Ideally, distributed algorithms isolate the side-effects of faults within local neighborhoods of impact. Failure locality quantifies this concept as the maximum radius of impact caused by a given fault. We present new locality results for the dining philosophers problem subject to crash failures. The optimal crash locality for dining is 0 in synchronous networks, but degrades to 2 in asynchronous networks. Using the eventually-perfect failure detector /spl diams/P , we construct the first known dining algorithms with crash locality 1 under partial synchrony. These algorithms close the failure-locality complexity gap and improve the crash tolerance of resource allocation algorithms in practical networks. We prove the optimality of our results with two fundamental theorems. First, no dining solution using /spl diams/P achieves locality 0. Second, /spl diams/P is the weakest failure detector in the Chandra-Toueg hierarchy to realize locality 1.
[fault tolerance, crash locality result, eventually-perfect failure detector, Computer crashes, Topology, dining philosophers problem, failure-locality complexity gap, Degradation, synchronous networks, resource allocation, Fault tolerant systems, distributed algorithms, resource allocation algorithms, Detectors, Timing, dining algorithms, Resource management, asynchronous networks, Distributed algorithms, Chandra-Toueg hierarchy]
Firewall design: consistency, completeness, and compactness
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
A firewall is often placed at the entrance of each private network in the Internet. The function of a firewall is to examine each packet that passes through the entrance and decide whether to accept the packet and allow it to proceed or to discard the packet. A firewall is usually designed as a sequence of rules. To make a decision concerning some packets, the firewall rules are compared, one by one, with the packet until one rule is found to be satisfied by the packet: this rule determines the fate of the packet. We present the first ever method for designing the sequence of rules in a firewall to be consistent, complete, and compact. Consistency means that the rules are ordered correctly, completeness means that every packet satisfies at least one rule in the firewall, and compactness means that the firewall has no redundant rules. Our method starts by designing a firewall decision diagram (FDD, for short) whose consistency and completeness can be checked systematically (by an algorithm). We then apply a sequence of five algorithms to this FDD to generate, reduce and simplify the target firewall rules while maintaining the consistency and completeness of the original FDD.
[Algorithm design and analysis, Transport protocols, telecommunication security, Design methodology, decision diagrams, packet switching, data integrity, Electronic mail, Distributed computing, firewall decision diagram, Network servers, firewall, authorisation, Computer networks, FDD completeness, Internet, IP networks, Web server, FDD consistency]
Certificate dispersal in ad-hoc networks
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
We investigate how to disperse the certificates, issued in an ad-hoc network, among the network nodes such that the following condition holds. If any node u approaches any other node v in the network, then u can use the certificates stored either in u or in v to obtain the public key of v (so that u can securely send messages to v). We define the cost of certificate dispersal as the average number of certificates stored in one node in the network. We give upper and lower bounds on the dispersability cost of certificates, and show that both bounds are tight. We also present two certificate dispersal algorithms, and show that one of those algorithms is more efficient than the other in several important cases. Finally, we identify a rich class of "certificate graphs" for which the dispersability cost is within a constant factor from the lower bound.
[telecommunication security, Costs, graph theory, Ad hoc networks, Electronic mail, Distributed computing, network nodes, Intelligent networks, ad-hoc networks, certificate dispersal algorithms, public key cryptography, Public key, message authentication, Computer networks, certificate dispersal, Cryptography, ad hoc networks]
Energy-efficient synthesis of periodic task systems upon identical multiprocessor platforms
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Multiprocessor implementations of real-time systems tend to be more energy-efficient than uniprocessor implementations. However several factors, including the nonexistence of optimal multiprocessor scheduling algorithms, combine to prevent all the computing capacity of a multiprocessor platform from being guaranteed available for executing the real-time workload. In this paper, this tradeoff - that while increasing the number of processors results in lower energy consumption for a given computing capacity, the fraction of the capacity of a multiprocessor platform that is guaranteed available for executing real-time work decreases as the number of processors increases - is explored in detail. Algorithms are presented for synthesizing multiprocessor implementations of hard-real-time systems comprised of independent periodic tasks in such a manner that the energy consumed by the synthesized system is minimized.
[multiprocessing systems, real-time systems, optimal multiprocessor scheduling algorithms, periodic task systems, Energy efficiency, Distributed computing, processor scheduling]
DisCo: middleware for securely deploying decomposable services in partly trusted environments
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
The DisCo middleware infrastructure facilitates the construction and deployment of decomposable applications for environments with dynamic network connectivity properties and unstable trust relationships spanning multiple administrative domains. Consumers of these services, who are mutually anonymous, must be able to discover, securely acquire the code for, and install service components over the network with only minimal a priori knowledge of their locations. Once installed, these components must be able to intemperate securely and reliably across the network. Solutions exist that address individual challenges posed by such an environment, but they rely upon mutually incompatible authorization models that are frequently insufficiently expressive. The primary contributions of DisCo are (1) a middleware toolkit for constructing such applications, (2) a unifying authorization abstraction, and (3) a realization of this authorization well suited for expressing partial trust relationships typical of such environments. We focus on the first two of these contributions, [E. Freudenthal et al., (2002)] presents the third.
[Access control, telecommunication security, authorization model, partly trusted environment, decomposable service deployment, Application software, Communication system security, DisCo middleware, Middleware, Authorization, Intelligent networks, Authentication, dynamic network connectivity property, Communication channels, authorisation, multiple administrative domain, Biomembranes, Protection, middleware]
iPass: an incentive compatible auction scheme to enable packet forwarding service in MANET
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
In a public mobile ad hoc network (MANET), users may be selfish and refuse to forward packets for other users. Therefore, an incentive mechanism must be in place. We adopt the "pay for service" model of cooperation, and propose an auction-based incentive scheme (called iPass) to enable cooperative packet forwarding behavior in MANET. Each flow pays the market price of packet forwarding service to the intermediate routers. The resource allocation mechanism in our scheme is based on the generalized Vickrey auction with reserve pricing. We prove that in our scheme, user's truthful bidding of utility remains a dominant strategy, users and routers have incentive to participate in the scheme, and packet forwarding always leads to higher social welfare for the whole network. We design a signaling protocol to implement the scheme, and show that it can serve as an explicit rate-based flow control mechanism for the network. Therefore, iPass is a joint solution of incentive engineering and flow control in a noncooperative MANET. Simulation results show that iPass is able to determine the auction outcome quickly, and at the same time achieve the goals of flow control.
[Military computing, telecommunication congestion control, Humans, packet switching, Mobile ad hoc networks, Incentive schemes, Intelligent networks, resource allocation, Forward contracts, reserve pricing, Bandwidth, Vickrey auction, iPass scheme, mobile ad hoc network, auction-based incentive scheme, Computer science, Waste materials, rate-based flow control, telecommunication network routing, Resource management, ad hoc networks, wireless LAN, intermediate router, signaling protocol, packet forwarding service]
Evaluating GUESS and non-forwarding peer-to-peer search
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Current search techniques over unstructured peer-to-peer networks rely on intelligent forwarding-based techniques to propagate queries to other peers in the network. Forwarding techniques are attractive because they typically require little state and offer robustness to peer failures; however they have inherent performance drawbacks due to the overhead of forwarding and lack of central control. We study GUESS, a nonforwarding search mechanism, as a viable alternative to currently popular forwarding-based mechanisms. We show how non-forwarding mechanisms can be over an order of magnitude more efficient than forwarding mechanisms; however, they must be deployed with care, as a naive implementation can result in highly suboptimal performance, and make them susceptible to hotspots and misbehaving peers.
[Protocols, Costs, unstructured peer-to-peer network, Peer to peer computing, computer networks, File servers, cache storage, Centralized control, Computer science, Robust control, Intelligent networks, Network servers, distributed algorithms, nonforwarding peer-to-peer search, Bandwidth, GUESS protocol evaluation, protocols, query formulation]
Uncheatable grid computing
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Grid computing is a type of distributed computing that has shown promising applications in many fields. A great concern in grid computing is the cheating problem described in the following: a participant is given D = {x/sub 1/,...,x/sub n/}, it needs to compute f(x) for all x/spl isin/D and return the results of interest to the supervisor. How does the supervisor efficiently ensure that the participant has computed f(x) for all the inputs in D, rather than a subset of it? If participants get paid for conducting the task, there are incentives for cheating. We propose a novel scheme to achieve the uncheatable grid computing. Our scheme uses a sampling technique and the Merkle-tree based commitment technique to achieve efficient and viable uncheatable grid computing.
[Merkle-tree based commitment technique, Costs, sampling methods, grid computing, Microcomputers, cheating problem, Supercomputers, Application software, Distributed computing, distributed computing, uncheatable grid computing, Grid computing, Computer industry, Sampling methods, sampling technique, Internet, Workstations]
A distributed approach to topology-aware overlay path monitoring
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Path probing is essential to maintain an efficient overlay network topology. However, the cost of complete probing can be as high as O(n/sup 2/), which is prohibitive in large-scale overlay networks. Recently we proposed a method that trades probing overhead for inference accuracy in sparse networks such as the Internet. The method uses physical path information to infer path quality for all of the n/spl times/(n-1) overlay paths, while actually probing only a subset of the paths. We propose and evaluate a distributed approach to implement this method. We describe a minimum diameter, link-stress bounded overlay spanning tree, which is used to collect and disseminate path quality information. All nodes in the tree collaborate to infer the quality of all paths. Simulation results show this approach can achieve a high-level of inference accuracy while reducing probing overhead and balancing link stress on the spanning tree.
[Peer to peer computing, Computerized monitoring, information dissemination, Routing, network topology, tree searching, Stress, Delay, sparse network, path quality information dissemination, Network topology, link-stress bounded overlay spanning tree, path probing, Bandwidth, Internet, IP networks, Probes, Software engineering, topology-aware overlay path monitoring]
A distributed formation of a virtual backbone in MANETs using adjustable transmission ranges
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Recently, the use of a virtual backbone (connected dominating set) in various applications in mobile ad hoc networks (MANETs) has become popular. These applications include topology management, point and area coverage, and routing protocol design. In a mobile environment such as a MANET, one challenging issue in constructing a virtual backbone is to accomplish a distributed and localized solution that aims at balancing several contradicting objectives: small approximation ratio, fast convergence, and low computation cost. Many existing distributed and localized algorithms select a virtual backbone without resorting to global or geographical information. However, these algorithms incur a high computation cost in a dense network. In this paper, we propose a distributed solution based on reducing the density of the network using two mechanisms: clustering and adjustable transmission range. By using adjustable transmission range, we also achieve another objective, energy-efficient design, as a by-product. As an application, we show an efficient broadcasting where nodes (and only nodes) in a virtual backbone are used to forward the broadcast message. The virtual backbone is constructed using Wu and Li's marking process [J. Wu et al., (1999)] and the proposed density reduction process. The application of the density reduction process to other localized algorithms is also discussed. The efficiency of our approach is confirmed through both analytic and simulation study.
[workstation clusters, Spine, Distributed computing, Mobile ad hoc networks, virtual backbone, mobile computing, Network topology, Clustering algorithms, mobile ad hoc networks, connected dominating set, Broadcasting, Routing protocols, Computer networks, Computational efficiency, routing protocol design, message passing, broadcast message, distributed algorithm, MANET, distributed algorithms, routing protocols, adjustable transmission range, ad hoc networks, Mobile computing, computational complexity]
DERMI: a decentralized peer-to-peer event-based object middleware
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
We present DERMI, a decentralized wide-area event-based object middleware built on top of a peer-to-peer substrate. Its main building block is the underlying publish/subscribe event notification system provided by the peer-to-peer layer. By using this methodology, innovative benefits like distributed interception, high performance synchronous/asynchronous one-to-one/one-to-many notifications and decentralized object location services are provided. Moreover, new programming abstractions (anycall and manycall) are introduced, which allow the programmer to make calls to groups of objects without taking care of which of them responds until a determinate condition is met. We believe that such middleware is a solid building block for future wide-area distributed component infrastructures.
[Availability, object-oriented programming, message passing, decentralized peer-to-peer event-based object middleware, wide area networks, subscribe event notification system, Peer to peer computing, Object oriented modeling, decentralized object location service, Dermis, Application software, Programming profession, Home computing, Message-oriented middleware, publish event notification, Bandwidth, remote procedure calls, Computer networks, object oriented programming, remote procedure call, middleware]
Exploiting congestion information in network and higher layer protocols in multihop wireless ad hoc networks
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
With most routing protocols for ad hoc networks, shorter paths are generally considered more desirable, making some areas of network more prone to congestion and decreasing overall network throughput. We examine the use of congestion information to avoid these network hotspots. By locally monitoring the network interface transmission queue length and MAC layer behavior at each node, a node can establish an approximation of the degree to which the wireless medium around it is busy; this measurement reflects not only the behavior of the node itself, but also the behavior of other nearby nodes sharing the wireless medium. We suggest a number of uses of such congestion information in an ad hoc network, in the network, transport, and higher layers, and we evaluate a set of such uses through simulation. Our results based on modifications to the dynamic source routing protocol (DSR) and TCP demonstrate substantial performance improvement in terms of scalability, packet delivery, overhead, and fairness resulting from this use of congestion information.
[TCP, telecommunication congestion control, Wireless application protocol, Scalability, congestion information, ad hoc network, MAC layer behavior, Length measurement, routing protocol, Throughput, dynamic source routing protocol, Ad hoc networks, access protocols, Network interfaces, Mobile ad hoc networks, network interface transmission queue length, transport protocols, multihop wireless network, routing protocols, Spread spectrum communication, Routing protocols, ad hoc networks, Monitoring, network hotspot]
ULC: a file block placement and replacement protocol to effectively exploit hierarchical locality in multi-level buffer caches
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
In a large client/server cluster system, file blocks are cached in a multilevel storage hierarchy. Existing file block placement and replacement are either conducted on each level of the hierarchy independently, or by applying an LRU policy on more than one levels. One major limitation of these schemes is that hierarchical locality of file blocks with nonuniform strengths is ignored, resulting in many unnecessary block misses, or additional communication overhead. To address this issue, we propose a client-directed, coordinated file block placement and replacement protocol, where the nonuniform strengths of locality are dynamically identified on the client level to direct servers on placing or replacing file blocks accordingly on different levels of the buffer caches. In other words, the caching layout of the blocks in the hierarchy dynamically matches the locality of block accesses. The effectiveness of our proposed protocol comes from achieving the following three goals: (1) The multilevel cache retains the same hit rate as that of a single level cache whose size equals to the aggregate size of multilevel caches. (2) The nonuniform locality strengths of blocks are fully exploited and ranked to fit into the physical multilevel caches. (3) The communication overheads between caches are also reduced.
[client-server systems, file block replacement protocol, Buffer storage, client/server cluster system, multilevel buffer cache, Access protocols, File servers, Educational institutions, Cache storage, cache storage, block caching layout, file block placement protocol, Degradation, Computer science, LRU policy, Aggregates, storage hierarchy, Hard disks, Personal communication networks, protocols]
On the confidential auditing of distributed computing systems
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
We propose a confidential logging and auditing service for distributed information systems. We propose a cluster-based TTP (trusted third party) architecture for the event log auditing services, so that no single TTP node can have the full knowledge of the logs, and thus no single node can misuse the log information without being detected. On the basis of a relaxed form of secure distributed computing paradigms, one can implement confidential auditing service so that the auditor can retrieve certain aggregated system information, e.g. the number of transactions, the total volume, the event traces, etc., without having to access the full log data. Similar to the peer relationship of routers to provide global network routing services, the mutually supported, mutually monitored cluster TTP architecture allows independent systems to collaborate in network-wide auditing without compromising their private information.
[network-wide auditing, Protocols, Event detection, open systems, Peer to peer computing, cluster-based trusted third party architecture, Distributed information systems, distributed processing, Information retrieval, cryptography, Distributed computing, Computer science, distributed computing systems, Privacy, confidential logging, Intrusion detection, telecommunication network routing, message authentication, Computer architecture, confidential auditing, data privacy, distributed information systems, network routing services]
A server side caching scheme for CORBA
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
We propose a caching scheme for saving the results of invocations of read methods at server side. Server side caching helps in reducing load on frequently accessed servers when computing the result is relatively costly. Our model provides strong consistency by using a dependency graph that specifies dependence information among method invocations of an object. We have implemented this scheme as a service in CORBA.
[client-server systems, frequently accessed servers, Telecommunication traffic, server side caching scheme, File servers, cache storage, dependency graph, Delay, Computer science, CORBA, Network servers, File systems, method invocations, Operating systems, remote procedure calls, Hardware, Web sites, Web server, distributed object management]
Analyzing the secure overlay services architecture under intelligent DDoS attacks
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Distributed denial of service (DDoS) attacks are currently major threats to communication in the Internet. A secure overlay services (SOS) architecture has been proposed to provide reliable communication between clients and a target under DDoS attacks. The SOS architecture employs a set of overlay nodes arranged in three hierarchical layers that controls access to the target. Although the architecture is novel and works well under simple congestion based attacks, we observe that it is vulnerable under more intelligent attacks. We generalize the SOS architecture by introducing more flexibility in layering to the original architecture. We define two intelligent DDoS attack models and develop an analytical approach to study the impacts of the number of layers, number of neighbors per node and the node distribution per layer on the system performance under these two attack models. Our data clearly demonstrate that performance is indeed sensitive to the design features and the different design features interact with each other to impact overall system performance.
[Availability, telecommunication security, telecommunication services, Communication system control, secure overlay services architecture, Communication system security, Computer crime, Resilience, Computer science, security of data, System performance, Web and internet services, Computer architecture, Performance analysis, Internet, intelligent distributed denial of service attacks]
Distributed network monitoring for evolving IP networks
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Evolving monitoring infrastructure in response to network growth is a critical aspect of network management. Previous work in network management primarily focused on optimizing monitoring systems for static networks. In this paper, we address the problem of optimally upgrading the existing monitoring infrastructure as the network evolves. The problem formulation presented here captures the trade off between adding new monitoring resources vs. disrupting the existing infrastructure. We show that this problem is NP hard and not approximable within a factor better than n/sup /spl epsi// in the general case and no better than log(n) when only shortest paths are considered. We develop a heuristic algorithm and evaluate its performance using simulated network evolution scenarios. We show that in spite of not allowing poller relocation, our adaptive online algorithm has comparable performance to that of the offline algorithm where such constraint does not exist.
[Availability, Costs, Protocols, Scalability, Heuristic algorithms, distributed network monitoring, Quality of service, network polling architecture, communication complexity, network management, Condition monitoring, Computer science, heuristic algorithm, computer network management, Bandwidth, NP hard problem, IP networks, protocols, minimisation]
Fault-tolerant data delivery for multicast overlay networks
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Overlay networks represent an emerging technology for rapid deployment of novel network services and applications. However, since public overlay networks are built out of loosely coupled end-hosts, individual nodes are less trustworthy than Internet routers in carrying out the data forwarding function. Here we describe a set of mechanisms designed to detect and repair errors in the data stream. Utilizing the highly redundant connectivity in overlay networks, our design splits each data stream to multiple sub-streams which are delivered over disjoint paths. Each sub-stream carries additional information that enables receivers to detect damaged or lost packets. Furthermore, each node can verify the validity of data by periodically exchanging Bloom filters, the digests of recently received packets, with other nodes in the overlay. We have evaluated our design through both simulations and experiments over a network testbed. The results show that most nodes can effectively detect corrupted data streams even in the presence of multiple tampering nodes.
[Protocols, computer networks, data forwarding, Application software, Internet router, Computer science, Fault tolerance, Filters, multicast overlay network, Fault detection, multicast communication, Streaming media, fault-tolerant data delivery, Robustness, fault tolerant computing, IP networks, Testing]
Location management &amp; message delivery protocol in multi-region mobile agent computing environment
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Location management and message delivery protocol is fundamental to the further development of mobile agent systems in a multiregion mobile agent computing environment in order to control mobile agents and guarantee message delivery between them. However, previous works have some problems when they are applied to a multiregion mobile agent computing environment. First, the cost of location management and message delivery is increased relatively. Second, a following problem arises. Finally, cloned mobile agents and parent & child mobile agents don't get dealt with respect to location management and message delivery. We present a HB (home-blackboard) protocol which is a new location management and message delivery protocol for mobile agents in a multiregion mobile agent computing environment. We have implemented the HB protocol. The HB protocol decreases the cost of location management and message delivery and solves the following problem with low communication cost. In addition, the HB protocol deals with the location management and message delivery of cloned and parent & child mobile agents, so that it guarantees message delivery of these mobile agents.
[Protocols, Costs, message passing, location management, home-blackboard protocol, Control systems, Mobile communication, message delivery protocol, multiregion mobile agent computing, communication complexity, Environmental management, Distributed computing, Home computing, Computer science, mobile computing, Engineering management, Mobile agents, mobile agents, protocols]
Adaptive plausible clocks
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Having small-sized logical clocks with high causal-ordering accuracy is useful, especially where (i) the precision of the knowledge of the causal dependencies among events implies savings in time overhead and (ii) the cost of transmitting full vector clock timestamps - that precisely characterise the causal relation - is high. Plausible clocks can be used as timestamps to order events in a distributed system in a way that is consistent with the causal order as long as the events are causally dependent. We introduce the nonuniformly mapped R-entries vector (NUREV) clocks, a general class of plausible clocks that allow accuracy adaptation and we analyse the ways that these clocks may relate causally independent event pairs. Our analysis resulted in a set of conclusions and the formulation of new, adaptive plausible clocks algorithms, with improved accuracy, even when the number of clock entries is very small, which is important in peer-to-peer communication systems.
[clocks, logical clock timestamps, distributed algorithms, adaptive plausible clock, computer networks, distributed system, nonuniformly mapped R-entries vector clock, Distributed computing, peer-to-peer communication system, Clocks, NUREV]
Modeling and analysis of 2D service differentiation on e-commerce servers
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
A scalable e-commerce server should be able to provide different levels of quality of service (QoS) to different types of requests according to clients' navigation patterns and the server capacity. In this paper, we propose a two-dimensional (2D) service differentiation (DiffServ) model for online transactions: inter-session and intra-session. The inter-session model aims to provide different levels of QoS to sessions from different customer classes, and the intra-session model aims to provide different levels of QoS to requests in different states of a session. We introduce service slowdown as a QoS metric of e-commerce servers. It is defined as the weighted sum of request slowdown in different sessions and in different session states. We formulate the problem of 2D DiffServ provisioning as an optimization of processing rate allocation with the objective of minimizing service slowdown. We derive the optimal allocations for an M/G/1 server under various server load conditions and prove that the optimal allocations guarantees requests' slowdown to be square-root proportional to their pre-specified differentiation weights in both dimensions. We evaluate the optimal allocation scheme via extensive simulations and compare it with a tailored proportional DiffServ scheme. Simulation results validate that both allocation schemes can achieve predictable, controllable, and fair 2D slowdown differentiation on e-commerce servers. The optimal allocation scheme guarantees 2D DiffServ at a minimum cost of service slowdown.
[2D service differentiation, client-server systems, queueing theory, Navigation, Quality of service, Predictive models, server capacity, quality of service, Electronic commerce, online transactions, Delay, Computer science, Network servers, Diffserv networks, resource allocation, QoS, DiffServ model, Cost function, optimal processing rate allocation, Internet, Springs, electronic commerce, e-commerce servers]
Multi-layer active queue management and congestion control for scalable video streaming
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Video streaming is becoming an increasingly important part of the present Internet. To guarantee a high-quality streaming environment to end users, many video applications require a strict form of network QoS that is not available in the present Internet. Thus, to supplement the best-effort model of existing networks, we study a new video streaming framework that allows applications to mark their own packets with different priority and use multiqueue congestion control inside routers to effectively drop the less-important packets during buffer overflows. We describe priority AQM algorithms that provide "optimal" performance to video applications under arbitrary network loss and study a variation of Kelly's congestion control in combination with our framework. We call the combined architecture PELS - partitioned enhancement layer streaming.
[Video coding, queueing theory, telecommunication congestion control, Quality of service, Decoding, Partitioning algorithms, quality of service, multilayer active queue management, multiqueue congestion control, video servers, Diffserv networks, Streaming media, Performance loss, video streaming, Internet, Buffer overflow, IP networks, video signal processing, partitioned enhancement layer streaming, network QoS]
/spl Delta/-reliable broadcast: a probabilistic measure of broadcast reliabillity
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
We introduce a new probabilistic specification of reliable broadcast communication primitives, called /spl Delta/ - reliable broadcast. This specification captures in a precise way the reliability of practical broadcast algorithms that, on the one hand, were devised with some form of reliability in mind but, on the other hand, are not considered reliable according to "traditional" reliability specifications. We illustrate the use of our specification by precisely measuring and comparing the reliability of two popular broadcast algorithms, namely bimodal multicast and IP multicast. In particular, we quantify how the reliability of each algorithm scales with the size of the system.
[Transport protocols, Peer to peer computing, Scalability, Laboratories, probability, distributed processing, Multicast protocols, bimodal multicast algorithm, Sun, Stress, /spl Delta/-reliable broadcast, Multicast algorithms, probabilistic specification, IP multicast algorithm, multicast protocols, Broadcasting, broadcast algorithm, Large-scale systems, computer network reliability, IP networks, reliable broadcast communication primitive]
An incentive mechanism for P2P networks
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
The current peer-to-peer (P2P) information sharing paradigm does not provide incentive and service differentiation for users. Since there is no motivation to share information or resources, this leads to the "free-riding" and the "tragedy of the commons" problems. We address how one can incorporate incentive into the P2P information sharing paradigm so as to encourage users to share information and resources. Our mechanism (or protocol) provides service differentiation to users with different contribution values and connection types. The mechanism also has some desirable properties: (1) conservation of cumulative contribution and social utility in the P2P community, (2) maximization of social utility if all requesting clients have the same contribution value, and (3) incentive-based resource distribution. The resource distribution algorithm and the contribution update algorithm are computationally efficient and can be easily implemented. Experimental results illustrate the efficiency and fairness of our algorithms.
[Protocols, Peer to peer computing, incentive-based resource distribution, cumulative contribution conservation, distributed processing, Scheduling, Mechanical factors, free-riding problem, Distributed computing, service differentiation, peer-to-peer information sharing, Computer science, resource allocation, Aggregates, contribution update algorithm, Bandwidth, Resource management, IP networks, protocols, social utility maximization]
Extended consistent hashing: an efficient framework for object location
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Content caching and location are key enabling technologies for achieving the high throughput needed to sustain current Internet infrastructure, both for peer-to-peer as well as client-server applications. An important aspect of distributed caching techniques is the mapping of data and requests to maximize system throughput while minimizing costs in the presence of network and cache failures. We describe a new cache protocol based on consistent hashing (CH) [D. Karger et al., (1997), (1999)]. Compared to consistent hashing, our protocol, called extended consistent hashing (ECH), can handle flash access to objects significantly better and yields better worst-case response times and lower load variance. Due to multiplicity of client views in a distributed hashing scheme, a single object (or its reference) may be cached at multiple locations. This is referred to as the spread of an object. Consistent hashing maps a request to a cache irrespective of the spread of the requested object. ECH, on the other hand, estimates the spread of an object and randomizes requests over expected spread. In doing so, it amortizes requests over a larger number of caches. While the expected load on target caches in ECH remains the same as consistent hashing (asymptotically optimal), load variance is significantly reduced. We present analytical results as well as simulations to demonstrate significant improvements for querying frequently accessed objects, up to 80% in worst-case response time and 30% in variance of server/target cache loads. We also show excellent correlation between expected and observed results. What makes ECH particularly attractive is that it can be integrated into existing infrastructure based on consistent hashing with minimal software overhead.
[client-server systems, object location, load balancing, cache storage, data integrity, content caching, Distributed computing, resource allocation, extended consistent hashing protocol, distributed databases, Internet, protocols, distributed caching, client-server system]
Efficient Web services response caching by selecting optimal data representation
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
We discuss the design for an efficient response cache mechanism appropriate for the Web services architecture. The important feature of Web services is its interoperability between heterogeneous platforms. This interoperability is based on widely accepted standards such as XML, SOAP, and WSDL. We describe a response cache mechanism for Web services client middleware without any extensions to these standards so that the client can participate transparently in the existing Web services community. We propose three optimization methods to improve the performance of our response cache. The first optimization is caching the post-parsing representation instead of the XML message itself. The second is caching application objects. For this optimization, we show some copying processes that are dependent on the type of cached objects. The third optimization is for read-only objects. These methods reduce the overhead of XML processing or object copying. We have implemented a prototype of a response cache on Apache-Axis, and evaluated these optimization methods through experiments for Google Web services. Finally, based on the experimental results, we discuss the optimal configuration of these methods based on data types.
[open systems, Laboratories, Optimization methods, Throughput, cache storage, Simple object access protocol, Distributed computing, optimisation, Google Web, optimization, data structures, post-parsing representation, middleware, message passing, Service oriented architecture, performance evaluation, optimal data representation, interoperability, Middleware, client middleware, Web services response caching, Web services, XML, read-only object, Internet, Portals, application object caching]
Epidemic-style proactive aggregation in large overlay networks
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Aggregation - that is, the computation of global properties like average or maximal load, or the number of nodes - is an important basic functionality in fully distributed environments. In many cases - which include protocols responsible for self-organization in large-scale systems and collaborative environments - it is useful if all nodes know the value of some aggregates continuously. We present and analyze novel protocols capable of providing this service. The proposed antientropy aggregation protocols compute different aggregates of component properties like extremal values, average and counting. Our protocols are inspired by the antientropy epidemic protocol where random pairs of databases periodically resolve their differences. In the case of aggregation, resolving difference is generalized to an arbitrary (numeric) computation based on the states of the two communicating peers. The advantage of this approach is that it is proactive and "democratic\
[Protocols, overlay network, aggregation, distributed environment, numeric computation, Distributed computing, large-scale system, Centralized control, Intelligent networks, Databases, Large-scale systems, protocols, collaborative environment, databases, Peer to peer computing, Decision making, computer networks, epidemic-style proactive aggregation protocol, distributed decision making, network topology, large-scale systems, Aggregates, collective decision making, distributed algorithms, Collaboration, automatic system maintenance]
SRB: shared running buffers in proxy to exploit memory locality of multiple streaming media sessions
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
With the falling price of the memory, an increasing number of multimedia servers and proxies are now equipped with a large DRAM memory space. Caching media objects in the memory of a proxy helps to reduce network traffic, disk I/O bandwidth requirement, and data delivery latency. The running buffer approach and its alternatives are representative techniques to cache streaming data in the memory. However, there are two limits in the existing techniques. First, although multiple running buffers for the same media object co-exist in a given processing period, data sharing among the multiple buffers is not considered. Second, user access patterns are not insightfully considered in the buffer management. In this paper, we propose two techniques based on shared running buffers (SRB) in the proxy to address these limits. Considering user access patterns and characteristics of the requested media objects, our techniques adoptively allocate memory buffers to fully utilize the currently buffered data of streaming sessions, with the aim to reduce both the server load and the network traffic. Experimentally comparing with several existing techniques, we show that the proposed techniques have achieved significant performance improvement by effectively using the shared running buffers.
[multimedia servers, DRAM memory space, memory buffers, data delivery latency, Multimedia systems, media objects, Laboratories, Telecommunication traffic, Educational institutions, cache storage, Delay, content management, Computer science, bandwidth allocation, network traffic, Network servers, shared running buffers, cache streaming, Bandwidth, Streaming media, Internet, bandwidth requirement, Mobile computing, telecommunication traffic]
CLASH: a protocol for Internet-scale utility-oriented distributed computing
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Distributed hash table (DHT) overlay networks offer an efficient and robust technique for wire-area data storage and queries. Workload from real applications that use DHT networks will likely exhibit significant skews that can result in bottlenecks and failures that limit the overall scalability of the DHT approach. We present the content and load-aware scalable hashing (CLASH) protocol that can enhance the load distribution behavior of a DHT. CLASH relies on a variable-length identifier key scheme, where the length of any individual key is a function of load. CLASH uses variable-length keys to cluster content-related objects on single nodes to achieve processing efficiencies, and minimally disperse objects across multiple servers when hotspots occur. We demonstrate the performance benefits of CLASH through analysis and simulation.
[Protocols, Costs, wide area networks, Scalability, cluster content-related object, Quality of service, wire-area data storage, Distributed computing, load distribution behavior, variable-length identifier key scheme, Network servers, DHT overlay network, Query processing, distributed hash table, robust technique, content and load-aware scalable hashing, CLASH protocol, Robustness, Internet, protocols, Web server]
Choosing replica placement heuristics for wide-area systems
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Data replication is used extensively in wide-area distributed systems to achieve low data-access latency. A large number of heuristics have been proposed to perform replica placement. Practical experience indicates that the choice of heuristic makes a big difference in terms of the cost of required infrastructure (e.g., storage capacity and network bandwidth), depending on system topology, workload and performance goals. We describe a method to assist system designers choose placement heuristics that meet their performance goals for the lowest possible infrastructure cost. Existing heuristics are classified according to a number of properties. The inherent cost (lower bound) for each class of heuristics is obtained for given system, workload and performance goals. The system designer compares different classes of heuristics on the basis of these lower bounds. Experimental results show that choosing a heuristic with the proposed methodology results in up to 7 times lower cost compared to using an "obvious " heuristic, such as caching.
[Costs, Video on demand, wide area networks, replicated databases, data-access latency, Laboratories, Linear programming, cache storage, network bandwidth, Distributed computing, caching, Delay, bandwidth allocation, Network topology, NP-hard problem, wide-area distributed system, data replication, Bandwidth, system topology, storage capacity]
Roaming honeypots for mitigating service-level denial-of-service attacks
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Honeypots have been proposed to act as traps for malicious attackers. However, because of their deployment at fixed (thus detectable) locations and on machines other than the ones they are supposed to protect, honeypots can be avoided by sophisticated attacks. We propose roaming honeypots, a mechanism that allows the locations of honeypots to be unpredictable, continuously changing, and disguised within a server pool. A (continuously changing) subset of the servers is active and providing service, while the rest of the server pool is idle and acting as honeypots. We utilize our roaming honeypots scheme to mitigate the effects of service-level DoS attacks, in which many attack machines acquire service from a victim server at a high rate, against back-end servers of private services. The roaming honeypots scheme detects and filters attack traffic from outside a firewall (external attacks), and also mitigates attacks from behind a firewall (internal attacks) by dropping all connections when a server switches from acting as a honeypot into being active. Through ns-2 simulations, we show the effectiveness of our roaming honeypots scheme. In particular, against external attacks, our roaming honeypots scheme provides service response time that is independent of attack load for a fixed number of attack machines.
[network servers, computer networks, Telecommunication traffic, back-end server, Computer crime, Computer science, Network servers, Information science, Filters, roaming honeypot, service-level DoS attack, firewall, Intrusion detection, server pool, Traffic control, Protection, Web server]
A feasible region for meeting aperiodic end-to-end deadlines in resource pipelines
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
This paper generalizes the notion of utilization bounds for schedulability of aperiodic tasks to the case of distributed resource systems. In the basic model, aperiodically arriving tasks are processed by multiple stages of a resource pipeline within end-to-end deadlines. The authors consider a multidimensional space in which each dimension represents the instantaneous utilization of a single stage. A feasible region is derived in this space such that all tasks meet their deadlines as long as pipeline resource consumption remains within the feasible region. The feasible region is a multidimensional extension of the single-resource utilization bound giving rise to a bounding surface in the utilization space rather than a scalar bound. Extensions of the analysis are provided to nonindependent tasks and arbitrary task graphs. We evaluate the performance of admission control using simulation, as well as demonstrate the applicability of these results to task schedulability analysis in the total ship computing environment envisioned by the US navy.
[Real time systems, resource pipeline, Military computing, task schedulability analysis, Pipelines, aperiodically arriving tasks, Aerospace electronics, Jitter, naval engineering computing, total ship computing environment, Distributed computing, processor scheduling, arbitrary task graphs, ships, US navy, Processor scheduling, resource allocation, Admission control, real-time systems, pipeline processing, Resource management, real-time scheduling, Marine vehicles, distributed resource systems]
Distributed caching and adaptive search in multilayer P2P networks
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
To improve the scalability of Gnutella-like unstructured peer-to-peer (P2P) networks, a uniform index caching (UIC) mechanism was suggested in some earlier work. In UIC, query results are cached in all peers along the inverse query path such that the same query of other peers can be replied from their nearby-cached results. However, our experiments show that the UIC method causes a large amount of duplicated and unnecessary caching of items among neighboring peers. Aiming at improving the search efficiency, we propose a distributed caching mechanism, which distributes the cache results among neighboring peers. Furthermore, based on the distributed caching mechanism, an adaptive search approach is built which selectively forwards the query to the peers with a high probability of providing the desired cache results. All the enhancements above are defined in a protocol called distributed caching and adaptive search (DiCAS). In the DiCAS enhanced Gnutella network, all the peers are logically divided into multiple layers, with the character that all the peers in the same layer have the same group ID. The query flooding is restricted in one layer with the matched group ID. Our simulation study shows that, with the help of the index caching and search space division, the DiCAS protocol can significantly reduce the network search traffic in unstructured P2P systems without degrading query success rate.
[multilayer P2P network, uniform index caching, Protocols, Peer to peer computing, Scalability, computer networks, Telecommunication traffic, Nonhomogeneous media, cache storage, Gnutella network, Floods, adaptive search method, Computer science, network traffic, Intelligent networks, Network topology, query flooding, Internet, unstructured P2P system, protocols, query formulation, distributed caching, telecommunication traffic]
Scalable self-stabilization via composition
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Objections to the practical use of stabilization have centered around problems of scale. Because of potential interferences between actions, global reasoning over the entire system is in general necessary. The complexity of this task increases dramatically as systems grow in size. Alternatives to dealing with this complexity focus on reset and composition. For reset, the problem is that any fault, no matter how minor, will cause a complete system reset with potentially significant lack of availability. For existing compositional alternatives, including compositional reset, severe restrictions on candidate systems are imposed. To address these issues, we give a framework for composition in which global reasoning and detailed system knowledge are not necessary, and which apply to a significantly wider range of systems than has hitherto been possible. We explicitly identify for each component which other components it can corrupt. Additionally, the correction of one component often depends on the prior correction of one or more other components, constraining the order in which correction can take place. Given appropriate component stabilizers such as detectors and correctors, we offer several ways to coordinate system correction, depending on what is actually known about the corruption and correction relations. By reducing the design of and reasoning about stabilization to local activities involving each component and the neighbors with which it interacts, the framework is scalable. Reset is generally avoided by using the correction relation to check and correct only where necessary. By including both correction and corruption relations, the framework subsumes and extends other compositional approaches. Though not directly a part of this work, we mention tools and techniques that can be used to help calculate the dependency and corruption relations and to help create the necessary stabilizers. To illustrate the theory, we show how this framework has been applied in our work in sensor networks.
[scalable self-stabilization, Interference, global reasoning, coordinate system correction, distributed processing, sensor networks, system recovery, Temperature sensors, Information science, Network servers, Wireless networks, Computer bugs, Detectors, Computer errors, Local activities, appropriate component stabilizers, Contracts]
A distributed approach to solving overlay mismatching problem
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
In unstructured peer-to-peer (P2P) systems, the mechanism of a peer randomly joining and leaving a P2P network causes topology mismatching between the P2P logical overlay network and the physical underlying network, causing a large volume of redundant traffic in the Internet. In order to alleviate the mismatching problem, we propose adaptive connection establishment (ACE), an algorithm of building an overlay multicast tree among each source node and the peers within a certain diameter from the source peer, and further optimizing the neighbor connections that are not on the tree, while retaining the search scope. Our simulation study shows that this approach can effectively solve the mismatching problem and significantly reduce P2P traffic. We further study the tradeoffs between the topology optimization rate and the information exchange overhead by changing the diameter used to build the tree.
[Costs, Peer to peer computing, Telecommunication traffic, overlay network, adaptive connection establishment algorithm, Floods, network topology, tree searching, overlay mismatching problem, Computer science, P2P network traffic, Multicast algorithms, optimisation, Network topology, distributed algorithms, multicast communication, overlay multicast tree, Traffic control, peer-to-peer system, Internet, topology optimization, IP networks, telecommunication traffic]
Slowing down Internet worms
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
An Internet worm automatically replicates itself to vulnerable systems and may infect hundreds of thousands of servers across the Internet. It is conceivable that the cyber-terrorists may use a wide-spread worm to cause major disruption to our Internet economy. While much recent research concentrates on propagation models, the defense against worms is largely an open problem. We propose a distributed antiworm architecture (DAW) that automatically slows down or even halts the worm propagation. New defense techniques are developed based on behavioral difference between normal hosts and worm-infected hosts. Particularly, a worm-infected host has a much higher connection-failure rate when it scans the Internet with randomly selected addresses. This property allows DAW to set the worms apart from the normal hosts. We propose a temporal rate-limit algorithm and a spatial rate-limit algorithm, which makes the speed of worm propagation configurable by the parameters of the defense system. DAW is designed for an Internet service provider to provide the anti-worm service to its customers. The effectiveness of the new techniques is evaluated analytically and by simulations.
[invasive software, Computer worms, Europe, cyber-terrorists, distributed antiworm architecture, Information science, Analytical models, Web and internet services, Asia, Computer bugs, distributed algorithms, Intrusion detection, temporal rate-limit algorithm, Internet economy, Software systems, spatial rate-limit algorithm, Internet, Internet worm, Web server]
Leak-free group signatures with immediate revocation
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Group signatures are an interesting and appealing cryptographic construct with many promising potential applications. This work is motivated by attractive features of group signatures, particularly, their potential to serve as foundation for anonymous credential systems. We reexamine the entire notion of group signatures from a systems perspective and identify two new security requirements: leak-freedom and immediate-revocation, which are crucial for a large class of applications. We then present a new group signature scheme that achieves all identified properties. Our scheme is based on the so-called systems architecture approach. It is more efficient than the state-of-the-art and facilitates easy implementation. Moreover, it reflects the well-known separation-of-duty principle. Another benefit of our scheme is the obviated reliance on underlying anonymous communication channels, which are necessary in previous schemes.
[telecommunication security, immediate revocation, Protocols, telecommunication channels, open systems, cryptography, Information management, leak-free group signatures, systems architecture, Security, Application software, credential systems, Computer science, Public key, message authentication, Management information systems, Communication channels, Ear, groupware, Public key cryptography, security requirements, communication channels]
Self-healing workflow systems under attacks
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Workflow systems are popular in daily business processing. Since vulnerability cannot be totally removed from a workflow management system, successful attacks always happen and may inject malicious tasks or incorrect data into the workflow system. Referring to the incorrect data further corrupt more data objects in the system, which comprises the integrity level of the system. This problem cannot be efficiently solved by existing defense mechanisms, such as access control, intrusion detection, and checkpoints. In this paper, we propose a practical solution for online attack recovery of workflows. The recovery system discovers all damages caused by the malicious tasks that the intrusion detection system reports and automatically repairs the damages based on data and control dependencies among workflow tasks. We analyze the behaviors of our attack recovery system based on the continuous time Markov chain model. The analytical results demonstrate that our system is practical when the parameters of the system are reasonably designed.
[Access control, self-healing workflow systems, Control systems, Credit cards, checkpoints, continuous time Markov chain model, Computer crashes, Scheduling, intrusion detection, Transaction databases, Distributed computing, system recovery, online attack recovery, Intrusion detection, authorisation, Automatic control, Markov processes, access control, Internet, workflow management software, Workflow management software, business data processing, workflow management system]
Selection of mobile agents
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
When a task is assigned to mobile agents, those suitable to perform the task need to be selected according to, not only their application-specific behaviors, but also their mobilities. The focus of current research, however, is on the development of execution platforms and applications for mobile agents and not on methodologies for selection of mobile agents. We present a general approach to selecting mobile agents according to their itineraries among multiple hosts. The approach offers a process algebra-based language for formally specifying the itineraries of mobile agents and an algebraic order relation between two itineraries specified as terms of the language. The relation can strictly decide whether or not the itineraries of mobile agents can satisfy the itinerary required by a given task, in the sense that the agents can migrate to all the hosts required by the task in the permissible order specified by the task. A prototype implementation of this approach was constructed on a Java-based mobile agent system. It enables each mobile agent to specify its itinerary as a term of the language and to migrate over a network according to only the itinerary. Also, when it receives a task request from its external environment, it can select a suitable mobile agent to perform the task by using the order relation. We also describe its implementation and a practical application.
[Java, mobile agent selection, Costs, computability, algebra-based language, Application software, formal specification, Intelligent agent, process algebra, Mobile agents, Prototypes, mobile agents, application-specific behavior, Java-based mobile agent system, Software agents, Computer networks, Informatics, Computational intelligence]
Exchange-based incentive mechanisms for peer-to-peer file sharing
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Performance of peer-to-peer resource sharing networks depends upon the level of cooperation of the participants. To date, cash-based systems have seemed too complex, while lighter-weight credit mechanisms have not provided strong incentives for cooperation. We propose exchange-based mechanisms that provide incentives for cooperation in peer-to-peer file sharing networks. Peers give higher service priority to requests from peers that can provide a simultaneous and symmetric service in return. We generalize this approach to n-way exchanges among rings of peers and present a search algorithm for locating such rings. We have used simulation to analyze the effect of exchanges on performance. Our results show that exchange-based mechanisms can provide strong incentives for sharing, offering significant improvements in service times for sharing users compared to free-riders, without the problems and complexity of cash- or credit-based systems.
[search algorithm, Peer to peer computing, computer networks, exchange-based incentive mechanism, Data processing, Computational Intelligence Society, Research initiatives, Proposals, credit-based system, Degradation, Analytical models, resource allocation, network operating systems, resource sharing network, peer-to-peer file sharing, Large-scale systems, Performance analysis, Resource management, n-way exchange, search problems, cash-based system]
Data indexing in peer-to-peer DHT networks
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Peer-to-peer distributed hash table (DHT) systems make it simple to discover specific data when their complete identifiers - or keys - are known in advance. In practice, however, users looking up resources stored in peer-to-peer systems often have only partial information for identifying these resources. We describe techniques for indexing data stored in peer-to-peer DHT networks, and discovering the resources that match a given user query. Our system creates multiple indexes, organized hierarchically, which permit users to locate data even using scarce information, although at the price of a higher lookup cost. The data itself is stored on only one (or few) of the nodes. Experimental evaluation demonstrates the effectiveness of our indexing techniques on a distributed peer-to-peer bibliographic database with realistic user query workloads.
[Peer to peer computing, computer networks, cache storage, bibliographic systems, Distributed computing, query processing, table lookup, Intelligent networks, bibliographic database, database indexing, resource allocation, data indexing, distributed hash table, distributed databases, data location, peer-to-peer network, Indexing]
Scalability in adaptive multi-metric overlays
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Increasing application requirements have placed heavy emphasis on building overlay networks to efficiently deliver data to multiple receivers. A key performance challenge is simultaneously achieving adaptivity to changing network conditions and scalability to large numbers of users. In addition, most current algorithms focus on a single performance metric, such as delay or bandwidth, particular to individual application requirements. We introduce a two-fold approach for creating robust, high-performance overlays called adaptive multimetric overlays (AMMO). First, AMMO uses an adaptive, highly-parallel, and metric-independent protocol, TreeMaint, to build and maintain overlay trees. Second, AMMO provides a mechanism for comparing overlay edges along specified application performance goals to guide TreeMaint transformations. We have used AMMO to implement and evaluate a single-metric (bandwidth-optimized) tree similar to Overcast and a two-metric (delay-constrained, cost-optimized) overlay.
[Measurement, adaptive multimetric overlay network, Protocols, metric-independent protocol, Scalability, Drives, Data engineering, two-metric overlay, Delay, AMMO, optimisation, overlay trees, delay performance metric, Bandwidth, multicast communication, Overcast, protocols, Probes, TreeMaint transformation, Peer to peer computing, computer networks, tree searching, Computer science, bandwidth allocation, bandwidth-optimized tree, network scalability]
Impact of layer two ARQ on TCP performance in W-CDMA networks
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
We address the interaction between TCP and RLC (Radio Link Control), the link layer retransmission protocol in W-CDMA, a 3rd generation cellular wireless network technology, in evaluating TCP performance in following points; l) Supress delay-jitter in link layer to avoid excess retransmissions. 2) Clarify the trade-off between jitter-suppression and link utilization to improve TCP throughput. 3) Optimize the link layer and TCP parameters. We show how to find the TCP and link ARQ parameters that yield optimum overall performance; simulations and emulation confirm their effectiveness. The coexistence of ARQ and TCP can lead to inefficient interaction; the delay-jitter on the link layer may trigger spurious TCP retransmission. The solution is to suppress jitter on the link layer. To manage the trade-off between this suppression and link utilization, we optimized with link parameter.
[TCP, Wireless application protocol, 3G mobile communication, RLC, Throughput, Multiaccess communication, Delay, Wireless networks, Emulation, Radio control, radio link control, W-CDMA, Radio link, code division multiple access, TCP parameter, Cellular networks, jitter, transport protocols, 3rd generation cellular wireless network technology, TCP throughput, link layer retransmission protocol, delay-jitter, ARQ parameter, Automatic repeat request, cellular radio]
Adaptive replication in peer-to-peer systems
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Peer-to-peer systems can be used to form a low-latency decentralized data delivery system. Structured peer-to-peer systems provide both low latency and excellent load balance with uniform query and data distributions. Under the more common skewed access distributions, however, individual nodes are easily overloaded, resulting in poor global performance and lost messages. This paper describes a lightweight, adaptive, and system-neutral replication protocol, called LAR, that maintains low access latencies and good load balance even under highly skewed demand. We apply LAR to Chord and show that it has lower overhead and better performance than existing replication strategies.
[Adaptive systems, replicated databases, Peer to peer computing, Access protocols, distributed processing, Routing, Educational institutions, adaptive replication strategies, Topology, Delay, LAR system-neutral replication protocol, Computer science, resource allocation, structured peer-to-peer systems, Streaming media, Workstations, low-latency decentralized data delivery system]
Efficient end to end data exchange using configurable compression
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
We explore the use of compression methods to improve the middleware-based exchange of information in interactive or collaborative distributed applications. In such applications, good compression factors must be accompanied by compression speeds suitable for the data transfer rates sustainable across network links. Our approach combines methods that continuously monitor current network and processor resources and assess compression effectiveness, with techniques that automatically choose suitable compression techniques. The resulting network- and user-aware compression methods are evaluated experimentally across a range of network links and application data, the former ranging from low end links to homes, to wide-area Internet links, to high end links in intranets, the latter including both scientific (binary molecular dynamics data) and commercial (XML) data sets. Results attained demonstrate substantial improvements of this adaptive technique for data compression over non-adaptive approaches, where better compression methods are used when CPU loads are low and/or network links are slow, and where less effective and typically, faster compression techniques are used in high end network infrastructures.
[data compression, message passing, electronic data interchange, end-end data exchange, network monitoring, data transfer rate, configurable data compression, network links, grid computing, data communication lines, Distributed computing, middleware]
Session-affinity aware request allocation for Web clusters
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Persistent connections are increasingly being used in Web retrieval due to wide adoption of HTTP/1.1 standards. With persistent connections, the request allocation algorithm used by Web clusters is often session-grained. This article studies the caching performance of Web clusters under session-grained request allocation. It is shown that although content-based algorithms considerably improve caching performance over content-blind algorithms at the request-grained level, most performance gain is offset by the allocation dependency that arises when the requests are allocated at the session-grained level. The performance loss increases with cluster size and connection holding time. An optimization problem is then formulated for improving the caching effectiveness of session-grained allocation. The problem is proven to be NP-complete. Based on a heuristic approach, a session-affinity aware algorithm is presented that makes use of the correlation between the requests in a session. The new algorithm is shown to significantly outperform the content-based algorithm under session-grained allocation. It is also shown that optimizing session-grained allocation cannot fully compensate for the performance loss caused by allocation dependency.
[workstation clusters, optimization problem, Protocols, content-based algorithm, content-blind algorithm, Switches, Performance gain, HTML, cache storage, digital simulation, optimisation, Web cluster caching, resource allocation, Clustering algorithms, Web server, session-affinity aware request allocation, client-server systems, HTTP/1.1 standard, Service oriented architecture, NP-complete problem, Scheduling algorithm, trace-driven simulation, telecommunication standards, Computer science, Web retrieval, distributed algorithms, session correlation, Performance loss, Internet]
LISP: a link-indexed statistical traffic prediction approach to improving IEEE 802.11 PSM
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
We present a novel and complementary mechanism, called link-indexed statistical traffic predictor (LISP) to improve IEEE 802.11 power saving mechanism (PSM). LISP employs a simple, light-weight traffic prediction mechanism and enables each node to seek the inherent correlation between ATIM.ACKs and incoming traffic. Thereby nodes en route can stay awake in the BI in which a packet is anticipated to arrive, thus bridging a "freeway" for the packet to rapidly traverse the route. Meanwhile, the number of duty cycles is minimized and more energy is conserved. We conduct analytical and simulation studies and demonstrate the effectiveness of LISP.
[Energy consumption, telecommunication links, power saving mechanism, Network interfaces, telecommunication power supplies, Delay, Communication switching, Mobile ad hoc networks, IEEE 802.11 link, Wireless communication, Wireless networks, telecommunication network routing, Traffic control, Bismuth, energy conservation, light-weight traffic prediction mechanism, LISP, Energy efficiency, IEEE standards, wireless LAN, ad hoc networks, telecommunication traffic, link-indexed statistical traffic predictor]
An approach to programmable RDF-model transformations
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
The general principles and implementation details of a graph-oriented transformation of RDF models are described. The approach suggested allows for facilitating the semantic interoperability of data circulated in a distributed software system based on loosely coupled components.
[open systems, distributed software system, Ontologies, programmable RDF-model transformation, data semantic interoperability, Resource description framework, data integrity, Application software, graph grammars, loosely coupled components, Message-oriented middleware, Software architecture, graph-oriented transformation, Computer architecture, Information processing, Software systems, Hardware, Pattern matching, distributed programming, middleware]
SOAP-binQ: high-performance SOAP with continuous quality management
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
There is substantial interest in using SOAP (simple object access protocol) in distributed applications' interprocess communications due to its promise of universal interoperability. The utility of SOAP is limited, however, by its inefficient implementation. Our research aims to make SOAP useful for high end or resource-constrained applications. The resulting SOAP-bin communication protocol exhibits substantially improved performance compared to regular SOAP communications. Gains are particularly evident when the same types of parameters are exchanged repeatedly, examples including transactional applications, remote graphics or visualization, and distributed scientific codes. A further improvement to SOAP-bin, termed SOAP-binQ, addresses resource-constrained applications like distributed media codes, where scarce communication bandwidth, for example, may prevent end users from interacting in real-time. SOAP-binQ offers quality management functions that permit SOAP to reduce parameter sizes dynamically, as and when needed. The methods used in size reduction are provided by end users and/or by applications, thereby enabling domain-specific tradeoffs in quality vs. performance. An adaptive use of SOAP-binQ's quality management techniques presented significantly reduces the jitter experienced in two sample applications like remote sensing and remote visualization.
[Visualization, open systems, resource-constrained applications, access protocols, Simple object access protocol, Distributed computing, software architecture, interprocess communication, distributed scientific code, Bandwidth, quality management function, universal interoperability, remote visualization, Quality management, client-server systems, SOAP-binQ, message passing, communication bandwidth, Access protocols, simple object access protocol, distributed media code, Educational institutions, remote sensing, Graphics, quality management, jitter, SOAP-bin communication protocol, XML, Collaboration]
Message from the General Co-Chairs
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Presents the welcome message from the conference proceedings.
[]
Message from the Program Co-Chairs
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Presents the welcome message from the conference proceedings.
[]
Organizing Committee
24th International Conference on Distributed Computing Systems, 2004. Proceedings.
None
2004
Provides a listing of current committee members.
[]
General Chair's message
25th IEEE International Conference on Distributed Computing Systems
None
2005
Presents the welcome message from the conference proceedings.
[]
Program Chairs' message
25th IEEE International Conference on Distributed Computing Systems
None
2005
Presents the welcome message from the conference proceedings.
[]
Organizing Committee
25th IEEE International Conference on Distributed Computing Systems
None
2005
Provides a listing of current committee members.
[]
Program Committee
25th IEEE International Conference on Distributed Computing Systems
None
2005
Provides a listing of current committee members.
[]
Distributed Computation in the Physical World
25th IEEE International Conference on Distributed Computing Systems
None
2005
Summary form only given. Networks of intelligent sensors that are distributed through the physical world will revolutionize practices in the life sciences, civil engineering, manufacturing, security, agriculture, ubiquitous computing, and many other areas. They also present an opportunity and a need to explore distributed algorithms that are wedded to the noisy, localized, time varying physical world. Bandwidth, storage, and energy limitations make in-network processing essential - within the node and among collections of nodes. The algorithms should be resource efficient, but also deal with noise, uncertainty and dynamically changing connectivity. A broad research community has been exploring these issues in the context of TinyOS and the Berkeley motes. This talk will highlight novel distributed algorithms coming out of these efforts and discuss issues in making such networks robust and programmable
[Pervasive computing, wireless sensor networks, in-network processing, computer networks, Ubiquitous computing, distributed computation, Distributed computing, Intelligent sensors, Physics computing, distributed algorithms, Bandwidth, Agriculture, intelligent sensors, Manufacturing, Distributed algorithms, Civil engineering]
MNP: Multihop Network Reprogramming Service for Sensor Networks
25th IEEE International Conference on Distributed Computing Systems
None
2005
Reprogramming of sensor networks is an important and challenging problem as it is often necessary to reprogram the sensors in place. In this paper, we propose a multihop reprogramming service designed for Mica-2/XSM motes. One of the problems in reprogramming is the issue of message collision. To reduce the problem of collision and hidden terminal problem, we propose a sender selection algorithm that attempts to guarantee that in a neighborhood there is at most one source transmitting the program at a time. Further, our sender selection is greedy in that it tries to select the sender that is expected to have the most impact. We also use pipelining to enable fast data propagation. MNP is energy efficient because it reduces the active radio time of a sensor node by putting the node into "sleep" state when its neighbors are transmitting a segment that is not of interest. Finally, we argue that it is possible to tune our service according to the remaining battery level of a sensor, i.e., it can be tuned so that the probability that a sensor is given the responsibility of transmitting the code is proportional to its remaining battery life
[multihop network reprogramming, Mica-2/XSM motes, wireless sensor networks, message collision, Laboratories, Sensor Networks, computer networks, sensor network reprogramming, Sensor systems, Batteries, Network Reprogramming, Pipeline processing, Computer science, Wireless sensor networks, sender selection algorithm, Spread spectrum communication, Bandwidth, Energy efficiency, pipeline processing, Software engineering, sensor node, Code Dissemination]
Exploring the Energy-Latency Trade-Off for Broadcasts in Energy-Saving Sensor Networks
25th IEEE International Conference on Distributed Computing Systems
None
2005
Networking protocols for multi-hop wireless sensor networks (WSNs) are required to simultaneously minimize resource usage as well as optimize performance metrics such as latency and reliability. This paper explores the energy-latency-reliability trade-off for broadcast in multi-hop WSNs, by presenting a new protocol called PBBF (probability-based broadcast forwarding). PBBF works at the MAC layer and can be integrated into any sleep scheduling protocol. For a given application-defined level of reliability for broadcasts, the energy required and latency obtained are found to be inversely related to each other. Our analysis and simulation study quantify this relationship at the reliability boundary, as well as performance numbers to be expected from a deployment. PBBF essentially offers a WSN application designer considerable flexibility in choice of desired operation points
[Measurement, PBBF protocol, wireless sensor networks, sleep scheduling protocol, Wireless application protocol, networking protocols, energy-saving sensor networks, access protocols, Delay, probability-based broadcast forwarding protocol, Wireless sensor networks, Analytical models, Sleep, energy latency trade-off, Spread spectrum communication, Broadcasting, multi-hop wireless sensor networks, Media Access Protocol, Performance analysis, computer network reliability, MAC layer]
Localized Low-Power Topology Control Algorithms in IEEE 802.15.4-Based Sensor Networks
25th IEEE International Conference on Distributed Computing Systems
None
2005
Sensor networks have emerged as a promising technology with various applications, and power consumption is one of the key issues. Since each full function device can act as a coordinator or a device in IEEE 802.15.4 standard, 802.15.4-based sensor networks have various possible network topologies. In this paper, we try to construct network topologies with small number of coordinators while still maintaining network connectivity. By reducing the number of coordinators, the average duty cycle is reduced and the battery life is prolonged. Three topology control algorithms are proposed in this paper. Self-pruning is the simplest one with O(l) running time. Ordinal pruning significantly improves self-pruning in terms of power saving with O(n) running time. Layered pruning is a tradeoff between the first two pruning algorithms with O(radicn) running time and a little higher power consumption than ordinal pruning. Furthermore, all three algorithms are independent of the physical radio propagation characteristics
[Energy consumption, Costs, wireless sensor networks, telecommunication control, computer networks, Sensor phenomena and characterization, telecommunication network topology, Batteries, Application software, telecommunication power supplies, telecommunication standards, Computer science, Intelligent networks, Wireless sensor networks, Network topology, Asia, network connectivity, localized low-power topology control algorithms, network topologies, IEEE standards, IEEE 802.15.4 standard]
The Impossibility of Boosting Distributed Service Resilience
25th IEEE International Conference on Distributed Computing Systems
None
2005
We prove two theorems saying that no distributed system in which processes coordinate using reliable registers and f-resilient services can solve the consensus problem in the presence of f + 1 undetectable process stopping failures. (A service is f-resilient if it is guaranteed to operate as long as no more than f of the processes connected to it fail.) Our first theorem assumes that the given services are atomic objects, and allows any connection pattern between processes and services. In contrast, we show that it is possible to boost the resilience of systems solving problems easier than consensus: the k-set consensus problem is solvable for 2k - 1 failures using 1-resilient consensus services. The first theorem and its proof generalize to the larger class of failure-oblivious services. Our second theorem allows the system to contain failure-aware services, such as failure detectors, in addition to failure-oblivious services; however, it requires that each failure-aware service be connected to all processes. Thus, f + 1 process failures overall can disable all the failure-aware services. In contrast, it is possible to boost the resilience of a system solving consensus if arbitrary patterns of connectivity are allowed between processes and failure-aware services: consensus is solvable for any number of failures using only 1-resilient 2-process perfect failure detectors
[fault diagnosis, failure detectors, failure-aware services, distributed system, distributed processing, Registers, k-set consensus problem, Distributed computing, system recovery, Information science, distributed service resilience, Detectors, Broadcasting, register reliability, f-resilient services, failure-oblivious services, Reliability theory, operating systems, Boosting, Educational institutions, process stopping failures, Resilience, Computer science, distributed algorithms, operating systems (computers), fault tolerant computing, failure detection]
Efficient Wait-Free Implementation of Multiword LL/SC Variables
25th IEEE International Conference on Distributed Computing Systems
None
2005
Since the design of lock-free data structures often poses a formidable intellectual challenge, researchers are constantly in search of abstractions and primitives that simplify this design. The multiword LL/SC object is such a primitive: many existing algorithms are based on this primitive, including the nonblocking and wait-free universal constructions (Anderson et. al., 1995), the closed objects construction (Chandra et. al., 1998) and the snap-shot algorithms (Jayanti, 2005, 2002). In this paper, the authors considered the problem of implementing a W-word LL/SC object shared by N processes. The previous best algorithm, due to Anderson and Moir, is time optimal (LL and SC operations run in O(W) time), but has a space complexity of O(N2 W). An algorithm that uses novel buffer management ideas to cut down the space complexity by a factor of N to O(NW), while still being time optimal was presented
[Tree data structures, lock free data structures, snap shot algorithms, Read-write memory, multiword LL/SC variables, Data structures, Educational institutions, Registers, wait free implementation, Counting circuits, synchronisation, Computer science, Content addressable storage, intellectual challenge, distributed algorithms, Parallel processing, Hardware, data structures, closed objects construction]
Adaptive Collaboration in Peer-to-Peer Systems
25th IEEE International Conference on Distributed Computing Systems
None
2005
We consider a simple model for reputation systems such as the one used by eBay. In the model there are n players, some of which may exhibit arbitrarily malicious (Byzantine) behavior, and there are m objects, some of which are bad. The goal of the honest players is to find a good object. To facilitate collaboration, the system maintains a shared billboard. A basic step of a player consists of consulting the billboard, probing an object to learn its true value, and posting the result on the billboard for the benefit of others. Probing an object incurs a unit cost to the player, and consulting the billboard is free. The dilemma of an honest player is how to balance between the desire to reduce its cost by taking advantage of the reports posted by honest peers, and the fear of being exploited by adopting reports posted by malicious players. In prior work, the authors presented an algorithm solving this problem in an asynchronous model, and the total cost of the probes made by honest players during the algorithm was analyzed. In this paper, the focus is on the individual cost, and a synchronous model in which each player takes a step in each round was considered. The prior algorithm has individual cost O(1/alphalog n) in this model, assuming that an alpha fraction of players are honest. In this paper, it is proven that no algorithm could guarantee individual cost of less than Omega(1/alpha), which is essentially constant if there are enough honest players. The main result is a new algorithm that achieves O(1) individual cost when there are many honest players, and achieves individual cost O((1/alpha)(log n/ log log n)) even when there are not. It is also shown that this algorithm generalizes to other interesting scenarios
[Algorithm design and analysis, workstation clusters, Costs, Protocols, peer-to-peer computing, Peer to peer computing, peer to peer systems, Byzantine behavior, adaptive systems, reputation systems model, adaptive collaboration, Collaboration, groupware, Collaborative work, eBay, Probes, shared billboard, Business]
On Cooperative Content Distribution and the Price of Barter
25th IEEE International Conference on Distributed Computing Systems
None
2005
We study how a server may disseminate a large volume of data to a set of clients in the shortest possible time. Cooperative scenarios where clients are willing to upload data to each other and, under a simple bandwidth model, derive an optimal solution involving communication on a hypercube-like overlay network were first considered. The authors also studied different randomized algorithms, and show that their performance is surprisingly good. Then non-cooperative scenarios based on the principle of barter, in which one client does not upload to another unless it receives data in return were considered. A strict barter requirement increases the optimal completion time considerably compared to the cooperative case. Relaxations of the barter model in which an efficient solution is theoretically feasible was also considered, and show that obtaining a high-performance practical solution may require careful choices of overlay networks and data-transfer algorithms
[Algorithm design and analysis, barter price, Protocols, peer-to-peer computing, hypercube like overlay network, File servers, hypercube networks, data dissemination, Network servers, Multicast algorithms, cooperative content distribution, file servers, randomized algorithms, Bandwidth, groupware, data transfer algorithms, Context modeling]
Non-Cooperation in Competitive P2P Networks
25th IEEE International Conference on Distributed Computing Systems
None
2005
Large-scale competitive P2P networks are threatened by the non-cooperation problem, where peers do not forward queries to potential competitors. Non-cooperation will be a growing problem in such applications as pay-per-transaction file-sharing, P2P auctions, and P2P service discovery networks, where peers are in competition with each other to provide services. Here, the authors showed how non-cooperation causes unacceptable degradation in quality of results, and present an economic protocol to address this problem. This protocol, called the RTR protocol, is based on the buying and selling of the right-to-respond (RTR) to each query in the network. Through simulations it is shown how the RTR protocol not only overcomes non-cooperation by providing proper incentives to peers, but also results in a network that is even more effective and efficient through intelligent, incentive-compatible routing of messages
[Costs, peer-to-peer computing, Peer to peer computing, Information retrieval, Application software, P2P networks, P2P service discovery networks, Computer science, Degradation, query processing, Intelligent networks, right to respond protocol, routing protocols, RTR protocol, Routing protocols, competitive networks, Large-scale systems, cooperative systems, noncooperation, P2P auctions, Usability, pay per transaction file sharing, economic protocol]
ZAL: Zero-Maintenance Address Allocation in Mobile Wireless Ad Hoc Networks
25th IEEE International Conference on Distributed Computing Systems
None
2005
The allocation of IP addresses in hybrid wireless networks is one of the most critical issues in all-IP converged wireless networks. The reason is that centralized IP address allocation mechanisms may not be available in networks comprised of heterogeneous mobile wireless devices. In this paper, the authors proposed zero-maintenance address allocation (ZAL), a fully distributed address allocation algorithm with extremely low communication overhead. ZAL outperforms existing solutions in many important aspects, and eliminates permanent duplication of IP addresses. ZAL is completely free from periodical maintenance messages, timeouts, delays, and modification of existing network protocols. Theoretically, it is proven that ZAL suffers negligible probability of temporary address duplication, while minimizing the usage of address space. In the experiments, addresses to a network of 480 nodes with no duplication of IP addresses when the size of available address space is 1024 (210) or larger can be allocated. Even for an available address space of size 512, in average, temporary address duplication can be resolved within 60 seconds after the node joins the network
[Wireless LAN, Base stations, Protocols, Ground penetrating radar, ZAL, Ad hoc networks, zero maintenance address allocation, mobile wireless networks, communication overhead, Mobile ad hoc networks, Cellular networks, Intelligent networks, distributed algorithm, mobile computing, IP addresses allocation, Wireless networks, Space technology, distributed algorithms, ad hoc networks, IP networks, wireless LAN, hybrid wireless networks]
VPDS: Virtual Home Region Based Distributed Position Service in Mobile Ad Hoc Networks
25th IEEE International Conference on Distributed Computing Systems
None
2005
Position-based ad hoc routing algorithms have proved to have decent performance in delivery ratio and end-to-end delay. Position service is essential for such algorithms. In this paper, a distributed position service system, named VDPS, is proposed and evaluated. In the system, each node has a virtual home region (VHR), which is a geographic area. Nodes residing in a node's VHR will act as its position servers. A node updates its position to the servers, through which other nodes can obtain its position. To reduce the overhead on position management and simplify the operation, in VDPS, a VHR is divided into subregions. A region-based broadcast is used for position update, and a sequential searching is used for position retrieving. Different mechanisms for improving system robustness have been proposed. Mathematical models are built for analyzing system robustness and control overhead. Illustrated results show that VDPS can maintain a high system robustness at a relatively low control overhead
[Distributed position service, Scalability, geographic routing, Floods, Mobile ad hoc networks, Intelligent networks, Network servers, mobile computing, mobile ad hoc networks, Broadcasting, VDPS, Robustness, position control, sequential searching, network routing, position retrieval, Routing, Ad hoc networks, Virtual home region based Distributed Position Service, ad hoc routing algorithms, Global Positioning System, Robust control, distributed algorithms, region based broadcast, position security, ad hoc networks]
Rcast: A Randomized Communication Scheme for Improving Energy Efficiency in MANETs
25th IEEE International Conference on Distributed Computing Systems
None
2005
In a typical wireless mobile ad hoc network (MANET) using a shared communication medium, every node receives or overhears every data transmission occurring in its vicinity. However, this technique is not applicable when a power saving mechanism (PSM) such as the one specified in IEEE 802.11 is employed, where a packet advertisement period is separated from the actual data transmission period. When a node receives an advertised packet that is not destined to itself, it switches to a low-power state during the data transmission period, and thus, conserves power. However, since some MANET routing protocols such as dynamic source routing (DSR) collect route information via overhearing, they would suffer if they were used with the IEEE 802.11 PSM. Allowing no overhearing may critically deteriorate the performance of the underlying routing protocol, while unconditional overhearing may offset the advantage of using PSM. This paper proposes a new communication mechanism, called RandomCast or Rcast, via which a sender can specify the desired level of overhearing in addition to the intended receiver. Therefore, it is possible that only a random set of nodes overhear and collect route information for future use. Rcast improves not only the energy efficiency, but also the energy balance among the nodes, without significantly affecting the routing efficiency. Extensive simulation using the ns-2 network simulator shows that Rcast is highly energy-efficient compared to the original IEEE 802.11 PSM and On-demand power management (ODPM) protocol in terms of total energy consumption (157% to 236% less than PSM and 28% to 131% less than ODPM) and energy balance (four times less variance than ODPM) among the nodes
[IEEE 802.11, Energy consumption, randomized communication scheme, shared communication medium, power saving mechanism, Switches, Data engineering, Mobile communication, energy balance, Mobile ad hoc networks, mobile computing, mobile ad hoc networks, energy efficiency, overhearing, IEEE standards, Data communication, dynamic source routing, route information collection, energy efficiency improvement, Rcast, routing protocol, Routing, RandomCast, randomised algorithms, MANET, routing protocols, data transmission, Energy efficiency, wireless mobile ad hoc network, ad hoc networks, wireless LAN, Energy management, network lifetime, Power engineering and energy]
Systems Support for Pervasive Query Processing
25th IEEE International Conference on Distributed Computing Systems
None
2005
Database queries, in particular, event-driven continuous queries, are useful for many pervasive computing applications, such as video surveillance. In order to enable these applications, we have developed a pervasive query processing framework called Aorta. Unlike traditional database systems, a pervasive query processor requires systems support for managing a large number of networked, heterogeneous devices. In this paper, the authors presented the communication, synchronization, and scheduling mechanisms in Aorta. Even though these techniques have their roots in distributed and parallel systems, the authors showed how these techniques are customized and applied for pervasive query processing. In essence, communication between heterogeneous devices enables network data independence, synchronization on devices protects action atomicity, and scheduling works for adaptive, cost-based multi-query optimization. Empirical studies on the prototype as well as simulation studies to evaluate the system performance were conducted
[Pervasive computing, event driven continuous query, database query, systems support, database management systems, ubiquitous computing, pervasive computing, query processing, Adaptive scheduling, parallel systems, Processor scheduling, Query processing, System performance, Video surveillance, Aorta, distributed systems, Database systems, Computer network management, Virtual prototyping, pervasive query processing, Protection, video surveillance, database systems]
Topk Queries across Multiple Private Databases
25th IEEE International Conference on Distributed Computing Systems
None
2005
Advances in distributed service-oriented computing and global communications have formed a strong technology push for large-scale data integration among organizations and enterprises. However, concerns about data privacy become increasingly important for large-scale mission-critical data integration applications. Ideally, given a database query spanning multiple private databases, the authors wished to compute the answer to the query without revealing any additional information of each individual database apart from the query result. In practice, this constraint can be relaxed to allow efficient information integration while minimizing the information disclosure. In this paper, the authors proposed an efficient decentralized peer-to-peer protocol for supporting aggregate queries over multiple private databases while respecting the privacy constraints of participants. The paper has three main contributions. First, it formalizes the notion of loss of privacy in terms of information revealed at individual participating databases. Second, it presents a novel probabilistic decentralized protocol for topk selection across multiple private databases that minimizes the loss of privacy. Third, it experimentally evaluates the protocol in terms of its correctness, efficiency and privacy characteristics
[Data privacy, Protocols, peer-to-peer computing, Data security, Mission critical systems, topk query, multiple private databases, Educational institutions, Large scale integration, Distributed computing, query processing, Databases, distributed algorithms, Information security, distributed databases, global communications, probabilistic decentralized protocol, data privacy, peer to peer protocol, distributed service oriented computing, Global communication, protocols]
Supporting Complex Multi-Dimensional Queries in P2P Systems
25th IEEE International Conference on Distributed Computing Systems
None
2005
More and more applications require peer-to-peer (P2P) systems to support complex queries over multi-dimensional data. For example, a P2P auction network for real estate frequently needs to answer queries such as "select five available buildings closest to the airport". Such queries are not efficiently supported in current P2P systems. Towards an efficient and scalable P2P system capable of processing complex multi-dimensional queries, the authors first proposed a comprehensive framework for sharing, indexing, and querying multi-dimensional data, where (i) peers with more computational power coordinate indexing and query processing, and (ii) other peers participate in part of the computation in order to achieve scalability and load-balance. Based on this framework, Network-R-tree (NR-tree), a P2P adaptation of the dominant spatial index - R*-tree was proposed. NR-tree, indexing spatial data at clustered peers, is capable of processing complex queries such as range queries and k-nearest neighbor queries. The authors proposed query processing algorithms for range and k-nearest neighbor queries and experimentally prove the effectiveness of proposed techniques with real data
[Network-R-tree, Multidimensional systems, peer-to-peer computing, Peer to peer computing, complex multidimensional query, comprehensive framework, Routing, Data engineering, Application software, Computer science, query processing, Network servers, database indexing, resource allocation, P2P auction network, Query processing, peer-to-peer systems, Bandwidth, Indexing]
Optimal Asynchronous Garbage Collection for RDT Checkpointing Protocols
25th IEEE International Conference on Distributed Computing Systems
None
2005
Communication-induced checkpointing protocols that ensure rollback-dependency trackability (RDT) guarantee important properties to the recovery system without explicit coordination. However, there was no garbage collection algorithm for them which did not use some type of process synchronization, like time assumptions or reliable control message exchanges. This paper addresses the problem of garbage collection for RDT checkpointing protocols and presents an optimal solution for the case where coordination is done only by means of timestamps piggybacked in application messages. The algorithm uses the same timestamps as off-the-shelf RDT protocols and ensures the tight upper bound on the number of uncollected checkpoints for each process during all the system execution
[Checkpointing, checkpointing, Protocols, garbage collection algorithm, Communication system control, distributed checkpointing, rollback-dependency trackability, Electronic mail, garbage collection, Distributed computing, recovery system, optimal asynchronous garbage collection, Fault tolerance, storage management, rollback dependency trackability, optimal systems, rollback-recovery, protocols, checkpointing protocols, synchronisation, Upper bound, System recovery, synchronization, fault tolerant computing, Force control, Mobile computing]
Application-Driven Coordination-Free Distributed Checkpointing
25th IEEE International Conference on Distributed Computing Systems
None
2005
Distributed checkpointing is an important concept in providing fault tolerance in distributed systems. In today's applications, e.g., grid and massively parallel applications, the imposed overhead of taking a distributed checkpoint using the known approaches can often outweigh its benefits due to coordination and other overhead from the processes. This paper presents an innovative approach for distributed checkpointing. In this approach, the checkpoints are obtained using offline analysis based on the application level. During execution, no coordination is required. After presenting the approach, the authors proved its safety and present a performance analysis of it using stochastic models
[Checkpointing, checkpointing, Protocols, fault tolerance, Stochastic processes, Distributed computing, software fault tolerance, application driven coordination, stochastic models, Fault tolerant systems, distributed algorithms, Grid computing, distributed systems, Safety, Performance analysis, Internet, free distributed checkpointing, stochastic processes, Force control]
On the Impact of Replica Placement to the Reliability of Distributed Brick Storage Systems
25th IEEE International Conference on Distributed Computing Systems
None
2005
Data reliability of distributed brick storage systems critically depends on the replica placement policy, and the two governing forces are repair speed and sensitivity to multiple concurrent failures. In this paper, the authors provided an analytical framework to reason and quantify the impact of replica placement policy to system reliability. The novelty of the framework is its consideration of the bounded network bandwidth for data maintenance. The framework was applied to two popular schemes, namely sequential placement and random placement, and showed that both have drawbacks that significantly degrade data reliability. Then the stripe placement scheme was proposed and find the near-optimal configuration parameter such that it provides much better reliability. The possibility of addressing the problem of correlated brick failures in the analytical framework was further discussed
[distributed processing, Maintenance, Proposals, Distributed computing, Degradation, data reliability, Storage area networks, storage management, distributed brick storage systems, Asia, Failure analysis, Bandwidth, replica placement, fault tolerant computing, data maintenance, Reliability, Local area networks, bounded network bandwidth]
Flexible Consistency for Wide Area Peer Replication
25th IEEE International Conference on Distributed Computing Systems
None
2005
The lack of a flexible consistency management solution hinders P2P implementation of applications involving updates, such as read-write file sharing, directory services, online auctions and wide area collaboration. Managing mutable shared data in a P2P setting requires a consistency solution that can operate efficiently over variable-quality failure-prone networks, support pervasive replication for scaling, and give peers autonomy to tune consistency to their sharing needs and resource constraints. Existing solutions lack one or more of these features. In this paper, we described a new consistency model for P2P sharing of mutable data called composable consistency, and outline its implementation in a wide area middleware file service called Swarm. Composable consistency lets applications compose consistency semantics appropriate for their sharing needs by combining a small set of primitive options. Swarm implements these options efficiently to support scalable, pervasive, failure-resilient, wide-area replication behind a simple yet flexible interface. Two applications was presented to demonstrate the expressive power and effectiveness of composable consistency: a wide area file system that outperforms Coda in providing close-to-open consistency over WANs, and a replicated BerkeleyDB database that reaps order-of-magnitude performance gains by relaxing consistency for queries and updates
[wide area networks, flexible consistency management solution, read-write file sharing, wide area collaboration, ubiquitous computing, Swarm, File systems, online auctions, directory services, composable consistency, mutable data, Online Communities/Technical Collaboration, middleware, Availability, peer-to-peer computing, wide area peer replication, Peer to peer computing, Calendars, wide area middleware file service, Application software, Middleware, P2P implementation, shared data management, Resource management, Computer network management, Frequency synchronization]
Mixed Consistency Model: Meeting Data Sharing Needs of Heterogeneous Users
25th IEEE International Conference on Distributed Computing Systems
None
2005
Heterogeneous users usually have different requirements as far as consistency of shared data is concerned. This paper proposes and investigates a mixed consistency model to meet this heterogeneity challenge in large scale distributed systems that support shared objects. This model allows combining strong (sequential) consistency and weak (causal) consistency. The paper defines the model, motivates it and proposes a protocol implementing it
[Context, peer-to-peer computing, Access protocols, Telecommunication traffic, Educational institutions, heterogeneity challenge, Power system modeling, heterogeneous users, Wireless networks, mixed consistency model, Traffic control, distributed systems, Computer networks, fault tolerant computing, Large-scale systems, Mobile computing, data sharing]
Network-Centric Buffer Cache Organization
25th IEEE International Conference on Distributed Computing Systems
None
2005
A pass-through server such as an NFS server backed by an iSCSI (Internet Small Computer System Interface) storage server only passes data between the storage server and NFS clients. Ideally it should require at most one data copying operation on sending or receiving, as in normal IP routers. In practice, pass-through servers actually incur multiple data copying operations because they are implemented using a layered architecture where each layer has its own internal data representation. This paper describes the design, implementation and evaluation of a novel network-centric buffer-caching scheme called NCache that minimizes data copying overhead in pass-through servers without requiring significant changes to their existing implementation. By organizing the data being passed or cached in a network friendly format, NCache is able to eliminate all unnecessary data copying. The key innovation in NCache is that it exploits the fact that pass-through servers do not interpret data, by replacing physical copying with logical copying in a way transparent to existing software. This transparency enables NCache to be easily portable to many operating systems. The authors have successfully built a Linux-based NCache prototype that can be applied to in kernel NFS and static Web servers. Empirical performance measurements collected from this prototype show that by reducing the CPU load associated with data copying, NCache is able to provide up to 92% improvement in throughput for NFS server and up to 47% for Web server
[Computer interfaces, Software prototyping, Technological innovation, network centric buffer, NCache, cache organization, NFS server, cache storage, data copying, Organizing, Network servers, computer interfaces, storage server, Operating systems, Linux, Prototypes, file servers, data structures, Internet, Web server, Kernel, Internet Small Computer System Interface]
Cache Clouds: Cooperative Caching of Dynamic Documents in Edge Networks
25th IEEE International Conference on Distributed Computing Systems
None
2005
Caching on the edge of the Internet is becoming a popular technique to improve the scalability and efficiency of delivering dynamic Web content. In this paper, we study the challenges in designing a large scale cooperative edge cache network, focusing on mechanisms and methodologies for efficient cooperation among caches to improve the overall performance of the edge cache network. This paper makes three original contributions. First, we introduce the concept of cache clouds, which forms the fundamental framework for cooperation among caches in the edge network. Second, we present dynamic hashing-based protocols for document lookups and updates within each cache cloud, which are not only efficient, but also effective in dynamically balancing lookup and update loads among the caches in the cloud. Third, we outline a utility-based mechanism for placing dynamic documents within a cache cloud. Our experiments indicate that these techniques can significantly improve the performance of the edge cache networks
[document handling, Protocols, Cooperative caching, edge networks, cooperative edge cache network, Clouds, Scalability, document lookups, Educational institutions, cache storage, dynamic hashing based protocols, Intelligent networks, Network servers, dynamic documents, Large-scale systems, Internet, Web sites, cooperative caching, cache clouds]
Adaptive Counting Networks
25th IEEE International Conference on Distributed Computing Systems
None
2005
Counting networks are well studied parallel and distributed data structures, which are useful in synchronization applications such as distributed counting and load balancing. However, current constructions of counting networks are static, since their width (the degree of parallelism), and hence the size of the network, have to be fixed in advance. This present an obstacle in efficiently implementing them in a large distributed system whose size may be changing, due to nodes joining and leaving the network. The authors presented an adaptive construction of the bitonic counting network. The network tunes its width to the system size in a distributed and local way. With high probability, the effective "width" of the network is Omega(N/log2  N), where N is the number of nodes currently in the system, and the effective '"depth" of the network is O(log2 N). In contrast, a static implementation would have the same width irrespective of the system size. When the system size changes, the network adapts by splitting or merging its components. All decisions and actions are decentralized: these include the decision of when to split and merge the components, and the action of splitting and merging them. The construction is layered on an overlay network which provides an efficient peer-to-peer lookup service, and uses the recursive structure present in the bitonic network to adapt its implementation. Though the bitonic network was discussed, the technique could be applied to build an adaptive implementation of any distributed data structure which could be decomposed in a recursive way
[Adaptive systems, load balancing, Peer to peer computing, adaptive counting networks, Merging, Data structures, Application software, degree of parallelism, Distributed computing, adaptive systems, synchronisation, distributed counting, resource allocation, Wires, bitonic counting network, telecommunication network routing, distributed data structures, Parallel processing, Load management, synchronization, Computer networks, data structures, parallel data structures]
A Dynamic Group Mutual Exclusion Algorithm Using Surrogate-Quorums
25th IEEE International Conference on Distributed Computing Systems
None
2005
The group mutual exclusion problem extends the traditional mutual exclusion problem by associating a type with each critical section. In this problem, processes requesting critical sections of the same type can execute their critical sections concurrently. However, processes requesting critical sections of different types must execute their critical sections in a mutually exclusive manner. In this paper, we provide a distributed algorithm for solving the group mutual exclusion problem based on the notion of surrogate-quorum. Intuitively, the algorithm uses the quorum that has been successfully locked by a request as a surrogate to service other compatible requests for the same type of critical section. Unlike the existing quorum-based algorithms for group mutual exclusion, the algorithm achieves a low message complexity of O(q), where q is the maximum size of a quorum, while maintaining both synchronization delay and waiting time at two message hops. Moreover, like the existing quorum-based algorithms, the algorithm has high maximum concurrency of n, where n is the number of processes in the system. The existing quorum-based algorithms assume that the number of groups is static and does not change during runtime. However, the algorithm can adapt without performance penalties to dynamic changes in the number of groups. Simulation results indicate that our algorithm outperforms the existing quorum-based algorithms for group mutual exclusion by as much as 50% in some cases
[processes requesting critical sections, multiprocessing systems, waiting time, Delay effects, surrogate-quorums, communication complexity, Distributed computing, synchronisation, Computer science, Concurrent computing, distributed algorithm, message complexity, Runtime, Web services, distributed algorithms, maximum concurrency, dynamic group mutual exclusion algorithm, Distributed algorithms, synchronization delay]
Transformations of Mutual Exclusion Algorithms from the Cache-Coherent Model to the Distributed Shared Memory Model
25th IEEE International Conference on Distributed Computing Systems
None
2005
We present two transformations that convert a class of local-spin mutual exclusion algorithms on the cache-coherent model to local-spin mutual exclusion algorithms on the distributed shared memory model without increasing their time complexity. The first transformation uses registers and test-and-set objects, and does not increase the number of busy-waiting periods. The second transformation uses only registers, but contains two busy-waiting periods for each busy-waiting period of the input algorithm. The class of mutual exclusion algorithms that are applicable to the transformations was carefully defined, and formally prove the correctness of the transformations
[Performance evaluation, Algorithm design and analysis, busy waiting periods, mutual exclusion algorithms transformation, distributed shared memory model, Telecommunication traffic, Time measurement, cache storage, Registers, local spin, Distributed computing, Computer science, cache coherent model, distributed algorithms, Traffic control, distributed shared memory systems, Testing]
On the Possibility of Consensus in Asynchronous Systems with Finite Average Response Times
25th IEEE International Conference on Distributed Computing Systems
None
2005
It has long been known that the consensus problem cannot be solved deterministically in completely asynchronous distributed systems, i.e., systems (1) without assumptions on communication delays and relative speed of processes and (2) without access to real-time clocks. In this paper, we define a new asynchronous system model. Instead of assuming reliable channels with finite transmission delays, stubborn channels with a finite average response time was assumed (if neither the sender nor the receiver crashes), and it is assumed that there exists some unknown physical bound on how fast an integer can be incremented. Note that there is no limit on how slow a program can be executed or how fast other statements can be executed. Also, there exists no upper or lower bound on the transmission delay of messages or the relative speed of processes. The are no additional assumptions about clocks, failure detectors, etc. that would aid in solving consensus either. It is shown that consensus can nevertheless be solved deterministically in this asynchronous system model
[Real time systems, IEEE news, impossibility, deterministic solving, Delay effects, stubborn channels, reliability, Computer crashes, Synchronization, finite average response times, Distributed computing, failure analysis, Computer science, consensus, eventually perfect failure detector, mobile computing, channel estimation, distributed algorithms, Detectors, distributed systems, consensus possibility, Time factors, asynchronous systems, Clocks]
Virtual Leashing: Internet-Based Software Piracy Protection
25th IEEE International Conference on Distributed Computing Systems
None
2005
Software-splitting is a technique for protecting software from piracy by removing code fragments from an application and placing them on a remote trusted server. The server provides the missing functionality but never the missing code. As long as the missing functionality is hard to reverse-engineer, the application cannot run without validating itself to the server. Current software-splitting techniques scale poorly to the Internet because interactions with the remote server are synchronous: the application must frequently block waiting for a response from the server. Perceptible delays due to network latency are unacceptable for many kinds of highly-reactive applications, such as games or graphics applications. This paper introduces virtual leashing, the first non-blocking software-splitting technique. Virtual leashing ensures that the application and the server communicate asynchronously, so the application's performance is independent (within reason) of large or variable network latencies. Experiments show that virtual leashing makes only modest demands on communication bandwidth, space, and computation
[virtual leashing, software splitting, Application software, Internet-based software piracy protection, Computer crime, Delay, Computer science, Network servers, computer crime, software engineering, Internet, Cryptography, Protection, Web server, Coprocessors]
Robust Information Dissemination in Uncooperative Environments
25th IEEE International Conference on Distributed Computing Systems
None
2005
The open nature of peer-to-peer systems has played an important role in their growing popularity. The current file-sharing applications, for instance, have been widely used largely because they allow anyone to participate in them. This openness, however, brings up new issues because selfish, malicious, faulty, compromised, or resource-constrained peers may degrade a system. The authors explored the case for large-scale information dissemination through the design of the trust-aware multicast (TAM) protocol. Nodes in TAM can exhibit uncooperative behavior such as delaying, discarding, modifying, replaying, and fabricating messages. While detecting such behaviors, TAM computes a level of trust for each node and adapts the underlying multicast tree according to trustworthiness of nodes, which leads to performance improvement in the system. The results from our simulation and PlanetLab experiments show that even with a significant portion of nodes being uncooperative, TAM is able to build a stable dissemination tree that provides lower message delay to well-behaved nodes
[uncooperative environments, peer-to-peer computing, Peer to peer computing, Computational modeling, information dissemination, peer to peer systems, Multicast protocols, Educational institutions, trust aware multicast protocol, Relays, Delay, Degradation, level of trust, current file sharing, routing protocols, Robustness, Large-scale systems, Internet, robust information dissemination]
Efficient Group Rekeying Using Application-Layer Multicast
25th IEEE International Conference on Distributed Computing Systems
None
2005
In secure group communications, there are both rekey and data traffic. The authors proposed to use application-layer multicast to support concurrent rekey and data transport. Rekey traffic is bursty and requires fast delivery. It is desired to reduce rekey bandwidth overhead as much as possible since it competes for bandwidth with data traffic. Towards this goal, a multicast scheme that exploits proximity in the underlying network was proposed. The authors further proposed a rekey message splitting scheme to significantly reduce rekey bandwidth overhead at each user access link and network link. Correctness properties for the multicast scheme and rekey message splitting scheme was formulated and proved. The authors have conducted extensive simulations to evaluate the approach. The simulation results show that the approach can reduce rekey bandwidth overhead from several thousand encrypted new keys (encryptions, in short) to less than ten encryptions for more than 90% of users in a group of 1024 users
[multiprocessing systems, Computational modeling, Telecommunication traffic, Routing, File servers, cryptography, message splitting, data traffic, Teleconferencing, transport protocols, secure group communications, rekey bandwidth reduction, Bandwidth, multicast communication, Grid computing, Internet, Cryptography, application layer multicast, Web server, group rekeying]
An Update Protocol for XML Documents in Distributed and Cooperative Systems
25th IEEE International Conference on Distributed Computing Systems
None
2005
Securing data is becoming a crucial need for most Internet-based applications. Whereas the problem of data confidentiality has been widely investigated, the problem of how to ensure that data, when moving among different parties, are modified only according to the stated policies has been so far not deeply investigated. In this paper, the authors proposed an approach supporting parallel and distributed secure updates to XML documents. The approach, based on the use of a security region-object parallel flow (S-RPF) graph protocol, is particularly suited for all environments requiring cooperative updates to XML documents. It allows different users to simultaneously update different portions of the same document, according to the specified access control policies. Additionally, it supports a decentralized management of update operations in that a subject can exercise its privileges and verify the correctness of the operations performed so far on the document without interacting, in most of the cases, with the document server
[Access control, parallel algorithms, Data security, data security, XML documents, Cooperative systems, Access protocols, data confidentiality, update protocol, access control policy, Network servers, security of data, XML, Information security, distributed systems, data privacy, Internet, cooperative systems, protocols, Web server, Computer security, security region object parallel flow graph protocol]
Similarity Searching in Peer-to-Peer Databases
25th IEEE International Conference on Distributed Computing Systems
None
2005
We consider the problem of handling similarity queries in peer-to-peer databases. We propose an indexing and searching mechanism which, given a query object, returns the set of objects in the database that are semantically related to the query. We propose an indexing scheme which clusters data such that semantically related objects are partitioned into a small set of clusters, allowing for a simple and efficient similarity search strategy. Our indexing scheme also decouples object and node locations. Our adaptive replication and randomized lookup schemes exploit this feature and ensure that the number of copies of an object is proportional to its popularity and all replicas are equally likely to serve a given query, thus achieving perfect load balancing. The techniques developed in this work are oblivious to the underlying DHT topology and can be implemented on a variety of structured overlays such as CAN, CHORD, Pastry, and Tapestry. We also present DHT-independent analytical guarantees for the performance of our algorithms in terms of search accuracy, cost, and load-balance; the experimental results from our simulations confirm the insights derived from these analytical models
[load balancing, data clustering, randomized lookup, DHT topology, query object, query processing, Analytical models, peer-to-peer databases, Databases, database indexing, similarity query handling, Performance analysis, query formulation, peer-to-peer computing, Peer to peer computing, Information retrieval, Educational institutions, Pastry, Tapestry, Topology, similarity searching, CHORD, Computer science, CAN, similarity search strategy, data indexing, Load management, adaptive replication, Indexing]
Keyword Search in DHT-Based Peer-to-Peer Networks
25th IEEE International Conference on Distributed Computing Systems
None
2005
Existing techniques for keyword/attribute search in structured P2P overlays suffer from several problems: unbalanced load, hot spots, fault tolerance, storage redundancy, and unable to facilitate ranking. In this paper, we present a general keyword index and search scheme for structured P2P networks that avoids these problems, and in which object insert, delete, and search can be efficiently performed. Some experimental results are also presented to support our claim
[Costs, fault tolerance, peer-to-peer computing, Peer to peer computing, keyword searching, Keyword search, Redundancy, structured P2P overlays, DHT-based peer-to-peer networks, storage redundancy, Routing, Information management, Information systems, Intelligent networks, Fault tolerance, database indexing, search scheme, keyword index, Bandwidth, query formulation, structured P2P networks]
DSI: A Fully Distributed Spatial Index for Location-Based Wireless Broadcast Services
25th IEEE International Conference on Distributed Computing Systems
None
2005
Recent announcement of the MSN Direct Service has demonstrated the feasibility and industrial interest in utilizing wireless broadcast for pervasive information services. To support location-based services in wireless data broadcast systems, a distributed spatial index (called DSI) is proposed in this paper. DSI is highly efficient because it has a linear yet fully distributed structure that facilitates multiple search paths to be naturally mixed together by sharing links. Moreover, DSI is very resilient in error-prone wireless communication environments. Search algorithms for two classical location-based queries, window queries and kNN queries, based on DSI are presented. Performance evaluation of DSI shows that DSI significantly outperforms R-tree and Hilbert Curve Index, two state-of-the-art spatial indexing techniques for wireless data broadcast
[IEEE news, Satellite broadcasting, Location-based services, Broadcast technology, distributed structure, visual databases, search algorithms, Spatial indexes, Wireless communication, mobile computing, database indexing, USA Councils, location-based services, spatial indexing, query formulation, error-prone wireless communication, multiple search paths, Job shop scheduling, location-based wireless broadcast services, location-based queries, performance evaluation, spatial index, wireless data broadcast systems, Hilbert curve index, Radio broadcasting, window queries, MSN direct service, information services, distributed spatial index, kNN queries, Query processing, R-tree index, Indexing]
Efficient Power Management Based on Application Timing Semantics for Wireless Sensor Networks
25th IEEE International Conference on Distributed Computing Systems
None
2005
This paper proposes Efficient Sleep Scheduling based on Application Timing (ESSAT), a novel power management scheme that aggressively exploits the timing semantics of wireless sensor network applications. We present three ESSAT protocols each of which integrates (1) a lightweight traffic shaper that actively shapes the workload inside the network to achieve predictable timing properties over multiple hops, and (2) a local scheduling algorithm that wakes up nodes just-in-time based on the timing properties of shaped workloads. Our ESSAT protocols have several distinguishing features. First, they can save significant energy with minimal delay penalties. Second, they do not maintain TDMA schedules or communication backbones; as such, they are highly efficient and suitable for resource constrained sensor platforms. Moreover, the protocols are robust in highly dynamic network environments, i.e., they can handle variable multi-hop communication delays and aggregate workloads involving multiple queries, and can adapt to varying workload and network topologies. Our simulations showed that DTS-SS, an ESSAT protocol, achieved an average node duty cycle 38-87% lower than SPAN, and query latencies 36-98% lower than PSM and SYNC
[Protocols, Shape, wireless sensor networks, Telecommunication traffic, query latency, access protocols, TDMA schedules, Delay, local scheduling algorithm, Time division multiple access, resource constrained sensor platforms, multiple hops, node duty cycle, traffic shaper, predictable timing property, PSM, network topology, SPAN, application timing semantics, highly dynamic network environments, Scheduling algorithm, power management scheme, Wireless sensor networks, ESSAT protocols, communication backbones, Sleep, efficient sleep scheduling based on application timing, delays, Timing, minimal delay penalty, SYNC, Energy management, multiple queries, multihop communication delays]
infer: A Bayesian Inference Approach towards Energy Efficient Data Collection in Dense Sensor Networks
25th IEEE International Conference on Distributed Computing Systems
None
2005
In this paper, we propose a novel approach for efficiently sensing a remote field using wireless sensor networks. Our approach, the infer algorithm, is fully distributed, has low overhead and saves considerable energy compared to using just the data aggregation communication paradigm. This is accomplished by using a distributed algorithm to put nodes into sleep mode for a given period of time, thereby trading off energy usage for the accuracy of the data received at the sink. Bayesian inference is used to infer the missing data from the nodes that were not active during each sensing epoch. As opposed to other methods that have been considered, such as wavelet compression and distributed source coding, our algorithm has lower overhead in terms of both inter-node communication and computational complexity. Our simulations show that on average our algorithm produces energy savings of 59% while still maintaining data that is accurate to within 7.9%. We also show how the parameters of the algorithm may be tuned to optimize network lifetime for a desired level of data accuracy
[energy efficient data collection, wireless sensor networks, inter-node communication, remote field sensing, Batteries, Temperature sensors, Intelligent networks, infer algorithm, dense sensor networks, wavelet compression, Large-scale systems, belief networks, Bayesian inference, Distributed algorithms, data accuracy, Source coding, remote sensing, distributed sensors, inference mechanisms, radio access networks, Wireless sensor networks, distributed algorithm, Bayesian methods, distributed source coding, data aggregation communication, Energy efficiency, Inference algorithms, computational complexity]
A Spatiotemporal Query Service for Mobile Users in Sensor Networks
25th IEEE International Conference on Distributed Computing Systems
None
2005
This paper presents MobiQuery, a spatiotemporal query service that allows mobile users to periodically gather information from their surrounding areas through a wireless sensor network. A key advantage of MobiQuery lies in its capability to meet stringent spatiotemporal performance constraints crucial to many applications. These constraints include query latency, data freshness and fidelity, and changing query areas due to user mobility. A novel just-in-time prefetching algorithm enables MobiQuery to maintain robust spatiotemporal guarantees even when nodes operate under extremely low duty cycles. Furthermore, it significantly reduces the storage cost and network contention caused by continuous queries from mobile users. We validate our approach through both theoretical analysis and simulation results under a range of realistic settings
[wireless sensor networks, wireless sensor network, network contention, MobiQuery, Delay, Intelligent networks, mobile computing, Fires, Robot sensing systems, Robustness, spatiotemporal query service, mobile users, continuous queries, query formulation, just-in-time prefetching algorithm, Prefetching, data freshness, storage cost, Computer science, Wireless sensor networks, data fidelity, mobile communication, user mobility, Spatiotemporal phenomena, Mobile computing]
Timer Interaction in Route Flap Damping
25th IEEE International Conference on Distributed Computing Systems
None
2005
Route Flap Damping is a mechanism generally used in network routing protocols. Its goal is to limit the global impact of unstable routes by temporarily suppressing routes with rapid changes over short time periods. Although route damping is a clearly defined and simple procedure at each router, its effect in a large network setting is not well understood. We show that the current damping design leads to the intended behavior only under persistent route flapping. When the number of flaps is small, the global routing dynamics deviates significantly from the expected behavior with a longer convergence delay. Previous work observed that a single route flap can falsely trigger route suppression due to path exploration. However our simulations show that this false suppression only accounts for 30% of the convergence delay after a single route flap. Our study reveals previously unknown interactions between reuse timers at different routers. Route suppression and reuse at different routers are triggered at different times and thus affect the number of updates received by other routers. In turn, this impacts other routers' damping behavior. We propose to use Root Cause Notification to eliminate both false suppression and undesirable timer interaction
[Damping, network routing protocols, route suppression, false suppression, route flap damping, root cause notification, timer interaction, path exploration, Delay, Ribs, convergence delay, Convergence, Intelligent networks, route flapping, delays, routing protocols, Bandwidth, damping design, global routing dynamics, Routing protocols, unstable routes, Large-scale systems, Internet]
An In-Depth, Analytical Study of Sampling Techniques for Self-Similar Internet Traffic
25th IEEE International Conference on Distributed Computing Systems
None
2005
Techniques for sampling Internet traffic are very important to understand the traffic characteristics of the Internet (Feldman et al., 2000). In spite of alt the research efforts on packet sampling, none has taken into account of self-similarity of Internet traffic in devising sampling strategies. In this paper, we perform an in-depth, analytical study of three sampling techniques for self-similar Internet traffic, namely static systematic sampling, stratified random sampling and simple random sampling. We show that while all three sampling techniques can accurately capture the Hurst parameter (second order statistics) of Internet traffic, they fail to capture the mean (first order statistics) faithfully. We also show that static systematic sampling renders the smallest variation of sampling results in different instances of sampling (i.e., it gives sampling results of high fidelity). Based on an important observation, we then devise a new variation of static systematic sampling, called biased systematic sampling (BSS), that gives much more accurate estimates of the mean, while keeping the sampling overhead low. Both the analysis on the three sampling techniques and the evaluation of BSS are performed on synthetic and real Internet traffic traces. Our performance study shows that BSS gives a performance improvement of 40% and 20% (in terms of efficiency) as compared to static systematic and simple random sampling
[Performance evaluation, sampling methods, static systematic sampling, second order statistics, Scalability, computer networks, Telecommunication traffic, Hurst parameter, biased systematic sampling, first order statistics, Helium, self-similar Internet traffic, Statistics, Computer crime, simple random sampling, stratified random sampling, Sampling methods, Internet, Performance analysis, IP networks, packet sampling, telecommunication traffic]
Characterizing and Predicting TCP Throughput on the Wide Area Network
25th IEEE International Conference on Distributed Computing Systems
None
2005
DualPats exploits the strong correlation between TCP throughput and flow size, and the statistical stability of Internet path characteristics to accurately predict the TCP throughput of large transfers using active probing. We propose additional mechanisms to explain the correlation, and then analyze why traditional TCP benchmarking fails to predict the throughput of large transfers well. We characterize stability and develop a dynamic sampling rate adjustment algorithm so that we probe a path based on its stability. Our analysis, design, and evaluation is based on a large-scale measurement study
[Wide area networks, Internet path, Stability, wide area networks, Throughput, Time measurement, DualPats, dynamic sampling rate adjustment algorithm, Computer science, flow size, transport protocols, TCP throughput, Failure analysis, Bandwidth, Sampling methods, Internet, Probes, wide area network, active probing, TCP benchmarking]
HYPER: A Hybrid Approach to Efficient Content-Based Publish/Subscribe
25th IEEE International Conference on Distributed Computing Systems
None
2005
Publish/Subscribe (pub/sub) is an important paradigm for distributed content delivery. Traditionally, there have been two approaches to supporting pub/sub service: subject-based and content-based. Content-based pub/sub allows fine-grained expressiveness, and thus is a more attractive solution for content dissemination. However, the performance of a content-based pub/sub network is bounded by the expensive matching cost of content messages. In this paper, we propose a hybrid approach capable of minimizing both the matching and forwarding overhead within the pub/sub network and the delay experienced by clients receiving the content. The hybrid approach aims to eliminate redundant matching and forwarding inside the pub/sub network. In particular, it identifies a number of virtual groups by exploring common subscription interests among clients, and messages for each virtual group are only matched once at the group entry point. In addition, for each virtual group, the content delivery tree embedded in the underlying pub/sub network can benefit from shortcutting forwarding-only paths. Simulations have shown that the hybrid approach is highly effective in improving the service efficiency and quality of a content-based pub/sub system
[Algorithm design and analysis, forwarding-only paths, Costs, electronic publishing, redundant matching, Subscriptions, subject-based publish/subscribe, pub/sub service, content dissemination, Network servers, Network topology, Forward contracts, telecommunication networks, content delivery tree, Clustering algorithms, Virtual groups, distributed content delivery, pub/sub network, Content management, information dissemination, content-based publish/subscribe, HYPER, group entry point, virtual groups, Multicast algorithms, data communication]
Content-Based Publish-Subscribe over Structured Overlay Networks
25th IEEE International Conference on Distributed Computing Systems
None
2005
This paper introduces a novel architecture for implementing content-based pub/sub communications on top of structured overlay networks. This architecture overcomes some well-known limitations of existing infrastructures, i.e. lack of self-configuration and of adaptiveness to dynamic changes. This is achieved by devising a mediator stratum between the rich event subscription semantics of content-based pub/sub systems and the standard logical addressing scheme of overlays. The paper describes details of the design and provides considerations in selecting the subscription-to-node and event-to-node mappings suitable for the solution. We identify the lack of native support for one-to-many communication by overlay networks as the main impediment for efficient system operation. The paper introduces a novel primitive for one-to-many message delivery, showing through simulation how this can improve performance of the architecture. The simulation study also shows performance comparison between the different mappings proposed as well as evaluation of other optimizations discussed in the paper
[Content management, electronic publishing, rich event subscription semantics, Scalability, content-based publish-subscribe, standard logical addressing, Subscriptions, computer networks, Sensor phenomena and characterization, mediator stratum, subscription-to-node mappings, Routing, Sensor systems and applications, event-to-node mappings, Unicast, structured overlay networks, computer architecture, Publish-subscribe, Computer architecture, content-based pub/sub communications, Impedance, one-to-many message delivery, one-to-many communication, optimizations]
A Unified Approach to Routing, Covering and Merging in Publish/Subscribe Systems Based on Modified Binary Decision Diagrams
25th IEEE International Conference on Distributed Computing Systems
None
2005
The challenge faced by content-based publish/subscribe systems is the ability to handle a vast amount of dynamic information with limited system resources. In current p/s systems, each subscription is processed in isolation. Neither relationships among individual subscriptions are exploited, nor historic information about subscriptions and publications is taken into account. We believe that this neglect limits overall system efficiency. In this paper, we represent subscriptions using modified binary decision diagrams (MBDs), and design an index data structure to maintain distinct predicates and manage associated Boolean variables. Our MBD-based approach can address, in a unified way, publication routing and subscription/advertisement covering and merging. We propose a novel covering algorithm based on MBDs. The algorithm can take historic information about subscription and publication populations into account and exploits relations between subscriptions. We explore merging, especially imperfect merging, and discuss an advertisement-based optimization applicable to subscription merging
[advertisement-based optimization, electronic publishing, covering algorithm, Merging, Subscriptions, MBD-based approach, subscription/advertisement covering, Jacobian matrices, binary decision diagrams, Boolean functions, Technology management, index data structure, Boolean variables, data structures, Logic, publish/subscribe systems, subscription/advertisement merging, Java, computer networks, Routing, Data structures, modified binary decision diagrams, publication routing, Middleware, telecommunication network routing]
Symmetrical Fairness in Infrastructure Access in Multi-hop Wireless Networks
25th IEEE International Conference on Distributed Computing Systems
None
2005
In this paper, we study the problem of providing fairness in multi-hop wireless infrastructure access. In such networks, it is well known that the use of current media access and transport protocols can result in severe unfairness and even starvation for flows originated from different numbers of hops away from a wired infrastructure point or gateway. In this paper, we study a different type of fairness that exists in such networks - flows initiated by nodes that are similar numbers of hops away from the gateway can experience significant unfairness, and such unfairness exists even for perfectly symmetrical node distribution and channel conditions. We denote such fairness as symmetrical fairness. We first provide a framework to characterize and measure symmetrical fairness. We then perform an extensive set of simulation experiments to quantify the causes of symmetrical unfairness. Finally, we develop and study a distributed routing algorithm that significantly improves the symmetrical fairness
[Transport protocols, media access, multihop wireless infrastructure access, wireless sensor networks, perfectly symmetrical node distribution, multihop wireless networks, Command and control systems, Intelligent networks, Wireless networks, wired infrastructure point, Spread spectrum communication, multi-access systems, IP networks, symmetrical fairness, Routing, Application software, radio access networks, Wireless sensor networks, distributed routing algorithm, mobile communication, transport protocols, telecommunication network routing, channel conditions, Internet, gateway]
End-to-End Fair Bandwidth Allocation in Multi-Hop Wireless Ad Hoc Networks
25th IEEE International Conference on Distributed Computing Systems
None
2005
The shared-medium multi-hop nature of wireless ad hoc networks poses fundamental challenges to the design of an effective resource allocation algorithm to maximize spatial reuse of spectrum, while maintaining basic fairness among multiple flows. When previously proposed scheduling algorithms have been shown to perform well in providing fair shares of bandwidth among single-hop wireless flows, they do not consider multi-hop flows with an end-to-end perspective when maximizing spatial reuse of spectrum. Instead, previous work attempts to break each multi-hop end-to-end flow into multiple single-hop flows for scheduling purposes. While this may be sufficient for maintaining basic fairness properties among single-hop subflows with respect to bandwidth, we show that, due to the intra-flow correlation between upstream and downstream hops, it may not be appropriate for maximizing spatial reuse of bandwidth. In this paper, we analyze the issue of increasing such spatial reuse of bandwidth from an end-to-end perspective of multi-hop flows. Through analysis and simulation results, we show that our proposed algorithm is able to appropriately distribute resources among multi-hop flows, so that end-to-end throughput may be maximized in wireless ad hoc networks, while still maintaining basic fairness across the multi-hop flows
[Algorithm design and analysis, wireless sensor networks, multiple flows, single-hop wireless flows, intra-flow correlation, Throughput, multiple single-hop flows, Mobile ad hoc networks, Intelligent networks, scheduling algorithms, resource allocation, multihop end-to-end flow, Spread spectrum communication, Bandwidth, resource allocation algorithm, multihop wireless ad hoc networks, computer networks, downstream hops, Ad hoc networks, Scheduling algorithm, bandwidth allocation, upstream hops, mobile communication, end-to-end fair bandwidth allocation, single-hop subflows, Channel allocation, Resource management, ad hoc networks]
FraNtiC: A Fractal Geometric Framework for Mesh-Based Wireless Access Networks
25th IEEE International Conference on Distributed Computing Systems
None
2005
The design of the access networks of next generation broadband wireless systems requires special attention in the light of changing network characteristics. In this paper, we present a mesh-based distributed radio access network (RAN) framework for future wireless systems. Using short, high bandwidth optical wireless links to interconnect the various network elements, we identify a generic fractal or self similar structure in the network. A mathematical model for the framework is presented and the benefits of its scale-invariant properties on robustness, reliability and flexibility analyzed. We focus on three design parameters - carrier-class network reliability, network exposure due to failure conditions and system cost. The dynamics of these parameters on our proposed architecture are studied and compared against existing access network topologies like tree/ring and square-grid. The generality and recursive nature of our framework lends itself to be applied in interconnecting various heterogeneous broadband wireless access networks of the future
[ring topology, wireless sensor networks, mesh-based distributed radio access network, access network topology, fractal geometric framework, square-grid topology, Optical fiber networks, Geometrical optics, broadband networks, Fractals, Next generation networking, network exposure, network characteristics, carrier-class network reliability, broadband wireless systems, Wireless networks, scale-invariant property, Bandwidth, broadband wireless access networks, mesh-based wireless access networks, Robustness, Mathematical model, bandwidth optical wireless links, generic fractal, Optical interconnections, computer networks, network topology, FraNtiC, radio access networks, Radio access networks, mobile communication, tree topology]
On Flow Marking Attacks in Wireless Anonymous Communication Networks
25th IEEE International Conference on Distributed Computing Systems
None
2005
This paper studies the degradation of anonymity in a flow-based wireless mix network under flow marking attacks, in which an adversary embeds a recognizable pattern of marks into wireless traffic flows by electromagnetic interference. We find that traditional mix technologies are not effective in defeating flow marking attacks, and it may take an adversary only a few seconds to recognize the communication relationship between hosts by tracking such artificial marks. Flow marking attacks utilize frequency domain analytical techniques and convert time domain marks into invariant feature frequencies. To counter flow marking attacks, we propose a new countermeasure based on digital filtering technology, and show that this filter-based countermeasure can effectively defend a wireless mix network from flow marking attacks
[wireless sensor networks, digital filtering technology, Telecommunication traffic, Frequency conversion, Digital filters, Wireless communication, Degradation, Electromagnetic interference, electromagnetic interference, anonymity, Communication networks, frequency domain analytical techniques, computer networks, filter-based countermeasure, Time domain analysis, flow marking attacks, wireless traffic flows, Pattern recognition, invariant feature frequencies, Frequency domain analysis, wireless anonymous communication networks, time domain marks, flow-based wireless mix network, mobile communication, telecommunication traffic]
Scalable, Server-Passive, User-Anonymous Timed Release Cryptography
25th IEEE International Conference on Distributed Computing Systems
None
2005
We consider the problem of sending messages into the future, commonly known as timed release cryptography. Existing schemes for this task either solve the relative time problem with uncontrollable, coarse-grained release time (time-lock puzzle approach) or do not provide anonymity to senders and/or receivers and are not scalable (server-based approach). Using a bilinear pairing on any Gap Diffie-Hellman group, we solve this problem by giving scalable, server-passive and user-anonymous timed release public-key encryption schemes allowing precise absolute release time specifications. Unlike the existing server-based schemes, the trusted time server in our scheme is completely passive - no interaction between it and the sender or receiver is needed; it is even not aware of the existence of a user, thus assuring the privacy of a message and the anonymity of both its sender and receiver. Besides, our scheme also has a number of desirable properties including a single form of update for all users, self-authenticated time-bound key updates, and key insulation, making it a scalable and appealing solution. It could also be easily generalized to a more general policy lock mechanism
[policy lock, key insulation, server-passive timed release cryptography, self-authenticated key updates, time-bound key updates, scalable timed release cryptography, Distributed computing, Delay, server-based approach, Privacy, relative time problem, public key cryptography, Cryptography, public-key encryption, Insulation, Government, computer networks, user-anonymous timed release cryptography, Global Positioning System, Atomic clocks, uncontrollable release time, Gap Diffie-Hellman group, time server, Public key, server-based schemes, Seals, bilinear pairing, data communication, time-lock puzzle, coarse-grained release time]
Anonymity vs. Information Leakage in Anonymity Systems
25th IEEE International Conference on Distributed Computing Systems
None
2005
Measures for anonymity in systems must be on one hand simple and concise, and on the other hand reflect the realities of real systems. Such systems are heterogeneous, as are the ways they are used, the deployed anonymity measures, and finally the possible attack methods. Implementation quality and topologies of the anonymity measures must be considered as well. We therefore propose a new measure for the anonymity degree, which takes into account possible heterogeneity. We model the effectiveness of single mixes or of mix networks in terms of information leakage and measure it in terms of covert channel capacity. The relationship between the anonymity degree and information leakage is described, and an example is shown
[anonymity systems, Channel capacity, computer networks, anonymity degree, Anonymity, Information retrieval, Routing, Entropy, Information technology, Covert Channels, Telecommunication network topology, Computer science, attack methods, covert channel capacity, Mix Networks, Network topology, security of data, information leakage, anonymity measures, data communication, mix networks]
eScience Meets eBusiness: Blueprint for Next Generation Grid Computing
25th IEEE International Conference on Distributed Computing Systems
None
2005
Summary form only given. Grid computing is increasingly being viewed as the next phase of distributed computing. Built on pervasive Internet standards, grid computing enables organizations to share computing and information resources across department and organizational boundaries in a secure, highly efficient manner. Grid computing originates in eScience and its early development was driven to a large extent by the requirements of large-scale computing and efficient sharing of huge datasets. eBusiness requirements led to the adoption of emerging Web services technologies - initially developed for distributed business application integration. Therefore grid computing can be applied to enterprise computing within and across organizations and pave the way for utility computing. I present major Grid computing projects for both eScience and eBusiness in Japan (including NaReGI and Business Grid) and several key Grid products and applications (including CyberGRIP and Resource Coordinator both developed by Fujitsu). The Global Grid Forum (GGF) is leading the standardization of grid computing. In particular, GGF has developed the Open Grid Services Architecture (OGSA) and is working throughout the industry to champion this "architectural blueprint" and the associated specifications that will enable the pervasive adoption of grid computing for eBusiness and eScience worldwide
[ebusiness, large-scale computing, grid computing, Standardization, Business Grid, enterprise computing, Distributed computing, distributed computing, open grid services architecture, Computer architecture, architectural blueprint, Resource Coordinator, Grid computing, electronic commerce, NaReGI, Pervasive computing, information resources, Information resources, Large scale integration, Web services technologies, distributed business application integration, Global Grid Forum, CyberGRIP, Web services, utility computing, grid products, Standards organizations, escience, Internet]
Using a Layered Markov Model for Distributed Web Ranking Computation
25th IEEE International Conference on Distributed Computing Systems
None
2005
The link structure of the Web graph is used in algorithms such as Kleinberg's HITS and Google's PageRank to assign authoritative weights to Web pages and thus rank them. Both require a centralized computation of the ranking if used to rank the complete Web graph. In this paper, we propose a new approach based on a Layered Markov Model to distinguish transitions among Web sites and Web documents. Based on this model, we propose two different approaches for computation of ranking of Web documents, a centralized one and a decentralized one. Both produce a well-defined ranking for a given Web graph. We then formally prove that the two approaches are equivalent. This provides a theoretical foundation for decomposing link-based rank computation and makes the computation for a Web-scale graph feasible in a decentralized fashion, such as required for Web search engines having a peer-to-peer architecture. Furthermore, personalized rankings can be produced by adapting the computation at both the local layer and the global layer. Our empirical results show that the ranking generated by our model is qualitatively comparable to or even better than the ranking produced by PageRank
[layered Markov model, Content based retrieval, search engines, link structure, Web-scale graph, Proposals, Distributed computing, Web graph, link-based rank computation, Computer architecture, Search engines, Web documents, distributed Web ranking, HITS, Google, Peer to peer computing, Service oriented architecture, Kleinberg, PageRank, Vectors, peer-to-peer architecture, Web search engines, Web pages, Markov processes, Internet, Web sites, Web search]
Comparison of Approaches to Service Deployment
25th IEEE International Conference on Distributed Computing Systems
None
2005
IT today is driven by the trend of increasing scale and complexity. Utility and grid computing models, PlanetLab, and traditional data centers, are reaching the scale of thousands of computers. Installed software consists of dozens of interdependent applications and services. As the complexity and scale of these systems continues to grow, it becomes increasingly difficult to administer and manage them. At the same time, the service deployment technologies are still based on scripts and configuration files with minimal ability to express dependencies, to document and to verify configurations. This results in hard-to-use and erroneous system configurations. Language- and model-based tools, such as SmartFrog and Radio, are proposed for addressing these deployment challenges, but it is unclear whether they are beneficial over traditional solutions. In this paper, we quantitatively compare manual, script-, language-, and model-based deployment solutions as a function of scale, complexity, and susceptibility to change. We also qualitatively compare them in terms of expressiveness and barrier to first use. We demonstrate that script-based solutions are well matched for large scale deployments, language-based for services of large complexity, and model-based for dynamic changes to the design. Finally, we offer a table summarizing rules of thumb regarding which solution to use in which case, subject to deployment needs
[information technology, change susceptibility, grid computing, utility programs, DP management, programming languages, software installation, Operating systems, model-based tool, interdependence, Grid computing, Fabrics, Large-scale systems, Web server, service deployment, Automation, grid computing model, Thumb, Data processing, utility computing model, Radio, Transaction databases, computer system administration, Application software, PlanetLab, data center, SmartFrog, script-based solution, information technology complexity, management of change, language-based tool]
Supporting Live Development of SOAP and CORBA Servers
25th IEEE International Conference on Distributed Computing Systems
None
2005
We present middleware for a server development environment that facilitates live development of SOAP and CORBA servers. As the underlying implementation platform, we use JPie, a tightly integrated programming environment for live software construction of Java applications. JPie provides dynamic classes whose signature and implementation can be modified at run time, with changes taking effect immediately upon existing instances of the class. We extend this model by automating the server deployment process, allowing developers to devote their full attention to the implementation of server logic. Moreover, the live development model enables the construction of server applications while they are running, connected, and communicating with test clients. Combined with our client development environment, these features facilitate the live, simultaneous construction of both the client and server applications
[JPie, Java, server logic implementation, Genetic mutations, Buildings, server deployment automation, server development environment, Aerodynamics, Simple object access protocol, Application software, CORBA server, Middleware, live software construction, Programming environments, SOAP server, live development support, Java application, file servers, software engineering, Logic, distributed object management, Testing, middleware, integrated programming]
Resilient Capacity-Aware Multicast Based on Overlay Networks
25th IEEE International Conference on Distributed Computing Systems
None
2005
The global deployment of IP multicast has been slow due to the difficulties related to heterogeneity, scalability, manageability, and lack of a robust inter-domain multicast routing protocol. Application-level multicast becomes a promising alternative. Many overlay multicast systems have been proposed in recent years. However, they are insufficient in supporting applications that require large-scale any-source multicast with highly varied host capacities and highly dynamic membership. In this paper, we propose two capacity-aware multicast systems that focus on host heterogeneity, dynamic membership, scalability, and any source multicast. We extend Chord and Koorde to be capacity-aware. We then embed implicit degree-varying multicast trees on top of the overlay network and develop multicast routines that automatically follow the trees to disseminate multicast messages. The implicit trees are well balanced with workload evenly spread across the network. We also perform extensive simulations to evaluate the proposed multicast systems
[application-level multicast, host heterogeneity, Peer to peer computing, Scalability, Laboratories, overlay network, Multicast protocols, Throughput, large-scale multicast, Information science, resilient capacity-aware multicast, interdomain multicast routing protocol, Technology management, Unicast, network operating systems, routing protocols, multicast communication, Robustness, Routing protocols, IP multicast deployment, IP networks, dynamic membership]
Semi-Probabilistic Content-Based Publish-Subscribe
25th IEEE International Conference on Distributed Computing Systems
None
2005
Mainstream approaches to content-based distributed publish-subscribe typically route events deterministically based on information collected from subscribers, and do so by relying on a tree-shaped overlay network. While this solution achieves scalability in fixed, large-scale settings, it is less appealing in scenarios characterized by high dynamicity, e.g., mobile ad hoc networks or peer-to-peer systems. At the other extreme, researchers in the related fields of multicast and group communication have successfully exploited probabilistic techniques that provide increased fault tolerance, resilience to changes, and yet are scalable. In this paper, we propose a novel approach where event routing relies on deterministic decisions driven by a limited view on the subscription information and, when this is not sufficient, resorts to probabilistic decisions performed by selecting links at random. Simulations show that the particular mix of deterministic and probabilistic decisions we put forth in this work is very effective at providing high event delivery and low overhead in highly dynamic scenarios, without sacrificing scalability
[electronic publishing, Scalability, Subscriptions, distributed processing, tree-shaped overlay network, Mobile communication, Mobile ad hoc networks, Fault tolerance, scalable system, multicast communication, distributed publish subscribe, Large-scale systems, mobile ad hoc network, fault tolerance, Peer to peer computing, Routing, semi-probabilistic content-based publish subscribe, group communication, deterministic event routing, Resilience, probabilistic decision, routing protocols, Publish-subscribe, decision trees, peer-to-peer system, ad hoc networks, deterministic decision]
Filter Based Directory Replication: Algorithms and Performance
25th IEEE International Conference on Distributed Computing Systems
None
2005
Directories have become an important component of the enterprise security and identity management middleware. This paper describes a novel filter based replication model for lightweight directory access protocol (LDAP) directories. Instead of replicating entire subtrees from a directory information tree (DIT), only entries matching a filter specification are replicated. Efficient algorithms for selecting such filters, keeping them synchronized with the master copy and for using them to answer directory queries have been proposed. Advantages of the filter based replication framework over existing subtree based mechanisms have been demonstrated for a real enterprise directory using real workloads
[telecommunication security, filter selection algorithm, identity management middleware, filter based replication model, Costs, Relational databases, business communication, information filtering, Information filtering, access protocols, query processing, directory information tree, Information filters, lightweight directory access protocol, Books, filter specification, middleware, replicated databases, Access protocols, filter based directory replication, enterprise security, Middleware, Identity management systems, Matched filters, Information security, directory query]
Enhancing Source-Location Privacy in Sensor Network Routing
25th IEEE International Conference on Distributed Computing Systems
None
2005
One of the most notable challenges threatening the successful deployment of sensor systems is privacy. Although many privacy-related issues can be addressed by security mechanisms, one sensor network privacy issue that cannot be adequately addressed by network security is source-location privacy. Adversaries may use RF localization techniques to perform hop-by-hop traceback to the source sensor's location. This paper provides a formal model for the source-location privacy problem in sensor networks and examines the privacy characteristics of different sensor routing protocols. We examine two popular classes of routing protocols: the class of flooding protocols, and the class of routing protocols involving only a single path from the source to the sink. While investigating the privacy performance of routing protocols, we considered the tradeoffs between location-privacy and energy consumption. We found that most of the current protocols cannot provide efficient source-location privacy while maintaining desirable system performance. In order to provide efficient and private sensor communications, we devised new techniques to enhance source-location privacy that augment these routing protocols. One of our strategies, a technique we have called phantom routing, has proven flexible and capable of protecting the source's location, while not incurring a noticeable increase in energy overhead. Further, we examined the effect of source mobility on location privacy. We showed that, even with the natural privacy amplification resulting from source mobility, our phantom routing techniques yield improved source-location privacy relative to other routing methods
[Energy consumption, Sensor phenomena and characterization, Sensor systems, sensor deployment, source mobility, Floods, Radio frequency, Privacy, sensor communication, single path routing, System performance, Routing protocols, source-location privacy, Protection, energy consumption, phantom routing, RF localization, hop-by-hop traceback, distributed sensors, flooding protocol, source separation, formal model, routing protocols, system performance, Imaging phantoms, data privacy, sensor routing protocol, sensor network routing]
Detecting Malicious Beacon Nodes for Secure Location Discovery in Wireless Sensor Networks
25th IEEE International Conference on Distributed Computing Systems
None
2005
Sensors' locations play a critical role in many sensor network applications. A number of techniques have been proposed recently to discover the locations of regular sensors based on a few special nodes called beacon nodes, which are assumed to know their locations (e.g., through GPS receivers or manual configuration). However, none of these techniques can work properly when there are malicious attacks, especially when some of the beacon nodes are compromised. This paper introduces a suite of techniques to detect and remove compromised beacon nodes that supply misleading location information to the regular sensors, aiming at providing secure location discovery services in wireless sensor networks. These techniques start with a simple but effective method to detect malicious beacon signals. To identify malicious beacon nodes and avoid false detection, this paper also presents several techniques to detect replayed beacon signals. This paper then proposes a method to reason about the suspiciousness of each beacon node at the base station based on the detection results collected from beacon nodes, and then revoke malicious beacon nodes accordingly. Finally, this paper provides detailed analysis and simulation to evaluate the proposed techniques. The results show that our techniques are practical and effective in detecting malicious beacon nodes
[Base stations, Protocols, Target tracking, signal detection, wireless sensor networks, GPS receiver, computer networks, Manuals, wireless sensor network, Routing, malicious beacon node detection, malicious attack, beacon signal detection, Global Positioning System, Intelligent networks, Wireless sensor networks, security of data, secure location discovery, Signal processing, sensor location, base station, Signal detection]
Location Privacy in Mobile Systems: A Personalized Anonymization Model
25th IEEE International Conference on Distributed Computing Systems
None
2005
This paper describes a personalized k-anonymity model for protecting location privacy against various privacy threats through location information sharing. Our model has two unique features. First, we provide a unified privacy personalization framework to support location k-anonymity for a wide range of users with context-sensitive personalized privacy requirements. This framework enables each mobile node to specify the minimum level of anonymity it desires as well as the maximum temporal and spatial resolutions it is willing to tolerate when requesting for k-anonymity preserving location-based services (LBSs). Second, we devise an efficient message perturbation engine which runs by the location protection broker on a trusted server and performs location anonymization on mobile users' LBS request messages, such as identity removal and spatio-temporal cloaking of location information. We develop a suite of scalable and yet efficient spatio-temporal cloaking algorithms, called CliqueCloak algorithms, to provide high quality personalized location k-anonymity, aiming at avoiding or reducing known location privacy threats before forwarding requests to LBS provider(s). The effectiveness of our CliqueCloak algorithms is studied under various conditions using realistic location data synthetically generated using real road maps and traffic volume data
[Data privacy, Roads, traffic volume data, Telecommunication traffic, CliqueCloak algorithm, information filtering, Engines, location-based service, personalized anonymization model, mobile computing, location privacy, request messages, Protection, location data, Pervasive computing, road traffic, k-anonymity model, location protection broker, location anonymization, location information sharing, Educational institutions, Application software, information services, road map, spatiotemporal cloaking, Computer science, identity removal, mobile node, mobile system, message perturbation engine, data privacy, Spatial resolution]
Location Estimation in Ad Hoc Networks with Directional Antennas
25th IEEE International Conference on Distributed Computing Systems
None
2005
With the development of location aware sensor applications, location determination has become an increasingly important middleware technology. Numerous current technologies for location determination of sensor nodes use the received signal strength from sensor nodes using omnidirectional antennas. However, an increasing number of sensor systems are now deploying directional antennas due to their advantages like energy conservation and better bandwidth utilization. In this paper, we present techniques for location determination in a sensor network with directional antennas under different kinds of deployment of the nodes. We show how the location estimation problem can be solved by measuring the received signal strength from just one or two anchors in a 2D plane with directional antennas. We implement our technique using Berkeley MICA2 sensor motes and show that it is up to three times more accurate than triangulation using omnidirectional antennas. We also perform Matlab simulations that show the accuracy of location determination with increasing node density
[node deployment, mathematics computing, ad-hoc network, Intelligent networks, mobile computing, Berkeley MICA2 sensor mote, directive antennas, Directional antennas, Bandwidth, received signal strength, directional antenna, Hardware, Computer networks, middleware, triangulation, Matlab simulation, location estimation, Routing, location aware sensor application, Ad hoc networks, location determination, Application software, Middleware, Global Positioning System, source separation, signal sources, omnidirectional antenna, sensor network, received signal angle, sensor node location, ad hoc networks]
Resilient Localization for Sensor Networks in Outdoor Environments
25th IEEE International Conference on Distributed Computing Systems
None
2005
The process of computing the physical locations of nodes in a wireless sensor network is known as localization. Self-localization is critical for large-scale sensor networks because manual or assisted localization is often impractical due to time requirements, economic constraints or inherent limitations of deployment scenarios. We have developed a service for reliably localizing wireless sensor networks in environments conducive to ranging errors by using a custom hardware-software solution for acoustic ranging and a family of self-localization algorithms. The ranging solution improves on previous work, extending the practical measurement range threefold (20-30m) while maintaining a distance-invariant median measurement error of about 1% of maximum range (33cm). The localization scheme is based on least squares scaling with soft constraints. Evaluation using ranging results obtained from sensor network field experiments shows that the localization scheme is resilient against large-magnitude ranging errors and sparse range measurements, both of which are common in large-scale outdoor sensor network deployments
[Measurement errors, wireless sensor networks, wireless sensor network, sonar, Environmental economics, self-localization algorithm, outdoor environment, Physics computing, least squares scaling, Acoustic sensors, Computer networks, Large-scale systems, hardware-software solution, middleware, least squares approximations, 20 to 30 m, node location computing, Maintenance, service development, ranging error, Wireless sensor networks, resilient localization, large-scale sensor network, Acoustic measurements, Time factors, sensor network deployment, acoustic ranging]
Rapid Development and Flexible Deployment of Adaptive Wireless Sensor Network Applications
25th IEEE International Conference on Distributed Computing Systems
None
2005
Wireless sensor networks (WSNs) are difficult to program and usually run statically-installed software limiting its flexibility. To address this, we developed Agilla, a new middleware that increases network flexibility while simplifying application development. An Agilla network is deployed with no pre-installed application. Instead, users inject mobile agents that spread across nodes performing application-specific tasks. Each agent is autonomous, allowing multiple applications to share a network. Programming is simplified by allowing programmers to create agents using a high-level language. Linda-like tuple spaces are used for inter-agent communication and context discovery. This preserves each agent's autonomy while providing a rich infrastructure for building complex applications, and marks the first time mobile agents and tuple spaces are used in a unified framework for WSNs. Our efforts resulted in an implementation for MICA2 motes and the development of several applications. The implementation consumes a mere 41.6KB of code and 3.59KB of data memory. An agent can migrate 5 hops in less than 1.1 seconds with 92% reliability. In this paper, we present Agilla and provide a detailed evaluation of its implementation, an empirical study of its overhead, and a case study demonstrating its use
[Adaptive systems, wireless sensor networks, Linda-like tuple space, interagent communication, microsensors, Mobile communication, high-level language, adaptive wireless sensor network application, autonomous agent, network flexibility, Mobile agents, wireless sensor network deployment, Fires, mobile agents, Agilla, middleware, network sharing, Application software, Middleware, High level languages, Programming profession, Wireless sensor networks, mobile agent, wireless sensor network development, 3.59 KB, Biomedical monitoring, MICA2 mote]
Controlling Gossip Protocol Infection Pattern Using Adaptive Fanout
25th IEEE International Conference on Distributed Computing Systems
None
2005
We propose and evaluate a model for controlling infection patterns defined over rounds or real time in a gossip-based protocol using adaptive fanout. We model three versions of gossip-based protocols: the synchronous protocol, the pseudosynchronous protocol and the asynchronous protocol. Our objective is to ensure that the members of a group receive a desired message within a bounded latency with very high probability. We argue that the most important parameter that controls the latency of message delivery is the fanout used during gossiping, i.e., the number of gossip targets chosen in a particular instance of gossip. We formally analyze the three protocols and provide expressions for fanout. We introduce the idea of using variable fanouts in different rounds in the synchronous protocol. We define fanout as a function of time for the asynchronous protocol such that an expected infection pattern is observed with high probability. For a better understanding of the theoretical model, we develop a pseudosynchronous protocol to highlight the modelling done in order to derive time dependent fanout. We show that our protocols generate Theta(n log n) messages, which is optimal for gossip protocols. We aim to use the gossiping mechanism for large-scale group communication with soft real time constraints. This would alleviate the dependence on tree-based deterministic protocols which usually lack scalability
[Algorithm design and analysis, Protocols, message passing, adaptive fanout, synchronous protocol, tree-based deterministic protocol, Scalability, pseudosynchronous protocol, gossip protocol infection pattern control, Adaptive control, group communication, Delay, Computer science, Programmable control, asynchronous protocol, gossip target, Multicast algorithms, bounded latency, soft real time constraint, groupware, message delivery, Robustness, Large-scale systems, protocols]
Robust Task Allocation for Dynamic Distributed Real-Time Systems Subject to Multiple Environmental Parameters
25th IEEE International Conference on Distributed Computing Systems
None
2005
Some distributed real-time systems interact with external environments that change dynamically, and it is necessary to take the external variables into account when performing task allocation. We developed an approximation algorithm for task allocation, and it finds allocations that are maximally robust against dynamic changes in multiple external variables. Such an algorithm will help to reduce expensive reallocations triggered by changes in unpredictable environments. The algorithm has a polynomial running time, and its robustness optimality is given by an approximation ratio, which equals 2.41 asymptotically, when workloads are large and workload independent utilization of tasks is insignificant
[Real time systems, Missiles, approximation theory, distributed processing, Radar tracking, task analysis, Distributed computing, Concurrent computing, Computer science, resource allocation, multiple environmental parameters, robust task allocation, Radar detection, real-time systems, approximation algorithm, Approximation algorithms, Robustness, Polynomials, unpredictable environment, dynamic distributed real-time systems]
Fine-Grain Adaptive Compression in Dynamically Variable Networks
25th IEEE International Conference on Distributed Computing Systems
None
2005
Despite voluminous previous research on adaptive compression, we found significant challenges when attempting to fully utilize both network bandwidth and CPU. We describe the fine-grain (FG) mixing strategy that compresses and sends as much data as possible, and then uses any remaining bandwidth to send uncompressed packets. Experimental measurements show that FG mixing achieves significant gains in effective throughput, particularly at higher network bandwidths. However, non-trivial interactions between system components and layers (e.g., compression algorithms and middleware settings such as block size and buffer size) have significant impact on the overall system performance. Finally, the trade-offs and performance profiles of FG mixing are measured, observed, and found to be consistent over a wide range of combinations of compression algorithms (GZIP, LZO, BZ1P2), workload compression ratios (from 1 to 4), and network bandwidth (from 0 to 400 Mbps)
[data compression, Adaptive systems, dynamically variable networks, computer networks, Throughput, fine-grain adaptive compression, bandwidth compression, network bandwidth, Middleware, Compression algorithms, fine-grain mixing strategy, Intelligent networks, Wireless networks, Bandwidth, Gain measurement, workload compression, Particle measurements, Robustness]
Lightweight Morphing Support for Evolving Middleware Data Exchanges in Distributed Applications
25th IEEE International Conference on Distributed Computing Systems
None
2005
Most systems must evolve as their missions or roles change and/or as they adapt to new execution environments. When evolving large distributed applications, it is particularly difficult to make changes to the data formats that underlie their components' communications, because such 'format evolution' can affect all or many application components. Prior approaches to the problem of implementing changes in the communications of a deployed system have relied upon ad-hoc solutions or on protocol negotiation to avoid message format mismatches. Unfortunately, such solutions tend to increase the complexity of application code. This paper presents a novel approach to the problem of data format evolution that combines meta-data about the data being exchanged with dynamic binary code generation to create a robust data exchange system that naturally supports application evolution. The idea is to specialize the communications of application components by dynamically generating the code that can automatically transform incoming data into forms that receiving components can understand. A realistic example in the context of publish/subscribe middleware is used to illustrate how this technique can be applied to enhance interoperability between different version of distributed applications
[Context, meta data, binary codes, object-oriented programming, application components, Wireless application protocol, protocol negotiation, Data structures, Educational institutions, evolving middleware data exchanges, distributed application, Middleware, Distributed computing, data format evolution, electronic data interchange, lightweight morphing support, Binary codes, binary code generation, Robustness, Hardware, Large-scale systems, protocols, middleware]
A Self-Organizing Lookup Service for Dynamic Ambient Services
25th IEEE International Conference on Distributed Computing Systems
None
2005
The provisioning of ambient services is gaining importance as users become more and more embedded in environments that are saturated with electronic devices. In our previous work, we have proposed the ad hoc service grid (ASG) approach as a means for deliberately setting up an infrastructure for providing ambient services at medium-sized locations like shopping malls. In this paper, we introduce a self-repairing lookup service architecture that is able to handle dynamic services in an AGS. These services can autonomously replicate and migrate within an ASG to optimize resource usage and message latency. Our lookup service is able to cope with the inconsistencies caused by service migrations efficiently by applying a lazy, request-driven update protocol. We discuss the architecture and the protocols employed by the lookup service. Moreover, we provide experimental results that show the efficiency and effectiveness of our approach
[Wireless LAN, self-organizing lookup service, Protocols, dynamic ambient services, lazy request-driven update protocol, Delay, Mobile ad hoc networks, Cellular networks, table lookup, mobile computing, Land mobile radio cellular systems, Web and internet services, Computer architecture, ad hoc service grid, self-repairing lookup service architecture, IP networks, ad hoc networks, protocols, Mobile computing]
ReDAL: Request Distribution for the Application Layer
25th IEEE International Conference on Distributed Computing Systems
None
2005
Modern application infrastructures are based on clustered, multi-tiered architectures, where request distribution occurs in two sequential stages: over a cluster of Web servers, and over a cluster of application servers. Much work has focused on strategies for distributing requests across a Web server cluster in order to improve overall throughput across the cluster. The strategies applied at the application layer are the same as those at the Web server layer, because it is assumed that they transfer directly. In this paper, we argue that the problem of distributing requests across an application server cluster is fundamentally different from the Web server request distribution problem, due to core differences in request processing in Web and application servers. We devise an approach for distributing requests across a cluster of application servers such that overall system throughput is enhanced, and load across the application servers is balanced. We compare the performance of our approach-with widely used industrial and recently proposed techniques from the literature experimentally in terms of throughput and response time performance, as well as resource utilization. Our experimental results show a significant improvement of up to nearly 80% in both throughput and response time, with a very low additional cost in terms of CPU overheads, 0.1% to 1.5%, on the Web server, and virtually no impact on CPU overheads on the application server
[workstation clusters, Java, Costs, request distribution, Service oriented architecture, Switches, Throughput, Routing, Web servers, application server cluster, Delay, ReDAL, Runtime, file servers, application infrastructures, application layer, Internet, clustered multitiered architectures, Resource management, Web server]
On Exploring Channel Allocation in the Diverse Data Broadcasting Environment
25th IEEE International Conference on Distributed Computing Systems
None
2005
In recent years, data broadcasting becomes a promising technique to design a mobile information system with power conservation, high scalability and high bandwidth utilization. However, prior research topics in data broadcasting are mainly based on the assumption that the disseminated data items are of the same size. We explore in this paper the problem of generating broadcast programs in a diverse data broadcasting environment, in which disseminated data items can be of different sizes. Given the broadcast database and the channel number, we propose algorithms DRP (dimension reduction partitioning) to perform the channel allocation for each data item. Moreover, a cost-diminishing selection mechanism is also used to help DRP achieve the local optimum with low complexity. With the capability of generating effective broadcast programs efficiently, the proposed mechanism can be practically used in a diverse data broadcasting environment
[broadcast database, Weather forecasting, Mobile communication, Multimedia communication, broadcast programs, Wireless communication, data broadcasting, Bandwidth, Management information systems, Broadcasting, cost-diminishing selection, channel allocation, mobile information system, computer networks, data dissemination, dimension reduction partitioning, broadcasting, mobile communication, power conservation, Channel allocation, Frequency, data communication, bandwidth utilization, broadcast channels, Mobile computing]
Time-Constrained Service on Air
25th IEEE International Conference on Distributed Computing Systems
None
2005
Data broadcasting is an efficient and highly scalable technique for delivering data to mobile clients in wireless environments. In this paper, we study the problem of scheduling broadcast data that are with an expected time within which the client is expecting to receive the data item. We analyze the problem and derive the minimum number of broadcast channels required for such a task. Also, we discuss the problems when the number of available channels is not enough. We propose novel solutions for both of the cases and the performance study indicates that our method is much better than the previous ones and performs very close to optimal
[service on air, broadcast data scheduling, Switches, Data engineering, Mobile communication, Information management, broadcasting, Computer science, Processor scheduling, Chaotic communication, mobile communication, data broadcasting, mobile clients, wireless environments, Bandwidth, Broadcasting, scheduling, data communication, broadcast channels, wireless LAN, data delivery, Mobile computing]
Handling Asymmetry in Power Heterogeneous Ad Hoc Networks: A Cross Layer Approach
25th IEEE International Conference on Distributed Computing Systems
None
2005
Power heterogeneous ad hoc networks are characterized by link layer asymmetry: the ability of lower power nodes to receive transmissions from higher power nodes but not vice versa. This not only poses challenges at the routing layer, but also results in an increased number of collisions at the MAC layer due to high power nodes initiating transmissions while low power communications are in progress. Previously proposed routing protocols for handling unidirectional links largely ignore MAC layer dependencies. In this paper, we propose a cross layer framework that effectively improves the performance of the MAC layer in power heterogeneous ad hoc networks. In addition, our approach seamlessly supports the identification and usage of unidirectional links at the routing layer. The framework is based on intelligently propagating low power MAC layer control messages to higher power nodes so as to preclude them from initiating transmissions while the low power communications are in progress within their sensing range. The integrated approach also constructs reverse tunnels to bridge unidirectional links thereby facilitating their effective usage at the routing layer. Extensive simulations are performed to study the proposed framework in various settings. The use of our framework improves the overall throughput of the power heterogeneous network by as much as 25% over traditional layered approaches. In summary, our framework offers a simple, yet effective and viable approach for media access control and to support routing in power heterogeneous ad hoc networks
[Cross layer design, telecommunication links, Throughput, access protocols, Degradation, Intelligent networks, handling asymmetry, power heterogeneous ad hoc network, Routing protocols, computer networks, link layer asymmetry, power nodes, MAC layer dependency, Ad hoc networks, unidirectional link handling, reverse tunnels, routing layer, media access control, power communications, Computer science, routing protocols, Bidirectional control, Media Access Protocol, ad hoc networks, Power engineering and energy, cross layer approach]
DISC: Dynamic Interleaved Segment Caching for Interactive Streaming
25th IEEE International Conference on Distributed Computing Systems
None
2005
Streaming media objects have become widely used on the Internet, and the demand of interactive requests to these objects has increased dramatically. Typical interactive requests include fast forward and direct jumps. Unfortunately, most of existing streaming proxies are designed for sequential accesses, and only a few solutions have been proposed to maintain additional data structures in the proxy to support some interactive operations (such as fast forward) other than jumps, which are among the most common interactive requests from the clients. Focusing on interactive accesses, in this paper, we present an analysis of streaming media workload collected from thousands of broadband users hosted by a major ISP. Our analysis shows that jump accesses (48%) and pauses (51%) are the dominant client interactive requests and that jump accesses often suffer serious delays due to slow buffering through the network. To support jump accesses effectively, we further propose a novel caching algorithm - DISC (dynamic interleaved segment caching), which trades cache performance for response time to client interactive requests. In this algorithm, segments of a media object are cached dynamically according to client access patterns. DISC can support direct jumps efficiently while ensuring timely prefetching of uncached segments for sequential accesses. Trace-driven simulations demonstrate that DISC outperforms other caching schemes significantly for interactive requests with only a small degradation in cache performance
[client-server systems, interactive streaming, media object streaming, Heuristic algorithms, Prefetching, interactive access, Educational institutions, Data structures, cache storage, Delay, Computer science, Degradation, sequential access, dynamic interleaved segment caching, Software libraries, Streaming media, interactive systems, data structures, direct jumps, Internet, multimedia communication]
Optimal Component Composition for Scalable Stream Processing
25th IEEE International Conference on Distributed Computing Systems
None
2005
Stream processing has become increasingly important with emergence of stream applications such as audio/video surveillance, stock price tracing, and sensor data analysis. A challenging problem is to provide optimal component composition in a distributed stream processing environment. The goal of optimal component composition is to achieve load balancing subject to multiple function, resource, and quality-of-service (QoS) constraints while composing stream applications. In this paper, we present an adaptive composition probing (ACP) approach to the problem. Different from previous work, ACP provides a new hybrid approach that combines distributed composition probing with coarse-grain global state management. Guided by the coarse-grain global state information, ACP selectively probes a subset of candidate components to discover an approximately optimal component composition. Further, ACP is self-tuning, which can adoptively adjust the number of probes to maintain a specified composition performance target (i.e., composition success rate) in a dynamic stream environment. While the optimal component composition problem is NP-hard, our ACP approach provides an adaptive polynomial approximation solution. We have conducted extensive simulation experiments to show the efficiency, scalability, and adaptability of the ACP approach by comparing with other alternative solutions
[Data analysis, load balancing, Scalability, Data security, audio/video surveillance, sensor data analysis, coarse-grain global state management, distributed stream processing, Quality of service, multimedia systems, stock price tracing, quality of service, adaptive composition probing, Analytical models, optimal component composition, polynomial approximation, distributed composition probing, Streaming media, Video surveillance, Load management, data communication, Polynomials, adaptive polynomial approximation, Probes]
Resource-Aware Distributed Stream Management Using Dynamic Overlays
25th IEEE International Conference on Distributed Computing Systems
None
2005
We consider distributed applications that continuously stream data across the network, where data needs to be aggregated and processed to produce a 'useful' stream of updates. Centralized approaches to performing data aggregation suffer from high communication overheads, lack of scalability, and unpredictably high processing workloads at central servers. This paper describes a scalable and efficient solution to distributed stream management based on (1) resource-awareness, which is middleware-level knowledge of underlying network and processing resources; (2) overlay-based in-network data aggregation; and (3) high-level programming constructs to describe data-flow graphs for composing useful streams. Technical contributions include a novel algorithm based on resource-aware network partitioning to support dynamic deployment of data-flow graph components across the network, where efficiency of the deployed overlay is maintained by making use of partition-level resource-awareness. Contributions also include efficient middleware-based support for component deployment, utilizing runtime code generation rather than interpretation techniques, thereby addressing both high performance and resource-constrained applications. Finally, simulation experiments and benchmarks attained with actual operational data corroborate this paper's claims
[Scalability, resource-awareness, data flow graphs, distributed processing, Educational institutions, data aggregation, Middleware, Distributed computing, middleware-level knowledge, Network servers, Runtime, Technology management, network partitioning, runtime code generation, data communication, Computer networks, distributed stream management, Resource management, Computer network management, middleware]
Equational Approach to Formal Analysis of TLS
25th IEEE International Conference on Distributed Computing Systems
None
2005
TLS has been formally analyzed with the OTS/CafeOBJ method. In the method, distributed systems are modeled as transition systems, which are written in terms of equations, and it is verified that the models have properties by means of equational reasoning. TLS is the latest version, or the successor of SSL, which is probably the most widely deployed security protocol. Among the results of the analysis are that pre-master secrets cannot be leaked, when a client has negotiated a cipher suite and security parameters with a server, the server has really agreed on them, and client cannot be identified if they do not send their certificates to servers
[Transport protocols, security protocol, secure sockets layer, equational reasoning, Information analysis, transport layer security, Information science, cipher suite, security, formal verification, OTS/CafeOBJ method, rewriting, National electric code, distributed systems, interactive theorem proving, theorem proving, protocols, Web server, Protection, algebraic specification, verification, rewriting systems, Data security, Specification languages, Equations, security of data, Information security, observational transition systems]
Distributed Approximation of Fixed-Points in Trust Structures
25th IEEE International Conference on Distributed Computing Systems
None
2005
We consider distributed algorithms for solving a range of problems in a framework for trust in large-scale distributed systems. The framework is based on the notion of trust structures; a set of 'trust-levels' with two distinct partial orderings. In the trust model, a global trust-state is defined as the least fixed-point of a collection of local policies of nodes in the network. We show that it is possible to compute the global trust-state using a simple, robust and totally asynchronous distributed-algorithm. We also consider a distributed notion of proof-carrying-requests as a means of approximating the least fixed-point, enabling sound reasoning about the global trust-state without computing the exact fixed-point. Our proof-carrying-request model is different than the notion of proof-of-compliance from traditional trust-management; in particular, all proofs are efficiently verifiable or easily rejected, but, in the worst case, may require as much communication as computing the actual trust-state itself
[proof-of-compliance, Laboratories, proof-carrying-requests, problem solving, Communication system security, trust structures, Distributed computing, least fixed-point, Authorization, formal verification, security of data, distributed algorithms, Collaboration, trust-management, Writing, distributed systems, Robustness, Large-scale systems, Mathematical model, Distributed algorithms]
Distributed Blinding for Distributed ElGamal Re-Encryption
25th IEEE International Conference on Distributed Computing Systems
None
2005
A protocol is given to take an ElGamal ciphertext encrypted under the key of one distributed service and produce the corresponding ciphertext encrypted under the key of another distributed service, but without the plaintext ever becoming available. Each distributed service comprises a set of servers and employs threshold cryptography to maintain its service private key. Unlike prior work, the protocol requires no assumptions about execution speeds or message delivery delays. The protocol also imposes fewer constraints on where and when various steps are performed, which can bring improvements in end-to-end performance for some applications (e.g., a trusted publish/subscribe infrastructure). Two new building blocks employed - a distributed blinding protocol and verifiable dual encryption proofs - could have uses beyond re-encryption protocols
[Performance evaluation, distributed blinding, distributed processing, ElGamal ciphertext, distributed service, cryptography, Time measurement, Distributed computing, Delay, private key, Cryptographic protocols, Computer science, ElGamal re-encryption, dual encryption, Numerical analysis, re-encryption protocol, Public key, Public key cryptography, Silicon, end-to-end performance, protocols]
Message from the General Chair
26th IEEE International Conference on Distributed Computing Systems
None
2006
Presents the welcome message from the conference proceedings.
[]
Program Co-chair's Message
26th IEEE International Conference on Distributed Computing Systems
None
2006
Presents the welcome message from the conference proceedings.
[]
Program and Organizing Committee
26th IEEE International Conference on Distributed Computing Systems
None
2006
Provides a listing of current committee members.
[]
Full Program Committee
26th IEEE International Conference on Distributed Computing Systems
None
2006
Provides a listing of current committee members.
[]
Analysis of the Message Waiting Time for the FioranoMQ JMS Server
26th IEEE International Conference on Distributed Computing Systems
None
2006
The Java messaging service (JMS) is a means to organize communication among distributed applications according to the publish/subscribe principle. If the subscribers install filter rules on the JMS server, JMS can be used as a message routing platform, but it is not clear whether its message throughput is sufficiently high to support large-scale systems. We perform measurements for the FioranoMQ JMS server and derive a simple model for its message processing time that takes message filters and the message replication grade into account. Then, we analyze the JMS server capacity and the message waiting time for various application scenarios. We show that the message waiting time is not an issue as long as the server throughput is sufficiently high. Finally, we assess the capacity of two different distributed JMS architectures whose objective is to increase the capacity of the JMS beyond the limit of a single server.
[Java, Filters, Computer architecture, Predictive models, Throughput, Routing, Message service, Time measurement, Large-scale systems, Application software]
Highly Available Long Running Transactions and Activities for J2EE Applications
26th IEEE International Conference on Distributed Computing Systems
None
2006
Today&#146;s business applications are typically built on top of middleware platforms such as J2EE and use transactions that have evolved into long running activities able to adapt to different circumstances. Specifications, such as the J2EE Activity Service, have arised for applications requiring that support. These applications also demand high availability to prevent financial losses and/or service level agreements (SLAs) violations due to service unavailability or crashes. Replication is a means to attain high availability but current middleware does not provide highly available transactions. In the advent of crashes, running transactions abort and the application is forced to re-execute them, what results in a loss of availability and transparency. Most approaches using J2EE consider the replication of either the application server or the database. This results in poor availability when the non-replicated tier crashes. This paper presents a novel J2EE replication support for both, application server and database layers providing highly available transactions and long running activities. Failure masking is transparent to client applications. A prototype has been implemented and evaluated.
[Availability, CADCAM, Web services, Councils, Computer aided manufacturing, Prototypes, Companies, Computer crashes, Transaction databases, Middleware]
A Bridging Framework for Universal Interoperability in Pervasive Systems
26th IEEE International Conference on Distributed Computing Systems
None
2006
We explore the design patterns and architectural tradeoffs for achieving interoperability across communication middleware platforms, and describe uMiddle, a bridging framework for universal interoperability that enables seamless device interaction over diverse platforms. The proliferation of middleware platforms that cater to specific devices has created isolated islands of devices with no uniform protocol for interoperability across these islands. This void makes it difficult to rapidly prototype pervasive computing applications spanning a wide variety of devices. We discuss the design space of architectural solutions that can address this void, and detail the trade-offs that must be faced when trying to achieve cross-platform interoperability. uMiddle is a framework for achieving such interoperability, and serves as a powerful platform for creating applications that are independent of specific underlying communication platforms.
[Pervasive computing, Protocols, Bluetooth, Prototypes, Educational institutions, Computer networks, Space exploration, Application software, Middleware, Global Positioning System]
A Loss and Queuing-Delay Controller for Router Buffer Management
26th IEEE International Conference on Distributed Computing Systems
None
2006
Active queue management (AQM) in routers has been proposed as a solution to some of the scalability issues associated with TCP&#146;s pure end-to-end approach to congestion control. However, beyond congestion control, controlling queues in routers is important because unstable router queues can cause poor application performance. Existing AQM schemes explicitly try to control router queues by probabilistically dropping (or marking) packets. We argue that while controlling router queues is important, this control needs to be tempered by a consideration of the overall lossrate at the router. Solely attempting to control queue length can induce loss-rates that have as negative an effect on application and network performance as the large queues that existing AQM schemes were trying to avoid. Thus controlling queue length without regard to loss-rate can be counterproductive. In this work we demonstrate that by jointly controlling queue length and loss-rate, both network and application performance are improved. We present a novel AQM design that attempts to simultaneously optimize queue length and loss-rate. Our algorithm, called loss and queuing delay control (LQD), is a control theoretic scheme that explicitly treats loss-rate as a control parameter. LQD is shown to provide stable control analytically and is evaluated empirically by comparing its performance against other control theoretic AQM designs (PI and REM). The results of evaluation in a laboratory testbed under realistic traffic mixes and loads show that LQD results in lower overall loss rates and that applications see lower average queue lengths than with PI or REM.
[Scalability, Laboratories, Traffic control, Performance loss, Communication system traffic control, Performance analysis, Queueing analysis, Delay, Design optimization, Testing]
The Confluent Capacity of the Internet: Congestion vs. Dilation
26th IEEE International Conference on Distributed Computing Systems
None
2006
Using shortest paths, the Internet scales very poorly with respect to congestion [2]. Two main reasons for using shortest paths are dilation (or delay) and size of routing tables. As the Internet grows, the small size of routing tables is important for scaling, but it does not require shortest paths. As long as the paths are confluent, the routing table size is unchanged. In this paper we study the confluent capacity of the Internet. We use the preferential attachment model [5] for the Internet, and all-pair uniform demand for the traffic pattern. Our main theoretical result is that the confluent congestion1 is within a logarithmic factor of the optimal splittable congestion and can be achieved using a simple randomized and distributed scheme called Locally Independent Rounding Algorithm (LIRA). We reinforce this result experimentally by employing simulations to demonstrate that for almost all instances the confluent congestion is (nearly) equal to the splittable congestion. Thus we conclude that the Internet scales well using confluent paths. We combine known results on expanders and the expansion properties of the preferential attachment model to show that for almost all Internet-like networks, we can find a confluent flow that simultaneously achieves O(log n)- approximate congestion and O(1)-approximate dilation. We confirm, using simulations, the intuition that confluence does not come at the cost of dilation.
[Costs, Engineering profession, Computational modeling, Laboratories, Traffic control, Routing, Internet, IP networks, Bioinformatics, Delay]
FastFlow: A Framework for Accurate Characterization of Network Traffic
26th IEEE International Conference on Distributed Computing Systems
None
2006
This paper proposes a new measurement architecture and associated traffic estimation algorithm called FastFlow that uses the heavy-tailed nature of Internet traffic in order to distinguish packets belonging to short lived flows (SLFs) and long lived flows (LLFs). While complete information is stored for SLFs, only partial information related to LLFs is collected using systematic sampling. The absence of data points in LLFs is approximated using a likelihood function defined over the coupon collector problem and the distribution of underlying traffic estimated using the non-parametric Parzen window technique. We validate the performance of our approach using traffic traces collected from our lab and observe that the estimated statistics match the observed traces with high accuracy.
[Computer science, Knowledge engineering, Protocols, Statistical distributions, Telecommunication traffic, Computer architecture, Bandwidth, Traffic control, Sampling methods, IP networks]
Autonomic Management of Stream Processing Applications via Adaptive Bandwidth Control
26th IEEE International Conference on Distributed Computing Systems
None
2006
We present a novel autonomic control system for high performance stream processing systems. The system uses bandwidth controls on incoming or outgoing streams to achieve a desired resource utilization balance among a set of concurrently executing stream processing tasks. We show that CPU prioritization and allocation mechanisms in schedulers and virtual machine managers are not sufficient to control such I/O-centric applications, and present an autonomic bandwidth control system that adaptively adjusts incoming and outgoing traffic rates to achieve system management goals. The system dynamically learns the bandwidth rate necessary to meet the system management goals using stochastic nonlinear optimization, and detects changes in the stream processing applications that require bandwidth adjustment. Our prototype Linux implementation is lightweight, has low overhead, and is capable of effectively managing stream processing applications.
[Programmable control, Linux, Stochastic systems, Prototypes, Bandwidth, Control systems, Virtual machining, Large-scale systems, Resource management, Adaptive control]
SysProf: Online Distributed Behavior Diagnosis through Fine-grain System Monitoring
26th IEEE International Conference on Distributed Computing Systems
None
2006
Runtime monitoring is key to the effective management of enterprise and high performance applications. To deal with the complex behaviors of today&#146;s multi-tier applications running across shared platforms, such monitoring must meet three criteria: (1) fine granularity, including being able to track the resource usage of specific application behaviors like individual client-server interactions, (2) real-time response, referring to the monitoring system&#146;s ability to both capture and analyze currently needed monitoring information with the delays required for online management, and (3) enterprise-wide operation, which means that the monitoring information captured and analyzed must span across the entire software stack and set of machines involved in request generation, request forwarding, service provision, and return. This paper presents the SysProf system-level monitoring toolkit, which provides a flexible, low overhead framework for enterprise-wide monitoring. The toolkit permits the capture of monitoring information at different levels of granularity, ranging from tracking the system-level activities triggered by a single system call, to capturing the client-server interactions associated with certain request classes, to characterizing the server resources consumed by sets of clients or client behaviors. The paper demonstrates the efficacy of SysProf by using it to manage two different enterprise applications: (1) detecting performance bottlenecks in a high performance shared network file service, and (2) enforcing service level agreements in a multi-tier auctioning web site.
[Condition monitoring, Runtime, Operating systems, High performance computing, Educational institutions, Performance analysis, Resource management, Standards development, Middleware, Information analysis]
A Hierarchical Optimization Framework for Autonomic Performance Management of Distributed Computing Systems
26th IEEE International Conference on Distributed Computing Systems
None
2006
This paper develops a scalable online optimization framework for the autonomic performance management of distributed computing systems operating in a dynamic environment to satisfy desired quality-ofservice objectives. To efficiently solve the performance management problems of interest in a distributed setting, we develop a hierarchical structure where a highlevel limited-lookahead controller manages interactions between lower-level controllers using forecast operating and environment parameters. We develop the overall control structure, and as a case study, show how to efficiently manage the power consumed by a computer cluster. Using workload traces from the Soccer World Cup 98 web site, we show via simulations that the proposed method is scalable, has low run-time overhead, and adapts quickly to time-varying workload patterns.
[Time varying systems, Power system management, Distributed control, Nonlinear control systems, Control systems, Hardware, Distributed computing, Environmental management, Energy management, Quality management]
Loud and Clear: Human-Verifiable Authentication Based on Audio
26th IEEE International Conference on Distributed Computing Systems
None
2006
Secure pairing of electronic devices that lack any previous association is a challenging problem which has been considered in many contexts and in various flavors. In this paper, we investigate the use of audio for human-assisted authentication of previously un-associated devices. We develop and evaluate a system we call Loud-and-Clear (L&amp;C) which places very little demand on the human user. L&amp;C involves the use of a text-to-speech (TTS) engine for vocalizing a robust-sounding and syntactically-correct (English-like) sentence derived from the hash of a device&#146;s public key. By coupling vocalization on one device with the display of the same information on another device, we demonstrate that L&amp;C is suitable for secure device pairing (e.g., key exchange) and similar tasks. We also describe several common use cases, provide some performance data for our prototype implementation and discuss the security properties of L&amp;C.
[Data security, Authentication, Humans, Public key, Prototypes, Information security, Displays, Robustness, Speech synthesis, Engines]
Design and Performance Evaluation of a Proxy-based Java Rewriting Security System
26th IEEE International Conference on Distributed Computing Systems
None
2006
Binary rewriting techniques have been developed to allow users to enforce security policies directly on mobile code. However, the performance overheads incurred for improved security, particularly in large organizations with many end-hosts, accentuate the inherent challenges of code rewriting and limit the rate of improvement in these systems. We integrate a binary code rewriter with a web caching proxy and build the security system called PBJARS, a Proxy-based JAva Rewriting System. PBJARS compliments existing JVM security mechanisms by placing another line of security defense in the code path code associated with code downloads. It gives system administrators centralized security control at the level of administrative domains at proxy servers. We evaluated PB-JARS using real Java binary traffic models derived from analyzing real web trace records. Our experimental results show that the overhead added by binary rewriting can be significantly amortized by web caching and PB-JARS adds negligible performance impact on proxy servers.
[Computer science, Java, Design engineering, Binary codes, Virtual machining, Power system security, Computer security, Environmental management, Mobile computing, Centralized control]
Store, Forget, and Check: Using Algebraic Signatures to Check Remotely Administered Storage
26th IEEE International Conference on Distributed Computing Systems
None
2006
The emerging use of the Internet for remote storage and backup has led to the problem of verifying that storage sites in a distributed system indeed store the data; this must often be done in the absence of knowledge of what the data should be. We use m/n erasure-correcting coding to safeguard the stored data and use algebraic signatures hash functions with algebraic properties for verification. Our scheme primarily utilizes one such algebraic property: taking a signature of parity gives the same result as taking the parity of the signatures. To make our scheme collusionresistant, we blind data and parity by XORing them with a pseudo-random stream. Our scheme has three advantages over existing techniques. First, it uses only small messages for verification, an attractive property in a P2P setting where the storing peers often only have a small upstream pipe. Second, it allows verification of challenges across random data without the need for the challenger to compare against the original data. Third, it is highly resistant to coordinated attempts to undetectably modify data. These signature techniques are very fast, running at tens to hundreds of megabytes per second. Because of these properties, the use of algebraic signatures will permit the construction of large-scale distributed storage systems in which large amounts of storage can be verified with minimal network bandwidth.
[Knowledge engineering, Peer to peer computing, Bandwidth, Data engineering, Systems engineering and theory, Internet, Large-scale systems, IP networks, Distributed computing, Protection]
WhoPay: A Scalable and Anonymous Payment System for Peer-to-Peer Environments
26th IEEE International Conference on Distributed Computing Systems
None
2006
An electronic payment system ideally should provide security, anonymity, fairness, transferability and scalability. Existing payment schemes often lack either anonymity or scalability. In this paper we propose WhoPay, a peer-topeer payment system that provides all the above properties. For anonymity, we represent coins with public keys; for scalability, we distribute coin transfer load across all peers, rather than rely on a central entity such as the broker. This basic version of WhoPay is as secure and scalable as existing peer-to-peer payment schemes, while providing a much higher level of user anonymity. We also introduce the idea of real-time double spending detection by making use of distributed hash tables (DHT). Simulation results show that the majority of the system load is handled by the peers under typical peer availability, indicating that WhoPay should scale well.
[Computer science, Availability, Costs, Peer to peer computing, Scalability, Aggregates, Laboratories, Public key, Credit cards, Computer security]
Elastic Routing Table with Provable Performance for Congestion Control in DHT Networks
26th IEEE International Conference on Distributed Computing Systems
None
2006
Distributed hash table (DHT) networks based on consistent hashing functions have an inherent load balancing problem. The problem becomes more severe due to the heterogeneity of network nodes and the non-uniform and timevarying file popularity. Existing DHT load balancing algorithms are mainly focused on the issues caused by node heterogeneity. To deal with skewed lookups, this paper presents an elastic routing table (ERT) mechanism for query load balancing, based on the observation that high degree nodes tend to experience more traffic load. The mechanism allows each node to have a routing table of variable size corresponding to its capacity. The indegree and outdegree of the routing table can also be adjusted dynamically in response to the change of file popularity and network churn. Theoretical analysis proves the routing table degree is bounded. The ERT mechanism facilitates locality-aware randomized query forwarding to further improve lookup efficiency. By relating query forwarding to a supermarket customer service model, we prove a 2-way randomized query forwarding policy leads to an exponential improvement in query processing time over random walking. Simulation results demonstrate the effectiveness of the ERT mechanism and its related query forwarding policy for congestion and query load balancing. In comparison with the existing "virtual-server"-based load balancing algorithm and other routing table control approaches, the ERT-based congestion control protocol yields significant improvements in query lookup efficiency.
[Intelligent networks, Peer to peer computing, Query processing, Intrusion detection, Telecommunication traffic, Routing, Load management, Computer networks, Customer service, Distributed computing]
Computing in the Presence of Timing Failures
26th IEEE International Conference on Distributed Computing Systems
None
2006
Timing failures refer to a situation where the environment in which a system operates does not behave as expected regarding the timing assumptions, that is, the timing constraints are not met. In the immense body of work on the designing fault-tolerant systems, the type of failures that are usually considered are, process failures, link failures, messages loss and memory failures; and it is usually (implicitly) assumed that there are no timing failures. In this paper we investigate the ability to recover automatically from transient timing failures. We introduce and formally define the concept of algorithms that are resilient to timing failures, and demonstrate the importance of the new concept by presenting consensus and mutual exclusion algorithms, using atomic registers only, that are resilient to timing failures.
[Algorithm design and analysis, Process design, Costs, Fault tolerant systems, Resumes, Nominations and elections, Robustness, Timing, Safety, Distributed computing]
Failure classification and analysis of the Java Virtual Machine
26th IEEE International Conference on Distributed Computing Systems
None
2006
This paper presents a failure analysis of the Java Virtual Machine providing useful insights into the nature of reported failures and to improve the understanding of its dependability aspects. Failure data is extracted from publicly available bug databases, where developers and users of Java applications usually submit failures/bugs. Presented results clearly indicate that much more efforts have still to be done in order to improve the dependability of the JVM. In particular, the conducted analysis revealed that i) builtin error detection mechanism are characterized by a low coverage; ii) the JVM does not achieve the same levels of dependability across different platforms iii) developers have to pursue a tradeoff between performance and reliability. Finally, code fragments reproducing failures submitted in bug database are injected into Java Applications. Preliminary results show that often these faults could be removed changing the environment of the JVM.
[Java, Failure Diagnosis, Dependability, Laboratories, Virtual machining, Data mining, Failure Analysis, Programming profession, Java Virtual Machine, Fault tolerance, Databases, Computer bugs, Failure analysis, Performance analysis]
Efficient Incremental Optimal Chain Partition of Distributed Program Traces
26th IEEE International Conference on Distributed Computing Systems
None
2006
An important problem in distributed systems is observation of global properties of distributed computations. What makes this problem difficult is that events in the computation can be concurrent, i.e. the relation between events forms a partial order, not a total order. One of the fundamental parameters of a partial order is the width, which corresponds to the maximum number of mutually incomparable elements. For example, a process-time diagram that shows this partial order decomposition in minimum number of chains can be very useful in monitoring or debugging such computations. In this paper, we present an incremental algorithm to compute the optimal chain partition. We compare our algorithm with existing chain reduction algorithms. From a practical point of view, performance evaluation shows that our approach achieves up to 90% run-time improvement over the previously known algorithms.
[Concurrent computing, Computer science, Visualization, Runtime, Debugging, Partitioning algorithms, Distributed computing, Monitoring, Clocks, Testing]
An empirical evaluation of work stealing with parallelism feedback
26th IEEE International Conference on Distributed Computing Systems
None
2006
A-STEAL is a provably good adaptive work-stealing thread scheduler that provides parallelism feedback to a multiprocessor job scheduler. A-STEAL uses a simple multiplicative-increase, multiplicative-decrease algorithm to provide continual parallelism feedback to the job scheduler in the form of processor requests. Although jobs scheduled by A-STEAL can be shown theoretically to complete in near-optimal time asymptotically while utilizing at least a constant fraction of the allotted processors, the constants in the analysis leave it open on whether A-STEAL works well in practice. This paper confirms with simulation studies that A-STEAL performs well when scheduling adaptively parallel work-stealing jobs on large-scale multiprocessors. Our studies monitored the behavior of A-STEAL on a simulated multiprocessor system using synthetic workloads. We measured the completion time and waste of A-STEAL on over 2300 job runs using a variety of processor availability profiles. Linear-regression analysis indicates that ASTEAL provides almost perfect linear speedup. In addition, A-STEAL typically wasted less than 20% of the processor cycles allotted to the job. We compared A-STEAL with the ABP algorithm, an adaptive work-stealing thread scheduler developed by Arora, Blumofe, and Plaxton which does not employ parallelism feedback. On moderately to heavily loaded large machines with predetermined availability profiles, A-STEAL typically completed jobs more than twice as quickly, despite being allotted the same or fewer processors on every step, while wasting only 10% of the processor cycles wasted by ABP. We compared the utilization of A-STEAL and ABP when many jobs with varying characteristics are using the same multiprocessor. These experiments provide evidence that A-STEAL consistently provides higher utilization than ABP for a variety of job mixes.
[Availability, Adaptive scheduling, Processor scheduling, Feedback, Time measurement, Large-scale systems, Yarn, Scheduling algorithm, Monitoring, Multiprocessing systems]
Load Unbalancing to Improve Performance under Autocorrelated Traffic
26th IEEE International Conference on Distributed Computing Systems
None
2006
Size-based policies have been shown to successfully balance load and improve performance in homogeneous cluster environments where a dispatcher assigns a job to a server strictly based on the job size. While the success of size-based policies is based on separating jobs to different servers according to their sizes by avoiding the unfavorable performance effects of having short jobs been stuck behind long jobs, we show that their effectiveness quickly deteriorates in the presence of job arrivals that are characterized by correlation in their dependence structure. We propose a new policy that still strives to separate jobs according to their sizes, but this separation is biased by the effort to reduce the performance loss due to autocorrelation. As a result, not all servers are equally utilized (i.e., the load in the system becomes unbalanced) but the performance benefits of this load unbalancing are significant. The proposed policy can be used on-line, i.e., it does not assume any knowledge neither of the correlation structure of the arrival stream, nor of the job size distribution in the system. Via detailed trace-driven simulation we quantify the performance benefits of the proposed policy and we show that it can effectively self adjust its configuration parameters to improve performance under continuously changing workload conditions.
[load balancing, self adaptive policies., autocorrelated arrivals, Educational institutions, highly variable service times, Computer science, Network servers, System performance, Stochastic systems, Load management, Performance loss, Dispatching, Autocorrelation, Web server]
ParRescue: Scalable Parallel Algorithm and Implementation for Biclustering over Large Distributed Datasets
26th IEEE International Conference on Distributed Computing Systems
None
2006
Biclustering refers to simultaneously capturing correlations present among subsets of attributes (columns) and records (rows). It is widely used in data mining applications including biological data analysis, financial forecasting, and text mining. Biclustering algorithms are significantly more complex compared to the classical one dimensional clustering techniques, particularly those requiring multiple computing platforms for large and distributed data sets. In this paper, we develop an efficient scalable algorithm, referred to as ParRescue(Parallel Residue Co-clustering), that is capable of performing biclustering on extremely large or geographically distributed data sets. ParRescue divides the cluster tasks among processors with minimal communication costs thus making it scalable over large number of computing nodes. The proposed implementation is based on an existing sequential approach that has been modified for amenable parallel implementation. The proposed Par- Rescue algorithm has been implemented using MPI and the performance results are reported based on executions on a 64 node Linux PC cluster connected over 100 Mbits links. The experimental results show scalable performance with near linear speedups across different data and machine sizes compared to the modified sequential algorithm.
[Computer science, Text mining, Data analysis, Costs, Clustering algorithms, Biology computing, Data mining, Application software, Parallel algorithms, Distributed computing]
A Semantic Overlay for Self- Peer-to-Peer Publish/Subscribe
26th IEEE International Conference on Distributed Computing Systems
None
2006
Publish/Subscribe systems provide a useful platform for delivering data (events) from publishers to subscribers in an anonymous fashion in distributed networks. In this paper, we promote a novel design principle for self-. dynamic and reliable content-based publish/subscribe systems and perform a comparative analysis of its probabilistic and deterministic implementations. More specifically, we present a generic content-based publish/subscribe system, called DPS (Dynamic Publish/Subscribe). DPS combines classical content-based filtering with self-. (self-organizing, selfconfiguring, and self-healing) subscription-driven clustering of subscribers. DPS gracefully adapts to failures and changes in the system while achieving scalable events delivery. DPS includes a variety of fault-tolerant deterministic and probabilistic content-based publication/subscription schemes. These schemes are targeted toward scalability, and aim at reducing and distributing the number of messages exchanged. Reliability and scalability of our system are shown through analytical and experimental evaluation.
[Computer science, Fault tolerance, Filters, Filtering, Peer to peer computing, Scalability, Subscriptions, Telecommunications, Performance analysis, Research and development]
PastryStrings: A Comprehensive Content-Based Publish/Subscribe DHT Network
26th IEEE International Conference on Distributed Computing Systems
None
2006
In this work we propose and develop a comprehensive infrastructure, coined PastryStrings, for supporting rich queries on both numerical (with range, and comparison predicates) and string attributes, (accommodating equality, prefix, suffix, and containment predicates) over DHT networks utilising prefix-based routing. As event-based, publish/ subscribe information systems are a champion application class, we formulate our solution in terms of this environment.
[Network topology, Peer to peer computing, Buildings, Intrusion detection, Routing, Computer networks, Equal opportunities, Large-scale systems, Informatics, Information systems]
Utility Optimization for Event-Driven Distributed Infrastructures
26th IEEE International Conference on Distributed Computing Systems
None
2006
Event-driven distributed infrastructures are becoming increasingly important for information dissemination and application integration. We examine the problem of optimal resource allocation for such an infrastructure composed of an overlay of nodes. Resources, like CPU and network bandwidth, are consumed by both message flows and message consumers; therefore, we consider both rate control for flows and admission control for consumers. This makes the optimization problem difficult because the objective function is nonconcave and the constraint set is nonconvex. We present LRGP (Lagrangian Rates, Greedy Populations), a scalable and efficient distributed algorithm to maximize the total system utility. The key insight of our solution involves partitioning the optimization problem into two types of subproblems: a greedy allocation for consumer admission control and a Lagrangian allocation to compute the flow rates, and linking the subproblems in a manner that allows tradeoffs between consumer admission and flow rates while satisfying the nonconvex constraints. LRGP allows an autonomic approach to system management where nodes collaboratively optimize aggregate system performance. We evaluate the quality of results and convergence characteristics under various workloads.
[Constraint optimization, Aggregates, System performance, Admission control, Bandwidth, Collaborative work, Resource management, Distributed algorithms, Joining processes, Lagrangian functions]
Controlling Quality of Service in Multi-Tier Web Applications
26th IEEE International Conference on Distributed Computing Systems
None
2006
The need for service differentiation in Internet services has motivated interest in controlling multi-tier web applications. This paper describes a tier-to-tier (T2T) management architecture that supports decentralized actuator management in multi-tier systems, and a testbed implementation of this architecture using commercial software products. Based on testbed experiments and analytic models, we gain insight into the value of coordinated exploitation of actuators on multiple tiers, especially considerations for control efficiency and control granularity. For control efficiency, we show that more effective utilization of tiers can be achieved by using actuators on the bottleneck tier rather than only using actuators on the entry tier. For granularity of control (the ability to achieve a wide range of service level objectives) we show that a fine granularity of control can be achieved through a coordinated, cross-tier exploitation of coarse grained actuators (e.g., multiprogramming level), an approach that can greatly reduce controllerinduced variability.
[Actuators, Network servers, Databases, Web and internet services, Quality of service, Computer architecture, Routing, Web server, Yarn, Delay]
File System Support for Collaboration in theWide Area
26th IEEE International Conference on Distributed Computing Systems
None
2006
We describe the design, implementation, and performance of MFS, a new file system designed to support efficient widearea collaboration. MFS is structured around the twin abstractions of lightweight sessions and snapshots, along with a highly configurable capability-based security architecture. Sessions simplify and clarify collaborative semantics. Snapshots allow atomic access to arbitrary collections of files, and allow sharing to be defined in a simple and expressive fashion. MFS&#146;s security architecture is a layered system that allows diverse usage scenarios. Pure capability-based access allows clients to access data without needing expensive public key or authentication servers, or complicated administration. However, MFS&#146;s capabilities can also be watermarked, allowing a range of services to be added on a per-mount basis, up to and including traditional user authentication based on passwords or public keys. Basing the system around the use of immutable snapshots enables the underlying system to use several performance optimizations aggressively. Performance results from our MFS prototype show that, far from adding overhead, the use of snapshots allows the system to perform comparably to NFS in the local-area case and significantly outperform existing systems in wide-area environments.
[Computer science, File systems, Data security, Atomic layer deposition, Collaboration, Prototypes, Public key, Authentication, Collaborative work, Educational institutions]
A Secure and Efficient Large Scale Distributed System for Data Sharing
26th IEEE International Conference on Distributed Computing Systems
None
2006
In this paper we consider a large distributed system in which data is shared among several users. Specifically, we present a secure adaptive algorithm for data fragment allocation on multiple nodes of the system. The algorithm handles (replicated) data fragments, stored by nodes without the need of encryption, in such a way to ease information sharing. Data confidentiality is guaranteed in the presence of passive and active attacks, and fragments are dynamically reallocated/replicated in the system to converge, under assumptions of regularity of the read-write activity, to an allocation that provably guarantees highest performance in terms of network load.
[Availability, Peer to peer computing, Data security, Adaptive algorithm, Globalization, Collaboration, Robustness, Large-scale systems, Cryptography, Remuneration]
Distributed Computing for Efficient Hyperspectral Imaging Using Fully Heterogeneous Networks of Workstations
26th IEEE International Conference on Distributed Computing Systems
None
2006
Hyperspectral imaging is a new technique which has become increasingly important in many remote sensing applications, including automatic target recognition for military and defense/security deployment, risk/hazard prevention and response including wild land fire tracking, biological threat detection, monitoring of oil spills and other types of chemical contamination, etc. Hyperspectral imaging applications generate massive volumes of data and require timely responses for swift decisions which depend upon high computing performance of algorithm analysis. Although most currently available parallel processing strategies for hyperspectral image analysis assume homogeneity in the computing platform, heterogeneous networks of workstations represent a very promising cost-effective solution expected to play a major role in the design of highperformance computing platforms for many on-going and planned remote sensing missions. This paper explores innovative techniques for mapping hyperspectral analysis algorithms onto heterogeneous networks of workstations available at NASA&#146;s Goddard Space Flight Center and University of Maryland. Experimental results reveal that heterogeneous networks of workstations represent a source of computational power that is both accessible and applicable in hyperspectral imaging studies.
[Algorithm design and analysis, Concurrent computing, Image analysis, Hyperspectral sensors, Military computing, Computer networks, Workstations, Distributed computing, Remote monitoring, Hyperspectral imaging]
On Scheduling Expansive and Reductive Dags for Internet-Based Computing
26th IEEE International Conference on Distributed Computing Systems
None
2006
Earlier work has developed the underpinnings of a theory of scheduling computations having intertask dependencies - modeled via dags - for Internet-based computing. The goal of the schedules produced is to render tasks eligible for execution at the maximum possible rate. This goal aims: (a) to utilize remote clients&#146; computational resources well, by always having work to allocate to an available client; (b) to lessen the likelihood of the "gridlock" that ensues when a computation stalls for lack of eligible tasks. The dags handled by the theory thus far are those that can be constructed from a given collection of bipartite building-block dags via the operation of dagcomposition. The current paper extends the range of applicability of the theory by significantly expanding the repertoire of building-block dags that the scheduling algorithms can handle. Thereby, the theory can now schedule large classes of "expansive" and "reductive" dags optimally.
[Computer science, Processor scheduling, Computerized monitoring, Optimal scheduling, Grid computing, Internet, Resource management, Scheduling algorithm]
Reputation-Based Scheduling on Unreliable Distributed Infrastructures
26th IEEE International Conference on Distributed Computing Systems
None
2006
This paper presents a design and analysis of scheduling techniques to cope with the inherent unreliability and instability of worker nodes in large-scale donation-based distributed infrastructures such as P2P and Grid systems. In particular, we focus on nodes that execute tasks via donated computational resources and may behave erratically or maliciously. We present a model in which reliability is not a binary property but a statistical one based on a node&#146;s prior performance and behavior. We use this model to construct several reputation-based scheduling algorithms that employ estimated reliability ratings of worker nodes for efficient task allocation. Through simulation of a BOINC-like distributed computing infrastructure, we demonstrate that our algorithms can significantly improve throughput, while maintaining a very high success rate of task completion.
[Computer science, Processor scheduling, Peer to peer computing, Computational modeling, Redundancy, Throughput, Large-scale systems, Maintenance, Distributed computing, Scheduling algorithm]
On Store Placement for Response Time Minimization in Parallel Disks
26th IEEE International Conference on Distributed Computing Systems
None
2006
We investigate the placement of N enterprise data-stores (e.g., database tables, application data) across an array of disks with the aim of minimizing the response time averaged over all served requests, while balancing the load evenly across all the disks in the parallel disk array. Incorporating the non-FCFS serving discipline and non work-conserving nature of disk drives in formulation of the placement problem is difficult and current placement strategies do not take them into account. We present a novel formulation of the placement problem to incorporate these crucial features and identify the runlength of requests accessing a store as the most important criterion for placing the stores. We use these insights to design a fast (running time of N logN) placement algorithm that is optimal under the assumption that transfer times are small. Comprehensive experimental studies establish the efficacy of the proposed algorithm under a wide variety of workloads with the proposed algorithm reducing the response time for real storage traces by more than a factor of 2 under heterogeneous workload scenarios.
[Algorithm design and analysis, Disk drives, Databases, Laboratories, Streaming media, Throughput, Distributed computing, Delay]
PRINS: Optimizing Performance of Reliable Internet Storages
26th IEEE International Conference on Distributed Computing Systems
None
2006
Distributed storage systems employ replicas or erasure code to ensure high reliability and availability of data. Such replicas create great amount of network traffic that negatively impacts storage performance, particularly for distributed storage systems that are geographically dispersed over a wide area network (WAN). This paper presents a performance study of our new data replication methodology that minimizes network traffic for data replications. The idea is to replicate the parity of a data block upon each write operation instead of the data block itself. The data block will be recomputed back at the replica storage site upon receiving the parity. We name the new methodology PRINS (Parity Replication in IP-Network Storages). PRINS trades off highspeed computation for communication that is costly and more likely to be the performance bottleneck for distributed storages. By leveraging the parity computation that exists in common storage systems (RAID), our PRINS does not introduce additional overhead but dramatically reduces network traffic. We have implemented PRINS using iSCSI protocol over a TCP/IP network interconnecting a cluster of PCs as storage nodes. We carried out performance measurements on Oracle database, Postgres database, MySQL database, and Ext2 file system using TPC-C, TPC-W, and Micro benchmarks. Performance measurements show up to 2 orders of magnitudes bandwidth savings of PRINS compared to traditional replicas. A queueing network model is developed to further study network performance for large networks. It is shown that PRINS reduces response time of the distributed storage systems dramatically.
[Wide area networks, Measurement, Availability, Databases, High performance computing, Telecommunication traffic, Traffic control, Computer networks, Internet, Distributed computing]
Genesis: A Scalable Self-Evolving Performance Management Framework for Storage Systems
26th IEEE International Conference on Distributed Computing Systems
None
2006
data-center environment, the administrator needs to understand the root-cause of the issue. The growing trend of system virtualization, combined with the need to support end-to-end performance goals for enterprise applications, have made root-cause analysis a nontrivial problem - administrators are required to manually parse all hardware events, configuration modifications, and changes in access characteristics, across all tiers of the IO path from application servers to the disks. We propose a framework that assists storage administrators with root-cause analysis in distributed systems. GENESIS consists of three key modules: Abnormality Detection, Snapshot Generation, and Diagnosis. The Abnormality Detection module uses clustering algorithms to create and constantly evolve the normality models of measurable parameters in components. The Snapshot Generator is triggered by a combination of abnormality detection and policies to take compact snapshots of the system state for analysis whenever a significant change occurs. The Diagnosis module parses the snapshots and shortlists the root-cause for the administrator using knowledge about the impact of the run-time changes on IO performance. We have implemented an initial proof-of-concept of GENESIS in GPFS (a high performance distributed file-system) and validated its operation for several interesting real-world scenarios. Encouraged by the results, we are currently deploying our prototype in an existing data center environment.
[Runtime, Change detection algorithms, Switches, Hardware, Performance analysis, Transaction databases, History, Environmental management, Delay, Application virtualization]
Tolerating Byzantine Faulty Clients in a Quorum System
26th IEEE International Conference on Distributed Computing Systems
None
2006
Byzantine quorum systems have been proposed that work properly even when up to f replicas fail arbitrarily. However, these systems are not so successful when confronted with Byzantine faulty clients. This paper presents novel protocols that provide atomic semantics despite Byzantine clients. Our protocols prevent Byzantine clients from interfering with good clients: bad clients cannot prevent good clients from completing reads and writes, and they cannot cause good clients to see inconsistencies. In addition we also prevent bad clients that have been removed from operation from leaving behind more than a bounded number of writes that could be done on their behalf by a colluder. Our protocols are designed to work in an asynchronous system like the Internet and they are highly efficient. We require 3f +1 replicas, and either two or three phases to do writes; reads normally complete in one phase and require no more than two phases, no matter what the bad clients are doing. We also present strong correctness conditions for systems with Byzantine clients that limit what can be done on behalf of bad clients once they leave the system. Furthermore we prove that our protocols are both safe (they meet those conditions) and live.
[Access control, Protocols, Interference, Computer crashes, Internet, Proposals, Delay]
Sharing Memory between Byzantine Processes using Policy-Enforced Tuple Spaces
26th IEEE International Conference on Distributed Computing Systems
None
2006
Despite the large amount of Byzantine fault-tolerant algorithms for message-passing systems designed through the years, only recently algorithms for the coordination of processes subject to Byzantine failures using shared memory have appeared. This paper presents a new computing model in which shared memory objects are protected by fine-grained access policies, and a new shared memory object, the policy-enforced augmented tuple space (PEATS). We show the benefits of this model by providing simple and efficient consensus algorithms. These algorithms are much simpler and use less memory bits than previous algorithms based on ACLs and sticky bits. We also prove that PEATSs are universal (they can be used to implement any shared memory object), and present a universal construction.
[Algorithm design and analysis, Access control, Upper bound, Computer hacking, Fault tolerant systems, Emulation, Registers, Security, Protection, Resilience]
Practical Byzantine Group Communication
26th IEEE International Conference on Distributed Computing Systems
None
2006
This paper presents an adaptation of the JazzEnsemble group communication system that enables it to tolerate Byzantine failures. The work emphasizes scalability and good performance in the normal case, i.e., when there are no failures, while providing strong semantics to the application. The paper presents the main concepts and protocols that enable the Byzantine tolerant version of JazzEnsemble to obtain these goals. In particular, this includes fuzzy mute and fuzzy verbose failure detectors, an efficient Byzantine vector consensus protocol, and a novel Byzantine uniform broadcast protocol, as well as modifications at each layer of the system to overcome potential Byzantine attacks. Additionally, high-level protocols only rely on the oral messages model, and thus messages need to be signed only once at a low level of the system. Finally, the paper presents an extensive performance evaluation, which demonstrates the system&#146;s scalability and efficiency, and is used to analyze the sources of performance degradation associated with various aspects of overcoming Byzantine failures.
[Computer science, Scalability, Failure analysis, Detectors, Broadcasting, Public key cryptography, Power system reliability, Performance analysis, Fuzzy systems, Cryptographic protocols]
Spoof Detection for Preventing DoS Attacks against DNS Servers
26th IEEE International Conference on Distributed Computing Systems
None
2006
The Domain Name System (DNS) is a critical element of the Internet infrastructure. Even a small part of the DNS infrastructure being unavailable for a very short period of time could potentially upset the entire Internet and is thus totally unacceptable. Unfortunately, because DNS queries and responses are mostly UDP-based, it is vulnerable to spoofing-based denial of service (DoS) attacks, which are difficult to defeat without incurring significant collateral damage. The key to thwart this type of DoS attacks is spoof detection, which enables selective discarding of spoofed DNS requests without jeopardizing the quality of service to legitimate requests. This paper presents spoof detection strategies for protecting DNS servers from DoS attacks. These strategies create some form of cookies for a DNS server to check if each incoming request is indeed from where the request packet says it is from. We have implemented them as a firewall module called DNS guard. Measurements on the current DNS guard prototype show that it can deliver up to 80K requests/sec to legitimate users in the presence of DoS attacks at the rate of 250K requests/sec.
[Computer science, Current measurement, Prototypes, Bandwidth, Quality of service, Internet, Domain Name System, Computer crime, Web server, Protection]
Provenance-Aware Tracing ofWorm Break-in and Contaminations: A Process Coloring Approach
26th IEEE International Conference on Distributed Computing Systems
None
2006
To investigate the exploitation and contamination by self-propagating Internet worms, a provenance-aware tracing mechanism is highly desirable. Provenance unawareness causes difficulties in fast, accurate identification of a worm&#146;s break-in point, and incurs significant log inspection overhead. This paper presents the design, implementation, and evaluation of process coloring, an efficient provenance-aware approach to worm break-in and contamination tracing. More specifically, process coloring assigns a "color\
[Computer science, Computer worms, Forensics, Color, Inspection, Virtual machining, Internet, Performance analysis, Face detection, Contamination]
A DoS Resilient Flow-level Intrusion Detection Approach for High-speed Networks
26th IEEE International Conference on Distributed Computing Systems
None
2006
Global-scale attacks like viruses and worms are increasing in frequency, severity and sophistication, making it critical to detect outbursts at routers/gateways instead of end hosts. In this paper we leverage data streaming techniques such as the reversible sketch to obtain HiFIND, a High-speed Flow-level Intrusion Detection system. In contrast to existing intrusion detection systems, HiFIND I ) is scalable to flow-level detection on high-speed networks; 2) zs DoS resilient; 3) can distinguish SYN flooding and various port scans (mostly for worm propagation) for effective mitigation; 4 ) enables aggregate detection over multiple routers/gateways; and 5) separates anomalies to limit false positives in detection. Both theoretical analysis and evaluation with several router traces show that HiFIND achieves these properties. To the best of our knowledge, HiFIND is the first online DoS resilient flow-level intrusion detection system for high-speed networks (approximately 10s of Gigabit/second), even for the worst case trafic of 40-byte-packet streams with each packet forming a flow.
[High-speed networks, Scalability, Aggregates, Intrusion detection, Telecommunication traffic, Bandwidth, Routing, Frequency, Hardware, Viruses (medical)]
DNScup: Strong Cache Consistency Protocol for DNS
26th IEEE International Conference on Distributed Computing Systems
None
2006
Effective caching in Domain Name System (DNS) is critical to its performance and scalability. Existing DNS only supports weak cache consistency by using the Time-To-Live (TTL) mechanism, which functions reasonably well in normal situations. However, maintaining strong cache consistency in DNS as an indispensable exceptional handling mechanismhas become more and more demanding for three important objectives: (1) to quickly respond and handle exceptional incidents, such as sudden and dramatic Internet failures caused by natural and human disasters, (2) to adapt increasingly frequent changes of IP addresses due to the introduction of dynamic DNS techniques for various stationed and mobile devices on the Internet, and (3) to provide finegrain controls for content delivery services to timely balance server load distributions. With agile adaptation to various exceptional Internet dynamics, strong DNS cache consistency improves the availability and reliability of Internet services. In this paper, we propose a proactive DNS cache update protocol, called DNScup, running as middleware in DNS nameservers, to provide strong cache consistency for DNS. The core of DNScup is a dynamic lease technique to keep track of the local DNS nameservers, whose clients need cache coherence to avoid losing service availability. Based on the DNS Dynamic Update protocol, we have built a DNScup prototype with minor modifications to the current DNS implementation. Our trace-driven simulation and system prototype demonstrate the effectiveness of DNScup and its easy and incremental deployment on the Internet.
[Availability, Protocols, Scalability, Humans, Cache Consistency, Domain Name System, Middleware, Web and internet services, Prototypes, Service Availability, Lease., Virtual prototyping, Web server]
Application-Tailored Cache Consistency for Wide-Area File Systems
26th IEEE International Conference on Distributed Computing Systems
None
2006
The inability to perform optimizations based on application-specific information presents a hurdle to the deployment of pervasive LAN file systems across WAN environments. This paper proposes a novel approach addressing this problem through application-tailored caching and consistency in widearea file systems. It leverages widely available Network File System (NFS) deployments without any modifications to kernels nor applications, and employs middleware to dynamically establish Grid-wide Virtual File System (GVFS) sessions with application-tailored cache consistency. Two consistency models are discussed in this paper: a relaxed model based on invalidation polling, and a stronger model based on delegation and callback. Experimental evaluation based on microbenchmarks and scientific applications show that with application-tailored cache consistency, GVFS is able to both improve application runtimes and reduce server load significantly, compared to kernel-level NFS in WAN.
[Wide area networks, Pervasive computing, Runtime, File systems, Processor scheduling, Kernel, Middleware, Local area networks, Delay, Information systems]
A Locality-Aware Cooperative Cache Management Protocol to Improve Network File System Performance
26th IEEE International Conference on Distributed Computing Systems
None
2006
In a distributed environment the utilization of file buffer caches in different clients may vary greatly. Cooperative caching is used to increase cache utilization by coordinating the usage of distributed caches. Existing cooperative caching protocols mainly address organizational issues, paying little attention to exploiting locality of file access patterns. We propose a locality-aware cooperative caching protocol, called LAC, that is based on analysis and manipulation of data block reuse distance to effectively predict cache utilization and the probability of data reuse. Using a dynamically controlled synchronization technique, we make local information consistently comparable among clients. The system is highly scalable in the sense that global coordination is achieved without centralized control.
[Computer science, Cooperative caching, File systems, Space technology, Access protocols, Bandwidth, File servers, Los Angeles Council, Computer network management, Centralized control]
Efficient Formation of Edge Cache Groups for Dynamic Content Delivery
26th IEEE International Conference on Distributed Computing Systems
None
2006
Cost-effective cooperation among a network of edge caches is widely accepted as an effective mechanism for enhancing the scalability, performance, and reliability of edge cache networks. However, the problem of how to form cache groups for achieving effective and efficient cooperation in edge cache networks has largely been unexplored. In this paper, we identify two important factors that need to be considered while forming cooperative groups, namely, network proximities of edge caches and network distances of the caches to the origin server. We propose two novel cache clustering schemes for accurately partitioning the caches of a given edge cache network into specified number of cache groups. The first scheme, called the Selective Landmarks scheme (SL scheme), accurately partitions the edge cache network into cooperative groups based on the network proximities of the caches. The second cache group formation scheme, called Server Distance sensitive Selective Landmarks scheme (SDSL scheme), provides a careful combination network proximities and server distances. Our experiments indicate that the proposed techniques can yield significant performance benefits.
[Computer science, Network servers, Cooperative caching, Scalability, Document handling, Collaboration, Educational institutions, Computer networks, IP networks, Resource management]
Content-based Dissemination of Fragmented XML Data
26th IEEE International Conference on Distributed Computing Systems
None
2006
Content-based dissemination of data using pub/sub systems is an effective means to deliver relevant data to interested data consumers. With the emergence of XML as the standard for data representation and exchange, a lot of attention has been focused on pub/sub systems for XML-based dissemination, where subscriptions are specified using more expressive XML-based languages (e.g., XPath). In this paper, we address the problem of matching XPath-based subscriptions on fragmented XML data, which is motivated by both the prevalance of resource-constrained mobile devices for accessing/monitoring data as well as by the optimization opportunities from processing data in terms of fragments. We investigate efficient strategies to schedule and optimize the evaluation of XPath-based subscriptions on XML fragments. Our experimental results not only demonstrate the effectiveness of our proposed optimizations but also reveal several interesting performance tradeoffs.
[Computer science, Query processing, Subscriptions, XML, Information filters, Routing, Information filtering, Internet, Standards publication, Monitoring]
CREW: A Gossip-based Flash-Dissemination System
26th IEEE International Conference on Distributed Computing Systems
None
2006
In this paper, we explore a new form of dissemination called Flash Dissemination that involves dissemination of fixed, rich information to a large number of recipients in as short a time as possible. Key characteristics of Flash Dissemination include unpredictability in its need, scalability to large number of recipients and autonomic performance in highly heterogenous and failureprone environments. Previous work either addresses large content delivery in heterogenous networks or fault-tolerant dissemination of (streaming) events. We investigate a peer-based approach using foundations from broadcast networks, gossip theory and random networks. In this paper, we propose CREW (Concurrent Random Expanding Walkers), a scalable, lightweight, and autonomic gossip-based protocol. CREW is also explicitly designed to maximize the speed of dissemination using adaptive and intelligent intra and inter node concurrency. We implemented CREW on top of a scalable middleware environment and compared it to optimized implementations of popular gossip and peer-based systems. Our experiments show that CREW outperforms both traditional gossip and current large content dissemination systems, across a wide range of comparative metrics, even though its design is counterintuitive from a systems perspective.
[Geographic Information Systems, Peer to peer computing, Scalability, Peer-to-Peer, Middleware, Fault Resilience, Middleware., Earthquakes, Gossip, Broadcast, Autonomic Adaptation, Disaster management, Bandwidth, Broadcasting, Cities and towns, Resource management]
Analysis of Clustering and Routing Overhead for Clustered Mobile Ad Hoc Networks
26th IEEE International Conference on Distributed Computing Systems
None
2006
This paper presents an analysis of the control overhead involved in clustering and routing for one-hop clustered mobile ad hoc networks. Previous work on the analysis of control overhead incurred by clustering algorithms focused mainly on the derivation of control overhead in the Knuth big-O notation with respect to network size. However, we observe that the control overhead in a clustered network is closely related to different network parameters, e.g. node mobility, node transmission range, network size, and network density. We present an analysis that captures the effects of different network parameters on the control overhead. The results of our work can provide valuable insights into the amount of overhead that clustering algorithms may incur in different network environments. This facilitates the design of efficient clustering algorithms in order to minimize the control overhead.
[Algorithm design and analysis, Computer science, Network topology, Clustering algorithms, Bandwidth, Routing protocols, Computer networks, Size control, Erbium, Mobile ad hoc networks]
Mitigating the FloodingWaves Problem in Energy-Efficient Routing for MANETs
26th IEEE International Conference on Distributed Computing Systems
None
2006
In wireless mobile adhoc networks (MANETs) channel and energy capacities are scarce resources, a lot of energy-efficient routing protocols for MANETs have been previously proposed to take into consideration the nodes&#146; residual energies when establishing routes between source-destination pairs. In this paper we are not trying to introduce a new routing algorithm to be added to the already proposed stack of energy-efficient protocols, but rather, we identify a problem in cost-based energy-efficient routing for MANETs, we call this problem "Flooding Waves". We show that the "Flooding Waves" is a serious problem in dense networks, to the extent that the excessive energy overhead consumed in these waves can outweigh the gain achieved by energy-efficient path selection. We propose the "Delayed-Forwarding" as a solution for this problem. We provide both a simulation analysis and a simple theoretical framework to validate and support this solution.
[Computer science, Analytical models, Intelligent networks, Storms, Broadcasting, Energy efficiency, Routing protocols, Batteries, Relays, Mobile ad hoc networks]
High-Throughput Multicast Routing Metrics in Wireless Mesh Networks
26th IEEE International Conference on Distributed Computing Systems
None
2006
The stationary nature of nodes in a mesh network has shifted the main design goal of routing protocols from maintaining connectivity between source and destination nodes to finding high-throughput paths between them. In recent years, numerous link-quality-based routing metrics have been proposed for choosing high-throughput paths for unicast protocols. In this paper we study routing metrics for high-throughput tree or mesh construction in multicast protocols. We show that there is a fundamental difference between unicast and multicast routing in how data packets are transmitted at the link layer, and accordingly there is a difference in how the routing metrics for each of these primitives are designed. We adapt certain routing metrics for unicast for high-throughput multicast routing and propose news ones not previously used for high-throughput. We then study the performance improvement achieved by using different link-quality-based routing metrics via extensive simulation and experiments on a mesh network testbed, using ODMRP as a representative multicast protocol. Our testbed experiment results show that ODMRP enhanced with linkquality routing metrics can achieve up to 17.5% throughput improvement as compared to the original ODMRP.
[Intelligent networks, Mesh networks, Unicast, Wireless mesh networks, Spread spectrum communication, Multicast protocols, Throughput, Routing protocols, Computer networks, Testing]
GMP: Distributed Geographic Multicast Routing in Wireless Sensor Networks
26th IEEE International Conference on Distributed Computing Systems
None
2006
In this paper, we propose a novel Geographic Multicast routing Protocol (GMP) for wireless sensor networks1. The proposed protocol is fully distributed and stateless. Given a set of the destinations, the transmitting node first constructs a virtual Euclidean Steiner tree rooted at itself and including the destinations, using a novel and highly efficient reduction ratio heuristic (called rrSTR). Based on this locally computed tree and the information regarding the locations of its immediate neighbors, the transmitting node then splits the destinations into a set of groups and calculates a next hop for each of these groups. A copy of the packet and the locations of the corresponding group of destination nodes are directed towards the corresponding hop. The simulation results on NS2 show that the average per-destination hop count obtained using GMP is comparable to the existing PBM [21] algorithm and significantly less than obtained by using LGS [5]. Most significantly, GMP requires 25% less hops and energy than alternative algorithms.
[Computer science, Intelligent networks, Wireless sensor networks, Multicast algorithms, Tree graphs, Unicast, Bandwidth, Multicast protocols, Routing protocols, Electronic mail]
Distributed Minimal Time Convergecast Scheduling in Wireless Sensor Networks
26th IEEE International Conference on Distributed Computing Systems
None
2006
We consider applications of sensor networks wherein data packets generated by every node have to reach the base station. This results in a many-to-one communication paradigm referred to as convergecast. We are interested in determining a TDMA schedule that minimizes the total time required to complete the convergecast. We consider a simple version of the problem wherein every node generates exactly one packet. We propose a distributed convergecast scheduling algorithm that requires at most 3N timeslots, where N represents the number of nodes in the network. Through extensive simulations, we demonstrate that actual number of timeslots needed is around 1.5N. In addition to time efficiency, we prove that our convergecast scheduling algorithm requires the nodes to buffer no more than two packets at any instance. We propose a sleep schedule that conserves more than 50% of the energy. We present simulation results for a real application scenario to show that our convergecast scheduling algorithm performs significantly better than existing convergecast algorithms.
[Base stations, Convergecast, Scheduling, Wireless Sensor Networks, Scheduling algorithm, Delay, Computer science, Intelligent networks, Wireless sensor networks, Time division multiple access, Processor scheduling, Sleep, TDMA., Media Access Protocol]
In-Network Outlier Detection in Wireless Sensor Networks
26th IEEE International Conference on Distributed Computing Systems
None
2006
To address the problem of unsupervised outlier detection in wireless sensor networks, we develop an algorithm that (1) is flexible with respect to the outlier definition, (2) works in-network with a communication load proportional to the outcome, and (3) reveals its outcome to all sensors. We examine the algorithm&#146;s performance using simulation with real sensor data streams. Our results demonstrate that the algorithm is accurate and imposes a reasonable communication load and level of power consumption.
[Computer science, Intelligent networks, Wireless sensor networks, Energy consumption, Data analysis, Change detection algorithms, Neural networks, Robustness, Pattern analysis, Distributed computing]
POS: A Practical Order Statistics Service forWireless Sensor Networks
26th IEEE International Conference on Distributed Computing Systems
None
2006
We present the design and implementation of POS, an in-network service that computes accurate order statistics energy-efficiently. POS returns a stream of periodic samples from any order statistic. It initially computes the value of the order statistic and then periodically runs a validation protocol to determine whether the value is still valid. If not, it uses an optimized binary search to determine the new value and then resumes periodic validation. POS uses in-network aggregation and transmission suppression to reduce communication complexity. Results from both experiments on a mote testbed and simulations show that POS can compute order statistics accurately while consuming less energy than the best techniques to compute averages in common cases.
[Base stations, Wireless sensor networks, Protocols, Statistical distributions, Sensor phenomena and characterization, Computer networks, Energy efficiency, Complexity theory, Batteries, Statistics]
Sluice: Secure Dissemination of Code Updates in Sensor Networks
26th IEEE International Conference on Distributed Computing Systems
None
2006
Existing network reprogramming protocols target the efficient, reliable, multi-hop dissemination of application updates in sensor networks, but assume correct or fail-stop behavior from participating sensors. Compromised nodes can subvert such protocols to result in the propagation and remote installation of malicious code. Sluice aims for the progressive, resource-sensitive verification of updates in sensor networks to ensure that malicious updates are not disseminated or installed, while trusted updates continue to be efficiently disseminated. Our verification mechanism provides authenticity and integrity through a hash-chain construction that amortizes the cost of a single digital signature over an entire update. We integrate Sluice with an existing network reprogramming protocol and empirically evaluate its effectiveness both in a real sensor testbed and through simulation.
[Intelligent networks, Wireless sensor networks, Costs, Wireless application protocol, Spread spectrum communication, Access protocols, Large scale integration, Monitoring, Digital signatures, Testing]
Overlay Multicast with Inferred Link Capacity Correlations
26th IEEE International Conference on Distributed Computing Systems
None
2006
We model the overlay using linear capacity constraints, which accurately and succinctly capture overlay link correlations. We show that finding a maximum-bandwidth multicast tree in an overlay with LCC is NP-complete and propose an efficient distributed heuristics algorithm.
[Multicast algorithms, Network topology, Unicast, Heuristic algorithms, Bandwidth, Multicast protocols, IP networks, Proposals, Delay, Stress measurement]
On Estimating Tight-Link Bandwidth Characteristics over Multi-Hop Paths
26th IEEE International Conference on Distributed Computing Systems
None
2006
In this paper, we explore multi-hop bandwidth estimation assuming arbitrary cross-traffic at each node and develop a new probing method called Envelope that can asymptotically estimate not only the available bandwidth but also the raw capacity of the tight link. Envelope is based on a multi-link recursive extension of unbiased single-hop estimators proposed in the past (e.g., [14]) and a variation of the packet-cartouche technique [6]. Through extensive simulations, we evaluate Envelope in various network settings and cross-traffic conditions and find that it can measure tight-link bandwidth characteristics with accuracy that significantly surpasses that of the existing methods. We also find that Envelope can measure non-tight links in certain path and cross-traffic configurations.
[Computer science, Phase measurement, Phase estimation, Bandwidth, Interference, Internet, Probes, Statistics, Recursive estimation, Convergence]
Interplay of ISPs: Distributed Resource Allocation and Revenue Maximization
26th IEEE International Conference on Distributed Computing Systems
None
2006
The Internet is a hierarchical architecture comprising heterogeneous entities of privately owned infrastructures, where higher level Internet service providers (ISPs) supply connectivity to the local ISPs and charge the local ISPs for the transit services. One of the challenging problems facing service providers today is how to increase the profitability while maintaining good service qualities. In this work, we seek to understand the fundamental issues on the "interplay" (or interaction) between ISPs at different tiers. While the local ISPs (which we term peers) can communicate with each other by purchasing the connectivity from transit ISPs, there stands an opportunity for them to set up private peering relationships. Under this competitive framework, we explore the issues on (a) impact of peering relationship, (b) resource distribution and (c) revenue maximization. Firstly, a generalized model is presented to characterize the behaviors of peers and the transit ISP, in which their economic interests are reflected. We study how a peer can distributively determine its optimal peering strategy. Furthermore, we show how a transit ISP is able to utilize the available information to infer its optimal pricing strategy, under which a revenue maximization is achieved. A distributed algorithm is proposed to help ISPs to provide a fair and efficient bandwidth allocation to peers, avoiding a resource monopolization of the market. Extensive simulations are carried out to support our claims.
[ISP peering, economic pricing, Profitability, Service oriented architecture, Computer science, distributed resource allocation, Web and internet services, Pricing, Computer architecture, Bandwidth, Channel allocation, Resource management, Distributed algorithms]
Cycling Through a Dangerous Network: A Simple Efficient Strategy for Black Hole Search
26th IEEE International Conference on Distributed Computing Systems
None
2006
In this paper we consider a dangerous process located at a node of a network (called Black Hole ) and a team of mobile agents deployed to locate that node. The nature of the danger is such that when an agent enters the dangerous node, it is trapped there leaving no trace of its destruction. The goal is to deploy as few agents as possible and to locate the black hole in as few moves as possible. We present a simple algorithm that works on any topology (a-priori known by the agents). Our algorithm, based on the pre-computation of an open vertex cover by cycles of the network, uses the optimal number of agents (two); its cost (number of moves) depends on the choice of the cover and it is optimal for several classes of networks.
[Performance evaluation, Mobile Agents, Malicious Host, Network topology, Mobile agents, Undetectable, Cost function, Size measurement, Black Hole Search, Distributed computing, Failure]
Fast data access over asymmetric channels using fair and secure bandwidth sharing
26th IEEE International Conference on Distributed Computing Systems
None
2006
We propose a peer-to-peer architecture designed to overcome asymmetries in upload/download speeds that are typical in end-user dialup, broadband and cellular wireless Internet connections. Our approach allows users at remote locations to access information stored on their home computers at rates often exceeding their home connection&#146;s upload capacity. The key to this approach is to share file data when communications are idle using random linear coding, so that, when needed, an end-user can download a file from several sources at a higher data rate than his home computer&#146;s upload capacity. We prove that our proposed system is asymptotically fair, in that (even malicious) users are proportionally assigned idle bandwidth depending on how much bandwidth they contribute, and that there is a natural incentive to join and cooperate fairly in the system. In addition, our approach provides cryptographic security and geographic data robustness to the participating peers.
[Home computing, Wireless sensor networks, Data security, Peer to peer computing, Web and internet services, Laboratories, Bandwidth, Robustness, Web server, Radio spectrum management]
M2: Multicasting Mixes for Efficient and Anonymous Communication
26th IEEE International Conference on Distributed Computing Systems
None
2006
We present a technique to achieve anonymous multicasting in mix networks to deliver content from producers to consumers. Employing multicast allows content producers to send (and mixes to forward) information to multiple consumers without repeating work for each individual consumer. In our approach, consumers register interest for content by creating paths in the mix network to the content&#146;s producers. When possible, these paths are merged in the network so that paths destined for the same producer share a common path suffix to the producer. When a producer sends content, the content travels this common suffix toward its consumers (in the reverse direction) and "branches" into multiple messages when necessary. We detail the design of this technique and then analyze the unlinkability of our approach against a global, passive adversary who controls both the producer and some mixes. We show that there is a subtle degradation of unlinkability that arises from multicast. We discuss techniques to tune our design to mitigate this degradation while retaining the benefits of multicast.
[Degradation, Network servers, Costs, Unicast, Merging, Routing, Data structures, Multicast protocols]
Dynamic Access Control in a Content-based Publish/Subscribe System with Delivery Guarantees
26th IEEE International Conference on Distributed Computing Systems
None
2006
Content-based publish/subscribe (pub/sub) is a promising paradigm for building asynchronous distributed applications. In many application scenarios, these systems are required to provide stringent service guarantees such as reliable delivery, high performance, high availability and dynamic system security. In this paper, we address the issue of dynamic access control in a content-based system that provides reliable delivery and high availability through redundant routes. We define a deterministic service model of dynamic access controls that enables precise control over event confidentiality. Under this model, the semantics of reliable delivery is clearly defined, that is, the messages delivered in response to the same subscriptions from pub/sub clients running on behalf of the same principal will be exactly the same, regardless of their connecting locations, network latency and failures. We present an algorithm that implements this service model. The algorithm is efficient and highly available in that it enables uniform enforcement of access control and enables content-based routing to choose any path from among several redundant routes without requiring consensus among the brokers.
[Access control, Availability, Filtering, Scalability, Subscriptions, Routing, Application software, Security, Joining processes, Delay]
On the Access Pricing Issues of Wireless Mesh Networks
26th IEEE International Conference on Distributed Computing Systems
None
2006
This paper studies the use of pricing as an incentive mechanism to encourage private, self-interested nodes to participate in a public wireless mesh network and cooperate in the packet forwarding service. Our focus is on the "economic behavior" of the network nodes the pricing and purchasing strategies of the access point, wireless relaying nodes, and clients. We use a "game theoretic approach" to analyze their interactions from one-hop to multihop network and when the network has an unlimited or limited channel capacity. We show that the access point and relaying wireless nodes will adopt a simple, yet optimal, fixed-rate pricing strategy in a multi-hop network with an unlimited capacity. Yet, the fixed-rate pricing strategy fails to be optimal in the limited capacity case. To this end, we focus on the access point adopting a more practical "fixedrate, non-interrupted service" model and propose an algorithm based on the Markovian decision theory to devise the optimal pricing strategy.
[Mesh networks, Costs, Wireless networks, Wireless mesh networks, Pricing, Spread spectrum communication, Power generation economics, IP networks, Relays, Power engineering and energy]
Modeling and Analysis of Generalized Slotted-Aloha MAC Protocols in Cooperative, Competitive and Adversarial Environments
26th IEEE International Conference on Distributed Computing Systems
None
2006
Aloha [1] and its slotted variant [2] are commonly deployed Medium Access Control (MAC) protocols in environments where multiple transmitting devices compete for a medium, yet may have difficulty sensing each other&#146;s presence. This paper models and evaluates the throughput that can be achieved in a system where nodes compete for bandwidth using a generalized version of slotted- Aloha protocols. We evaluate the channel utilization and fairness of these types of protocols for a variety of node objectives, including maximizing aggregate throughput of the channel, each node greedily maximizing its own throughput, and attacker nodes that attempt to jam the channel. If all nodes are selfish and greedily attempt to maximize their own throughputs, a situation similar to the traditional Prisoner&#146;s Dilemma[3] arises. Our results reveal that under heavy loads, greedy strategies reduce the utilization, and that attackers cannot do much better than attacking during randomly selected slots.
[Wireless sensor networks, Costs, Aggregates, Access protocols, Bandwidth, Media Access Protocol, Throughput, Ad hoc networks, Communication networks, Multiaccess communication]
Detecting MAC Layer Back-off Timer Violations in Mobile Ad Hoc Networks
26th IEEE International Conference on Distributed Computing Systems
None
2006
In IEEE 802.11 based ad hoc networks, by simply manipulating the back-off timers and/or wait times prior to transmission, malicious nodes can cause a drastically reduced allocation of bandwidth to well-behaved nodes. This can result in causing bandwidth starvation and hence, a denial of service to legitimate nodes. We propose a combination of deterministic and statistical methods that facilitate detection of such misbehavior. With our approach, each of the nodes is made aware of the pseudo-random sequences that dictate the back-off times of all its one-hop neighbors. A blatant violation of the timer is thus, immediately detected. In certain cases, a node may be unable to monitor the activities of its neighbor and therefore deterministically ascertain if the neighbor is misbehaving. To cope with such cases, we propose a statistical inference method, wherein based on an auto-regressive moving average (ARMA) of observations of the system state, a node is able to estimate if its neighbor is indulging in misbehavior. Simulation results show that with our methods, it is possible to detect a malicious node with a probability close to one. Furthermore, the probability of false alarms is lower than 1%.
[Computer science, Intelligent networks, Statistical analysis, Access protocols, Bandwidth, Ad hoc networks, Computer crime, State estimation, Monitoring, Mobile ad hoc networks]
Delay-Bounded Range Queries in DHT-based Peer-to-Peer Systems
26th IEEE International Conference on Distributed Computing Systems
None
2006
Many general range query schemes for DHT-based peer-to-peer (P2P) systems have been proposed, which do not need to modify the underlying DHTs. However, most existing works have the query delay depending on both the scale of the system and the size of the query space or the specific query, and thus cannot guarantee to return the query results in a bounded delay. In this paper, we propose Armada, an efficient general range query scheme to support single-attribute and multipleattribute range queries. Armada is the first delaybounded range query scheme over constant-degree DHTs, and can return the results for any range query within 2logN hops in a P2P system with N peers. Results of analysis and simulations show that the average delay of Armada is less than logN, and the average message cost of single-attribute range queries is about logN+2n..2 (n is the number of peers that intersect with the query). These results are very close to the lower bounds on delay and message cost of range queries over constant-degree DHTs.
[Analytical models, Peer to peer computing, Computational modeling, Memory management, Cost function, Robustness, Large-scale systems, Topology, Distributed computing, Delay]
Search-and-Discover in Mobile P2P Network Databases
26th IEEE International Conference on Distributed Computing Systems
None
2006
In this paper we propose a novel algorithm called Rank-Based Broadcast (RBB) for discovery of local resources in mobile P2P networks. With RBB, each moving object periodically broadcasts the most relevant resource reports and queries it knows to its neighbors, and the contribution is in determining how to rank the reports and queries in terms of their relevance, when to broadcast them, and how many to broadcast. A major difference between RBB and many existing algorithms in the resource discovery and publish/subscribe literature is that RBB does not rely on any pre-established routing structure, and therefore is able to adapt to both high mobility environments. In the paper we experimentally compare RBB with flooding and PSTree, a publish/subscribe algorithm for wireless ad-hoc networks. The results show that RBB by far outperforms the other two algorithms.
[Intelligent networks, Databases, Social network services, Terrorism, Transportation, Broadcasting, Routing, Ad hoc networks, Application software, Electronic commerce]
Improving Traffic Locality in BitTorrent via Biased Neighbor Selection
26th IEEE International Conference on Distributed Computing Systems
None
2006
Peer-to-peer (P2P) applications such as BitTorrent ignore traffic costs at ISPs and generate a large amount of cross-ISP traffic. As a result, ISPs often throttle BitTorrent traffic to control the cost. In this paper, we examine a new approach to enhance BitTorrent traffic locality, biased neighbor selection, in which a peer chooses the majority, but not all, of its neighbors from peers within the same ISP. Using simulations, we show that biased neighbor selection maintains the nearly optimal performance of Bit- Torrent in a variety of environments, and fundamentally reduces the cross-ISP traffic by eliminating the traffic&#146;s linear growth with the number of peers. Key to its performance is the rarest first piece replication algorithm used by Bit- Torrent clients. Compared with existing locality-enhancing approaches such as bandwidth limiting, gateway peers, and caching, biased neighbor selection requires no dedicated servers and scales to a large number of BitTorrent networks.
[Computer science, Analytical models, Network servers, Costs, Peer to peer computing, Bandwidth, Telecommunication traffic, Traffic control, Internet, Application software]
Self-Protection for Wireless Sensor Networks
26th IEEE International Conference on Distributed Computing Systems
None
2006
Wireless sensor networks have recently been suggested for many surveillance applications such as object monitoring, path protection, or area coverage. Since the sensors themselves are important and critical objects in the network, a natural question is whether they need certain level of protection, so as to resist the attacks targeting on them directly. If this is necessary, then who should provide this protection, and how it can be achieved? We refer to the above problem as self-protection, as we believe the sensors themselves are the best (and often the only) candidate to provide such protection. In this papel; we for the jirst time present a formal study on the selfprotection problem in wireless sensor networks. We show that, if we simply focus on the quality ofjield or object covering, the sensors might not necessarily be self-protected, which in turn makes the system vulnerable. We then investigate dzreerent forms of self-pmtections, and show that the problems are generally NP-complete. We develop eficient approximation algorithms for centrally-controlled sensors. We then extend the algorithms to filly distributed implementation, and introduce a smart sleep-scheduling algorithm that minimize the energy consumption.
[Computer science, Wireless sensor networks, Energy consumption, Military computing, Surveillance, Computerized monitoring, Computer networks, Application software, Protection, Intelligent sensors]
Fault-Tolerant Clustering in Ad Hoc and Sensor Networks
26th IEEE International Conference on Distributed Computing Systems
None
2006
In this paper, we study distributed approximation algorithms for fault-tolerant clustering in wireless ad hoc and sensor networks. A k-fold dominating set of a graph G = (V,E) is a subset S of V such that every node v \\in V \\ S has at least k neighbors in S. We study the problem in two network models. In general graphs, for arbitrary parameter t, we propose a distributed algorithm that runs in time O(t^2) and achieves an approximation ratio of O(t\\delta^2/t log\\delta), where n and \\delta denote the number of nodes in the network and the maximal degree, respectively. When the network is modeled as a unit disk graph, we give a probabilistic algorithm that runs in time O(log log n) and achieves an O(1) approximation in expectation. Both algorithms require only small messages of size O(log n) bits.
[Fault tolerance, Intelligent networks, Wireless sensor networks, Laboratories, Redundancy, Clustering algorithms, Sensor phenomena and characterization, Approximation algorithms, Computer networks, Distributed computing]
Scalable and Robust Aggregation Techniques for Extracting Statistical Information in Sensor Networks
26th IEEE International Conference on Distributed Computing Systems
None
2006
Wireless sensor networks have stringent constraints on system resources and data aggregation techniques are critically important. However, accurate data aggregation is difficult due to the variation of sensor readings and due to the frequent communication failures. To address these difficulties, we propose a scalable and robust data aggregation algorithm. The novelty of our work includes two aspects. First, our algorithm exploits the mixture model and the Expectation Maximization (EM) algorithm for parameter estimation. Hence, it captures the effects of aggregation over different scales while keeping the communication cost low. Second, our algorithm exploits loss-tolerant multi-path routing schemes. Hence, it obtains accurate statistical information even in the presence of high link and node failure rates. We demonstrate that our techniques reduce communication cost while retaining the precious statistical information otherwise neglected by other aggregation techniques. Our evaluation shows the proposed techniques are robust against link and node failures, and perform consistently well.
[Temperature sensors, Intelligent networks, Wireless sensor networks, Energy consumption, Costs, Parameter estimation, Sensor phenomena and characterization, Routing, Robustness, Data mining]
ASAP: an AS-Aware Peer-Relay Protocol for High Quality VoIP
26th IEEE International Conference on Distributed Computing Systems
None
2006
Peer-to-peer (P2P) technology has been successfully applied in Internet telephony or Voice over Internet Protocol (VoIP), such as the Skype system, where P2P is used for both searching clients and relaying voice packets. Selecting one or multiple suitable peers to relay voice packets is a critical factor for the quality, scalability and cost of a VoIP system. In this paper, we first present two sets of intensive Internet measurement results to confirm the benefits gained by peer relays in VoIP, and to investigate the performance of the Skype system. We obtain the following results: (1) many relay peer selections are suboptimal; (2) the waiting time to select a relay node can be quite long; and (3) there are a large number of unnecessary probes, resulting in heavy network traffic to limit scalability of the VoIP system. Our further analysis shows that two main reasons cause these problems. First, the peer selections do not take Autonomous System (AS) topology into consideration, and second, the complex communication relationships among peers are not well utilized. Motivated by our measurements and analysis, we propose an AS-aware peer-relay protocol called ASAP. Our objective is to significantly improve VoIP quality and system scalability with low overhead. Our intensive evaluation by trace-driven simulation shows ASAP is highly effective and easy to implement on the Internet for building large and scalable VoIP systems.
[Protocols, Costs, Network topology, Peer to peer computing, Scalability, Telecommunication traffic, Gain measurement, Relays, Internet telephony, Probes]
Adaptive Control of Extreme-scale Stream Processing Systems
26th IEEE International Conference on Distributed Computing Systems
None
2006
Distributed stream processing systems offer a highly scalable and dynamically configurable platform for time-critical applications ranging from real-time, exploratory data mining to high performance transaction processing. Resource management for distributed stream processing systems is complicated by a number of factors processing elements are constrained by their producer-consumer relationships, data and processing rates can be highly bursty, and traditional measures of effectiveness, such as utilization, can be misleading. In this paper, we propose a novel distributed, adaptive control algorithm that maximizes weighted throughput while ensuring stable operation in the face of highly bursty workloads. Our algorithm is designed to meet the challenges of extreme-scale stream processing systems, where overprovisioning is not an option, by making the best use of resources even when the proffered load is greater than available resources. We have implemented our algorithm in a real-world distributed stream processing system and a simulation environment. Our results show that our algorithm is not only self-stabilizing and robust to errors, but also outperforms traditional approaches over a broad range of buffer sizes, processing graphs, and burstiness types and levels.
[Real time systems, Programmable control, Signal processing algorithms, Streaming media, Throughput, Control systems, Resource management, Time factors, Data mining, Adaptive control]
Greedy is Good: On Service Tree Placement for In-Network Stream Processing
26th IEEE International Conference on Distributed Computing Systems
None
2006
This paper is concerned with reducing communication costs when executing distributed user tasks in a sensor network. We take a service-oriented abstraction of sensor networks, where a user task is composed of a set of data processing modules (called services) with dependencies. Communications in sensor networks consume significant energy and introduce uncertainty in data fidelity due to high bit error rate. These constraints are abstracted as costs on the communication graph. The goal is to place the services within the sensor network so that the communication cost in performing the task is minimized. In addition, since the lifetime of a node, the quality of network links, and the composition of the service graph may change over time, the quality of the placement must be maintained in the face of these dynamics. In this paper, we take a fresh look at what is generally considered a simple but poor performance approach for service placement, namely the greedy algorithm. We prove that a modified greedy algorithm is guaranteed to have cost at most 8 times the optimum placement. In fact, the guarantee is even stronger if there is a high degree of data reduction in the service graph. The advantage of the greedy placement strategy is that when there are local changes in the service graph or when a hosting node fails, the repair only affects the placement of services that depend on the changes. Simulations suggest that in practice the greedy algorithm finds a low cost placement. Furthermore, the cost of repairing a greedy placement decreases rapidly as a function of the proximity of the services to be aggregated.
[Greedy algorithms, Computer science, Uncertainty, Energy capture, Network topology, Bit error rate, Bandwidth, Data processing, Cost function, Telecommunication network reliability]
A Hierarchical Approach to Internet Distance Prediction
26th IEEE International Conference on Distributed Computing Systems
None
2006
Internet distance prediction gives pair-wise latency information with limited measurements. Recent studies have revealed that the quality of existing prediction mechanisms from the application perspective is short of satisfactory. In this paper, we explore the root causes and remedies for this problem. Our experience with different landmark selection schemes shows that although selecting nearby landmarks can increase the prediction accuracy for short distances, it can cause the prediction accuracy for longer distances to degrade. Such uneven prediction quality significantly impacts application performance. Instead of trying to select the landmark nodes in some "intelligent" fashion, we propose a hierarchical prediction approach with straightforward landmark selection. Hierarchical prediction utilizes multiple coordinate sets at multiple distance scales, with the "right" scale being chosen for prediction each time. Experiments with Internet measurement datasets show that this hierarchical approach is extremely promising for increasing the accuracy of network distance prediction.
[Degradation, Computer science, Accuracy, Electric variables measurement, Interference, Prediction algorithms, Internet, Application software, IP networks, Delay]
Stable and Accurate Network Coordinates
26th IEEE International Conference on Distributed Computing Systems
None
2006
Network coordinates provide a scalable way to estimate latencies among large numbers of hosts. While there are several algorithms for producing coordinates, none account for the fact that nodes observe a stream of distinct observations that may vary by as much as three orders-ofmagnitude. With such variable data, coordinate systems are prone to high error and instability in live deployments. In addition, dynamics such as triangle violations can lead to coordinate oscillations, producing further instability and making it difficult for applications to know when their coordinates have truly changed. Because simulation results demonstrate that network coordinates are capable of providing low cost and sufficiently accurate answers to common queries, it is vital that we develop the ability to obtain similar results in practice. We propose two filters which combined to improve network coordinate accuracy by 54% and coordinate stability by 96% when run on a real, largescale network.
[Context, Filters, Coordinate measuring machines, Costs, Stability, Distributed databases, Routing, Frequency measurement, Large-scale systems, Delay]
Routing in Networks with Low Doubling Dimension
26th IEEE International Conference on Distributed Computing Systems
None
2006
This paper studies compact routing schemes for networks with low doubling dimension. Two variants are explored, name-independent routing and labeled routing. The key results obtained for this model are the following. First, we provide the first name-independent solution. Specifically, we achieve constant stretch and polylogarithmic storage. Second, we obtain the first truly scale-free solutions, namely, the network&#146;s aspect ratio is not a factor in the stretch. Scale-free schemes are given for three problem models: name-independent routing on graphs, labeled routing on metric spaces, and labeled routing on graphs. Third, we prove a lower bound requiring linear storage for stretch \\gt 3 schemes. This has the important ramification of separating for the first time the name-independent problem model from the labeled model for these networks, since compact stretch-1+e labeled schemes are known to be possible.
[Computer science, Intelligent networks, Costs, Upper bound, Routing, Extraterrestrial measurements, Silicon, Books]
Message from the General Chair
27th International Conference on Distributed Computing Systems
None
2007
Presents the welcome message from the conference proceedings.
[]
Message from the Program Chair
27th International Conference on Distributed Computing Systems
None
2007
Presents the welcome message from the conference proceedings.
[]
Committee Organization
27th International Conference on Distributed Computing Systems
None
2007
Provides a listing of current committee members.
[]
Technical Program Committee
27th International Conference on Distributed Computing Systems
None
2007
Provides a listing of current committee members.
[]
Panel - Looking ahead: A ten-year outlook for internet security and privacy
27th International Conference on Distributed Computing Systems
None
2007
Summary form only given, as follows. Internet attacks are no longer for fun. Driven by financial gain, well-organized criminal groups, operating internationally while facing little obstruction from law enforcement, are collaborating to rapidly discover and exploit new vulnerabilities. These skilled professionals are using increasingly sophisticated attacks that target both businesses and individuals. The attacks employ multiple methods (e.g., phishing, spam, malware) and can spread in various ways (e.g., email, IM, P2P, Bluetooth, etc). At the same time, users are increasingly relying on the Internet, which poses serious privacy risks such as tracking users' activities and identity theft. The panelists will discuss these threats, describe their current research and bravely predict the state of Internet security and privacy in the coming ten years.
[]
A Virtual Node-Based Tracking Algorithm for Mobile Networks
27th International Conference on Distributed Computing Systems
None
2007
We introduce a virtual-node based mobile object tracking algorithm for mobile sensor networks, VINESTALK. The algorithm uses the virtual stationary automata programming layer, consisting of mobile clients, virtual timed machines distributed at known locations in the plane, called virtual stationary automata (VSAs), and a communication service connecting VSAs and mobile clients. VINESTALK maintains a data structure on top of an underlying hierarchical partitioning of the network. In a grid partitioning, operations to find a mobile object distance d away take O(d) time and communication to complete. Updates to the tracking structure after the object has moved a total of d distance take O{d*log network diameter) amortized time and communication to complete. The tracked object may relocate without waiting for VINESTALK to complete updates for prior moves, and while a find is in progress.
[virtual reality, communication service, grid partitioning, VINESTALK, mobile client, grid computing, data structure, Mobile communication, sensor networks, Sensor networks and ubiquitous computing, finite state machines, tracking, Technical areas: Algorithms and theory, computing, hierarchical partitioning, virtual node-based tracking, Automatic programming, mobile radio, distributed data structures., Data structures, Ubiquitous computing, Virtual nodes, Partitioning algorithms, Wireless and mobile, Wireless sensor networks, Automata, virtual stationary automata programming, virtual machines, mobile sensor network, mobile object tracking, virtual timed machine, Joining processes, Mobile computing, Clocks]
Exploiting Symbolic Techniques in Automated Synthesis of Distributed Programs with Large State Space
27th International Conference on Distributed Computing Systems
None
2007
Automated formal analysis methods such as program verification and synthesis algorithms often suffer from time complexity of their decision procedures and also high space complexity known as the state explosion problem. Symbolic techniques, in which elements of a problem are represented by Boolean formulae, are desirable in the sense that they often remedy the state explosion problem and time complexity of decision procedures. Although symbolic techniques have successfully been used in program verification, their benefits have not yet been exploited in the context of program synthesis and transformation extensively. In this paper, we present a symbolic method for automatic synthesis of fault-tolerant distributed programs. Our experimental results on synthesis of classical fault-tolerant distributed problems such as Byzantine agreement and token ring show a significant performance improvement by several orders of magnitude in both time and space complexity. To the best of our knowledge, this is the first illustration where programs with large state space (beyond 2100) is handled during synthesis.
[Algorithm design and analysis, program verification, automated distributed program synthesis, fault-tolerance, Electronic mail, large state space explosion problem, Fault tolerance, symbolic technique, Boolean functions, Automatic control, formal analysis, Safety, distributed programming, automatic programming, time complexity, Explosions, State-space methods, software fault tolerance, Computer science, Automata, decision procedure, Boolean formula, program transformation, space complexity, Context modeling, computational complexity]
Streaming Algorithms for Robust, Real-Time Detection of DDoS Attacks
27th International Conference on Distributed Computing Systems
None
2007
Effective mechanisms for detecting and thwarting distributed denial-of-service (DDoS) attacks are becoming increasingly important to the success of today's Internet as a viable commercial and business tool. In this paper, we propose novel data-streaming algorithms for the robust, real-time detection of DDoS activity in large ISP networks. The key element of our solution is a new, hash-based synopsis data structure for network-data streams that allows us to efficiently track, in guaranteed small space and time, destination IP addresses in the underlying network that are "large" with respect to the number of distinct source IP addresses that have established potentially-malicious (e.g., "half-open") connections to them. Our work is the first to address the problem of efficiently tracking the top distinct-source frequencies over a general stream of updates (insertions and deletions) to the set of underlying network flows, thus enabling us to effectively distinguish between DDoS activity and flash crowds. Preliminary experimental results verify the effectiveness of our approach.
[ISP networks, distributed denial-of-service attacks detection, IP addresses, Data structures, cryptography, Floods, Computer crime, data-streaming algorithms, Network servers, hash-based synopsis data structure, TCPIP, Frequency, Robustness, data structures, Internet, IP networks, Monitoring, Business]
Effective mechanisms for detecting and thwarting Distributed
27th International Conference on Distributed Computing Systems
None
2007
The emergence of applications producing continuous high-frequency data streams has brought forth a large body of research in the area of distributed stream processing. In presence of high volumes of data, efforts have primarily concentrated on providing approximate aggregate or top-k type results. Scalable solutions for providing answers to window join queries in distributed stream processing systems have received limited attention to date. We provide a solution for the window join in a distributed stream processing system which features reduced inter-node communications achieved through automatic throughput handling based on resource availability. Our approach is based on incrementally updated discrete Fourier transforms (DFTs). Furthermore, we provide formulae for computingDFT compression factors in order to achieve information reduction. We perform WAN-based prototype experiments to ascertain the viability and establish the effectiveness of our method. Our experimental results reveal that our method scales in terms of throughput and error rates, achieving sub-linear message complexity in domains that exhibit a geographic skew in the joining attributes.
[Temperature sensors, Availability, Costs, Aggregates, Discrete Fourier transforms, Prototypes, Telecommunication traffic, Throughput, Distributed computing, Monitoring]
Fast Algorithms for Heavy Distinct Hitters using Associative Memories
27th International Conference on Distributed Computing Systems
None
2007
Real-time detection of worm attacks, port scans and distributed denial of service (DDoS) attacks, as network packets belonging to these security attacks flow through a network router, is of paramount importance. In a typical worm attack, a worm infected host tries to spread the worm by scanning a number of other hosts thus resulting in significant number of network connections at an intermediate router. Detecting such attacks amounts to finding all hosts that are associated with unusually high number of other hosts, which is equivalent to solving the classic heavy distinct hitter problem over data streams. While several heavy distinct hitter solutions have been proposed and evaluated in a standard CPU setting, most of the above applications typically execute on special networking architectures called network processing units (NPUs). These NPUs interface with special associative memories known as the ternary content addressable memories (TCAMs) to provide gigabit rate forwarding at network routers. In this paper, we describe how the integrated architecture of NPU and TCAMs can be exploited to develop high-speed solutions for heavy distinct hitters.
[telecommunication security, invasive software, Computer worms, data streams, heavy distinct hitter, Computer crime, associative memory, ternary content addressable memory, network processing unit, Associative memory, worm attack, security attack, attack detection, Hardware, Computer security, Stock markets, network packet, distributed denial of service attack, Data security, network router, Computer science, telecommunication network routing, Sampling methods, Central Processing Unit, content-addressable storage]
uSense: A Unified Asymmetric Sensing Coverage Architecture for Wireless Sensor Networks
27th International Conference on Distributed Computing Systems
None
2007
As a key approach to achieve energy efficiency in sensor networks, sensing coverage has been studied extensively. Researchers have designed many coverage protocols to provide various kinds of service guarantees on the network lifetime, coverage ratio and detection delay. While these protocols are effective, they are not flexible enough to meet multiple design goals simultaneously. In this paper, we propose a unified sensing coverage architecture, called uSense, which features three novel ideas: asymmetric architecture, generic switching and global scheduling. We propose asymmetric architecture based on the conceptual separation of switching from scheduling. Switching is efficiently supported in sensor nodes, while scheduling is done in a separated computational entity, where multiple scheduling algorithms are supported. As an instance, we propose a two-level global coverage algorithm, called uScan. At the first level, coverage is scheduled to activate different portions of an area. We propose an optimal scheduling algorithm to minimize area breach. At the second level, sets of nodes are selected to cover active portions. Importantly, we show the feasibility to obtain optimal set-cover results in linear time if the layout of areas satisfies certain conditions. We evaluate our architecture with a network of 30 MicaZ motes, an extensive simulation with 10,000 nodes, as well as theoretical analysis. The results indicate that uSense is a promising architecture to support flexible and efficient coverage in sensor networks.
[Algorithm design and analysis, Protocols, coverage ratio, wireless sensor networks, unified asymmetric sensing coverage architecture, uScan, Optimal scheduling, Scheduling algorithm, Delay, Intelligent sensors, Wireless sensor networks, uSense, Processor scheduling, generic switching, Computer architecture, asymmetric architecture, global scheduling, Energy efficiency, two-level global coverage algorithm, network lifetime]
Non-Threshold based Event Detection for 3D Environment Monitoring in Sensor Networks
27th International Conference on Distributed Computing Systems
None
2007
Event detection is a crucial task for wireless sensor network applications, especially environment monitoring. Existing approaches for event detection are mainly based on some predefined threshold values, and thus are often inaccurate and incapable of capturing complex events. For example, in coal mine monitoring scenarios, gas leakage or water osmosis can hardly be described by the overrun of specified attribute thresholds, but some complex pattern in the full-scale view of the environmental data. To address this issue, we propose a non-threshold based approach for the real 3D sensor monitoring environment. We employ energy-efficient methods to collect a time series of data maps from the sensor network and detect complex events through matching the gathered data to spatio-temporal data patterns. Finally, we conduct trace driven simulations to prove the efficacy and efficiency of this approach on detecting events of complex phenomena from real-life records.
[Event detection, wireless sensor networks, Computerized monitoring, environmental factors, nonthreshold based event detection, wireless sensor network, Sensor phenomena and characterization, coal mine monitoring, gas leakage, Application software, monitoring, Computer science, Wireless sensor networks, Vehicle detection, 3D environment monitoring, water osmosis, Energy efficiency, Safety, Pattern matching]
A Multiphased Approach for Modeling and Analysis of the BitTorrent Protocol
27th International Conference on Distributed Computing Systems
None
2007
BitTorrent is one of the most popular protocols for content distribution and accounts for more than 15% of the total Internet traffic. In this paper, we present an analytical model of the protocol. Our work differs from previous works as it models the BitTorrent protocol specifically and not as a general file-swarming protocol. In our study, we observe that to accurately model the download process of a BitTorrent client, we need to split this process into three phases. We validate our model using simulations and real-world traces. Using this model, we study the efficiency of the protocol based on various protocol-specific parameters such as the maximum number of connections and the peer set size. Furthermore, we study the relationship between changes in the system parameters and the stability of the protocol. Our model suggests that the stability of BitTorrent protocol depends heavily on the number of pieces a file is divided into and the arrival rate of clients to the network.
[multiphased approach, Protocols, Costs, peer-to-peer computing, Peer to peer computing, Decision making, Stability analysis, Internet traffic, BitTorrent protocol analysis, Analytical models, content distribution, Bandwidth, peer-to-peer protocol, Traffic control, Internet, protocols]
Efficient Execution of Continuous Incoherency Bounded Queries over Multi-Source Streaming Data
27th International Conference on Distributed Computing Systems
None
2007
On-line decision making often involves query processing over time-varying data which arrives in the form of data streams from distributed locations. In such environments typically, a user application is interested in the value of some function defined over the data items. For example, the traffic management system can make control decisions based on the observed traffic at major intersections; stock investors can manage their investments based on the value of their portfolios. In this paper we present a system that supports pull based data refresh and query processing techniques where such queries access data from multiple distributed sources. Key challenges in supporting such Continuous Multi-Data Incoherency Bounded Queries lie in minimizing network and source overheads, without loss of fidelity in the query responses provided to users. We address these challenges by using mathematically sound approaches based on Gradient Descent and Constraint Optimization which allow us to adapt the refresh frequencies of the dynamically changing data and adjust the quality of service provided to different users.
[multiple distributed source, constraint optimization, Decision making, Process control, Quality of service, online decision making, pull-based data refresh, Control systems, quality of service, time-varying data, Constraint optimization, query processing, multisource streaming data, Exchange rates, optimisation, Query processing, Investments, decision making, distributed databases, Frequency, continuous incoherency bounded query execution, gradient descent method, gradient methods, Portfolios]
Automated Storage Reclamation Using Temporal Importance Annotations
27th International Conference on Distributed Computing Systems
None
2007
This work focuses on scenarios that require the storage of large amounts of data. Such systems require the ability to either continuously increase the storage space or reclaim space by deleting contents. Traditionally, storage systems relegated object reclamation to applications. In this work, content creators explicitly annotate the object using a temporal importance function. The storage system uses this information to evict less important objects. The challenge is to design importance functions that are simple and expressive. We describe a two step temporal importance function. We introduce the notion of storage importance density to quantify the importance levels for which the storage is full. Using extensive simulations and observations of a university wide lecture video capture and storage application, we show that our abstraction allows the users to express the amount of persistence for each individual object.
[automated storage reclamation, Multimedia systems, Roads, Transcoding, Application software, Distributed computing, Videoconference, storage management, Software libraries, temporal importance annotations, temporal importance function, video capture, Storage automation, storage systems, Frequency, Resource management]
LagOver: Latency Gradated Overlays
27th International Conference on Distributed Computing Systems
None
2007
We propose a new genre of overlay network for disseminating information from popular but resource constrained sources. We call this communication primitive as latency gradated overlay, where information consumers self- organize themselves according to their individual resource constraints and the latency they are willing to tolerate in receiving the information from the source. Such a communication primitive finds immediate use in applications like RSS feeds aggregation. We propose heuristic algorithms to construct LagOver based on preferably some partial knowledge of the network at users (no knowledge slows the construction process) but no global coordination. The algorithms are evaluated based on simulations and show good characteristics including convergence, satisfying peers' latency and bandwidth constraints even in presence of moderately high membership dynamics. There are two points worth noting. First, optimizing jointly for latency and capacity (i.e., placing nodes that have free capacity close to the source) as long as latency constraint of other nodes are not violated performs better than optimizing for latency only. The joint optimization strategy has faster convergence of the LagOver network, and can deal with adversarial workloads that optimization of only latency can not deal with. Secondly, somewhat counter-intuitively, in order to do the aforementioned joint optimization, it is sufficient to find random nodes based on only the latency constraint, since even if the capacity of individual nodes is saturated it does not matter since the LagOver network can potentially be reconfigured.
[Protocols, peer-to-peer computing, Peer to peer computing, Heuristic algorithms, joint optimization strategy, latency gradated overlays, Peer-to-Peer, latency constraints, Delay, Information dissemination, Convergence, LagOver network, Constraint optimization, RSS feeds aggregation, Web 2.0/RSS, Network servers, Heterogeneity, Algorithms, Bandwidth, Overlay Networks, bandwidth constraints, Feeds, Web server]
Defragmenting DHT-based Distributed File Systems
27th International Conference on Distributed Computing Systems
None
2007
Existing DHT-based file systems use consistent hashing to assign file blocks to random machines. As a result, a user task accessing an entire file or multiple files needs to retrieve blocks from many different machines. This paper demonstrates that significant availability and performance gains can be achieved if instead, users are able to retrieve all the data needed for a given task from only a few DHT nodes. We explore the design and implications of such a "defragmented" DHT-based distributed file system, called D2, that also maintains important DHT properties like storage load balance. We show using real-world file system traces that a simple key encoding scheme is sufficient to maintain good defragmentation for most user tasks. Using both simulation and an actual 1,000 node deployment, we show that D2 increases availability by over an order of magnitude and improves user-perceived latency by 30- 100% compared to a traditional design.
[Availability, Costs, distributed file systems, storage load balancing, Performance gain, distributed processing, Information retrieval, Encoding, D2 system, Delay, storage management, File systems, resource allocation, Aggregates, Prototypes, Concrete, distributed hash tables defragmenting]
Flashback: A Peer-to-PeerWeb Server for Flash Crowds
27th International Conference on Distributed Computing Systems
None
2007
We present Flashback, a ready-to-use system for scalably handling large unexpected traffic spikes on web-sites. Unlike previous systems, our approach does not rely on any intermediate nodes to cache content. Instead, the clients (browsers) create a dynamic, self-scaling peer-to-peer (P2P) Web-server that grows and shrinks according to the load. This approach translates into a challenging problem - a P2P data exchange protocol that can operate in churn rates where more than 90% of peers can leave the overlay in under 10 seconds. This is at least an order of magnitude higher churn rate than previously addressed research. Additionally, our system operates under two strict constraints - users are assured that they upload only as much as they download and second, end-user browsing experience is preserved, i.e., low latency downloads and zero configuration or download of any software. Various innovations were required to meet these challenges. Key among them are (a) A TCP-friendly, UDP protocol (Roulette) for tit-for-tat data exchange under extreme churn, (b) A novel data structure (NOIS) for partial-data management and (c) A distributed hole-punching protocol for automatic NAT traversal. Experimental results show the effectiveness and near optimal scaling of Flashback. For a Web-server (and clients) running on a DSL-like connection, end-user latency increases only one second for every doubling in Web-server load.
[Protocols, partial data management, Scalability, peer-to-peer networks, data exchange protocol, data structure, distributed hole punching protocol, Delay, tit-for-tat data exchange, Statistical distributions, flash crowds, data structures, Web server, Advertising, Network address translation, traffic spikes, Technological innovation, Flashback, peer-to-peer computing, Peer to peer computing, Data structures, electronic data interchange, transport protocols, Internet, Web sites, TCP-friendly UDP protocol, telecommunication traffic]
Selfishness, Not Always A Nightmare: Modeling Selfish MAC Behaviors in Wireless Mobile Ad Hoc Networks
27th International Conference on Distributed Computing Systems
None
2007
In wireless mobile ad hoc networks where nodes are selfish and non-cooperative, a natural and crucial question is how well or how bad the MAC layer protocol IEEE 802.11 DCF performs. In this paper, we study this question by modeling the selfish MAC protocol as a non- cooperative repeated game where players follow the TIT- FOR-TAT (TFT) strategy which is regarded as the best strategy in such environments. We show for single-hop ad hoc networks the game admits a number of Nash Equilibria (NE). We then perform NE refinement to eliminate the inefficient NE and show that there exists one efficient NE maximizing both local and global payoff. We also propose an algorithm to approach the efficient NE. We then extend our efforts to multi-hop case by showing that the game converges to a NE which may not be globally optimal but quasi- optimal in the sense that the global payoff is only slightly less than the optimal case. As conclusion, we answer the posed question by showing that selfishness does not always lead to network collapse. On the contrary, it can help the network operate at a NE globally which is optimal or quasi-optimal under the condition that players are long-sighted and follow the TFT strategy.
[IEEE 802.11 DCF, mobile radio, Stability, Wireless application protocol, game theory, Predictive models, MAC layer protocol, Ad hoc networks, Thin film transistors, access protocols, Game theory, Mobile ad hoc networks, single-hop ad hoc network, Feedback, Nash equilibria, Spread spectrum communication, Media Access Protocol, TIT-FOR-TAT strategy, wireless mobile ad hoc network, ad hoc networks, wireless LAN]
Optimizing Multicast Performance in Large-Scale WLANs
27th International Conference on Distributed Computing Systems
None
2007
Support for efficient multicasting in WLANs can enable new services such as streaming TV channels, radio channels, and visitor's information. With increasing deployments of large-scale WLANs, such services can have a significant impact. However, for a solution to be viable, the mutlicast services must minimally impact the existing unicast services which are currently the core services offered by most WLANs. This paper focuses on three objective functions motivated by different revenue functions and network scenarios: maximizing the number of users (MNU), balancing the load among APs (BLA), and minimizing the load of APs (MLA). We show that these problems are NP-hard and present centralized approximation algorithms and distributed approaches to solve them. Using simulations we evaluate the performance of these algorithms. We observe that the number of users can be increased by up to 36.9%, and the maximum AP load and the total load can be reduced by up to 52.9% and 31.1%, respectively.
[Wireless LAN, approximation theory, TV, NP-hard, Quality of service, multicast performance, centralized approximation algorithms, Multicast algorithms, Unicast, large-scale WLAN, multicast communication, Streaming media, Approximation algorithms, Communication system traffic control, Large-scale systems, wireless LAN, Signal to noise ratio, computational complexity]
A High Throughput Atomic Storage Algorithm
27th International Conference on Distributed Computing Systems
None
2007
This paper presents an algorithm to ensure the atomicity of a distributed storage that can be read and written by any number of clients. In failure-free and synchronous situations, and even if there is contention, our algorithm has a high write throughput and a read throughput that grows linearly with the number of available servers. The algorithm is devised with a homogeneous cluster of servers in mind. It organizes servers around a ring and assumes point-to-point communication. It is resilient to the crash failure of any number of readers and writers as well as to the crash failure of all but one server. We evaluated our algorithm on a cluster of 24 nodes with dual fast ethernet network interfaces (100 Mbps). We achieve 81 Mbps of write throughput and 8 x 90 Mbps of read throughput (with up to 8 servers) which conveys the linear scalability with the number of servers.
[Context, ethernet network interfaces, client-server systems, atomic storage algorithm, Ethernet networks, distributed storage, Scalability, orig-research, client servers, Throughput, Computer crashes, local area networks, crash failure, Network interfaces, point-to-point communication, system recovery, Delay, Resilience, network interfaces, Network servers, storage management, write throughput, read throughput, Clustering algorithms]
Fault Tolerance in Multiprocessor Systems Via Application Cloning
27th International Conference on Distributed Computing Systems
None
2007
Record and replay (RR) is a software based state replication solution designed to support recording and subsequent replay of the execution of unmodified applications running on multiprocessor systems for fault-tolerance. Multiple instances of the application are simultaneously executed in separate virtualized environments called containers. Containers facilitate state replication between the application instances by resolving the resource conflicts and providing a uniform view of the underlying operating system across all clones. The virtualization layer that creates the container abstraction actively monitors the primary instance of the application and synchronizes its state with that of the clones by transferring the necessary information to enforce identical state among them. In particular, we address the replication of relevant operating system state, such as network state to preserve network connections across failures, and the state that results from nondeterministic interleaved accesses to shared memory in SMP systems. We have implemented RR's state replication mechanisms in the Linux operating system by making novel use of existing features on the Intel and PowerPC architectures.
[Costs, multiprocessing systems, fault tolerance, virtualization, network state, Cloning, Containers, Application software, container abstraction, Application virtualization, Multiprocessing systems, Fault tolerance, record and replay, Operating systems, Linux, software based state replication, Fault tolerant systems, network connections, multiprocessor system, Linux operating system, Hardware, data structures, fault tolerant computing]
Secure Event Dissemination in Publish-Subscribe Networks
27th International Conference on Distributed Computing Systems
None
2007
Secure event dissemination in a pub-sub network refers to secure distribution of events to clients subscribing to those events without revealing the secret attributes in the event to the unauthorized subscribers and the routing nodes in a pub-sub network. A common solution to provide confidentiality guarantees for the secret attributes in an event is to encrypt so that only authorized subscribers can read them. The key challenge here is to build a secure and scalable content-based event dissemination infrastructure that can handle complex and flexible subscription models while preserving the efficiency and scalability of key management algorithms. In this paper, we describe the design and implementation of PSGuard, for secure event dissemination in pub-sub networks. PSGuard exploit hierarchical key derivation algorithms to encode publication-subscription matching semantics for scalable key management. An experimental evaluation of our prototype system shows that PSGuard meets the security requirements while maintaining the performance and scalability of a pub-sub network.
[telecommunication security, pattern matching, telecommunication network management, Scalability, Subscriptions, hierarchical key derivation algorithm, routing node, publish-subscribe network, Information filtering, Authorization, encryption, authorisation, Information filters, secret attribute confidentiality guarantee, Cryptography, middleware, message passing, PSGuard event dissemination, secure event distribution, Routing, Educational institutions, cryptography, secure content-based event dissemination, Matched filters, key management algorithm, telecommunication network routing, Publish-subscribe, publication-subscription matching semantics encoding]
Temporal Privacy in Wireless Sensor Networks
27th International Conference on Distributed Computing Systems
None
2007
Although the content of sensor messages describing "events of interest" may be encrypted to provide confidentiality, the context surrounding these events may also be sensitive and therefore should be protected from eavesdroppers. An adversary armed with knowledge of the network deployment, routing algorithms, and the base-station (data sink) location can infer the temporal patterns of interesting events by merely monitoring the arrival of packets at the sink, thereby allowing the adversary to remotely track the spatio-temporal evolution of a sensed event. In this paper, we introduce the problem of temporal privacy for delay- tolerant sensor networks and propose adaptive buffering at intermediate nodes on the source-sink routing path to obfuscate temporal information from an adversary. We first present the effect of buffering on temporal privacy using an information-theoretic formulation and then examine the effect that delaying packets has on buffer occupancy. We evaluate our privacy enhancement strategies using simulations, where privacy is quantified in terms of the adversary's estimation error.
[Disruption tolerant networking, Adaptive systems, wireless sensor networks, Delay effects, source-sink routing path, Sensor phenomena and characterization, temporal privacy, network deployment, Routing, cryptography, routing algorithms, Privacy, Wireless sensor networks, spatio-temporal evolution, privacy enhancement strategies, telecommunication network routing, temporal patterns, Cryptography, Protection, Remote monitoring, information-theoretic formulation]
Compromising Location Privacy inWireless Networks Using Sensors with Limited Information
27th International Conference on Distributed Computing Systems
None
2007
We propose a methodology to identify nodes in fully anonymized wireless networks using collections of very simple sensors. Based on time series of counts of anonymous packets provided by the sensors, we estimate the number of nodes using principal component analysis. We then proceed to separate the collected packet data into traffic flows that, with help of the spatial diversity in the available sensors, can be used to estimate the location of the wireless nodes. Our simulation experiments indicate that the estimators show high accuracy and high confidence for anonymized TCP traffic. Additional experiments indicate that the estimators perform very well in anonymous wireless networks that use traffic padding.
[telecommunication security, Wireless LAN, Source separation, wireless sensor networks, traffic padding, Telecommunication traffic, wireless networks, anonymized TCP traffic, Information analysis, Privacy, Wireless sensor networks, sensors, Wireless networks, location privacy, Traffic control, Signal analysis, principal component analysis, telecommunication traffic, Principal component analysis]
Testing Security Properties of Protocol Implementations - a Machine Learning Based Approach
27th International Conference on Distributed Computing Systems
None
2007
Security and reliability of network protocol implementations are essential for communication services. Most of the approaches for verifying security and reliability, such as formal validation and black-box testing, are limited to checking the specification or conformance of implementation. However, in practice, a protocol implementation may contain engineering details, which are not included in the system specification but may result in security flaws. We propose a new learning-based approach to systematically and automatically test protocol implementation security properties. Protocols are specified using symbolic parameterized extended finite state machine (SP-EFSM) model, and an important security property - message confidentiality under the general Dolev-Yao attacker model - is investigated. The new testing approach applies black-box checking theory and a supervised learning algorithm to explore the structure of an implementation under test while simulating the teacher with a conformance test generation scheme. We present the testing procedure, analyze its complexity, and report experimental results.
[telecommunication security, network protocol implementation security property, network protocol reliability, System testing, Protocols, supervised learning algorithm, Reliability engineering, Communication system security, symbolic parameterized extended finite state machine model, conformance test generation scheme, black-box checking theory, learning (artificial intelligence), protocols, network protocol security, message confidentiality, Automation, testing, Dolev-Yao attacker model, machine learning, security of data, Automatic testing, Supervised learning, Automata, Machine learning, Telecommunication network reliability]
Protocol Design for Dynamic Delaunay Triangulation
27th International Conference on Distributed Computing Systems
None
2007
Delaunay triangulation (DT) is a useful geometric structure for networking applications. In this paper we investigate the design of join, leave, and maintenance protocols to construct and maintain a distributed DT dynamically. We define a distributed DT and present a necessary and sufficient condition for a distributed DT to be correct. This condition is used as a guide for protocol design. We present join and leave protocols as well as correctness proofs for serial joins and leaves. In addition, to handle concurrent joins and leaves as well as node failures, we present a maintenance protocol. An accuracy metric is defined for a distributed DT. Experimental results show that our join, leave and maintenance protocols are scalable, and they achieve high accuracy for systems under churn and with node failures. We also present application protocols for greedy routing, clustering, broadcast, and multicast within a radius, and discuss and prove their correctness.
[broadcast, Multicast protocols, mesh generation, Application software, History, greedy routing, multicast, Computer science, Sufficient conditions, dynamic Delaunay triangulation, Virtual reality, Broadcasting, Computer networks, Routing protocols, clustering, protocol design, maintenance protocol, protocols, node failures]
Stabilizing Peer-to-Peer Spatial Filters
27th International Conference on Distributed Computing Systems
None
2007
In this paper, we propose and prove correct a distributed stabilizing implementation of an overlay, called DR-tree, optimized for efficient selective dissemination of information. DR-tree copes with nodes dynamicity (frequent joins and leaves) and memory and counter program corruptions, that is, the processes can connect/disconnect at any time, and their memories and programs can be corrupted. The maintenance of the structure is local and requires no additional memory to guarantee its stabilization. The structure is balanced and is of height 0(log<sub>m</sub>(N)), which makes it suitable for performing efficient data storage or search. We extend our overlay in order to support complex content-based filtering in publish/subscribe systems. Publish/subscribe systems provide useful platforms for delivering data (events) from publishers to subscribers in a decoupled fashion in distributed networks. Developing efficient publish/subscribe schemes in dynamic distributed systems is still an open problem for complex subscriptions (spanning multi-dimensional intervals). Embedding a publish/subscribe system in a DR-trees is a new and viable solution. The DR-tree overlay also guarantees subscription and publication times logarithmic in the size of the network while keeping its space requirement low (comparable to its DHT-based counterparts). Nonetheless, the DR- tree overlay helps in eliminating the false negatives and drastically reduces the false positives in the embedded publish/subscribe system.
[Costs, self-organization, Subscriptions, Memory, peer-to-peer spatial filter, Counting circuits, dynamic distributed system, data storage, tree data structures, publish/subscribe system, middleware, distributed network, Content-based routing, publish/subscribe, Filtering, peer-to-peer computing, Peer to peer computing, information dissemination, Routing, publication times logarithmic, Maintenance, peer-to-peer, Computer science, telecommunication network routing, DR-tree, Spatial filters, stabilizing dynamic R-trees., content-based filter, program corruption]
Distributed Density Estimation Using Non-parametric Statistics
27th International Conference on Distributed Computing Systems
None
2007
Learning the underlying model from distributed data is often useful for many distributed systems. In this paper, we study the problem of learning a non-parametric model from distributed observations. We propose a gossip-based distributed kernel density estimation algorithm and analyze the convergence and consistency of the estimation process. Furthermore, we extend our algorithm to distributed systems under communication and storage constraints by introducing a fast and efficient data reduction algorithm. Experiments show that our algorithm can estimate underlying density distribution accurately and robustly with only small communication and storage overhead.
[Algorithm design and analysis, Protocols, Density measurement, Peer to peer computing, Distributed Estimation, Distributed computing, Statistics, Convergence, Non-parametric, Asia, Gossip, Statistical distributions, Kernel Density Estimation, Data Reduction, Robustness, data handling, gossip-based distributed kernel density estimation algorithm, statistical analysis, nonparametric statistics, Kernel, distributed data]
Dynamic and Redundant Data Placement
27th International Conference on Distributed Computing Systems
None
2007
We present a randomized block-level storage virtualization for arbitrary heterogeneous storage systems that can distribute data in a fair and redundant way and can adapt this distribution in an efficient way as storage devices enter or leave the system. More precisely, our virtualization strategies can distribute a set of data blocks among a set of storage devices of arbitrary non-uniform capacities so that a storage device representing x% of the capacity in the system will get x% of the data (as long as this is in principle possible) and the different copies of each data block are stored so that no two copies of a data block are located in the same device. Achieving these two properties is not easy, and no virtualization strategy has been presented so far that has been formally shown to satisfy fairness and redundancy while being time- and space-efficient and allowing an efficient adaptation to a changing set of devices.
[Power system management, Scalability, virtual storage, redundant data placement, randomized block-level storage virtualization, dynamic data placement, heterogeneous storage system]
Optimizing Peer Relationships in a Super-Peer Network
27th International Conference on Distributed Computing Systems
None
2007
Super-peer architectures exploit the heterogeneity of nodes in a P2P network by assigning additional responsibilities to higher-capacity nodes. In the design of a super-peer network for file sharing, several issues have to be addressed: how client peers are related to super-peers, how super-peers locate files, how the load is balanced among the super-peers, and how the system deals with node failures. In this paper we introduce a self-organizing super-peer network architecture (SOSPNET) that solves these issues in a fully decentralized manner. SOSPNET maintains a super-peer network topology that reflects the semantic similarity of peers sharing content interests. Super-peers maintain semantic caches of pointers to files which are requested by peers with similar interests. Client peers, on the other hand, dynamically select super-peers offering the best search performance. We show how this simple approach can be employed not only to optimize searching, but also to solve generally difficult problems encountered in P2P architectures such as load balancing and fault tolerance. We evaluate SOSPNET using a model of the semantic structure derived from the 8-month traces of two large file-sharing communities. The obtained results indicate that SOSPNET achieves close-to-optimal file search performance, quickly adjusts to changes in the environment (node joins and leaves), survives even catastrophic node failures, and efficiently distributes the system load taking into account peer capacities.
[peer relationships, Protocols, fault tolerance, peer-to-peer computing, load balancing, Peer to peer computing, P2P network, telecommunication network topology, Routing, self-organizing super-peer network architecture, Design optimization, SOSPNET, Fault tolerance, Network servers, Network topology, resource allocation, Load management, file sharing, super-peer network topology]
Scale-Free Overlay Topologies with Hard Cutoffs for Unstructured Peer-to-Peer Networks
27th International Conference on Distributed Computing Systems
None
2007
In unstructured peer-to-peer (P2P) networks, the overlay topology (or connectivity graph) among peers is a crucial component in addition to the peer/data organization and search. Topological characteristics have profound impact on the efficiency of search on such unstructured P2P networks as well as other networks. A key limitation of scale- free (power-law) topologies is the high load (i.e. high degree) on very few number of hub nodes. In a typical unstructured P2P network, peers are not willing to maintain high degrees/loads as they may not want to store large number of entries for construction of the overlay topology. So, to achieve fairness and practicality among all peers, hard cutoffs on the number of entries are imposed by the individual peers, which limits scale-freeness of the overall topology. Thus, it is expected that efficiency of the flooding search reduces as the size of the hard cutoff does. We investigate construction of scale-free topologies with hard cutoffs and effect of these hard cutoffs on the search efficiency.
[Protocols, peer-to-peer computing, Peer to peer computing, Laboratories, graph theory, connectivity graph, telecommunication network topology, Data engineering, Distributed power generation, Floods, Distributed computing, Computer science, data organization, Network topology, scale-free overlay topology, unstructured peer-to-peer networks, peer organizatiion]
Build One, Get One Free: Leveraging the Coexistence of Multiple P2P Overlay Networks
27th International Conference on Distributed Computing Systems
None
2007
Many different P2P overlay networks providing various functionalities, targeting specific applications, have been proposed in the past five years. It is now reasonable to consider that multiple overlays may be deployed over a large set of nodes so that the most appropriate overlay might be chosen depending on the application. A physical peer may then host several instances of logical peers belonging to different overlay networks. In this paper, we show that the coexistence of a structured P2P overlay and an unstructured one may be leveraged so that, by building one, the other is automatically constructed as well. More specifically, we show that the randomness provided by an unstructured gossip-based overlay can be used to build the routing table of a structured P2P overlay and the randomness in the numerical proximity links in the structured networks provides the random peer sampling required by gossip-based unstructured overlays. In this paper, we show that maintaining the leaf set of Pastry and the proximity links of an unstructured overlay is enough to build the complete overlays. Simulation results comparing our approach with both a Pastry-like system and a gossip-based unstructured overlay show that we significantly reduce the overlay maintenance overhead without sacrificing the performance.
[Costs, peer-to-peer computing, Peer to peer computing, Keyword search, Telecommunication traffic, multiple P2P overlay network, Routing, Distributed computing, numerical proximity link, Network servers, telecommunication network routing, unstructured gossip-based overlay, routing table, Sampling methods, Internet, structured P2P overlay coexistence, protocols, Web server, random peer sampling, Pastry leaf set, clustered-gossip protocol]
EnviroMic: Towards Cooperative Storage and Retrieval in Audio Sensor Networks
27th International Conference on Distributed Computing Systems
None
2007
This paper presents EnviroMic, a novel distributed acoustic monitoring, storage, and trace retrieval system. Audio represents one of the least exploited modalities in sensor networks to date. The relatively high frequency and large size of audio traces motivate distributed algorithms for coordinating recording tasks, reducing redundancy of data stored by nearby sensors, filtering out silence, and balancing storage utilization in the network. Applications of acoustic monitoring with EnviroMic range from the study of mating rituals and social behavior of animals in the wild to audio surveillance of military targets. EnviroMic is designed for disconnected operation, where the luxury of having a basestation cannot be assumed. We implement the system on a Tiny OS-based platform and systematically evaluate its performance through both indoor testbed experiments and a preliminary outdoor deployment. Results demonstrate up to a 4-fold improvement in effective storage capacity of the network compared to uncoordinated recording.
[trace retrieval system, System testing, distributed acoustic monitoring, wireless sensor networks, orig-research, sensor networks, audio representation, audio surveillance, Acoustic sensors, Distributed algorithms, Monitoring, Acoustic applications, Filtering, information retrieval, Audio recording, information storage, audio sensor networks, Animals, storage utilization, Surveillance, cooperative storage, distributed algorithms, EnviroMic, Frequency, military targets]
Supporting Multi-Dimensional Range Query for Sensor Networks
27th International Conference on Distributed Computing Systems
None
2007
This paper presents the design of Pool, an efficient and scalable data storage scheme for supporting multidimensional queries. The foundation of the work that makes the Pool approach superior in executing multi-dimensional queries is that it provides a novel and elegant higher dimension to two-dimensional data mapping mechanism. Our performance study proves the efficiency of the design.
[Multidimensional systems, Costs, Memory, multi-dimensional range query, Sensor phenomena and characterization, Data engineering, Data-centric, Sensor systems, sensor networks, two-dimensional data mapping, distributed sensors, Temperature sensors, Computer science, query processing, scalable data storage scheme, Range query, Design engineering, Multidimensional, Distributed control, Sensor networks, Pool, Query processing.]
Iso-Map: Energy-Efficient Contour Mapping in Wireless Sensor Networks
27th International Conference on Distributed Computing Systems
None
2007
Contour mapping is a crucial part of many wireless sensor network applications. Many efforts have been made to avoid collecting data from all the sensors in the network and producing maps at the sink, which is proven to be inefficient. The existing approaches (often aggregation based), however, suffer from heavy transmission traffic and incur large computational overheads on each sensor node. We propose Iso-Map, an energy-efficient protocol for contour mapping, which builds contour maps based solely on the reports collected from intelligently selected "isoline nodes" in wireless sensor networks. Iso-Map achieves high-quality contour mapping while significantly reducing the generated traffic from O(n) to O(radicn), where n is the total number of sensor nodes in the field. The per-node computation overhead is also restrained as a constant. We conduct comprehensive trace-driven simulations to verify this protocol, and demonstrate that Iso-Map outperforms the previous approaches in the sense that it produces contour maps of high fidelity with significantly reduced energy cost.
[Costs, Iso-Map, wireless sensor networks, Wireless application protocol, Computational modeling, Telecommunication traffic, communication complexity, Intelligent sensors, Wireless sensor networks, Intelligent networks, energy-efficient protocol, Traffic control, Energy efficiency, contour mapping, Computational intelligence, telecommunication traffic, isoline nodes]
D-Swoosh: A Family of Algorithms for Generic, Distributed Entity Resolution
27th International Conference on Distributed Computing Systems
None
2007
Entity resolution (ER) matches and merges records that refer to the same real-world entities, and is typically a compute-intensive process due to complex matching functions and high data volumes. We present a family of algorithms, D-Swoosh, for distributing the ER workload across multiple processors. The algorithms use generic match and merge functions, and ensure that new merged records are distributed to processors that may have matching records. We perform a detailed performance evaluation on a testbed of 15 processors. Our experiments use actual comparison shopping data provided by Yahoo!.
[Performance evaluation, Dictionaries, merging, generic distributed entity resolution, D-Swoosh, Distributed computing, Program processors, Databases, merge functions, Parallel processing, Cameras, Digital audio players, generic match functions]
A Weighted Moving Average-based Approach for Cleaning Sensor Data
27th International Conference on Distributed Computing Systems
None
2007
Nowadays, wireless sensor networks have been widely used in many monitoring applications. Due to the low quality of sensors and random effects of the environments, however, it is well known that the collected sensor data are noisy. Therefore, it is very critical to clean the sensor data before using them to answer queries or conduct data analysis. Popular data cleaning approaches, such as the moving average, cannot meet the requirements of both energy efficiency and quick response time in many sensor related applications. In this paper, we propose a hybrid sensor data cleaning approach with confidence. Specifically, we propose a smart weighted moving average (WMA) algorithm that collects confidence data from sensors and computes the weighted moving average. The rationale behind the WMA algorithm is to draw more samples for a particular value that is of great importance to the moving average, and provide higher confidence weight for this value, such that this important value can be quickly reflected in the moving average. Based on our extensive simulation results, we demonstrate that, compared to the simple moving average (SMA), our WMA approach can effectively clean data and offer quick response time.
[Data analysis, wireless sensor networks, weighted moving average algorithm, Noise reduction, Cleaning, confidence data, Delay, Intelligent sensors, Wireless sensor networks, Working environment noise, sensor data cleaning, Sampling methods, Energy efficiency, data handling, moving average processes, Monitoring]
Two-Tier Multiple Query Optimization for Sensor Networks
27th International Conference on Distributed Computing Systems
None
2007
When there are multiple queries posed to the resource-constrained wireless sensor network, it is critical to process them efficiently. In this paper, we propose a two-tier multiple query optimization (TTMQO) scheme. The first tier, called base station optimization, adopts a cost-based approach to rewrite a set of queries into an optimized set that shares the commonality and eliminates the redundancy among the queries in the original set. The optimized queries are then injected into the wireless sensor network. In the second tier, called in-network optimization, our scheme efficiently delivers query results by taking advantage of the broadcast nature of the radio channel and sharing the sensor readings among similar queries over time and space at a finer granularity. Our experimental results indicate that our proposed TTMQO scheme offers significant improvements over the traditional single query optimization technique.
[Base stations, in-network optimization, wireless sensor networks, Heuristic algorithms, Data acquisition, radio channel, base station optimization, Sensor systems, Radio broadcasting, resource-constrained wireless sensor network, Computer science, Wireless sensor networks, optimisation, Query processing, two-tier multiple query optimization, Cost function, wireless channels, Monitoring, single query optimization technique]
Resilient Cluster Formation for Sensor Networks
27th International Conference on Distributed Computing Systems
None
2007
Sensor nodes are often organized into clusters to facilitate many network operations such as data aggregation. Clustering has to be protected in hostile environments. Otherwise, an attacker can easily mislead a cluster-based application by attacking the clustering protocol. This paper proposes three techniques for resilient cluster formation. The simple neighbor validation provides a simple yet effective way to validate a sensor's neighbors; the priority-based selection organizes clusters basedon the sensor's priority of being a cluster head; and the centralized detection further enhances the security by detecting misbehaving nodes. Another appealing benefit of this protocol is that a sensor node can make a clustering decision immediately once the neighborhood information is available. This further increases the difficulty of attacking the clustering protocol. The analysis also shows that the proposed protocol is efficient and effective in dealing with attacks.
[telecommunication security, Protocols, Portable computers, wireless sensor networks, misbehaving node detection, distributed sensor network, network security, Laboratories, Electronic mail, resilient cluster formation, pattern clustering, clustering protocol, priority-based selection, Information security, Authentication, Computer networks, Cryptography, protocols, Protection, Personal digital assistants]
Combating Double-Spending Using Cooperative P2P Systems
27th International Conference on Distributed Computing Systems
None
2007
An electronic cash system allows users to withdraw coins, represented as bit strings, from a bank or broker, and spend those coins anonymously at participating merchants, so that the broker cannot link spent coins to the user who withdraws them. A variety of schemes with various security properties have been proposed for this purpose, but because strings of bits are inherently copyable, they must all deal with the problem of double-spending. In this paper, we present an electronic cash scheme that introduces a new peer-to-peer system architecture to prevent double-spending without requiring an on-line trusted party or tamper-resistant software or hardware. The scheme is easy to implement, computationally efficient, and provably secure. To demonstrate this, we report on a proof-of-concept implementation for Internet vendors along with a detailed complexity analysis and selected security proofs.
[Costs, peer-to-peer computing, Peer to peer computing, cooperative P2P systems, Internet vendors, peer-to-peer system architecture, Credit cards, security properties, Security, Application software, electronic money, Computer science, security of data, Computer architecture, double-spending, Hardware, online trusted party, security proofs, Internet, Consumer electronics, electronic cash system, tamper-resistant software]
Modeling Propagation Dynamics of Bluetooth Worms
27th International Conference on Distributed Computing Systems
None
2007
The growing popularity of mobile devices in the last few years has made them attractive to virus and worm writers. One communication channel exploited by mobile malware is the Bluetooth interface. In this paper, we present a detailed analytical model that characterizes the propagation dynamics of Bluetooth worms. Our model captures not only the behavior of the Bluetooth protocol but also the impact of mobility patterns on the Bluetooth worm propagation. Validation experiments against a detailed discrete-event Bluetooth worm simulator reveal that our model predicts the propagation dynamics of Bluetooth worms with high accuracy.
[discrete-event Bluetooth worm simulator, invasive software, Bluetooth, Computer worms, Protocols, Peer to peer computing, Laboratories, Predictive models, user interfaces, Surges, Analytical models, mobile malware, propagation dynamics, Communication channels, Internet, Bluetooth interface]
Asymptotic Connectivity in Wireless Networks Using Directional Antennas
27th International Conference on Distributed Computing Systems
None
2007
Connectivity is a crucial issue in wireless networks. Gupta and Kumar show that with omnidirectional antennas, the critical transmission range for a wireless network to achieve asymptotic connectivity is O(radiclog n/n) if n nodes are uniformly and independently distributed in a disk of unit area. In this paper, we investigate the connectivity problem when directional antennas are used. We find that there also exists a critical transmission range, which corresponds to a critical transmission power. We show that in the same propagation environment, when directional antennas use the optimal antenna pattern, the critical transmission power could be much smaller than that in networks using omnidirectional antennas. Moreover, to achieve asymptotic connectivity, it is known that each node has to have O(log n) neighbors when using omnidirectional antennas. We show that even using the transmission power level at which each node has only O(1) neighbors when using omnidirectional antennas, we can still achieve the asymptotic connectivity with directional antennas.
[Energy consumption, wireless sensor networks, Transmitting antennas, optimal antenna pattern, Interference, wireless networks, critical transmission power, asymptotic connectivity, Wireless networks, Wireless personal area networks, directive antennas, antenna radiation patterns, omnidirectional antennas, Directional antennas, Directive antennas, critical transmission range, Brain modeling, Propagation losses, Antennas and propagation, transmission power level, computational complexity]
Heuristic Approaches to Energy-Efficient Network Design Problem
27th International Conference on Distributed Computing Systems
None
2007
Energy management remains a critical problem in wireless networks since battery technology cannot keep up with rising communication expectations. Current approaches to energy conservation reduce the energy consumption of the wireless interface either for a given communication task or during idling. However, a complete solution requires minimizing the energy spent for both communication (i.e., for data and control overhead) and idling. This problem can be expressed as an energy-efficient network design problem, which is, not surprisingly, NP-hard. Therefore, in this paper, we study three heuristic approaches. Our study shows that the first approach that prioritizes communication energy conservation does not save energy. The second approach, which tries to reduce energy used for both data and in idling, becomes cost-prohibitive due to its high control overhead. Hence, we propose a third approach that prioritizes idling energy conservation. Due to its low control overhead, this approach meets the challenge of operating the network with low energy cost.
[Energy consumption, radio networks, Costs, energy-efficient network design problem, Communication system control, wireless interface, Switches, Routing, Batteries, idling energy conservation, NP-hard problem, Wireless networks, Energy conservation, energy management, communication energy conservation, Energy efficiency, wireless network, Energy management, energy consumption, heuristic approach]
Efficient Backbone Construction Methods in MANETs Using Directional Antennas
27th International Conference on Distributed Computing Systems
None
2007
In this paper, we consider the issue of constructing an energy-efficient virtual network backbone in mobile ad hoc networks (MANETs) for broadcasting applications using directional antennas. In directional antenna models, the transmission/reception range is divided into several sectors and one or more sectors can be switched on for transmission. Therefore, data forwarding can be restricted to certain directions (sectors), and both energy consumption and interference can be reduced. We develop the notation of directional network backbone using the directional antenna model, and form the problem of the directional connected dominating set (DCDS) which is an extreme case of the directional network backbone using an unlimited number of directional antennas. The minimum DCDS problem is proved to be NP-complete. A localized heuristic algorithm for constructing a small DCDS is proposed. Performance analysis includes an analytical study in terms of an approximation ratio and a simulation study on the proposed algorithms using a custom simulator.
[Energy consumption, localized heuristic algorithm, Heuristic algorithms, Spine, data forwarding, set theory, broadcast antennas, Mobile ad hoc networks, energy-efficient virtual network backbone construction, Analytical models, reception range, optimisation, directive antennas, interference reduction, Directional antennas, Broadcasting, minimum DCDS problem, directional antenna, Performance analysis, energy consumption, mobile ad hoc network, interference suppression, mobile radio, Interference, NP-complete problem, broadcasting application, Energy efficiency, directional connected dominating set, ad hoc networks, transmission range, computational complexity]
Conflict Managers for Self-stabilization without Fairness Assumption
27th International Conference on Distributed Computing Systems
None
2007
In this paper, we specify the conflict manager abstraction. Informally, a conflict manager guarantees that any two nodes that are in conflict cannot enter their critical section simultaneously (safety), and that at least one node is able to execute its critical section (progress). The conflict manager problem is strictly weaker than the classical local mutual exclusion problem, where any node that requests to enter its critical section eventually does so (fairness). We argue that conflict managers are a useful mechanism to transform a large class of self-stabilizing algorithms that operate in an essentially sequential model, into self-stabilizing algorithm that operate in a completely asynchronous distributed model. We provide two implementations (one deterministic and one probabilistic) of our abstraction, and provide a composition mechanism to obtain a generic transformer. Our transformers have low overhead: the deterministic transformer requires one memory bit, and guarantees time overhead in order of the network degree, the probabilistic transformer does not require extra memory. While the probabilistic algorithm performs in anonymous networks, it only provides probabilistic stabilization guarantees. In contrast, the deterministic transformer requires initial symmetry breaking but preserves the original algorithm guarantees.
[Performance evaluation, probabilistic transformer, self-stabilization, probabilistic abstraction, probabilistic stabilization, distributed processing, asynchronous distributed model, Discrete event simulation, conflict manager abstraction, Distributed computing, system recovery, Scheduling algorithm, scheduler, Sufficient conditions, generic transformer, Feedback, probabilistic algorithm, Writing, scheduling, Safety, Large-scale systems, deterministic transformer, deterministic abstraction]
Differentiated Data Persistence with Priority Random Linear Codes
27th International Conference on Distributed Computing Systems
None
2007
Both peer-to-peer and sensor networks have the fundamental characteristics of node churn and failures. Peers in P2P networks are highly dynamic, whereas sensors are not dependable. As such, maintaining the persistence of periodically measured data in a scalable fashion has become a critical challenge in such systems, without the use of centralized servers. To better cope with node dynamics and failures, we propose priority random linear codes, as well as their affiliated pre-distribution protocols, to maintain measurement data in different priorities, such that critical data have a higher opportunity to survive node failures than data of less importance. A salient feature of priority random linear codes is the ability to partially recover more important subsets of the original data with higher priorities, when it is not feasible to recover all of them due to node dynamics. We present extensive analytical and experimental results to show the effectiveness of priority random linear codes.
[differentiated data persistence, Protocols, linear codes, peer-to-peer computing, Peer to peer computing, Computerized monitoring, peer-to-peer networks, pre-distribution protocols, Sensor phenomena and characterization, Time measurement, sensor networks, node churn, distributed sensors, P2P networks, priority random linear codes, random codes, Linear code, Network servers, Wireless sensor networks, Streaming media, Network coding, protocols]
Scheduling to Minimize theWorst-Case Loss Rate
27th International Conference on Distributed Computing Systems
None
2007
We study link scheduling in networks with small router buffers, with the goal of minimizing the guaranteed packet loss rate bound for each ingress-egress traffic aggregate (connection). Given a link scheduling algorithm (a service discipline and a packet drop policy), the guaranteed loss rate for a connection is the loss rate under worst-case routing and bandwidth allocations for competing traffic. Under simplifying assumptions, we show that a local min-max fairness property with respect to apportioning loss events among the connections sharing each link, and a condition on the correlation of scheduling decisions at different links are two necessary and (together) sufficient conditions for optimality in the minimization problem. Based on these conditions, we introduce a randomized link-scheduling algorithm called rolling priority where packet scheduling at each link relies exclusively on local information. We show that RP satisfies both conditions and is therefore optimal.
[packet switching, Telecommunication traffic, router buffers, minimax techniques, worst-case loss rate, scheduling, packet scheduling, Optical packet switching, local min-max fairness property, Routing, rolling priority, bandwidth allocations, Scheduling algorithm, Computer science, bandwidth allocation, worst-case routing, Processor scheduling, Aggregates, Optical buffering, telecommunication network routing, network scheduling, Channel allocation, minimization problem, Performance loss, Internet, randomized link-scheduling algorithm, minimisation]
mTreebone: A Hybrid Tree/Mesh Overlay for Application-Layer Live Video Multicast
27th International Conference on Distributed Computing Systems
None
2007
Application-layer overlay networks have recently emerged as a promising solution for live media multicast on the Internet. A tree is probably the most natural structure for a multicast overlay, but is vulnerable in the presence of dynamic end-hosts. Data-driven approaches form a mesh out of overlay nodes to exchange data, which greatly enhances the resilience. It however suffers from an efficiency-latency tradeoff, given that the data have to be pulled from mesh neighbors with periodical notifications. In this paper, we suggest a novel hybrid tree/mesh design that leverages both overlays. The key idea is to identify a set of stable nodes to construct a tree-based backbone, called treebone, with most of the data being pushed over this backbone. These stable nodes, together with others, are further organized through an auxiliary mesh overlay, which facilitates the treebone to accommodate node dynamics and fully exploit the available bandwidth between overlay nodes. This hybrid design, referred to as mTreebone, is braced by our real trace studies, which show strong evidence that the performance of an overlay closely depends on a small set of backbone nodes. It however poses a series of unique and critical design challenges, in particular, the identification of stable nodes and seamless data delivery using both push and pull methods. In this paper, we present optimized solutions to these problems, which reconcile the two overlays under a coherent framework with controlled overhead. We evaluate mTreebone through both simulations and PlanetLab experiments. The results demonstrate the superior efficiency and robustness of this hybrid solution.
[Tree data structures, live media multicast, Peer to peer computing, multicast protocol, Spine, trees (mathematics), auxiliary mesh overlay, Proposals, mTreebone, Vehicle dynamics, Delay, Resilience, Asia, multicast protocols, live video streaming system, application-layer live video multicast, data-driven approach, tree-based backbone, Bandwidth, video streaming, Internet, IP networks, hybrid tree-mesh overlay]
Understanding Instant Messaging Traffic Characteristics
27th International Conference on Distributed Computing Systems
None
2007
Instant messaging (IM) has become increasingly popular due to its quick response time, its ease of use, and possibility of multitasking. It is estimated that there are several millions of instant messaging users who use IM for various purposes: simple requests and responses, scheduling face to face meetings, or just to check the availability of colleagues and friends. Despite its popularity and user base, little has been done to characterize IM traffic. One reason might be its relatively small traffic volume, although this is changing as more users start using video or voice chats and file attachments. Moreover, all major instant messaging systems route text messages through central servers. While this facilitates firewall traversal and gives instant messaging companies more control, it creates a potential bottleneck at the instant messaging servers. This is especially so for large instant messaging operators with tens of millions of users and during flash crowd events. Another reason for the lack of previous studies is the difficulty in getting access to instant messaging traces due to privacy concerns. In this paper, we analyze the traffic of two popular instant messaging systems, AOL Instant Messenger (AIM) and MSN/Windows Live Messenger, from thousands of employees in a large enterprise. We found that most instant messaging traffic is due to presence, hints, or other extraneous traffic. Chat messages constitute only a small percentage of the total IM traffic. This means, during overload, IM servers can protect the instantaneous nature of the communication by dropping extraneous traffic. We also found that the social network of lM users does not follow a power law distribution. It can be characterized by a Weibull distribution. Our analysis sheds light on instant messaging system design and optimization and provides a scientific basis for instant messaging workload generation.
[privacy concerns, Social network services, instant messaging traffic characteristics, MSN/Windows Live Messenger, electronic messaging, Telecommunication traffic, Multitasking, file attachment, text messages, Delay, System analysis and design, firewall traversal, Design optimization, Privacy, Network servers, instant messaging systems, face to face meeting, Power system protection, instant messaging operators, AOL Instant Messenger, Weibull distribution, telecommunication traffic]
On Hit Inflation Techniques and Detection in Streams of Web Advertising Networks
27th International Conference on Distributed Computing Systems
None
2007
Click fraud is jeopardizing the industry of Internet advertising. Internet advertising is crucial for the thriving of the entire Internet, since it allows producers to advertise their products, and hence contributes to the well being of e-commerce. Moreover, advertising supports the intellectual value of the Internet by covering the running expenses of the content publishers' sites. Some publishers are dishonest, and use automation to generate traffic to defraud the advertisers. Similarly, some advertisers automate clicks on the advertisements of their competitors to deplete their competitors ' advertising budgets. In this paper, we describe the advertising network model, and discuss the issue of fraud that is an integral problem in such setting. We propose using online algorithms on aggregate data to accurately and proactively detect automated traffic, preserve surfers' privacy, while not altering the industry model. We provide a complete classification of the hit inflation techniques; and devise stream analysis techniques that detect a variety of fraud attacks. We abstract detecting the fraud attacks of some classes as theoretical stream analysis problems that we bring to the data management research community as open problems. A framework is outlined for deploying the proposed detection algorithms on a generic architecture. We conclude by some successful preliminary findings of our attempt to detect fraud on a real network.
[telecommunication security, Data privacy, data management, Telecommunication traffic, privacy, Electronic commerce, data aggregation, hit inflation, fraud attack, Traffic control, Web streams, advertising network model, e-commerce, IP networks, Advertising, electronic commerce, Automation, advertising data processing, network traffic, Web advertising network, Aggregates, fraud, click fraud, fraud detection, data privacy, Internet, Detection algorithms]
Overlay Node Placement: Analysis, Algorithms and Impact on Applications
27th International Conference on Distributed Computing Systems
None
2007
Overlay routing has emerged as a promising approach to improving performance and reliability of Internet paths. To fully realize the potential of overlay routing under the constraints of deployment costs in terms of hardware, network connectivity and human effort, it is critical to carefully place infrastructure overlay nodes to balance the trade-off between performance and resource constraints. In this paper, we investigate approaches to perform intelligent placement of overlay nodes to facilitate (i) resilient routing and (ii) TCP performance improvement. We formulate objective functions to accurately capture application behavior: reliability and TCP performance, and develop several placement algorithms, which offer a wide range of trade-offs in complexity and required knowledge of the client- server location and traffic load. Using simulations on synthetic and real Internet topologies, and PlanetLab experiments, we demonstrate the effectiveness of the placement algorithms and objective functions developed, respectively. We conclude that an approach, hybrid of random and greedy approaches, provides the best tradeoff between computational efficiency and accuracy. We also uncover the fundamental challenge in simultaneously optimizing for reliability and TCP performance, and propose a simple unified algorithm to achieve the same.
[Algorithm design and analysis, TCP, client-server systems, Costs, Computational modeling, Humans, Telecommunication traffic, Routing, Topology, client-server location, overlay node placement, traffic load, transport protocols, telecommunication network routing, overlay routing, Hardware, Internet, Web server, telecommunication traffic]
Communication-Efficient Tracking of Distributed Cumulative Triggers
27th International Conference on Distributed Computing Systems
None
2007
In recent work, we proposed D-Trigger, a framework for tracking a global condition over a large network that allows us to detect anomalies while only collecting a very limited amount of data from distributed monitors. In this paper, we expand our previous work by designing a new class of queries (conditions) that can be tracked for anomaly violations. We show how security violations can be detected over a time window of any size. This is important because security operators do not know in advance the window of time in which measurements should be made to detect anomalies. We also present an algorithm that determines how each machine should filter its time series measurements before back-hauling them to a central operations center. Our filters are computed analytically such that upper bounds on false positive and missed detection rates are guaranteed. In our evaluation, we show that botnet detection can be carried out successfully over a distributed set of machines, while simultaneously filtering out 80 to 90% of the measurement data.
[D-Trigger, Data Aggregation, communication-efficient tracking, Queueing Theory., Filtering, distributed monitors, Scalability, Data security, Network Monitoring, botnet detection, Distributed Triggering, Time measurement, Condition monitoring, Anomaly Detection, Filters, Upper bound, security of data, Aggregates, security violations, Large-scale systems, distributed cumulative triggers, anomaly violations, Remote monitoring]
AVMON: Optimal and Scalable Discovery of Consistent Availability Monitoring Overlays for Distributed Systems
27th International Conference on Distributed Computing Systems
None
2007
This paper addresses the problem of selection and discovery of a consistent availability monitoring overlay for computer hosts in a large-scale distributed application, where hosts may be selfish or colluding. We motivate six significant goals for the problem - consistency, verifiability, and randomness, in selecting the availability monitors of nodes, as well as discoverability, load-balancing, and scalability in finding these monitors. We then present a new system, called AVMON, that is the first to satisfy these six requirements. The core algorithmic contribution of this paper is a protocol for discovering the availability monitoring overlay in a scalable and efficient manner, given any arbitrary monitor selection scheme that is consistent and verifiable. We mathematically analyze the performance of AVMON's discovery protocols, and derive an optimal variant that minimizes memory, bandwidth, computation, and discovery time of monitors. Our experimental evaluations of AVMON use three types of availability traces - synthetic, from PlanetLab, and from a peer-to-peer system (Overnet) - and demonstrate that AVMON works well in a variety of distributed systems.
[Protocols, load balancing, Scalability, reliability, scalable discovery, Consistency, Distributed computing, scalability, resource allocation, discoverability, distributed systems, consistent availability monitoring overlays, Large-scale systems, Performance analysis, Overlay, Monitoring, Availability, Optimality., peer-to-peer computing, Computerized monitoring, Peer to peer computing, AVMON, Application software, Churn, Computer displays, optimal discovery, peer-to-peer system, computer hosts]
An Empirical Study of Collusion Behavior in the Maze P2P File-Sharing System
27th International Conference on Distributed Computing Systems
None
2007
Peer-to-peer networks often use incentive policies to encourage cooperation between nodes. Such systems are generally susceptible to collusion by groups of users in order to gain unfair advantages over others. While techniques have been proposed to combat Web spam collusion, there are few measurements of real collusion in deployed systems. In this paper, we report analysis and measurement results of user collusion in Maze, a large-scale peer-to-peer file sharing system with a non-net-zero point-based incentive policy. We search for colluding behavior by examining complete user logs, and incrementally refine a set of collusion detectors to identify common collusion patterns. We find collusion patterns similar to those found in Web spamming. We evaluate how proposed reputation systems would perform on the Maze system. Our results can help guide the design of more robust incentive schemes.
[Performance evaluation, collusion behavior, peer-to-peer computing, Peer to peer computing, peer-to-peer networks, maze P2P file-sharing system, Web spam collusion, Incentive schemes, Embedded software, Asia, Detectors, Robustness, Communication system traffic control, Large-scale systems, Resource management, nonnet-zero point-based incentive policy]
Supporting Robust and Secure Interactions in Open Domains through Recovery of Trust Negotiations
27th International Conference on Distributed Computing Systems
None
2007
Trust negotiation supports authentication and access control across multiple security domains by allowing parties to use non-forgeable digital credentials to establish trust. By their nature trust negotiation systems are used in environments that are not always reliable. In particular, it is important not only to protect negotiations against malicious attacks, but also against failures and crashes of the parties or of the communication means. To address the problem of failures and crashes, we propose an efficient and secure recovery mechanism. The mechanism includes two recovery protocols, one for each of the two main negotiation phases. In fact, because of the requirements that both services and credentials have to be protected on the basis of the associated disclosure policies, most approaches distinguish between a phase of disclosure policy evaluation from a phase devoted to actual credentials exchange. We prove that the protocols, besides being efficient, are secure with respect to integrity, and confidentiality and are idempotent. To the best of our knowledge, this is the first effort for achieving robustness and fault tolerance of trust negotiation systems.
[Access control, Protocols, Peer to peer computing, malicious attacks, trust negotiations recovery, recovery mechanism security, Computer crashes, Distributed computing, negotiation support systems, recovery protocols, Robust control, Computer science, security of data, Authentication, trust negotiation systems, Robustness, access control, Protection]
Embedded Gossip: Lightweight Online Measurement for Large-Scale Applications
27th International Conference on Distributed Computing Systems
None
2007
For large-scale parallel applications, lightweight online monitoring can enable a wide range of online adaptations, including load balancing, power management, and progress monitoring. The processing and monitoring overhead of centralized global tracing techniques make them unsuitable for such tasks. Purely local tools, on the other hand, fail to provide the global information necessary for many desirable online adaptations of large-scale applications. In this paper, we describe a novel distributed online measurement method for large-scale applications called Embedded Gossip (EG). EG works by piggybacking performance information about application behavior on existing application messages and merging received information with previously known data in a fashion customized to the needs of a particular monitoring task. EG thus provides each process with both local and global views of application behavior with low overhead. To illustrate the capabilities of Embedded Gossip, we also show that it disseminates global information in a timely fashion for a wide range of monitoring tasks, including critical path profiling, workload imbalance monitoring, and progress monitoring. This global information has a wide range of potential uses, including imbalance detection for load balancing and energy management tools, progress monitoring for batch schedulers, and a wide range of other performance debugging and optimization techniques.
[Condition monitoring, Bridges, Computer science, Computerized monitoring, Laboratories, Computer architecture, Load management, Large-scale systems, Application software, Energy management]
Characterizing Network Traffic in a Cluster-based, Multi-tier Data Center
27th International Conference on Distributed Computing Systems
None
2007
With the increasing use of various Web-based services, design of high performance, scalable and dependable data centers has become a critical issue. Recent studies show that a clustered, multi-tier architecture is a cost-effective approach to design such servers. Since these servers are highly distributed and complex, understanding the workloads driving them is crucial for the success of the ongoing research to improve them. In view of this, there has been a significant amount of work to characterize the workloads of Web-based services. However, all of the previous studies focus on a high level view of these servers, and analyze request-based or session-based characteristics of the workloads. In this paper, we focus on the characteristics of the network behavior within a clustered, multi-tiered data center. Using a real implementation of a clustered three-tier data center, we analyze the arrival rate and inter-arrival time distribution of the requests to individual server nodes, the network traffic between tiers, and the average size of messages exchanged between tiers. The main results of this study are; (1) in most cases, the request inter-arrival rates follow log-normal distribution, and self-similarity exists when the data center is heavily loaded, (2) message sizes can be modeled by the log-normal distribution, and (3) service times fit reasonably well with the Pareto distribution and show heavy tailed behavior at heavy loads.
[log normal distribution, Telecommunication traffic, Data engineering, Web-based services, Delay, log-normal distribution, multitier architecture, Computer science, network traffic, Network servers, Design engineering, software architecture, cluster-based data center, Log-normal distribution, Pareto distribution, Web services, Hardware, multitier data center, Large-scale systems, Web server, telecommunication traffic]
GeoGrid: A Scalable Location Service Network
27th International Conference on Distributed Computing Systems
None
2007
This paper presents GeoGrid, a geographical location service overlay network system, for providing scalable location-based services to a large and growing number of mobile users. GeoGrid is designed as a decentralized and geographical location aware overlay network and provides system-level facilities and optimizations for balancing load in the presence of node heterogeneity, dynamically moving hot-spots (location queries), and unpredictable rate of node join, departure and failure. GeoGrid uses geographical mapping of nodes to regions and geographical proximity based routing to take advantage of the similarity between physical and network proximity. Furthermore, GeoGrid exploits multiple opportunities for dynamic workload adaptation in the presence of static hotspot queries and moving hotspot queries. Its dynamic load balancing algorithms can efficiently utilize the heterogeneous capacities of end systems and balance both the location query workload and the routing workload. Our initial prototype development and experimental study demonstrate that GeoGrid can effectively reduce the workload imbalance by an order of magnitude.
[load balancing, Heuristic algorithms, geographical proximity based routing, network proximity, grid computing, dynamic load balancing algorithms, Mobile communication, geographic information systems, geographical mapping, Delay, mobile computing, Network topology, moving hotspot queries, Computer networks, mobile users, static hotspot queries, scalable location-based services, geographical location aware overlay network, Peer to peer computing, GeoGrid, Routing, Application software, scalable location service network, geographical location service overlay network system, Load management, Mobile computing, routing workload]
SCAP: Smart Caching inWireless Access Points to Improve P2P Streaming
27th International Conference on Distributed Computing Systems
None
2007
The increasing number of wireless users in Internet P2P applications causes two new performance problems due to the requirement of uploading the downloaded traffic for other peers, limited bandwidth of wireless communications, and resource competition between the access point and wireless stations. First, an active P2P wireless user can significantly reduce the downloading throughput of other wireless users in the WLAN. Second, the slowdown of a P2P wireless user communication can also delay its relay and data sharing service for other dependent wired/wireless peers. In order to address these problems, in this paper, we propose an efficient caching mechanism called SCAP (Smart Caching in Access Points). Conducting intensive Internet measurements on representative P2P streaming applications, we observe a high percentage of duplicated data packets in successive downloading and uploading data streams. Through duplication detection and caching at the access point, these duplicated packets can be compressed so that the uploading traffic in the WLAN is significantly reduced. Our prototype-based experimental evaluation demonstrates that by effectively reducing the redundant P2P traffic in the WLAN, SCAP improves the throughput of the WLAN by up to 88% and reduces the response delay to other Internet users meanwhile.
[Wireless LAN, data streams, Telecommunication traffic, data sharing service, Throughput, P2P streaming, Relays, Delay, Wireless communication, Internet P2P applications, wireless user communication, Prototypes, Bandwidth, P2P wireless user, SCAP, smart caching, peer-to-peer computing, WLAN, duplicated data packets, wireless access points, Streaming media, Internet, wireless LAN, telecommunication traffic, peer-to-peer network, redundant P2P traffic]
Magellan: Charting Large-Scale Peer-to-Peer Live Streaming Topologies
27th International Conference on Distributed Computing Systems
None
2007
Live peer-to-peer (P2P) streaming applications have been successfully deployed in the Internet. With relatively simple peer selection protocol design, modern live P2P streaming applications are able to provide millions of concurrent users adequately satisfying viewing experiences. That said, few existing research has provided sufficient insights on the time-varying internal characteristics of P2P topologies in live streaming. With 120 GB worth of traces in late 2006 from a commercial P2P live streaming system of UUSee Inc. in Beijing, this paper represents the first attempt in the research community to explore topological properties in practical P2P streaming, and how they behave over time. Starting from classical graph metrics, such as degree, clustering coefficient, and reciprocity, we explore and extend them in specific perspectives of streaming applications. We also compare our findings with existing insights from topological studies of P2P file sharing applications, which shed new and unique insights specific to streaming. Our characterization reveals the scalability of the commercial P2P streaming application even in case of large flash crowds, the clustering phenomenon of peers in each ISP, as well as the reciprocal behavior among peers, all of which play important roles in achieving its current success.
[Protocols, Law, peer-to-peer computing, Magellan, Peer to peer computing, Scalability, telecommunication network topology, Topology, Application software, multimedia computing, peer selection protocol, peer-to-peer live streaming topologies, multimedia streaming, Collaboration, Streaming media, Large-scale systems, Internet, protocols, file sharing]
Deploying Video-on-Demand Services on Cable Networks
27th International Conference on Distributed Computing Systems
None
2007
Efficient video-on-demand (VoD) is a highly desired service for media and telecom providers. VoD allows subscribers to view any item in a large media catalog nearly-instantaneously. However, systems that provide this services currently require large amounts of centralized resources and significant bandwidth to accommodate their subscribers. Hardware requirements become more substantial as the service providers increase the catalog size or number of subscribers. In this paper, we describe how cable companies can leverage deployed hardware in a peer- to-peer architecture to provide an efficient alternative We propose a distributed VoD system, and use real measurements from a deployed VoD system to evaluate different design decisions. Our results show that with minor changes, currently deployed cable infrastructures can support a video-on-demand system that scales to a large number of users and catalog size with low centralized resources.
[cable network, Coaxial cables, Cable TV, Power cables, peer-to-peer computing, Peer to peer computing, Multimedia systems, Coaxial components, TV broadcasting, peer-to-peer architecture, video-on-demand service, video on demand, Bandwidth, Hardware, Communication cables, distributed VoD system]
STEP: Sequentiality and Thrashing Detection Based Prefetching to Improve Performance of Networked Storage Servers
27th International Conference on Distributed Computing Systems
None
2007
State-of-the-art networked storage servers are equipped with increasingly powerful computing capability and large DRAM memory as storage caches. However, their contribution to the performance improvement of networked storage system has become increasingly limited. This is because the client-side memory sizes are also increasing, which reduces capacity misses in the client buffer caches as well as access locality in the storage servers, thus weakening the caching effectiveness of server storage caches. Proactive caching in storage servers is highly desirable to reduce cold misses in clients. We propose an effective way to improve the utilization of storage server resources through prefetching in storage servers for clients. In particular, our design well utilizes two unique strengths of networked storage servers which are not leveraged in existing storage server prefetching schemes. First, powerful storage servers have idle CPU cycles, under-utilized disk bandwidth, and abundant memory space, providing many opportunities for aggressive disk data prefetching. Second, the servers have the knowledge about high-latency operations in storage devices, such as disk head positioning, which enables efficient disk data prefetching based on an accurate cost-benefit analysis of prefetch operations. We present STEP - a Sequentiality and Thrashing dEtec- tion based Prefetching scheme, and its implementation with Linux Kernel 2.6.16. Our performance evaluation by replaying storage performance council (SPC) 's OLTP traces shows that server performance improvements are up to 94% with an average of 25%. Improvements with frequently used Unix applications are up to 53% with an average of 12%. Our experiments also show that STEP has little effect on workloads with random access patterns, such as SPC Web-search traces.
[network servers, networked storage servers, Prefetching, Buffer storage, Random access memory, Cache storage, STEP, Cost benefit analysis, DRAM memory, proactive caching, Network servers, storage management, Linux, Bandwidth, thrashing detection-based prefetching, storage caches, DRAM chips, Computer networks, Kernel, client buffer caches, client-side memory, SPC Web-search traces]
Stampede RT: Programming Abstractions for Live Streaming Applications
27th International Conference on Distributed Computing Systems
None
2007
We present StampedeRT, middleware designed to provide a natural programming model appropriate for live streaming applications. Such applications require pervasive access to multiple streaming data sources for distributed online analysis. One motivating example is a distributed robotics application which analyzes live camera feeds for control and planning. Most existing middlewares for streaming data focus on media streams and low-level transport characteristics such as delivery latency and efficient transfer, but do not define a programming model to succinctly express applications that manipulate and analyze the streaming content. StampedeRT provides for straightforward transport and manipulation of temporally-ordered data streams, enabling simple synchronization and correlation of data sources. We present an abstract programming model to support the aforementioned class of applications and then describe a concrete realization of the model as a distributed middleware architecture. We also evaluate our implementation of the architecture and present several motivating applications StampedeRT is designed to support.
[programming abstractions, Robot vision systems, temporally-ordered data streams, Data structures, distributed robotics application, Application software, ubiquitous computing, Middleware, Stampede, Robot programming, live streaming applications, pervasive access, Computer architecture, Streaming media, Cameras, Concrete, data handling, robots, straight-forward transport, Feeds, control engineering computing, distributed programming, distributed online analysis, middleware]
Distributed Slicing in Dynamic Systems
27th International Conference on Distributed Computing Systems
None
2007
Peer to peer (P2P) systems are moving from application specific architectures to a generic service oriented design philosophy. This raises interesting problems in connection with providing useful P2P middleware services capable of dealing with resource assignment and management in a large-scale, heterogeneous and unreliable environment. The slicing service, has been proposed to allow for an automatic partitioning of P2P networks into groups (slices) that represent a controllable amount of some resource and that are also relatively homogeneous with respect to that resource. In this paper we propose two gossip-based algorithms to solve the distributed slicing problem. The first algorithm speeds up an existing algorithm sorting a set of uniform random numbers. The second algorithm statistically approximates the rank of nodes in the ordering. The scalability, efficiency and resilience to dynamics of both algorithms rely on their gossip-based models. These algorithms are proved viable theoretically and experimentally.
[resource management, P2P middleware services, Scalability, peer to peer systems, slicing service, Environmental management, P2P networks, automatic partitioning, resource allocation, resource assignment, gossip-based algorithms, Automatic control, Large-scale systems, application specific architectures, program slicing, middleware, peer-to-peer computing, Peer to peer computing, distributed slicing, service oriented design philosophy, Partitioning algorithms, Middleware, Sorting, Resilience, dynamic systems, Resource management]
Self-Similar Algorithms for Dynamic Distributed Systems
27th International Conference on Distributed Computing Systems
None
2007
This paper proposes a methodology for designing a class of algorithms for computing functions in dynamic distributed systems in which communication channels and processes may cease functioning temporarily or permanently. Communication and computing may be interrupted by an adversary or by environmental factors such as noise and power loss. The set of processes may be partitioned into subsets that cannot communicate with each other; algorithms in which all such subsets behave in a similar fashion, regardless of size and identities of processes, are called self-similar algorithms. Algorithms adapt to changing conditions, speeding up or slowing down depending on the resources available. The paper presents necessary and sufficient conditions for the application of a self-similar strategy. Self-similar algorithms are developed for several problems by applying the methodology.
[Heuristic algorithms, environmental factors, self-similar algorithms, distributed processing, Mobile communication, International collaboration, Partitioning algorithms, State-space methods, Distributed computing, Postal services, Computer science, Sufficient conditions, dynamic distributed systems, Communication channels, communication channels]
Distributed Resource Management and Admission Control of Stream Processing Systems with Max Utility
27th International Conference on Distributed Computing Systems
None
2007
A fundamental problem in a large scale decentralized stream processing system is how to best utilize the available resources and admission control the bursty and high volume input streams so as to optimize overall system performance. We consider a distributed stream processing system consisting of a network of servers with heterogeneous capabilities that collectively provide processing services to multiple data streams. Our goal is to design a joint source admission control, data routing, and resource allocation mechanism that maximizes the overall system utility. Here resources include both link bandwidths and processor resources. The problem is formulated as a utility optimization problem. We describe an extended graph representation that unifies both types of resources seamlessly and present a novel scheme that transforms the admission control problem to a routing problem by introducing dummy nodes at sources. We then present a distributed gradient-based algorithm that iteratively updates the local resource allocation based on link data rates. We show that our algorithm guarantees optimality and demonstrate its performance through simulation.
[distributed resource management, Multicommodity Flow Model, graph theory, distributed processing, distributed gradient-based algorithm, admission control, Distributed computing, Stream Processing, Distributed Algorithms, Network servers, optimisation, resource allocation, System performance, Bandwidth, graph representation, Distributed algorithms, gradient methods, Gradient Methods, system utility maximisation, large scale decentralized stream processing, Routing, data routing, distributed stream processing system, Admission control, Time sharing computer systems, Iterative algorithms, Resource management]
Catching "Moles" in Sensor Networks
27th International Conference on Distributed Computing Systems
None
2007
False data injection is a severe attack that compromised sensor nodes ("moles"1) can launch. These moles inject large amount of bogus traffic that can lead to application failures and exhausted network resources. Existing sensor network security proposals only passively mitigate the damage by filtering injected packets; they do not provide active means for fight back. This paper studies how to locate such moles within the framework of packet marking, when forwarding moles collude with source moles to manipulate the marks. Existing Internet traceback mechanisms do not assume compromised forwarding nodes and are easily defeated by manipulated marks. We propose a probabilistic nested marking (PNM) scheme that is secure against such colluding attacks. No matter how colluding moles manipulate the marks, PNM can always locate them one by one. We prove that nested marking is both sufficient and necessary to resist colluding attacks. PNM also has fast-traceback: within about 50 packets, it can track down a mole up to 20 hops away from the sink. This virtually prevents any effective data injection attack: moles will be caught before they have injected any meaningful amount of bogus traffic.
[telecommunication security, wireless sensor networks, packet marking, Data security, network security, sensor networks, Information filtering, Proposals, Floods, probabilistic nested marking, Wireless sensor networks, Resists, Bandwidth, Information filters, Internet, false data injection, Protection, Internet traceback mechanisms]
Location Verification Algorithms forWireless Sensor Networks
27th International Conference on Distributed Computing Systems
None
2007
Because the knowledge of sensors' locations is very important to many location-based applications proposed for wireless sensor networks, many secure localization and location verification schemes have been proposed to provide robust location estimations for sensors. We propose two lightweight location verification algorithms, namely, greedy filtering by matrix (GFM) and trustability indicator (TI). Unlike other schemes, our algorithms do not require any specialized hardware or deployment knowledge. In GFM algorithm, the verification center(VC) calculates several matrices based on sensors' estimated locations and their neighborhood observations, and uses these matrixes to identify and revoke inconsistent locations. In TI algorithm, VC calculates trustability indicators for the sensors and reject those whose indicators are below a threshold. Simulation results demonstrated the effectiveness of both algorithms in detecting abnormal location and their robustness under attacks.
[Availability, Base stations, Virtual colonoscopy, wireless sensor networks, greedy algorithms, mobility management (mobile radio), location-based applications, verification center, Wireless sensor networks, mobile computing, trustability indicator algorithm, location verification algorithms, Filtering algorithms, Robustness, Hardware, Iterative algorithms, greedy filtering by matrix, Virtual manufacturing, State estimation]
An Asymmetric Quorum-based Power Saving Protocol for Clustered Ad Hoc Networks
27th International Conference on Distributed Computing Systems
None
2007
Clustering in Mobile Ad Hoc Networks (MANETs) has shown to be a promising technique to ensure the scalability and efficiency of various communication protocols. Since stations in MANETs are usually equipped with batteries as the power source, it is critical to ensure the energy efficiency of clustering schemes. The Quorum-based Power Saving (QPS) protocols render extensive energy conservation as compared with IEEE 802.11 Power Saving (PS) mode and are widely studied over the past years. However, most existing QPS protocols adopt a symmetric design where stations in the network are guaranteed to discover each other. Observing that in clustered environments there is no need to insist in all-pair neighbor discovery, we propose an Asymmetric Cyclic Quorum (ACQ) system. The ACQ system guarantees the neighbor discovery between each member node and the clusterhead in a cluster, and between clusterheads in the network. A construction scheme is presented in this work, which assembles the ACQ system in O(1) time. We show that by taxing slightly more energy consumption on the clusterhead, the average energy consumption of stations in a cluster can reduce substantially than can be achieved by traditional QPS protocols. Simulation results show that the ACQ system outperforms the previous studies up to 52% in energy efficiency, while introducing no extra worst-case latency.
[Energy consumption, Protocols, Assembly systems, Scalability, Energy conservation, Mobile communication, Ad hoc networks, Energy efficiency, Batteries, Mobile ad hoc networks]
Uniform Data Sampling from a Peer-to-Peer Network
27th International Conference on Distributed Computing Systems
None
2007
Uniform random sample is often useful in analyzing data. Usually taking a uniform sample is not a problem if the entire data resides in one location. However, if the data is distributed in a peer-to-peer (P2P) network with different amount of data in different peers, collecting a uniform sample of data becomes a challenging task. A random sampling can be performed using random-walk, but due to varying degrees of connectivity and different sizes of data owned by each peer, this random walk gives a biased sample. In this paper, we propose a random walk-based sampling algorithm that can be used to sample data tuples uniformly from a large, unstructured P2P network. We model the random walk as a Markov chain and derive conditions to bound the length of the random walk necessary to achieve uniformity. A formal communication analysis shows logarithmic communication cost to discover a uniform data sample.
[sampling methods, random walk-based sampling algorithm, data analysis, peer-to-peer computing, Peer to peer computing, formal communication analysis, uniform data sampling, Equations, Markov chain, Distributed databases, Markov processes, Nickel, Eigenvalues and eigenfunctions, data tuples, peer-to-peer network, unstructured P2P network]
Welcome Message from the Conference Chairs
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Presents the introductory welcome message from the conference proceedings.
[]
Welcome Message from the Program Chairs
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Presents the introductory welcome message from the conference proceedings.
[]
Organizing Committee
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Provides a listing of current committee members.
[]
Program Committee
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Provides a listing of current committee members.
[]
Computer Science as a Lens on the Sciences
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Summary form only given. This article trace the growing influence of fundamental ideas from computer science on the nature of research in a number of scientific fields. There is a growing awareness that information processing lies at the heart of the processes studied in fields as diverse as quantum mechanics, statistical physics, nanotechnology, neuroscience, linguistics, economics and sociology. Increasingly, mathematical models in these fields are expressed in algorithmic languages and describe algorithmic processes. The author describe connections between quantum computing and the foundations of quantum mechanics, and between statistical mechanics and phase transitions in computation. The author indicates how the growth of the Web has created new phenomena to be investigated by sociologists and economists, spurring new developments in computational game theory and the study of social networks. The author then focus on computational molecular biology, where the view of living cells as complex information processing systems has become the dominant paradigm.
[Algorithm design and analysis, information processing, game theory, computational game theory, World Wide Web, Computer science, social network, Quantum computing, scientific field, quantum mechanics, computational molecular biology, Awards activities, computer science, Quantum mechanics, quantum computing, Information processing, mathematical model, algorithmic language process, living cell, Internet, natural sciences computing, phase transition, Medals, statistical mechanics]
Real-Time Detection of Clone Attacks in Wireless Sensor Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
A central problem in sensor network security is that sensors are susceptible to physical capture attacks. Once a sensor is compromised, the adversary can easily launch clone attacks by replicating the compromised node, distributing the clones throughout the network, and starting a variety of insider attacks. Previous works against clone attacks suffer from either a high communication/storage overhead or a poor detection accuracy. In this paper, we propose a novel scheme for detecting clone attacks in sensor networks, which computes for each sensor a social fingerprint by extracting the neighborhood characteristics, and verifies the legitimacy of the originator for each message by checking the enclosed fingerprint. The fingerprint generation is based on the superimposed s-disjunct code, which incurs a very light communication and computation overhead. The fingerprint verification is conducted at both the base station and the neighboring sensors, which ensures a high detection probability. The security and performance analysis indicate that our algorithm can identify clone attacks with a high detection probability at the cost of a low computation/communication/storage overhead. To our best knowledge, our scheme is the first to provide realtime detection of clone attacks in an effective and efficient way.
[fingerprint identification, telecommunication security, Base stations, wireless sensor networks, network security, Communities, Cloning, probability, high detection probability, real-time clone attack detection, wireless sensor network, Fingerprint recognition, social fingerprint, Security, Computer science, clone attacks, USA Councils, physical capture attack, fingerprint verification, s-disjunct code]
Comparing Symmetric-key and Public-key Based Security Schemes in Sensor Networks: A Case Study of User Access Control
2008 The 28th International Conference on Distributed Computing Systems
None
2008
While symmetric-key schemes are efficient in processing time for sensor networks, they generally require complicated key management, which may introduce large memory and communication overhead. On the contrary, public-key based schemes have simple and clean key management, but cost more computational time. The recent progress of elliptic curve cryptography (ECC) implementation on sensors motivates us to design a public-key scheme and compare its performance with the symmetric-key counterparts. This paper builds the user access control on commercial off-the-shelf sensor devices as a case study to show that the public-key scheme can be more advantageous in terms of the memory usage, message complexity, and security resilience. Meanwhile, our work also provides insights in integrating and designing public-key based security protocols for sensor networks.
[Access control, wireless sensor networks, security resilience, sensor networks, memory usage, Security, Wireless Sensor Networks, Remote sensing, public-key based security schemes, message complexity, sensors, public key cryptography, Memory management, Public Key, Public key, Elliptic curve cryptography, access control, Access Control, Cryptography, user access control, clean key management, symmetric-key based security schemes, elliptic curve cryptography, commercial off-the-shelf sensor devices]
Securing Wireless Data Networks against Eavesdropping using Smart Antennas
2008 The 28th International Conference on Distributed Computing Systems
None
2008
In this paper, we focus on securing communication over wireless data networks from malicious eavesdroppers, using smart antennas. While conventional cryptography based approaches focus on hiding the meaning of the information being communicated from the eavesdropper, we consider a complimentary class of strategies that limit knowledge of the existence of the information from the eavesdropper. We profile the performance achievable using simple beamforming strategies using a newly defined metric called exposure region. We then present three strategies within the context of an approach called virtual arrays of physical arrays to significantly improve the exposure region performance of a wireless LAN environment. Using simulations and analysis, we validate and evaluate the proposed strategies.
[telecommunication security, exposure region, wireless data networks, Array signal processing, network security, Interference, cryptography, Communication system security, malicious eavesdroppers, Security, Eavesdropping, Wireless communication, computer network management, antenna arrays, Smart Antennas, smart antennas, beamforming strategies, Cryptography, wireless LAN, Antennas, Antenna arrays]
On Security Vulnerabilities of Null Data Frames in IEEE 802.11 Based WLANs
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Null data frames are a special but important type of frames in IEEE 802.11 based wireless local area networks (e.g., 802.11 WLANs). They are widely used for power management, channel scanning and association keeping alive. The wide applications of null data frames come from their salient features such as lightweight frame format and implementation flexibility. However, such features can be taken advantage of by malicious attackers to launch a variety of attacks. In this paper, we identify the potential security vulnerabilities in the current applications of null data frames. We then study two types of attacks taking advantage of these vulnerabilities in detail, and evaluate their effectiveness based on extensive experiments. Finally, we point out that our work has broader impact in that similar vulnerabilities exist in many other networks.
[telecommunication security, security vulnerabilities, lightweight frame format, IEEE 802.11 Standards, telecommunication network management, Switches, Receivers, wireless local area networks, Fingerprint recognition, Throughput, IEEE 802.11 based WLAN, null data frames, Security, implementation flexibility, Wireless communication, power management, wireless LAN, channel scanning]
On the Impact of Mobile Hosts in Peer-to-Peer Data Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Peer-to-peer (P2P) data networks dominate Internet traffic. In this work, we study the problems that arise when mobile hosts participate in P2P networks. We primarily focus on the performance issues as experienced by the mobile host, but also study the impact on other fixed peers. Using Bit Torrent as a key example, we identify several unique problems that arise due to the design aspects of P2P networks being incompatible with typical characteristics of wireless and mobile environments. We then present a wireless P2P (wP2P) client application that is backward compatible with existing fixed peer client applications, but when used on mobile hosts can significant improve performance.
[mobile environments, peer-to-peer computing, Peer to peer computing, peer-to-peer data networks, Mobile communication, Throughput, Mobile hosts, Internet traffic, wireless P2P client application, Wireless communication, mobile computing, Bit Torrent, mobile hosts, wireless environments, peer to peer, Bidirectional control, Bandwidth, Internet, telecommunication traffic]
Is Random Scheduling Sufficient in P2P Video Streaming?
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Peer-to-peer (P2P) has recently been employed to deliver large scale video multicast services on the Internet. Considerable efforts have been made by both academia and industry on P2P streaming design. While academia mostly focus on exploring design space to approach the theoretical performance bounds, our recent measurement study on several commercial P2P streaming systems indicates that they are able to deliver good user quality of experience with seemingly simple designs. One intriguing question remains: how elaborate should a good P2P video streaming design be? Towards answering this question, we developed and implemented several representative P2P streaming designs, ranging from theoretically proved optimal designs to straight forward "naive" designs. Through an extensive comparison study on PlanetLab, we unveil several key factors contributing to the successes of simple P2P streaming designs, including system resource index, sever capacity and chunk scheduling rule, peer download buffering and peering degree. We also identify regions where naive designs are inadequate and more elaborate designs can improve things considerably. Our study not only brings us better understandings and more insights into the operation of existing systems, it also sheds lights on the design of future systems that can achieve a good balance between the performance and the complexity.
[peer-to-peer technology, peer-to-peer computing, Peer to peer computing, Optimal scheduling, Servers, Indexes, PlanetLab, Delay, peer-to-peer, streaming, Bandwidth, large scale video multicast services, Streaming media, scheduling, video streaming, random scheduling, Internet, Internet telephony, P2P video streaming]
Circumventing Server Bottlenecks: Indirect Large-Scale P2P Data Collection
2008 The 28th International Conference on Distributed Computing Systems
None
2008
In most large-scale peer-to-peer (P2P) applications, it is necessary to collect vital statistics data - sometimes referred to as logs - from up to millions of peers. Traditional solutions involve sending large volumes of such data to centralized logging servers, which are not scalable. In addition, they may not be able to retrieve statistics data from departed peers in dynamic peer-to-peer systems. In this paper, we solve this dilemma through an indirect collection mechanism that distributes data using random network coding across the network, from which servers proactively pull such statistics. By buffering data in a decentralized fashion with only a small portion of peer resources, we show that our new mechanism provides a "buffering" zone and a "smoothing" factor to collect large volumes of statistics, with appropriate resilience to peer dynamics and scalability to a large peer population.
[Protocols, peer-to-peer computing, Peer to peer computing, Encoding, server bottlenecks, Servers, indirect large-scale P2P data collection, random network coding, Bandwidth, large-scale peer-to-peer applications, Particle measurements, Bipartite graph, statistical analysis, centralized logging servers]
Strong WORM
2008 The 28th International Conference on Distributed Computing Systems
None
2008
We introduce a Write-Once Read-Many (WORM) storage system providing strong assurances of data retention and compliant migration, by leveraging trusted secure hardware in close data proximity. This is important because existing compliance storage products and research prototypes are fundamentally vulnerable to faulty or malicious behavior, as they rely on simple enforcement primitives ill-suited for their threat model. This is hard because tamper-proof processing elements are significantly constrained in both computation ability and memory capacity - as heat dissipation concerns under tamper-resistant requirements limit their maximum allowable spatial gate-density. We achieve efficiency by (i) ensuring the secure hardware is accessed sparsely, minimizing the associated overhead for expected transaction loads, and (ii) using adaptive overhead-amortized constructs to enforce WORM semantics at the throughput rate of the storage servers ordinary processors during burst periods. With a single secure co-processor, on single-CPU commodity x86 hardware, our architecture can support over 2500 transactions per second.
[regulatory compliance WORM write once read many storage, Security, Grippers, data proximity, spatial gate density, tamper-resistant requirements, storage management, File systems, compliance storage product, storage servers, Write-Once Read-Many storage system, data retention, file servers, adaptive overhead-amortized constructs, Tin, tamper-proof processing element, Hardware, Software, Cryptography]
Detecting Click Fraud in Pay-Per-Click Streams of Online Advertising Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
With the rapid growth of the Internet, online advertisement plays a more and more important role in the advertising market. One of the current and widely used revenue models for online advertising involves charging for each click based on the popularity of keywords and the number of competing advertisers. This pay-per-click model leaves room for individuals or rival companies to generate false clicks (i.e., click fraud), which pose serious problems to the development of healthy online advertising market. To detect click fraud, an important issue is to detect duplicate clicks over decaying window models, such as jumping windows and sliding windows. Decaying window models can be very helpful in defining and determining click fraud. However, although there are available algorithms to detect duplicates, there is still a lack of practical and effective solutions to detect click fraud in pay-per-click streams over decaying window models. In this paper, we address the problem of detecting duplicate clicks in pay-per-click streams over jumping windows and sliding windows, and are the first that propose two innovative algorithms that make only one pass over click streams and require significantly less memory space and operations. GBF algorithm is built on group Bloom filters which can process click streams over jumping windows with small number of sub-windows, while TBF algorithm is based on a new data structure called timing Bloom filter that detects click fraud over sliding windows and jumping windows with large number of sub-windows. Both GBF algorithm and TBF algorithm have zero false negative. Furthermore, both theoretical analysis and experimental results show that our algorithms can achieve low false positive rate when detecting duplicate clicks in pay-per-click streams over jumping windows and sliding windows.
[Algorithm design and analysis, advertising data processing, Radiation detectors, jumping windows, Data structures, sliding windows, pay-per-click model, online advertising networks, Memory management, fraud, click fraud detection, online advertisement, Filtering algorithms, pay-per-click streams, Timing, Internet, Advertising]
Exploring Anti-Spam Models in Large Scale VoIP Systems
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Although the problem of spam detection in email is well understood and has been extensively researched, a significant portion of emails today are spam. A most widely used method to detect spam involves content filtering, where the spam detector scans the received email for keywords. However, the same approach cannot be applied to detect Voice over IP (VoIP) spam, since a call has to be categorized as a legitimate or a spam (each to a degree with a certain reliability) before the connection is established. Also, spammers over IP can potentially generate orders of magnitude more spam volume, at far less cost, and with greater anonymity than telemarketers using the Public Switch Telephone Network (PSTN). The spam problem in VoIP is further compounded by the absence of a do-not-call-list, which has been the main reason for the reduction of spam calls in PSTN. Thus, the spam issue for VoIP is as important as those pertaining to quality-of-service (QoS) of the voice traffic itself. To this end, we propose two different anti-spam frameworks for large scale VoIP systems. The first one is a centralized SIP-based spam detection framework that relies on SIP messages during the call establishment phase to identify spam calls, and the second one is a distributed referral social network model, where a user is assigned a reputation score by its neighbors. Based on the reputation, a callee can decide either to accept or decline a call. Our simulation results indicate that the referral model can provide better anti-spam capabilities by isolating a spammer faster than the SIP based approach, and can also correctly identify spam calls over 98% of time.
[Protocols, Peer to peer computing, Social network services, Unsolicited electronic mail, content filtering, VoIP, unsolicited e-mail, Electronic mail, quality of service, Servers, social network, spam detection, antispam models, large scale VoIP systems, IP networks, spam, Internet telephony, email, quality-of-service, public switch telephone network]
Distributed Divide-and-Conquer Techniques for Effective DDoS Attack Defenses
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Distributed Denial-of-Service (DDoS) attacks have emerged as a popular means of causing mass targeted service disruptions, often for extended periods of time. The relative ease and low costs of launching such attacks, supplemented by the current woeful state of any viable defense mechanism, have made them one of the top threats to the Internet community today. While distributed packet logging and/or packet marking have been explored in the past for DDoS attack traceback/mitigation, we propose to advance the state of the art by using a novel distributed divide-and-conquer approach in designing a new data dissemination architecture that efficiently tracks attack sources. The main focus of our work is to tackle the three disjoint aspects of the problem, namely attack tree construction, attack path frequency detection, and packet to path association, independently and to use succinct recurrence relations to express their individual implementations. We also evaluate the network traffic and storage overhead induced by our proposed deployment on real-life Internet topologies, supporting hundreds of victims each subject to thousands of high-bandwidth flows simultaneously, and conclude that we can truly achieve single packet traceback guarantees with minimal overhead and high efficiency.
[telecommunication security, Pediatrics, divide and conquer methods, Frequency measurement, distributed denial-of-service attack, Internet threat, real-life Internet topology, Information filters, distributed packet marking, Filtering, trees (mathematics), distributed packet logging, attack path frequency detection, telecommunication network topology, Routing, packet-to-path association, DDoS attack mitigation, Construction industry, network traffic, attack tree construction, DDoS attack traceback, data dissemination architecture, Internet, distributed divide-and-conquer technique, telecommunication traffic]
Mobility-Assisted Spatiotemporal Detection in Wireless Sensor Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Wireless sensor networks (WSNs) deployed for mission-critical applications face the fundamental challenge of meeting stringent spatiotemporal performance requirements using nodes with limited sensing capacity. Although advance network planning and dense node deployment may initially achieve the required performance, they often fail to adapt to the unpredictability of physical reality. This paper explores efficient use of mobile sensors to address the limitations of static WSNs in target detection. We propose a data fusion model that enables static and mobile sensors to effectively collaborate in target detection. An optimal sensor movement scheduling algorithm is developed to minimize the total moving distance of sensors while achieving a set of spatiotemporal performance requirements including high detection probability, low system false alarm rate and bounded detection delay. The effectiveness of our approach is validated by extensive simulations based on real data traces collected by 23 sensor nodes.
[Schedules, mobile sensor, wireless sensor networks, system false alarm rate, wireless sensor network, Mobile communication, sensor fusion, optimal sensor movement scheduling algorithm, bounded detection delay, Delay, advance network planning, scheduling, Sensors, mission-critical application, mobile radio, signal detection, dense node deployment, telecommunication network planning, mobility-assisted spatiotemporal detection, Energy measurement, probability, spatiotemporal phenomena, data fusion model, target detection, Surveillance, total moving distance minimization, target tracking, detection probability, Distance measurement, minimisation]
Performance Analysis of Group Based Detection for Sparse Sensor Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
In this paper, we analyze the performance of group based detection in sparse sensor networks, when the system level detection decision is made based on the detection reports generated from multiple sensing periods. Sparse deployment is essential for reducing cost of large scale sensor networks, which cover thousands of square miles. In a sparse deployment, the sensor field is only partially covered by sensorspsila sensing ranges, resulting in void sensing areas in the region, but all nodes are connected through multi-hop networking. Further, due to the unavoidable false alarms generated by a single sensor in a network, many deployed systems use group based detection to reduce system level false alarms. Despite the popularity of group based detection, few analysis works in the literature deal with group based detection. In this paper, we propose a novel approach called Markov chain based Spatial approach (MS-approach) to model group based detection in sensor networks. The M-S-approach successfully overcomes the complicated conditional detection probability of a target in each sensing period, and reduces the execution time of the analysis from many days to 1 minute. The analytical model is validated through extensive simulations. This analytical work is important because it provides an easy way to understand the performance of a system that uses group based detection without running countless simulations or deploying real systems.
[telecommunication security, Target tracking, signal detection, wireless sensor networks, Computational modeling, spatial approach, probability, false alarm, conditional detection probability, sparse wireless sensor network, Group based detection, group based detection, Markov chain approach, Markov chain, Analytical models, Accuracy, sparse sensor networks, multihop networking, Object detection, system level false alarm, Markov processes, Cameras, Sensors, performance analysis]
Enabling Accurate Node Control in Randomized Duty Cycling Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
In this paper, we propose a novel duty cycling algorithm for a large-scale dense wireless sensor networks. The proposed algorithm is based on a social behavior of nodes in the sense that individual node's sleep/wakeup decision is influenced by the state of its neighbors. We analyze the behavior of the proposed duty cycling algorithm using a stochastic spatial process. In particular, we consider a geometric form of neighborhood dependence and a reversible Markov chain, and apply this model to analyze the behavior of the duty cycling network. We then identify a set of parameters for the reversible spatial process model, and study the steady state of the network with respect to these parameters. We report that our algorithm is scalable to a large network, and can effectively control the active node density while achieving a small variance. We also report that the social behavior of nodes has interesting and non-obvious impacts on the performance of duty cycling. Finally, we present how to set the parameters of the algorithm to obtain a desirable duty cycling behavior.
[Algorithm design and analysis, duty cycling, wireless sensor networks, telecommunication control, social behavior, Steady-state, Equations, sleep/wakeup decision, reversible spatial process, Analytical models, Wireless sensor networks, large-scale dense wireless sensor networks, accurate node control, markov random fields, reversible Markov chain, Probability density function, Markov processes, randomized duty cycling networks, Mathematical model, stochastic spatial process]
CME: A Contour Mapping Engine in Wireless Sensor Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Contour maps, showing topological distribution of extracted features, are crucial for many applications. Building a dynamic contour map in a wireless sensor network is a challenging task due to the constrained network resources. In this paper, we present the design of a contour mapping engine (CME) in wireless sensor networks. Our design incorporates in-network processing based on binary classification to reduce the total number of active nodes. The underlying network architecture is analyzed to derive an optimal configuration. We show, by extensive simulations, the superiority of CME over the state-of-the-art contour mapping techniques.
[topological distribution, Energy consumption, wireless sensor networks, topology, cartography, network architecture, contour mapping techniques, Engines, Contour maps, Temperature sensors, Temperature measurement, contour mapping engine, feature extraction, constrained network resources, optimal configuration, Feature extraction, contour maps, binary classification, Kernel, Monitoring]
LHT: A Low-Maintenance Indexing Scheme over DHTs
2008 The 28th International Conference on Distributed Computing Systems
None
2008
DHT is a widely-used building block in P2P systems, and complex queries are gaining popularity in P2P applications. To support efficient query processing over DHTs, effective indexing structures are essential. Recently, a number of indexing schemes have been proposed. However, these schemes have focused on improving query efficiency, and as a trade-off, sacrificed maintenance efficiency - an important performance measure in the P2P context, where frequent data updating and high peer dynamism are typically incurred. In this paper, we propose LHT, a Low maintenance Hash Tree, for efficient data indexing over DHTs. LHT employs a novel naming function and a tree summarization strategy to gracefully distribute its index structure. It is adaptable to any DHT substrates, and is easy to be implemented and deployed. Experiments show that in comparison with the state-of-the-art indexing technique, LHT saves up to 75% (at least 50%) maintenance cost, and achieves better performance for exact-match queries and range queries.
[Maintenance Efficiency, Range Queries, peer-to-peer computing, Peer to peer computing, indexing, low maintenance hash tree, Maintenance engineering, Data structures, Indexes, software maintenance, Substrates, low-maintenance indexing scheme, P2P, query processing, DHT, Indexing Over DHT, Query processing, complex queries, P2P systems, file organisation, query efficiency, Indexing, indexing structures]
Distributed Line Graphs: A Universal Framework for Building DHTs Based on Arbitrary Constant-Degree Graphs
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Most proposed DHTs have their unique maintenance mechanisms specific to the static graphs on which they are based. In this paper we propose distributed line graphs (DLG), a universal framework for building DHTs based on arbitrary constant-degree graphs. We prove that in a DLG-enabled, N-node DHT, the out-degree is d, the in-degree is between 1 and 2d, and the diameter is less than 2(log<sub>d</sub>N-log<sub>d</sub>N<sub>0</sub>+D<sub>0</sub>+1), where d, D<sub>0</sub> and N<sub>0</sub> represent the degree, diameter and number of nodes of the initial graph, respectively. The maintenance cost of DLG-enabled DHTs is O(log<sub>d</sub>N). We show the power of DLG technique by applying it to Kautz graphs to propose a new DHT scheme.
[Industries, Buildings, graph theory, distributed line graphs, Maintenance engineering, Routing, Topology, Construction industry, maintenance mechanisms, initial graph, arbitrary constant-degree graphs, static graphs, Distance measurement, computational complexity]
iSky: Efficient and Progressive Skyline Computing in a Structured P2P Network
2008 The 28th International Conference on Distributed Computing Systems
None
2008
An interesting problem in peer-based data management is efficient support for skyline queries within a multiattribute space. A skyline query retrieves from a set of multidimensional data points a subset of interesting points, compared to which no other points are better. Skyline queries play an important role in multi-criteria decision making and user preference applications. In this paper, we address the skyline computing problem in a structured P2P network. We exploit the iMinMax(thetas) transformation to map high-dimensional data points to 1-dimensional values. All transformed data points are then distributed on a structured P2P network called BATON, where all peers are virtually organized as a balanced binary search tree. Subsequently, a progressive algorithm is proposed to compute skyline in the distributed P2P network. Further, we propose an adaptive skyline filtering technique to reduce both processing cost and communication cost during distributed skyline computing. Our performance study, with both synthetic and real datasets, shows that the proposed approach can dramatically reduce transferred data volume and gain quick response time.
[Algorithm design and analysis, peer-based data management, Protocols, Filtering, peer-to-peer computing, Peer to peer computing, multicriteria decision making, multiattribute space, Artificial neural networks, skyline computing, iSky, information filtering, skyline queries, Indexes, tree searching, multidimensional data points, iMinMax transformation, query processing, user preference application, balanced binary search tree, Query processing, BATON, adaptive skyline filtering, structured P2P network]
Peer-to-Peer File Sharing Based on Network Coding
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Network coding is a promising enhancement of routing to improve network throughput and provide high reliability. It allows a node to generate output messages by encoding its received messages. Peer-to-peer networks are a perfect place to apply network coding due to two reasons: the topology of a peer-to-peer network is constructed arbitrarily, thus it is easy to tailor the topology to facilitate network coding; the nodes in a peer-to-peer network are end hosts which can perform more complex operations such as decoding and encoding than simply storing and forwarding messages. In this paper, we propose a scheme to apply network coding to peer-to-peer file sharing which employs a peer-to-peer network to distribute files resided in a web server or a file server. The scheme exploits a special type of network topology called combination network. It is proved that combination networks can achieve unbounded network coding gain measured by the ratio of network throughput with network coding to that without network coding. The scheme encodes a file into multiple messages and divides peers into multiple groups with each group responsible for relaying one of the messages. The encoding scheme is designed to satisfy the property that any subset of the messages can be used to decode the original file as long as the size of the subset is sufficiently large. To meet this requirement, we first define a deterministic linear network coding scheme which satisfies the desired property, then we connect peers in the same group to flood the corresponding message, and connect peers in different groups to distribute messages for decoding. Moreover, the scheme can be readily extended to support topology awareness to further improve system performance in terms of throughput, reliability and link stress. Our simulation results show that the new scheme can achieve 15%-20% higher throughput than Narada which does not employ network coding. In addition, it achieves good reliability and robustness to link failure or churn.
[deterministic linear network coding scheme, peer-to-peer computing, Peer to peer computing, peer-to-peer networks, Receivers, telecommunication network topology, Encoding, Topology, Servers, encoding, network topology, Relays, multicast, combination network, Network topology, Network coding, peer-to-peer network, web-based applications, file sharing]
Analysis of Maximum Executable Length for Detecting Text-Based Malware
2008 The 28th International Conference on Distributed Computing Systems
None
2008
The possibility of using purely text stream (keyboard-enterable) as carrier of malware is under-researched and often under estimated. A text attack can happen at multiple levels, from code-injection attacks at the top level to host-compromising text-based machine code at the lowest level. Since a large number of protocols are text-based, at times the servers based on those protocols use ASCII filters to allow text input only. However, simply applying ASCII filters to weed out the binary data is not enough from the security viewpoint since the assumption that malware are always binary is false. We show that although text is a subset of binary, binary malware detectors cannot always detect text malware. We analyze the MEL (maximum executable length)-based detection schemes, and make two contributions by this analysis. First, although the concept of MEL has been used in various detection schemes earlier, we are the first to provide its underlying mathematical foundation. We show that the threshold value can be calculated from the input character frequencies and that it can be tuned to control the detection sensitivity. Second, we demonstrate the effectiveness of a MEL-based text malware detector by exploiting the specific properties of text streams.
[text stream, invasive software, detection scheme, text attack, Network security, Probabilistic logic, Registers, Servers, Grippers, maximum executable length, Detectors, detection sensitivity, text malware detector, Malware, Worm, Cryptography, Payloads]
Efficient Distributed Third-Party Data Authentication for Tree Hierarchies
2008 The 28th International Conference on Distributed Computing Systems
None
2008
In the third-party model for the distribution of data, the trusted data creator or owner provides an untrusted distributor D with integrity verification (IV) items that are stored at D in addition to the n data items. When a user U has a subset of n' of those n data items and needs to verify their integrity, U is provided by D with a number of IV items that U uses to verify its data's integrity. The model forbids U from receiving any information about the n-n' data items that the user is not authorized to access, and assumes that D has no signature authority (it stores only pre-signed IVs). Most of the published work in this area uses the Merkle tree or variants thereof, and typically requires D to store a linear or close to linear (in n) number s(n) of IV items that are pre-signed by the trusted authority. Moreover, most of the existing schemes impose on D a non-constant amount of computation work t(n) (typically logarithmic in n) in order to provide U with the IV items that enable U to verify the integrity of its data; we call h(n) the number of such IV items. The h(n) values found in the literature are non-constant, i.e., they actually do depend on the number of data items. The main contribution of this paper is to achieve linear s(n), constant h(n) and constant or logarithmic t(n) when the n data items are organized in a tree hierarchy T, and the user's subset of n' items form a subtree T'. The cases of T' considered are when T' is (i) rooted at a node v and of depth k below v; and (ii) reachable in k hops from v going both up and down in T.
[data distribution, trees (mathematics), Data structures, Generators, data integrity, Servers, Aggregates, Authentication, Distributed databases, message authentication, distributed third-party data authentication, signature authority, tree hierarchies, Merkle tree, Cryptography, integrity verification]
Updates and Asynchronous Communication in Trusted Computing Systems
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Software-based usage controls typically are vulnerable to attacks. Trusted platform modules (TPMs) can enable much more robust controls. However, as conventionally understood, TPM-secured systems may not support software updates or asynchronous communication. We contribute techniques that overcome these limitations, enabling updates, secure transmission of usage-controlled files via email or removable disks, and secure use of such files on low-cost commercially available computers. We implemented the proposed scheme on Linux and report on its performance.
[Computers, Usage controls, asynchronous communication, trusted computing systems, removable disks, Servers, Security, files security, digital rights management (DRM), software updates, security of data, Current measurement, Linux, trusted platform module (TPM), Software, Cryptography, software-based usage controls, Kernel, trusted computing, email]
On Detection of Malicious Users Using Group Testing Techniques
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Despite decades of research, there have not been developed concrete defense solutions for most of current attacks to Internet services, let alone new attack types. An essential problem to overcome is that malicious traffic can be similar to legitimate ones. Thus a more fundamental model which should be based on the overall performance of servers/subnets without inspecting each traffic must be remedied. Based on this observation, we propose a novel system framework, called detection of malicious users (DMU) which attempts to solve various attack types. Motivated by DMU, we introduce a new theoretical model, called size constraint group testing (SCGT). Several algorithms based on SCGT for various networking scenarios are proposed. We also provide several fundamental results on SCGT, revealing some necessary conditions to obtain an O(1) detection time algorithm.
[Algorithm design and analysis, Real time systems, Internet services, group testing techniques, servers/subnets, Servers, size constraint group testing, DMU, security of data, malicious users detection, malicious traffic, Performance analysis, Internet, SCGT, IP networks, Monitoring, Testing]
The SCREAM Approach for Efficient Distributed Scheduling with Physical Interference in Wireless Mesh Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
It is known that CSMA/CA channel access schemes are not well suited to meet the high traffic demand of wireless mesh networks. One possible way to increase traffic carrying capacity is to use a spatial TDMA (STDMA) approach in conjunction with the physical interference model, which allows more aggressive scheduling than the protocol interference model on which CSMA/CA is based. While an efficient centralized solution for STDMA with physical interference has been recently proposed, no satisfactory distributed approaches have been introduced so far. In this paper, we first prove that no localized distributed algorithm can solve the problem of building a feasible schedule under the physical interference model. Motivated by this, we design a global primitive, called SCREAM, which is used to verify the feasibility of a schedule during an iterative distributed scheduling procedure. Based on this primitive, we present two distributed protocols for efficient, distributed scheduling under the physical interference model, and we prove an approximation bound for one of the protocols. We also present extensive packet-level simulation results, which show that our protocols achieve schedule lengths very close to those of the centralized algorithm and have running times that are practical for mesh networks.
[CSMA/CA channel access schemes, radio networks, distributed protocols, physical interference model, Distributed computing, wireless mesh networks, iterative distributed scheduling procedure, extensive packet-level simulation, physical interference, time division multiple access, aggressive scheduling, scheduling, traffic carrying capacity, protocols, SCREAM approach, carrier sense multiple access, high traffic demand, spatial TDMA]
Achieving Global End-to-End Maxmin in Multihop Wireless Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Following the huge commercial success of WLAN, multihop wireless networks are expected to lead in the next wave of deployment. Fundamental methods for traffic engineering must be developed to support diverse application requirements in these networks. This paper studies the problem of how to support weighted bandwidth allocation among all end-to-end flows in a multihop wireless network. Our goal is to enable the network to adapt the flow rates such that global maxmin can be achieved. Our approach is to transform the global maxmin objective into four local conditions and design a distributed rate adaptation protocol based on those local conditions. Comparing with the prior art, our protocol has a number of advantages. It is designed for the popular IEEE 802.11 DCF. It replaces per-flow queueing with per-destination queueing. It achieves far better fairness (or weighted fairness) among end-to-end flows.
[IEEE 802.11 DCF, Protocols, multihop wireless networks, minimax techniques, Wireless communication, end-to-end flow, traffic engineering, per-destination queueing, Wireless networks, flow rate adaptation, end-to-end maxmin, distributed rate adaptation protocol, protocols, distributed coordination function, fairness, queueing theory, IEEE 802.11 Standards, Routing, WLAN, Wireless sensor networks, weighted bandwidth allocation, Current measurement, multihop wireless network, wireless LAN, telecommunication traffic]
PAS: A Wireless-Enabled, Cell-Phone-Incorporated Personal Assistant System for Independent and Assisted Living
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Advances in networking, sensors, medical devices and smart phones have made it feasible to monitor and provide medical and other assistance to people either in their homes or outside. Aging populations will benefit from reduced costs and improved healthcare through assisted living based on these technologies. However, these systems challenge current state-of-the-art techniques for usability, reliability, and security. In this paper we present the PAS open architecture for assisted living, which allows independently developed third party components to collaborate. Furthermore, we incorporate cell phones in PAS as the local intelligence in order to enhance the robustness and ubiquity. We discuss key technological issues in assisted living systems, such as software architecture layout, power preserving, security and privacy; and results from our pilot study in a real assisted living facility are presented.
[open systems, assisted living, privacy, networking, Security, Communication system security, cell phone, healthcare, Privacy, software architecture, security, groupware, Monitoring, health care, cost reduction, third party component collaboration, power preserving, object-oriented programming, aging populations, personal computing, smart phones, security of data, sensors, Cellular phones, medical devices, personal assistant system open architecture, software architecture layout, Software, data privacy, Biomedical monitoring]
Optimized Multipath Network Coding in Lossy Wireless Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Network coding has been a prominent approach to a series of problems that used to be considered intractable with traditional transmission paradigms. Recent work on network coding includes a substantial number of optimization based protocols, but mostly for wireline multicast networks. In this paper, we consider maximizing the benefits of network coding for unicast sessions in lossy wireless environments. We propose Optimized Multipath Network Coding (OMNC), a rate control and routing protocol that dramatically improves the throughput of lossy wireless networks. OMNC employs multiple paths to push coded packets to the destination, and uses the broadcast MAC to deliver packets between neighboring nodes. The coding and broadcast rate is allocated to transmitters by a distributed optimization algorithm that maximizes the advantage of path diversity while avoiding congestion. With extensive experiments on an emulation testbed, we find that OMNC achieves significant throughput improvement over traditional best path routing protocols, and existing multipath routing protocols with network coding.
[optimized multipath network coding, radio networks, optimization based protocol, Protocols, distributed optimization, routing protocol, Throughput, Routing, Encoding, encoding, Optimization, wireline multicast networks, path diversity, optimisation, Unicast, Wireless networks, routing protocols, multicast communication, lossy wireless networks, unicast sessions, rate control]
Compiler-Assisted Application-Level Checkpointing for MPI Programs
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Application-level checkpointing can decrease the overhead of fault tolerance by minimizing the amount of checkpoint data. However this technique requires the programmer to manually choose the critical data that should be saved. In this paper, we firstly propose a live-variable analysis method for MPI programs. Then, we provide an optimization method of data saving for application-level checkpointing based on the analysis method. Based on the theoretical foundation, we implement a source-to-source precompiler (ALEC) to automate application-level checkpointing. Finally, we evaluate the performance of five FORTRAN/MPI programs which are transformed and integrated checkpointing features by ALEC on a 512-CPU cluster system. The experimental results show that i) the application-level checkpointing based on live-variable analysis for MPI programs can efficiently reduce the amount of checkpoint data, thereby decrease the overhead of checkpoint and restart; ii) ALEC is capable of automating application-level checkpointing correctly and effectively.
[Checkpointing, Computers, Algorithm design and analysis, checkpointing, data saving, message passing, optimising compilers, application program interfaces, compiler-assisted application-level checkpointing automation, source-to-source precompiler, program diagnostics, CPU cluster system, optimization method, Flow graphs, Equations, Program processors, live-variable analysis method, Algorithms, fault tolerance overhead, fault tolerant computing, program performance evaluation, software performance evaluation, MPI program]
Byzantine Fault-Tolerant Web Services for n-Tier and Service Oriented Architectures
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Mission-critical services must be replicated to guarantee correctness and high availability in spite of arbitrary (Byzantine) faults. Traditional Byzantine fault tolerance protocols suffer from several major limitations. Some protocols do not support interoperability between replicated services. Other protocols provide poor fault isolation between services leading to cascading failures across organizational and application boundaries. Moreover, traditional protocols are unsuitable for applications with tiered architectures, long-running threads of computation, or asynchronous interaction between services. We present Perpetual, a protocol that supports Byzantine fault-tolerant execution of replicated services while enforcing strict fault isolation. Perpetual enables interaction between replicated services that may invoke and process remote requests asynchronously in long-running threads of computation. We present a modular implementation, an Axis2 Web Services extension, and experimental results that demonstrate only a moderate overhead due to replication.
[Protocols, Byzantine agreement, Service oriented architecture, Fault tolerance, software architecture, Web services, n-Tier systems, Byzantine fault-tolerance protocol, Fault tolerant systems, fault tolerant computing, Perpetual, Safety, n-tier architecture, service oriented architecture, Axis2, Web Services, Driver circuits, asynchrnous invocation and processing]
A Formal Specification and Verification Framework for Designing and Verifying Reliable and Dependable Software for Computerized Numerical Control (CNC) Systems
2008 The 28th International Conference on Distributed Computing Systems
None
2008
As a distributed computing system, a CNC system needs to be operated reliably, dependably and safely. How to design reliable and dependable software and perform effective verification for CNC systems becomes an important research problem. In this paper, we propose a new modeling method called TTM/ATRTTL (Timed Transition Models/All-Time Real-Time Temporal Logics) for specifying CNCsystems. TTM/ATRTTL provides full supports for specifying hard real-time and feedback that are needed for modeling CNC systems. We also propose a verification framework with verification rules and theorems and implement it with STeP and SF2STeP. The proposed verification framework can check reliability, dependability and safety of systems specified by our TTM/ATRTTL method. We apply our modeling and verification techniques on an open architecture CNC (OAC) system and conduct comprehensive studies on modeling and verifying a logical controller that is the key part of OAC. The results show that our method can effectively model and verify CNC systems and generate CNC software that can satisfy system requirements in reliability, dependability and safety.
[Real time systems, timed transition models/all-time real-time temporal logics, Current transformers, systems safety, production engineering computing, reliability, distributed processing, temporal logic, open architecture CNC system, STeP, Computer numerical control, formal specification, distributed computing system, TTM/ATRTTL, formal verification, Computed tomography, Safety, check reliability, systems dependability, formal specification and verification, Dependability, Software reliability, SF2STeP, Circuit faults, Fault currents, computerised numerical control, computerized numerical control systems, CNC, Reliability]
A Connectivity-Driven Retransmission Scheme Based On Transport Layer Readdressing
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Migration between different wireless access networks often involves disconnected period, which is caused by passing an area of bad wireless coverage and potential overhead to switch the network on the network interface to connect to. The disconnected period can cause extra transmission delay due to the timer-driven retransmission behavior in the transport protocols, such as TCP and SCTP. We propose a new retransmission scheme to achieve better migration performance in SCTP, which is a newer connection-oriented and reliable transport protocol that is becoming popular. Our scheme minimizes the extra transmission delay by leveraging address reconfiguration operation in SCTP without involving other layers. It decreases the delay more than 5 seconds compared to the original SCTP when migration involves approximately ten-second disconnected period. The implementation of our scheme is already imported in FreeBSD.
[TCP, mobility, Peer to peer computing, connectivity-driven retransmission scheme, transport layer readdressing, wireless coverage, Mobile communication, SCTP, Delay, radio access networks, Wireless communication, network interface, transport protocols, retransmission behavior, potential overhead, wireless access networks, disruption tolerance, IP networks, FreeBSD, Mobile computing, Manganese, timer-driven retransmission behavior]
A Sophisticated Privacy-Enhanced Yet Accountable Security Framework for Metropolitan Wireless Mesh Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Recently, multi-hop wireless mesh networks (WMNs) have attracted increasing attention and deployment as a low-cost approach to provide broadband Internet access at metropolitan scale. Security and privacy issues are of most concern in pushing the success of WMNs for their wide deployment and for supporting service-oriented applications. Despite the necessity, limited security research has been conducted towards privacy preservation in WMNs. This motivates us to develop PEACE, a soPhisticated privacy-Enhanced yet Accountable seCurity framEwork, tailored for WMNs. At the one hand, PEACE enforces strictuser access control to cope with both free riders and malicious users. On the other hand, PEACE offers sophisticated user privacy protection against both adversaries and various other network entities. PEACE is presented as a suite of authentication and key agreement protocols built upon our proposed short group signature variation. Our analysis shows that PEACE is resilient to a number of security and privacy related attacks.
[telecommunication security, radio networks, cryptographic protocols, service-oriented application, broadband networks, PEACE, Security, Communication system security, broadband Internet access, Wireless communication, Privacy, metropolitan area networks, authorisation, access control, user privacy protection, telecommunication network topology, multihop metropolitan wireless mesh network, short group signature variation, Wireless sensor networks, Authentication, message authentication, data privacy, digital signatures, Internet, sophisticated privacy-enhanced security framework, key agreement protocol]
How to Monitor for Missing RFID tags
2008 The 28th International Conference on Distributed Computing Systems
None
2008
As RFID tags become more widespread, new approaches for managing larger numbers of RFID tags will be needed. In this paper, we consider the problem of how to accurately and efficiently monitor a set of RFID tags for missing tags. Our approach accurately monitors a set of tags without collecting IDs from them. It differs from traditional research which focuses on faster ways for collecting IDs from every tag. We present two monitoring protocols, one designed for a trusted reader and another for an untrusted reader.
[Protocols, trusted reader, radiofrequency identification, Radiation detectors, untrusted reader, RFID, Servers, Circuit faults, monitoring protocols, Fault currents, missing RFID tags, Monitoring, Radiofrequency identification]
Controlled Information Sharing in Collaborative Distributed Query Processing
2008 The 28th International Conference on Distributed Computing Systems
None
2008
We present a simple, yet powerful, approach for the specification and enforcement of authorizations regulating data release among data holders collaborating in a distributed computation, to ensure that query processing discloses only data whose release has been explicitly authorized. Data disclosure is captured by means of profiles, associated with each data computation, that describe the information carried by the result. We also present an algorithm that, given a query plan, determines whether it can be safely executed and produces a safe execution strategy. The main advantage of our approach is its simplicity that, without impacting expressiveness, makes it nicely interoperable with current solutions for collaborative computations in distributed database systems.
[query plan, information dissemination, Medical services, distributed computation, data holders, Servers, collaborative computation, data computation, Diseases, Authorization, query processing, data disclosure, Hospitals, collaborative distributed query processing, Distributed databases, Insurance, distributed databases, groupware, controlled information sharing, distributed database system]
Efficient Privacy-Preserving k-Nearest Neighbor Search
2008 The 28th International Conference on Distributed Computing Systems
None
2008
We give efficient protocols for secure and private k-nearest neighbor (k-NN) search, when the data is distributed between two parties who want to cooperatively compute the answers without revealing to each other their private data. Our protocol for the single-step k-NN search is provably secure and has linear computation and communication complexity. Previous work on this problem had a quadratic complexity, and also leaked information about the parties' inputs. We adapt our techniquesto also solve the general multi-step k-NN search, and describe a specific embodiment of it for the case of sequence data. The protocols and correctness proofs can be extended to suit other privacy-preserving data mining tasks, such as classification and outlier detection.
[Protocols, private k-nearest neighbor search, Artificial neural networks, Search problems, Distance measurement, data privacy, privacy-preserving task, Complexity theory, Cryptography, search problems, Nearest neighbor searches]
Prospect and Challenges for Network Development
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Summary form only given. The development situation of telecom services is firstly reviewed in this presentation. The evolution trends of network services with IP, broadband, streaming, P2P (peer to peer) and mobility features are described. The presentation introduces the evolution direction and prospect of networks technologies especially NGI (next generation Internet) and NGN (next generation network) as well as broadband mobile IP network. The presentation also gives a brief overview of representative projects within research activities on NGI including the CNGI (China's next Internet demonstration project). Finally, the challenges that network development faced are discussed such as scalability, QoS, pervasive computing and ubiquitous networking, security issue and shortage of frequency spectrum as well as dilemma whether to do evolution or revolution strategy of network development.
[telecommunication security, ubiquitous networking, broadband networks, network development, next generation Internet, ubiquitous computing, pervasive computing, Next generation networking, streaming, Evolution (biology), QoS, broadband network, Book reviews, IP networks, network service, peer-to-peer computing, next generation network, network security, Broadband communication, Telecommunications, quality of service, mobility features, IP network, mobile communication, telecom service, Internet, frequency spectrum, peer-to-peer network, mobile network]
Efficient and Robust Local Mutual Exclusion in Mobile Ad Hoc Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
This paper presents two algorithms for the local mutual exclusion problem, an extension of the dining philosophers problem for mobile ad hoc networks. A solution to this problem allows nodes that are currently geographically close to obtain exclusive access to a resource. The algorithms exhibit different tradeoffs between response time and failure locality (the size of the neighborhood adversely affected by a node crash). The first algorithm has two variations, one of which has response time that depends very weakly on the number of nodes in the entire system and is polynomial in the maximum number of neighboring nodes; the failure locality, although not optimal, is small and grows very slowly with system size. The second algorithm has optimal failure locality and response time that is quadratic in the number of nodes. A pleasing aspect of this algorithm is that, when run in a system with no node movement, it has linear response time, improving on previous results for static algorithms with optimal failure locality.
[Algorithm design and analysis, mobile ad hoc network, Protocols, mobile radio, failure locality, polynomials, Color, Mobile communication, Ad hoc networks, response time, local mutual exclusion, Time factors, ad hoc networks, Mobile computing]
On the Longest RNG Edge of Wireless Ad Hoc Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Relative neighborhood graph (RNG) has been widely used in topology control and geographic routing in wireless ad hoc networks. Its maximum edge length is the minimum requirement on the maximum transmission radius by those applications of RNG. In this paper, we derive the precise asymptotic probability distribution of the maximum edge length of the RNG on a Poisson point process over a unit-area disk. Since the maximum RNG edge length is a lower bound on the critical transmission radius for greedy forward routing, our result also leads to an improved asymptotic almost sure lower bound on the critical transmission radius for greedy forward routing.
[asymptotic probability distribution, maximum edge length, Area measurement, graph theory, probability, geographic routing, wireless ad hoc networks, telecommunication network topology, Routing, Electronic mail, Topology, Mobile ad hoc networks, RNG edge, Network topology, Poisson point process, maximum transmission radius, unit-area disk, telecommunication network routing, relative neighborhood graph, critical transmission radius, Distance measurement, ad hoc networks, topology control, greedy forward routing]
Two-Phased Approximation Algorithms for Minimum CDS in Wireless Ad Hoc Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Connected dominating set (CDS) has a wide range of applications in wireless ad hoc networks. A number of distributed algorithms for constructing a small CDS in wireless ad hoc networks have been proposed in the literature. The majority of these distributed algorithms follow a general two-phased approach. The first phase constructs a dominating set, and the second phase selects additional nodes to interconnect the nodes in the dominating set. In this paper, we prove that the approximation ratio of the two-phased algorithm in [10] is at most 7 1/3, improving upon the previous best-known approximation ratio of 7.6 due to [12]. We also propose a new two-phased approximation algorithm and prove that its approximation ratio is at most 6 7/18. Our analyses exploit an improved upper bound on the number independent points that can be packed in the neighborhood of a connected finite planar set.
[Algorithm design and analysis, approximation theory, wireless ad hoc networks, Approximation methods, Mobile ad hoc networks, Upper bound, distributed algorithms, Linear approximation, connected dominating set, Approximation algorithms, two-phased approximation algorithms, ad hoc networks, Distributed algorithms]
DTP: Double-Pairwise Time Protocol for Disruption Tolerant Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Network time synchronization (NTS) is essential for any distributed systems, including disruption tolerant networks (DTNs). The feature of frequent contact disruptions and discontinuous network connections in DTNs raises a new challenge in providing the NTS service: synchronization operations may be interrupted for a long period of time when links between the time synchronization peers are opportunistic. To address this challenge, we propose double- pairwise time protocol (DTP) that can achieve better clock estimations and synchronization results in DTNs than NTP-Core, which models the major functionality of processing synchronization messages in network time protocol (NTP) that is the time-keeping standard in the Internet. The characteristics of DTP include: i) DTP achieves approximately half of the maximum time error achieved by NTP-Core; and ii) DTP only requires minor modifications to be implemented in the current architecture of NTP. Simulation results based on trace data collected from existing DTN testbeds validate such characteristics of DTP.
[Real time systems, telecommunication links, Protocols, network time protocol, Time Synchronization, NTP, Servers, Delay, synchronization messages, network time synchronization, double-pairwise time protocol, distributed systems, discontinuous network connections, time synchronization peer links, contact disruptions, protocols, Disruption/Delay Tolerant Networks, mobile radio, time keeping, Synchronization, synchronisation, disruption tolerant networks, Internet, ad hoc networks, clock estimation, Clocks]
End-to-End Congestion Control for High Speed Networks Based on Population Ecology Models
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Since TCP congestion control is ill-suited for high speed networks, designing a replacement for TCP has become a challenge. To address this problem, we extend the population ecology theory to design a novel congestion control algorithm. We treat the network flows as the species in nature, the throughput of the flows as the population number, and the bottleneck bandwidth as the food resources. Then we use the key idea of constructing population ecology models to develop a novel congestion control model, and implement the corresponding end-to-end transport protocol through measurement, which called Population Ecology TCP (PE-TCP). The theoretical analysis and simulation results validate that PE-TCP achieves high utilization, fast convergence, fair bandwidth allocation, and near-zero packet drops. These qualities are desirable for high speed networks.
[end-to-end congestion control, population ecology models, telecommunication congestion control, Biological system modeling, Throughput, Environmental factors, PE-TCP, Congestion Control, Equations, Convergence, high speed network, transport protocol, transport protocols, Bandwidth, population ecology model, Internet, Mathematical model, high speed networks]
Measurement Manipulation and Space Selection in Network Coordinates
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Internet coordinate systems have emerged as an efficient method to estimate the latency between pairs of nodes without any communication between them. However, most coordinate systems have been evaluated solely on data sets built by their authors from measurements gathered over large periods of time. Although they show good prediction results, it is unclear whether the accuracy is the result of the system design properties or is more connected to the characteristics of the data sets. In this paper, we revisit a simple question: how do the features of the embedding space and the inherent attributes of the data sets interact in producing good embeddings? We adapt the Vivaldi algorithm to use Hyperbolic space for embedding and evaluate both Euclidean and Hyperbolic Vivaldi on seven sets of real-world latencies. Our results show that node filtering and latency distributions can significantly influence the accuracy of the predictions. For example, although Euclidean Vivaldi performs well on data sets that were chosen, constructed and filtered by the designers of the algorithm, its performance and robustness decrease considerably when run on third party data sets that were not filtered a priori. Our results offer important insight into designing and building coordinate systems that are both robust and accurate in Internet-like environments.
[space selection, Accuracy, Filtering, Peer to peer computing, Extraterrestrial measurements, Distance measurement, Internet, measurement manipulation, Euclidean Vivaldi, Springs, coordinate systems]
Inbound Traffic Load Balancing in BGP Multi-homed Stub Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Multihoming load balancing improves network performance by leveraging the traffic among the access links in a multi-homed network. Currently, no effective load balancing system is available to handle the inbound traffic in a BGP multi-homed stub network, where the traffic volume is unknown to the network and the route of the traffic is hard to control. In this paper, we propose ILBO, an inbound traffic load balancing mechanism to effectively balance the inbound traffic in a BGP multi-homed stub network. ILBO predicts and schedules the inbound traffic based on outbound traffic. It also provides an inbound traffic control scheme that can guarantee the successful execution of the traffic scheduling.
[Encapsulation, telecommunication links, inbound traffic scheduling, access links, traffic route, internetworking, network traffic volume, multi-homed, Support vector machines, Training, resource allocation, network performance, traffic leveraging, traffic control, routing protocols, inbound traffic load balancing, Traffic control, BGP multihomed stub networks, Load management, load balance, Internet, Border Gateway Protocol, IP networks, telecommunication traffic]
Relative Network Positioning via CDN Redirections
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Many large-scale distributed systems can benefit from a service that allows them to select among alternative nodes based on their relative network positions. A variety of approaches propose new measurement infrastructures that attempt to scale this service to large numbers of nodes by reducing the amount of direct measurements to end hosts. In this paper, we introduce a new approach to relative network positioning that eliminates direct probing by leveraging pre-existing infrastructure. Specifically, we exploit the dynamic association of nodes with replica servers from large content distribution networks (CDNs) to determine relative position information - we call this approach CDN-based relative network positioning (CRP). We demonstrate how CRP can support two common examples of location information used by distributed applications: server selection and dynamic node clustering. After describing CRP in detail, we present results from an extensive wide-area evaluation that demonstrates its effectiveness.
[wide area networks, Peer to peer computing, Servers, large-scale distributed systems, replica servers, measurement infrastructures, Accuracy, Atmospheric measurements, Network positioning systems, measurement reuse, Position measurement, Particle measurements, relative network positioning, Distance measurement, CDN redirections, content distribution networks]
Optimal Placements in Ring Network for Data Replicas in Distributed Database with Majority Voting Protocol
2008 The 28th International Conference on Distributed Computing Systems
None
2008
In a distributed database system, data replicas are placed at different locations to achieve high data availability in the presence of link failures. With majority voting protocol, a location is survived for read/write operations if and only if it is accessible to more than half of the replicas. The problem is to find out the optimal placements for a given number of data replicas in a ring network. When the number of replicas is odd, it was conjectured by Hu et al. that every uniform placement is optimal, which is proved by Shekhar and Wu later. However, when the number of replicas is even, it was pointed out by Hu et al. that uniform placements are not optimal and the optimal placement problem may be very complicated. In this paper, we study the optimal placement problem in a ring network with majority voting protocol and even number of replicas, and give a complete characterization of optimal placements when the number of replicas is not too large compared with the number of locations.
[distributed database, Protocols, Terminology, replicated databases, optimal placements, data replica, Mathematics, majority voting protocol, data replicas, Computer science, Distributed databases, ring network, Database systems, Clocks]
Wavenet: A Wavelet-Based Approach to Monitor Changes on Data Distribution in Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
A massive amount of data is available in distributed fashion on various networks, including Internet, peer-to-peer networks, and wireless sensor networks. Users are often interested in monitoring interesting patterns or abnormal events hidden in these data. Transferring all the raw data from each host node to a central coordinator for processing is costly and unnecessary. In this study, we investigate the problem of monitoring changes on the data distribution in the networks (MCDN). To address this problem, we propose a technique, called wavenet, by compressing the local item set in each host node into a compact yet accurate summary, called local wavelet, for communication with the coordinator. We also propose adaptive monitoring to address the issues of local wavelet propagation in wavenet. An extensive performance evaluation has been conducted to validate our proposal and demonstrates the efficiency of wavenet.
[Wavelet transforms, data compression, wavelet-based approach, adaptive monitoring, peer-to-peer computing, wireless sensor networks, Peer to peer computing, wavelet transforms, data distribution, wireless sensor network, Entropy, item set compression, Wavelet coefficients, Histograms, network monitoring, Distributed databases, Internet, Monitoring, wavenet, peer-to-peer network]
Scalable and Adaptive Metadata Management in Ultra Large-Scale File Systems
2008 The 28th International Conference on Distributed Computing Systems
None
2008
This paper presents a scalable and adaptive decentralized metadata lookup scheme for ultra large-scale file systems (ges Petabytes or even Exabytes). Our scheme logically organizes metadata servers (MDS) into a multi-layered query hierarchy and exploits grouped bloom filters to efficiently route metadata requests to desired MDS through the hierarchy. This metadata lookup scheme can be executed at the network or memory speed, without being bounded by the performance of slow disks. Our scheme is evaluated through extensive trace-driven simulations and prototype implementation in Linux. Experimental results show that this scheme can significantly improve metadata management scalability and query efficiency in ultra large-scale storage systems.
[scalable-adaptive metadata management, meta data, metadata servers, adaptive decentralized metadata lookup scheme, ultralarge-scale file systems, Throughput, Information filtering, bloom filters, Servers, table lookup, trace-driven simulations, File systems, Linux, multilayered query hierarchy, Information filters, Arrays, Mathematical model, query efficiency]
MR-PDP: Multiple-Replica Provable Data Possession
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Many storage systems rely on replication to increase the availability and durability of data on untrusted storage systems. At present, such storage systems provide no strong evidence that multiple copies of the data are actually stored. Storage servers can collude to make it look like they are storing many copies of the data, whereas in reality they only store a single copy. We address this shortcoming through multiple-replica provable data possession (MR-PDP): A provably-secure scheme that allows a client that stores t replicas of a file in a storage system to verify through a challenge-response protocol that (1) each unique replica can be produced at the time of the challenge and that (2) the storage system uses t times the storage required to store a single replica. MR-PDP extends previous work on data possession proofs for a single copy of a file in a client/server storage system (Ateniese et al., 2007). Using MR-PDP to store t replicas is computationally much more efficient than using a single-replica PDP scheme to store t separate, unrelated files (e.g., by encrypting each file separately prior to storing it). Another advantage of MR-PDP is that it can generate further replicas on demand, at little expense, when some of the existing replicas fail.
[Availability, client-server systems, storage system, Protocols, replicated databases, Biological system modeling, Materials, multiple-replica provable data possession, Servers, storage management, storage server, challenge-response protocol, Titanium, client/server system, Cryptography]
Opportunity-Based Topology Control in Wireless Sensor Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Topology control is an effective method to improve the energy efficiency of wireless sensor networks (WSNs). Traditional approaches are based on the assumption that a pair of nodes is either "connected" or "disconnected". These approaches are called connectivity-based topology control. In real environments however, there are many intermittently connected wireless links called lossy links. Taking a succeeded lossy link as an advantage, we are able to construct more energy-efficient topologies. Towards this end, we propose a novel opportunity-based topology control. We show that opportunity-based topology control is a problem of NPhard. To address this problem in a practical way, we design a fully distributed algorithm called CONREAP based on reliability theory. We prove that CONREAP has a guaranteed performance. The worst running time is O(jEj) where E is the link set of the original topology, and the space requirement for individual nodes is O(d) where d is the node degree. To evaluate the performance of CONREAP, we design and implement a prototype system consisting of 50 BerkeleyMica2 motes. We also conducted comprehensive simulations. Experimental results show that compared with the connectivity-based topology control algorithms, CONREAP can improve the energy efficiency of a network up to 6 times.
[Algorithm design and analysis, wireless sensor networks, telecommunication control, wireless sensor network, telecommunication network topology, CONREAP distributed algorithm, connectivity-based topology control, connected wireless link, Topology, reliability theory, Approximation methods, network topology, opportunity-based topology control, Niobium, lossy link, wireless communication, Wireless sensor networks, Network topology, NP-hard problem, distributed algorithms, telecommunication network reliability, Energy efficiency, computational complexity]
Connectivity-Guaranteed and Obstacle-Adaptive Deployment Schemes for Mobile Sensor Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Mobile sensors can move and self-deploy into a network. While focusing on the problems of coverage, existing deployment schemes mostly over-simplify the conditions for network connectivity: they either assume that the communication range is large enough for sensors in geometric neighborhoods to obtain each other's locationby local communications, or assume a dense network that remains connected. At the same time, an obstacle-free field or full knowledge of the field layout is often assumed. We present new schemes that are not restricted by these assumptions, and thus adapt to a much wider range of application scenarios. While maximizing sensing coverage, our schemes can achieve connectivity for a network with arbitrary sensor communication/sensing ranges or node densities, at the cost of a small moving distance; the schemes do not need any knowledge of the field layout, which can be irregular and have obstacles/holes of arbitrary shape. Simulations results show that the proposed schemes achieve the targeted properties.
[Base stations, Pediatrics, obstacle-adaptive deployment, Obstacle, wireless sensor networks, Sensor Networks, mobile sensor networks, connectivity-guaranteed deployment, Deployment, Mobile, mobile communication, Layout, network connectivity, Chromium, Connectivity, Distance measurement, Sensors, Floors]
Sensor Node Localization Using Uncontrolled Events
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Many event-driven localization methods have been proposed as low cost, energy efficient solutions for wireless senor networks. In order to eliminate the requirement of accurately controlled events in existing approaches, we present a practical design using totally uncontrolled events for stationary sensor node positioning. The novel idea of this design is to estimate both the event generation parameters and the location of each sensor node by processing node sequences easily obtained from uncontrolled event distribution. To demonstrate the generality of our design, both straight-line scan and circular wave propagation events are addressed in this paper, and we evaluated our approach through theoretical analysis, extensive simulation and a physical test bed implementation with 41 MICAz motes. The evaluation results illustrate that with only randomly generated events, our solution can effectively localize sensor nodes with excellent flexibility while adding no extra cost at the resource constrained sensor node side. In addition, localization using uncontrolled events provides a nice potential option of achieving node positioning through natural ambient events.
[straight-line scan, Parameter estimation, wireless sensor networks, uncontrolled event distribution, Estimation, wireless senor network, Laser beams, Image segmentation, stationary sensor node positioning, Measurement by laser beam, sensor node localization, event-driven localization, parameter estimation, Distance measurement, electromagnetic wave propagation, Joints, circular wave propagation event]
Quality of Trilateration: Confidence Based Iterative Localization
2008 The 28th International Conference on Distributed Computing Systems
None
2008
The proliferation of wireless and mobile devices has fostered the demand of context aware applications. Location is one of the most significant contexts. Multilateration, as a basic building block of localization, however, have not yet overcome the challenges of (1) poor ranging measurement; (2) dynamic and noisy environments; (3) fluctuations in wireless communications. Hence, they often suffer poor accuracy and can hardly be employed in practical applications. In this study, we propose quality of trilateration (QoT) that quantifies the geometric relationship of objects and the ranging noise. Based on QoT, we design a confidence based iterative localization scheme, in which nodes dynamically select trilaterations with the highest quality for localization. To validate this design, a wireless sensor network prototype is deployed and results show that QoT well represents trilateration accuracy, and the proposed scheme significantly improve localization performances.
[wireless communications, iterative methods, wireless sensor networks, Noise, Noise measurement, Wireless Sensor Networks, wireless sensor network prototype, Trilateration, Wireless sensor networks, Accuracy, trilateration quality, wireless devices, geometric relationship, Prototypes, mobile devices, Distance measurement, context aware applications, multilateration, Copper, Localization, mobile handsets, iterative localization]
Deadlock-Free Fully Adaptive Routing in 2-Dimensional Tori Based on New Virtual Network Partitioning Scheme
2008 The 28th International Conference on Distributed Computing Systems
None
2008
A new deadlock-free fully adaptive routing algorithm is proposed for 2-dimensional tori with only two virtual channels. The deadlock avoidance technique is presented based on a new virtual network partitioning scheme. Unlike the previous virtual network partitioning schemes, the new method allows all virtual networks to share some common virtual channels. Two virtual channels should be the lower bound for fully adaptive deadlock-free routing in tori because the dimension order routing for 2-dimensional tori also needs two virtual channels. The proposed virtual network partitioning scheme can avoid all potential deadlocks and provides fully adaptive routing. Enough theoretical analyses on the proposed virtual network partitioning scheme are presented. Sufficient simulation results are presented to demonstrate the effectiveness of the proposed algorithm by comparing with the dimension-order routing, two partially adaptive routing schemes, Duato's protocol and the load-balanced routing algorithm GOAL.
[Adaptive systems, Protocols, deadlock-free fully adaptive routing, tori, Adaptation model, virtual network partition, Routing, virtual private networks, Partitioning algorithms, virtual network partitioning scheme, Fully adaptive, deadlock avoidance technique, telecommunication network routing, System recovery, Routing protocols, 2D tori]
DCAR: Distributed Coding-Aware Routing in Wireless Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
The practical network coding system proposed in (S. Katti et al., 2006) has two fundamental limitations: 1) the coding opportunity is crucially dependent on the established routes; 2) the coding structure is limited within a two-hop region. To overcome these limitations, we propose DCAR, the first distributed coding-aware routing mechanism which combines (a) the discovery for available paths between a given source and destination, and (b) the detection for potential network coding opportunities. DCAR has the potential to find high throughput paths with coding opportunities while conventional routing fails to do so. In addition, DCAR can detect coding opportunities on the entire path, thus eliminating the "two-hop" coding limitation in (S. Katti et al., 2006). We also propose a novel routing metric called "CRM" (coding-aware routing metric) which facilitates the comparison between coding-possible and coding-impossible paths. We implement the DCAR system in NS-2 and conduct extensive evaluation, which shows that DCAR achieves 7% to 20% throughput gain over the coding system in [1].
[radio networks, distributed coding-aware routing, Routing, Throughput, wireless networks, Encoding, Wireless Networks, Decoding, encoding, DCAR, Wireless networks, Network Coding, telecommunication network routing, Bandwidth, coding-aware routing metric, network coding system, Gain, Coding-Aware Routing]
Utility-Based Opportunistic Routing in Multi-Hop Wireless Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Recently, opportunistic routing (OR) has been widely used to compensate for the low packet delivery ratio of multi-hop wireless networks. Previous works either provide heuristic solutions without optimality analysis, or assume that unlimited retransmission is available for delivering a data packet. In this paper, we apply OR to a utility-based routing where the successful delivery of a data packet generates benefit. The objective is to maximize utility, defined as a function of benefit and cost of transmission. As the link reliability of each relay determines eventual packet delivery and hence utility, OR offers the ability to increase reliability through opportunistic relays. We explore the optimality of utility-based routing through OR without allowing retransmission, and observe that the optimal scheme requires exhaustive searching of all paths from source to destination. We then propose a heuristic solution to select relays and determine priorities among them. Finally, we provide distributed implementations for both schemes. Simulations on NS-2 and our customized simulator are conducted to verify the effectiveness of the heuristic compared with the optimal.
[radio networks, Computational modeling, Heuristic algorithms, exhaustive search method, utility-based opportunistic routing, opportunistic relay, multi-hop, Routing, Complexity theory, Relays, Network topology, multihop wireless network, packet delivery ratio compensation, telecommunication network routing, telecommunication network reliability, Reliability, Distributed algorithms, search problems, link reliability]
Decentralized Routing in Nonhomogeneous Poisson Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
In his seminal work, Jon Kleinberg considers a small-world network model consisting of a k-dimensional lattice augmented with shortcuts. Under the assumption that the probability of a shortcut being present between two nodes u and v decays as a power, d(u,v) -\\alpha, of the distance d(u,v) between them, Kleinberg shows that decentralized routing scheme such as greedy geographic routing is efficient if alpha=k and that there is no efficient decentralized routing algorithm if alpha\\neq k. The results are extended to a continuum model recently, wherein the nodes are distributed as a homogeneous Poisson point process by Franceschetti and Meester, Draief and Ganesh. In our work, we extend the result further to a more realistic model constructed from a nonhomogeneous Poisson point process, wherein each node is connected to all its neighbors within some fixed radius, as well as possessing random shortcuts to more distant nodes. More importantly, we show that in nonhomogeneous cases, the necessary and sufficient condition for greedy geographic routing to be efficient is that the probability of a shortcut being present from node u to v should be inversely proportional to the number of nodes which are closer to u than v is. We also demonstrate some applications of our results to wireless networks.
[Algorithm design and analysis, radio networks, small-world network model, Social network services, Lattices, probability, nonhomogeneous Poisson networks, greedy geographic routing, homogeneous Poisson point process, Routing, Ad hoc networks, continuum model, Wireless networks, k-dimensional lattice, telecommunication network routing, decentralized routing, Distance measurement, stochastic processes, shortcut probability, wireless network]
Multi-Site Retrieval of Declustered Data
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Declustering techniques reduce query response times through parallel I/O by distributing data among multiple devices. Recently, replication based approaches were proposed to further reduce the response time. All of the replication based schemes assume that replication is done at a single site. In this paper, we consider replicated data stored at multiple sites. We formulate multi-site retrieval problem as a maximum flow problem and solve it using maximum flow techniques. We propose a low complexity online algorithm for the problem. We investigate the proposed scheme using various replication schemes, query types and query loads. Proposed scheme can easily be extended to nonuniform data and to any number of sites. Experimental results show that replication using orthogonal allocation performs the best under various settings.
[Current transformers, replicated databases, declustering technique, Circuit faults, parallel I/O, Delay, Fault currents, query processing, multisite retrieval problem, Computed tomography, retrieval, Chromium, maximum flow problem, declustering, Resource management, computational complexity, replicated data]
A Nonblocking Approach for Reaching an Agreement on Request Total Orders
2008 The 28th International Conference on Distributed Computing Systems
None
2008
In distributed systems that use active replication to achieve robustness, it is important to efficiently enforce consistency among replicas. The nonblocking mode helps to speed up system execution. Unfortunately, this benefit comes at the expense of introducing decision conflicts when the replicas form a single logical token ring and client requests are processed in sequence following the ring. In order to reach an agreement regarding request total orders, this paper proposes a forward-confirmation (FC) approach to identify and solve decision conflicts when up to k successive replicas fail simultaneously. The FC approach can obtain consistent decisions among replicas. An implementation of the FC approach, namely, the queueing method, is proposed. Test results show that our protocol in the nonblocking mode outperforms the Totem protocol regarding delays and failure recovery.
[request total orders, Protocols, queueing theory, agreement, Computational modeling, Decision making, failure recovery, nonblocking, distributed processing, total order, active replication, Indexes, replica consistency, system recovery, Fault tolerance, nonblocking approach, queueing method, Evolution (biology), performance, USA Councils, forward-confirmation approach, distributed systems]
An Arbitrary Tree-Structured Replica Control Protocol
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Traditional replication protocols that arrange logically the replicas into a tree structure have reasonable availability, low communication costs but induce high system load. We propose in this paper the arbitrary protocol: a tree-based replica control protocol that can be configured based on the frequencies of read and write operations in order to provide lower system load than existing tree replication protocols, yet with comparable cost and availability. Our protocol enables the shifting from one configuration into another by just modifying the structure of the tree. There is no need to implement a new protocol whenever the frequencies of read and write operations change. At the heart of our protocol lies the new idea of logical and physical levels in a tree. In short, read operations are carried out on any physical node of every physical level of the tree whereas the write operation is performed on all physical nodes of a single physical level of the tree. We discuss optimal configurations, proving in particular a new lower bound, of independent interest, for the case of a binary tree.
[Availability, Heart, Protocols, binary tree, replicated databases, replication protocols, tree structure, Computer crashes, replica control, Equations, Fault tolerance, Binary trees, tree data structures, protocols]
Correlation-Aware Object Placement for Multi-Object Operations
2008 The 28th International Conference on Distributed Computing Systems
None
2008
A multi-object operation incurs communication or synchronization overhead when the requested objects are distributed over different nodes. The object pair correlations (the probability for a pair of objects to be requested together in an operation) are often highly skewed and yet stable over time for real-world distributed applications. Thus, placing strongly correlated objects on the same node (subject to node space constraint) tends to reduce communication overhead for multi-object operations. This paper studies the optimization of correlation-aware data placement. First, we formalize a restricted form of the problem as a variant of the classic Quadratic Assignment problem and we show that it is NP-hard. Based on a linear programming relaxation, we then propose a polynomial-time approximation algorithm that finds an object placement with communication overhead at most two times that of the optimal placement. We further show that the computation cost can be reduced by limiting the optimization scope to a relatively small number of most important objects. We quantitatively evaluate our approach on keyword index placement for full-text search engines using real traces of 3.7 million web pages and 6.8 million search queries. Compared to the correlation-oblivious random object placement, our approach achieves 37-86% communication overhead reduction on a range of optimization scopes and system sizes. The communication reduction is 30-78% compared to a correlation-aware greedy approach.
[full-text search engine, keyword index placement, Correlation, classic quadratic assignment problem, distributed processing, Linear programming, Stability analysis, linear programming, correlation-aware object placement, Approximation methods, Indexes, distributed application, quadratic assignment, Optimization, optimization, Search engines, linear programming relaxation, polynomial-time approximation algorithm, multiobject operation, computational complexity, Multi-object operations]
Locality Sensitive Information Brokerage in Distributed Sensor Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
In sensor network applications, sensors often need to retrieve data from each other. Information brokerage is a scheme that stores data (or index files of data) at rendezvous nodes, so that every sensor can efficiently finds the data it needs. A very useful property for information brokerage is locality sensitivity, which means that a sensor close the original source of the data should also be able to retrieve the data with a small communication cost. Given the locality sensitivity requirement, the key is to design an information brokerage scheme that minimizes the storage cost. In this paper, we present a locality sensitive information brokerage scheme. It is designed for general locality-sensitive requirements, which include the linear data-retrieval cost (a frequently studied scenario) as a special case. We also prove that for a large class of networks, in the scenario of linear data-retrieval cost, our scheme achieves the asymptotically optimal storage cost. The result also proves the optimality of a few other schemes in the literature.
[data retrieval, Barium, wireless sensor networks, locality sensitive information brokerage, distributed sensor network, information retrieval, asymptotically optimal storage cost, Data structures, Distributed computing, rendezvous node, Boolean functions, mobile computing, Variable speed drives, Bismuth]
Mobile Filtering for Error-Bounded Data Collection in Sensor Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
In wireless sensor networks, filters, which suppress data update reports within predefined error bounds, effectively reduce the traffic volume for continuous data collection. All prior filter designs, however, are stationary in the sense that each filter is attached to a specific sensor node and remains stationary over its lifetime. In this paper, we propose mobile filter, a novel design that explores migration of filters to maximize overall traffic reduction. A mobile filter moves upstream along the data collection path, with its residual size being updated according to the collected data. Intuitively, this migration extracts and relays unused filters, leading to more proactive suppressing of update reports. We start by presenting an optimal filter migration algorithm for a chain topology. The algorithm is then extended to general multi-chain and tree topologies. Extensive simulations demonstrate that, for both synthetic and real data traces, the mobile filtering scheme significantly reduces data traffic and extends network lifetime against a state-of-the-art stationary filtering scheme.
[Base stations, error-bounded data collection, Filtering, wireless sensor networks, error bounds, Sensor Networks, filtering theory, data collection path, Mobile communication, Data Collection, Topology, Mobile Filter, optimal filter migration, general multichain topology, tree topology, Filtering algorithms, filter designs, mobile filtering, Distance measurement, data update, Gain, telecommunication traffic, continuous data collection, traffic reduction]
Data Estimation in Sensor Networks Using Physical and Statistical Methodologies
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Wireless sensor networks (WSNs) are employed in many applications in order to collect data. One key challenge is to minimize energy consumption to prolong network lifetime. A scheme of making some nodes asleep and estimating their values according to the other active nodespsila readings has been proved energy-efficient. For the purpose of improving the precision of estimation, we propose two powerful estimation models, data estimation using physical model (DEPM) and data estimation using statistical model (DESM). DEPM estimates the values of sleeping nodes by the physical characteristics of sensed attributes, while DESM estimates the values through the spatial and temporal correlations of the nodes. Experimental results on real sensor networks show that the proposed techniques provide accurate estimations and conserve energy efficiently.
[Base stations, estimation theory, wireless sensor networks, Heuristic algorithms, data estimation-statistical model, Estimation, wireless sensor network, spatial-temporal correlation, spatiotemporal phenomena, Equations, Light sources, prolong network lifetime, Wireless sensor networks, energy consumption minimization, data estimation-physical model, telecommunication network reliability, Data models, minimisation, statistical analysis, energy consumption, correlation methods]
Probing Queries in Wireless Sensor Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
For queries in wireless sensor networks, empty sets may be returned as query results which could confuse users a lot and users obtain no useful information about the monitored objects from the empty sets. To solve the problem, this paper proposes methods to provide users with approximate answer sets in the case where no sensing data satisfies the query conditions. The approximate answer sets can be used not only to answer the query approximately but also to guide users to modify their queries for further probing the monitored objects. The distance between sensing data and a query and the dominating relationship between sensing data are first defined. Then, three algorithms for processing probing queries are proposed, which compute the best approximate answer sets that consist of the sensing data with the smallest distance from given queries. All the algorithms utilize the dominating relationship to reduce the amount of data transmitted in sensor networks by filtering out the unnecessary data. Experimental results on real and synthetic data sets show that the proposed algorithms have high performance and energy efficiency.
[Algorithm design and analysis, data filtering, probing query, wireless sensor networks, wireless sensor network, distributed processing, information filtering, query processing, Wireless sensor networks, Clustering algorithms, Approximation algorithms, Distance measurement, probing queries, Sensors, Monitoring, approximate answer set, power efficiency]
Quantum-Adaptive Scheduling for Multi-Core Network Processors
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Efficiency and effectiveness are always the emphases of a scheduler, for both link and processor scheduling. Well-known scheduling algorithms such as surplus round robin (SRR) and elastic round robin (ERR) suffer from two fold shortcomings: 1) additional pre-processing queuing delay and post-processing resequencing delay are incurred due to the lack of short-term load-balancing; 2) bursty scheduling is caused due to blind preservation of scheduling history under non-backlogged traffic. In this paper, we propose a quantum-adaptive scheduling (QAS) algorithm, which: 1) synchronizes all the quanta in a fine-grained manner and, 2) adjusts the quanta intelligently based on processor utilization. We theoretically prove that the queuing fairness bound (QFB) for QAS is one third tighter than SRR and ERR. This result approaches the optimal value as obtained in shortest queue first (SQF) algorithm, while still maintaining O(1) complexity. Trace-driven simulations show that QAS reduces average packet delay by 18%~24% while cutting down the resequencing buffer size by more than 40% compared to SRR and ERR.
[queueing theory, shortest queue first algorithm, surplus round robin, elastic round robin, Scheduling, Complexity theory, multi-core network processors, queuing fairness bound, Scheduling algorithm, Delay, processor scheduling, Program processors, Processor scheduling, resource allocation, delays, quantum-adaptive scheduling, additional preprocessing queuing delay, short-term load-balancing, bursty scheduling, post-processing resequencing delay, Round robin, computational complexity]
SERvartuka: Dynamic Distribution of State to Improve SIP Server Scalability
2008 The 28th International Conference on Distributed Computing Systems
None
2008
A growing class of applications, including VoIP, IM and presence, are enabled by the session initiation protocol (SIP). Requests in SIP typically traverse through multiple proxies. The availability of multiple proxies offers the flexibility to distribute proxy functionality across several nodes. In particular, after experimentally demonstrating that the resource consumption of maintaining state is significant, we define the problem of state distribution across multiple nodes when the goal is to increase overall call throughput. We first formulate this as an optimization problem and then derive a distributed algorithm from it. This distributed algorithm leads to the design and evaluation of SERvartuka, a more scalable SIP server that dynamically determines the number of SIP requests for which the server is stateful while delegating state maintenance for the remainder of the requests to a server further downstream. This design is in contrast to existing SIP servers that are statically configured to either be stateless or stateful and therefore result in sub-optimal call throughput. We implement SERvartuka on top of OpenSER, a commercial SIP proxy server and measure performance benefits of different server configurations. An example of our results is a 20% percent increase in call throughput when using our algorithm for a configuration of two servers in series.
[server, Heuristic algorithms, LP, Maintenance engineering, VoIP, Throughput, Routing, Servers, Optimization, signalling protocols, proxy functionality, scalability, dynamic distribution, session initiation protocol, suboptimal call throughput, profiling, SIP server scalability, Authentication, dynamic, SIP, state, OpenSER, Internet telephony, authentication]
Fast Path Session Creation on Network Processors
2008 The 28th International Conference on Distributed Computing Systems
None
2008
The security gateways today are required not only to block unauthorized accesses by authenticating packet headers, but also by inspecting connection states to defend against malicious intrusions. Hence session creation rate plays a key role in determining the overall performance of stateful intrusion prevention systems. In this paper, we propose a high-speed session creation scheme optimized for network processors. Main contribution includes: a) A high-performance flow classification algorithm on network processors; b) An efficient TCP three-way handshake scheme designed for fast-path processing using a two-stage intelligent hashing. Experimental results show that: a) The presented parallel optimized flow classification algorithm, Parallel Search Cross-Producting, outperforms the original Cross-Producting and Binary Search Cross-Producting algorithms with 300% and 60% increase of classification speed; b) The proposed fast path three-way handshake scheme, IntelliHash, achieves a TCP connection creation rate over 2M connections per second.
[unauthorized accesses, Current transformers, packet headers authentication, Random access memory, two-stage intelligent hashing, TCP three-way handshake scheme, parallel optimized flow classification algorithm, Program processors, Computed tomography, Network Processor, IntelliHash, authorisation, Classification, Argon, network processors, parallel search cross-producting, malicious intrusions, stateful intrusion prevention systems, computer networks, cryptography, microprocessor chips, Circuit faults, path session creation, Fault currents, TCP connection, security gateways, transport protocols, message authentication, fast-path processing, Session]
Reconfigurable Real-Time Middleware for Distributed Cyber-Physical Systems with Aperiodic Events
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Different distributed cyber-physical systems must handle a periodic and periodic events with diverse requirements. While existing real-time middleware such as real-time CORBA has shown promise as a platform for distributed systems with time constraints, it lacks flexible configuration mechanisms needed to manage end-to-end timing easily for a wide range of different cyber-physical systems with both aperiodic and periodic events. The primary contribution of this work is the design, implementation and performance evaluation of the first configurable component middleware services for admission control and load balancing of a periodic and periodic event handling in distributed cyber-physical systems. Empirical results demonstrate the need for, and the effectiveness of, our configurable component middleware approach in supporting different applications with a periodic and periodic events, and providing a flexible software platform for distributed cyber-physical systems with end-to-end timing constraints.
[Real time systems, load balancing, admission control, Servers, Middleware, reconfigurable real-time middleware, distributed cyber-physical systems, Program processors, resource allocation, Admission control, aperiodic events, Load management, Software, distributed object management, real-time CORBA, middleware]
Availability and Fairness Support for Storage QoS Guarantee
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Multi-dimensional storage virtualization (MDSV) technology allows multiple virtual disks, each with a distinct combination of capacity, latency and bandwidth requirements, to be multiplexed on a physical disk storage system with performance isolation. This paper presents novel design and implementation techniques that solve the availability guarantee and fairness assurance problems in multi-dimensional storage virtualization. First, we show that a measurement-based admission control algorithm can reduce the effective resource requirement of a virtual disk with availability guarantee by accurately estimating its resource needs without prior knowledge of its input workload characteristics. Moreover, to accurately factor disk access overhead into real-time disk request scheduling algorithm, we propose a virtual disk switching overhead extraction and distribution algorithm that can derive the intrinsic disk access overhead associated with each virtual disk so as to achieve perfect performance isolation. Finally, we develop an adaptive server time leap-forward algorithm to effectively address the short-term unfairness problem of virtual clock-based disk scheduler, the only known proportional-share scheduler that is based on wall-clock time and thus enables disk utilization efficiency optimization while delivering disk QoS guarantees.
[disk QoS, Quality of service, Switches, latency, proportional-share scheduler, Servers, disk access overhead, resource allocation, storage QoS guarantee, disk QoS guarantee, Bandwidth, multidimensional storage virtualization, scheduling, disk utilization efficiency optimization, disk storage system, workload characteristics, storage virtualization, resource requirement, dual-queue disk scheduling, fairness support, Availability, virtual clock, adaptive server time leap-forward algorithm, virtual storage, multiple virtual disks, virtual clock-based disk scheduler, real-time disk request scheduling algorithm, measurement-based admission control, quality of service, virtual disk switching overhead extraction, Scheduling algorithm, resource needs, availability guarantee, wall-clock time, storage capacity, fairness assurance problem, virtual disk switching overhead, Arrays, bandwidth requirement, performance isolation]
Can We Really Recover Data if Storage Subsystem Fails?
2008 The 28th International Conference on Distributed Computing Systems
None
2008
This paper presents a theoretical and experimental study on the limitations of copy-on-write snapshots and incremental backups in terms of data recoverability. We provide mathematical proofs of our new findings as well as implementation experiments to show how data recovery is done in case of various failures. Based on our study, we propose a new system architecture that will overcome the problems of existing technologies. The new architecture can provide two-way data recovery capability with the same storage overheads and can be implemented fairly easily on existing systems. We show that the new architecture has maximum data recoverability and is practically feasible.
[Coupling Updates by Parities, Memory, copy-on-write snapshots, Servers, system recovery, Storage Systems, Equations, incremental backups, storage management, data recovery, Computer architecture, Production, Benchmark testing, Data Protection and Recovery, Software, storage system failure]
QoS Scheduling for Networked Storage System
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Networked storage incorporates networking technology and storage technology, greatly extending the reach of the storage subsystem. In this paper, we present a novel Quality of Service (QoS) scheduling scheme to satisfy the requirements of different QoS requests for access to the networked storage system. Our key ideas include breaking down the requests into appropriate chunks of smaller sizes and taking the network characteristics into consideration such that 1) each session channel has smoother data access, 2) resource requirements such as buffer usage are reduced, and 3) more urgent requests can preempt a less urgent request. Our experimental results show that our scheme is effective in obtaining these goals.
[Real time systems, Schedules, session channel, buffer storage, Electric breakdown, resource requirements, Quality of service, breakdown, quality of service, QoS scheduling scheme, buffer usage, Delay, networked storage system, storage area networks, resource allocation, networked storage, QoS, storage subsystem, Bandwidth, scheduling, storage technology, Arrays, networking technology, quality-of-service]
stdchk: A Checkpoint Storage System for Desktop Grid Computing
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Checkpointing is an indispensable technique to provide fault tolerance for long-running high-throughput applications like those running on desktop grids. This article argues that a checkpoint storage system, optimized to operate in these environments, can offer multiple benefits: reduce the load on a traditional file system, offer high-performance through specialization, and, finally, optimize data management by taking into account checkpoint application semantics. Such a storage system can present a unifying abstraction to checkpoint operations, while hiding the fact that there are no dedicated resources to store the checkpoint data. We prototype stdchk, a checkpoint storage system that uses scavenged disk space from participating desktops to build a low-cost storage system, offering a traditional file system interface for easy integration with applications. This article presents the stdchk architecture, key performance optimizations, and its support for incremental checkpointing and increased data availability. Our evaluation confirms that the stdchk approach is viable in a desktop grid setting and offers a low cost storage system with desirable performance characteristics: high write throughput as well as reduced storage space and network effort to save checkpoint images.
[Checkpointing, Availability, checkpointing, desktop grid computing, fault tolerance, data management, Desktop Grids, traditional file system, scavenged disk space, grid computing, checkpoint storage system, Throughput, stdchk, Storage Systems, software fault tolerance, storage management, File systems, Aggregates, Reliability, Transient analysis]
Challenges and Future Research Directions in Large-Scale Complex Systems
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Summary form only given. Advances in networking and wireless technologies, coupled with integration of information networks and social applications, bring about unprecedented challenges, technical and non-technical, underscoring the need for insight into the capabilities of our complex distributed and networked systems. As critical as these physical and cyber infrastructures are to our lives and diverse sectors of our society, we have little rigorous knowledge for understanding their structure and dynamics. The talk will discuss challenges and future directions in how to effectively design robust and secure large-scale complex systems, so that we can engineer them to have predictable behaviors.
[wireless technologies, large-scale complex systems, Computational modeling, Conferences, distributed processing, Ad hoc networks, networked systems, information networks, large-scale systems, Wireless communication, Computer science, predictable behaviors, complex distributed systems, Editorials, Mobile computing]
Routing of XML and XPath Queries in Data Dissemination Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
XML-based data dissemination networks are rapidly gaining momentum. In these networks XML content is routed from data producers to data consumers throughout an overlay network of content-based routers. Routing decisions are based on XPath expressions (XPEs) stored at each router. To enable efficient routing, while keeping the routing state small, we introduce an advertisement-based routing algorithm for XML content, present a novel data structure for managing XPEs, especially apt for the hierarchical nature of XPEs and XML, and develop several optimizations for reducing the number of XPEs required to manage the routing state. The experimental evaluation shows that our algorithms and optimizations reduce the routing table size by up to 90%, improve the routing time by roughly 85%, and reduce overall network traffic by about 35%. Experiments running on PlanetLab show the scalability of our approach.
[XML queries routing, data dissemination networks, advertisement-based routing algorithm, Data Dissemination Networks, Corporate acquisitions, Subscriptions, Merging, overlay network, data structure, Routing, Data structures, Covering, Publish/Subscribe, content-based routers, PlanetLab, Optimization, query processing, optimisation, XML, data structures, XPath expressions, XPath queries routing]
Multi-query Optimization for Distributed Similarity Query Processing
2008 The 28th International Conference on Distributed Computing Systems
None
2008
This paper considers a multi-query optimization issue for distributed similarity query processing, which attempts to exploit the dependencies in the derivation of a query evaluation plan. To the best of our knowledge, this is the first work investigating a multi- query optimization technique for distributed similarity query processing (MDSQ). Four steps are incorporated in our MDSQ algorithm. First when a number of query requests(i.e., m query vectors and m radiuses) are simultaneously submitted by users, then a cost-based dynamic query scheduling(DQS) procedure is invoked to quickly and effectively identify the correlation among the query spheres (requests). After that, an index-based vector set reduction is performed at data node level in parallel. Finally, a refinement process of the candidate vectors is conducted to get the answer set. The proposed method includes a cost-based dynamic query scheduling, a Start-Distance(SD)-based load balancing scheme, and an index-based vector set reduction algorithm. The experimental results validate the efficiency and effectiveness of the algorithm in minimizing the response time and increasing the parallelism of I/O and CPU.
[Heuristic algorithms, cost-based dynamic query scheduling, query spheres, CPU parallelism, multiquery optimization, response time minimization, I/O parallelism, Optimization, query processing, start-distance-based load balancing, database indexing, resource allocation, index-based vector set reduction, distributed databases, scheduling, data node level, Dynamic scheduling, query evaluation, query vectors, distributed similarity query processing, Query processing, query request, Load management, Distance measurement, candidate vector refinement process, Indexing]
Towards Multi-Site Collaboration in 3D Tele-Immersive Environments
2008 The 28th International Conference on Distributed Computing Systems
None
2008
3D tele-immersion (3DTI) has recently emerged as a new way of video-mediated collaboration across the Internet. Unlike conventional 2D video-conferencing systems, it can immerse remote users into a shared 3D virtual space so that they can interact or collaborate "virtually". However, most existing 3DTI systems can support only two sites of collaboration, due to the huge demand of networking resources and the lack of a simple yet efficient data dissemination model. In this paper, we propose to use a general publish-subscribe model for multi-site 3DTI systems, which efficiently utilizes limited network resources by leveraging user interest. We focus on the overlay construction problem in the publish-subscribe model by exploring a spectrum of heuristic algorithms for data dissemination. With extensive simulation, we identify the advantages of a simple randomized algorithm. We present optimization to further improve the randomized algorithm by exploiting semantic correlation. Experimental results demonstrate that we can achieve an improvement by a factor of five.
[message passing, Economic indicators, Subscriptions, randomized algorithm, data dissemination model, teleconferencing, Construction industry, randomised algorithms, 2D video-conferencing system, overlay construction problem, heuristic algorithm, multisite video-mediated collaboration, Publish-subscribe, Bandwidth, groupware, Cameras, Three dimensional displays, Internet, 3D tele-immersive environment, shared 3D virtual space, publish-subscribe model, middleware]
Fair K Mutual Exclusion Algorithm for Peer to Peer Systems
2008 The 28th International Conference on Distributed Computing Systems
None
2008
k-mutual exclusion is an important problem for resource-intensive peer-to-peer applications ranging from aggregation to file downloads. In order to be practically useful, k-mutual exclusion algorithms not only need to be safe and live, but they also need to be fair across hosts. We propose a new solution to the k-mutual exclusion problem that provides a notion of time-based fairness. Specifically, our algorithm attempts to minimize the spread of access time for the critical resource. While a client's access time is the time between it requesting and accessing the resource, the spread is defined as a system-wide metric that measures some notion of the variance of access times across a homogeneous host population, e.g., difference between max and mean. We analytically prove the correctness of our algorithm, and evaluate its fairness experimentally using simulations. Our evaluation under two settings - a LAN setting and a WAN based on the King latency data set - shows even with 100 hosts accessing one resource, the spread of access time is within 15 seconds.
[Pediatrics, access time minimization, peer-to-peer computing, mutual exlusion, Radiation detectors, Heuristic algorithms, peer to peer systems, resource-intensive peer-to-peer system, Classification algorithms, Servers, resource allocation, access time variance, Motion pictures, Safety, k-mutual exclusion algorithm, homogeneous host population, computational complexity]
Conditions for Set Agreement with an Application to Synchronous Systems
2008 The 28th International Conference on Distributed Computing Systems
None
2008
The k-set agreement problem is a generalization of the consensus problem: considering a system made up of n processes where each process proposes a value, each non-faulty process has to decide a value such that a decided value is a proposed value, and no more than k different values are decided. While this problem cannot be solved in an asynchronous system prone to t process crashes when tgesk, it can always be solved in a synchronous system; lfloort/krfloor+1 is then lower bound on the number of rounds (consecutive communication steps) for the non-faulty processes to decide. The condition-based approach has been introduced in the consensus context. Its aim was to both circumvent the consensus impossibility in asynchronous systems, and allow for more efficient consensus algorithms in synchronous systems. This paper addresses the condition-based approach in the context of the k-set agreement problem.
[Algorithm design and analysis, Hamming distance, Law, distributed processing, consensus algorithm, Computer crashes, Encoding, consensus problem, consecutive communication steps, synchronous systems, Detectors, Distance measurement, k-set agreement problem, nonfaulty processes]
Distributed Connected Dominating Set Construction in Geometric k-Disk Graphs
2008 The 28th International Conference on Distributed Computing Systems
None
2008
In this paper, we study the problem of minimum connected dominating set in geometric k-disk graphs. This research is motivated by the problem of virtual backbone construction in wireless ad hoc and sensor networks, where the coverage area of nodes are disks with different radii. We derive the size relationship of any maximal independent set and the minimum connected dominating set in geometric k-disk graphs, and apply it to analyze the performances of two distributed connected dominating set algorithms we propose in this paper. These algorithms have a bounded performance ratio and low communication overhead, and therefore have the potential to be applied in real ad hoc and sensor networks.
[Algorithm design and analysis, graph theory, Optimized production technology, Color, maximal independent set, computational geometry, performance ratio, set theory, Approximation methods, Construction industry, distributed algorithms, distributed connected dominating set algorithm, connected dominating set, Lead, Approximation algorithms, geometric k-disk graph]
Weak vs. Self vs. Probabilistic Stabilization
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Self-stabilization is a strong property which guarantees that a network always resume a correct behavior starting from an arbitrary initial state. Weaker guarantees have later been introduced to cope with impossibility results: probabilistic stabilization only gives probabilistic convergence to a correct behavior. Also, weak-stabilization only gives the possibility of convergence. In this paper, we investigate the relative power of weak, self, and probabilistic stabilization, with respect to the set of problems that can be solved. We formally prove that in that sense, weak stabilization is strictly stronger that self-stabilization. Also, we refine previous results on weak stabilization to prove that, for practical schedule instances, a deterministic weak-stabilizing protocol can be turned into a probabilistic self-stabilizing one. This latter result hints at more practical use of weak-stabilization, as such algorithms are easier to design and prove than their (probabilistic) self-stabilizing counterparts.
[Algorithm design and analysis, Pediatrics, Protocols, fault-tolerance, self-stabilization, convergence, probabilistic self-stabilization, probabilistic stabilization, Lead, scheduling, probabilistic weak-stabilization, fault tolerance, deterministic weak-stabilizing algorithm, Nominations and elections, probability, Probabilistic logic, distributed randomized scheduling algorithm, weak-stabilization, deterministic algorithms, randomised algorithms, distributed algorithms, fault tolerant computing, probabilistic convergence, probabilistic self-stabilizing algorithm, Manganese]
WebIBC: Identity Based Cryptography for Client Side Security in Web Applications
2008 The 28th International Conference on Distributed Computing Systems
None
2008
The growing popularity of web applications in the last few years has led users to give the management of their data to online application providers, which will endanger the security and privacy of the users. In this paper, we present WebIBC, which integrates public key cryptography into web applications without any browser plugins. The public key of WebIBC is provided by identity based cryptography, eliminating the need of public key and certificate online retrieval; the private key is supplied by the fragment identifier of the URL inspired by BeamAuth. The implementation and performance evaluation demonstrate that WebIBC is secure and efficient both in theory and practice.
[WebIBC, Electronic mail, Servers, Security, security, Elliptic curves, public key cryptography, web, Public key, client side security, Elliptic curve cryptography, Internet, identity based cryptography, Cryptography]
J2EE Architecture for Database Cluster-Based High Volume E-Commerce Web Applications
2008 The 28th International Conference on Distributed Computing Systems
None
2008
High volume database-driven e-commerce applications demand a cluster-based infrastructure to offers high availability, scalability and fault tolerance. The current J2EE architecture and containers restrict the transparent deployment of applications over database clusters without engineering data access logic into the applications. Our work extends the J2EE architecture to allow transparent deployment of J2EE applications on a database cluster. The key challenge is to load balance read and write queries between the master and replica database instance and yet provide the application with the most recent data in the cluster while enabling service class based query routing. We validate the applicability and effectiveness of the proposed architecture using IBM WebSphere Trade3 stock trading application.
[Java, fault tolerance, replicated databases, J2EE architecture, query routing, Quality of service, IBM WebSphere Trade3 stock trading, Routing, database-driven e-commerce, Servers, Synchronization, replica database, query processing, cluster-based infrastructure, Databases, e-commerce Web applications, Database systems, Internet, engineering data access logic, Driver circuits, electronic commerce, database cluster]
Online Measurement of the Capacity of Multi-Tier Websites Using Hardware Performance Counters
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Understanding server capacity is crucial for system capacity planning, configuration, and QoS-aware resource management. Conventional stress testing approaches measure the server capacity in terms of application-level performance metrics like response time and throughput. They are limited in measurement accuracy and timeliness. In a multitier website, resource bottleneck often shifts between tiers as client access pattern changes. This makes the capacity measurement even more challenging. This paper presents a measurement approach based on hardware performance counter metrics. The approach uses machine learning techniques to infer application-level performance at each tier. A coordinated predictor is induced over individual tier models to estimate system-wide performance and identify the bottleneck when the system becomes overloaded. Experimental results demonstrate that this approach is able to achieve an overload prediction accuracy of higher than 90% for a priori known input traffic patterns and over 85% accuracy even for traffic causing frequent bottleneck shifting. It costs less than 0.5% runtime overhead for data collection and no more than 50 ms for each on-line decision.
[resource management, hardware performance counters, online measurement, Radiation detectors, quality of service, Servers, machine learning, Training, Accuracy, Runtime, system capacity planning, QoS, system capacity configuration, Hardware, Time factors, learning (artificial intelligence), Web sites, traffic patterns, software performance evaluation, telecommunication traffic]
Heuristics-Based Strategies for Resolving Context Inconsistencies in Pervasive Computing Applications
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Context-awareness allows pervasive applications to adapt to changeable computing environments. Contexts, the pieces of information that capture the characteristics of environments, are often error-prone and inconsistent due to noises. Various strategies have been proposed to enable automatic context inconsistency resolution. They are formulated on different assumptions that may not hold in practice. This causes applications to be less context-aware to different extents. In this paper, we investigate such impacts and propose our new resolution strategy. We conducted experiments to compare our work with major existing strategies. The results showed that our strategy is both effective in resolving context inconsistencies and promising in its support of applications using contexts.
[Pervasive computing, Radiation detectors, Reliability theory, Proposals, ubiquitous computing, pervasive computing, Middleware, context-awareness, Integrated circuits, heuristics-based strategies, context inconsistency resolution, context inconsistency, Radiofrequency identification]
MC2: Multiple Clients on a Multilevel Cache
2008 The 28th International Conference on Distributed Computing Systems
None
2008
In today's networked storage environment, it is common to have a hierarchy of caches where the lower levels of the hierarchy are accessed by multiple clients. This sharing can have both positive or negative effects. While data fetched by one client can be used by another client without incurring additional delays, clients competing for cache buffers can evict each other's blocks and interfere with exclusive caching schemes. Our algorithm, MC2, combines local, per client management with a global, system-wide, scheme, to emphasize the positive effects of sharing and reduce the negative ones. The local scheme uses readily available information about the client's future access profile to save the most valuable blocks, and to choose the best replacement policy for them. The global scheme uses the same information to divide the shared cache space between clients, and to manage this space. Exclusive caching is maintained for non-shared data and is disabled when sharing is identified. Our simulation results show that the combined algorithm significantly reduces the overall I/O response times of the system.
[cache space sharing, Cache memories, per client management, cache storage, Delay, client access profile, multiple clients, resource allocation, File systems, networked storage environment, I/O response time, cache buffer, Benchmark testing, Buffers, client-server systems, cache hierarchy, replacement policy, Partitioning algorithms, Indexes, multilevel cache, Memory management, data fetching, Resource management, Time factors, Gain]
On Cooperative Caching in Wireless P2P Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Some recent studies have shown that cooperative cache can improve the system performance in wireless P2P networks such as ad hoc networks and mesh networks. However, all these studies are at a very high level, leaving many design and implementation issues unanswered. In this paper, we present our design and implementation of cooperative cache in wireless P2P networks. We propose a novel asymmetric cooperative cache approach, where the data requests are transmitted to the cache layer on every node, but the data replies are only transmitted to the cache layer at the intermediate nodes that need to cache the data. This solution not only reduces the overhead of copying data between the user space and the kernel space, it also allows data pipelines to reduce the end-to-end delay. We also study the effects of different MAC layers such as 802.11 based ad hoc networks and multi-interface multi-channel based mesh networks, on the performance of cooperative cache. Our results show that the asymmetric approach outperforms the symmetric approach in traditional 802.11 based ad hoc networks by removing most of the processing overhead. In mesh networks, the asymmetric approach can significantly reduce the data access delay compared to the symmetric approach due to data pipelines.
[radio networks, Pipelines, cache storage, access protocols, Wireless communication, kernel space, wireless P2P networks, end-to-end delay, mesh networks, System performance, multiinterface multichannel based mesh networks, Routing protocols, 802.11 based ad hoc networks, MAC layers, Kernel, cache, peer-to-peer computing, Routing, wireless networks, Ad hoc networks, asymmetric cooperative cache approach, delays, cooperative caching, ad hoc networks, user space]
PFC: Transparent Optimization of Existing Prefetching Strategies for Multi-Level Storage Systems
2008 The 28th International Conference on Distributed Computing Systems
None
2008
The multi-level storage architecture has been widely adopted in servers and data centers. However, while prefetching has been shown as a crucial technique to exploit the sequentiality in accesses common for such systems and hide the increasing relative cost of disk I/O, existing multi-level storage studies have focused mostly on cache replacement strategies. In this paper, we show that prefetching algorithms designed for single-level systems may have their limitations magnified when applied to multi-level systems. Overly conservative prefetching will not be able to effectively use the lower-level cache space, while overly aggressive prefetching will be compounded across levels and generate large amounts of wasted prefetch. We take an innovative approach to this problem: rather than designing a new, multi-level prefetching algorithm, we developed prefetching-coordinator (PFC), a hierarchy-aware optimization applicable to any existing prefetching algorithms. PFC does not require any application hints, a priori knowledge on the application access pattern or the native prefetching algorithm, or modification to the I/O interface. Instead, it monitors the upper-level access patterns as well as the lower-level cache status, and dynamically adjusts the aggressiveness of the lower-level prefetching activities. We evaluated PFC with extensive simulation study using a verified multi-level storage simulator, an accurate disk simulator, and access traces with different access patterns. Our results indicate that PFC dynamically controls lower-level prefetching in reaction to multiple system and workload parameters, improving the overall system performance in all 96 test cases. Working with four well-known existing prefetching algorithms adopted in real systems, PFC obtains an improvement of up to 35% to the average request response time, with an average improvement of 14.6% over all cases.
[Algorithm design and analysis, Prefetching, Heuristic algorithms, prefetching-coordinator, multilevel storage architecture, access pattern, Storage system, cache storage, cache status, Collaborative caching, Servers, multilevel storage simulator, Optimization, hierarchy-aware optimization, File system, Linux, disk simulator, Distance measurement]
Online Optimization for Latency Assignment in Distributed Real-Time Systems
2008 The 28th International Conference on Distributed Computing Systems
None
2008
As distributed real-time applications gain in popularity, a key challenge is to allocate resources so that diverse real-time requirements (including non-real-time applications), distributed application components and varying workloads can all be accommodated without violating timeliness constraints. We examine the problem of resource allocation in distributed soft real-time systems, where both network and CPU resources are consumed. The timeliness constraints of applications are expressed through utility functions, which compute "benefit" as a function of end-to-end latency. We present LLA (Lagrangian Latency Assignment), a scalable and efficient distributed algorithm which maximizes aggregate utility by computing an optimal trade-off between end-to-end latency and allocated resources. The algorithm runs continuously and adapts to both workload and resource variations. LLA is guaranteed to converge if the workload and resource requirements stabilize. We evaluate the quality of results and convergence characteristics under various workloads, using both simulation and real-world experimentation.
[Real time systems, Computational modeling, distributed soft real-time system, utility functions, latency assignment, Optimization, Equations, Convergence, distributed real-time application, distributed algorithm, resource allocation, online optimization, real-time, distributed algorithms, CPU resources, optimization, real-time systems, Lagrangian latency assignment, Bandwidth, distributed systems, Resource management, end-to-end latency]
Resource Bundles: Using Aggregation for Statistical Wide-Area Resource Discovery and Allocation
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Resource discovery is an important process for finding suitable nodes that satisfy application requirements in large loosely-coupled distributed systems. Besides inter-node heterogeneity, many of these systems also show a high degree of intra-node dynamism, so that selecting nodes based only on their recently observed resource capacities for scalability reasons can lead to poor deployment decisions resulting in application failures or migration overheads. In this paper, we propose the notion of a resource bundle - a representative resource usage distribution for a group of nodes with similar resource usage patterns - that employs two complementary techniques to overcome the limitations of existing techniques: resource usage histograms to provide statistical guarantees for resource capacities, and clustering-based resource aggregation to achieve scalability. Using trace-driven simulations and data analysis of a month-long Planet Lab trace, we show that resource bundles are able to provide high accuracy for statistical resource discovery (up to 56% better precision than using only recent values), while achieving high scalability (up to 55% fewer messages than a non-aggregation algorithm). We also show that resource bundles are ideally suited for identifying group-level characteristics such as finding load hot spots and estimating total group capacity (within 8% of actual values).
[Algorithm design and analysis, resource usage pattern, Data analysis, data analysis, resource discovery, Scalability, inter-node heterogeneity, statistical wide-area resource allocation, distributed processing, loosely-coupled distributed system, clustering-based resource aggregation, statistical distributions, representative resource usage distribution, trace-driven simulation, month-long Planet Lab trace, Histograms, Analytical models, Accuracy, resource bundle, resource allocation, Clustering algorithms, statistical wide-area resource discovery, intra-node dynamism, resource usage histogram]
Autotuning Configurations in Distributed Systems for Performance Improvements Using Evolutionary Strategies
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Distributed systems usually have many configurable parameters such as those included in common configuration files. Performance of distributed systems is partially dependent on these system configurations. While operators may choose default settings or manually tune parameters based on their experience and intuition, the resulted settings may not be the optimal one for specific services running on the distributed system. In this paper, we formulate the problem of autotuning configurations as a black-box optimization problem. This problem becomes quite challenging since the joint parameter search space is huge and also no explicit relationship between performance and configurations exists. We propose to use a well known evolutionary algorithm called covariance matrix adaptation (CMA) to automatically tune system parameters. We compare CMA algorithm to another existing techniques called smart hill climbing (SHC) and demonstrate that CMA algorithm outperforms SHC algorithm both on synthetic data and in a real system.
[covariance matrices, System Configuration, Covariance Matrix Update, Heuristic algorithms, covariance matrix adaptation, distributed processing, Throughput, Covariance matrix, black-box optimization problem, Servers, Optimization, evolutionary algorithm, Heuristic Methods, evolutionary computation, Automatic Tuning, Evolution (biology), autotuning configurations, distributed systems, Evolutionary Strategies, Time factors, performance improvements]
Accessibility-Based Resource Selection in Loosely-Coupled Distributed Systems
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Large-scale distributed systems provide an attractive scalable infrastructure for network applications. However,the loosely-coupled nature of this environment can make data access unpredictable, and in the limit, unavailable. We introduce the notion of accessibility to capture both availability and performance. An increasing number of data intensive applications require not only considerations of node computation power but also accessibility for adequate job allocations. For instance, selecting a node with intolerably slow connections can offset any benefit to running on a fast node. In this paper, we present accessibility-aware resource selection techniques by which it is possible to choose nodes that will have efficient data access to remote data sources. We show that the local data access observations collected from a node's neighbors are sufficient to characterize accessibility for that node. We then present resource selection heuristics guided by this principle, and show that they significantly out perform standard techniques. The suggested techniques are also shown to be stable even under churn despite the loss of prior observations.
[Availability, accessibility-aware resource selection techniques, Correlation, Peer to peer computing, Data accessibility, Estimation, Resource selection, Throughput, local data access observations, Servers, large-scale distributed systems, large-scale systems, accessibility-based resource selection, loosely-coupled distributed systems, Distance measurement, Peer-to-peer computing, data intensive applications, Network performance]
Fully Adaptive Power Saving Protocols for Ad Hoc Networks Using the Hyper Quorum System
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Quorum-based power saving (QPS) protocols have been proposed for ad hoc networks (e.g., IEEE 802.11 ad hoc mode) to increase energy efficiency and prolong the operational time of mobile stations. These protocols assign to each station a cycle pattern that specifies when the station should wake up (to transmit/receive data) and sleep (to save battery power). In all existing QPS protocols, the cycle length is either identical for all stations or is restricted to certain numbers (e.g. squares or primes). These restrictions on cycle length severely limit the practical use of QPS protocols as each individual station may want to select a cycle length that is best suited for its own need (in terms of remaining battery power, tolerable packet delay, and drop ratio). In this paper we propose the notion of hyper quorum system (HQS)-a generalization of QPS that allows for arbitrary cycle lengths. We describe algorithms to generate two different classes of HQS given any set of arbitrary cycle lengths as input. We then present analytical and simulation results that show the benefits of HQS-based power saving protocols over the existing QPS protocols.
[Schedules, Protocols, mobile radio, IEEE 802.11 Standards, telecommunication network management, HQS-based power management protocols, Ad hoc networks, Batteries, Delay, mobile stations, fully adaptive power saving protocols, hyper quorum system, QPS protocols, quorum-based power saving protocols, cycle pattern, Data communication, ad hoc networks, protocols]
Hotness-Aware Sensor Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
In a realistic sensor network, in particular with a non-uniform deployment, sensor nodes inevitably have varying workloads. This causes a natural problem that some sensor nodes are subject to excessive power consumption and thus become hot. These hot nodes deplete much earlier resulting in system performance degradation. This paper proposes a systematic approach to design a hotness-aware sensor network where each node is able to obtain its own hotness information. Based on these vital information, the system is able to provide various technologies to protect the critical set of hot nodes. More specifically, we design a centralized optimal algorithm to derive the precise hotness of each node. In addition, we develop a completely distributed algorithm to estimate hotness with high accuracy. An effective hotness-aware MAC is developed to offer medium access priority to the nodes with higher hotness to protect and prolong their lifetimes. It is demonstrated, through both theoretical analysis and comprehensive simulations, that our approach is valuable to improving system performance of practical sensor networks.
[Algorithm design and analysis, wireless sensor networks, nonuniform deployment, hot nodes problem, hotness-aware, Routing, Complexity theory, Topology, access protocols, medium access priority, power consumption, Wireless sensor network, Analytical models, sensor nodes, distributed algorithm, Accuracy, centralized optimal algorithm, distributed algorithms, hotness-aware sensor networks, system performance degradation, realistic sensor network, hotness-aware MAC, MAC protocol, Media Access Protocol]
Improving Energy Conservation Using Bulk Transmission over High-Power Radios in Sensor Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Low power radios, such as the CC2420, have been widely popular with recent sensor platforms. This paper explores the potential for energy savings from adding a high-power, high-bandwidth radio to current sensor platforms. High-bandwidth radios consume more power but significantly reduce the time for transmissions. Consequently, they offer net savings in total communication energy when there is enough data to offset wake-up energy overhead. The analysis on energy characteristics of several IEEE 802.11 radios show that a feasible crossover point exists (in terms of data size) after which energy savings are possible. Based on this analysis, we present a bulk data transmission protocol for dual radio systems. The results of simulations and prototype implementation show significant energy savings at the expense of introducing acceptable delay.
[Energy consumption, Protocols, dual-radio, IEEE 802.11 Standards, wireless sensor networks, high-power high-bandwidth radio transmission, Receivers, wireless sensor network, Routing, energy saving, prototype, simulations, bandwidth allocation, energy conservation, Sensor networks, Communication cables, IEEE 802.11 radio, bulk data transmission protocol, Data communication, protocols, wireless LAN, dual radio system]
Energy and Timing Constrained System Reward Maximization on Wireless Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Power efficiency is an important design issue in embedded devices with limited power supplies. In this paper, we study a reward-based packet scheduling problem in wireless environments. We consider a general scenario in which a transmitter communicates with multiple receivers periodically. To guarantee timely transmission of data, each packet is associated with a delay constraint. The periodic data streams have different importance levels, power functions, and levels of data sizes. The more data a transmitter delivers, the more rewards it obtains. Our objective is to develop schemes that selectively transmit data streams of different data sizes at different transmission rates to maximize the system reward under given time and energy constraints. We show that this problem is NP-hard and develop a dynamic programming algorithm for the optimal solution in pseudo-polynomial time. A fast polynomial-time heuristic approach is presented to achieve close approximation. Simulation results demonstrate that the proposed heuristic approach can achieve a significant improvement over other approaches adapted from existing studies at a small runtime overhead.
[Energy consumption, timing constrained system, Heuristic algorithms, Receivers, dynamic programming, wireless networks, energy constrained system, Delay, radio access networks, Wireless communication, Wireless sensor networks, pseudo-polynomial time, Transmitters, NP-hard problem, receivers, embedded systems, embedded devices, data communication, packet scheduling, reward maximization, data transmitter, computational complexity, power efficiency, multiple receivers]
Game Theoretic Peer Selection for Resilient Peer-to-Peer Media Streaming Systems
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Peer-to-peer (P2P) media streaming quickly emerges as an important application over the Internet. A plethora of approaches have been suggested and implemented to support P2P media streaming. In our study, we first classified existing approaches and studied their characteristics by looking at three important quantities: number of upstream peers (parents), number of downstream peers (children) and average number of links per peer. We find that in existing approaches, peers are assigned with a fixed number of parents without regard to their contributions, measured by the amount of outgoing bandwidths. Obviously, this is an undesirable arrangement as it leads to highly inefficient use of the P2P links. This observation motivates us to model the peer selection process as a cooperative game among peers. This results in a novel peer selection protocol such that the number of upstream peers of a peer is related to its outgoing bandwidth. Specifically, peers with larger outgoing bandwidth are given more parents, which makes them less vulnerable to peer dynamics. Simulation results show that the proposed protocol improves delivery ratio with similar number of links per peer, comparing with existing approaches in a wide range of settings.
[Pediatrics, Protocols, peer-to-peer computing, Peer to peer computing, game theory, Media, Servers, peer selection protocol, incentives, Games, Bandwidth, media streaming, resilient peer-to-peer media streaming system, coalition, Internet, protocols, cooperative game theory, P2P media streaming]
Toward Predictive Failure Management for Distributed Stream Processing Systems
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Distributed stream processing systems (DSPSs) have many important applications such as sensor data analysis, network security, and business intelligence. Failure management is essential for DSPSs that often require highly-available system operations. In this paper, we explore a new predictive failure management approach that employs online failure prediction to achieve more efficient failure management than previous reactive or proactive failure management approaches. We employ light-weight stream-based classification methods to perform online failure forecast. Based on the prediction results, the system can take differentiated failure preventions on abnormal components only. Our failure prediction model is tunable, which can achieve a desired tradeoff between failure penalty reduction and prevention cost based on a user-defined reward function. To achieve low-overhead online learning, we propose adaptive data stream sampling schemes to adaptively adjust measurement sampling rates based on the states of monitored components, and maintain a limited size of historical training data using reservoir sampling. We have implemented an initial prototype of the predictive failure management framework within the IBM System S distributed stream processing system. Experiment results show that our system can achieve more efficient failure management than conventional reactive and proactive approaches, while imposing low overhead to the DSPS.
[light-weight stream-based classification methods, IBM System S distributed stream processing systems, Predictive models, distributed processing, Data Stream Processing, Classification algorithms, business intelligence, adaptive data stream sampling schemes, query processing, failure penalty reduction, failure penalty prevention, continuous query processing, Decision trees, System Mining, Monitoring, measurement sampling rates, sampling methods, sensor data analysis, network security, reservoir sampling, online failure prediction, predictive failure management, Failure Prediction, Computer bugs, Streaming media, Software, fault tolerant computing, Fault Tolerance]
Crystal: An Emulation Framework for Practical Peer-to-Peer Multimedia Streaming Systems
2008 The 28th International Conference on Distributed Computing Systems
None
2008
To rapidly evolve new designs of peer-to-peer (P2P) multimedia streaming systems, it is highly desirable to test and troubleshoot them in a controlled and repeatable experimental environment in a local cluster of servers, as it is risky to integrate untested protocols in live production and mission-critical peer-to-peer sessions, such as live P2P streaming. Though it is possible to construct such controlled experiments with virtual machine monitors, there are a number of challenges and roadblocks: (1) The deployment of such resource-hungry virtual machine environments are complicated and time-consuming for researchers without prior systems expertise; (2) The system designer needs to implement many basic streaming elements, such as playback buffers and message switches. In this paper, we seek to address these challenges by introducing Crystal, an emulation framework for practical P2P multimedia streaming systems, which provides support for developing, testing, and troubleshooting new streaming system designs in a controlled server cluster environment. It is our imperative design objective that Crystal offers ease of use, rapid experimental turnaround, and the capability of emulating realistic P2P environments.
[Algorithm design and analysis, playback buffer, Peer-to-peer multimedia streaming, Protocols, peer-to-peer computing, Peer to peer computing, live P2P streaming, Crystals, multimedia systems, P2P environment, Servers, Engines, peer-to-peer multimedia streaming systems, server cluster, P2P system, resource-hungry virtual machine environment, streaming system design, Bandwidth, media streaming, development toolkit, Crystal emulation framework, message switching, emulation framework, mission-critical peer-to-peer sessions]
Video Distribution Under Multiple Constraints
2008 The 28th International Conference on Distributed Computing Systems
None
2008
We consider the optimization problem of providing a set of video streams to a set of clients, where each stream has costs in m possible measures (such as communication bandwidth, processing bandwidth etc.), and each client has its own utility function for each stream. We assume that the server has a budget cap on each of the m cost measures; each client has an upper bound on the utility that can be derived from it, and potentially also upper bounds in each of the m cost measures. The task is to choose which streams the server will provide, and out of this set, which streams each client will receive. The goal is to maximize the overall utility subject to the budget constraints. We give an efficient approximation algorithm with approximation factor of O(m) with respect to the optimal possible utility for any input, assuming that clients have only a bound on their maximal utility. If, in addition, each client has at most mc capacity constraints, then the approximation factor increases by another factor of O(m<sub>c</sub> log n), where n is the input length. We also consider the special case of "small" streams, namely where each stream has cost of at most O(1/ log n) fraction of the budget cap, in each measure. For this case we present an algorithm whose approximation ratio is O(log n).
[Algorithm design and analysis, Greedy algorithms, video distribution, approximation ratio, approximation theory, small streams, Optimized production technology, Resource allocation, capacity constraints, Scheduling, Servers, Approximation methods, approximation factor, multiple constraints, Bandwidth, Approximation algorithms, Budgeted set cover, video streaming]
Bandwidth Allocation for Elastic Real-Time Flows in Multihop Wireless Networks Based on Network Utility Maximization
2008 The 28th International Conference on Distributed Computing Systems
None
2008
In this paper, we consider distributed utility maximizing rate allocation in cyber-physical multihop wireless networks carrying prioritized elastic flows with different end-to-end delay requirements. This scenario arises in military wireless networks (dominated by audio and video flows) that must satisfy end-to-end deadlines. Due to the inherent difficulty in providing hard guarantees in such wireless environments, the problem is cast as one of utility maximization, where utility depends on meeting deadlines. Based on a recent result in real-time scheduling, we relate end-to-end delay of prioritized flows to flow rates and priorities, then impose end-to-end delay constraints that can be expressed in a decentralized manner in terms of flow information available locally at each node. The problem of utility maximization in the presence of these constraints is formulated, where utility depends on the ability to meet deadlines. The solution to the network utility maximization (NUM) problem yields a distributed rate control algorithm that nodes can independently execute to collectively maximize global network utility, taking into account delay constraints. Results from simulations demonstrate that a low deadline miss ratio is achieved for real-time packets, without significantly impacting throughput, resulting in a higher total utility compared to a previous state-of-the-art approach.
[Real time systems, radio networks, network utility maximization problem, Routing, Throughput, Delay, Wireless communication, bandwidth allocation, Wireless sensor networks, computer network management, end-to-end delay, distributed rate control algorithm, Wireless networks, delays, cyber-physical multihop wireless networks, military communication, scheduling, real-time scheduling, military wireless networks]
PIRD: P2P-Based Intelligent Resource Discovery in Internet-Based Distributed Systems
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Internet-based distributed systems enable globally scattered resources to be collectively pooled and used in a cooperative manner to achieve unprecedented petascale super computing capabilities. Numerous resource discovery approaches have been proposed to help achieve this goal. To report or discover a multi-attribute resource, most approaches use multiple messages with each message for an attribute, leading to high overhead. Anther approach can reduce multi-attribute to one index, but it is not practically effective in an environment with a large number of different resource attributes. Furthermore, few approaches are able to locate resources geographically close to the requesters, which is critical to system performance. This paper presents a P2P-based intelligent resource discovery (PIRD) mechanism that weaves all attributes into a set of indices using locality sensitive hashing, and then maps the indices to a structured P2P. It further incorporates Lempel-Ziv-Welch algorithm to compress attribute information for higher efficiency. In addition, it helps to search resources geographically close to requesters by relying on a hierarchical P2P structure. PIRD significantly reduces overhead and improves the efficiency and effectiveness of resource discovery. Theoretical analysis and simulation results demonstrate the efficiency of PIRD in comparison with other approaches. It dramatically reduces overhead and yields significant improvements on the efficiency of resource discovery.
[data compression, peer-to-peer computing, Peer to peer computing, Aerospace electronics, Maintenance engineering, distributed system, hierarchical P2P structure, Peer-to-Peer, Indexes, Resource Discovery, Lempel-Ziv-Welch algorithm, peer-to-peer, information compression, resource allocation, Space technology, Internet-based Distributed Systems, Distance measurement, Internet, intelligent resource discovery, Resource management, Distributed Hash Table]
Frequency Domain Filter Design and Analysis of Request Scheduling in Internet Servers
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Internet traffic has a characteristic of strong correlation. This traffic characteristic greatly complicates the problem of server performance modeling and optimization. Conventional time domain analysis has limitations in the study of the impact of complex traffic on server performance, because self-similarity of Internet traffic is often characterized in frequency domain. In this paper, we present a frequency domain filter model to characterize the relationship between server capacity, resource allocation, and service quality for general input traffic. Power spectral density (PSD) shows the strength of variations (power) as a function of frequency. By the model, server scheduler operates as a filter of input traffic that transforms its PSD function into another PSD function of server utilization process. The optimality of the scheduler in second-order statistics is to minimize the power leakage in the transformation. Most Internet traffic has monotonically decreasing PSD functions. For this type of input traffic, we prove that the optimal schedulers have a convex structure. Uniform allocation is an extreme case of the convexity and is proven to be optimal for traffic of independent arrivals. We integrate the convex-structured scheduling principle with GPS discipline and show that the enhanced GPS policy improves the service quality significantly.
[request scheduling analysis, power spectral density, Request scheduling, power leakage minimization, Optimal scheduling, Internet servers, server capacity, Information filtering, server performance optimization, Servers, Internet traffic self-similarity, resource allocation, file servers, scheduling, Information filters, service quality, higher order statistics, frequency-domain analysis, second-order statistics, filter design, Frequency domain analysis, frequency domain filter design, frequency domain analysis, general input traffic, convex-structured scheduling, Finite impulse response filter, server performance modeling, Internet, telecommunication traffic, server utilization]
Adaptive Distributed Time-Slot Based Scheduling for Fairness in Multi-Hop Wireless Networks
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Recent research indicates that multi-hop wireless networks can suffer from extreme imbalances in the throughput achieved by simultaneous competing flows. We address this problem by designing a practical distributed algorithm to compute a time-slot based schedule that provides end-to-end max-min fairness. Our system uses randomized priorities based on local weights to arbitrate access between nodes that directly compete with each other (we call this weighted slot allocation or WSA). The local weights are in turn computed by a higher layer called end-to-end fairness using local weights (EFLoW). EFLoW implements an additive-increase multiplicative-decrease (AIMD) algorithm that can automatically adapt to changes in traffic demands and network conditions. In each iteration, EFLoW only uses state obtained from within a given node's contention region. We have implemented WSA and EFLoW in both a simulator and a real system by using the overlay MAC layer (OML). Unlike previous work on end-to-end fairness, our approach does not use a centralized coordinator and works for traffic patterns with any number of sources and sinks. Also, since we compute both the fair allocation and a schedule to achieve it, we do not make any assumptions about the efficiency of carrier-sense (CS) based MACs - this is very important in the light of recent work which shows that current CS-based MACs can be very unfair even when all nodes are limited to sending at their fair rate. Our results show that WSA and EFLoW can prevent starvation of flows and improve fairness without sacrificing efficiency for a wide variety of traffic patterns.
[end-to-end fairness using local weights, fairness, Schedules, radio networks, AIMD, overlay MAC layer, TDMA, adaptive distributed time-slot based scheduling, Interference, Throughput, weighted slot allocation, access protocols, multihop wireless networks, end-to-end max-min fairness, Time division multiple access, distributed algorithm, mesh networks, Wireless networks, Traffic control, scheduling, additive-increase multiplicative-decrease algorithm, Resource management, telecommunication traffic]
[Publisher information]
2008 The 28th International Conference on Distributed Computing Systems
None
2008
Provides a listing of current committee members and society officers.
[]
Message from the General Chair
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Presents the welcome message from the conference proceedings.
[]
Message from the Program Chair
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Presents the welcome message from the conference proceedings.
[]
Committee Lists
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Provides a listing of current committee members.
[]
Rethinking Multicast for Massive-Scale Platforms
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
A dramatic scale-up of distributed computing platforms is underway. Internet routers can contain hundreds or thousands of line cards. Cloud computing platforms may contain tens or even hundreds of thousands of machines. What is gluing all of this together? Multicast to support data replication, event streams, and coordination. Yet yesterday&#x02019;s multicast protocols are poorly matched to this new generation of uses; so much so that many cloud platforms refuse to deploy multicast as such, and have instead resorted to clumsy alternatives, mapping multicast to TCP or even web services method invocations. This talk will explore inadequacies of existing protocols, early progress towards better ones, and the longer term research agenda.
[Computer science, Cloud computing, Waste materials, Web services, Biographies, Mashups, Multicast protocols, Magnetic heads, Internet, Distributed computing]
A Reinforcement Learning Approach to Online Web Systems Auto-configuration
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
In a web system, configuration is crucial to the performance and service availability. It is a challenge, not only because of the dynamics of Internet traffic, but also the dynamic virtual machine environment the system tends to be run on. In this paper, we propose a reinforcement learning approach for autonomic configuration and reconfiguration of multi-tier web systems. It is able to adapt performance parameter settings not only to the change of workload, but also to the change of virtual machine configurations. The RL approach is enhanced with an efficient initialization policy to reduce the learning time for online decision. The approach is evaluated using TPC-W benchmark on a three-tier website hosted on a Xen-based virtual machine environment. Experiment results demonstrate that the approach can auto-configure the web system dynamically in response to the change in both workload and VM resource. It can drive the system into a near-optimal configuration setting in less than 25 trial-and-error iterations.
[online Web systems auto-configuration, Xen-based virtual machine environment, Internet traffic, dynamic virtual machine environment, Distributed computing, reinforcement learning, Learning, System performance, Web and internet services, Automatic control, Hardware, Virtual manufacturing, learning (artificial intelligence), Availability, Auto-configuration, TPC-W benchmark, autonomic configuration, multi-tier Web systems, Reinforcement Learning, Virtual machining, Web Systems, software fault tolerance, virtual machines, Internet, Resource management]
Graduated QoS by Decomposing Bursts: Don't Let the Tail Wag Your Server
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
The growing popularity of hosted storage services and shared storage infrastructure in data centers is driving the recent interest in resource management and QoS in storage systems. The bursty nature of storage workloads raises significant performance and provisioning challenges, leading to increased infrastructure, management, and energy costs. We present a novel dynamic workload shaping framework to handle bursty workloads, where the arrival stream is dynamically decomposed to isolate its bursts, and then rescheduled to exploit available slack. We show how decomposition reduces the server capacity requirements dramatically while affecting QoS guarantees minimally. We present an optimal decomposition algorithm RTT and a recombination algorithm Miser, and show the benefits of the approach by performance evaluation using several storage traces.
[resource management, Costs, shared storage infrastructure, data centers, Probability distribution, dynamic workload shaping framework, Distributed computing, Delay, Network servers, storage management, resource allocation, USA Councils, QoS, Tail, decomposing bursts, server capacity requirements, performance evaluation, Resource, Scheduling, quality of service, storage traces, bursty workloads, graduated QoS, hosted storage services, storage systems, optimal decomposition algorithm, recombination algorithm Miser, Resource management, Performance, storage workloads, Energy storage, Bursty]
Reducing Disk I/O Performance Sensitivity for Large Numbers of Sequential Streams
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Retrieving sequential rich media content from modern commodity disks is a challenging task. As disk capacity increases, there is a need to increase the number of streams that are allocated to each disk. However, when multiple streams are accessing a single disk, throughput is dramatically reduced because of disk head seek overhead, resulting in requirements for more disks. Thus, there is a tradeoff between how many streams should be allowed to access a disk and the total throughput that can be achieved. In this work we examine this tradeoff and provide an understanding of issues along with a practical solution. We use Disksim, a detailed architectural simulator, to examine several aspects of a modern I/O subsystem and we show the effect of various disk parameters on system performance under multiple sequential streams. Then, we propose a solution that dynamically adjusts I/O request streams, based on host and I/O subsystem parameters. We implement our approach in a real system and perform experiments with a small and a large disk configuration. Our approach improves disk throughput up to a factor of 4 with a workload of 100 sequential streams, without requiring large amounts of memory on the storage node. Moreover, it is able to adjust (statically) to different storage node configurations, essentially making the I/O subsystem insensitive to the number of I/O streams used.
[I/O streaming, I/O Performance, disk capacity, Disksim, Content based retrieval, sequential rich media content retrieval, information retrieval, Throughput, Information retrieval, Application software, sequential streams, Distributed computing, disk head, Computer science, Degradation, System performance, Operating systems, data access, Block-level Storage, Disk performance, Streaming media, media streaming, performance sensitivity]
Fragmentation Design for Efficient Query Execution over Sensitive Distributed Databases
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
The balance between privacy and utility is a classical problem with an increasing impact on the design of modern information systems. On the one side it is crucial to ensure that sensitive information is properly protected; on the other side, the impact of protection on the workload must be limited as query efficiency and system performance remain a primary requirement. We address this privacy/efficiency balance proposing an approach that, starting from a flexible definition of confidentiality constraints on a relational schema, applies encryption on information in a parsimonious way and mostly relies on fragmentation to protect sensitive associations among attributes. Fragmentation is guided by workload considerations so to minimize the cost of executing queries over fragments. We discuss the minimization problem when fragmenting data and provide a heuristic approach to its solution.
[Data privacy, Costs, query execution, cryptography, Information management, relational databases, Distributed computing, Diffusion tensor imaging, Information systems, relational schema, query processing, sensitive distributed databases, System performance, encryption, Distributed databases, distributed databases, system performance, fragmentation design, information systems, Cryptography, Protection]
Lightweight Secure Search Protocols for Low-cost RFID Systems
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
RFID technology can potentially be used in many applications. A typical RFID system involves a reader and a number of tags, which may range from the battery-powered tags that have Wi-Fi capabilities, to the low-cost tags that are constrained in computation capacities and hardware resources. Keeping RFID systems secure is crucial since RFID systems are vulnerable to a number of malicious attacks. As for low-cost RFID systems, security problems become much more challenging, because traditional security mechanisms are infeasible to be used on low-cost tags due to their resource constraints. Tag search is an important functionality that a RFID system should provide. In this paper, we study how to secure tag search with a focus on low-cost RFID systems. Existing solutions are mostly based on hash functions and consume 8000 to 10000 gates, however, the low-cost tags can afford at most 2000 gates for secure features. In this paper, we propose several lightweight secure search protocols based on linear feedback shift registers (LFSR) and physically unclonable functions (PUF). Our protocols prevent adversaries from learning tag identity and impersonating RFID reader or tags. Experimental results show that our solutions have hundreds of nanoseconds processing time and require no more than 1400 hardware gates on tags.
[Protocols, cryptographic protocols, radiofrequency identification, RFID technology, tag search protocol, Communication system security, Distributed computing, lightweight, Wireless communication, feedback, security, hash function, Hardware, battery-powered tags, physically unclonable functions, low-cost, Power system security, Application software, RFID tags, lightweight secure search protocols, RFID system, linear feedback shift register, Linear feedback shift registers, shift registers, tag search, Radiofrequency identification]
The Impact of Communication Models on Routing-Algorithm Convergence
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Autonomous routing algorithms, such as BGP, are intended to reach a globally consistent set of routes after nodes iteratively and independently collect, process, and share network information. Generally, the important role of the mechanism used to share information has been overlooked in previous analyses of these algorithms. In this paper, we explicitly study how the network-communication model affects algorithm convergence. To do this, we consider a variety of factors, including channel reliability, how much information is processed from channels, and how many channels are processed simultaneously. Using these factors, we define a taxonomy of communication models and identify particular models of interest, including those used in previous theoretical work, those that most closely model real-world implementations of BGP, and those of potential interest for the design of future routing algorithms. We characterize an extensive set of relationships among models in our taxonomy and show that convergence depends on the communication model in nontrivial ways. These results highlight that certain models are best for proving conditions that guarantee convergence, while other models are best for characterizing conditions that might permit nonconvergence.
[Algorithm design and analysis, telecommunication channels, Computational modeling, Taxonomy, BGP, Distributed computing, autonomous routing algorithms, Convergence, Information analysis, channel reliability, routing-algorithm convergence, routing protocols, telecommunication network reliability, network-communication model, communication models, Iterative algorithms, Routing protocols, network information, Space exploration, Telecommunication network reliability]
Selective Protection: A Cost-Efficient Backup Scheme for Link State Routing
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
In recent years, there are substantial demands to reduce packet loss in the Internet. Among the schemes proposed, finding backup paths in advance is considered to be an effective method to reduce the reaction time. Very commonly, a backup path is chosen to be a most disjoint path from the primary path, or in the network level, backup paths are computed for all links (e.g., IPRFF). The validity of this straightforward choice is based on 1) all the links may fail with equal probability; and 2) facing the high protection requirement today, having links not protected or sharing links between the primary and backup paths just simply look weird. Nevertheless, indications from many research studies have confirmed that the vulnerability of the links in the Internet is far from equality. In addition, we have seen that full protection schemes may introduce high costs. In this paper, we argue that such approaches may not be cost effective. We first analyze the failure characteristics based on real world traces from CERNET2, the China education and Research NETwork 2. We observe that the failure probabilities of the links is heavy-tail, i.e., a small set of links caused most of the failures. We thus propose a selective protection scheme. We carefully analyze the implementation details and the overhead for general backup path schemes of the Internet today. We formulate an optimization problem where the routing performance (in terms of network level availability) should be guaranteed and the backup cost should be minimized. This cost is special as it involves computation overhead. Consequently, we propose a novel critical-protection algorithm which is fast itself. We evaluate our scheme systematically, using real world topologies and randomly generated topologies. We show significant gain even when the network availability requirement is 99.99% as compared to that of the full protection scheme.
[Research NETwork 2, routing performance, randomly generated topology, backup path, link state routing, Distributed computing, system recovery, backup scheme, Network topology, cost-efficient backup scheme, optimization, Failure analysis, Cost function, Computer networks, Protection, Stock markets, Availability, CERNET2, probability, back-up procedures, telecommunication network topology, Routing, selective protection, network level availability, telecommunication network routing, real world topology, Internet, failure probability, critical-protection algorithm]
Centaur: A Hybrid Approach for Reliable Policy-Based Routing
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
In this paper, we consider the design of a policy-based routing system and the role that link state might play. Looking at the problem from a link-state perspective, we propose Centaur, a hybrid routing protocol combining the benefits of both link state and path vector. Through analytical and experimental studies, we demonstrate Centaur's potential in achieving rich policy expressiveness and high network availability. Our work shows that it is possible to combine link-state and path-vector approaches into a practical and efficient algorithm for policy-based routing.
[IEEE news, path vector, Filtering, Peer to peer computing, Scalability, link state, reliable policy-based routing protocol, Centaur, Maintenance, path vector routing, Distributed computing, link state vector, Convergence, Privacy, routing, Network topology, routing protocols, Routing protocols, computer network reliability]
Joint Optimization of Spectrum Handoff Scheduling and Routing in Multi-hop Multi-radio Cognitive Networks
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Spectrum handoff causes performance degradation of the cognitive network when the primary user reclaims its right to access the licensed spectrum. In a multi-hop cognitive network, this problem becomes even worse since multiple links are involved. Spectrum handoff of multiple links seriously affects the network connectivity and routing. In this paper, we describe a cross-layer optimization approach to solve the spectrum handoff problem with joint consideration of spectrum handoff scheduling and routing. We propose a protocol, called Joint Spectrum Handoff Scheduling and Routing Protocol (JSHRP). This paper makes the following major contributions. First, the concept "spectrum handoff of single link" is extended to "spectrum handoff of multiple links\
[greedy algorithms, joint spectrum handoff scheduling-and-routing protocol, JSHRP, Throughput, spectral analysis, distributed greedy algorithm, Cognitive radio, Distributed computing, Delay, rerouting mechanism, Degradation, optimisation, Processor scheduling, cognitive radio, centralized greedy algorithm, routing protocols, cross-layer optimization approach, Spread spectrum communication, multihop multiradio cognitive network, scheduling, Frequency, Routing protocols, Computer networks]
Simulation Framework and Performance Analysis of Multimedia Broadcasting Service over Wireless Networks
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
The recent development of high-speed data transmission over wireless networks enables multimedia broadcasting service to mobile users. Multimedia broadcasting service involves interactions among different system and network components, so it is crucial for the service provider to verify the correctness of system/service model and design, and their behaviors before a new type of service is deployed. However, due to limitations of using network simulations or scaled experimental testbeds, there has been none of research on such verification and simulation framework in 3G broadcasting networks. Therefore, we propose a simulation and analysis framework for multimedia broadcasting service over wireless networks. With concrete modeling of wireless physical channel, network, and data processing on a client device, it enables the prediction of various interesting system parameters and perceived quality of multimedia streams to users. Different models of system and network components can be plugged easily in our simulation framework for further extensions. Using this framework, we analyze the processing performance for decoding scalable videos on mobile devices in CDMA2000 wireless networks.
[radio networks, service provider, 3G mobile communication, simulation, wireless physical channel, multimedia broadcasting service, Multimedia communication, client device, Digital multimedia broadcasting, Analytical models, Wireless networks, CDMA2000, network simulations, media streaming, Performance analysis, mobile users, Data communication, Testing, client-server systems, mobile radio, high-speed data transmission, simulation framework, Multimedia systems, 3G broadcasting networks, data processing, mobile, wireless networks, CDMA, broadcasting, network components, multimedia, multimedia streams, Streaming media, data communication, Concrete, wireless, performance analysis]
Transactional Mobility in Distributed Content-Based Publish/Subscribe Systems
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
This paper formalizes transactional properties for publish/subscribe client mobility and develops protocols to realize them. Evaluations show that compared to traditional protocols, those developed in this paper, in addition to supporting transactional properties, are more stable with respect to message and processing overheads. Changes in factors such as the number of moving clients have little impact, making the protocols more scalable and simpler to administer due to predictable resource requirements.
[Computers, transaction processing, message passing, client mobility, Software algorithms, Dynamic scheduling, Application software, distributed content-based publish/subscribe systems, Power system modeling, Distributed computing, transactional mobility, Engines, processing overhead, Jacobian matrices, mobile computing, Load management, Routing protocols, message overhead, middleware]
Multicast Throughput of Hybrid Wireless Networks Under Gaussian Channel Model
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
We study the multicast capacity for hybrid wireless networks consisting of ordinary wireless nodes and base stations under Gaussian channel model, which generalizes both the unicast capacity and broadcast capacity for hybrid wireless networks. We simply consider the hybrid extended network, where the ordinary wireless nodes are placed in the square region A(n) with side-length radicn according to a Poisson point process with unit intensity. In addition, m additional base stations (BSs) serving as the relay gateway are placed regularly in the region A(n) and they are connected by a high-bandwidth wired network. Three broad categories of multicast strategies are proposed in this paper. According to the different scenarios in terms of m, n and n<sub>d</sub>, we select the optimal scheme from the three categories of strategies, and derive the achievable multicast throughput based on the optimal decision.
[Base stations, mobile radio, relay gateway, Gaussian channel model, ad hoc network, Educational technology, Interference, channel capacity, Throughput, multicast capacity, Relays, Distributed computing, Computer science, Unicast, Wireless networks, Poisson point process, multicast communication, wireless channels, ad hoc networks, stochastic processes, m additional base station, Gaussian channels, hybrid wireless networks]
Distributed Key Generation for the Internet
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Although distributed key generation (DKG) has been studied for some time, it has never been examined outside of the synchronous setting. We present the first realistic DKG architecture for use over the Internet. We propose a practical system model and define an efficient verifiable secret sharing scheme in it. We observe the necessity of Byzantine agreement for asynchronous DKG and analyze the difficulty of using a randomized protocol for it. Using our verifiable secret sharing scheme and a leader-based agreement protocol, we then design a DKG protocol for public-key cryptography. Finally, along with traditional proactive security, we also introduce group modification primitives in our system.
[public-key cryptography, leader-based agreement protocol, Byzantine agreement, Security, Distributed computing, Cryptographic protocols, Variable structure systems, Computer science, public key cryptography, Public key, Computer architecture, secret sharing scheme, Public key cryptography, distributed key generation, Synchronous generators, Internet, protocols]
Clock-like Flow Replacement Schemes for Resilient Flow Monitoring
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
In the context of a collaborating surveillance system for active TCP sessions handled by a networking device, we consider two problems. The first is the problem of protecting a flow table from overflow and the second is developing an efficient algorithm for estimating the number of active flows coupled with the identification of "heavy-hitter" TCP sessions. Our proposed techniques are sensitive to limited hardware and software resources allocated for this purpose in the linecards in addition to the very high data rates that modern line cards handle; specifically we are interested in cooperatively maintaining a per-flow state with a low cost, which has resiliency on dynamic traffic mix. We investigate a traditional timeout processing mechanism to manage the flow table for per-flow monitoring, called Timeout-Based Purging (TBP), our proposed Clock-like Flow Replacement (CFR) algorithms using a replacement policy, called "clock\
[telecommunication security, Software maintenance, Costs, active TCP sessions, Resilient flow monitoring, Per-flow state, surveillance system, Hardware, clock-like flow replacement schemes, Protection, Monitoring, resilient flow monitoring, networking device, timeout-based purging, Session purging, quality of service, timeout processing mechanism, Surveillance, transport protocols, Collaboration, telecommunication network routing, flow table management, dynamic traffic mix, per-flow monitoring, Internet, Resource management, telecommunication traffic, Clocks]
The Taming of the Shrew: Mitigating Low-Rate TCP-Targeted Attack
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
A Shrew attack, which uses a low-rate burst carefully designed to exploit TCP's retransmission timeout mechanism, can throttle the bandwidth of a TCP flow in a stealthy manner. While such an attack can significantly degrade the performance of all TCP-based protocols and services including Internet routing (e.g., BGP), no existing scheme clearly solves the problem in real network scenarios. In this paper, we propose a simple protection mechanism, called SAP (Shrew Attack Protection), for defending against a Shrew attack. Rather than attempting to track and isolate Shrew attackers, SAP identifies TCP victims by monitoring their drop rates and preferentially admits those packets from victims with high drop rates to the output queue. This is to ensure that well-behaved TCP sessions can retain their bandwidth shares. Our simulations indicate that under a Shrew attack, SAP can prevent TCP sessions from closing, and effectively enable TCP flows to maintain high throughput. SAP is a destination-port-based mechanism and requires only a small number of counters to find potential victims, which makes SAP readily implementable on top of existing router mechanisms.
[telecommunication security, shrew attack protection, Network Security, Throughput, Distributed computing, Counting circuits, Bandwidth, TCP-targeted attack, Routing protocols, IP networks, Protection, Monitoring, Internet routing, transmission control protocols, security of data, Aggregates, transport protocols, TCP retransmission timeout mechanism, TCP flow, TCP-based protocols, router mechanism, TCP sessions, destination-port-based mechanism, Frequency synchronization, Shrew attack]
The Design and Evaluation of Accountable Grid Computing System
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Accountability is an important aspect of any computer system. It assures that every action executed in the system can be traced back to some entity. Accountability is even more crucial for assuring the safety and security in grid systems, given the very large number of users active in these sophisticated environments. However, to date no comprehensive approach to accountability for grid systems exists. Our work addresses such inadequacy by developing a comprehensive accountability system driven by policies and supported by accountability agents. In this paper we first discuss the requirements that have driven the design of our accountability system and then present some interesting aspects related to our accountability framework. We describe a fully working implementation of our accountability system, and conduct extensive experimental evaluations. Our experiments, carried out using the Emulab testbed, demonstrate that the implemented system is efficient and it scales for grid systems of large number of resources and users.
[Availability, System testing, Policies, Data security, Scalability, grid computing, accountable grid computing system, computer system, Distributed computing, Emulab testbed, Grid Computing, Computer science, Design engineering, Information science, grid system safety, comprehensive accountability system, security of data, Accountability, Grid computing, Safety, Distributed Computing, grid system security, accountability agents]
The Case for Spam-Aware High Performance Mail Server Architecture
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
The email volume per mailbox has largely remained low and unchanged in the past several decades, and hence mail server performance has largely remained a secondary issue. The steep rise in the amount of unsolicited emails, i.e. spam, in the past decade, however, has permanently disrupted this tranquility of largely steady email volume and turned mail server performance into an increasingly important issue. In this paper, we point out that modern mail servers were not originally designed with email spam in mind, and as such, as the "common case'' workload for mail servers has shifted from legitimate emails to spam emails, we argue it is time to revisit mail server architecture design in following the system design principle of optimizing the common case. In particular, we show how to optimize the performance of three major components of modern mail servers, the concurrency architecture, the disk I/O, and DNSBL lookups, by exploiting the new "common case" workload. An evaluation of our prototype implementation of the enhanced postfix architecture shows that the optimizations significantly reduce the CPU, disk, and network resource consumptions, and improves the throughput of the mail server by 18% under a university departmental mail server workload and by 40% under a spam sinkhole workload.
[mail server, Unsolicited electronic mail, File servers, unsolicited e-mail, Electronic mail, Distributed computing, Spam, Design optimization, Postal services, Concurrent computing, Network servers, unsolicited emails, disk I/O, File systems, security of data, concurrency architecture, spam sinkhole workload, Computer architecture, spam-aware high performance mail server architecture, network resource consumptions, postfix, DNSBL lookups]
An Approach to Sharing Legacy TV/Arcade Games for Real-Time Collaboration
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Interactive TV/arcade games have been entertaining people for over 50 years. Nowadays a large number of legacy TV/arcade games have been ported to new platforms such as PCs by emulation. However, they generally require that the players be co-located to interact with one computer that emulates the game. This paper proposes a novel approach to turning those single-computer games into multi-computer games such that multiple players can play their favorite legacy games in real time over a computer network. The main challenge in this work is how to synchronize multiple replicas of a game without semantic knowledge about or modifications to the game. We have developed a novel synchronization algorithm and a working system to validate the ideas. In this paper we present the approach, especially the synchronization algorithm, and evaluate its effectiveness under a variety of network conditions. In future research we will extend this work on mobile devices.
[interactive TV, TV, arcade game, Collaborative software, real-time collaboration, Multiplayer Gaming, Consistency, Virtual machining, Application software, Synchronization, Voice mail, legacy TV game, Emulation, Collaboration, computer games, groupware, interactive television, Local Lag, Computer networks, synchronization algorithm, Real Time, Personal communication networks, Virtual manufacturing, multicomputer game]
Minimizing Latency in Fault-Tolerant Distributed Stream Processing Systems
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Event stream processing (ESP) applications target the real-time processing of huge amounts of data. Events traverse a graph of stream processing operators where the information of interest is extracted. As these applications gain popularity, the requirements for scalability, availability, and dependability increase. In terms of dependability and availability, many applications require a precise recovery, i.e., a guarantee that the outputs during and after a recovery would be the same as if the failure that triggered recovery had never occurred. Existing solutions for precise recovery induce prohibitive latency costs, either by requiring continuous checkpoint or logging (in a passive replication approach) or perfect synchronization between replicas executing the same operations (in an active replication approach). We introduce a novel technique to guarantee precise recovery for ESP applications while minimizing the latency costs as compared to traditional approaches. The technique minimizes latencies via speculative execution in a distributed system. In terms of scalability, the key component of our approach is a modified software transactional memory that provides not only the speculation capabilities but also optimistic parallelization for costly operations.
[Checkpointing, checkpointing, storage allocation, transaction processing, Costs, event stream processing, Scalability, graph theory, distributed processing, Data engineering, graph traversal, Data mining, Distributed computing, Delay, Electrostatic precipitators, Fault tolerant systems, optimistic parallelization, Event Stream Processing, Parallel Computing, Computer crashes, synchronisation, software transactional memory, fault-tolerant distributed stream processing system recovery, system monitoring, synchronization, fault tolerant computing, latency cost minimization, Fault-tolerance, Software Transactional Memory]
Automatic Performance Tuning for the Virtualized Cluster System
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
System virtualization can aggregate the functionality of multiple standalone computer systems into a single hardware computer. It is significant to virtualize the computing nodes with multi-core processors in the cluster system, in order to promote the usage of the hardware while decrease the cost of the power. In the virtualized cluster system, multiple virtual machines are running on a computing node. However, it is a challenging issue to automatically balance the workload in virtual machines on each physical computing node, which is different from the traditional cluster system's load balance. In this paper, we propose a management framework for the virtualized cluster system, and present an automatic performance tuning strategy to balance the workload in the virtualized cluster system. We implement a working prototype of the management framework (VEMan) based on Xen, and test the performance of the tuning strategy on a virtualized heterogeneous cluster system. The experimental result indicates that the management framework and tuning strategy are feasible to improve the performance of the virtualized cluster system.
[workstation clusters, multicore cluster system, Multicore processing, automatic performance tuning, physical resource allocation, Virtual machining, multiple virtual machine, hardware computer, Distributed computing, Voice mail, workload balancing, virtualized cluster system management, Power engineering computing, resource allocation, Operating systems, Aggregates, Physics computing, virtual machines, multiple standalone computer system, Hardware, Virtual manufacturing]
m-LIGHT: Indexing Multi-Dimensional Data over DHTs
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
In this paper, we study the problem of indexing multidimensional data in the P2P networks based on distributed hash tables (DHTs). We identify several design issues and propose a novel over-DHT indexing scheme called m- LIGHT. To preserve data locality, m-LIGHT employs a clever naming mechanism that gracefully maps the index tree into the underlying DHT so that it achieves efficient index maintenance and query processing. Moreover, m- LIGHT leverages a new data-aware index splitting strategy to achieve optimal load balance among peer nodes. We conduct an extensive performance evaluation for m-LIGHT. Compared to the state-of-the-art indexing schemes, m-LIGHT substantially saves the index maintenance overhead, achieves a more balanced load distribution, and improves the range query performance in both bandwidth consumption and response latency.
[load balancing, clever naming mechanism, Distributed computing, Delay, P2P networks, query processing, resource allocation, USA Councils, distributed hash tables, complex query processing, multidimensional data indexing, data-aware index splitting strategy, Bandwidth, Robustness, Large-scale systems, index maintenance, Distributed hash tables, Multidimensional systems, peer-to-peer computing, Peer to peer computing, indexing, P2P indexing, performance evaluation, Query processing, m-LIGHT, Indexing]
Two-Tier Air Indexing for On-Demand XML Data Broadcast
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
XML data broadcast is an efficient way to disseminate semi-structured information in wireless mobile environments. Air indexing is the common method to improve the access time, and reduce the energy consumption in a broadcast system. In this paper, we propose a novel two-tier air indexing method that provides an overall picture of the document set in the server which is necessary for XML data retrieving in on-demand mode. The efficiency of our indexing method is contributed by two distinct advantages. First, the proposed pruning technique and the two-tier structure significantly reduce the index size. Second, the two-tier structure enables efficient access protocol at the client which can further reduce the tuning time during the index look up. Simulation experiments show the benefits of our indexing methods.
[Satellite broadcasting, two-tier air indexing structure, Conference management, access protocols, Distributed computing, Environmental management, Network servers, database indexing, document retrieval, Management information systems, energy consumption, on-demand XML data broadcast, wireless mobile environment, mobile radio, semistructured information dissemination, information dissemination, Access protocols, information retrieval, access protocol, two-tier, broadcasting, pruning technique, Computer science, air indexing, XML, on-demand data broadcast, Indexing]
Distributed Processing of Spatial Alarms: A Safe Region-Based Approach
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Spatial alarms are considered as one of the basic capabilities in future mobile computing systems for enabling personalization of location-based services. In this paper, we propose a distributed architecture and a suite of safe region techniques for scalable processing of spatial alarms. We show that safe region-based processing enables resource optimal distribution of partial alarm processing tasks from the server to the mobile clients. We propose three different safe region computation algorithms to explore the impact of size and shape of the safe region on network bandwidth, server load and client energy consumption. Concretely, we show that the maximum weighted perimeter rectangular safe region approach outperforms previous techniques in terms of performance and accuracy. We further explore finer granularity safe regions by introducing grid-based and pyramid-based representation of rectilinear polygonal shapes using bitmap encoding. Our experimental evaluation shows that the distributed safe region-based architecture outperforms the two most popular server-centric approaches, periodic and safe period-based, for spatial alarm processing.
[spatial alarms, Shape, pyramid-based representation, grid computing, distributed processing, Mobile communication, Mobile Computing, Distributed computing, distributed architecture, Safe Region, Global Positioning System, grid-based representation, Distributed processing, Wireless sensor networks, Network servers, software architecture, mobile computing, Spatial Alarms, safe region-based approach, location-based services, Internet, Personal digital assistants, Mobile computing]
Maintaining Probabilistic Consistency for Frequently Offline Devices in Mobile Ad Hoc Networks
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Maintaining cache consistency in mobile environment is an important issue which is extensively studied in the last decade. In mobile ad hoc networks (MANETs), a large number of nonstationary mobile terminals connect with each other through multi-hop unreliable communication channels, coupling with the fact that disconnections from the network are very frequent. Most existing cache consistency strategies assume reliable communication between mobile terminals, which cannot handle frequently offline devices adequately. In this paper, we introduce the probabilistic cache consistency model for applications not requiring strong consistency. Based on this model, a probability consistency strategy (ProP) for frequently offline devices in MANETs is studied. ProP is a randomized pull-based Strategy. It is demostrated to guarantee cache consistency with a high probability. A theoretical model is developed to investigate the performance of the proposed cache consistency strategies, and design guidelines are provided for ProP to choose proper system parameters to achieve probabilistic cache consistency.
[Stochastic processes, Switches, multihop unreliable communication channels, Mobile communication, cache storage, probability consistency strategy, Delay, Mobile ad hoc networks, mobile ad hoc networks, Spread spectrum communication, Bandwidth, telecommunication network reliability, Broadcasting, frequently offline devices, cache consistency strategies, Mobile ad hoc network, mobile terminals, mobile radio, probability, mobile environment, Probabilistic cache consistency, Offline device, nonstationary mobile terminals, Wireless sensor networks, MANET, probabilistic cache consistency model, randomized pull-based Strategy, ad hoc networks, Mobile computing]
Roadcast: A Popularity Aware Content Sharing Scheme in VANETs
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Content sharing through vehicle-to-vehicle communication can help people find their interested content on the road. In VANETs, due to limited contact duration and unreliable wireless connection, a vehicle can get the useful data only when it meets another vehicle and the encountered vehicle has the exactly matched data. However, the probability of such case is very low. To improve the performance of content sharing in intermittently connected VANETs, we propose a novel P2P content sharing scheme called Roadcast. Roadcast ensures popular data is more likely to be shared with other vehicles so that the overall query delay and the query hit ratio can be improved. Roadcast consists of two components called popularity aware content retrieval and popularity aware data replacement. The popularity aware content retrieval scheme makes use of information retrieval (IR) techniques to find the most relevant and popular data towards user's query. The popularity aware data replacement algorithm ensures that the density of different data is proportional to their popularity in the system steady state, which firmly obeys the optimal "square-root" replication rule. Results based on real city map and real traffic model show that Roadcast outperforms other content sharing schemes in VANETs.
[Content based retrieval, popularity aware data replacement, Telecommunication traffic, Content Sharing, P2P content sharing scheme, query delay, Distributed computing, Delay, popularity aware content sharing scheme, Road vehicles, Roadcast, mobile radio, popularity aware content retrieval, peer-to-peer computing, vehicle-to-vehicle communication, Peer to peer computing, Vehicle driving, information retrieval, optimal square-root replication rule, VANET, Information retrieval, Replacement, traffic engineering computing, Computer science, VANETs, information retrieval techniques, Road safety, ad hoc networks]
TBD: Trajectory-Based Data Forwarding for Light-Traffic Vehicular Networks
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
This paper proposes a trajectory-based data forwarding (TBD) scheme, tailored for the data forwarding in light- traffic vehicular ad-hoc networks. State-of-the-art schemes have demonstrated the effectiveness of their data forwarding strategies by exploiting known vehicular traffic statistics (e.g., densities and speeds) in these vehicular networks. These results are encouraging, however, further improvements can be made by taking advantage of the growing popularity of GPS-based navigation systems. This paper presents the first attempt to investigate how to effectively utilize vehicles" trajectory information in a privacy-preserving manner. In our design, the trajectory information is combined with the traffic statistics to improve the performance of data forwarding in road networks. Through theoretical analysis and extensive simulation, it is shown that our design outperforms the existing scheme.
[GPS-based navigation system, Telecommunication traffic, Expected Delivery Delay, Delay, Analytical models, light-traffic vehicular ad-hoc network, road vehicles, traffic statistics, Road vehicles, Traffic control, TBD scheme, privacy-preserving manner, Trajectory, road traffic, mobile radio, Navigation, Vehicular Networks, VANET, Ad hoc networks, Statistics, Data Forwarding, Global Positioning System, trajectory-based data forwarding, road network, state-of-the-art scheme, Link Delay, Internet, ad hoc networks, statistical analysis]
PADD: Power Aware Domain Distribution
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Modern data centers usually have computing resources sized to handle expected peak demand, but average demand is generally much lower than peak. This means that the systems in the data center usually operate at very low utilization rates. Past techniques have exploited this fact to achieve significant power savings, but they generally focus on centrally managed, throughput-oriented systems that process a single fine-grained request stream. We propose a more general solution - a technique to save power by dynamically migrating virtual machines and packing them onto fewer physical machines when possible. We call our scheme power-aware domain distribution (PADD). In this paper, we report on simulation results for PADD and demonstrate that the power and performance changes from using PADD are primarily dependent on how much buffering or reserve capacity it maintains. Our adaptive buffering scheme achieves energy savings within 7% of the idealized system that has no performance penalty. Our results also show that we can achieve an energy savings up to 70% with fewer than 1% of the requests violating their service level agreements.
[power aware domain distribution, buffer storage, Power system management, Laboratories, data centers, Throughput, Virtual machining, Transaction databases, Distributed computing, computer centres, adaptive buffering scheme, Computer science, power aware computing, Physics computing, physical machines, virtual machines, power savings, Logic, Energy management, throughput-oriented systems]
REMO: Resource-Aware Application State Monitoring for Large-Scale Distributed Systems
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
To observe, analyze and control large scale distributed systems and the applications hosted on them, there is an increasing need to continuously monitor performance attributes of distributed system and application states. This results in application state monitoring tasks that require fine-grained attribute information to be collected from relevant nodes efficiently. Existing approaches either treat multiple application state monitoring tasks independently and build ad-hoc monitoring trees for each task, or construct a single static monitoring tree for multiple tasks. We argue that a careful planning of multiple application state monitoring tasks by jointly considering multi-task optimization and node level resource constraints can provide significant gains in performance and scalability. In this paper, we present REMO, a REsource-aware application state MOnitoring system. REMO produces a forest of optimized monitoring trees through iterations of two phases, one phase exploring cost sharing opportunities via estimation and the other refining the monitoring plan through resource-sensitive tree construction. Our experimental results include those gathered by deploying REMO on a BlueGene/P rack running IBM's large-scale distributed streaming system - System S. Using REMO running over 200 monitoring tasks for an application deployed across 200 nodes results in a 35%-45% decrease in the percentage error of collected attributes compared to existing schemes.
[Continuous Query, iterative methods, Control system analysis, Scalability, Performance gain, distributed processing, Control systems, multitask optimization, Distributed Systems, Constraint optimization, optimisation, ad-hoc monitoring tree, Cost function, fine-grained attribute information, Large-scale systems, Performance analysis, tree data structures, Overlay, Monitoring, Data Stream, Resource-Aware, State monitoring, resource-sensitive tree construction, Distributed control, resource-aware application state monitoring, large-scale distributed system]
On the Utility of Inference Mechanisms
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
A number of network path delay, loss, or bandwidth inference mechanisms have been proposed over the past decade. Concurrently, several network measurement services have been deployed over the Internet and intranets. We consider inference mechanisms that use O(n) end-to-end measurements to predict the O(n2) end-to-end pairwise measurements among n nodes, and investigate when it is beneficial to use them in measurement services. In particular, we address the following questions: (1) For which measurement request patterns would using an inference mechanism be advantageous? (2) How does a measurement service determine the set of hosts that should utilize inference mechanisms, as opposed to those that are better served using direct end-to-end measurements? (3) How can the answer to question 2 be efficiently computed as measurement requests arrive and terminate? Our solution is able to identify groups of hosts which are likely to benefit from inference, by utilizing a probabilistically generated spanning forest on the measurement request graph. We compare our solution to a simple heuristic that uses the number of measurements a host participates in. Results with synthetic datasets as well as datasets from a popular peer-to-peer system demonstrate that our technique identifies host subsets that benefit from inference quite accurately, and in significantly less time than an algorithm that identifies optimal subsets. The measurement savings are large when measurement request patterns exhibit small-world characteristics, which is often the case for peer-to-peer and other popular distributed systems.
[bandwidth inference mechanism, measurement request graph, inference, Telecommunication traffic, end-to-end pairwise measurement, distributed system, Distributed computing, network loss, intranet, Inference mechanisms, measurement request pattern, network measurement, Probes, spanning forest, peer-to-peer computing, Peer to peer computing, network measurement service, Delay estimation, Mechanical factors, network path delay, intranets, Computer science, host subset, Position measurement, peer-to-peer system, Particle measurements, Internet]
Networking the Cloud
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
The data centers used to create cloud services represent a significant investment in capital outlay and ongoing costs. We examine the costs of cloud service data centers today, and discuss challenges in optimizing work completed per dollar invested. To be agile and cost effective, data centers should allow agile resource allocation across large server pools. We discuss a practical network architecture that scales to support huge data centers with uniform high capacity between servers, performance isolation between services, and Ethernet layer-2 semantics. A working prototype, built using commodity switches, approaches in practice the high level of performance that the theory predicts.
[Multiprotocol label switching, Network servers, Costs, Clouds, Engineering management, Computerized monitoring, Control systems, Computer networks, Communication system traffic control, Computer network management]
On the Connected k-Coverage Problem in Heterogeneous Sensor Nets: The Curse of Randomness and Heterogeneity
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Coverage is an essential task in sensor deployment for the design of wireless sensor networks. While most existing studies on coverage consider homogeneous sensors, the deployment of heterogeneous sensors represents more accurately the network design for real-world applications. In this paper, we focus on the problem of connected k-coverage in heterogeneous wireless sensor networks. Precisely, we distinguish two deployment strategies, where heterogeneous sensors are either randomly or pseudo-randomly distributed in a field. While the first deployment approach considers a single layer of heterogeneous sensors, the second one proposes a multi-tier architecture of heterogeneous sensors to better address the problems introduced by pure randomness and heterogeneity.
[wireless sensor networks, random processes, wireless sensor network, Sensor systems, Ad hoc networks, Application software, Distributed computing, heterogeneous sensor net, connected k-coverage problem, Computer science, Wireless sensor networks, multi-tier architecture, USA Councils, telecommunication network reliability, pseudo-random distribution, Approximation algorithms, Mobile computing, network lifetime, Energy storage]
Sampling Based (epsilon, delta)-Approximate Aggregation Algorithm in Sensor Networks
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Aggregation operations are important for users to get summarization information in WSN applications. As large numbers of applications only require approximate aggregation results rather than the exact ones, some approximate aggregation algorithms are proposed to save energy. However, the error bounds of these algorithms are fixed and it is impossible to adjust their error bounds automatically. Therefore, these algorithms cannot reach arbitrary precision requirement given by user. This paper proposes a sampling based approximate aggregation algorithm to satisfy the requirement of arbitrary precision. Besides, two sample data adaptive algorithms are also provided. One is to adapt the sample with the varying of precision requirement. The other is to adapt the sample with the varying of the sensed data in networks. The theoretical analysis and experiment results show that the proposed algorithms have high performance in terms of accuracy and energy cost.
[approximation theory, arbitrary precision, Costs, wireless sensor networks, Adaptive algorithm, error bounds, sampling, Sensor systems and applications, aggregation, sampling based-approximate aggregation, Distributed computing, adaptive systems, Wireless sensor networks, Filters, approximate aggregation, sensor network, Computer errors, Sampling methods, energy cost, Inference algorithms, save energy, data adaptive algorithms, Monitoring, error statistics]
Available Bandwidth in Multirate and Multihop Wireless Sensor Networks
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
In this paper, we derive a theoretical model to calculate the available bandwidth of a path and study its upper and lower bounds with background traffic. We show that the clique constraint widely used to construct upper bounds does not hold any more when links are allowed to use different rates at different time. In our proposed model, traditional clique is coupled with rate vector to more properly characterize the conflicting relationships among links in wireless sensor networks where time-varying link adaption is used. Based on the model, we also investigate the problem of joint optimization of QoS routing and propose several routing metrics. The newly proposed conservative clique constraint performs the best among the studied metrics in estimating available bandwidth of flows with background traffic.
[multihop wireless sensor networks, bandwidth estimation, wireless sensor networks, rate vector, Telecommunication traffic, Interference, Routing, Throughput, quality of service, time-varying link adaption, background traffic, Distributed computing, bandwidth allocation, Wireless sensor networks, quality of service routing, USA Councils, telecommunication network routing, Bandwidth, Spread spectrum communication, Traffic control, telecommunication traffic]
FLASH: Fine-Grained Localization in Wireless Sensor Networks Using Acoustic Sound Transmissions and High Precision Clock Synchronization
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Sensor localization in wireless sensor networks is an important component of many applications. Previous work has demonstrated how localization can be achieved using various methods. In this paper we focus on achieving fine-grained localization that does not require external infrastructure, specialized hardware support, or excessive sensor resources. We use a real sensor network and provide measurements on the actual system. We adopt a localization approach that relies on acoustic sounds and clock synchronization. The contribution of our work is achieving consistent sound pulse detection at each sensor and precise range estimation using a high-precision clock synchronization implementation. We first describe our technique and then we evaluate our approach using a real setup. Our results show that our approach achieves an average clock synchronization accuracy of 5 mus. We verify this accuracy using an external global clock via an interrupt mechanism. Our sound detection technique is able to consistently identify sound pulses up to 10 m distances in indoor environments. Combining the two techniques, we find that our localization method results in accurate range estimation with an average error of 11 cm in distances up to 7 m and in consistent range estimation up to 10 m in various indoor environments.
[wireless sensor networks, clock synchronization, Indoor environments, wireless sensor network, Sensor systems, Acoustic signal detection, range estimation, Synchronization, indoor environment, synchronisation, Wireless sensor networks, fine-grained localization, acoustic sound transmission, acoustic wave transmission, Acoustic sensors, Acoustic measurements, Acoustic pulses, Hardware, indoor radio, acoustic sound, high precision clock synchronization, pulse detection, Clocks]
Q-Tree: A Multi-Attribute Based Range Query Solution for Tele-immersive Framework
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Users and administrators of large distributed systems are frequently in need of monitoring and management of its various components, data items and resources. Though there exist several distributed query and aggregation systems, the clustered structure of tele-immersive interactive frameworks and their time-sensitive nature and application requirements represent a new class of systems which poses different challenges on this distributed search. Multi-attribute composite range queries are one of the key features in this class. Queries are given in high level descriptions and then transformed into multi-attribute composite range queries. Designing such a query engine with minimum traffic overhead, low service latency, and with static and dynamic nature of large datasets, is a challenging task. In this paper, we propose a general multi-attribute based range query framework, Q-Tree, that provides efficient support for this class of systems. In order to serve efficient queries, Q-Tree builds a single topology-aware tree overlay by connecting the participating nodes in a bottom-up approach, and assigns range intervals on each node in a hierarchical manner. We show the relative strength of Q-Tree by analytically comparing it against P-Tree, P-Ring, Skip-Graph and Chord. With fine-grained load balancing and overlay maintenance, our simulations with PlanetLab traces show that our approach can answer complex queries within a fraction of a second.
[overlay maintenance, topology-aware tree overlay, fine-grained load balancing, Conference management, P-Ring, Distributed computing, Delay, Engines, distributed aggregation systems, query processing, distributed query systems, USA Councils, Bandwidth, interactive systems, multiattribute composite range queries, Chord, Computerized monitoring, trees (mathematics), Skip-Graph, multiattribute based range query solution, query engine, software maintenance, PlanetLab, Computer science, Q-tree, tele-immersive interactive frameworks, Load management, multisite interactive system, Resource management, P-Tree]
Optimizing File Retrieval in Delay-Tolerant Content Distribution Community
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
In WiFi-based content distribution community infrastructure (CDCI), file servers are deployed in diverse locations around cities, caching popular files interesting to a community. They serve file download requests from mobile users in proximity via WiFi. In this paper, we study the optimal caching strategy for file servers in CDCI so that the mean file retrieval probability within deadline is maximized, subject to storage capacity constraints of each file server. The optimal caching strategy depends on many factors, such as users' mobility patterns, access point topology, file popularity, etc. We have formalized this content management problem as a mixed integer optimization problem. Because of NPhardness and a large variable space of this optimization problem, we propose a heuristic algorithm MobaSsign to allocate file blocks. Extensive experiments show that our heuristic caching strategy considering mobility patterns improves the file retrieval probability within deadline.
[file popularity, Content Distribution, Content based retrieval, mean file retrieval probability, access point topology, Mobility Pattern, MobaSsign, telecommunication network topology, Delay Tolerant Network, Delay, WiFi-based content distribution community infrastructure, mobility patterns, Optimization, delay-tolerant content distribution community, content management problem, optimisation, storage capacity constraints, NPhardness, file servers, file blocks, mixed integer optimization problem, optimal caching strategy, wireless LAN, computational complexity]
On Optimal Concurrency Control for Optimistic Replication
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Concurrency control is a core component in optimistic replication systems. To detect concurrent updates, the system associates each replicated object with metadata, such as, version vectors or causal graphs exchanged on synchronization opportunities. However, the size of such metadata increases at least linearly with the number of active sites. With trends in cloud computing, multi-regional collaboration, and mobile networks, the number of sites within a single replication system becomes very large. This imposes substantial overhead in communication and computation on every site. In this paper, we first present three version vector implementations that significantly reduce the cost of vector exchange by incrementally transferring vector elements. Basic rotating vectors (BRV) support systems providing no conflict reconciliation, whereas conflict rotating vectors (CRV) extend BRV to overcome this limitation. Skip rotating vectors (SRV) based on CRV further reduce data transmission. We show that both BRV and SRV are optimal implementations of version vectors, which, in turn, have minimal storage complexity among all known concurrency control schemes for state-transfer systems. We then present a causal graph exchange algorithm for operation-transfer systems with optimal communication overhead. All these algorithms adopt network pipelining to reduce running time.
[Cloud computing, Costs, metadata, optimistic replication systems, Scalability, graph theory, optimal concurrency control, state-transfer systems, Control systems, Distributed computing, causal graph exchange algorithm, version vectors, multiregional collaboration, single replication system, concurrent update detection, cloud computing, Availability, meta data, network pipelining, operation-transfer systems, Concurrency control, mobile networks, conflict rotating vectors, optimal communication overhead, Collaboration, concurrency control, Object detection, basic rotating vectors support systems, skip rotating vectors, Mobile computing]
File Versioning for Block-Level Continuous Data Protection
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Block-level continuous data protection (CDP) logs every disk block update so that disk updates within a time window are undoable. Standard file servers and DBMS servers can enjoy the data protection service offered by block-level CDP without any modification. Unfortunately, no existing block-level CDP systems can provide users a file versioning view on top of the block versions they maintain. As a result, the data they maintain cannot be used as an extension to the on-line system with which users routinely interact. This paper describes a name-based user-level file versioning system called UVFS that is designed to reconstruct file versions from disk block versions maintained by a block-level CDP. UVFS reconstructs file versions by following the last modified time of files and directories, a common file metadata supported by almost all modern file systems, and therefore does not require any modification to the host file system that a block-level CDP system protects. In addition, UVFS incorporates a file system-specific incremental consistency check mechanism to quickly convert an arbitrary point-in-time block-level snapshot to a file system-consistent one. Performance measurements taken from a fully operational UVFS prototype show that the average end-to-end elapsed time required to discover a file version is under 50 msec from the perspective of an NFS client serviced by an NFS server backed by a block-level CDP system.
[Measurement, DBMS servers, NFS client, incremental consistency check mechanism, File servers, NFS server, name-based user-level file versioning system, Distributed computing, block-level continuous data protection, CDP, file metadata, File systems, Operating systems, Prototypes, file servers, disk block update, Protection, disk block versions, Image converters, host file system, Image restoration, File Versioning, security of data, standard file servers, Linux, file organisation]
Model Checking Transactional Memory with Spin
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
We used the Spin model checker to show that Intel's implementation of software transactional memory is correct. Transactional memory makes it possible to write properly-synchronized multi-threaded programs without the explicit use of locks. We describe our model of Intel's implementation, our experience with Spin, what we have shown, and what obstacles remain to showing more.
[transaction processing, multi-threading, program verification, Spin, Read-write memory, Yarn, Distributed computing, synchronisation, Transactional memory, software transactional memory, synchronized multithreaded program, model checking, Privatization, Spin model checker, Database systems]
A Distributed Termination Detection Algorithm for Dynamic Asynchronous Systems
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Termination detection in distributed systems has been a popular problem of study. It involves determining whether a computation running on multiple nodes has ceased all its activities. A large number of termination detection algorithms have been proposed for static distributed systems in which the number of nodes present in the system is fixed and never changes during runtime. Recently, the termination detection problem has been investigated in the context of dynamic distributed systems in which individual nodes may join and/or leave the system at any time. In this paper, we propose an efficient algorithm for detecting termination of a computation in a dynamic, asynchronous, distributed system that allows nodes to join as well as leave the system while the computation is in progress. Our simulation results indicate that our algorithm has lower message complexity as well as lower detection latency than other comparable algorithms for solving the same problem.
[dynamic asynchronous systems, Tree-based Algorithm, Change detection algorithms, Asynchronous Systems, Peer to peer computing, Computational modeling, distributed processing, detection latency, Distributed computing, Delay, distributed termination detection algorithm, Computer science, message complexity, Runtime, USA Councils, static distributed systems, Grid computing, Dynamic Systems, Detection algorithms, Termination Detection]
Towards Optimal Resource Utilization in Heterogeneous P2P Streaming
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Though plenty of research has been conducted to improve Internet P2P streaming quality perceived by end-users, little has been known about the upper bounds of achievable performance with available resources so that different designs could compare against. On the other hand, the current practice has shown increasing demand of server capacities in P2P-assisted streaming systems in order to maintain high-quality streaming to end-users. Both research and practice call for a design that can optimally utilize available peer resources. In the paper, we first present a new design, aiming to reveal the best achievable throughput for heterogeneous P2P streaming systems. We measure the performance gaps between various designs and this optimal resource allocation. Through extensive simulations, we find out that several typical existing designs have not fully exploited the potential of system resources. However, the control overhead prohibits the adoption of this optimal approach. Then, we design a hybrid system in trading off the cost of assignment and utilization of resources. This hybrid approach has a proved theoretical bound on efficiency of utilization. Simulation results show that compared with the optimal resource allocation, our proposed hybrid design can achieve near-optimal (up to 90%) utilization while only use much less (below 4%) control overhead. Our results provide a basis for both server capacity planning in current P2P-assisted streaming practice and future protocol designs.
[optimal resource utilization, resource management, high-quality streaming, peer-to-peer computing, telecommunication network planning, server capacity planning, Access protocols, Throughput, P2P streaming, Distributed computing, heterogeneous, Internet P2P streaming quality, Computer science, resource allocation, Web and internet services, Optimal control, Bandwidth, Streaming media, Internet, protocol design, Resource management, Web server]
NAT-resilient Gossip Peer Sampling
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Gossip peer sampling protocols now represent a solid basis to build and maintain peer to peer (p2p) overlay networks. They provide peers with a random sample of the network and maintain connectivity in highly dynamic settings. They rely on the assumption that, at any time, each peer is able to communicate with any other peer. Yet, this ignores the fact that there is a significant proportion of peers that now sit behind NAT devices, preventing direct communication without specific mechanisms. In this paper, we propose a NAT-resilient gossip peer sampling protocol called Nylon, that accounts for the presence of NATs. Nylon is fully decentralized and spreads evenly among peers the extra load caused by the presence of NATs. Nylon ensures that a peer can always communicate with any peer in its sample. This is achieved through a simple, yet efficient mechanism, establishing a path of relays between peers. Our results show that the randomness of the generated samples is preserved, and that the connectivity is not impacted even in the presence of high churn and a high ratio of peers sitting behind NAT devices.
[Context, NAT, Protocols, peer-to-peer computing, Nylon, peer sampling, Laboratories, NAT-resilient gossip peer sampling protocol, gossip protocol, Relays, Distributed computing, network address translator, peer to peer overlay networks, NAT devices, Sampling methods, Solids, Robustness, protocols, Network address translation]
Fault-Tolerant Consensus in Unknown and Anonymous Networks
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
This paper investigates under which conditions information can be reliably shared and consensus can be solved in unknown and anonymous message-passing networks that suffer from crash-failures. We provide algorithms to emulate registers and solve consensus under different synchrony assumptions. For this, we introduce a novel pseudo leader-election approach which allows a leader-based consensus implementation without breaking symmetry.
[Protocols, message passing, fault-tolerant consensus, message passing networks, Computer crashes, Electronic switching systems, Distributed computing, Fault tolerance, Wireless sensor networks, anonymous networks, Message passing, Fault tolerant systems, unknown networks, Detectors, Hardware, fault tolerant computing]
A Practical Study of Regenerating Codes for Peer-to-Peer Backup Systems
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
In distributed storage systems, erasure codes represent an attractive solution to add redundancy to stored data while limiting the storage overhead. They are able to provide the same reliability as replication requiring much less storage space. Erasure coding breaks the data into pieces that are encoded and then stored on different nodes. However, when storage nodes permanently abandon the system, new redundant pieces must be created. For erasure codes, generating a new piece requires the transmission of k pieces over the network, resulting in a k times higher reconstruction traffic as compared to replication. Dimakis proposed a new class of codes, called regenerating codes, which are able to provide both the storage efficiency of erasure codes and the communication efficiency of replication. However, Dimakis gave only a theoretical description of the codes without discussing implementation issues or computational costs. We have done a real implementation of random linear regenerating codes that allows us to measure their computational cost, which can be significant if the parameters are not chosen properly. However, we also find that there exist parameter values that result in a significant reduction of the communication overhead at the expense of a small increase in storage cost and computation, which makes these codes very attractive for distributed storage systems.
[Evaluation, Costs, linear codes, peer-to-peer computing, Peer to peer computing, random linear regenerating code, Redundancy, Telecommunication traffic, Backup Systems, erasure coding, distributed storage system, Peer-to-Peer, Distributed computing, random codes, storage management, Storage, peer-to-peer backup system, Bandwidth, Regenerating Codes, Computational efficiency, Personal communication networks, Erasure Codes]
CARP: Handling Silent Data Errors and Site Failures in an Integrated Program and Storage Replication Mechanism
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
This paper presents CARP, an integrated program and storage replication solution. CARP extends program replication systems which do not currently address storage errors, builds upon a record-and-replay scheme that handles nondeterminism in program execution, and uses a scheme based on recorded program state and I/O logs to enable efficient detection of silent data errors and efficient recovery from such errors. CARP is designed to be transparent to applications with minimal run-time impact and is general enough to be implemented on commodity machines. We implemented CARP as a prototype on the Linux operating system and conducted extensive sensitivity analysis of its overhead with different application profiles and system parameters. In particular, we evaluated CARP with standard unmodified email, database, and web server benchmarks and showed that it imposes acceptable overhead while providing sub-second program state recovery times on detecting a silent data error.
[recorded program state, silent data error handling, CARP, sensitivity analysis, record-and-replay scheme, Electronic mail, Distributed computing, program replication system, storage management, Runtime, Databases, Operating systems, Prototypes, Hardware, integrated program, Web server, storage replication mechanism, nondeterminism, Sensitivity analysis, program execution, Storage Replcation, program state recovery, Program Replication, Silent Data Error, Linux, commodity machines, Site Failure, Linux operating system, data handling]
A Commutative Replicated Data Type for Cooperative Editing
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
A commutative replicated data type (CRDT) is one where all concurrent operations commute. The replicas of a CRDT converge automatically, without complex concurrency control. This paper describes Treedoc, a novel CRDT design for cooperative text editing. An essential property is that the identifiers of Treedoc atoms are selected from a dense space. We discuss practical alternatives for implementing the identifier space based on an extended binary tree. We also discuss storage alternatives for data and meta-data, and mechanisms for compacting the tree. In the best case, Treedoc incurs no overhead with respect to a linear text buffer. We validate the results with traces from existing edit histories.
[cooperative text editing, commutative replicated data type, extended binary tree, Compaction, tree compacting, History, Distributed computing, Delay, Convergence, Concurrent computing, storage management, groupware, concurrent operations, Automatic control, dense identifier space, replicated data, co-operative editing, metadata storage, text editing, trees (mathematics), Treedoc, Concurrency control, distributed algorithms, concurrency control, Binary trees, identifier space, Writing]
Logoot: A Scalable Optimistic Replication Algorithm for Collaborative Editing on P2P Networks
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Massive collaborative editing becomes a reality through leading projects such as Wikipedia. This massive collaboration is currently supported with a costly central service. In order to avoid such costs, we aim to provide a peer-to-peer collaborative editing system. Existing approaches to build distributed collaborative editing systems either do not scale in terms of number of users or in terms of number of edits. We present the Logoot approach that scales in these both dimensions while ensuring causality, consistency and intention preservation criteria. We evaluate the Logoot approach and compare it to others using a corpus of all the edits applied on a set of the most edited and the biggest pages of Wikipedia.
[Costs, peer-to-peer computing, Scalability, Collaborative tools, Wikipedia, Control systems, International collaboration, scalable optimistic replication algorithm, peer-to-peer collaborative editing system, Distributed computing, Logoot, P2P networks, Radiofrequency interference, Convergence, P2P, Collaborative Editing, distributed collaborative editing systems, Web sites, Online Communities/Technical Collaboration]
ISP Friend or Foe? Making P2P Live Streaming ISP-Aware
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Current peer-to-peer systems are network-agnostic, often generating large volumes of unnecessary inter-ISP traffic. Although recent work has shown the benefits of ISP-awareness on bulk transfer applications, no studies have focused on optimizing P2P live streaming systems. These are harder to design, as data must be diffused to all receivers within short delays. In this paper we propose a novel scheme for ISP-friendly mesh-based live streaming. Each peer maintains two distinct sets of overlay neighbors, used respectively for local and global stream propagation. A dynamic unchoke mechanism minimizes inter-ISP traffic in normal operation, enabling it promptly when local diffusion is impaired, e.g., when fast local sources become suddenly unavailable. Our scheme is independent of the chunk scheduling algorithm, and thus can be applied to a wide range of existing systems. We have integrated our ISP-friendly scheme to our P2P live streaming prototype, and evaluated its performance through emulation and Planetlab experiments. Our results show that our scheme adapts quickly to churn and dynamic network conditions, and achieves up to a ten-fold reduction in transit traffic.
[isp-friendly, peer-to-peer computing, Peer to peer computing, Internet service provider, Telecommunication traffic, chunk scheduling algorithm, Electronic mail, Distributed computing, Delay, Scheduling algorithm, peer-to-peer, streaming, Network topology, Emulation, mesh-based live streaming, Prototypes, peer-to-peer system, video streaming, dynamic unchoke mechanism, Internet, P2P live streaming system, telecommunication traffic, interISP traffic]
A Case Study of Traffic Locality in Internet P2P Live Streaming Systems
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
With the ever-increasing P2P Internet traffic, recently much attention has been paid to the topology mismatch between the P2P overlay and the underlying network due to the large amount of cross-ISP traffic. Mainly focusing on BitTorrent-like file sharing systems, several recent studies have demonstrated how to efficiently bridge the overlay and the underlying network by leveraging the existing infrastructure, such as CDN services or developing new application-ISP interfaces, such as P4P. However, so far the traffic locality in existing P2P live streaming systems has not been well studied. In this work, taking PPLive as an example, we examine traffic locality in Internet P2P streaming systems. Our measurement results on both popular and unpopular channels from various locations show that current PPLive traffic is highly localized at the ISP level. In particular, we find: (1) a PPLive peer mainly obtains peer lists referred by its connected neighbors (rather than tracker servers) and up to 90% of listed peers are from the same ISP as the requesting peer; (2) the major portion of the streaming traffic received by a requesting peer (up to 88% in popular channels) is served by peers in the same ISP as the requestor; (3) the top 10\\% of the connected peers provide most (about 70%) of the requested streaming data and these top peers have smaller RTT to the requesting peer. Our study reveals that without using any topology information or demanding any infrastructure support, PPLive achieves such high ISP level traffic locality spontaneously with its decentralized, latency based, neighbor referral peer selection strategy. These findings provide some new insights for better understanding and optimizing the network- and user-level performance in practical P2P live streaming systems.
[PPLive traffic, Telecommunication traffic, cross-ISP traffic, traffic locality, P2P streaming, Distributed computing, Delay, P2P Internet traffic, Network topology, application-ISP interfaces, neighbor referral peer selection strategy, media streaming, Communication system traffic control, Internet P2P live streaming systems, IP networks, peer-to-peer computing, Peer to peer computing, telecommunication network topology, BitTorrent-like file sharing systems, Application software, topology mismatch, ISP level traffic locality, Internet, Resource management, telecommunication traffic]
EUL: An Efficient and Universal Localization Method for Wireless Sensor Network
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Localization is a crucial service for various applications in wireless sensor networks (WSNs). Although most researches assume stationary nodes, sensor mobility can enrich the application scenarios. Existing dynamic localization approaches require high seed density or incur a large communication overhead. In order to address these problems, we propose an efficient rang-free localization algorithm, EUL, which utilizes the relationship between neighboring nodes to estimate their possible location boundaries. Our algorithm not only allows all the nodes to remain static or move freely but also reduces the dependence on seeds, which achieves a uniform energy distribution to address the excessive energy drain around seeds and lengthen the network lifetime. We have evaluated EUL together with other major dynamic localization approaches. Simulation results show that EUL outperforms existing approaches in terms of accuracy under many different mobility conditions.
[Energy consumption, Costs, Event detection, wireless sensor networks, Context awareness, wireless sensor network, Mobility, mobility management (mobile radio), Application software, efficient universal localization, Distributed computing, Computer science, Low energy, Wireless sensor networks, sensor mobility, Collaboration, stationary nodes, Sensor networks, Computational efficiency, location boundaries, Localization, uniform energy distribution]
CLIQUE: Role-Free Clustering with Q-Learning for Wireless Sensor Networks
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Clustering and aggregation inherently increase wireless sensor network (WSN) lifetime by collecting information within a cluster at a cluster head, reducing the amount of data through computation, then forwarding it. Traditional approaches, however, both spend extensive communication energy to identify the cluster heads and are inflexible to network dynamics such as those arising from sink mobility, node failure, or dwindling battery reserves. This paper presents CLIQUE, an approach for data clustering that saves cluster head selection energy by using machine learning to enable nodes to independently decide whether or not to act as a cluster head on a per-packet basis. We refer to this lack of actual cluster head assignment as being role-free, and demonstrate through simulations that, when combined with learning dynamic network properties such as battery reserves, up to 25% less energy is consumed in comparison to a traditional, random cluster head selection approach.
[Q-learning, wireless sensor networks, role-free clustering, WSN lifetime, CLIQUE approach, wireless sensor network, Batteries, cluster heads identification, data aggregation, Distributed computing, telecommunication computing, reinforcement learning, Clustering algorithms, Spread spectrum communication, telecommunication network reliability, learning (artificial intelligence), Informatics, Base stations, role-free, Routing, Magnetic heads, machine learning, Wireless sensor networks, energy-efficient, Machine learning, clustering, q-learning]
QVS: Quality-Aware Voice Streaming for Wireless Sensor Networks
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Recent years have witnessed the pilot deployments of audio or low-rate video wireless sensor networks for a class of mission-critical applications including search and rescue, security surveillance, and disaster management. In this paper, we report the design and implementation of Quality-aware Voice Streaming (QVS) for wireless sensor networks. QVS is built upon SenEar, a new sensor hardware platform we developed for high-bandwidth wireless audio communication. QVS comprises several novel components, which include an empirical model for online voice quality evaluation and control, dynamic voice compression/duplication adaptation for lossy wireless links, and distributed stream admission control that exploits network capacity for rate allocation. We have extensively tested QVS on a 20-node network deployment. Our experimental results show that QVS delivers satisfactory voice quality under a range of realistic settings while achieving high network capacity utilization.
[wireless sensor networks, disaster management, wireless audio communication, Mission critical systems, security surveillance, Sensor Networks, Communication system control, voice quality evaluation, audio streaming, Communication system security, Admission Control, Voice Streaming, Wireless communication, dynamic voice compression, Wireless sensor networks, quality-aware voice streaming, Surveillance, Admission control, Streaming media, Hardware, duplication adaptation, Testing]
TDMA-ASAP: Sensor Network TDMA Scheduling with Adaptive Slot-Stealing and Parallelism
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
TDMA has been proposed as a MAC protocol for wireless sensor networks (WSNs) due to its efficiency in high WSN load. However, TDMA is plagued with shortcomings; we present modifications to TDMA that will allow for the same efficiency of TDMA, while allowing the network to conserve energy during times of low load (when there is no activity being detected). Recognizing that aggregation plays an essential role in WSNs, TDMA-ASAP adds to TDMA: (a) transmission parallelism based on a level-by-level localized graph-coloring, (b) appropriate sleeping between transmissions (ldquonappingrdquo), (c) judicious and controlled TDMA slot stealing to avoid empty slots to be unused and (d) intelligent scheduling/ordering transmissions. Our results show that TDMA-ASAP's unique combination of TDMA, slot-stealing, napping, and message aggregation significantly outperforms other hybrid WSN MAC algorithms and has a performance that is close to optimal in terms of energy consumption and overall delay.
[transmission parallelism, Energy consumption, wireless sensor networks, graph theory, wireless sensor network, sensor networks, access protocols, Delay, Time division multiple access, message aggregation, Parallel processing, scheduling, adaptive slot-stealing-and-parallelism, energy consumption, level-by-level localized graph-coloring, TDMA, ordering transmission, TDMA-ASAP scheduling, Access protocols, Synchronization, MAC, Adaptive scheduling, Wireless sensor networks, WSN, sensors, time division multiple access, MAC protocol, Media Access Protocol, intelligent scheduling, Clocks]
A Note on Distributed Stable Matching
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
We consider the distributed complexity of the stable marriage problem. In this problem, the communication graph is undirected and bipartite, and each node ranks its neighbors. Given a matching of the nodes, a pair of nodes is called blocking if they prefer each other to their assigned match. A matching is called stable if it does not induce any blocking pair. In the distributed model, nodes exchange messages in each round over the communication links, until they find a stable matching. We show that if messages may contain at most B bits each, then any distributed algorithm that solves the stable marriage problem requires Omega (radicn/B log n) communication rounds in the worst case, even for graphs of diameter Theta (log n), where n is the number of nodes in the graph. Furthermore, the lower bound holds even if we allow the output to contain O(radicn) blocking pairs. We also consider epsiv-stability, where a pair is called epsiv-blocking if they can improve the quality of their match by more than an epsilon fraction, for some 0 les e les 1. Our lower bound extends to epsiv-stability where epsiv is arbitrarily close to 1/2. We also present a simple distributed algorithm for epsiv-stability whose time complexity is O(n/epsiv).
[pattern matching, Robust stability, graph theory, Switches, distrubuted algorithms, time complexity, Educational institutions, Nash equilibrium, bipartite graph, Distributed computing, Game theory, stable matching, Communication standards, distributed algorithm, worst case analysis, distributed algorithms, stable marriage, Bipartite graph, Books, undirected graph, communication, Distributed algorithms, stable marriage problem, computational complexity]
Communication Efficiency in Self-Stabilizing Silent Protocols
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
In this paper, our focus is to lower the communication complexity of self-stabilizing protocols below the need of checking every neighbor forever. Our contribution is threefold: (i) We provide new complexity measures for communication efficiency of self-stabilizing protocols, especially in the stabilized phase or when there are no faults, (ii) On the negative side, we show that for non-trivial problems such as coloring, maximal matching, and maximal independent set, it is impossible to get (deterministic or probabilistic) self-stabilizing solutions where every participant communicates with less than every neighbor in the stabilized phase, and (iii) On the positive side, we present protocols for maximal matching and maximal independent set such that a fraction of the participants communicates with exactly one neighbor in the stabilized phase.
[Context, Protocols, Phase measurement, self-stabilization, self-stabilizing silent protocol, maximal independent set, Complexity theory, communication complexity, communication-efficiency, Distributed computing, maximal matching, Information science, silent algorithms, Fault tolerant systems, System recovery, Particle measurements, Global communication, protocols, communication efficiency]
On the Impossibility of Maximal Scheduling for Strong Fairness with Interleaving
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
A strongly fair schedule is one in which tasks that are enabled infinitely often are also executed infinitely often. When tasks execute atomically, a strongly fair scheduler can be implemented in a maximal manner. That is, an algorithm exists that, for any valid schedule, is capable of generating that schedule. We show that this assumption of atomicity is necessary. That is, when task execution can be interleaved with other tasks, no algorithm is capable of generating all valid schedules. In other words, any algorithm that correctly generates some strongly fair schedules must also be incapable of generating some other valid schedules. This impossibility result is the first example of an implementable UNITY specification for which no maximal solution exists.
[maximal scheduling, System testing, Protocols, UNITY specification, task execution interleavING, Distributed computing, formal specification, Scheduling algorithm, Concurrent computing, Computer science, Processor scheduling, Operating systems, TCPIP, scheduling, Interleaved codes, strongly fair schedule]
Pushing the Envelope: Extreme Network Coding on the GPU
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
While it is well known that network coding achieves optimal flow rates in multicast sessions, its potential for practical use has remained to be a question, due to its high computational complexity. With GPU computing gaining momentum as a result of increased hardware capabilities and improved programmability, we show in this paper how the GPU can be used to improve network coding performance dramatically. Our previous work presented the first attempt in the literature to maximize the performance of network coding by taking advantage of not only multi-core CPUs, but also hundreds of computing cores in commodity off-the-shelf Graphics Processing Units (GPU). This paper represents another step forward, and presents a new array of GPU-based algorithms that improve network encoding by a factor of 2.2, and network decoding by a factor of 2.7 to 27.6 across a range of practical configurations. With just a single NVIDIA GTX 280 GPU, our implementation of GPU-based network encoding outperforms an 8-core Intel Xeon server by a margin of at least 4.3 to 1 in all practical test cases, and over 3000 peers can be served at high-quality video rates if network coding is used in a streaming server. With 128 blocks, for example, coding rates up to 294 MB/second can be achieved with a variety of block sizes.
[commodity off-the-shelf graphics processing units, network coding, optimal flow rates, computer graphic equipment, microprocessor chips, Encoding, Decoding, encoding, Computational complexity, Graphics, Network servers, 8-core Intel Xeon server, Network coding, Streaming media, Computer networks, Hardware, performance optimization, GPU computing, Testing, NVIDIA GTX 280]
Stochastic Multicast with Network Coding
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
The usage of network resources by content providers is commonly governed by service level agreements (SLA) between the content provider and the network service provider. Resource usage exceeding the limits specified in the SLA incurs the content provider additional charges, usually at a higher cost. Hence, the content provider's goal is to provision adequate resources in the SLA based on forecasts of future demand. We study capacity purchasing strategies in this setting when the content provider employs network coded multicast as the data delivery mechanism. We model this problem as a two-stage stochastic optimization problem with recourse, and we design two approximation algorithms to solve such problems. The first is a heuristic that exploits properties unique to network coding. It performs well in general scenarios, but may be unbounded with respect to the optimal solution in the worst case. This motivates our second approach, a sampling algorithm partly inspired from the work of Gupta et al. (2004). We employ techniques from duality theory in linear optimization to prove that sampling provides a 3-approximate solution to the stochastic multicast problem. We conduct simulations to illustrate the efficacy of both algorithms, and show that the performance of both is usually within 10% of the optimal solution in practice.
[Algorithm design and analysis, Costs, network service provider, Stochastic processes, Demand forecasting, stochastic programming, linear optimization, linear programming, stochastic optimization, stochastic multicast, Multicast, duality theory, resource allocation, Bandwidth, multicast communication, approximation algorithm, duality (mathematics), capacity purchasing, Linear Programming, approximation theory, network coding, Stochastic Optimization, resource usage, capacity management (computers), service level agreement, sampling algorithm, Multicast algorithms, content provider, Network Coding, Network coding, Streaming media, network resources, Approximation algorithms, Sampling methods, Network Optimization, Internet, data delivery]
High-Speed Flow Nature Identification
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
This paper concerns the fundamental problem of identifying the content nature of a flow, namely text, binary, or encrypted, for the first time. We propose Iustitia, a tool for identifying flow nature on the fly. The key observation behind Iustitia is that text flows have the lowest entropy and encrypted flows have the highest entropy, while the entropy of binary flows stands in between. The basic idea of Iustitia is to classify flows using machine learning techniques where a feature is the entropy of every certain number of consecutive bytes. The key features of Iustitia are high speed (10% of average packet inter-arrival time) and high accuracy (86%).
[text analysis, Flow identification, flow classification, Telecommunication traffic, text flow, Entropy, machine learning, high-speed flow nature identification, Iustitia, Law enforcement, Intrusion detection, Machine learning, Feature extraction, Internet, encrypted flows, Cryptography, learning (artificial intelligence), Monitoring, encrypted flow, Payloads]
Locality-Preserving Clustering and Discovery of Wide-Area Grid Resources
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
In large-scale computational or P2P grids, discovery of heterogeneous resources as a working group is crucial to achieving scalable performance. This paper presents a hierarchical cycloid overlay (HCO) architecture with resource clustering and discovery algorithms for efficient and robust resource discovery in wide-area distributed grid systems. We establish program/data locality by clustering resources based on their physical proximity and functional matching with user applications. We further develop randomized probing and cluster-token forwarding algorithms. The novelty of the HCO scheme lies in low overhead, fast speed and dynamism resilience in multi-resource discovery. The paper presents the HCO framework, new performance metrics, and simulation experimental results. This HCO scheme compares favorably with other resource management methods in static and dynamic grid applications. In particular, it supports efficient resource clustering, reduces communications cost, and enhances resource discovery success rate in promoting large-scale distributed supercomputing applications.
[workstation clusters, Scalability, locality-preserving clustering, grid computing, physical proximity, resource clustering, randomized probing, Distributed computing, wide-area grid resources discovery, data locality, Clustering algorithms, Computer architecture, wide-area distributed grid systems, functional matching, Grid computing, resource management methods, Large-scale systems, software performance evaluation, heterogeneous resources discovery, scalable performance, peer-to-peer computing, large-scale distributed supercomputing applications, cluster-token forwarding algorithms, Topology, Application software, Computer science, P2P grids, performance metrics, Resource management, hierarchical cycloid overlay architecture, software metrics]
MOPS: Providing Content-Based Service in Disruption-Tolerant Networks
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Content-based service, which dynamically routes and delivers events from sources to interested users, is extremely important to network services. However, existing content-based protocols for static networks will incur unaffordable maintenance costs if they are applied directly to the highly mobile environment that is featured in disruption-tolerant networks (DTNs). In this paper, we propose a unique publish/subscribe scheme that utilizes the long-term social network properties, which are observed in many DTNs, to facilitate content-based services in DTNs. We distributively construct communities based on the neighboring relationships from nodes' encounter histories. Brokers are deployed to bridge the communities, and they adopt a locally prioritized pub/sub scheme which combines the structural importance with subscription interests, to decide what events they should collect, store, and propagate. Different trade-offs for content-based service can be achieved by tuning the closeness threshold in community formation or by adjusting the broker-to-broker communication scheme. Extensive real-trace and synthetic-trace driven simulation results are presented to support the effectiveness of our scheme.
[Disruption tolerant networking, subscription interests, MOPS, Costs, Protocols, Subscriptions, disruption-tolerant networks, Distributed computing, Delay, mobile computing, localized algorithms, Network topology, content-based protocols, middleware, publish/subscribe, Disruption-tolerant networks (DTNs), Social network services, content-based service, publish/subscribe scheme, computer networks, mobile environment, network services, social network property, broker-to-broker communication scheme, Bridges, Computer science, static networks, social network analysis, unaffordable maintenance costs]
FlashLite: A User-Level Library to Enhance Durability of SSD for P2P File Sharing
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Peer-to-peer file sharing is popular, but it generates random write traffic to storage due to the nature of swarming. NAND flash memory based solid-state drive (SSD) technology is available as an alternative to hard drives for notebook and tablet PCs. As it turns out, random write is extremely detrimental to the lifetime of SSD drives. This paper focuses on the following problem, namely, P2P file downloading when the target of the download is an SSD drive. We make three contributions: first, analysis of write patterns of downloading program to establish the premise of the problem; second, development of a simple yet powerful technique called FlashLite to combat this problem, by automatically converting the random writes to sequential writes; third, showing through performance evaluation using modified eMule file downloading program that FlashLite does change random writes to sequential, and most importantly eliminates about 94% of erase operations of the original eMule program.
[solid-state drive technology, NAND flash memory, peer-to-peer computing, Peer to peer computing, eMule file downloading program, SSD, Random access memory, durability, Educational institutions, Solid state circuits, Distributed computing, P2P, FlashLite, flash memories, P2P file downloading, Libraries, Robustness, peer-to-peer file sharing, Performance analysis, NAND circuits, user-level library, Personal communication networks, Pattern analysis]
Better Architectures and New Security Applications for Network Monitoring
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Busy networks today cannot afford to log all traffic traversing them, and consequently many network-monitoring applications make due with coarse traffic summaries. In this talk we will describe an approach we have developed to improve the fidelity of these traffic summaries, by coordinating the monitoring performed by the network's routers so as to achieve network-wide monitoring goals while respecting each router's processing constraints. We will also describe our use of traffic summaries to detect a variety of stealthy network abuses&#x2014;e.g., file-sharing traffic masquerading on other application ports, "hit-list" scans and malware propagation, data exfiltration by spyware, and botnet command-and-control traffic&#x2014;and even to identify the origin of epidemic malware spreads.
[Computer science, Biographies, Information security, Telecommunication traffic, Computer architecture, Communication system security, Application software, Distributed computing, Computer security, Monitoring]
Explicit Batching for Distributed Objects
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Although distributed object systems, including RMI and CORBA, enable object-oriented programs to be easily distributed across a network, achieving acceptable performance usually requires client-specific optimization of server interfaces, making such systems difficult to maintain and evolve. Automatic optimization techniques, including Batched Futures and Communication Restructuring, do not work as well as hand optimization. This paper presents Batched Remote Method Invocation (BRMI), a language-level technique for clients to specify explicit batches of operations on remote objects. We have implemented BRMI for Java as an extension of RMI, with support for batches with array cursors, custom exception handling, conditionals and loops. BRMI allows common design patterns, including Data Transfer Objects and Remote Object Facade, to be constructed on the fly by clients. The performance benefits of batching operations are well known; our evaluation focuses on the usability of explicit batches, but we also confirm that BRMI outperforms RMI and scales significantly better when clients make multiple remote calls. The applicability of BRMI is demonstrated by rewriting third-party RMI client applications to use BRMI.
[distributed object systems, application program interfaces, programming abstractions, exception handling, Mobile communication, Distributed computing, Delay, Design optimization, data transfer objects, Network servers, explicit batching, object-oriented programs, language-level technique, automatic optimization techniques, distributed object management, middleware, Java, Object oriented modeling, Middleware, batching, RMI, Computer science, CORBA, Remote Object Facade, client-specific optimization, batched futures and communication restructuring, distributed objects, custom exception handling, batched remote method invocation, Usability]
Minimizing the Hidden Cost of RDMA
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Remote Direct Memory Access (RDMA) is a mechanism whereby data is moved directly between the application memory of the local and remote computer. In bypassing the operating system, RDMA significantly reduces the CPU cost of large data transfers and eliminates intermediate copying across buffers, thereby making it very attractive for implementing distributed applications. With the advent of hardware implementations of RDMA over Ethernet (iWARP), its advantages have become even more obvious. In this paper we analyze the applicability of RDMA and identify hidden costs in the setup of its interactions that, if not handled carefully, remove any performance advantage, especially in hardware implementations. From an application point of view, the major difference to TCP/IP based communication is that the buffer management has to be done explicitly by the application. Without the proper optimizations, RDMA loses all its advantages. We discuss the problem in detail, analyze what applications can profit from RDMA, present a number of optimization strategies, and show through extensive performance experiments that these optimizations make a substantial difference in the overall performance of RDMA based applications.
[iWARP, Costs, Ethernet networks, Switches, remote direct memory access, local area networks, Distributed computing, distributed applications, Delay, large data transfers, Buffer Management, Operating systems, TCPIP, Application Suitability, Hardware, Performance analysis, RDMA, magnetic disc storage, buffer storage, buffers, Application software, transport protocols, buffer management, Ethernet, TCP/IP based communication, CPU cost, Performance, optimizations]
Autonomous Resource Selection for Decentralized Utility Computing
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Many large-scale utility computing infrastructures comprise heterogeneous hardware and software resources. This raises the need for scalable resource selection services, which identify resources that match application requirements, and can potentially be assigned to these applications. We present a fully decentralized resource selection algorithm by which resources autonomously select themselves when their attributes match a query. An application specifies what it expects from a resource by means of a conjunction of (attribute, value-range) pairs, which are matched against the attribute values of resources. We show that our solution scales in the number of resources as well as in the number of attributes, while being relatively insensitive to churn and other membership changes such as node failures.
[Costs, distributed processing, utility programs, autonomous resource selection, Distributed computing, application requirements, resource allocation, decentralized utility computing, fully decentralized resource selection, Utility computing, Grid computing, Epidemic protocols, Hardware, Large-scale systems, Fluctuations, Cooling, Peer to peer computing, software resources, Resource selection, Application software, attribute values, large-scale utility computing, Resource management, heterogeneous hardware, scalable resource selection services]
Information Value-Driven Near Real-Time Decision Support Systems
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
In this paper, we focus on challenges of supporting a decision support system (DSS) based on a hybrid approach (i.e. a federation system with data placement) for agile business intelligence applications. A DSS needs to be designed to handle a workload of potentially complex queries for important decision-making processes. The response time requirement (and a realistic goal) for such a DSS is near real time. The users of a DSS care about not only the response time but also the time stamp of the business operation report since both of them introduce uncertainty and risks to business decision-making. In our proposed DSS, each report is assigned with a business value; denoting its importance to business decision-making. An information value (IV) is a business value of a report discounted by time to reflex the uncertainty and risks associated with the computational latency and synchronization latency. We propose a novel information value-driven query processing (IVQP) framework specific for near real time DSS applications. The framework enables dynamic query plan selection by taking into account of information value and adaptation for online-arrival ad hoc queries. The framework works with single query as well as a workload of queries. The experimental results based on synthetic data and TPC-H show the effectiveness of our approach in achieving optimal information values for the workloads.
[Real time systems, Decision support systems, risk management, information value-driven query processing, response time requirement, Uncertainty, online-arrival ad-hoc query, Decision making, real-time decision support system, Companies, Financial management, competitive intelligence, Environmental management, Delay, decision support systems, query processing, Query processing, business decision-making process, agile business intelligence application, real-time systems, decision making, Business]
Protecting Neighbor Discovery Against Node Compromises in Sensor Networks
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
The neighborhood information has been frequently used by protocols such as routing in sensor networks. Many methods have been proposed to protect such information in hostile environments. However, these methods can only protect neighbor relations between benign nodes. A compromised node can easily circumvent them and setup false neighbor relations with sensor nodes in many places, impacting the network at a large scale. This paper presents a theoretic model for neighbor discovery in sensor networks and describes a fundamental security limitation and a generic attack against this model. The paper then proposes an efficient and localized solution based on a security property achievable during sensor deployment. This technique provides a threshold security guarantee in dealing with compromised sensor nodes. The analytical and simulation studies show that the technique is practical and effective for sensor networks.
[telecommunication security, Base stations, wireless sensor networks, Laboratories, neighbor discovery protection, Sensor phenomena and characterization, telecommunication network topology, Sensor systems, sensor networks, Distributed computing, security property, secure neighbor discovery, node compromises, Analytical models, fundamental security limitation, Clustering algorithms, telecommunication network routing, Routing protocols, Large-scale systems, Protection, threshold security guarantee]
The Digital Marauder's Map: A New Threat to Location Privacy
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
"The Marauder's Map" is a magical map in J. K. Rowling's fantasy series, "Harry Potter and the Prisoner of Azkaban". It shows all moving objects within the boundary of the "Hogwarts School of Witchcraft and Wizardry". In this paper, we introduce a similar attack to location privacy in wireless networks. Our system, namely the digital Marauder's map, can reveal the locations of WiFi-enabled mobile devices within the coverage area of a single high-gain antenna. The digital Marauder's map is built solely with off-the-shelf wireless equipments, and features a mobile design that can be quickly deployed to a new location and instantly used without training. We present a comprehensive set of theoretical analysis and experimental results which demonstrate the coverage and localization accuracy of the digital Marauder's map.
[telecommunication security, Low-noise amplifiers, Positioning, antennas, digital Marauder map, Telecommunication traffic, Mobile antennas, wireless networks, Attack, Distributed computing, Privacy, WiFi-enabled mobile devices, Wireless networks, location privacy, Land mobile radio cellular systems, Training data, Signal generators, Location Privacy, wireless LAN, high-gain antenna, Monitoring]
Characterization and Solution to a Stateful IDS Evasion
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
We identify a new type of stateful IDS evasion, named signature evasion. We formalize the signature evasion on those Stateful IDSs whose state can be modeled using Deterministic Finite State Automata (DFAs). We develop an efficient algorithm which operates on rule set DFAs and derives a minimal rectification of evasive paths. Finally, we evaluate our solution on Snort signatures, identify and rectify existing vulnerable flowbit rule sets.
[Intrusion Detection, Protocols, Regular automata, Reverse engineering, signature evasion, deterministic finite state automata, Doped fiber amplifiers, File servers, set theory, finite state machines, Distributed computing, stateful IDS evasion, IDS evasion, intrusion detection system, deterministic automata, rule set, Intrusion detection, Automata, Lead, Signal processing, digital signatures, Protection, Signature matching]
Down the Block and Around the Corner The Impact of Radio Propagation on Inter-vehicle Wireless Communication
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Vehicular networks are emerging as a new distributed system environment with myriad possible applications. Most studies on vehicular networks are carried out via simulation, given the logistical and economical problems with large-scale deployments. This paper investigates the impact of realistic radio propagation settings on the evaluation of VANET-based systems. Using a set of instrumented cars, we collected IEEE 802.11b signal propagation measurements between vehicles in a variety of urban and suburban environments. We found that signal propagation between vehicles varies in different settings, especially between line-of-sight ("down the block") and non line-of-sight ("around the corner") communication in the same setting. Using a probabilistic shadowing model, we evaluate the impact of different parameter settings on the performance of an epidemic data dissemination protocol and discuss the implications of our findings. We also suggest a variation of a basic signal propagation model that incorporates additional realism without sacrificing scalability by taking advantage of environmental information, including node locations and street information.
[inter-vehicle wireless communication, Protocols, modeling, Scalability, vanets, Environmental economics, Vehicles, Wireless communication, non line-of-sight communication, Radio propagation, vehicular ad hoc networks, mobile radio, probabilistic shadowing model, Instruments, Peer to peer computing, signal propagation, VANET, Ad hoc networks, IEEE 802.11b signal propagation measurements, radiowave propagation, Shadow mapping, telecommunication standards, epidemics, ad hoc networks, radio propagation]
Deterministic Replay for Transparent Recovery in Component-Oriented Middleware
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
We present and evaluate a low-overhead approach for achieving high-availability in distributed event-processing middleware systems consisting of networks of stateful software components that communicate by either one-way (send) or two-way (call) messages. The approach is based on transparently augmenting each component to produce a deterministic component whose state can be recovered by checkpoint and replay. Determinism is achieved by augmenting messages with virtual times, and by scheduling message handling in virtual time order. Scheduling delays are reduced by computing virtual times with estimators: deterministic functions that approximate the expected real times of arrival. We describe our algorithms, show how Java components can be transparently augmented with checkpointing code and with good estimators, discuss how our deterministic runtime can be tuned to reduce overhead, and provide experimental results to measure the overhead of determinism relative to non-determinism.
[Checkpointing, checkpointing, checkpointing code, Discrete event simulation, scheduling message handling, recovery, Distributed computing, transparent recovery, software component, Runtime, virtual time, USA Councils, Java component, scheduling, distributed event processing middleware system, determinism, Logic, middleware, Java, message passing, object-oriented programming, Delay estimation, component-oriented middleware, replay, Middleware, high availability, Processor scheduling, component oriented middleware, distributed algorithms]
Modeling Probabilistic Measurement Correlations for Problem Determination in Large-Scale Distributed Systems
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
With the growing complexity in computer systems, it has been a real challenge to detect and diagnose problems in today's large-scale distributed systems. Usually, the correlations between measurements collected across the distributed system contain rich information about the system behaviors, and thus a reasonable model to describe such correlations is crucially important in detecting and locating system problems. In this paper, we propose a transition probability model based on Markov properties to characterize pair-wise measurement correlations. The proposed method can discover both the spatial (across system measurements) and temporal (across observation time) correlations, and thus such a model can successfully represent the system normal profiles. Problem determination and localization under this framework is fast and convenient. The framework is general enough to discover any types of correlations (e.g. linear or non-linear). Also, model updating, system problem detection and diagnosis can be conducted effectively and efficiently. Experimental results show that, the proposed method can detect the anomalous events and locate the problematic sources by analyzing the real monitoring data collected from three companies' infrastructures.
[Event detection, probabilistic measurement correlations, system problem diagnosis, Markov properties, distributed processing, Time measurement, large-scale distributed systems, Distributed computing, Modeling, large-scale systems, transition probability model, Operating systems, Distributed databases, computer systems, National electric code, Markov processes, system problem detection, Particle measurements, problem determination, Large-scale systems, Monitoring]
Collaboration-Oriented Data Recovery for Mobile Disk Arrays
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Mobile disk arrays, disk arrays located in mobile data centers, are crucial for mobile applications such as disaster recovery. Due to their unusual application domains, mobile disk arrays face several new challenges including harsh operating environments, very limited power supply, and extremely small number of spare disks. Consequently, data reconstruction schemes for mobile disk arrays must be performance-driven, reliability-aware, and energy-efficient. In this paper, we develop a flash assisted data reconstruction strategy called CORE (collaboration-oriented reconstruction) on top of a hybrid disk array architecture, where hard disks and flash disks collaborate to shorten data reconstruction time, alleviate performance degradation during disk recovery. Experimental results demonstrate that CORE noticeably improves the performance and energy-efficiency over existing schemes.
[flash disks, disk access pattern, hard disks, data reconstruction, flash disk, Reconstruction algorithms, disaster recovery, disc storage, data reconstruction time, hard disk, hard discs, Degradation, mobile computing, mobile disk array, Computer architecture, groupware, spare disks, collaboration-oriented data recovery, Web server, flash assisted data reconstruction strategy, disk recovery, data reconstruction schemes, mobile data centers, Power supplies, limited power supply, reliability-aware, hybrid disk array architecture, collaboration-oriented reconstruction, performance degradation, Application software, energy-efficient, mobile disk arrays, Collaboration, Hard disks, Energy efficiency, Mobile computing, harsh operating environments]
Implementing a Register in a Dynamic Distributed System
2009 29th IEEE International Conference on Distributed Computing Systems
None
2009
Providing distributed processes with concurrent objects is a fundamental service that has to be offered by any distributed system. The classical shared read/write register is one of the most basic ones. Several protocols have been proposed that build an atomic register on top of an asynchronous message-passing system prone to process crashes. In the same spirit, this paper addresses the implementation of a regular register (a weakened form of an atomic register) in an asynchronous dynamic message-passing system. The aim is here to cope with the net effect of the adversaries that are asynchrony and dynamicity (the fact that processes can enter and leave the system). The paper focuses on the class of dynamic systems the churn rate c of which is constant. It presents two protocols, one applicable to synchronous dynamic message passing systems, the other one to eventually synchronous dynamic systems. Both protocols rely on an appropriate broadcast communication service (similar to a reliable broadcast). Each requires a specific constraint on the churn rate c. Both protocols are first presented in an as intuitive as possible way, and are then proved correct.
[message passing, synchronous system, broadcast communication service, infinite arrival model, Access protocols, churn, Computer crashes, Registers, distributed process, concurrent object, synchronous dynamic message passing system, dynamic system, eventually synchronous system, Distributed computing, dynamic distributed system, asynchronous dynamic message-passing system, Concurrent computing, Message passing, concurrency control, Asynchronous message-passing system, Broadcasting, protocols, regular register]
Message from General Chairs
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Presents the welcome message from the conference proceedings.
[]
Message from Program Chairs
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Presents the welcome message from the conference proceedings.
[]
Organizing Committee
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Provides a listing of current committee members.
[]
Program Committee
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Provides a listing of current committee members.
[]
"Ethernet on AIR': Scalable Routing in very Large Ethernet-Based Networks
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Networks based on Ethernet bridging scale poorly as bridges flood the entire network repeatedly, and several schemes have been proposed to mitigate this flooding problem, however, none have managed to eliminate flooding completely. We present Automatic Integrated Routing (AIR) as the first routing protocol that eliminates flooding by assigning prefix labels to switches and building a Distributed Hash Table (DHT). The DHT maps host identifiers to the prefix labels of the switches through which they connect to the network. Each switch is assigned a prefix label using neighbor-to-neighbor messages. Prefix labels denote the locations of the switches in the network, and the prefix labels of any two switches automatically determine one or multiple routes between them. The DHT stores the mapping between the name of a host and its network location (prefix label) in a scalable fashion, with any one switch storing only a fraction of all the mappings. In contrast, prior approaches using DHTs to resolve host names incur the communication and storage overhead introduced by an underlying link-state routing protocol. Results using packet-level traces of Internet traffic demonstrate that AIR attains performance gains of orders of magnitude over Ethernet bridging and prior DHT-based schemes.
[Ethernet networks, automatic integrated routing, very large Ethernet-based networks, Ethernet on AIR, Switches, local area networks, Internet traffic, packet-level traces, prefix label assignment, Floods, Distributed computing, DHT, Scalable routing, Ethernet bridging, host identifiers, Prefix routing, link-state routing protocol, Broadcasting, Routing protocols, Computer networks, Enterprise Network, routing protocol, Routing, Communication switching, telecommunication switching, Bridges, DHT-based schemes, Anchor, distributed hash table, routing protocols, Ethernet, file organisation, LANs, DHT maps, network switches, Internet, Computer network management, telecommunication traffic, scalable routing]
Deployment of a Reinforcement Backbone Network with Constraints of Connection and Resources
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
In recent years, we have seen a surge of interest in enabling communications over meshed wireless networks. Particularly, supporting peer-to-peer communications over a multi-hop wireless network has a big potential in enabling ubiquitous computing. However, many wireless nodes have limited capabilities, for example, sensor nodes or small handheld devices. Also, the end-to-end capacity and delay degrade significantly as the path length increases with the number of network nodes. In these scenarios, the deployment of a backbone network could potentially facilitate higher performance network communications. In this paper, we study the novel Reinforcement Back-bone Network (RBN) deployment problem considering the practical limitation in the number of available backbone nodes and enforcing backbone network connectivity. We propose an iterative and adaptive (ITA) algorithm for efficient backbone network deployment. In addition, in order to provide the performance bound, we redefine and solve the problem by implementing the Generic Algorithm. Finally, we present our simulation results under various settings and compare the performance of the proposed ITA algorithm and the generic algorithm. Our study indicates that the proposed ITA algorithm is promising for deploying a connected RBN with a limited number of available backbone nodes.
[iterative methods, radio networks, Spine, ubiquitous computing, peer-to-peer communications, Degradation, sensor nodes, ITA algorithm, small handheld devices, generic algorithm, Wireless mesh networks, Spread spectrum communication, delay degrade, peer-to-peer computing, Peer to peer computing, iterative and adaptive algorithm, Ubiquitous computing, genetic algorithms, end-to-end capacity, Surges, Wireless sensor networks, Handheld computers, multihop wireless network, reinforcement backbone network deployment problem, meshed wireless networks, Iterative algorithms]
Phase Plane Analysis of Congestion Control in Data Center Ethernet Networks
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Ethernet has some attractive properties for network consolidation in the data center, but needs further enhancement to satisfy the additional requirements of unified network fabrics. Congestion management is introduced in Ethernet networks to avoid dropping packets due to congestion. The BCN (Backward Congestion Notification) mechanism is a basic element of several standard drafts, and its stability underlies normal network operations. Because the linear stability analysis method is incapable of handling the nonlinearity of the variable structure control employed by the BCN mechanism, some particular phenomena are unexposed and the insights are insufficient. In this paper, we propose the concept of strong stability of the queuing system to satisfy the requirements of no dropped packets in the data center, and build a fluid-flow analytical model for the BCN congestion control system. Considering the nonlinearity involved in the rate regulation laws, we classify the system into different categories according to the shapes of phase trajectories, and conduct a nonlinear stability analysis using phase plane analysis techniques on a case by case basis. The analysis details can provide a comprehensive understanding about the behaviors of the overall congestion control system. Finally, we also deduce an explicit stability criterion presenting the parameters constraints for the strongly stable BCN system, which can provide straightforward guidelines for proper parameter settings.
[data center Ethernet networks, Ethernet networks, Shape, telecommunication congestion control, telecommunication network management, congestion management, Control systems, local area networks, Guidelines, Analytical models, Control system synthesis, network consolidation, nonlinear stability analysis, Data Center Ethernet, Strong stability and Phase Trajectory, Fabrics, phase plane analysis techniques, Backward Congestion Notification, linear stability analysis method, queuing system, Stability analysis, backward congestion notification, BCN congestion control system, Stability criteria, fluid-flow analytical model, Queueing analysis]
3DLoc: Three Dimensional Wireless Localization Toolkit
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
In this paper, we present 3DLoc: an integrated system of hardware and software toolkits for locating an 802.11-compliant mobile device in a three dimensional (3D) space. 3DLoc features two specialized antennas: an azimuth antenna and an elevation antenna, for detecting the azimuth and elevation angles of a mobile device respectively in real time. To improve positioning accuracy in real-world urban settings, we propose various signal processing techniques such as clustering and wavelet-transform based denoising, and present theoretical analysis of the accuracy of these techniques. With different antenna configurations, 3DLoc is able to track single or multiple targets in one round of azimuth scanning and elevation scanning. We conduct extensive experiments to demonstrate the efficiency and accuracy of 3DLoc. 3DLoc can be used in various applications, including wireless network forensics for locating anonymous criminal mobile devices.
[Real time systems, real-world urban setting, wavelet transforms, Noise reduction, signal processing, Mobile antennas, Wavelet analysis, signal denoising, Azimuth, wavelet transform based denoising, Hardware, wireless network forensics, IEEE standards, 802.11, azimuth scanning, Target tracking, 3DLoc feature, antennas, antenna configuration, three dimensional space, anonymous criminal mobile device, three dimensional wireless localization toolkit, telecommunication standards, elevation antenna, azimuth antenna, Signal processing, radiocommunication, elevation angle, Software tools, Signal analysis, specialized antenna, positioning accuracy, elevation scanning]
Design and Analysis of a New GPS Algorithm
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
In this paper, we propose and analyze a new GPS positioning algorithm. Our algorithm uses the direct linearization technique to reduce the computation time overhead. We invoke the general least squares method in order to achieve optimality in the situation when the trilateration system of equations becomes over-determined. We systematically evaluate our new algorithms and show that they indeed take much less computation time than the traditional GPS method while maintaining reasonable accuracy.
[Algorithm design and analysis, Computers, Nonlinear equations, least squares approximations, linearisation techniques, GPS positioning algorithm, Newton-Raphson Method, Closed-form solution, GPS, Global Positioning System, least squares method, Trilateration, Satellites, Content addressable storage, direct linearization technique, computation time overhead reduction, Direct Method, Iterative algorithms, Iterative methods, Clocks]
ASAP: Scalable Identification and Counting for Contactless RFID Systems
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
The growing importance of operations such as identification, location sensing and object tracking has led to increasing interests in contact less Radio Frequency Identification (RFID) systems. Enjoying the low cost of RFID tags, modern RFID systems tend to be deployed for large-scale mobile objects. Both the theoretical and experimental results suggest that when tags are mobile and with large numbers, two classical MAC layer collision-arbitration protocols, slotted ALOHA and Tree-traversal, do not satisfy the scalability and time-efficiency requirements of many applications. To address this problem, we propose Adaptively Splitting-based Arbitration Protocol (ASAP), a scheme that provides low-latency RFID identification and has stable performance for massive RFID networks. Theoretical analysis and experimental evaluation show that ASAP outperforms most existing collision-arbitration solutions. ASAP is efficient for both small and large deployment of RFID tags, in terms of time and energy cost. Hence it can benefit dynamic and large-scale RFID systems.
[Costs, radiofrequency identification, ALOHA protocol, Scalability, radio frequency identification, access protocols, Distributed computing, Road transportation, location sensing, MAC layer collision-arbitration protocols, object tracking, Large-scale systems, scalable identification, Collision arbitration, trees (mathematics), Airports, RFID, slotted ALOHA, RFID tags, Computer science, adaptively splitting-based arbitration protocol, tree-traversal, Media Access Protocol, contactless RFID systems, Radiofrequency identification, ASAP]
Mistral: Dynamically Managing Power, Performance, and Adaptation Cost in Cloud Infrastructures
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Server consolidation based on virtualization is an important technique for improving power efficiency and resource utilization in cloud infrastructures. However, to ensure satisfactory performance on shared resources under changing application workloads, dynamic management of the resource pool via online adaptation is critical. The inherent tradeoffs between power and performance as well as between the cost of an adaptation and its benefits make such management challenging. In this paper, we present Mistral, a holistic controller framework that optimizes power consumption, performance benefits, and the transient costs incurred by various adaptations and the controller itself to maximize overall utility. Mistral can handle multiple distributed applications and large-scale infrastructures through a multi-level adaptation hierarchy and scalable optimization algorithm. We show that our approach outstrips other strategies that address the tradeoff between only two of the objectives (power, performance, and transient costs).
[Cloud computing, Energy consumption, holistic controller framework, transient cost, Distributed computing, Delay, Voice mail, power consumption, Degradation, power aware computing, resource pool management, optimization, Mistral, Cost function, adaptation overheads, power, Virtual manufacturing, power efficiency, cost reduction, virtualization, server consolidation, performance evaluation, virtual machine, performance, virtual machines, cloud infrastructure, scalable optimization algorithm, Internet, Resource management, Energy management]
ARiA: A Protocol for Dynamic Fully Distributed Grid Meta-scheduling
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Critical to the successful deployment of grid systems is their ability to guarantee efficient meta-scheduling, namely optimal allocation of jobs across a pool of sites with diverse local scheduling policies. The centralized nature of current meta-scheduling solutions is not well suited for the envisioned increasing scale and dynamicity of next-generation grids, the success of which relies on the development of fully distributed, flexible and autonomic systems tailored to very large sets of highly volatile and heterogeneous resources. In this respect, we introduce a fully distributed grid meta-scheduling scheme that effectively addresses the concerns regarding the scalability and adaptability of future grid systems. Our approach employs a lightweight protocol, called A RiA, that is based on peer-to-peer communication between grid nodes, and makes use of dynamic rescheduling to consider and adapt to changes in the availability of resources. Extensive evaluation by means of an in depth simulation study highlighted the effectiveness of the proposed solution in improving the overall performance in terms of job completion time and load-balancing.
[Protocols, Costs, Scalability, grid computing, distributed system, Distributed Systems, Grid Computing, resource allocation, scheduling, ARiA protocol, Grid computing, Large-scale systems, protocols, job allocation, peer-to-peer communication, Availability, peer-to-peer computing, Peer to peer computing, load-balancing, Network Protocol, Dynamic scheduling, grid system, Scheduling, meta scheduling, autonomic system, Processor scheduling, Self-Organization, fault tolerant computing, Resource management]
Stochastic Steepest-Descent Optimization of Multiple-Objective Mobile Sensor Coverage
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
We propose a steepest descent method to compute optimal control parameters for balancing between multiple performance objectives in stateless stochastic scheduling, wherein the scheduling decision is effected by a simple constant-time coin toss operation only. We apply our method to the scheduling of a mobile sensor's coverage time among a set of points of interest (PoIs). The coverage algorithm is guided by a Markov chain wherein the sensor at PoI i decides to go to the next PoI j with transition probability pij . We use steepest descent to compute the transition probabilities for optimal tradeoff between two performance goals concerning the distributions of per-PoI coverage times and exposure times, respectively. We also discuss how other important goals such as energy efficiency and entropy of the coverage schedule can be addressed. For computational efficiency, we show how to optimally adapt the step size in steepest descent to achieve fast convergence. However, we found that the structure of our problem is complex in that there may exist surprisingly many local optima in the solution space, causing basic steepest descent to get stuck easily at a local optimum. To solve the problem, we show how proper incorporation of noise in the search process can get us out of the local optima with high probability. We provide simulation results to verify the accuracy of our analysis, and show that our method can converge to the globally optimal control parameters under different assigned weights to the performance goals and different initial parameters.
[wireless sensor networks, stochastic steepest descent optimization, Stochastic processes, Optimization methods, stochastic programming, optimal control parameter computation, Sensor systems, Mobile Sensor Coverage, noise incorporation, stateless stochastic scheduling, Distributed computing, Optimization, Markov chain, gradient methods, coverage schedule, transition probability, probability, multiple objective mobile sensor coverage, Pol coverage time, multiple performance objective, Computational complexity, Scheduling algorithm, Processor scheduling, Stochastic systems, constant time coin toss operation, Optimal control, Markov processes, point of interest coverage time, Mobile computing, computational complexity]
Distributed Coverage in Wireless Ad Hoc and Sensor Networks by Topological Graph Approaches
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Coverage problem is a fundamental issue in wireless ad hoc and sensor networks. Previous techniques for coverage scheduling often require accurate location information or range measurements, which cannot be easily obtained in resource-limited ad hoc and sensor networks. Recently, a method based on algebraic topology has been proposed to achieve coverage verification using only connectivity information. The topological method sheds some light on the issue of location-free coverage. Unfortunately, the needs of centralized computation and rigorous restriction on sensing and communication ranges greatly limit the applicability in practical large-scale distributed sensor networks. In this work, we make the first attempt towards establishing a graph theoretical framework for connectivity-based coverage with configurable coverage granularity. We propose a novel coverage criterion and scheduling method based on cycle partition. Our method is able to construct a sparse coverage set in a distributed manner, using purely connectivity information. Compared with existing methods, our design has a particular advantage, which permits us to configure or adjust the quality of coverage by adequately exploiting diverse sensing ranges and specific requirements of different applications. We formally prove the correctness and evaluate the effectiveness of our approach through extensive simulations and comparisons with the state-of-the-art approaches.
[wireless sensor networks, Laboratories, location information, wireless ad hoc and sensor networks, wireless ad hoc networks, Sensor systems, topological graph, Distributed computing, coverage verification, Information science, connectivity, Computer networks, Large-scale systems, topological graph approaches, cycle partition, Monitoring, configurable coverage granularity, coverage, coverage scheduling, large-scale distributed sensor networks, connectivity-based coverage, telecommunication network topology, range measurements, distributed, Computer science, Wireless sensor networks, Processor scheduling, ad hoc networks]
Link-Centric Probabilistic Coverage Model for Transceiver-Free Object Detection in Wireless Networks
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Sensing coverage is essential for most applications in wireless networks. In traditional coverage problem study, the disk coverage model has been widely applied because of its simplicity. Though notable recent works point out that the disk model has many critical limitations when applied in practice, few successful works have been conducted to comprehensively study the issue. Motivated by this, in this paper we propose a new coverage model called T-R model. T-R model is derived from a real application of transceiver-free object detection. Compared with the traditional disk model, T-R model is able to describe many new coverage features such as the probabilistic coverage, the link-centric coverage units and the correlations between multiple coverage units. These new capabilities make T-R model a better abstraction of individual sensors. To evaluate the performance of T-R model, we conduct comprehensive empirical studies based on a test-bed of 30 telosB nodes. Experimental results show that the TR model can adequately describe the sensing behavior in the transceiver-free object detection applications. The average error between the model and the reality is only 8%. Moreover, T-R model presents attractive flexibility, making it more appropriate for general coverage problem studies than the transceiver-free object detection.
[wireless sensor networks, link-centric probabilistic coverage model, transceiver, probability, Sensor phenomena and characterization, object detection, Application software, Distributed computing, Computer science, Temperature sensors, Wireless sensor networks, Wireless networks, Vehicle detection, correlation, transceivers, T-R model, Object detection, disk coverage model, Testing, correlation methods]
StreamCloud: A Large Scale Data Streaming System
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Data streaming has become an important paradigm for the real-time processing of continuous data flows in domains such as finance, telecommunications, networking, Some applications in these domains require to process massive data flows that current technology is unable to manage, that is, streams that, even for a single query operator, require the capacity of potentially many machines. Research efforts on data streaming have mainly focused on scaling in the number of queries or query operators, but overlooked the scalability issue with respect to the stream volume. In this paper, we present StreamCloud a large scale data streaming system for processing large data stream volumes. We focus on how to parallelize continuous queries to obtain a highly scalable data streaming infrastructure. StreamCloud goes beyond the state of the art by using a novel parallelization technique that splits queries into subqueries that are allocated to independent sets of nodes in a way that minimizes the distribution overhead. StreamCloud is implemented as a middleware and is highly independent of the underlying data streaming engine. We explore and evaluate different strategies to parallelize data streaming and tackle with the main bottlenecks and overheads to achieve scalability. The paper presents the system design, implementation and a thorough evaluation of the scalability of the fully implemented system.
[Real time systems, parallelization technique, Scalability, Finance, data streaming system, continuous data flow processing, Distributed computing, Middleware, parallel processing, Engines, System analysis and design, query processing, Technology management, StreamCloud system, Telephony, query operator, Large-scale systems, continuous queries, middleware]
A Hybrid Approach to High Availability in Stream Processing Systems
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Stream processing is widely used by today's applications such as financial data analysis and disaster response. In distributed stream processing systems, machine fail-stop events are handled by either active standby or passive standby. However, existing high availability (HA) schemes have not sufficiently addressed the situation when a machine becomes temporarily unavailable due to data rate spikes, intensive analysis or job sharing, which happens frequently but lasts for short time. It is not clear how well active and passive standby fare against such transient unavailability. In this paper, we first critically examine the suitability of active and passive standby against transient unavailability in a real testbed environment. We find that both approaches have advantages and drawbacks, but neither is ideal to provide fast recovery at low overhead as required to handle transient unavailability. Based on the insights gained, we propose a novel hybrid HA method that switches between active and passive standby modes depending on the occurrence of failure events. It presents a desirable tradeoff that is different from existing HA approaches: low overhead during normal conditions and fast recovery upon transient or permanent failure events. We have implemented our hybrid method and compared it with existing HA designs with comprehensive evaluation. The results show that our hybrid method can reduce two-thirds of the recovery time compared to passive standby and 80% message overhead compared to active standby, allowing applications to enjoy uninterrupted processing without paying a high premium.
[Availability, Data analysis, hybrid method, Laboratories, transient failure events, Telecommunication traffic, Switches, reliability, machine fail-stop events, Distributed computing, failure analysis, system recovery, Delay, financial data analysis, Computer science, high availability, permanent failure events, high availability schemes, disaster response, stream processing systems, Streaming media, distributed stream processing systems, Testing]
Efficient and Progressive Algorithms for Distributed Skyline Queries over Uncertain Data
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
The skyline operator has received considerable attention from the database community, due to its importance in many applications including multi-criteria decision making, preference answering, and so forth. In many applications where uncertain data are inherently exist, i.e., data collected from different sources in distributed locations are usually with imprecise measurements, and thus exhibit kind of uncertainty. Taking into account the network delay and economic cost associated with sharing and communicating large amounts of distributed data over an internet, an important problem in this scenario is to retrieve the global skyline tuples from all the distributed local sites with minimum communication cost. Based on the well known notation of the probabilistic skyline query over centralized uncertain data, in this paper, for the first time, we propose the notation of distributed skyline queries over uncertain data. Furthermore, two communication-and computation-efficient algorithms are proposed to retrieve the qualified skylines from distributed local sites. Extensive experiments have been conducted to verify the efficiency and the effectiveness of our algorithms with both the synthetic and real data sets.
[Computers, Costs, Uncertainty, probability, Information retrieval, Application software, Distributed computing, distributed skyline queries, distributed computing, query processing, uncertain data, Distributed databases, Clustering algorithms, distributed databases, Grid computing, database community, Internet, Stock markets, distributed data, probabilistic skyline query, skyline query]
Reliability Calculus: A Theoretical Framework to Analyze Communication Reliability
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Communication reliability is one of the most important concerns and fundamental issues in network systems, such as cyber-physical systems, where network components, sensors, actuators, controllers are interconnected with each other. These systems are prevalent in many safety-critical areas, including aerospace, automotive, civil infrastructure, energy, healthcare, manufacturing, and transportation, etc. In such systems, a single link failure, or communication delay could lead to catastrophic consequences. Hence, there is an urgent demand on efficient methodologies to model and analyze the delay distribution of control messages or feedback signals, especially when networks grow more complex and more heterogenous. In this paper, a calculus based on frequency domain analysis is developed to address this goal, so we can model and analyze the reliability of communication in large-scale compositional networked systems. Several network structures (e.g. serial, parallel, circular and backup) are defined as building blocks to model a wide variety of connections in networked systems. The advantages of the proposed theoretical framework over the traditional time-domain approaches include the capability to capture higher order moments of system characteristics, scalability to analyze the reliability of complex systems, efficiency in calculation and practicability in simulation.
[Actuators, Communication system control, Medical services, Reliability theory, Control systems, Calculus, Sensor systems, frequency-domain analysis, Automotive engineering, frequency domain analysis, End-to-end Communication, communication reliability, Communication Reliability, telecommunication network reliability, Frequency-domain Analysis, Aerospace safety, reliability calculus, Telecommunication network reliability]
Resource Allocation in Distributed Mixed-Criticality Cyber-Physical Systems
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Large-scale distributed cyber-physical systems will have many sensors/actuators (each with local micro-controllers), and a distributed communication/computing backbone with multiple processors. Many cyber-physical applications will be safety critical and in many cases unexpected workload spikes are likely to occur due to unpredictable changes in the physical environment. In the face of such overload scenarios, the desirable property in such systems is that the most critical applications continue to meet their deadlines. In this paper, we capture this mixed-criticality property by developing a formal overload-resilience metric called ductility. The generality of ductility enables it to evaluate any scheduling algorithm from the perspective of mixed-criticality cyber-physical systems. In distributed cyber-physical systems, this ductility is the result of both the task-to-processor packing (a.k.a bin packing) and the uniprocessor scheduling algorithms used. In this paper, we present a ductility-maximization packing algorithm to complement our previous work on mixed-criticality uniprocessor scheduling. Our packing algorithm, known as Compress-on-Overload Packing (COP) is a criticality-aware greedy bin-packing algorithm that maximizes the tolerance of high-criticality tasks to overloads. We compare the ductility of COP against the Worst-Fit Decreasing (WFD) bin-packing heuristic used traditionally for load balancing in distributed systems, and show that the performance of COP dominates WFD in the average case and can reach close to five times better ductility when resources are limited. Finally, we illustrate the practical use of COP in distributed cyber-physical systems using a radar surveillance application, and provide an overview of the entire process from assigning task criticality levels to evaluating its performance
[Actuators, Spine, mixed criticality, compress-on-overload packing, distributed mixed criticality cyber physical systems, safety critical, Sensor systems, Distributed computing, distributed computing, uniprocessor scheduling algorithms, resource allocation, scheduling, distributed systems, Large-scale systems, Safety, local microcontrollers, distributed communication, COP, Radar applications, Scheduling algorithm, cyber-physical systems, radar, greedy binpacking algorithm, Load management, cyber physical applications, Resource management, real time systems]
Optimizing the Spatio-temporal Distribution of Cyber-Physical Systems for Environment Abstraction
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Cyber-physical systems (CPS) bridge the virtual cyber world with the real physical world. For representing a physical environment in cyber, CPS devices / nodes are assigned to collect data in a region of interest. In practice, the nodes seldom fully cover the region due to the restriction of quantity and cost. Hence, the sampled data are usually inadequate to describe the holistic environment. Recent researches mainly focus on the interpolation methods to generate an approximating model from the raw data. However, in this paper, we propose to study the spatio-temporal distribution of CPS nodes in order to obtain the crucial data for optimal environment abstraction. There are two target problems. First, when the environment changes little over time, what is the optimal spatial distribution of stationary nodes based on historical data? Second, when the environment is time-varying, what is the adaptive spatio-temporal distribution of mobile nodes? We show the NP hardness of the former problem and propose an approximation algorithm. For the latter problem, we develop a cooperative movement algorithm on nodes for achieving a curvature-weighted distribution pattern. A trace driven simulation based on real data of GreenOrbs project evaluates the performance of the proposed approaches.
[Costs, virtual reality, trace driven simulation, Communication system control, interpolation methods, environment abstraction, Distributed computing, virtual cyber world, NP hardness problem, approximating model, optimisation, curvature-weighted distribution pattern, optimal environment abstraction, real physical world, Robot sensing systems, adaptive spatio-temporal distribution optimization, GreenOrbs project, Data communication, CPS nodes, mobile nodes, CPS devices, mobile radio, Cyber-physical systems, spatio-temporal distribution, spatiotemporal phenomena, data collection, Bridges, cyber-physical systems, Interpolation, Wireless sensor networks, interpolation, environmental science computing, CPS bridge, Approximation algorithms, Surface fitting, computational complexity]
CONTRACT: Incorporating Coordination into the IP Network Control Plane
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
This paper presents the CONTRACT framework to address a fundamental deficiency of the IP network control plane, namely the lack of coordination between an IGP and other control functions involved in achieving a high level objective. For example, an IGP's default automatic reaction to a network failure may result in an SLA violation, even if the IGP link weights have been carefully chosen. This is because an IGP blindly routes traffic along the shortest paths based on link weights, and it is completely oblivious to the interactions between SLA compliance, load balancing and traffic policing objectives in a network. The CONTRACT framework makes it possible to coordinate these objectives. Under this framework, routers continue to operate autonomously, but they also coordinate their actions with a centralized network controller, which evaluates the impact of routing changes, decides whether the changes are SLA compliant, and performs load rebalancing and/or packet filter reconfiguration as necessary. The key contribution of CONTRACT is a set of coordination algorithms. We show that CONTRACT can effectively coordinate the actions of routing, load balancing and traffic policing to improve a network's SLA compliance.
[Performance evaluation, telecommunication congestion control, load balancing, coordinated traffic control, Telecommunication traffic, CONTRACT, Centralized control, IP network control plane, Filters, traffic control, Automatic control, traffic policing objectives, Communication system traffic control, IP networks, Contracts, SLA compliance, Routing, centralized network controller, coordination algorithms, IGP, interior gateway protocol, service level are agreements, telecommunication network routing, Load management, network coordination]
Does Link Scheduling Matter on Long Paths?
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
We seek to provide an analytical answer whether the impact of the selection of link scheduling algorithms diminishes on long network paths. The answer is provided through a detailed multi-node delay analysis, which is applicable to a broad class of scheduling algorithms, and which can account for statistical multiplexing. The analysis is enabled by two contributions: (1) We derive a function that can characterize the available bandwidth at a node for various scheduling algorithms. The function has an accuracy that recovers necessary and sufficient conditions for satisfying worst-case delay bounds at a single node, (2) We obtain end-to-end delay bounds by providing an explicit solution to an optimization problem, in which the service received at multiple nodes is subsumed into a single function. By presenting a unified analysis that captures the properties of a broad group of schedulers in a single parameter, we can provide insight how the choice of scheduling algorithms impacts end-to-end delay bounds. An important finding of this paper is that some schedulers show noticeable performance differences which persist in a network setting with long paths.
[Algorithm design and analysis, Measurement, Computer Networks, optimization problem, link scheduling, multinode delay analysis, Telecommunication traffic, Calculus, Added delay, Scheduling algorithm, long network paths, optimisation, Processor scheduling, end-to-end delays, link scheduling algorithm, network calculus, Bandwidth, scheduling, end-to-end delay bounds, statistical multiplexing, Computer networks, Internet, statistical analysis]
CacheCast: Eliminating Redundant Link Traffic for Single Source Multiple Destination Transfers
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Due to the lack of multicast services in the Internet, applications based on single source multiple destinations transfers such as video conferencing, IP radio, IPTV must use unicast or application layer multicast. This in turn has several well-known drawbacks. A basic insight is that this type of traffic exhibits high redundancy with temporal clustering of duplicated packets. The redundancy originates from multiple transfers of the same data chunk over the same link. We propose CacheCast-a link layer caching mechanism-that eliminates the redundant data transmissions using small caches on links. CacheCast's underlying principles are simplicity and reliability. It is a fully distributed and incrementally deployable architecture. It consists of small caches on links that act independently. A single cache removes redundant data from a packet on the link entry and recovers the data on the link exit. Thus, link caches are transparent to routers. We show through analysis and simulation that CacheCast achieves near multicast efficiency for superposition of unicast connections. We implemented CacheCast in ns-2 and show that it does not violate the current understanding of "fairness" in the Internet.
[telecommunication links, link layer caching mechanism, Scalability, distributed processing, redundant link traffic, link, IPTV, Proposals, Distributed computing, Videoconference, caching, Unicast, Web and internet services, computer architecture, multicast communication, IP networks, application layer multicast, Informatics, Redundancy, multicast services, temporal clustering, CacheCast, single source multiple destination transfers, multicast, IP radio, Internet, video conferencing, application layer unicast, telecommunication traffic]
Guaranteeing BGP Stability with a Few Extra Paths
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Policy autonomy exercised by Autonomous Systems (ASes) on the Internet can result in persistent oscillations in Border Gateway Protocol, the Internet's inter-domain routing protocol. Current solutions either rely on globally consistent policy assignments, or require significant deviations from locally assigned policies, resulting in significant loss of autonomy of ASes. In this paper, we take a different approach that guarantees stability with less restrictive policies. Namely, we propose multipath routing to find a better trade-off between AS policy autonomy and system stability. We design an algorithm, STABLE PATH(S) ASSIGNMENT (SPA), that provably detects persistent oscillations and eliminates these oscillations by assigning multiple paths to some ASes in the network. Such an assignment allows each AS to use its most-preferred available path, while requiring very few ASes to carry transit traffic along additional paths in order to break oscillations. We design a distributed protocol for SPA and present tight bounds on the number of paths assigned to the ASes in the network. Using simulations on the AS graph, we show that in presence of oscillations, SPA assigns at most two paths to any AS in the network (in 99.9% of the instances), with an extremely small fraction of ASes assigned the extra path.
[Algorithm design and analysis, autonomous systems, policy autonomy, internetworking, Switches, Telecommunication traffic, Distributed computing, stable path assignment, Convergence, inter-domain routing protocol, Web and internet services, Traffic control, Internetworking, Routing protocols, IP networks, BGP stability, system stability, Stability, Routing, border gateway protocol, Algorithms, routing protocols, multipath routing, Internet, Border Gateway Protocol]
ESCUDO: A Fine-Grained Protection Model for Web Browsers
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Web applications are no longer simple hyperlinked documents. They have progressively evolved to become highly complex-web pages combine content from several sources (with varying levels of trustworthiness), and incorporate significant portions of client-side code. However, the prevailing web protection model, the same-origin policy, has not adequately evolved to manage the security consequences of this additional complexity. As a result, web applications have become attractive targets of exploitation. We argue that this disconnection between the protection needs of modern web applications and the protection models used by web browsers that manage those applications amounts to a failure of access control. In this paper, we present Escudo, a new web browser protection model designed based on established principles of mandatory access control. We describe our implementation of a prototype of Escudo in the Lobo web browser, and illustrate how web applications can use Escudo for securing their resources. Our evaluation results indicate that Escudo incurs low overhead. To support backwards compatibility, Escudo defaults to the same-origin policy for legacy applications.
[Access control, Java, client-side code, Browser Security, Escudo, Lobo Web browser, Security, protection model, Distributed computing, Web pages, Prototypes, authorisation, online front-ends, Permission, Forgery, data privacy, access control, Web security, Access Control, Protection, Advertising, same-origin policy]
Are Your Hosts Trading or Plotting? Telling P2P File-Sharing and Bots Apart
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Peer-to-peer (P2P) substrates are now widely used for both file-sharing and botnet command-and-control. Despite the commonality of their substrates, we show that the different goals and circumstances of these applications give rise to behaviors that can be distinguished in network flow records. Using features related to traffic volume, &#x201C;churn&#x201D; among peers, and differences between human-driven and machine-driven traffic, we develop a technique for identifying P2P bots (the Plotters) and, in particular, separating them from file-sharing hosts (the Traders). Evaluations performed on traffic recorded at the edge of a university network show that we can achieve, e.g., 87.50% detection of Storm bots with a 0.47% false positive rate. We also demonstrate the significant extent to which Plotter behaviors would need to change to evade our technique.
[Performance evaluation, invasive software, Protocols, human-driven traffic, peer-to-peer computing, Peer to peer computing, peer-to-peer networks, botnet command-and-control, Humans, Telecommunication traffic, P2P file-sharing, Distributed computing, Storm bots, Storms, Communication channels, machine-driven traffic, P2P bots identification, Traders host, Testing, Payloads, Plotters technique]
Secure Ranked Keyword Search over Encrypted Cloud Data
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
As Cloud Computing becomes prevalent, sensitive information are being increasingly centralized into the cloud. For the protection of data privacy, sensitive data has to be encrypted before outsourcing, which makes effective data utilization a very challenging task. Although traditional searchable encryption schemes allow users to securely search over encrypted data through keywords, these techniques support only boolean search, without capturing any relevance of data files. This approach suffers from two main drawbacks when directly applied in the context of Cloud Computing. On the one hand, users, who do not necessarily have pre-knowledge of the encrypted cloud data, have to post process every retrieved file in order to find ones most matching their interest, On the other hand, invariably retrieving all files containing the queried keyword further incurs unnecessary network traffic, which is absolutely undesirable in today's pay-as-you-use cloud paradigm. In this paper, for the first time we define and solve the problem of effective yet secure ranked keyword search over encrypted cloud data. Ranked search greatly enhances system usability by returning the matching files in a ranked order regarding to certain relevance criteria (e.g., keyword frequency), thus making one step closer towards practical deployment of privacy-preserving data hosting services in Cloud Computing. We first give a straightforward yet ideal construction of ranked keyword search under the state-of-the-art searchable symmetric encryption (SSE) security definition, and demonstrate its inefficiency. To achieve more practical performance, we then propose a definition for ranked searchable symmetric encryption, and give an efficient design by properly utilizing the existing cryptographic primitive, order-preserving symmetric encryption (OPSE). Thorough analysis shows that our proposed solution enjoys ``as-strong-as-possible" security guarantee compared to previous SSE schemes, while correctly realizing the goal of ranked keyword search. Extensive experimental results demonstrate the efficiency of the proposed solution.
[privacy-preserving data hosting services, Cloud computing, Data privacy, encrypted cloud data, order-preserving symmetric encryption, Keyword search, Telecommunication traffic, Information retrieval, cryptography, Boolean search, searchable symmetric encryption security definition, secure ranked keyword search, data utilization, Frequency, data privacy, Outsourcing, Internet, Cryptography, cloud computing, Protection, Usability]
Practical Robust Communication in DHTs Tolerating a Byzantine Adversary
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
There are several analytical results on distributed hash tables (DHTs) that can tolerate Byzantine faults. Unfortunately, in such systems, operations such as data retrieval and message sending incur significant communication costs. For example, a simple scheme used in many Byzantine fault-tolerant DHT constructions of n nodes requires O(log3n) messages, this is likely impractical for real-world applications. The previous best known message complexity is O(log2n) in expectation, however, the corresponding protocol suffers from prohibitive costs owing to hidden constants in the asymptotic notation and setup costs. In this paper, we focus on reducing the communication costs against a computationally bounded adversary. We employ threshold cryptography and distributed key generation to define two protocols both of which are more efficient than existing solutions. In comparison, our first protocol is deterministic with O(log3n) message complexity and our second protocol is randomized with expected O(log n) message complexity. Further, both the hidden constants and setup costs for our protocols are small and no trusted third party is required. Finally, we present results from micro benchmarks conducted over PlanetLab showing that our protocols are practical for deployment under significant levels of churn and adversarial behaviour.
[Costs, cryptographic protocols, Peer to peer computing, Information retrieval, Proposals, communication complexity, Distributed computing, PlanetLab, Cryptographic protocols, Byzantine fault tolerant, Computer science, byzantine adversary, Fault tolerance, DHT, message complexity, distributed hash tables, practical robust communication, threshold cryptographic protocol, Computer network security, distributed key generation, Robustness, fault tolerant computing, Large-scale systems, Distributed algorithms]
A New Buffer Cache Design Exploiting Both Temporal and Content Localities
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
This paper presents a Least Popularly Used buffer cache algorithm to exploit both temporal locality and content locality of I/O requests. Popular data blocks are selected as reference blocks that are not only accessed frequently but also identical or similar in content to other blocks that are being accessed. Fast delta compression and decompression are used to satisfy as many I/O requests as possible using the popular reference blocks together with small deltas inside the buffer cache. The popularity of a reference block is calculated based on the statistical analysis of data contents and access frequency. A prototype LPU has been implemented as a new cache layer for Kernel Virtual Machine (KVM) on Linux system. Experimental results show LPU is effective for a variety of workloads with the maximum speed up of over 300% compared with LRU.
[Algorithm design and analysis, access frequency, temporal locality, cache storage, reference blocks, Distributed computing, Buffer Cache, content locality, data blocks, Virtual prototyping, Kernel, linux system, data contents, operating system kernels, least popularly used buffer cache, Statistical analysis, Virtual machining, Content addressable storage, Disk drives, prototype IPU, Linux, kernel virtual machine, cache layer, virtual machines, fast delta compression, fast delta decompression, Content Locality, Frequency, statistical analysis, Virtual Machine]
A Spinning Join That Does Not Get Dizzy
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
As network infrastructures with 10 Gb/s bandwidth and beyond have become pervasive and as cost advantages of large commodity-machine clusters continue to increase, research and industry strive to exploit the available processing performance for large-scale database processing tasks. In this work we look at the use of high-speed networks for distributed join processing. We propose Data Roundabout as alight weight transport layer that uses Remote Direct Memory Access (RDMA) to gain access to the throughput opportunities in modern networks. The essence of Data Roundabout is a ring shaped network in which each host stores one portion of a large database instance. We leverage the available bandwidth to (continuously) pump data through the high-speed network. Based on Data Roundabout, we demonstrate cyclo-join, which exploits the cycling flow of data to execute distributed joins. The study uses different join algorithms (hash join and sort-merge join) to expose the pitfalls and the advantages of each algorithm in the data cycling arena. The experiments show the potential of a large distributed main-memory cache glued together with RDMA into a novel distributed database architecture.
[Algorithm design and analysis, Costs, network infrastructures, remote direct memory access, Throughput, spinning join, Distributed computing, data roundabout, High-speed networks, database processing, Distributed databases, commodity machine clusters, Bandwidth, distributed databases, Hardware, Large-scale systems, RDMA, distributed database architecture, Distributed Query Processing, relational databases, Distributed Joins, file organisation, data handling, Spinning, data cycling]
RoLo: A Rotated Logging Storage Architecture for Enterprise Data Centers
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
We propose RoLo (Rotated Logging), a new logging architecture for RAID10 systems for enhanced energy efficiency, performance and reliability. By spreading destaging I/O activities among short idle time slots and proactively reclaiming the stale logging space, RoLo rotates loggers among a logical logging space pool formed collectively from the free storage space available among mirrored disks. Therefore, without the extra dedicated log disks and the corresponding centralized logging, RoLo eliminates the additional hardware and energy costs, potential single point of failure and performance bottleneck. Furthermore, RoLo prolongs the lifecycle of the disks and improves the system's energy efficiency by reducing the disk spin up/down frequency. We develop three flavors of RoLo, that is, RoLo-E/R/P, to emphasize energy efficiency, reliability, and performance respectively. Extensive trace-driven evaluations demonstrate the advantages of the three RoLo schemes over both a RAID10 system with centralized logging architecture and a typical RAID10 system.
[Potential energy, Costs, disk spin up/down frequency, Buffer storage, destaging I/O activities, Cache storage, RAID, centralized logging, Computer science, RoLo architecture, mirrored disks, storage management, rotated logging architecture, dedicated log disks, Computer architecture, Frequency, Energy efficiency, Hardware, enterprise data centers, RAID10 systems, logical logging space pool, Energy storage]
New Algorithms for Planning Bulk Transfer via Internet and Shipping Networks
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Cloud computing is enabling groups of academic collaborators, groups of business partners, etc., to come together in an ad-hoc manner. This paper focuses on the group-based data transfer problem in such settings. Each participant source site in such a group has a large dataset, which may range in size from gigabytes to terabytes. This data needs to be transferred to a single sink site (e.g., AWS, Google datacenters, etc.) in a manner that reduces both total dollar costs incurred by the group as well as the total transfer latency of the collective dataset. This paper is the first to explore the problem of planning a group-based deadline-oriented data transfer in a scenario where data can be sent over both: (1) the internet, and (2) by shipping storage devices (e.g., external or hot-plug drives, or SSDs) via companies such as Fedex, UPS, USPS, etc. We first formalize the problem and prove its NP-Hardness. Then, we propose novel algorithms and use them to build a planning system called Pandora (People and Networks Moving Data Around). Pandora uses new concepts of time-expanded networks and delta-time-expanded networks, combining them with integer programming techniques and optimizations for both shipping and internet edges. Our experimental evaluation using real data from Fedex and from PlanetLab indicate the Pandora planner manages to satisfy deadlines and reduce costs significantly.
[Cloud computing, Costs, integer programming, Urban planning, shipping networks, Distributed computing, Delay, Uninterruptible power systems, people and networks moving data around system, Computer science, Collaboration, bulk transfer planning, Internet, data handling, IP networks, cloud computing, NP-hardness problem, group-based data transfer problem, Pandora system, delta-time-expanded networks]
Empirical Study of a National-Scale Distributed Intrusion Detection System: Backbone-Level Filtering of HTML Responses in China
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
We present results from measurements of the filtering of HTTP HTML responses in China, which is based on string matching and TCP reset injection by backbone-level routers. This system, intended mainly for Internet censorship, is a national-scale filter based on intrusion detection system (IDS) technologies. Our results indicate that the Chinese censors discontinued this HTML response filtering for the majority of routes some time between August 2008 and January 2009 (other forms of censorship, including backbone-level GET request filtering, are still in place). In this paper, we give evidence to show that the distributed nature of this filtering system and the problems inherent to distributed filtering are likely among the reasons it was discontinued, in addition to potential traffic load problems. When the censor successfully detected a keyword in our measurements and attempted to reset the connection, their attempt to reset the connection was successful less than 51% of the time, due to late or out-of-sequence resets. In addition to shedding light on why HTML response filtering may have been discontinued by the censors, we document potential sources of uncertainty, which are due to routing and protocol dynamics, that could affect measurements of any form of censorship in any country. Between a single client IP address in China and several contiguous server IP addresses outside China, measurement results can be radically different. This is probably due to either traffic engineering or one node from a bank of IDS systems being chosen based on source IP address. Our data provides a unique opportunity to study a national-scale, distributed filtering system.
[client-server systems, backbone level filtering, filtering theory, Telecommunication traffic, national scale distributed intrusion detection system, Internet censorship, HTML, Information filtering, Distributed computing, Postal services, TCP reset injection, Computer science, Matched filters, security of data, HTML response filtering, transport protocols, Intrusion detection, China, Information filters, Internet, string matching, server-client IP address, IP networks, backbone level routers]
FLoc : Dependable Link Access for Legitimate Traffic in Flooding Attacks
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Malware-contaminated hosts organized as a &#x201C;bot network&#x201D; can target and flood network links (e.g., routers). Yet, none of the countermeasures to link flooding proposed to date have provided dependable link access (i.e., bandwidth guarantees) for legitimate traffic during such attacks. In this paper, we present a router subsystem called FLoc (Flow Localization) that confines attack effects and provides differential bandwidth guarantees at a congested link: (1) packet flows of uncontaminated domains (i.e., Autonomous Systems) receive better bandwidth guarantees than packet flows of contaminated ones, and (2) legitimate flows of contaminated domains are guaranteed substantially higher bandwidth than attack flows. FLoc employs new preferential packet-drop and traffic-aggregation policies that limit &#x201C;collateral damage&#x201D; and protect legitimate flows from a wide variety of flooding attacks. We present FLoc's analytical model for dependable link access, a router design based on it, and illustrate FLoc's effectiveness using simulations of different flooding strategies and comparisons with other flooding defense schemes.
[dependable link access, invasive software, Telecommunication traffic, Floods, Distributed computing, flow localization subsystem, Analytical models, traffic-aggregation policy, Aggregates, Bandwidth, Traffic control, flooding attacks, bot network, Computer networks, IP networks, link flooding, Protection, preferential packet-drop policy, malware-contaminated hosts, FLoc subsystem]
Analyzing Self-Defense Investments in Internet Security under Cyber-Insurance Coverage
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Internet users such as individuals and organizations are subject to different types of epidemic risks such as worms, viruses, and botnets. To reduce the probability of risk, an Internet user generally invests in self-defense mechanisms like antivirus and antispam software. However, such software does not completely eliminate risk. Recent works have considered the problem of residual risk elimination by proposing the idea of cyber-insurance. In this regard, an important decision for Internet users is their amount of investment in self-defense mechanisms when insurance solutions are offered. In this paper, we investigate the problem of self-defense investments in the Internet, under full and partial cyber-insurance coverage models. By the term `self-defense investment', we mean the monetary-cum-precautionary cost that each user needs to invest in employing risk mitigating self-defense mechanisms, given that it is fully or partially insured by the Internet insurance agencies. We propose a general mathematical framework by which co-operative and non-co-operative Internet users can decide whether or not to invest in self-defense for ensuring both, individual and social welfare. Our results show that (1) co-operation amongst users results in more efficient self-defense investments than those in a non-cooperative setting, under a full insurance coverage model and (2) partial insurance coverage motivates non-cooperative Internet users to invest more efficiently in self-defense mechanisms when compared to full insurance coverage.
[insurance, Computer viruses, risk probability, Internet security, co-operative and non co-operative users, Investments, Cyber insurance coverage, Internet risks, self-defense investments, cyber-insurance coverage, IP networks, socio-economic effects, antispam software, Computer security, Protection, self defense investment, monetary cum precautionary cost, Government, computer viruses, antivirus software, computer network security, cyber-insurance, risk analysis, Computer science, Insurance, Information security, Internet, software cost estimation]
Existence Theorems and Approximation Algorithms for Generalized Network Security Games
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Aspnes et al introduced an innovative game for modeling the containment of the spread of viruses and worms (security breaches) in a network. In this model, nodes choose to install anti-virus software or not on an individual basis while the viruses or worms start from a node chosen uniformly at random and spread along paths consisting of insecure nodes. They showed the surprising result that a pure Nash Equilibrium always exists when all nodes have identical installation costs and identical infection costs. In this paper we present a substantial generalization of the model of that allows for arbitrary security and infection costs, and arbitrary distributions for the starting point of the attack. More significantly, our model GNS(d) incorporates a network locality parameter d which represents a hop-limit on the spread of infection as accounted for in the strategic decisions, due to either the intrinsic nature of the infection or the extent of neighborhood information that is available to a node. We determine that the network locality parameter plays a key role in the existence of pure Nash equilibria (NE): local (d = 1) and global games (d = &#x221E;) have pure NE, while for GNS(d) games with 1 &lt;; d &lt;; &#x221E;, pure NE may not exist, and in fact, it is NP-complete to determine whether a given instance has a pure NE. For local and global games, we also characterize the price of anarchy in terms of the maximum degree and vertex expansion of the contact network; these suggest natural heuristics to aid a network planner in enforcing efficient equilibria. We design a general LP-based framework for approximating the NP-complete problem of finding a socially optimal configuration in our game. Our framework yields a 2d-approximation for general GNS(d) games, and an O(log n)-approximation for the global model where n is the number of network nodes; the latter result improves on the approximation bound of O(log1.5n) of achieved for a special case of our global model. We study the characteristics of NE and the quality of our approximations empirically in two distinct classes of graphs: random geometric graphs and power law graphs. We find that in local and global games on these real-world networks, best response dynamics converge in linear or sub-linear time and have costs comparable to the social optimum. Finally, we study the performance of our approximation algorithms, and find that the approximation guarantees with respect to social cost are much better in practice than our theoretical bounds.
[invasive software, Costs, Computer worms, Computer viruses, Nash equilibrium, approximation algorithms, infection costs, Distributed computing, network security games, optimisation, existence theorems, security breaches, Computer networks, power law graphs, innovative game, Protection, insecure nodes, geometric graphs, game theory, antivirus software, NP-complete problem, social optimum, Game theory, Sun, computer network security, Information security, Approximation algorithms, arbitrary security]
Design of Non-orthogonal Multi-channel Sensor Networks
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
A critical issue in wireless sensor networks (WSNs) is represented by the network throughput. To meet the throughput requirement, researchers propose multi-channel design in 802.15.4 networks to better utilize the wireless medium and avoid the co-channel interference. However, traditional orthogonal channel design restricts the number of channels and limits the throughput performance. We argue that the orthogonality is not necessary for multi-channel design in WSNs. In this paper, we investigate the feasibility of non-orthogonal channel design. In our experiment, we observe that with nonorthogonal transmission, the effect of interference comes from co-channel and inter-channel is different. More specifically, the inter-channel interference is tolerable with certain channel center frequency distance (CFD). According to that, we propose a novel scheme DCN (Dynamic CCA-threshold for Non-orthogonal transmission) which adjusts the CCA-threshold to enable the concurrent transmissions on adjacent non-orthogonal channels and thus improve the overall network throughput performance. Through comprehensive experiments on our testbed, we verify that our DCN achieves about 38.4% ~ 55.7% throughput improvement in general network configurations comparing to the default ZigBee design.
[Multi-channel, Protocols, wireless sensor networks, dynamic CCA-threshold, Throughput, network throughput, Distributed computing, interference effect, ZigBee design, orthogonal channel design, interchannel interference, Bandwidth, CCA-threshold, nonorthogonal multichannel sensor network design, ZigBee, interference suppression, Computational fluid dynamics, 802.15.4 networks, Decoding, cochannel interference, orthogonal channel, channel center frequency distance, Wireless sensor networks, nonorthogonal transmission, personal area networks, wireless medium, Frequency, interference, Interchannel interference]
An Efficient Algorithm for Cut Vertex Detection in Wireless Sensor Networks
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Due to lack of precise control on network topology, cut vertices often exist in wireless sensor networks (WSNs). A cut vertex is defined as a sensor node whose removal breaks network connectivity. Failures of such nodes make a network disconnected and block data transmission, hence it is vital to detect cut vertices in WSNs. This paper proposes a distributed algorithm CVD to identify cut vertices. The algorithm travels the nodes of a network in parallel, colors the edges based on the interval-coded spanning tree, and then determines the cut vertices by counting the edge colors. The parallel style of the algorithm significantly reduces the time delay. Further more, optimization strategies based on interval coding are proposed to reduce the communication cost of the algorithm. The correctness of the algorithm is proved and its performance is analyzed theoretically. Theoretical analysis shows that it requires O (dn) communication cost and has O (dn) time delay, where n is the number of nodes in a WSN, and d is the maximum degree of sensor node. Both the simulation results and the experiments on real sensor networks show that the algorithm outperforms previous algorithms with lower communication cost and shorter time delay.
[CADCAM, topology discovery, wireless sensor networks, edge colors, Delay effects, Computer aided manufacturing, telecommunication network topology, Routing, network topology, Distributed computing, interval-coded spanning tree, Wireless sensor networks, Network topology, precise control, energy efficiency, Cost function, Data communication, Distributed algorithms, cut vertex detection]
Adaptive Sampling and Diversity Reception in Multi-hop Wireless Audio Sensor Networks
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Wireless Audio Sensor Networks (WASNs) can provide event detection, object tracking and audio stream monitoring through cooperative audio sensor nodes. Extensive researches have focused on sound detection and source localization, but little work is on the monitoring and recovery of audio stream. To clearly reconstruct the real-time audio stream from WASNs, both the quantity and quality of samples provided by sensor nodes should be carefully considered. In this work, we propose a novel approach termed "adaptive sampling and diversity reception\
[optimal sampling nodes, data fusion, Energy consumption, wireless audio sensor networks, Event detection, wireless sensor networks, multi-hop wireless audio sensor networks, object detection, Spread spectrum communication, Acoustic sensors, object tracking, audio stream monitoring, energy consumption, Monitoring, adaptive sampling, diversity reception, maximal-ratio-combining-like signal fusion, sampling rate assignment, Wireless sensor networks, Diversity reception, event detection, data transmission, Streaming media, Sampling methods, data communication, Signal to noise ratio]
Versatile Stack Management for Multitasking Sensor Networks
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
The networked application environment has motivated the development of multitasking operating systems for sensor networks and other low-power electronic devices, but their multitasking capability is severely limited because traditional stack management techniques perform poorly on small memory systems. In this paper, we show that combining binary translation and a new kernel runtime can lead to efficient OS designs on resource-constrained platforms. We introduce SenSmart, a multitasking OS for sensor networks, and present new OS design techniques for supporting preemptive multi-task scheduling, memory isolation, and versatile stack management. We have implemented SenSmart on MICA2/MICAz motes. Evaluation shows that SenSmart performs efficient binary translation and demonstrates a significantly better capability in managing concurrent tasks than other sensor net operating systems.
[Performance evaluation, wireless sensor networks, concurrent task management, Sensor systems, Environmental management, Runtime, low-power electronic devices, versatile stack management, Operating systems, sensornet operating systems, resource constrained platform, binary translation, Kernel, memory isolation, MICAz, operating system kernels, multitask scheduling, Low power electronics, multitasking sensor networks, MICA2, Multitasking, Sensor systems and applications, multitasking operating systems, OS designs, Memory management, multiprogramming, SenSmart]
Complexity Analysis of Weak Multitolerance
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
In this paper, we classify multitolerant systems, i.e., systems that tolerate multiple classes of faults and provide potentially different levels of tolerance to them in terms of strong and weak multitolerance. Intuitively, this classification is based upon the guarantees provided by the program when one class of faults occurs while it is recovering from another class of faults. We focus on automated synthesis of weak multitolerant programs. Such weak multitolerance becomes necessary when it is impossible to provide strong multitolerance and/or when the probability of one class of faults occurring while the program is 'recovering' from a fault from another class is negligible. By considering the levels of fault-tolerance provided to each class of faults, we evaluate five possible combinations for weak multitolerance. We find a counterintuitive result that if masking fault-tolerance is desired for one class of faults and masking (or failsafe) fault-tolerance is desired for another class of faults then the problem is NP-hard. This result is surprising since the corresponding problem for strong multitolerance can be solved in polynomial time. Also, we show that the problem of synthesizing weak multitolerance for other combinations is in P. More broadly, this result demonstrates the role of assumptions, e.g., independence of occurrences of faults from different classes, in the complexity of automated synthesis.
[Costs, fault tolerance, complexity analysis, weak multitolerance, weak multitolerant programs, Multitolerance, Synchronization, program recovery, Distributed computing, system recovery, Program synthesis, software fault tolerance, Computer science, Fault tolerance, automated synthesis, NP-hard problem, Fault tolerant systems, Computer bugs, Polynomials, polynomial time, Safety, Formal methods, Clocks, computational complexity]
Message-Efficient Byzantine Fault-Tolerant Broadcast in a Multi-hop Wireless Sensor Network
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
We consider message-efficient broadcast tolerating Byzantine faults in a multi-hop wireless sensor network. Assuming a grid network where all nodes have a communication range of r, and a single neighborhood contains at most t dishonest and collision-capable (bad) nodes, each with a message budget m<sub>f</sub>, we investigate the minimum message budget m that each honest (good) node must have in order to achieve reliable broadcast. We consider three cases: (1) m<sub>f</sub> is known in advance and m is homogeneous among all good nodes; (2) m<sub>f</sub> is known in advance and m is heterogeneous among good nodes; (3) m<sub>f</sub> is unknown. For the first two cases, we present possibility results and broadcast protocols that have message costs within twice the lower bound. For the third case, we present a coding scheme that helps verify the integrity of messages at a receiving node without using any cryptographic techniques. This code leads to a reactive local broadcast primitive that has probabilistic reliability guarantees. Combined with a previously proposed scheme, it results in a broadcast protocol for t &lt;; 1/2r(2r + 1) that guarantees reliability with high probability.
[message budget, Costs, wireless sensor networks, reactive local broadcast, Distributed computing, Fault tolerance, Fault tolerant systems, probabilistic reliability, Broadcast, Spread spectrum communication, telecommunication network reliability, Broadcasting, multihop wireless sensor network, Cryptography, protocols, broadcast protocols, Base stations, fault tolerance, grid network, Byzantine, Cryptographic protocols, Wireless sensor networks, Sensor Network, Fault Tolerant, message-efficient Byzantine fault-tolerant broadcast]
Fault-Containing Self-Stabilization in Asynchronous Systems with Constant Fault-Gap
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
This paper presents a new transformation which adds fault-containment properties to any silent self-stabilizing protocol. The transformation features a constant slow-down factor and the fault-gap - that is the minimal time between two containable faults - is constant. The transformation scales well to arbitrarily large systems and avoids global synchronization.
[self-stabilization, Access protocols, distributed processing, fault-containment properties, Routing, Distributed computing, Contamination, slow-down factor feature, silent self-stabilizing protocol, fault-containment, Fault diagnosis, single transient faults, Fault tolerance, fault-gap feature, Telematics, fault tolerant computing, Large-scale systems, Pollution measurement, protocols, asynchronous systems, Contracts, stability]
Adaptive Jamming-Resistant Broadcast Systems with Partial Channel Sharing
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Wireless communication is particularly vulnerable to signal jamming attacks. Spread spectrum mitigates such problem by spreading normal narrowband signals over a much wider band of frequencies and forcing jammers who do not know such spread pattern to invest much more effort to launch attacks. However, in broadcast systems, jammers can easily find out the spread pattern by compromising some receivers. Several group-based approaches have been proposed to deal with insider jammers who can compromise receivers in broadcast systems; they can tolerate t malicious receivers as long as the system can afford 2t additional copies for each broadcast message. This paper introduces a novel jamming-resistant broadcast system that organizes receivers into multiple channel-sharing broadcast groups and isolates malicious receivers using adaptive re-grouping. By letting receivers in different groups partially share their channels, this scheme reduces the extra communication cost from 2t to (2 - &#x03C1;)t copies, where &#x03C1; is the channel sharing factor (0&lt;;&#x03C1;&lt;;1); this is much closer to optimal given the previously proven lower bound of t additional copies. In addition, a sequential test based scheme is also proposed to further improve the performance so that p can be set larger to save more communication cost without reducing security. The analytic and simulation results show that the proposed approaches greatly push limit of jamming-resistant broadcast towards optimal.
[telecommunication security, Adaptive systems, Costs, normal narrowband signals, OFDM, malicious receivers, Wireless Networks, spread spectrum communication, Jamming, wireless communication, spread spectrum, Wireless communication, sequential test based scheme, Jamming Attacks, signal jamming attacks, Spread spectrum communication, Bandwidth, Broadcasting, wireless channels, partial channel sharing, multiple channel-sharing broadcast groups, jamming, Interference, adaptive jamming-resistant broadcast systems, communication cost, radio receivers, broadcasting, Computer science, group theory]
How Wireless Power Charging Technology Affects Sensor Network Deployment and Routing
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
As wireless power charging technology emerges, some basic principles in sensor network design are changed accordingly. Existing sensor node deployment and data routing strategies cannot exploit wireless charging technology to minimize overall energy consumption. Hence, in this paper, we (a) investigate the impact of wireless charging technology on sensor network deployment and routing arrangement, (b) formalize the deployment and routing problem, (c) prove it as NP-complete, (d) develop heuristic algorithms to solve the problem, and (e) evaluate the performance of the solutions through extensive simulations. To the best of our knowledge, this is the first effort on adapting sensor network design to leverage wireless charging technology.
[Energy consumption, Costs, wireless sensor networks, wireless charging technology, network routing, Routing, Sensor systems, NP-complete problem, Wireless Sensor Networks, Wireless Charging, Computer science, Wireless sensor networks, optimisation, energy consumption minimization, Photovoltaic cells, Receiving antennas, data routing strategies, Acoustic sensors, sensor network deployment, Monitoring, Power, computational complexity, wireless power charging technology]
Distributed Construction of Connected Dominating Sets with Minimum Routing Cost in Wireless Networks
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
In this paper, we will study a special Connected Dominating Set (CDS) problem - between any two nodes in a network, there exists at least one shortest path, all of whose intermediate nodes should be included in a special CDS, named Minimum rOuting Cost CDS (MOC-CDS). Therefore, routing by MOC-CDS can guarantee that each routing path between any pair of nodes is also the shortest path in the network. Thus, energy consumption and delivery delay can be reduced greatly. CDS has been studied extensively in Unit Disk Graph (UDG) or Disk Graph (DG). However, nodes in networks may have different transmission ranges and some communications may be obstructed by obstacles. Therefore, we model network as a bidirectional general graph in this paper. We prove that constructing a minimum MOC-CDS in general graph is NPhard. We also prove that there does not exist a polynomial-time approximation algorithm for constructing a minimum MOCCDS with performance ratio pln&#x03B4;, where p is an arbitrary positive number (p &lt;; 1) and &#x03B4; is the maximum node degree in network. We propose a distributed heuristic algorithm (called as FlagContest) for constructing MOC-CDS with performance ratio (1 - ln2) + 2ln&#x03B4;. Through extensive simulations, we show that the results of FlagContest is within the upper bound proved in this paper. Simulations also demonstrate that the average length of routing paths through MOC-CDS reduces greatly compared to regular CDSs.
[Energy consumption, radio networks, Costs, NP-hard, Connected dominating set, Spine, delivery delay, Mathematics, Distributed computing, Delay, optimisation, FlagContest, Wireless networks, polynomial approximation, minimum routing cost, Routing protocols, connected dominating sets, obstacle, polynomial time approximation algorithm, bidirectional general graph, unit disk graph, general graph, wireless networks, CDS problem, shortest path, Computer science, Wireless sensor networks, MOC-CDS, NP-hard problem, directed graphs, telecommunication network routing, virtual backbones]
Opportunistic Routing for Interactive Traffic in Wireless Networks
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
To take advantage of the broadcast nature of wireless communication, a number of opportunistic routing protocols have recently been proposed. In order to manage the extra signaling overhead associated with operation of the opportunistic routing, these schemes work in terms of `batches' that consist of multiple packets. While these opportunistic protocols can dramatically improve the total throughput, the use of batches means that they are best suited to bulk UDP transfer. However, in the Internet and wireless networks, the vast majority of the traffic is interactive (e.g., TCP/VoIP which requires close interactions and feedback between the two communicating end points). To effectively support interactive traffic, we develop a new opportunistic routing protocol, called RIPPLE. RIPPLE uses an expedited multi-hop transmission opportunity mechanism to achieve low signaling overhead and eliminate re-ordering, and uses a two-way packet aggregation technique to further reduce overhead. We implement the RIPPLE in NS-2 along with several existing routing protocols, including predetermined routing, shortest path routing, the early version of ExOR, MCExOR, and an IEEE 802.11n-like single-hop packet aggregation scheme called AFR. We compare their performance for long-and short-lived TCP transfers and VoIP traffic over a wide range of network conditions, including varying wireless channel states, collision levels, and types of network topologies. Our results show that the RIPPLE scheme consistently achieves 100% - 300% performance gains over other approaches.
[IEEE 802.11, radio networks, wireless channel states, Telecommunication traffic, IEEE 802.11-like, Throughput, AFR, wireless communication, Wireless communication, interactive traffic, Medium access control (MAC), Network topology, Wireless networks, Feedback, ExOR, Broadcasting, Routing protocols, IP networks, multiple packets, UDP transfer, RIPPLE scheme, multihop transmission opportunity mechanism, telecommunication network topology, wireless networks, TCP transfers, two-way packet aggregation technique, Wireless Mesh Networks, VoIP traffic, Road accidents, single-hop packet aggregation scheme, routing protocols, network topologies, opportunistic routing protocols, Opportunistic Routing, predetermined routing, path routing, MCExOR, Internet telephony, telecommunication traffic, Wireless LANs (WLANs), collision levels]
E-SmallTalker: A Distributed Mobile System for Social Networking in Physical Proximity
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Small talk is an important social lubricant that helps people, especially strangers, initiate conversations and make friends with each other in physical proximity. However, due to difficulties in quickly identifying significant topics of common interest, real-world small talk tends to be superficial. The mass popularity of mobile phones can help improve the effectiveness of small talk. In this paper, we present E-SmallTalker, a distributed mobile communications system that facilitates social networking in physical proximity. It automatically discovers and suggests topics such as common interests for more significant conversations. We build on Bluetooth Service Discovery Protocol (SDP) to exchange potential topics by customizing service attributes to publish non-service-related information without establishing a connection. We propose a novel iterative Bloom filter (IBF) protocol that encodes topics to fit in SDP attributes and achieves a low false positive rate. We have implemented the system in Java ME for ease of deployment. Our experiments on real-world phones show that it is efficient enough at the system level to facilitate social interactions among strangers in physical proximity. To the best of our knowledge, E-SmallTalker is the first distributed mobile system to achieve the same purpose.
[ESmallTalker, Iterative Bloomfilter, Protocols, mobile phones, physical proximity, Mobile communication, Mobile handsets, social networking, Distributed computing, e-smalltalker, mobile computing, Web server, SDP, iterative bloom filter, Social network services, Lubricants, Social Networking, bluetooth service discovery protocol, social lubricant, Milling machines, Computer science, mobile communication, Bluetooth Service Discovery Protocol, Mobile System, social networking (online), Java system, Mobile computing, distributed mobile system]
Fingerprinting Mobile User Positions in Sensor Networks
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
We demonstrate that the network flux over the sensor network provides us fingerprint information about the mobile users within the field. Such information is exoteric in the physical space and easy to access through passive sniffing. We present a theoretical model to the network flux according to the statuses of mobile users. We fit the theoretical model with the network flux measurements through Non-linear Least Squares (NLS) and develop an algorithm that iteratively approaches the NLS solution by Sequential Monte Carlo Estimation. With sparse measurements of the flux information at individual sensor nodes, we are able to identify the mobile users within the network and instantly track their movements without breaking into the details of the communicational packets. A particular advantage of this approach is that compared to the vast information we can reveal the required knowledge is extremely cheap. As all fingerprint information comes from the network flux that is public under current wireless communication medium, our study indicates that most of existing systems are vulnerable in protecting the privacy of mobile users.
[telecommunication security, passive sniffing, wireless sensor networks, network flux, network flux measurements, Humans, Telecommunication traffic, mobile user positions, Fingerprint recognition, Sensor systems, sensor networks, Distributed computing, nonlinear least squares, Monte Carlo methods, Space technology, fingerprint, least squares approximations, wireless communication medium, Least squares approximation, fingerprint information, Computer science, Wireless sensor networks, mobile communication, communicational packets, mobile user, data privacy, sequential Monte Carlo estimation, Mobile computing]
Give2Get: Forwarding in Social Mobile Wireless Networks of Selfish Individuals
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
In this paper we present two forwarding protocols for mobile wireless networks of selfish individuals. We assume that all the nodes are selfish and show formally that both protocols are Nash equilibria, that is, no individual has an interest to deviate. Extensive simulations with real traces show that our protocols introduce an extremely small overhead in terms of delay, while the techniques we introduce to force faithful behavior have the positive side-effect to improve performance by reducing the number of message considerably (more than 20%). We test our protocols also in the presence of a natural variation of the notion of selfishness-nodes that are selfish with outsiders and faithful with people from the same community. Even in this case, our protocols are shown to be very efficient in detecting possible misbehavior.
[Disruption tolerant networking, Costs, Wireless application protocol, faithful behavior, positive side-effect, forwarding protocols, Distributed computing, Delay, Give2Get, mobile computing, packet radio networks, Wireless networks, Nash equilibria, social mobile wireless networks, protocols, mobile radio, social mobility, Buildings, selfish individuals, message, selfishness, Communication switching, Computer science, delay, social networking (online), pocket switched networks, Delay tolerant networks, Mobile computing]
Resource Allocation with Supply Adjustment in Distributed Computing Systems
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
We present two resource-allocation mechanisms for on-demand computing services in parallel and distributed systems, where users pay for their actual usage of the computational resources. We specialize our solution for allocation of grid resources which is a challenging issue due to the dynamic behavior of the system. The problem is studied from the seller's point of view and adjustment of the market capacity is proposed as his strategic move to maximize his profit from resource sharing.
[Protocols, Costs, on demand computing services, Decision making, grid computing, grid resources, Distributed computing, Game theory, Power system modeling, parallel processing, Concurrent computing, distributed computing systems, parallel systems, supply adjustment, resource allocation, resource sharing, Grid computing, Resource management, Capacity planning]
Distributed Gateway Placement for Cost Minimization in Wireless Mesh Networks
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
We study the problem of gateway placement for cost minimization (GPCM) in two-dimensional wireless mesh networks. We are given a set of mesh routers, assume they have identical transmission range r, represented by unit transmission disks around them. A router may be selected as a gateway at certain placing cost. A router is served by a gateway if and only if the gateway is within its transmission range. The goal of this work is to select a set of mesh routers as gateways to serve the rest routers with minimum overall cost. This problem is NP-hard. To the best of our knowledge, no distributed algorithm with a constant approximation ratio has been given before. When all weights are uniform, the best approximation ratio is 38. We present both centralized and distributed algorithms which can achieve approximation ratios 6 + &#x03F5; and 20 respectively. Our algorithms greatly improve the best approximation ratios.
[Portable computers, network servers, Military computing, Spine, wireless mesh networks, two dimensional wireless mesh networks, Mesh networks, optimisation, disk cover, Wireless mesh networks, Cost function, IP networks, Distributed algorithms, cost minimization, unit transmission disks, Application software, gateway placement, distributed algorithm, mesh routers, NP-hard problem, constant approximation ratio, distributed algorithms, telecommunication network routing, Internet, distributed gateway placement]
Efficient Workstealing for Multicore Event-Driven Systems
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Many high-performance communicating systems are designed using the event-driven paradigm. As multicore platforms are now pervasive, it becomes crucial for such systems to take advantage of the available hardware parallelism. Event-coloring is a promising approach in this regard. First, it allows programmers to simply and progressively inject support for the safe, parallel execution of multiple event handlers through the use of annotations. Second, it relies on a workstealing algorithm to dynamically balance the execution of event handlers on the available cores. This paper studies the impact of the workstealing algorithm on the overall system performance. We first show that the only existing workstealing algorithm designed for event-coloring runtimes is not always efficient: for instance, it causes a 33% performance degradation on a Web server. We then introduce several enhancements to improve the workstealing behavior. An evaluation using both micro benchmarks and real applications, a Web server and the Secure File Server (SFS), shows that our system consistently outperforms a state-of-the-art runtime (Libasync-smp), with or without workstealing. In particular, our new workstealing improves performance by up to +25% compared to Libasync-smp without workstealing and by up to +73% compared to the Libasync-smp workstealing algorithm, in the Web server case.
[Algorithm design and analysis, Heuristic algorithms, event-driven, File servers, parallel processing, multicore, Degradation, Runtime, System performance, file servers, Hardware, secure file server, Web server, Multicore processing, performance evaluation, workstealing, system services, Programming profession, workstealing algorithm, hardware parallelism, performance, micro benchmark, multicore event driven system, Internet, event coloring]
On Achieving Maximum Secure Throughput Using Network Coding against Wiretap Attack
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
In recent years network coding has attracted significant attention in telecommunication. The benefits of network coding to a communication network include the increased throughput as well as secure data transmission. The purpose of this work is to design secure linear network coding against wiretap attack. The problem is to maximize the transmission data rate of multiple unicast streams between a pair of source and destination nodes, under the condition of satisfying the weakly secure requirements. Different from most existing research on network coding that designs the network coding scheme based on a given network topology, we will consider the integrated network topology design and network coding design. Such an integrated approach has not been reported by other researchers. In this paper, we formally introduce the problem, prove the problem is computational intractable, and then develop efficient heuristic algorithms. We first try to find the transmission topology that is suitable for network coding. Based on the topology, we design linear network coding scheme that is weakly secure. We conduct simulations to show that the proposed algorithms can achieve good performance.
[telecommunication security, network coding, wiretap attack, communication network, heuristic algorithms, data transmission security, telecommunication network topology, Throughput, Decoding, network topology, Distributed computing, telecommunication computing, data rate transmission, Computer science, Telecommunication network topology, Network topology, Information security, Network coding, maximum secure throughput, Data communication, Communication networks, computational complexity]
LT Network Codes
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Network coding has been successfully applied in large-scale content dissemination systems. While network codes provide optimal throughput, its current forms suffer from a high decoding complexity. This is an issue when applied to systems composed of nodes with low processing capabilities, such as sensor networks. In this paper, we propose a novel network coding approach based on LT codes, initially introduced in the context of erasure coding. Our coding scheme, called LTNC, fully benefits from the low complexity of belief propagation decoding. Yet, such decoding schemes are extremely sensitive to statistical properties of the code. Maintaining such properties in a fully decentralized way with only a subset of encoded data is challenging. This is precisely what the recoding algorithms of LTNC achieve. We evaluate LTNC against random linear network codes in an epidemic content-dissemination application. Results show that LTNC increases communication overhead (20\\%) and convergence time (30\\%) but greatly reduces the decoding complexity (99%) when compared to random linear network codes. In addition, LTNC consistently outperforms dissemination protocols without codes, thus preserving the benefit of coding.
[LT network codes, linear codes, epidemic content-dissemination application, large-scale content dissemination systems, decoding complexity, Throughput, erasure coding, Sensor systems, sensor networks, recoding algorithms, set theory, Distributed computing, dissemination protocols, LT Codes, Large-scale systems, error correction codes, Multicast protocols, Encoding, Decoding, Peer-to-Peer, decoding, network coding approach, random codes, Wireless sensor networks, random linear network codes, Network Coding, Network coding, belief propagation decoding, Belief propagation, computational complexity, optimal throughput]
Using Analog Network Coding to Improve the RFID Reading Throughput
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
RFID promises to revolutionize the inventory management in large warehouses, retail stores, hospitals, transportation systems, etc. Periodically reading the IDs of the tags is an important function to guard against administration error, vendor fraud and employee theft. Given the low-speed communication channel in which a RFID system operates, the reading throughput is one of the most important performance metrics. The current protocols have reached the physical throughput limit that can possibly be achieved based on their design methods. To break that limit, we have to apply fundamentally different approaches. This paper investigates how much throughput improvement the analog network coding can bring when it is integrated into the RFID protocols. The idea is to extract useful information from collision slots when multiple tags transmit their IDs simultaneously. Traditionally, those slots are discarded. With analog network coding, we show that a collision slot is almost as useful as a non-collision slot in which exactly one tag transmits. We propose the framed collision-aware tag identification protocol that optimally applies analog network coding to maximize the reading throughput, which is 51.1% ~ 70.6% higher than the best existing protocols.
[Measurement, network coding, Protocols, radiofrequency identification, telecommunication congestion control, Transportation, collision-aware tag identification protocol, Throughput, collision slots, analog network coding, RFID protocols, Hospitals, Intrusion detection, Communication channels, Network coding, Inventory management, RFID reading throughput, protocols, Radiofrequency identification]
TSF: Trajectory-Based Statistical Forwarding for Infrastructure-to-Vehicle Data Delivery in Vehicular Networks
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
This paper proposes a data forwarding scheme called Trajectory-based Statistical Forwarding (TSF), tailored for the data delivery from infrastructure nodes (e.g., Internet access points) to moving vehicles in vehicular networks. To our knowledge, this paper presents the first attempt to investigate how to effectively utilize the packet destination vehicle's trajectory for such an infrastructure-to-vehicle data delivery. This data delivery is performed through the computation of a target point based on the destination vehicle's trajectory that is an optimal rendezvous point of the packet and the destination vehicle. TSF forwards packets over multi-hop to a selected target point where the vehicle is expected to pass by. Such a target point is selected optimally to minimize the packet delivery delay while satisfying the required packet delivery probability. The optimality is achieved analytically by utilizing the packet's delivery delay distribution and the destination vehicle's travel delay distribution. Through theoretical analysis and extensive simulation, it is shown that our design provides an efficient data forwarding under a variety of vehicular traffic conditions.
[Disruption tolerant networking, trajectory-based statistical forwarding, infrastructure nodes, infrastructure-to-vehicle data delivery, Distributed computing, Delay, Target Point, Vehicle, Road vehicles, Infrastructure, vehicular networks, Trajectory, travel delay distribution, data forwarding scheme, Navigation, Vehicular Networks, Vehicle driving, probability, Ad hoc networks, Data Forwarding, Global Positioning System, mobile communication, delivery delay distribution, Vehicle safety, Distribution, Road safety, statistical analysis, packet delivery probability]
When Transportation Meets Communication: V2P over VANETs
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Information interaction is a crucial part of modern transportation activities. In this paper, we propose the idea of Vehicle-to-Passenger communication (V2P), which allows direct, instant, and flexible communication between moving vehicles and roadside passengers. With pocket wireless devices, passengers can easily join VANETs as roadside nodes, and express their travel demands, e.g., taking a free ride or calling a taxi via radio queries over VANETs. Once a matched vehicle is found through the disseminated queries, the driver can decide whether to provide corresponding services, especially the carrying of passengers and goods. We investigate the main challenges in vehicle calling, establish a trip history model to predict vehicle movement, and develop typical query dissemination schemes to match the target vehicle in vehicular networks. With V2P over VANETs, vehicle transportation is capable of open and efficient P2P information interaction, and thus benefits from relevant efficiency improvement. Based on a realistic travel survey and simulation, we prove that vehicle calling is effective and efficient in casual carpooling and taxi calling.
[flexible communication, radio query, modern transportation activity, typical query dissemination schemes, Mobile communication, vehicles, Distributed computing, vehicle movement, vehicle transportation, Road transportation, vehicle calling, Supply and demand, target vehicle, Road vehicles, ride-sharing, vehicular networks, trip history model, peer-to-peer computing, Peer to peer computing, Vehicle driving, VANET, P2P information interaction, Computer science, pocket wireless devices, V2P, roadside passengers, vehicle-to-passenger communication, roadside nodes, carpool, Internet, ad hoc networks, taxi calling, disseminated query, Mobile computing, carpooling]
Safe and Stabilizing Distributed Cellular Flows
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Advances in wireless vehicular networks present us with opportunities for developing new distributed traffic control algorithms that avoid phenomena such as abrupt phase transitions. Towards this end, we study the problem of distributed traffic control in a partitioned plane where the movement of all entities (vehicles) within each partition (cell) is tightly coupled. We present a distributed traffic control protocol that guarantees minimum separation between vehicles at all times, even when some cells' control software may fail. Once failures cease, the protocol is guaranteed to stabilize and the vehicles with feasible paths to a target cell make progress towards it. The algorithm relies on two general principles: temporary blocking for maintenance of safety and local geographical routing for guaranteeing progress. Our proofs use mostly assertional reasoning and may serve as a template for analyzing other safe and stabilizing distributed traffic control protocols. We also present simulation results which provide estimates of throughput as a function of vehicle velocity, safety separation, path complexity, and failure-recovery rates.
[radio networks, Protocols, telecommunication congestion control, Routing, wireless vehicular networks, Partitioning algorithms, local geographical routing, Distributed computing, temporary blocking, Road transportation, Condition monitoring, Cellular networks, Vehicle safety, routing protocols, distributed cellular flows, Traffic control, Communication system traffic control, distributed traffic control protocols, telecommunication traffic, cellular radio]
P2P Streaming Capacity under Node Degree Bound
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Two of the fundamental problems in peer-to-peer (P2P) streaming are as follows: what is the maximum streaming rate that can be sustained for all receivers, and what peering algorithms can achieve close to this maximum? These problems of computing and approaching the P2P streaming capacity are often challenging because of the constraints imposed on overlay topology. In this paper, we focus on the limit of P2P streaming rate under node degree bound, i.e., the number of connections a node can maintain is upper bounded. We first show that the streaming capacity problem under node degree bound is NP Complete in general. Then, for the case of node out-degree bound, through the construction of a &#x201C;Bubble algorithm&#x201D;, we show that the streaming capacity is at least half of that of a much less restrictive and previously studied case, where we bound the node degree in each streaming tree but not the degree across all trees. Then, for the case of node total-degree bound, we develop a &#x201C;Cluster-Tree algorithm&#x201D; that provides probabilistic guarantee of achieving a rate close to the maximum rate achieved under no degree bound constraint, when the node degree bound is logarithmic in network size. The effectiveness of these algorithms in approaching the capacity limit is demonstrated in simulations using uplink bandwidth statistics of Internet hosts. Both analysis and numerical experiments show that peering in a locally dense and globally sparse manner achieves near-optimal streaming rate if the degree bound is at least logarithmic in network size.
[graph theory, peer-to-peer streaming, Distributed computing, cluster-tree algorithm, Node Degree Bound, node degree bound, Tree graphs, streaming capacity problem, Clustering algorithms, Bandwidth, P2P Streaming Capacity, Node Out-Degree Bound, Internet hosts, bubble algorithm, peer-to-peer computing, Peer to peer computing, probability, Topology, NP-complete problem, Statistics, uplink bandwidth statistics, probabilistic guarantee, Streaming media, Internet, P2P Streaming, P2P streaming capacity, Capacity planning, computational complexity]
Replica Placement in P2P Storage: Complexity and Game Theoretic Analyses
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
In peer-to-peer storage systems, peers replicate each others' data in order to increase availability. If the matching is done centrally, the algorithm can optimize data availability in an equitable manner for all participants. However, if matching is decentralized, the peers' selfishness can greatly alter the results, leading to performance inequities that can render the system unreliable and thus ultimately unusable. We analyze the problem using both theoretical approaches (complexity analysis for the centralized system, game theory for the decentralized one) and simulation. We prove that the problem of optimizing availability in a centralized system is NP-hard. In decentralized settings, we show that the rational behavior of selfish peers will be to replicate only with similarly-available peers. Compared to the socially-optimal solution, highly available peers have their data availability increased at the expense of decreased data availability for less available peers. The price of anarchy is high: unbounded in one model, and linear with the number of time slots in the second model. We also propose centralized and decentralized heuristics that, according to our experiments, converge fast in the average case. The high price of anarchy means that a completely decentralized system could be too hostile for peers with low availability, who could never achieve satisfying replication parameters. Moreover, we experimentally show that even explicit consideration and exploitation of diurnal patterns of peer availability has a small effect on the data availability-except when the system has truly global scope. Yet a fully centralized system is infeasible, not only because of problems in information gathering, but also the complexity of optimizing availability. The solution to this dilemma is to create system-wide cooperation rules that allow a decentralized algorithm, but also limit the selfishness of the participants.
[cooperation rules, distributed storage, Memory, complexity analysis, Data engineering, Distributed computing, socially optimal solution, Analytical models, price of anarchy, optimisation, NP hard, decentralized heuristics, Bandwidth, replica placement, Availability, peer-to-peer storage systems, centralized heuristics, peer-to-peer computing, centralized system, Peer to peer computing, Social network services, equitable optimization, game theory, game theoretic analyses, Game theory, information storage, Computer science, availability optimization, computational complexity]
Understanding and Improving Ratio Incentives in Private Communities
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Incentive mechanisms play a critical role in P2P systems. Private BitTorrent sites use a novel incentive paradigm, where the sites record upload and download amounts of users and require each user to maintain its upload-to-download ratio above a specified threshold. This paper explores in-depth incentives in private P2P file-sharing systems. Our contributions are threefold. We first conduct a measurement study on a representative private BitTorrent site, examining how incentives influence user behavior. Our measurement study shows that, as compared with public torrents, a private BitTorrent site provides more incentive for users to contribute and seed. Second, we develop a game theoretic model and analytically show that the ratio mechanism indeed provides effective incentives. But existing ratio incentives in private BitTorrent sites are vulnerable to collusions. Third, to prevent collusion, we propose an upload entropy scheme, and show through analysis and experiment that the entropy scheme successfully limits colluding, while rarely affecting normal users who do not collude.
[Chaos, incentive mechanisms, private BitTorrent sites, peer-to-peer computing, game theory, Maintenance engineering, Entropy, incentive schemes, user interfaces, Game theory, Distributed computing, Sun, Computer science, ratio incentives, entropy, upload entropy scheme, Aggregates, peer-to-peer systems, Bandwidth, game theoretic model, P2P systems, Robustness, collusion prevention, user behavior]
Divide and Conquer Algorithms for Publish/Subscribe Overlay Design
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Overlay network design for topic-based publish/subscribe systems is of primary importance because the overlay directly impacts the system's performance. Determining a topic-connected overlay, in which for every topic the graph induced by nodes interested in the topic is connected, is a fundamental problem. Existing algorithms for this problem suffer from three key drawbacks: (1) prohibitively high running time cost, (2) requirement of full system knowledge and centralized operation, and (3) constructing overlay from scratch. From a practical point of view, these are all significant limitations. To address these concerns, in this paper, we develop novel algorithms that efficiently solve the problem of dynamically joining two or more topic-connected overlays. Inspired from the divide-and-conquer character of our approach, we derive an algorithm that solves the original problem at a fraction (up to 1.7\\%) of the running time cost of alternative solutions, but at the expense of an empirically insignificant increase in the average node degree.
[Algorithm design and analysis, Costs, message passing, Filtering, divide and conquer methods, divide and conquer algorithms, publish/subscribe overlay network design, Distributed computing, Jacobian matrices, Computer science, optimisation, Publishing, System performance, topic-connected overlay, Feeds, Informatics, middleware]
B-SUB: A Practical Bloom-Filter-Based Publish-Subscribe System for Human Networks
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
The adoption of portable wireless devices is rapidly rising. The demand for efficient communication protocols amongst these devices is pressing. In this paper, we present a content-based publish-subscribe system, called B-SUB Bloom filter-based pub-SUB system), for the networks formed by human-carried wireless devices, which are called human networks (HUNETs). A novel data structure, called Temporal Counting Bloom Filter (TCBF), is proposed to perform content based networking tasks. The TCBF's novelty is that it is able to handle temporal operations, which are not supported in the classic Bloom filter (BF) and are crucial to the success of forwarding messages in HUNETs. B-SUB uses TCBFs to encode users' interests and embed routing information. Using the TCBF, B-SUB can propagate interests by transmitting at most two TCBFs of dozens of bytes, which makes B-SUB space efficient. B-SUB makes forwarding decisions through querying the TCBFs, which is simple and fast. These designs make B-SUB pretty suitable for resource-constrained HUNETs. However, the TCBF has false positives, which will potentially cause useless messages to be injected into the network. The issue that arises here is how to handle its false positives in queries, and at the same time maintain its spatial efficiency as well. So, we analyze several methods for controlling the TCBF's false positive rate. B-SUB's viability and usefulness are verified through extensive simulation studies using real-world human contact traces.
[Disruption tolerant networking, Protocols, Portable computers, Humans, data structure, Twitter, information filtering, Bloom filter based publish subscribe system, Distributed computing, Filters, content based publish subscribe system, content based networking task, Computer networks, data structures, portable wireless devices, Social network services, temporal counting Bloom filter, resource constrained HUNET, communication protocols, Bloom filter, delay tolerant networks, B-SUB Bloom filter based pub sub system, human networks, human network, Publish-subscribe, social network analysis, social networking (online)]
Parameterized Maximum and Average Degree Approximation in Topic-Based Publish-Subscribe Overlay Network Design
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Publish/subscribe communication systems where nodes subscribe to many different topics of interest are becoming increasingly more common. Designing overlay networks that connect the nodes subscribed to each distinct topic is hence a fundamental problem in these systems. For scalability and efficiency, it is important to keep the degree of the nodes in the publish/subscribe system low. Ideally one would like to be able not only to keep the average degree of the nodes low, but also to ensure that all nodes have equally the same degree, giving rise to the following problem: Given a collection of nodes and their topic subscriptions, connect the nodes into a graph with low average and maximum degree such that for each topic t, the graph induced by the nodes interested in t is connected. We present the first polynomial time parameterized sub linear approximation algorithm for this problem. We also propose two heuristics for constructing topic connected networks with low average degree and constant diameter and validate our results through simulations. In fact, the results in this section are a refinement of the preliminary results by Onus and Richa in INFOCOM'09.
[client-server systems, topic connected network, Costs, message passing, peer-to-peer computing, Peer to peer computing, Scalability, Subscriptions, graph theory, Telecommunication traffic, publish communication system, overlay network design, Distributed computing, polynomial time parameterized sublinear approximation algorithm, Computer science, Design engineering, Publish-subscribe, Computer networks, subscribe communication system]
Publisher Placement Algorithms in Content-Based Publish/Subscribe
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Many publish/subscribe systems implement a policy for clients to join to their physically closest broker to minimize transmission delays incurred on the clients' messages. However, the amount of delay reduced by this policy is only the tip of the iceberg as messages incur queuing, matching, transmission, and scheduling delays from traveling across potentially long distances in the broker network. Additionally, the clients' impact on system load is totally neglected by such a policy. This paper proposes two new algorithms that intelligently relocate publishers on the broker overlay to minimize both the overall end-to-end delivery delay and system load. Both algorithms exploit live publication distribution patterns but with different optimization metrics and computation methodologies to determine the best relocation point. Evaluations on PlanetLab and a cluster testbed show that our algorithms can reduce the average input load of the system by up to 68%, average broker message rate by up to 85%, and average delivery delay by up to 68%.
[publisher placement algorithms, Content-based publish/subscribe, System testing, load minimization, Delay systems, delivery delay minimization, Optimization methods, optimization metrics, publisher repositioning, Distributed computing, broker network, optimisation, content based subscribe, Clustering algorithms, content based publish, computation methodologies, transmission delays, publishing, publisher migration, scheduling delays]
DISCO: Memory Efficient and Accurate Flow Statistics for Network Measurement
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
A basic task in network passive measurement is collecting flow statistics information for network state characterization. With the continuous increase of Internet link speed and the number of flows, flow statistics has become a great challenge due to the demanding requirements on both memory size and memory bandwidth in measurement devices. In this paper, we propose a DIScount COunting (DISCO) method, which is designed for both flow size and flow volume counting. For each incoming packet of length l, DISCO increases the corresponding counter assigned to the flow with an increment that is less than l. With an elaborate design on the counter update rule and the inverse estimation, DISCO saves memory consumption while providing an accurate unbiased estimator. The method is evaluated thoroughly under theoretical analysis and simulations with synthetic and real traces. The results demonstrate that DISCO is more accurate than related work given the same counter size. DISCO is also implemented on network processor Intel IXP2850 for performance test. Using only one MicroEngine (ME) in IXP2850, the throughput can reach up to 11.1Gbps under a traditional traffic pattern, and it increases almost linearly with the number of MEs employed.
[Internet link speed, flow statistics, Design methodology, Size measurement, discount counting method, Statistics, Counting circuits, Analytical models, network passive measurement, network state characterization, Fluid flow measurement, Bandwidth, DISCO, Internet, Velocity measurement, statistical analysis, telecommunication traffic, Testing]
Minimizing Probing Cost and Achieving Identifiability in Network Link Monitoring
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Continuously monitoring the link performance is important to network diagnosis. Recently, active probes sent between end systems are widely used to monitor the link performance. In this paper, we address the problem of minimizing the probing cost and achieving identifiability in link monitoring. Given a set of links to monitor, our objective is to select as few probing paths as possible to cover all of them, and the selected probing paths can uniquely identify all identifiable links being monitored. We propose an algorithm based on the linear system model to find out all sets of probing paths that can uniquely identify an identifiable link. We extend the bipartite model to reflect the relation between a set of probing paths and the link that can be uniquely identified. Through the extended bipartite model, our optimization problem is transformed into the classic set cover problem, which is NP-hard. Therefore, we propose a heuristic based algorithm to greedily select the probing paths. Our method eliminates two types of redundant probing paths, i.e., those that can be replaced by others and those that cannot be used to achieving identifiability. Simulations based on real network topologies show that our approach can achieve identifiability with very low probing cost. Compared with prior work, our method is more general and has better performance.
[Linear systems, continuously monitoring, optimization problem, Costs, network link monitoring, extended bipartite model, Heuristic algorithms, linear system model, Electronic mail, identifiable link, Distributed computing, Network topology, probing path selection, link performance, Probes, Computerized monitoring, greedy algorithms, heuristic based algorithm, linear systems, Computer science, computer network management, NP-hard problem, greedy selection, Performance loss, minimisation, probing cost minimization, computational complexity]
Sifting through Network Data to Cull Activity Patterns with HEAPs
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Today's large campus and enterprise networks are characterized by their complexity, i.e. containing thousands of hosts, and diversity, i.e. with various applications and usage patterns. To effectively manage and secure such networks, network operators and system administrators are faced with the challenge of characterizing, profiling and tracking activity patterns passing through their networks. Because of the large number of IP addresses and the prevalence of dynamic IP addresses, profiling and tracking individual hosts may not be effective nor scalable. In this paper, we develop a hierarchical extraction of activity patterns (HEAPs), which is a method for characterizing and profiling activity patterns within subnets. By representing activities within a subnet in a host-port association matrix (HPAM) and applying pLSA, we obtain co-clusters that capture the significant and dominant activity patterns of the subnet. Using these co-clusters, we utilize hierarchical clustering to cluster activity patterns to assist network operators and security analysts gain a &#x201D;big-picture&#x201D; view of the network activity-patterns. We also develop a novel method to track and quantify changes in activity patterns within subnets over time and demonstrate how to utilize this method to identify major changes and anomalies within the network.
[Telecommunication traffic, IP addresses, business communication, Data engineering, Data mining, Application software, Distributed computing, HEAP, Secure storage, hierarchical extraction of activity pattern, host port association matrix, Computer science, Network servers, system administrator, pattern clustering, enterprise network, network operator, hierarchical clustering, data structures, Computer network management, IP networks, Pattern analysis, pLSA]
Adam2: Reliable Distribution Estimation in Decentralised Environments
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
To enable decentralised actions in very large distributed systems, it is often important to provide the nodes with global knowledge about the values of attributes across all nodes. This paper shows how, given an attribute whose values are distributed across a large decentralised system, each node can efficiently estimate the statistical distribution of these values. Simulations using heavily skewed real-world node attribute distributions show that our estimation methods outperform the state-of-the-art heuristics by an order of magnitude with an average error of 0.05% and a maximum error of 2%. To obtain this accuracy, each node sends on average just 120 kB of data independent of the system size. Our algorithms also achieve this accuracy in the presence of heavy churn of system membership. Furthermore, our algorithm enables self-tuning by continuously estimating the accuracy of its own distribution approximation.
[Adam2, Protocols, Peer to peer computing, decentralised environment, large distributed system, CDF, error analysis, distributed processing, aggregation, statistical distributions, Distributed computing, heavily skewed real world node, Computer science, reliable distribution estimation, Statistical distributions, statistical distribution, Approximation algorithms, Robustness, gossip, Large-scale systems, State estimation, Monitoring]
Distributed Node Coloring in the SINR Model
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Given a palette P of at most V colors, and a parameter d, a (d, V)-coloring of a graph is an assignment of a color from the palette P to every node in the graph such that any two nodes at distance at most d have different colors. We prove that for every n-node unit disk graph with maximum degree &#x0394;, there exists a distributed algorithm computing a (1,O(&#x0394;))-coloring under the SINR (Signal-to-Interferenceplus-Noise Ratio) physical model in at most O(&#x0394; log n) time slots, which is optimal up to a logarithmic factor. Our result is based on revisiting a previous coloring algorithm, due to T. Moscibroda and R. Wattenhofer, described in the so called graph-based model. We also prove that, for a well defined constant d, a (d, O(&#x0394;))-coloring allows us to schedule an interference free TDMA-like MAC protocol under the physical SINR constraints. As a corollary, any uniform interferencefree message passing algorithm with running time r can be simulated in the SINR model in O(&#x0394;(log n+&#x03C4;)) time slots. The latter generic result provides new insights into the distributed scheduling of radio network tasks under the harsh SINR constraints.
[signal-to-interference-plus-noise ratio model, Distributed computing, graph colouring, Time Complexity, Radio network, Distributed algorithms, SINR model, Coloring, message passing, TDMA, node unit disk graph, logarithmic factor, Distributed Algorithm, Color, Access protocols, graph coloring, message passing algorithm, media access control, Interference constraints, distributed node coloring, Wireless sensor networks, time division multiple access, Media Access Protocol, SINR Model, Radio communication, Signal to noise ratio, computational complexity, interference free TDMA-like MAC protocol]
Stabilizing Locally Maximizable Tasks in Unidirectional Networks Is Hard
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
A distributed algorithm is self-stabilizing if after faults and attacks hit the system and place it in some arbitrary global state, the system recovers from this catastrophic situation without external intervention in finite time. In this paper, we consider the problem of constructing self-stabilizingly a locally maximizable task (such as constructing a maximal independent set, a maximal matching, or a grundy coloring) in uniform unidirectional networks of arbitrary shape. On the negative side, we present evidence that in uniform networks, deterministic self-stabilization of this problem is impossible. Also, the silence property (i.e. having communication fixed from some point in every execution) is impossible to guarantee, either for deterministic or for probabilistic variants of protocols. On the positive side, we present a series of generic protocols that can be instantiated for all considered locally maximizable tasks. First, we design a deterministic protocol for arbitrary unidirectional networks with unique identifiers that exhibits polynomial space and time complexity in asynchronous scheduling. We complement the study with probabilistic protocols for the uniform case: the first probabilistic protocol requires infinite memory but copes with asynchronous scheduling, while the second probabilistic protocol has polynomial space complexity but can only handle synchronous scheduling. Both probabilistic solutions have expected polynomial time complexity.
[Distribtued algorithms, Protocols, Shape, self-stabilization, Humans, unidirectionnal networks, maximal independent set, Distributed computing, deterministic protocol, Network topology, Wireless networks, scheduling, Polynomials, Computer networks, protocols, Distributed algorithms, polynomial time complexity, unidirectional network, grundy coloring, asynchronous scheduling, maximal matching, distributed algorithm, distributed algorithms, Bidirectional control, probabilistic protocol, fault tolerant computing, polynomial space complexity, computational complexity]
Towards Power-Sensitive Communication on a Multiple-Access Channel
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
We are given n stations of which k are active, while the remaining n - k are asleep. The active stations communicate via a multiple-access channel. If a subset Q of active stations transmits in the same round, all active stations can recognize from the signal strength how many stations have transmitted (i.e., they learn the size of set Q), even though they may not be able to decode the contents of transmitted messages. The goal is to let each active station to learn about the set of all active stations. It is well known that &#x0398;(k log<sub>k + 1</sub> n) rounds are enough, even for non-adaptive deterministic algorithms. A natural interesting generalization arises when we are required to identify a subset of m &#x2264; k active stations. We show that while for randomized or for adaptive deterministic algorithms O(m log<sub>m+1</sub> n) rounds are sufficient, the non-adaptive deterministic counterpart still requires &#x0398;(k log<sub>k + 1</sub> n) rounds; therefore, finding any subset of active stations is not easier than finding all of them by a nonadaptive deterministic algorithm. We prove our results in the more general framework of combinatorial search theory, where the problem of identifying active stations on a multiple-access channel can be viewed as a variant of the well-known counterfeit coin problem.
[Algorithm design and analysis, multiple-access channel, combinatorial mathematics, Adaptive algorithm, nonadaptive deterministic algorithm, Decoding, randomized algorithm, communication complexity, Counterfeiting, Distributed computing, deterministic algorithms, randomised algorithms, signal strength, Councils, Feedback, randomized algorithms, power-sensitive communication, distributed learning, multi-access systems, wireless channels, Springs, search problems, Testing, combinatorial search theory]
Projection and Division: Linear-Space Verification of Firewalls
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
A firewall is a packet filter that is placed at the entrance of a private network. It checks the header fields of each incoming packet into the private network and decides, based on the specified rules in the firewall, whether to accept the packet and allow it to proceed, or to discard the packet. A property of a firewall is a set of packets that the firewall is required to accept or discard. Associated with each firewall is a very large set of properties that the firewall needs to satisfy. The space and time complexity of the best known deterministic algorithm, for verifying that a given firewall satisfies a given property, is 0(nd), where n is the number of rules in the given firewall and d is the number of fields checked by the firewall. Usually, n is around 2000 and d is 5. In this paper, we propose the first deterministic firewall verification algorithm whose space complexity is 0(nd), linear in both n and d. This algorithm consists of three components: a projection pass, a division pass, and a probe algorithm. We applied our verification algorithm to over two million firewall-property pairs, varying n from 100 to 10000 and fixing d at 5. From this experiment, we observed that the algorithm requires (900 + 0.5n) Kilobytes of storage and in the order of 10 seconds execution time.
[Algorithm design and analysis, private network, Information filtering, Distributed computing, Computational complexity, computer network security, packet filter, division, authorisation, Information filters, linear space verification, Computer networks, firewall verification algorithm, projection, IP networks, Probes, space complexity, computational complexity, firewall verification]
Localized Algorithm for Precise Boundary Detection in 3D Wireless Networks
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
This research focuses on distributed and localized algorithms for precise boundary detection in 3D wireless networks. Our objectives are in two folds. First, we aim to identify the nodes on the boundaries of a 3D network, which serve as a key attribute that characterizes the network, especially in such geographic exploration tasks as terrain and underwater reconnaissance. Second, we construct locally planarized 2-manifold surfaces for inner and outer boundaries, in order to enable available graph theory tools to be applied on 3D surfaces, such as embedding, localization, partition, and greedy routing among many others. To achieve the first objective, we propose a Unit Ball Fitting (UBF) algorithm that discovers a set of potential boundary nodes, followed by a refinement algorithm, named Isolated Fragment Filtering (IFF), which removes isolated nodes that are misinterpreted as boundary nodes by UBF. Based on the identified boundary nodes, we develop an algorithm that constructs a locally planarized triangular mesh surface for each 3D boundary. Our proposed scheme is localized, requiring information within one-hop neighborhood only. Our simulation results demonstrate that the proposed algorithms can effectively identify boundary nodes and surfaces, even under high measurement errors. As far as we know, this is the first work for discovering boundary nodes and constructing boundary surfaces in 3D wireless networks.
[radio networks, Event detection, wireless sensor networks, graph theory, isolated fragment filtering, boundary detection, Distributed computing, Temperature sensors, refinement algorithm, Wireless networks, Fires, localized algorithm, geographic exploration tasks, Filtering algorithms, Computer networks, unit ball fitting algorithm, graph theory tools, triangulation, 3D wireless network, underwater reconnaissance, Partitioning algorithms, greedy routing, Temperature measurement, distributed algorithm, 3D, planarized 2-manifold surfaces, Surface fitting]
Duty-Cycle-Aware Minimum Latency Broadcast Scheduling in Multi-hop Wireless Networks
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Broadcast is an essential and widely-used operation in multi-hop wireless networks. Minimum latency broadcast scheduling (MLBS) aims to provide a collision-free scheduling for broadcast with the minimum latency. Previous work on MLBS mostly assumes that nodes are always active, and thus is not suitable for duty-cycle-aware scenarios. In this paper, we investigate the duty-cycle-aware minimum latency broadcast scheduling (DCA-MLBS) problem in multi-hop wireless networks. We prove both the one-to-all and the all-to-all DCA-MLBS problems to be NP-hard. We propose a novel approximation algorithm called OTAB for the one-to-all DCA-MLBS problem, and two approximation algorithms called UTB and UNB for the all-to-all DCA-MLBS problem under the unit-size and the unbounded-size message models respectively. The OTAB algorithm achieves a constant approximation ratio of 17|T|, where |T| denotes the number of time-slots in a scheduling period. The UTB and UNB algorithms achieve the approximation ratios of 17|T|+20 and (&#x0394;+22)|T| respectively, where &#x0394; denotes the maximum node degree of the network. Extensive simulations are conducted to evaluate the performance of our algorithms.
[radio networks, DCA-MLBS, telecommunication congestion control, Broadcast technology, OTAB algorithm, multihop wireless networks, collision-free scheduling, Distributed computing, Delay, Scheduling algorithm, broadcasting, optimisation, duty-cycle-aware minimum latency broadcast scheduling, Processor scheduling, NP-hard problem, Wireless networks, Spread spectrum communication, Broadcasting, scheduling, Approximation algorithms, Computer networks]
On the Impact of MIMO Diversity on Higher Layer Performance
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
In this paper, we shed light on the cross-layer interactions between the PHY, link and routing layers in networks with MIMO links operating in the diversity mode. Many previous studies assume an overly simplistic PHY layer model that does not sufficiently capture these interactions. We show that the use of simplistic models can in fact lead to misleading conclusions with regards to the higher layer performance with MIMO diversity. Towards understanding the impact of various PHY layer features on MIMO diversity, we begin with a simple but widely-used model and progressively incorporate these features to create new models. We examine the goodness of these models by comparing the simulated performance results with each, with measurements on an indoor 802.11 n testbed. Our work reveals several interesting cross-layer dependencies that affect the gains due to MIMO diversity. In particular, we observe that relative to SISO links: (a) PHY layer gains due to MIMO diversity do not always carry over to the higher layers, (b) the use of other PHY layer features such as FEC codes significantly influence the gains due to MIMO diversity, and (c) the choice of the routing metric can impact the gains possible with MIMO.
[Protocols, diversity reception, modeling, diversity mode, Bit error rate, Transmitting antennas, higher layer performance, MIMO links, Physical layer, Throughput, Routing, Distributed computing, PHY layer, diversity, Wireless networks, cross-layer interactions, telecommunication network routing, MIMO diversity, MIMO, routing layers, MIMO communication, Testing]
Multi-dimensional Conflict Graph Based Computing for Optimal Capacity in MR-MC Wireless Networks
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Optimal capacity analysis in multi-radio multi-channel wireless networks by nature incurs the formulation of a mixed integer programming, which is NP-hard in general. The current state of the art mainly resorts to heuristic algorithms to obtain an approximate solution. In this paper, we propose a novel concept of multi-dimensional conflict graph (MDCG). Based on MDCG, the capacity optimization issue can be accurately modeled as a linear programming (LP) multi-commodity flow (MCF) problem, augmented with maximal independent set (MIS) constraints. The MDCG-based solution will provide not only the maximum throughput or utility, but also the optimal configurations on routing, channel assignment, and scheduling. Moreover, the MDCG-based optimal capacity planning can exploit dynamic channel swapping, which is difficult to achieve for those existing heuristic algorithms. A particular challenge associated with the MDCG-based capacity analysis is to search exponentially many possible MISs. We theoretically show that in fact only a small set of critical MISs, termed as critical MIS set, will be scheduled in the optimal resource allocation. We then develop a polynomial computing method, based on a novel scheduling index ordering (SIO) concept, to search the critical MIS set. Extensive numerical results are presented to demonstrate the efficiency of the MDCG-based resource allocation compared to well-known heuristic algorithm presented in, and the efficiency of SIO-based MIS computing compared to the widely adopted random algorithm for searching MISs.
[radio networks, scheduling index ordering, Heuristic algorithms, graph theory, channel capacity, maximal independent set, Throughput, multiradio multichannel wireless networks, linear programming, Constraint optimization, Wireless networks, Computer networks, wireless channels, channel allocation, telecommunication network planning, heuristic algorithms, multidimensional conflict graph, polynomial computing method, Linear programming, Routing, MR-MC wireless networks, Processor scheduling, NP-hard problem, telecommunication network routing, dynamic channel swapping, mixed integer programming, optimal capacity analysis, multicommodity flow, Resource management, Capacity planning, computational complexity]
Sentomist: Unveiling Transient Sensor Network Bugs via Symptom Mining
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Wireless Sensor Network (WSN) applications are typically event-driven. While the source codes of these applications may look simple, they are executed with a complicated concurrency model, which frequently introduces software bugs, in particular, transient bugs. Such buggy logics may only be triggered by some occasionally interleaved events that bear implicit dependency, but can lead to fatal system failures. Unfortunately, these deeply-hidden bugs or even their symptoms can hardly be identified by state-of-the-art debugging tools, and manual identification from massive running traces can be prohibitively expensive. In this paper, we present Sentomist (Sensor application anatomist), a novel tool for identifying potential transient bugs in WSN applications. The Sentomist design is based on a key observation that transient bugs make the behaviors of a WSN system deviate from the normal, and thus outliers (i.e., abnormal behaviors) are good indicators of potential bugs. Sentomist introduces the notion of event-handling interval to systematically anatomize the long-term execution history of an event-driven WSN system into groups of intervals. It then applies a customized outlier detection algorithm to quickly identify and rank abnormal intervals. This dramatically reduces the human efforts of inspection (otherwise, we have to manually check tremendous data samples, typically with brute force inspection) and thus greatly speeds up debugging. We have implemented Sentomist based on the concurrency model of TinyOS. We apply Sentomist to test a series of representative real-life WSN applications that contain transient bugs. These bugs, though caused by complicated interactions that can hardly be predicted during the programming stage, are successfully confined by Sentomist.
[telecommunication security, wireless sensor networks, Humans, data mining, wireless sensor network, History, telecommunication computing, Concurrent computing, Embedded Systems, Logic, Testing, TinyOS model, software bugs, Sensor Networks, buggy logics, Debugging, Inspection, Application software, sensor application anatomist tool, symptom mining, Wireless sensor networks, security of data, Computer bugs, concurrency control, transient bugs, operating systems (computers), Sentomist tool, Detection algorithms, complicated concurrency model]
Visual, Log-Based Causal Tracing for Performance Debugging of MapReduce Systems
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
The distributed nature and large scale of MapReduce programs and systems poses two challenges in using existing profiling and debugging tools to understand MapReduce programs. Existing tools produce too much information because of the large scale of MapReduce programs, and they do not expose program behaviors in terms of Maps and Reduces. We have developed a novel non-intrusive log-analysis technique which extracts state-machine views of the control- and data-flows in MapReduce behavior from the native logs of Hadoop MapReduce systems, and it synthesizes these views to create a unified, causal view of MapReduce program behavior. This technique enables us to visualize MapReduce programs in terms of MapReduce-specific behaviors, aiding operators in reasoning about and debugging performance problems in MapReduce systems. We validate our technique and visualizations using a realworld workload, showing how to understand the structure and performance behavior of MapReduce jobs, and diagnose injected performance problems reproduced from real-world problems.
[Visualization, Java, program debugging, Failure Diagnosis, Instruments, Laboratories, program behaviors, nonintrusive log analysis technique, Debugging, performance debugging, state-machine extraction, Data mining, finite state machines, Distributed Systems, Distributed computing, log based causal tracing, distributed nature, Radio access networks, debugging tools, Processor scheduling, Cloud Computing, data visualisation, MapReduce systems, Large-scale systems]
Sketch-Based Streaming PCA Algorithm for Network-Wide Traffic Anomaly Detection
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Internet has become an essential part of the daily life for billions of users worldwide, who are using a large variety of network services and applications everyday. However, there have been serious security problems and network failures that are hard to resolve, for example, botnet attacks, polymorphic worm/virus spreading, DDoS, and flash crowds. To address many of these problems, we need to have a network-wide view of the traffic dynamics, and more importantly, be able to detect traffic anomalies in a timely manner. Spatial analysis methods have been proved to be effective in detecting network-wide traffic anomalies that are not detectable at a single monitor. To our knowledge, Principle Component Analysis (PCA) is the best-known spatial detection method for the coordinated low-profile traffic anomalies. However, existing PCA-based solutions have scalability problems in that they require linear running time and space to analyze the traffic measurements within a sliding window, which makes it often infeasible to be deployed for monitoring large-scale high-speed networks. We propose a sketch-based streaming PCA algorithm for the network-wide traffic anomaly detection in a distributed fashion. Our algorithm only requires logarithmic running time and space at both local monitors and Network Operation Centers (NOCs), and can detect both high-profile and coordinated low-profile traffic anomalies with bounded errors.
[Botnet attacks, Data Streams, Scalability, Telecommunication traffic, High-speed networks, Web and internet services, network failures, spatial analysis methods, principle component analysis, Large-scale systems, DDoS, IP networks, Monitoring, Traffic Anomaly, Network Operation Centers, computer viruses, network services, Time measurement, network-wide traffic anomaly detection, sliding window, large-scale high-speed network monitoring, sketch-based streaming PCA algorithm, security problems, computer network security, Principle Component Analysis, traffic measurement analysis, polymorphic worm-virus spreading, spatial detection method, Internet, Spatial resolution, principal component analysis, telecommunication traffic, Principal component analysis]
Quantifying and Querying Network Reachability
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Quantifying and querying network reachability is important for security monitoring and auditing as well as many aspects of network management such as troubleshooting, maintenance, and design. Although attempts to model network reachability have been made, feasible solutions to computing network reachability have remained unknown. In this paper, we propose a suite of algorithms for quantifying reachability based on network configurations (mainly ACLs) as well as solutions for querying network reachability. We present a comprehensive network reachability model that considers connectionless and connection-oriented transport protocols, stateless and stateful routers/firewalls, static and dynamic NAT, PAT, etc. We implemented the algorithms in our network reachability analysis tool called Quarnet and conducted experiments on a university network. Experimental results show that the offline computation of reachability matrices takes a few hours and the online processing of a reachability query takes 0.075 seconds on average.
[Access control, Access control lists, online query processing, Quarnet, graph theory, Telecommunication traffic, firewalls, stateful routers, query processing, Privacy, network configuration, computing network reachability, Network Access Control, authorisation, security monitoring, Computer networks, security auditing, Computer security, Network address translation, reachability analysis, stateless firewalls, stateful firewalls, reachability matrix computation, network reachability query, Debugging, university network, connection oriented transport protocols, Routing, network reachability quantification, Reachability analysis, computer network security, matrix algebra, network management, Network Reachability, stateless routers, access control lists, Computer network management]
An Energy-Efficient Distributed Algorithm for Minimum-Latency Aggregation Scheduling in Wireless Sensor Networks
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Data aggregation is an essential yet time-consuming task in wireless sensor networks (WSNs). This paper studies the well-known Minimum-Latency Aggregation Schedule (MLAS) problem and proposes an energy-efficient distributed scheduling algorithm named Clu-DDAS based on a novel cluster-based aggregation tree. Our approach differs from all the previous schemes where Connected Dominating Sets or Maximal Independent Sets are employed. We prove that Clu-DDAS has a latency bound of 4R' + 2Delta - 2, where &#x0394; is the maximum degree and R' is the inferior network radius which is smaller than the network radius R. Clu-DDAS has comparable latency as the previously best centralized algorithm E-PAS, while Clu-DDAS consumes 78% less energy as shown by the simulation results. Clu-DDAS outperforms the previously best distributed algorithm DAS whose latency bound is 16R' + &#x0394; - 14 on both latency and energy consumption. On average, Clu-DDAS transmits 67% fewer total messages than DAS does. We also propose an adaptive strategy for updating the schedule to accommodate dynamic network topology.
[wireless sensor networks, minimum-latency aggregation schedule problem, Clu-DDAS, cluster-based aggregation tree, Cluster-based Aggregation Tree, Maximum Degree, Inferior Network Radius, centralized algorithm, data aggregation, inferior network radius, minimum-latency aggregation scheduling, Wireless sensor networks, power aware computing, dynamic network topology, distributed algorithms, MLAS, scheduling, energy-efficient distributed scheduling algorithm, Energy efficiency, connected dominating sets, Distributed algorithms, adaptive strategy, maximal independent sets]
Bounding Communication Delay in Energy Harvesting Sensor Networks
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
In energy-harvesting sensor networks, limited ambient energy from environment necessitates sensor nodes to operate at a low-duty-cycle, i.e., they communicate briefly and stay asleep most of time. Such low-duty-cycle operation leads to orders of magnitude longer communication delays in comparison with traditional always-active networks, imposing a new challenge in many time-sensitive sensor network applications (e.g., tracking and alert). In this paper, we introduce novel solutions for bounding sink-to-node communications in energy-harvesting sensor networks. We first present an optimal solution for the sink-to-one case and its distributed implementation. For the sink-to-many case, we theoretically prove its NP-Hardness and in approximability property, followed by an efficient heuristic solution. We have evaluated our design with both extensive simulation and a TinyOS/Mote based implementation. Compared with an improved version of a state-of-the-art design, our delay maintenance design effectively provides E2E delay guarantees while consuming as much as 60% less energy.
[always-active networks, wireless sensor networks, Delay effects, Delay systems, Sensor phenomena and characterization, Sensor systems, Helium, Distributed computing, magnitude longer communication delays, Computer science, Wireless sensor networks, bounding sink-to-node communications, TinyOS/Mote based implementation, Prototypes, energy-harvesting sensor networks, state-of-the-art design, time-sensitive sensor network, energy harvesting sensor networks, bounding communication delay, Power engineering and energy]
Capacity Scaling in Mobile Wireless Ad Hoc Network with Infrastructure Support
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
We study the throughput capacity of mobile wireless ad hoc networks with infrastructure support. Mobility and infrastructure support independently have been shown to be effective ways to improve capacity, but few work has analyzed the impact of their combination. In our work we consider an ad hoc network with n users and k base stations. All base stations are wired to each other with bandwidth c(n). We adopt a general mobility model where users move with arbitrary patterns within a bounded distance around their home-points, and let the area of the network scales as f2(n). We show that for different parameters, mobility can be divided into strong, weak and trivial regimes. The per-node capacity is &#x0398;(1/f(n)) + &#x0398;(min(k2 c/n, k/n)) under strong mobility, and is &#x0398;(min(k2 c/n, k/n)) in the two latter cases. We also discuss optimal communication schemes and system parameters in each regime. Our study provides fundamental insight on the understanding and design of wireless ad hoc network.
[mobility, Scalability, capacity scaling, mobile wireless ad hoc network, Throughput, Relays, Distributed computing, home-points, Mobile ad hoc networks, capacity, throughput capacity, Wireless networks, general mobility model, hybrid wireless network, Bandwidth, per-node capacity, Ad hoc wireless networks, Base stations, mobile radio, scaling law, Ad hoc networks, infrastructure support, ad hoc networks, Mobile computing, base stations]
Extracting More Capacity from Multi-channel Multi-radio Wireless Networks by Exploiting Power
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Transmission power plays a crucial role in the design and performance of wireless networks. The issue is therefore complex since an increase in transmission power implies that a high quality signal is received at the receiver and hence an increase in channel capacity. Conversely, due to the shared nature of the wireless medium an increase in transmission power also implies high interference in the surrounding region and hence a quadratic reduction in the capacity of wireless networks. Recent literatures indicate that employing multiple channels can mitigate the negative effects of wireless interference and thus greatly improve the overall network capacity. Therefore, it is worth investigating the effect of exploiting power on the capacity of multi-channel multi-radio (MC-MR) wireless networks. Specifically, in this paper we address the following questions: (a) Can we maximize the capacity of MC-MR wireless networks by exploiting power? (b) Under what criteria can we increase the transmission power of the nodes in a MC-MR network? When n nodes each with m half-duplex interfaces are optimally deployed in a torus of unit area, traffic patterns are optimally assigned, each transmission's range is optimally chosen and in the presence of c channels, we show that in contrast to the setting where nodes transmit at minimum power level Po the transport capacity, measured in bit-meters per second, of MC-MR network exploiting power is increased by &#x0398;(<sub>c</sub><sub>min</sub>) in region c<sub>min</sub> &lt;; c &lt;; mn/2 and by &#x0398;(n) in region c &#x2265; mn/2 when nodes tune to transmit power level of P<sub>0</sub>(c/<sub>c</sub><sub>min</sub>)&#x03B1;/2 and P<sub>0</sub>n&#x03B1;/2 respectively-where c<sub>min</sub> is the minimum number of channels required to achieve conflict-free transmissions in a network. Our analysis also sheds light into several insights that designers may want to consider to improve the performance of energy-efficient bandwidth-constrained wireless networks.
[radio networks, multi-channel, Telecommunication traffic, channel capacity, Throughput, quadratic reduction, channel capacity extraction, Relays, Distributed computing, capacity, Radio frequency, MC-MR wireless network capacity, radiofrequency interference, Power engineering computing, high quality signal, m-half-duplex interfaces, Wireless networks, wireless interference negative effect mitigation, multichannel multiradio wireless networks, transmission power, transmit power, interference suppression, receiver, Interference, wireless networks, energy-efficient bandwidth-constrained wireless networks, Computer science, wireless medium, transport capacity, Power engineering and energy]
[Publisher's information]
2010 IEEE 30th International Conference on Distributed Computing Systems
None
2010
Provides a listing of current committee members and society officers.
[]
General Chair's Welcome
2011 31st International Conference on Distributed Computing Systems
None
2011
Presents the welcome message from the conference proceedings.
[]
Message from the Program Chair
2011 31st International Conference on Distributed Computing Systems
None
2011
Presents the welcome message from the conference proceedings.
[]
Technical Program Committee
2011 31st International Conference on Distributed Computing Systems
None
2011
Provides a listing of current committee members.
[]
Provisioning a Multi-tiered Data Staging Area for Extreme-Scale Machines
2011 31st International Conference on Distributed Computing Systems
None
2011
Massively parallel scientific applications, running on extreme-scale supercomputers, produce hundreds of terabytes of data per run, driving the need for storage solutions to improve their I/O performance. Traditional parallel file systems (PFS) in high performance computing (HPC) systems are unable to keep up with such high data rates, creating a storage wall. In this work, we present a novel multi-tiered storage architecture comprising hybrid node-local resources to construct a dynamic data staging area for extreme-scale machines. Such a staging ground serves as an impedance matching device between applications and the PFS. Our solution combines diverse resources (e.g., DRAM, SSD) in such a way as to approach the performance of the fastest component technology and the cost of the least expensive one. We have developed an automated provisioning algorithm that aids in meeting the check pointing performance requirement of HPC applications, by using a least-cost storage configuration. We evaluate our approach using both an implementation on a large scale cluster and a simulation driven by six-years worth of Jaguar supercomputer job-logs, and show that our approach, by choosing an appropriate storage configuration, achieves 41.5% cost savings with only negligible impact on performance.
[Checkpointing, checkpointing, Random access memory, Throughput, parallel machines, extreme scale supercomputer, Provisioning, Jaguar supercomputer job logs, dynamic data staging area, Computer architecture, checkpointing performance requirement, automated provisioning algorithm, I/O performance, SSDs, Supercomputers, massively parallel scientific application, Hierarchial Storage, extreme scale machine, impedance matching, Aggregates, multitiered storage architecture, High performance computing, multitiered data staging area, file organisation, HPC systems, high performance computing, data handling, parallel file systems, Resource management, least cost storage configuration]
Design and Analysis of a Distributed Multi-leg Stock Trading System
2011 31st International Conference on Distributed Computing Systems
None
2011
We present the design, optimization and analysis of a highly flexible and efficient multi-leg stock trading system. Automated electronic multi-leg trading allows atomic processing of consolidated orders such as "Buy 200 shares of IBM and sell 100 shares of HPQ". While the expressive power of multi-leg trading brings significant value to investors, it also poses major challenges to stock exchange architecture design, due to additional complexities introduced in performance, tradability, and fairness. Performance can be significantly worse due to the need to coordinate transactions among multiple stocks at once. This paper studies the performance of multi-leg trading under different fairness constraints and variability in order price and order quantity. We identify the major performance bottlenecks when using traditional atomic commitment protocols such as 2-Phase Commit (2PC), and propose a new look-ahead algorithm to maximize transaction concurrency and minimize performance degradation. We have implemented a base-line 2PC prototype and a look-ahead optimized prototype on IBM z10 z Series e Server mainframes. Our experimental results show that the look-ahead optimization can improve throughput by 58% and reduce latency by 30%.
[order processing, transaction processing, Protocols, two-phase commit, order price, investors, Optimization, IBM zlO zSeries eServer mainframe, order quantity, computer-driven trading, transaction coordination, automated electronic multileg trading, Prototypes, atomic commitment protocol, transaction concurrency, look-ahead algorithm, Stock markets, commodity trading, look-ahead optimization, distributed multileg stock trading system design, electronic commerce, stock exchange architecture design, fairness constraints, mainframes, multiple stocks, Peer to peer computing, performance degradation minimization, investment, Leg, atomic order processing, multi-leg trading, concurrency control, System recovery, base-line 2PC prototype, minimisation, distributed coordination]
YSmart: Yet Another SQL-to-MapReduce Translator
2011 31st International Conference on Distributed Computing Systems
None
2011
MapReduce has become an effective approach to big data analytics in large cluster systems, where SQL-like queries play important roles to interface between users and systems. However, based on our Facebook daily operation results, certain types of queries are executed at an unacceptable low speed by Hive (a production SQL-to-MapReduce translator). In this paper, we demonstrate that existing SQL-to-MapReduce translators that operate in a one-operation-to-one-job mode and do not consider query correlations cannot generate high-performance MapReduce programs for certain queries, due to the mismatch between complex SQL structures and simple MapReduce framework. We propose and develop a system called Y Smart, a correlation aware SQL-to-MapReduce translator. Y Smart applies a set of rules to use the minimal number of MapReduce jobs to execute multiple correlated operations in a complex query. Y Smart can significantly reduce redundant computations, I/O operations and network transfers compared to existing translators. We have implemented Y Smart with intensive evaluation for complex queries on two Amazon EC2 clusters and one Facebook production cluster. The results show that Y Smart can outperform Hive and Pig, two widely used SQL-to-MapReduce translators, by more than four times for query execution.
[Decision support systems, workstation clusters, Correlation, Data analysis, query execution, SQL-like query, Programming, Amazon EC2 cluster, Optimization, SQL, program interpreters, query processing, Hive, YSmart, Production, correlation aware SQL-to-MapReduce translator, Facebook]
PET: Probabilistic Estimating Tree for Large-Scale RFID Estimation
2011 31st International Conference on Distributed Computing Systems
None
2011
Estimating the number of RFID tags in the region of interest is an important task in many RFID applications. In this paper we propose a novel approach for efficiently estimating the approximate number of RFID tags. Compared with existing approaches, the proposed Probabilistic Estimating Tree (PET) protocol achieves O(loglogn) estimation efficiency, which remarkably reduces the estimation time while meeting the accuracy requirement. PET also largely reduces the computation and memory overhead at RFID tags. As a result, we are able to apply PET with passive RFID tags and provide scalable and inexpensive solutions for large-scale RFID systems. We validate the efficacy and effectiveness of PET through theoretical analysis as well as extensive simulations. Our results suggest that PET outperforms existing approaches in terms of estimation accuracy, efficiency, and overhead.
[PET protocol, Protocols, large-scale RFID tag estimation, radiofrequency identification, probabilistic estimating tree protocol, Estimation, probability, estimation efficiency, Probabilistic algorithm, Probabilistic logic, passive RFID tags, RFID counting system, RFID tags, theoretical analysis, Accuracy, computation overhead, memory overhead, Probabilistic estimating tree, estimation accuracy, protocols, Positron emission tomography]
End-to-End Delay Analysis in Wireless Network Coding: A Network Calculus-Based Approach
2011 31st International Conference on Distributed Computing Systems
None
2011
Network coding provides a powerful mechanism for improving performance of wireless networks. In this paper, we present an analytical approach for end-to-end delay analysis in wireless networks that employ inter-session coding. Prior work on performance analysis in wireless network coding mainly focuses on the throughput of the overall network. Our approach aims to analyze the end-to-end delay performance of each flow in the network. The theoretical basis of our approach is network calculus. In order to apply network calculus to the analysis of wireless network coding, we address three specific problems: identifying traffic flows, characterizing broadcast links, and measuring coding opportunities. We make three main contributions. First, we obtain theoretical formulations for computing the delay bounds of bursty flows in wireless networks employing network coding. Second, based on the formulations, we figure out the factors that affect the end-to-end delays, and find an interesting phenomenon that, as traffic grows, the overall delay can potentially decrease. Third, in order to exploit the benefit of our findings, we introduce a new scheduling scheme that can improve the performance of current practical wireless network coding.
[network calculus-based approach, bursty flows, radio networks, end-to-end delay analysis, traffic flow identification, Throughput, network throughput, Calculus, calculus, Delay, scheduling scheme, wireless network coding, wirless, Wireless networks, scheduling, broadcast link characterization, network coding, intersession coding, Encoding, coding opportunity measurement, delay, network calculus, Network coding, telecommunication traffic, delay bounds]
Randomized Distributed Algorithms for Neighbor Discovery in Multi-hop Multi-channel Heterogeneous Wireless Networks
2011 31st International Conference on Distributed Computing Systems
None
2011
An important first step when deploying a wireless ad hoc network is neighbor discovery in which every node attempts to determine the set of nodes it can communicate within one wireless hop. In the recent years, cognitive radio (CR) technology has gained attention as an attractive approach to alleviate spectrum congestion. A CR transceiver can operate over a wide range of frequencies possibly spanning multiple frequency bands. A CR node can opportunistically utilize unused wireless spectrum without interference from other wireless devices in its vicinity. Due to spatial variations in frequency usage and hardware variations in radio transceivers, different nodes in the network may perceive different subsets of frequencies available to them for communication. This heterogeneity in the available channel sets across the network increases the complexity of solving the neighbor discovery problem in a CR network. In this paper, we design and analyze several randomized algorithms for neighbor discovery in such a (heterogeneous) network under a variety of assumptions.
[Algorithm design and analysis, Real time systems, frequency allocation, frequency usage, wireless spectrum utilization, Transceivers, Wireless communication, cognitive radio technology, channel set heterogeneity, cognitive radio, multihop multichannel heterogeneous wireless networks, wireless devices, asynchronous system, neighbor discovery, spectrum congestion, radio transceivers, CR transceiver, heterogeneous channel availability, randomized algorithm, Synchronization, clock drift, multi-hop multi-channel wireless network, Upper bound, CR technology, randomized distributed algorithms, wireless ad hoc network, distributed algorithms, CR node, ad hoc networks, Clocks]
An Energy-Efficient Markov Chain-Based Randomized Duty Cycling Scheme for Wireless Sensor Networks
2011 31st International Conference on Distributed Computing Systems
None
2011
To extend the life time of a wireless sensor network, sensor nodes usually switch between dormant and active states according to a duty cycling scheme. In randomized schemes, sensors use only partial or no information about their neighbors, and rely on randomness to generate working schedules. Duty cycling schemes are often evaluated in terms of the connection delay, i.e., the time until two neighboring nodes are simultaneously active, and the connection duration, i.e., the time until at least one of them switches to the dormant state. In this paper, we argue that duty cycling time (energy) efficiency, i.e., the ratio of time (energy) employed in ancillary operations when switching from and into deep sleep mode, is an important performance metric too. We present experimental results using Sun SPOT sensors that support our claim and highlight the performance trade-off between connection delay and time (energy) efficiency for a traditional scheme based on independent and identically distributed (i.i.d.) random variables. We propose a novel randomized duty cycling scheme based on Markov chains with the goal of (i) reducing the connection delay, while maintaining a given time (energy) efficiency, or (ii) keeping a constant connection delay, while increasing the time (energy) efficiency. The proposed scheme is analyzed mathematically by deriving the time efficiency, connection delay and duration in terms of the time slot length, duty cycle, and cost of set up and tear down operations. Analytical results demonstrate that the Markov chain-based scheme can improve the performance in terms of connection delay without affecting the time efficiency, or vice versa, as opposed to the trade-off observed in traditional schemes. Experimental results using Sun SPOT sensor nodes with the minimum number of operations during transitions from and into deep sleep mode confirm the mathematical analysis of the proposed Markov chain-based randomized scheme.
[Schedules, connection delay reduction, wireless sensor networks, energy-efficient Markov chain, WSN life time, Aerospace electronics, dormant state, active state, experimental study, Sun, Delay, randomized duty cycling scheme, iid random variables, connection delay-time efficiency, duty cycling scheme, Markov chain, Wireless sensor networks, sensor nodes, Sun SPOT sensors, deep sleep mode, telecommunication network reliability, Markov processes, duty cycling time efficiency, independent-identically-distributed random variables]
M-PSM: Mobility-Aware Power Save Mode for IEEE 802.11 WLANs
2011 31st International Conference on Distributed Computing Systems
None
2011
With the proliferation of Wi-Fi equipped mobile devices such as smart phones, it becomes even more important to design and implement effective power management schemes for Wi-Fi interfaces so that the battery lifetime can be prolonged. In this paper, we propose an enhancement to the default 802.11 Power Save Mode (PSM), called M-PSM, which exploits additional power-saving opportunities by considering user mobility and detailed traffic condition when making the sleep/wakeup schedules for Wi-Fi interfaces. We have implemented M-PSM in the Madwifi device driver and demonstrated its effectiveness via experiments and trace-based simulations.
[Energy consumption, IEEE 802.11 Standards, Madwifi device driver, trace-based simulations, Mobile communication, IEEE 802.11 WLAN, smart phones, mobility management (mobile radio), Indexes, effective power management schemes, Wi-Fi interfaces, traffic condition, mobile computing, mobility-aware power save mode, user mobility, Channel estimation, battery lifetime, M-PSM, Wi-Fi equipped mobile devices, sleep-wakeup schedules, wireless LAN, Monitoring, telecommunication traffic, Signal to noise ratio]
Meeting the Digital Rights Requirements of Live Broadcast in a Peer-to-Peer Network
2011 31st International Conference on Distributed Computing Systems
None
2011
Live broadcast over a P2P (peer-to-peer) network imposes a unique set of challenges to a digital rights management system. Highly correlated service request arrivals at the start of a live event require peak-load provisioning if clients acquire licenses at playback time. Distributing the license management load across a P2P network requires the digital rights management system to ensure the integrity of both digital rights, the protection of client privacy and, at the same time, system scalability. In this paper we describe the requirements imposed on a digital rights management system in distributing live broadcast over a P2P network and present our design of such a system to meet the above challenges. We discuss the system's operation under a number of threat models and how to extend the system to further improve scalability. We close the paper after presenting some scalability results collected from a production P2P live broadcast network using our DRM design.
[TV, Protocols, Scalability, Subscriptions, P2P network, digital rights management, Licenses, digital rights requirements, Authorization, live broadcast, system scalability, digital rights management system, service request arrivals, license management load, client-server systems, peer-to-peer computing, Peer to peer computing, DRM design, live broadcast distribution, client privacy protection, broadcasting, P2P, privacy protection, scalability improvement, data privacy, P2P live broadcast network, peer-to-peer network, DRM]
Peer-to-Peer Data Replication Meets Delay Tolerant Networking
2011 31st International Conference on Distributed Computing Systems
None
2011
Work on delay tolerant networks (DTN) and peer-to-peer replication systems has made rapid advances with the similar goal of permitting reliable message delivery in challenged communication environments. Techniques developed for both types of systems also bear some similarity. Both exploit opportunistic connectivity to route messages and updates to their desired destinations while making minimal assumptions about end-to-end connectivity. However, they also have some unique characteristics. DTNs have recently incorporated forwarding algorithms that make use of historical information on past encounters between participants and predictions of future connectivity. Modern replication systems utilize protocols with low overheads that guarantee eventual consistency and at-most-once delivery while supporting content-based filters. In this paper, we show how a DTN-like messaging system can be readily built as a simple application on top of a peer-to-peer replication platform. To reduce delivery delays while retaining the desirable replication guarantees, we then extend the replication substrate to permit pluggable DTN routing protocols. We describe the implementation of four representative DTN schemes as replication policies and evaluate these extensions with emulations driven by traces of e-mail messaging and vehicular mobility. We conclude that DTNs and replication systems can benefit substantially from a cross-fertilization of ideas.
[peer-to-peer computing, Peer to peer computing, Buildings, DTN-like messaging system, electronic mail, electronic messaging, e-mail messaging, Routing, peer-to-peer replication system, delay tolerant networking, Synchronization, Delay, routing protocols, reliable message delivery, pluggable DTN routing protocols, Routing protocols, peer-to-peer data replication, vehicular mobility]
On the Dependencies between Source Neighbors in Optimally DoS-stable P2P Streaming Topologies
2011 31st International Conference on Distributed Computing Systems
None
2011
We study tree-based peer-to-peer streaming topologies that minimize the maximum damage that can be caused by the failure of any number of peers. These optimally stable topologies can be characterized by a distinctive damage sequence. Although checking whether a given topology is optimally stable is a co-NP-complete problem, a large subclass of these topologies can be constructed by applying a simple set of rules. One of these rules states that every optimally stable topology must have optimally stable inter-dependencies between the nodes directly adjacent to the streaming source (called heads). However, until now, only a single stable head topology was known. In this article, we first give a short outline to previous results about optimally stable topologies. Then, we identify necessary and sufficient requirements for the optimal stability of head topologies, thereby largely increasing the number of known representatives from this class. All requirements can be checked in polynomial time. Furthermore, we show how to efficiently decide stability for head topologies with at most four stripes and give a procedure that, given a stable topology, produces a stable topology with an arbitrary number of stripes. Reversing this procedure can also speed up stability testing. Finally, we describe strategies how stable head topologies can be constructed in real-world streaming systems.
[optimally DoS-stable P2P streaming topologies, peer-to-peer computing, denial-of-service, Peer to peer computing, topology, telecommunication network topology, Topology, Electronic mail, head topologies, co-NP-complete problem, source neighbors, peer-to-peer, tree-based peer-to-peer streaming topologies, streaming, Stability criteria, streaming source, Bandwidth, media streaming, Polynomials, polynomial time, stability, theory, computational complexity]
Intelligent Placement of Datacenters for Internet Services
2011 31st International Conference on Distributed Computing Systems
None
2011
Popular Internet services are hosted by multiple geographically distributed data centers. The location of the data centers has a direct impact on the services' response times, capital and operational costs, and (indirect) carbon dioxide emissions. Selecting a location involves many important considerations, including its proximity to population centers, power plants, and network backbones, the source of the electricity in the region, the electricity, land, and water prices at the location, and the average temperatures at the location. As there can be many potential locations and many issues to consider for each of them, the selection process can be extremely involved and time-consuming. In this paper, we focus on the selection process and its automation. Specifically, we propose a framework that formalizes the process as a non-linear cost optimization problem, and approaches for solving the problem. Based on the framework, we characterize areas across the United States as potential locations for data centers, and delve deeper into seven interesting locations. Using the framework and our solution approaches, we illustrate the selection trade offs by quantifying the minimum cost of (1) achieving different response times, availability levels, and consistency times, and (2) restricting services to green energy and chiller-less data centers. Among other interesting results, we demonstrate that the intelligent placement of data centers can save millions of dollars under a variety of conditions. We also demonstrate that the selection process is most efficient and accurate when it uses a novel combination of linear programming and simulated annealing.
[Internet services, consistency times, nonlinear programming, Companies, linear programming, population centers, network backbones, operational costs, Servers, Optimization, Delay, Electricity, nonlinear cost optimization problem, water prices, availability levels, intelligent placement, multiple geographically distributed data centers, Availability, simulated annealing, services response times, computer centres, chiller-less datacenters, capital costs, green energy, power plants, carbon dioxide emissions, Internet, Time factors]
Data Migration in Heterogeneous Storage Systems
2011 31st International Conference on Distributed Computing Systems
None
2011
Large-scale storage systems are crucial components in data-intensive applications such as search engine clusters, video-on-demand servers, sensor networks and grid computing. A storage server typically consists of a set of storage devices. In such systems, data layouts may need to be reconfigured over time for load balancing or in the event of system failure/upgrades. It is critical to migrate data to their target locations as quickly as possible to obtain the best performance. Most of the previous results on data migration assume that each storage node can perform only one data transfer at a time. A storage node, however, can typically handle multiple transfers simultaneously and this can reduce the total migration time significantly. Moreover, storage devices tend to have heterogeneous capabilities as devices may be added over time due to storage demand increase. In this paper, we consider the heterogeneous data migration problem, where we assume that each storage node v has different transfer constraint cv, which represents how many simultaneous transfers v can handle. We develop algorithms to minimize the data migration time. We show that it is possible to find an optimal migration schedule when all c<sub>v</sub>s are even. Furthermore, though the problem is NP-hard in general, we give an efficient algorithm that offers a rigorous (1 + o(1))-approximation guarantee.
[Schedules, search engines, NP-hard, search engine, optimal migration schedule, Optimized production technology, grid computing, Color, Orbits, data migration, Approximation methods, video-on-demand server, electronic data interchange, Image color analysis, sensor network, video on demand, data transfer, Approximation algorithms, heterogeneous storage system, storage device]
Supporting Cooperative Caching in Disruption Tolerant Networks
2011 31st International Conference on Distributed Computing Systems
None
2011
Disruption Tolerant Networks (DTNs) are characterized by the low node density, unpredictable node mobility and lack of global network information. Most of current research efforts in DTNs focus on data forwarding, but only limited work has been done on providing effective data access to mobile users. In this paper, we propose a novel approach to support cooperative caching in DTNs, which enables the sharing and coordination of cached data among multiple nodes and reduces data access delay. Our basic idea is to intentionally cache data at a set of Network Central Locations (NCLs), which can be easily accessed by other nodes in the network. We propose an effective scheme which ensures appropriate NCL selection based on a probabilistic selection metric, and coordinate multiple caching nodes to optimize trade off between data accessibility and caching overhead. Extensive trace-driven simulations show that our scheme significantly improves data access performance compared to existing schemes.
[network central location, Peer to peer computing, probability, information retrieval, Mobile communication, data forwarding, Ad hoc networks, node density, mobility management (mobile radio), History, Relays, Delay, cooperative communication, node mobility, global network information, delays, NCL, disruption tolerant network, data access delay reduction, coordinate multiple caching node, cooperative caching, probabilistic selection metric]
Effective Channel Exploitation in IEEE 802.16j Networks for Maritime Communications
2011 31st International Conference on Distributed Computing Systems
None
2011
Broadband wireless access for maritime users is a brand-new topic which demands sophisticated transmission technology to support high speed and wide coverage communications. Although the IEEE 802.16j network has been identified as a promising solution to broadband wireless access, the deployment of such networks in a maritime environment faces significant challenges because the natural movement of the ocean surface often causes the motion of stations and further leads to the variation in quality of communication channels. To tackle with these challenges and facilitate channel exploitation for deploying 802.16j based maritime communication networks, we first propose a novel scheme to specify the communication channel state between different maritime stations, based on which a new adaptation scheme is developed to exploit the optimal channel transmission capacity. Moreover, an adaptive traffic scheduling scheme is proposed to fully utilize the channel transmission bandwidth in 802.16j maritime networks. Simulation experiments have shown that the proposed schemes can fully exploit the maritime channels to achieve both satisfactory QoS performance and system throughput for maritime users.
[Sea surface, Adaptation models, effective channel exploitation, channel capacity, maritime users, channel transmission bandwidth, communication channel quality, maritime communication networks, maritime channels, system throughput, high-speed communications, Bismuth, scheduling, adaptation scheme, optimal channel transmission capacity, wireless channels, marine communication, IEEE 802.16j networks, WiMax, quality of service, QoS performance, adaptive traffic scheduling scheme, radio access networks, broadband wireless access, Shadow mapping, Surface waves, wide-coverage communications, Communication channels, ocean surface natural movement, maritime stations, Signal to noise ratio]
Accuracy-Aware Interference Modeling and Measurement in Wireless Sensor Networks
2011 31st International Conference on Distributed Computing Systems
None
2011
Wireless Sensor Networks (WSNs) are increasingly available for mission-critical applications such as emergency management and health care. To meet the stringent requirements on communication performance, it is crucial to understand the complex wireless interference among sensor nodes. Recent empirical studies suggest that the packet-level interference model, also referred to as the packet reception ratio (PRR) versus SINR model or PRR-SINR model, offers significantly improved realism than other simplistic models such as the disc model. However, as shown in our experimental results, the PRR-SINR model yields considerable spatial and temporal variations in reality, which poses a major challenge for accurate measurement at run time. This paper presents a novel accuracy-aware approach to interference modeling and measurement for WSNs. First, we propose a new regression-based PRR-SINR model and analytically characterize its accuracy based on statistics theory. Second, we develop a novel protocol called accuracy-aware interference measurement (AIM) for measuring the proposed PRR-SINR model with assured accuracy at run time. AIM also adopts new clock calibration and in-network aggregation techniques to reduce the overhead of interference measurement. Our extensive experiments on a 17-node testbed of TelosB motes show that AIM achieves high accuracy of PRR-SINR modeling with significantly lower overhead than state of the art approaches.
[Adaptation models, packet reception ratio, wireless sensor networks, AIM, Interference, Time measurement, Wireless sensor networks, Analytical models, radiofrequency interference, PRR-SINR modeling, Accuracy, WSN, complex wireless interference, accuracy-aware interference modeling, accuracy-aware approach, Signal to noise ratio]
Coolest Path: Spectrum Mobility Aware Routing Metrics in Cognitive Ad Hoc Networks
2011 31st International Conference on Distributed Computing Systems
None
2011
Cognitive Radio (CR) emerges as a promising solution to current unbalanced spectrum utilization. The cognitive ad hoc network can take advantage of dynamic spectrum access and spectrum diversity over wide spectrum. It could achieve higher network capacity compared to traditional ad hoc networks, thus supporting bandwidth-demanding applications. A cognitive radio operates over wide spectrum with unpredictable channel availability. Moreover, the transmission opportunity of a cognitive node is not guaranteed due to the presence of primary users (PUs). These two unique features define new routing problems in cognitive ad hoc networks. To better characterize the unique features of cognitive radio networks, we propose new routing metrics, including accumulated spectrum temperature, highest spectrum temperature, and mixed spectrum temperature to account for the time-varying spectrum availability. The proposed metrics favor the "coolest'' path, or the path with the most balanced and/or the lowest spectrum utilization by the primary users. We also study the computational complexity of the routing algorithm in cognitive ad hoc networks. Experiment results on our USRP-2 testbed show that the proposed metrics are capable of capturing the fluctuation of spectrum availability and suitable for cognitive ad hoc networks.
[time-varying spectrum availability, bandwidth-demanding applications, coolest path, mobility management (mobile radio), Thermal stability, cognitive radio networks, routing, cognitive radio, cognitive node transmission opportunity, mobile ad hoc networks, Availability, primary users, dynamic spectrum access, spectrum mobility, Routing, spectrum diversity, Ad hoc networks, Cognitive radio, CR networks, Temperature measurement, USRP-2 testbed, cognitive ad hoc networks, telecommunication network routing, spectrum temperature, spectrum mobility aware routing metrics, spectrum utilization, computational complexity]
A MapReduce-Based Maximum-Flow Algorithm for Large Small-World Network Graphs
2011 31st International Conference on Distributed Computing Systems
None
2011
Maximum-flow algorithms are used to find spam sites, build content voting system, discover communities, etc., on graphs from the Internet. Such graphs are now so large that they have outgrown conventional memory-resident algorithms. In this paper, we show how to effectively parallelize a max-flow algorithm based on the Ford-Fulkerson method on a cluster using the MapReduce framework. Our algorithm exploits the property that such graphs are small-world networks with low diameter and employs optimizations to improve the effectiveness of MapReduce and increase parallelism. We are able to compute max-flow on a subset of the Face book social network graph with 411 million vertices and 31 billion edges using a cluster of 21 machines in reasonable time.
[Algorithm design and analysis, Ford-Fulkerson method, Facebook Graph, Social network services, graph theory, network theory (graphs), Complexity theory, MapReduce based maximum flow algorithm, large small world network graphs, Optimization, MapReduce framework, MapReduce, memory resident algorithms, Maximum-Flow, Small-World Network, Memory management, Clustering algorithms, Parallel processing, social networking (online), Internet, Facebook social network graph]
The Client Assignment Problem for Continuous Distributed Interactive Applications
2011 31st International Conference on Distributed Computing Systems
None
2011
Interactivity is a primary performance measure for distributed interactive applications (DIAs) that enable participants at different locations to interact with each other in real time. Wide geographical spreads of participants in large-scale DIAs necessitate distributed deployment of servers to improve interactivity. In a distributed server architecture, the interactivity performance depends on not only client-to-server network latencies but also inter-server network latencies as well as synchronization delays to meet the consistency and fairness requirements of DIAs. All of these factors are directly affected by how the clients are assigned to the servers. In this paper, we investigate the problem of effectively assigning clients to servers for maximizing the interactivity of DIAs. We focus on continuous DIAs that change their states not only in response to user operations but also due to the passing of time. We analyze the minimum achievable interaction time for DIAs to preserve consistency and provide fairness among clients, and formulate the client assignment problem as a combinational optimization problem. We prove that this problem is NP-complete. Four heuristic assignment algorithms are proposed and evaluated using real Internet latency data. The experimental results show that our proposed greedy algorithm generally produces near optimal interactivity and significantly reduces the interaction time between clients compared to the intuitive algorithm that assigns each client to its nearest server.
[combinational optimization problem, combinatorial mathematics, Jitter, inter-server network latency, Servers, Delay, client assignment problem, optimisation, interactive systems, Polynomials, greedy algorithm, client-server systems, greedy algorithms, Routing, continuous distributed interactive application, Synchronization, NP-complete problem, distributed server architecture, Internet latency data, heuristic assignment algorithm, delays, Games, Internet, client-to-server network latency, continuous systems, synchronization delay, computational complexity]
Guidelines for the Verification of Population Protocols
2011 31st International Conference on Distributed Computing Systems
None
2011
We address the problem of verification by model checking of the basic population protocol (PP) model of Angluin et al. This problem has received special attention in the last two years and new tools have been proposed to deal with it. We show that the problem can be solved by using the existing model-checking tools, e.g., Spin and Prism. In order to do so, we apply the counter abstraction to get an abstraction of the PP model which can be efficiently verified by the existing model-checking tools. Moreover, this abstraction preserves the correct stabilization property of PP models. To deal with the fairness assumed by the PP models, we provide two new recipes. The first one gives sufficient conditions under which the PP model fairness can be replaced by the weak fairness implemented in Spin. We show that this recipe can be applied to several PP models. In the second recipe, we show how to use probabilistic model-checking and, in particular, Prism to take completely in consideration the fairness of the PP models. The correctness of this recipe is based on existing theorems involving finite discrete Markov chains.
[Protocols, model-checking, Computational modeling, Radiation detectors, Biological system modeling, LTL, Spin, population protocols, distributed processing, Probabilistic logic, population protocol verification, vector addition systems, counter abstraction, formal verification, model checking, formal model, Semantics, Petri Nets, Markov processes, Concrete, finite discrete Markov chain, protocols, Prism]
Scaling Construction of Low Fan-out Overlays for Topic-Based Publish/Subscribe Systems
2011 31st International Conference on Distributed Computing Systems
None
2011
It is a key challenge and fundamental problem in the design of distributed publish/subscribe systems to construct the underlying dissemination overlay. In this paper, we focus on effective practical solution for the Min Max-TCO problem: Create a topic-connected pub/sub overlay in which all nodes interested in the same topic are organized in a directly connected dissemination sub-overlay while keeping the maximum node degree to the minimum. Previously known solutions provided an extensive analysis of the problem and an algorithm that achieves a logarithmic approximation for Min Max-TCO. Yet, they did not focus on efficiency of the solution or feasibility of decentralized operation that would not require full knowledge of the system. Compared to these solutions, our proposed algorithm produces an overlay with marginally higher degrees. At the same time, it has drastically reduced runtime cost, which is corroborated by both theoretical analysis and empirical evaluation. The latter shows a speedup by a factor of more than 25 on average for typical pub/sub workloads.
[Algorithm design and analysis, publish/subscribe, minmax-TCO problem, message passing, scaling construction, logarithmic approximation, cost, Scalability, information dissemination, Subscriptions, topic-based publish-subscribe system, overlay design, Partitioning algorithms, Approximation methods, minimax techniques, Algorithms, Clustering algorithms, fan-out overlay, Approximation algorithms]
Efficient Online WiFi Delivery of Layered-Coding Media Using Inter-layer Network Coding
2011 31st International Conference on Distributed Computing Systems
None
2011
A primary challenge in multi casting video in a wireless LAN to multiple clients is to deal with the client diversity -- clients may have different channel characteristics and hence receive different numbers of transmissions from the AP. A promising approach to overcome this problem is to combine multi-resolution (layered) video coding with interlayer network coding. The fundamental challenge in such an approach is to determine the strategy of coding the packets across different layers that maximizes the number of decoded layers at all clients. This paper makes three contributions. (1) We first show that even for one client, the previously proposed canonical triangular scheme for inter-layer network coding can perform poorly. We show how to enhance the triangular scheme by incorporating the estimated target number of layers which significantly improves its effectiveness. (2) We show that such an enhanced triangular scheme still performs poorly for multiple clients with diverse channel characteristics, which motivates the need for searching for the optimal coding strategy. The naive way of searching for the optimal strategy is computationally prohibitive. We present several optimizations that drastically reduce the complexity of exhaustively searching for the optimal strategy, making it feasible in real time. (3) Finally, we design and evaluate an on line video delivery scheme, Percy, to be deployed at a proxy behind the AP of a wireless LAN. Our simulation results show that Percy outperforms the previous inter-layer coding heuristic by up to 22-80% with varying numbers of clients.
[network coding, optimal coding strategy, interlayer network coding, multiresolution video coding, online video delivery scheme, Receivers, Encoding, WLAN, video coding, video, Optimization, multicasting video, online WiFi delivery, Wireless networks, multicast communication, Streaming media, Network coding, canonical triangular scheme, wireless LAN, layered coding media, MRC]
Dissecting Video Server Selection Strategies in the YouTube CDN
2011 31st International Conference on Distributed Computing Systems
None
2011
In this paper, we conduct a detailed study of the YouTube CDN with a view to understanding the mechanisms and policies used to determine which data centers users download video from. Our analysis is conducted using week-long datasets simultaneously collected from the edge of five networks - two university campuses and three ISP networks - located in three different countries. We employ state-of-the-art delay-based geolocation techniques to find the geographical location of YouTube servers. A unique aspect of our work is that we perform our analysis on groups of related YouTube flows. This enables us to infer key aspects of the system design that would be difficult to glean by considering individual flows in isolation. Our results reveal that while the RTT between users and data centers plays a role in the video server selection process, a variety of other factors may influence this selection including load-balancing, diurnal effects, variations across DNS servers within a network, limited availability of rarely accessed video, and the need to alleviate hot-spots that may arise due to popular video content.
[Google, video server selection process, load balancing, Geology, ISP network, internet services, Europe, YouTube CDN, geographic information systems, Servers, computer centres, video servers, YouTube, delay-based geolocation technique, data center, Content distribution networks, Web, social networking (online), Internet, IP networks, DNS server]
Resource Allocation for Medium Grain Scalable Videos over Femtocell Cognitive Radio Networks
2011 31st International Conference on Distributed Computing Systems
None
2011
Femtocells are shown highly effective on improving network coverage and capacity by bringing base stations closer to mobile users. In this paper, we investigate the problem of streaming scalable videos in femtocell cognitive radio (CR) networks. This is a challenging problem due to the stringent QoS requirements of real-time videos and the new dimensions of network dynamics and uncertainties in CR networks. We develop a framework that captures the key design issues and trade-offs with a stochastic programming problem formulation. In the case of a single FBS, we develop an optimum-achieving distributed algorithm, which is shown also optimal for the case of multiple non-interfering FBS's. In the case of interfering FBS's, we develop a greedy algorithm that can compute near-optimal solutions, and prove a closed-form lower bound for its performance. The proposed algorithms are evaluated with simulations, and are shown to outperform two alternative schemes with considerable margins.
[streaming scalable videos, stochastic programming, medium-grain scalable videos, Programming, Videos, Wireless communication, cognitive radio, resource allocation, Cognitive radios, near-optimal solutions, cross-layer optimization, video streaming, Sensors, mobile users, FBS interference, greedy algorithm, Base stations, PSNR, dynamic spectrum access, Interference, stochastic programming problem formulation, optimum-achieving distributed algorithm, Medium Grain Scalable, quality of service, distributed algorithms, femtocellular radio, femtocell CR networks, femtocell cognitive radio networks, real-time videos, QoS requirements, femtocell networks, base stations, network coverage improvement]
CloudMedia: When Cloud on Demand Meets Video on Demand
2011 31st International Conference on Distributed Computing Systems
None
2011
Internet-based cloud computing is a new computing paradigm aiming to provide agile and scalable resource access in a utility-like fashion. Other than being an ideal platform for computation-intensive tasks, clouds are believed to be also suitable to support large-scale applications with periods of flash crowds by providing elastic amounts of bandwidth and other resources on the fly. The fundamental question is how to configure the cloud utility to meet the highly dynamic demands of such applications at a modest cost. In this paper, we address this practical issue with solid theoretical analysis and efficient algorithm design using Video on Demand (VoD) as the example application. Having intensive bandwidth and storage demands in real time, VoD applications are purportedly ideal candidates to be supported on a cloud platform, where the on-demand resource supply of the cloud meets the dynamic demands of the VoD applications. We introduce a queueing network based model to characterize the viewing behaviors of users in a multichannel VoD application, and derive the server capacities needed to support smooth playback in the channels for two popular streaming models: client-server and P2P. We then propose a dynamic cloud resource provisioning algorithm which, using the derived capacities and instantaneous network statistics as inputs, can effectively support VoD streaming with low cloud utilization cost. Our analysis and algorithm design are verified and extensively evaluated using large-scale experiments under dynamic realistic settings on a home-built cloud platform.
[Cloud computing, client-server systems, queueing theory, peer-to-peer computing, Heuristic algorithms, streaming model, VoD streaming, Dynamic scheduling, cloud on demand, CloudMedia, client-server, Servers, P2P, peer-to-peer, queueing network, video on demand, Bandwidth, Streaming media, Internet-based cloud computing, video streaming, cloud computing, dynamic cloud resource provisioning algorithm, VoD application, Queueing analysis]
Correlated Resource Models of Internet End Hosts
2011 31st International Conference on Distributed Computing Systems
None
2011
Understanding and modelling resources of Internet end hosts is essential for the design of desktop software and Internet-distributed applications. In this paper we develop a correlated resource model of Internet end hosts based on real trace data taken from the SETI@home project. This data covers a 5-year period with statistics for 2.7 million hosts. The resource model is based on statistical analysis of host computational power, memory, and storage as well as how these resources change over time and the correlations between them. We find that resources with few discrete values (core count, memory) are well modeled by exponential laws governing the change of relative resource quantities over time. Resources with a continuous range of values are well modeled with either correlated normal distributions (processor speed for integer operations and floating point operations) or log-normal distributions (available disk space). We validate and show the utility of the models by applying them to a resource allocation problem for Internet-distributed applications, and demonstrate their value over other models. We also make our trace data and tool for automatically generating realistic Internet end hosts publicly available.
[desktop software design, Correlation, Multicore processing, log normal distribution, Computational modeling, volunteer computing, Internet end hosts, Internet end host, normal distribution, Internet computing, resource scheduling, correlated resource model, resource allocation problem, resource allocation, Benchmark testing, resource model, Data models, Hardware, host model, Internet, statistical analysis, Internet-distributed application]
Time-Dependent Broadband Pricing: Feasibility and Benefits
2011 31st International Conference on Distributed Computing Systems
None
2011
Charging different prices for Internet access at different times induces users to spread out their bandwidth consumption across times of the day. Potential impact on ISP revenue, congestion management, and consumer behavior can be significant, yet some fundamental questions remain: is it feasible to operate time dependent pricing and how much benefit can it bring? We develop an efficient way to compute the cost-minimizing time-dependent prices for an Internet service provider (ISP), using both a static session-level model and a dynamic session model with stochastic arrivals. A key step is choosing the representation of the optimization problem so that the resulting formulations remain computationally tractable for large-scale problems. We next show simulations illustrating the use and limitation of time-dependent pricing. These results demonstrate that optimal prices, which "reward'' users for deferring their sessions, roughly correlate with demand in each period, and that changing prices based on real-time traffic estimates may significantly reduce ISP cost. The degree to which traffic is evened out over times of the day depends on the time-sensitivity of sessions, cost structure of the ISP, and amount of traffic not subject to time-dependent prices. Finally, we present our system integration and implementation, called TUBE, and proof-of-concept experimentation.
[optimization problem, Computational modeling, Heuristic algorithms, Internet service provider, congestion management, dynamic session model, stochastic arrivals, Optimization, Equations, optimisation, time-dependent broadband pricing, Internet access, Electricity, consumer behavior, Pricing, Bandwidth, static session-level model, TUBE implementation, Internet, stochastic processes, pricing, bandwidth consumption, ISP revenue]
Partitioning Network Testbed Experiments
2011 31st International Conference on Distributed Computing Systems
None
2011
Understanding the behavior of large-scale systems is challenging, but essential when designing new Internet protocols and applications. It is often infeasible or undesirable to conduct experiments directly on the Internet. Thus, simulation, emulation, and testbed experiments are important techniques for researchers to investigate large-scale systems. In this paper, we propose a platform-independent mechanism to partition a large network experiment into a set of small experiments that are sequentially executed. Each of the small experiments can be conducted on a given number of experimental nodes, e.g., the available machines on a testbed. Results from the small experiments approximate the results that would have been obtained from the original large experiment. We model the original experiment using a flow dependency graph. We partition this graph, after pruning uncongested links, to obtain a set of small experiments. We execute the small experiments in two iterations. In the second iteration, we model dependent partitions using information gathered about both the traffic and the network conditions during the first iteration. Experimental results from several simulation and testbed experiments demonstrate that our techniques approximate performance characteristics, even with closed-loop traffic and congested links. We expose the fundamental trade off between the simplicity of the partitioning and experimentation process, and the loss of experimental fidelity.
[Internet protocols, Computational modeling, closed-loop traffic, Internet applications, Partitioning algorithms, Topology, large-scale systems, congested links, Upper bound, Network topology, flow dependency graph, network emulation, Emulation, network partitioning, network testbeds, Internet, protocols, network simulation]
TCP Congestion Avoidance Algorithm Identification
2011 31st International Conference on Distributed Computing Systems
None
2011
The Internet has recently been evolving from homogeneous congestion control to heterogeneous congestion control. Several years ago, Internet traffic was mainly controlled by the traditional AIMD algorithm, whereas Internet traffic is now controlled by many different TCP algorithms, such as AIMD, BIC, CUBIC, and CTCP. However, there is very little work on the performance and stability study of the Internet with heterogeneous congestion control. One fundamental reason is the lack of the deployment information of different TCP algorithms. In this paper, we first propose a tool called TCP Congestion Avoidance Algorithm Identification (CAAI) for actively identifying the TCP algorithm of a remote web server. CAAI can identify all default TCP algorithms (i.e., AIMD, BIC, CUBIC, and CTCP) and most non-default TCP algorithms of major operating system families. We then present, for the first time, the CAAI measurement result of the 5000 most popular web servers. Among the web servers with valid traces, we found that only 16.85~25.58\\% of web servers still use the traditional AIMD, 44.51\\% of web servers use BIC or CUBIC, and 10.27$\\sim$19\\% of web servers use CTCP. In addition, we found that, for the first time, some web servers use non-default TCP algorithms, some web servers use some unknown TCP algorithms which are not available in any major operating system family, and some web servers use abnormal slow start algorithms. Our CAAI measurement results show a strong sign that the majority of TCP flows are not controlled by AIMD anymore, and a strong sign that the Internet congestion control has already changed from homogeneous to highly heterogeneous.
[Algorithm design and analysis, BIC algorithm, telecommunication congestion control, CUBIC algorithm, Internet measurement, TCP congestion avoidance algorithm identification, Web servers, Internet traffic, heterogeneous congestion control, homogeneous congestion control, CTCP algorithm, Linux, transport protocols, AIMD algorithm, TCP congestion control, Feature extraction, Internet, Kernel]
What is Missing in Current Checkpoint Interval Models?
2011 31st International Conference on Distributed Computing Systems
None
2011
The growth in the number of components that compose parallel computers increases their fault frequency. Currently, in such systems faults are no longer a rare event but a common problem, thus some sort of fault tolerance should be provided. In general, fault tolerance protocols rely on checkpoints. A common question surrounding check pointing is the definition of the checkpoint interval. In this paper we propose the modelling of the relationship established between the parallel applications processes due to the messages exchange in order to incorporate this relationship into current checkpoint interval models. The experimental evaluation shows that the use of our checkpoint interval model based on the definition of the parallel application inter-process dependency factor is effective to calculate the checkpoint interval for parallel applications. Our results demonstrate that the overhead prediction error is smaller than 4% in comparison with the application execution.
[Checkpointing, checkpointing, Protocols, fault tolerance, Computational modeling, parallel computer fault frequency, checkpoint interval, fault tolerance protocol, mpi, parallel application interprocess dependency factor, parallel applications, checkpoint interval model, parallel processing, Equations, software fault tolerance, Fault tolerance, Fault tolerant systems, model, Mathematical model]
Scalable Symbolic Execution of Distributed Systems
2011 31st International Conference on Distributed Computing Systems
None
2011
Recent advances in symbolic execution have proposed a number of promising solutions to automatically achieve high-coverage and explore non-determinism during testing. This attractive testing technique of unmodified software assists developers with concrete inputs and deterministic schedules to analyze erroneous program paths. Being able to handle complex systems' software, these tools only consider single software instances and not their distributed execution which forms the core of distributed systems. The step to symbolic distributed execution is however steep, posing two core challenges: (1) additional state growth and (2) the state intra-dependencies resulting from communication. In this paper, we present SDE - a novel approach enabling scalable symbolic execution of distributed systems. The key contribution of our work is two-fold. First, we generalize the problem space of SDE and develop an algorithm significantly eliminating redundant states during testing. The key idea is to benefit from the nodes' local communication minimizing the number of states representing the distributed execution. Second, we demonstrate the practical applicability of SDE in testing with three sensor net scenarios running Contiki OS.
[Context, symbolic distributed execution, program testing, MIMICs, software testing technique, Receivers, distributed processing, distributed system, History, sensor net scenario, symbolic execution, operating systems (computers), Concrete, Software, complex system software, Contiki operating system, sensornet testing, Testing]
A Scheduling and Certification Algorithm for Defeating Collusion in Desktop Grids
2011 31st International Conference on Distributed Computing Systems
None
2011
By exploiting idle time on volunteer machines, desktop grids provide a way to execute large sets of tasks with negligible maintenance and low cost. Although desktop grids are attractive for their scalability and low cost, relying on external resources may compromise the correctness of application execution due to the well-known unreliability of nodes. In this paper, we consider a very challenging threat model: correlated errors caused either by organized groups of cheaters that may collude to produce incorrect results, or by buggy or so-called "unofficial" clients. By using a previously described on-line algorithm for detecting collusion and characterizing the participant behaviors, we propose a scheduling and result certification algorithm that tackles collusion. Using several real-life traces, we show that our approach minimizes both replication overhead and the number of incorrectly certified results.
[replication overhead minimization, Collusion, collusion detection, desktop grids, Estimation, grid computing, application execution, Desktop Grid, Scheduling, scheduling algorithm, Servers, Approximation methods, certification algorithm, threat model, Modeling, Sabotage, volunteer machines, certification, Processor scheduling, Random variables, Resource management]
A Self-stabilizing (k,r)-clustering Algorithm with Multiple Paths for Wireless Ad-hoc Networks
2011 31st International Conference on Distributed Computing Systems
None
2011
Wireless Ad-hoc networks are distributed systems that often reside in error-prone environments. Self-stabilization lets the system recover autonomously from an arbitrary state, making the system recover from errors and temporarily broken assumptions. Clustering nodes within ad-hoc networks can help forming backbones, facilitating routing, improving scaling, aggregating information, saving power and much more. We present the first self-stabilizing distributed (k,r)-clustering algorithm. A (k,r)-clustering assigns k cluster heads within r communication hops for all nodes in the network while trying to minimize the total number of cluster heads. The algorithm uses synchronous communication rounds and uses multiple paths to different cluster heads for improved security, availability and fault tolerance. The algorithm assigns, when possible, at least k cluster heads to each node within O(r) rounds from an arbitrary configuration. The set of cluster heads stabilizes, with high probability, to a local minimum within O(gr log n) rounds, where n is the size of the network and g is an upper bound on the number of nodes within 2r hops.
[telecommunication security, Law, (k, wireless ad hoc networks, k cluster heads, clustering nodes, arbitrary state, routing, Clustering algorithms, telecommunication network reliability, synchronous communication rounds, distributed systems, system error recovery, error-prone environments, Ad-hoc Networks, information aggregation, r communication hops, cluster head minimization, Head, fault tolerance, Self-Stabilization, multiple paths, probability, r)-dominating sets, Ad hoc networks, Clustering, scaling improvement, Sleep, pattern clustering, distributed algorithms, telecommunication network routing, Nickel, ad hoc networks, self-stabilizing distributed clustering algorithm, security improvement]
SpotME If You Can: Randomized Responses for Location Obfuscation on Mobile Phones
2011 31st International Conference on Distributed Computing Systems
None
2011
Nowadays companies increasingly aggregate location data from different sources on the Internet to offer location-based services such as estimating current road traffic conditions, and finding the best nightlife locations in a city. However, these services have also caused outcries over privacy issues. As the volume of location data being aggregated expands, the comfort of sharing one's whereabouts with the public at large will unavoidably decrease. Existing ways of aggregating location data in the privacy literature are largely centralized in that they rely on a trusted location-based service. Instead, we propose a piece of software (SpotMe) that can run on a mobile phone and is able to estimate the number of people in geographic locations in a privacy-preserving way: accurate estimations are made possible in the presence of privacy-conscious users who report, in addition to their actual locations, a very large number of erroneous locations. The erroneous locations are selected by a randomized response algorithm. We evaluate the accuracy of SpotMe in estimating the number of people upon two very different realistic mobility traces: the mobility of vehicles in urban, suburban and rural areas, and the mobility of subway train passengers in Greater London. We find that erroneous locations have little effect on the estimations (in both traces, the error is below 18% for a situation in which more than 99% of the locations are erroneous), yet they guarantee that users cannot be localized with high probability. Also, the computational and storage overheads for a mobile phone running Spot Me are negligible, and the communication overhead is limited.
[mobile phones, Companies, Mobile communication, privacy-conscious users, storage overheads, privacy, Mobile handsets, mobility management (mobile radio), SpotME software, Vehicles, road traffic condition estomation, Greater London, Privacy, mobile computing, location obfuscation, randomized response algorithm, location-based services, privacy literature, vehicle mobility, geographic locations, computational overheads, Estimation, privacy-preserving way, subway train passenger mobility, computer network security, Aggregates, mobility traces, Internet, nightlife location finding, data location aggregation]
HCPP: Cryptography Based Secure EHR System for Patient Privacy and Emergency Healthcare
2011 31st International Conference on Distributed Computing Systems
None
2011
Privacy concern is arguably the major barrier that hinders the deployment of electronic health record (EHR) systems which are considered more efficient, less error-prone, and of higher availability compared to traditional paper record systems. Patients are unwilling to accept the EHR system unless their protected health information (PHI) containing highly confidential data is guaranteed proper use and disclosure, which cannot be easily achieved without patients' control over their own PHI. However, cautions must be taken to handle emergencies in which the patient may be physically incompetent to retrieve the controlled PHI for emergency treatment. In this paper, we propose a secure EHR system, HCPP (Healthcaresystem for Patient Privacy), based on cryptographic constructions and existing wireless network infrastructures, to provide privacy protection to patients under any circumstances while enabling timelyPHI retrieval for life-saving treatment in emergency situations. Furthermore, our HCPP system restricts PHI access to authorized (not arbitrary) physicians, who can be traced and held accountable if the accessed PHI is found improperly disclosed. Last but not least, HCPP leverages wireless network access to support efficient and private storage/retrieval of PHI, which underlies a secure and feasible EHR system.
[radio networks, PHI access, paper record systems, wireless network infrastructures, Wireless Networks, Encryption, Servers, Security, EHR, Privacy, cryptography based secure EHR system, protected health information, healthcaresystem for patient privacy, HCPP, life-saving treatment, emergency services, health care, emergency healthcare, telemedicine, information retrieval, cryptography, medical information systems, Hospitals, timelyPHI retrieval, data privacy, Emergency, electronic health record]
Authorized Private Keyword Search over Encrypted Data in Cloud Computing
2011 31st International Conference on Distributed Computing Systems
None
2011
In cloud computing, clients usually outsource their data to the cloud storage servers to reduce the management costs. While those data may contain sensitive personal information, the cloud servers cannot be fully trusted in protecting them. Encryption is a promising way to protect the confidentiality of the outsourced data, but it also introduces much difficulty to performing effective searches over encrypted information. Most existing works do not support efficient searches with complex query conditions, and care needs to be taken when using them because of the potential privacy leakages about the data owners to the data users or the cloud server. In this paper, using on line Personal Health Record (PHR) as a case study, we first show the necessity of search capability authorization that reduces the privacy exposure resulting from the search results, and establish a scalable framework for Authorized Private Keyword Search (APKS) over encrypted cloud data. We then propose two novel solutions for APKS based on a recent cryptographic primitive, Hierarchical Predicate Encryption (HPE). Our solutions enable efficient multi-dimensional keyword searches with range query, allow delegation and revocation of search capabilities. Moreover, we enhance the query privacy which hides users' query keywords against the server. We implement our scheme on a modern workstation, and experimental results demonstrate its suitability for practical usage.
[Cloud computing, query privacy, APKS, Encryption, Servers, cloud storage server, Authorization, query processing, hierarchical predicate encryption, authorisation, privacy leakages, PHR, cloud computing, personal health records, multidimensional keyword search, sensitive personal information protection, cryptography, medical information systems, authorized private keyword search, data confidentiality, Indexes, range query, encrypted data, data privacy, search capability authorization, data outsourcing, HPE, searchable encryption, online personal health record]
Privacy-Preserving Query over Encrypted Graph-Structured Data in Cloud Computing
2011 31st International Conference on Distributed Computing Systems
None
2011
In the emerging cloud computing paradigm, data owners become increasingly motivated to outsource their complex data management systems from local sites to the commercial public cloud for great flexibility and economic savings. For the consideration of users' privacy, sensitive data have to be encrypted before outsourcing, which makes effective data utilization a very challenging task. In this paper, for the first time, we define and solve the problem of privacy-preserving query over encrypted graph-structured data in cloud computing (PPGQ), and establish a set of strict privacy requirements for such a secure cloud data utilization system to become a reality. Our work utilizes the principle of "filtering-and-verification". We prebuild a feature-based index to provide feature-related information about each encrypted data graph, and then choose the efficient inner product as the pruning tool to carry out the filtering procedure. To meet the challenge of supporting graph query without privacy breaches, we propose a secure inner product computation technique, and then improve it to achieve various privacy requirements under the known-background threat model.
[Data privacy, Cloud computing, cloud computing paradigm, graph theory, inner product computation technique, known-background threat model, privacy-preserving query problem, cryptography, Encryption, Indexes, Servers, economic savings, filtering-and-verification principle, PPGQ, query processing, Privacy, encrypted graph-structured data, data utilization, complex data management systems, data privacy, data structures, cloud computing]
Delay-Cognizant Reliable Delivery for Publish/Subscribe Overlay Networks
2011 31st International Conference on Distributed Computing Systems
None
2011
The number of real-world applications that require QoS guarantees is constantly increasing and they often follow the publish/subscribe (pub/sub)messaging paradigm, which provides loosely coupled many-to-many communication. Many QoS-aware systems use overlay networks as they allow flexible routing. To provide QoS-aware pub/sub messaging in overlay networks, the messaging system should be adaptive to the changes in network conditions (such as delay and failures). However, many pub/sub systems depend on a flexed routing topology and it is costly to rebuild this topology in case of failures. This study seeks to address this challenge with Delay-Cognizant Reliable Delivery (DCRD), a novel and delay-aware dynamic routing algorithm to provide reliable message delivery for pub/sub overlay networks. For reliable message delivery, DCRD no longer uses a flxed routing topology. Instead, it dynamically switches among different links to bypass link failures and increase the chance to meet QoS requirement. Each node tries different neighboring nodes in an order that is mathematically proven to minimize the expected delay of packet delivery. With all possible neighboring nodes sorted this way, DCRD guarantees that packets are delivered as long as there exists a path between the publisher and subscriber and that the expected delay is minimized. DCRD is extensively evaluated in simulation with comparison to existing tree-based routing approaches as well as a multi path approach using different network topologies, delay constraints, and loss probabilities. Simulation results show that DCRD performs better than all the baselines, providing reliable message delivery and satisfying the delay requirement for more than 98% of messages when the link failure probability is 4% or less.
[message passing, Heuristic algorithms, Peer to peer computing, network routing, QoS aware system, Receivers, Quality of service, Routing, quality of service, network topology, failure analysis, Delay, delay cognizant reliable delivery, multipath approach, reliable message delivery, packet delivery delay, Silicon, loosely coupled many-to-many communication, computer network reliability, publish-subscribe overlay network, delay aware dynamic routing algorithm]
Reducing the Delay and Power Consumption of Web Browsing on Smartphones in 3G Networks
2011 31st International Conference on Distributed Computing Systems
None
2011
Smart phone is becoming a key element in providing greater user access to the mobile Internet. Many complex applications, which are used to be only on PCs, have been developed and run on smart phones. These applications extend the functionalities of smart phones and make them more convenient for users to be connected. However, they also greatly increase the power consumption of smart phones and many users are frustrated with the long delay of web browsing when using smart phones. In this paper, we have discovered that the key reason of the long delay and high power consumption in web browsing is not due to the bandwidth limitation most of time in 3G networks. The local computation limitation at the smart phone is the real bottleneck for opening most web pages. To address this issue, we propose an architecture, called Virtual-Machine based Proxy (VMP), to shift the computing from smart phones to the VMP. To illustrate the feasibility of deploying the proposed VMP system in 3G networks, we have built a prototype using Xen virtual machines and Android Phones with T-Mobile UMTS network. Experimental results show that compared to normal smart phone browser, our VMP approach reduces the delay by more than 80% and reduces the power consumption during web browsing by more than 45%.
[Power demand, 3G mobile communication, mobile Internet, smart phones, Browsers, Servers, Delay, mobile computing, power aware computing, android phones, delay reduction, 3G networks, power consumption reduction, Computer architecture, Ash, online front-ends, virtual machines, Web browsing, virtual-machine based proxy, T-mobile UMTS network, Internet, Xen virtual machines, Smart phones, mobile handsets]
Energy-Aware Scheduling in Disk Storage Systems
2011 31st International Conference on Distributed Computing Systems
None
2011
This paper deals with the problem of scheduling requests on disks for minimizing energy consumption. We first analyze several versions of the energy-aware disk scheduling problem based on assumptions on the arrival pattern of the requests. We show that the corresponding optimization problems are NP-complete by reduction to the set cover or the independent set problem. Then both optimal and heuristic scheduling algorithms are proposed to maximize the energy saving of a storage system. Our evaluation results using two realistic traces show that our approach significantly reduces energy consumption up to 55% and achieves fewer disk spin-up/down operations and shorter request response time as compared to other approaches.
[Schedules, Energy consumption, energy aware computing, Optimal scheduling, Scheduling, NP-complete, disc storage, energy aware storage, Scheduling algorithm, energy-aware disk scheduling problem, independent set problem, computer power supplies, optimisation, energy consumption minimization, scheduling, disk storage systems, Approximation algorithms, energy saving maximization, disk spin-up/down operations, Spinning, disk scheduling, optimization problems, computational complexity]
Tians Scheduling: Using Partial Processing in Best-Effort Applications
2011 31st International Conference on Distributed Computing Systems
None
2011
To service requests with high quality, interactive services such as web search, on-demand video and online gaming keep average server utilization low. As servers become busy, queuing delays increase, and requests miss their deadlines, resulting in degraded quality of service with poor user experience and potential revenue loss. In this paper, we propose Tians scheduling, a group of scheduling algorithms for interactive services that can produce partial answers during overload. A Tians scheduler allocates processing time to each request based on system load with the objective of maximizing overall quality of responses. We propose three Tians scheduling algorithms -offline, online clairvoyant and online nonclairvoyant. For interactive applications with concave quality profile, we prove that the off line algorithm is optimal. We show the effectiveness of the online algorithms by conducting a simulation study modeling important applications - a web search engine and video-on-demand (VOD) system. Simulation results show a significant improvement of Tians over traditional server models: average response quality improves and the variance of responses decreases.
[Schedules, concave quality profile, search engines, Tians scheduling, Optimal scheduling, Servers, service request, offline, best-effort application, Web search engine, VOD bandwidth allocation, video on demand, partial answers, interactive systems, scheduling, best-effort applications, online clairvoyant, online nonclairvoyant, queueing theory, queuing delay, Optimized production technology, partial results, quality profile, interactive services, quality of service, offline scheduling, Scheduling algorithm, partial processing, web search engine, quality of responses, Software, VOD system, Internet, Web search, server utilization]
Starlink: Runtime Interoperability between Heterogeneous Middleware Protocols
2011 31st International Conference on Distributed Computing Systems
None
2011
Interoperability remains a challenging and growing problem within distributed systems. A range of heterogeneous network and middleware protocols which cannot interact with one another are now widely used, for example, the set of remote method invocation protocols, and the set of service discovery protocols. In environments where systems and services are composed dynamically, e.g. pervasive computing and systems-of-systems, the protocols used by two systems wishing to interact is unknown until runtime and hence interoperability cannot be guaranteed. In such situations, dynamic solutions are required to identify the differences between heterogeneous protocols and generate middleware connectors (or bridges) that will allow the systems to inter operate. In this paper, we present the Starlink middleware, a general framework into which runtime generated interoperability logic (in the form of higher level models) can be deployed to connect two heterogeneous protocols. For this, it provides: i) an abstract representation of network messages with a corresponding generic parser and composer, ii) an engine to execute coloured automata that represent the required interoperability behaviour between protocols, and iii) translation logic to describe the exchange of message content from one protocol to another. We show through case-study based evaluation that Starlink can bridge heterogeneous protocol types. Starlink is also compared against base-line protocol benchmarks to show that acceptable performance can still be achieved in spite of the high-level nature of the solution.
[Protocols, open systems, automata theory, distributed system, runtime interoperability, domain specific language, Runtime, runtime generated interoperability logic, protocol, translation logic, Semantics, heterogeneous network, remote method invocation protocol, protocols, middleware, heterogeneous middleware protocol, Starlink middleware, Color, coloured automata, interoperability, service discovery protocol, Middleware, abstract representation, Bridges, Automata]
WHISPER: Middleware for Confidential Communication in Large-Scale Networks
2011 31st International Conference on Distributed Computing Systems
None
2011
A wide range of distributed applications requires some form of confidential communication between groups of users. In particular, the messages exchanged between the users and the identity of group members should not be visible to external observers. Classical approaches to confidential group communication rely upon centralized servers, which limit scalability and represent single points of failure. In this paper, we present WHISPER, a fully decentralized middleware that supports confidential communications within groups of nodes in large-scale systems. It builds upon a peer sampling service that takes into account network limitations such as NAT and firewalls. WHISPER implements confidentiality in two ways: it protects the content of messages exchanged between the members of a group, and it keeps the group memberships secret to external observers. Using multi-hops paths allows these guarantees to hold even if attackers can observe the link between two nodes, or be used as content relays for NAT bypassing. Evaluation in real-world settings indicates that the price of confidentiality remains reasonable in terms of network load and processing costs.
[large-scale networks, Protocols, network servers, message exchange, Peer Sampling, confidential communication, Servers, Relays, centralized server, Privacy, middleware, peer sampling service, message passing, peer-to-peer computing, Peer to peer computing, Whisper middleware, multihop path, Group Communications, Anonymity, Middleware, computer network security, NAT bypassing, Membership Management, Public key, Virtual private networks]
sfatables: A Firewall-like Policy Engine for Federated Systems
2011 31st International Conference on Distributed Computing Systems
None
2011
Recent efforts to federate computation and communication resources across organizational boundaries face a challenge in establishing the policies by which one organization's users can access resources in other organizations. This paper describes an approach to defining, communicating, analyzing, and enforcing resource allocation policies in this new setting. Our approach was designed to address the needs of Planet Lab, but we demonstrate through a range of examples that it is general enough to accommodate a diverse collection of computing facilities. Our policy engine is implemented in a specific tool chain, called sfatables, that is patterned after the iptables mechanism used to define packet processing policies for network traffic. The interface to our policy engine thus uses the familiar paradigm of a firewall and provides a flexible interface for resource owners to specify access policies for their resources. Our implementation makes it possible to precisely document policies, query, and analyze them.
[packet processing policy, Resource allocation policies, firewall-like policy engine, Distributed computing, PlanetLab, computer network security, federated systems, testbeds, resource allocation policy, network traffic, cloud, resource allocation, sfatables tool chain, iptables mechanism, planetlab, firewall, authorisation, resource access, telecommunication traffic]
Impact of Mobility and Heterogeneity on Coverage and Energy Consumption in Wireless Sensor Networks
2011 31st International Conference on Distributed Computing Systems
None
2011
In this paper, we investigate the coverage of mobile heterogeneous wireless sensor networks (WSNs). By the term heterogeneous, we mean that sensors in the network have various sensing radii, which is an inherent property of many applied WSNs. Two sensor deployment schemes are considered-uniform and Poisson schemes. We study the asymptotic coverage under uniform deployment scheme with i.i.d. and 1-dimensional random walk mobility model, respectively. We propose the equivalent sensing radius (ESR) for both cases and derive the critical ESR correspondingly. From the perspective of critical ESR, we show that 1-dimensional random walk mobility can increase coverage under certain delay tolerance, and thus decreases sensing energy consumption. Also, we characterize the role of heterogeneity in coverage and energy performance of WSNs with these two mobility models, and present the discrepancy of the impact of heterogeneity under different models. Under the Poisson deployment scheme, we investigate dynamic k-coverage of WSNs with 2-dimensional random walk mobility model. There are many reasons for designers to require k-coverage rather than 1-coverage. Both k-coverage at an instant and over a time interval are explored and we derive the expectation of fraction of the whole operational region that is k-covered, which also identifies the coverage improvement brought by mobility.
[Energy consumption, sensor deployment schemes, mobility, wireless sensor networks, mobile heterogeneous wireless sensor networks, dynamic k-coverage, Mobile communication, iid random walk mobility model, sensor networks, mobility management (mobile radio), asymptotic coverage, Delay, uniform deployment scheme, sensor placement, delay tolerance, Robot sensing systems, 2-dimensional random walk mobility model, stochastic processes, Poisson deployment scheme, 1-dimensional random walk mobility model, energy, coverage, Atmospheric modeling, sensing energy consumption, mobile heterogeneous WSN, heterogeneity, Wireless sensor networks, critical ESR, equivalent sensing radius]
Cool: On Coverage with Solar-Powered Sensors
2011 31st International Conference on Distributed Computing Systems
None
2011
In this paper, we study the dynamic node activation schedule for the utility based coverage problem in solar-powered wireless sensor networks. We assume that the utility achieved by a WSN for coverage service is a sub modular function over the set of sensors that will provide the service. We first present an integer programming formulation with sub modular objective functions. We then present an efficient simple greedy hill-climbing algorithm such that the achieved average utility of the computed schedule is at least 1/2 times that achieved by the optimal schedule. To the best of our knowledge, this is the first polynomial time algorithm that can ensure a good constant approximation of the achieved utility for multi-target coverage problem. We conduct extensive evaluations to study the performances of our proposed aggregation scheduling algorithm on real testbed. Our evaluation results corroborate our theoretical analysis.
[multi-target coverage, Schedules, solar power, wireless sensor networks, greedy algorithms, integer programming, polynomials, submodular objective function, dynamic node activation schedule, Linear programming, Sensor systems, polynomial time algorithm, Wireless sensor networks, aggregation scheduling algorithm, rechargeable battery, greedy hill-climbing algorithm, multitarget coverage problem, integer programming formulation, scheduling, solar-powered wireless sensor network, secondary cells, Monitoring, Meteorology]
LR-Seluge: Loss-Resilient and Secure Code Dissemination in Wireless Sensor Networks
2011 31st International Conference on Distributed Computing Systems
None
2011
Code dissemination in wireless sensor networks refers to the process of disseminating a new code image via wireless links to all sensor nodes after they are deployed. It is desirable and often necessary due to the need for, e.g., removing program bugs and adding new functionalities in a multi-task sensor network. A sound code dissemination scheme need be both loss-resilient and attack-resilient, which are crucial for sensor networks deployed in lossy and hostile environments. To the best of our knowledge, no existing scheme simultaneously satisfies both requirements. This paper fills this gap with the design and evaluation of LR-Seluge, a novel loss-resilient and secure code dissemination scheme. The efficacy and efficiency of LR-Seluge are confirmed by both theoretical analysis and extensive simulation results. In particular, LR-Seluge can reduce up to 40% communication overhead in lossy environments with the same level of attack resilience in contrast to existing schemes.
[radio links, telecommunication security, wireless sensor networks, sensor networks, Computer crime, wireless links, Wireless communication, sensor nodes, attack-resilient scheme, sensor placement, code image, loss-resilient, Cryptography, Base stations, loss-resilient-secure code dissemination scheme, sound code dissemination scheme, Resilience, Wireless sensor networks, multitask sensor network, communication overhead reduction, LR-Seluge scheme, Authentication, Secure, sensor network deployment, code dissemination, image coding, program bugs]
Competitive and Fair Medium Access Despite Reactive Jamming
2011 31st International Conference on Distributed Computing Systems
None
2011
Intentional interference constitutes a major threat for communication networks operating over a shared medium where availability is imperative. Jamming attacks are often simple and cheap to implement. Today's jammers can perform physical carrier sensing in order to disrupt communication more efficiently, especially in a network of simple wireless devices such as sensor nodes, which usually operate over a single frequency (or a limited frequency band) and which cannot benefit from the use of spread spectrum or other more advanced technologies. This paper proposes the medium access (MAC) protocol ANTIjAM which is provably robust against a powerful reactive adversary who can jam a (1 - &#x03B5;)-portion of the time steps, where &#x03B5; is an arbitrary constant. The adversary uses carrier sensing to make informed decisions on when it is most harmful to disrupt communications. Moreover, we allow the adversary to be adaptive and to have complete knowledge of the entire protocol history. Our MAC protocol is able to make efficient use of the nonjammed time periods and achieves a &#x0398;(1) competitive throughput in this harsh scenario, if &#x03B5; is constant. In addition, ANTIjAM features a low convergence time and has excellent fairness properties in the sense that channel access probabilities among nodes do not differ by more than a small constant factor.
[telecommunication security, radio networks, TV, physical carrier sensing, communication networks, competitive throughput, jamming, Throughput, competitive medium access protocol, reactive jamming attacks, access protocols, Jamming, adaptive adversary, Wireless sensor networks, fair medium access protocol, sensor nodes, MAC protocol ANTIjAM, wireless devices, intentional interference, nonjammed time periods, Media Access Protocol, Sensors, channel access probabilities, arbitrary constant]
Localizing Multiple Jamming Attackers in Wireless Networks
2011 31st International Conference on Distributed Computing Systems
None
2011
Jamming attacks and unintentional radio interference are one of the most urgent threats harming the dependability of wireless communication and endangering the successful deployment of pervasive applications built on top of wireless networks. Unlike the traditional approaches focusing on developing jamming defense techniques without considering the location of jammers, we take a different viewpoint that the jammers' position should be identified and exploited for building a wide range of defense strategies to alleviate jamming. In this paper, we address the problem of localizing multiple jamming attackers coexisting in wireless networks by leveraging the network topology changes caused by jamming. We systematically analyze the jamming effects and develop a framework that can partition network topology into clusters and can successfully estimate the positions of multiple jammers even when their jamming areas are overlapping. Our experiments on a multi-hop network setup using MicaZ sensor nodes validate the feasibility of real-time collection of network topology changes under jamming and our extensive simulation results demonstrate that our approach is highly effective in localizing multiple attackers with or without the prior knowledge of the order that the jammers are turned on.
[telecommunication security, radio networks, multiple-jamming attacker localization, jamming, MicaZ sensor nodes, telecommunication network topology, wireless networks, Topology, network topology, real-time collection, Jamming, unintentional radio interference, wireless communication, jamming defense techniques, Wireless sensor networks, Network topology, jammer position estimation, Wireless networks, multihop network, Signal to noise ratio]
JR-SND: Jamming-Resilient Secure Neighbor Discovery in Mobile Ad Hoc Networks
2011 31st International Conference on Distributed Computing Systems
None
2011
Secure neighbor discovery is fundamental to mobile ad hoc networks (MANETs) deployed in hostile environments and refers to the process in which two neighboring nodes exchange messages to discover and authenticate each other. It is vulnerable to the jamming attack in which the adversary intentionally sends radio signals to prevent neighboring nodes from exchanging messages. Anti-jamming communications often rely on spread-spectrum techniques which depend on a spreading code common to the communicating parties but unknown to the jammer. The spread code is, however, impossible to establish before the communicating parties successfully discover each other. While several elegant approaches have been recently proposed to break this circular dependency, the unique features of neighbor discovery in MANETs make them not directly applicable. In this paper, we propose JR-SND, a jamming-resilient secure neighbor discovery scheme for MANETs based on Direct Sequence Spread Spectrum and random spread-code pre-distribution. JR-SND enables neighboring nodes to securely discover each other with overwhelming probability despite the presence of omnipresent jammers. Detailed theoretical and simulation results confirm the efficacy and efficiency of JR-SND.
[Real time systems, telecommunication security, radio signals, random spread code predistribution, Correlation, jamming, probability, Ad hoc networks, spread spectrum communication, jamming-resilient secure neighbor discovery scheme, Jamming, secure neighbor discovery, neighboring nodes, MANET, mobile ad hoc networks, Spread spectrum communication, jamming attack, antijamming communications, JR-SND scheme, Mobile computing, Monitoring, direct sequence spread spectrum]
Cloud4Home -- Enhancing Data Services with @Home Clouds
2011 31st International Conference on Distributed Computing Systems
None
2011
Mobile devices, net books and laptops, and powerful home PCs are creating ever-growing computational capacity at the periphery of the Internet, and this capacity is supporting an increasingly rich set of services, including media-rich entertainment and social networks, gaming, home security applications, flexible data access and storage, and others. Such 'at the edge' capacity raises the question, however, about how to combine it with the capabilities present in the cloud computing infrastructures residing in data center systems and reachable via the Internet. The Cloud4Home project and approach presented in this paper addresses this topic, by enabling and exploring the aggregate use of @home and @datacenter computational and storage capabilities. Cloud4Home uses virtualization technologies to create content storage, access, and sharing services that are fungible both in terms of where stored objects are located and in terms of where they are manipulated. In this fashion, data services can provide low latency response to @home events as well as high throughput response when the higher and less predictable latencies of data center access can be tolerated. Cloud4Home is implemented with the Xen open source hypervisors for standard x86-based mobile to server platforms, and is evaluated using sample applications based on home security and video conversion services.
[Cloud computing, public domain software, home security applications, netbooks, Hand-held Platforms, mobile computing, x86-based mobile to server platforms, Distributed databases, virtualization technologies, Public Cloud, cloud computing, laptops, data services, gaming, Peer to peer computing, social networks, Cloud4Home, Dynamic scheduling, Xen open source hypervisors, Surveillance, home PC, @Home Services, virtual machines, mobile devices, Objects, Internet, Virtualization, Distributed Hash Table, video conversion services]
Harnessing the Cloud for Securely Solving Large-Scale Systems of Linear Equations
2011 31st International Conference on Distributed Computing Systems
None
2011
Cloud computing economically enables customers with limited computational resources to outsource large-scale computations to the cloud. However, how to protect customers' confidential data involved in the computations then becomes a major security concern. In this paper, we present a secure outsourcing mechanism for solving large-scale systems of linear equations (LE) in cloud. Because applying traditional approaches like Gaussian elimination or LU decomposition (aka. direct method) to such large-scale LE problems would be prohibitively expensive, we build the secure LE outsourcing mechanism via a completely different approach -- iterative method, which is much easier to implement in practice and only demands relatively simpler matrix-vector operations. Specifically, our mechanism enables a customer to securely harness the cloud for iteratively finding successive approximations to the LE solution, while keeping both the sensitive input and output of the computation private. For robust cheating detection, we further explore the algebraic property of matrix-vector operations and propose an efficient result verification mechanism, which allows the customer to verify all answers received from previous iterative approximations in one batch with high probability. Thorough security analysis and prototype experiments on Amazon EC2 demonstrate the validity and practicality of our proposed design.
[iterative methods, secure outsourcing mechanism, security analysis, algebraic property, Amazon EC2, robust cheating detection, Encryption, Servers, iterative method, Equations, matrix algebra, large-scale systems, security of data, Gaussian elimination, LU decomposition, Outsourcing, Iterative methods, Mathematical model, cloud computing, linear equations, matrix-vector operations]
A Cost-Aware Elasticity Provisioning System for the Cloud
2011 31st International Conference on Distributed Computing Systems
None
2011
In this paper we present Kingfisher, a cost-aware system that provides efficient support for elasticity in the cloud by (i) leveraging multiple mechanisms to reduce the time to transition to new configurations, and (ii) optimizing the selection of a virtual server configuration that minimizes the cost. We have implemented a prototype of Kingfisher and have evaluated its efficacy on a laboratory cloud platform. Our experiments with varying application workloads demonstrate that Kingfisher is able to (i) decrease the cost of virtual server resources by as much as 24% compared to the current cost-unaware approach, (ii) reduce by an order of magnitude the time to transition to a new configuration through multiple elasticity mechanisms in the cloud, and (iii), illustrate the opportunity for design alternatives which trade-off the cost of server resources with the time required to scale the application.
[Cloud computing, cost-aware elasticity provisioning system, virtual reality, cloud elasticity, virtual server configuration, Random access memory, laboratory cloud platform, Elasticity, Kingfisher prototype, Servers, server resource cost, Engines, optimisation, Pricing, multiple elasticity mechanism, cloud computing, transition time, Monitoring]
Economical and Robust Provisioning of N-Tier Cloud Workloads: A Multi-level Control Approach
2011 31st International Conference on Distributed Computing Systems
None
2011
Resource provisioning for N-tier web applications in Clouds is non-trivial due to at least two reasons. First, there is an inherent optimization conflict between cost of resources and Service Level Agreement (SLA) compliance. Second, the resource demands of the multiple tiers can be different from each other, and varying along with the time. Resources have to be allocated to multiple (virtual) containers to minimize the total amount of resources while meeting the end-to-end performance requirements for the application. In this paper we address these two challenges through the combination of the resource controllers on both application and container levels. On the application level, a decision maker (i.e., an adaptive feedback controller) determines the total budget of the resources that are required for the application to meet SLA requirements as the workload varies. On the container level, a second controller partitions the total resource budget among the components of the applications to optimize the application performance (i.e., to minimize the round trip time). We evaluated our method with three different workload models -- open, closed, and semi-open - that were implemented in the RUBiS web application benchmark. Our evaluation indicates two major advantages of our method in comparison to previous approaches. First, fewer resources are provisioned to the applications to achieve the same performance. Second, our approach is robust enough to address various types of workloads with time-varying resource demand without reconfiguration.
[Cloud computing, open systems, robust provisioning, Containers, Servers, time varying resource demand, optimisation, resource allocation, N tier cloud workload, resource budget, semiopen workload model, Robustness, cloud computing, service-oriented architecture, adaptive control, economical provisioning, Generators, multilevel control approach, N-tier web application, service level agreement, RUBiS Web application, SLA requirement, Time factors, Resource management, Queueing analysis]
Secure Aggregation with Malicious Node Revocation in Sensor Networks
2011 31st International Conference on Distributed Computing Systems
None
2011
Sensor applications often leverage in-network aggregation to extract aggregates, such as predicate count and average, from the network. With in-network aggregation, a malicious sensor can easily manipulate the intermediate aggregation results and corrupt the final answer. Most existing secure aggregation schemes aim to defend against stealth attacks and can only raise an alarm when the final answer is corrupted, without being able to pinpoint and revoke the malicious sensors. While some recent protocols can pinpoint and revoke malicious sensors, they need to rely on expensive public key cryptography to be robust against certain attacks. Using only symmetric key cryptography, this paper aims to strictly diminish the capability of adversaries whenever they launch a successful attack, so that malicious sensors can only ruin the aggregation result for a small number of times before they are fully revoked. To this end, we propose VMAT (verifiable minimum with audit trail), a novel secure aggregation protocol with malicious sensor revocation capability. VMAT relies on symmetric key cryptography only, and provides provable guarantees that each execution can either produce the correct aggregation result efficiently, or revoke some key held by the adversary.
[telecommunication security, Base stations, Protocols, VMAT, cryptographic protocols, wireless sensor networks, secure aggregation, Aggregates, public key cryptography, malicious sensor revocation capability, Public key cryptography, Robustness, leverage in-network aggregation, malicious node revocation, protocols, symmetric key cryptography, verifiable minimum with audit trail, Clocks, malicious sensor]
Defending Against Traffic Analysis in Wireless Networks through Traffic Reshaping
2011 31st International Conference on Distributed Computing Systems
None
2011
Traffic analysis has been exploited by attackers to threaten user privacy in wireless networks. As an example, a user's on line activities may be exposed to strangers, even if the traffic is encrypted. However, the existing defense mechanisms against traffic analysis, such as packet padding and traffic morphing, are inefficient because they add noise traffic to blur the traffic features, therefore introducing significant overhead. In this paper, we propose the traffic reshaping technique to thwart traffic analysis. It creates multiple virtual media access control (MAC) interfaces over a single wireless card, dynamically schedules packets over these interfaces, thereby reshaping the packet features on each virtual interface. Hence, features of the original traffic are obscured and unavailable for the adversary to infer users' on line activities. Unlike the existing solutions, traffic reshaping enhances privacy protection without incurring overhead in items of adding noise traffic. We evaluate the performance of traffic reshaping through trace-based experiments. The results show that traffic reshaping is effective and efficient in defending against the traffic analysis attacks.
[Algorithm design and analysis, telecommunication security, radio networks, Users' Online Activities, Artificial neural networks, wireless networks, access protocols, Jamming, Wireless communication, packet padding, traffic morphing, Privacy, Accuracy, traffic analysis attacks, privacy protection, virtual media access control, traffic reshaping, traffic analysis, Traffic Analysis, Cryptography, Virtualization, telecommunication traffic, Traffic Reshaping]
Confidential Gossip
2011 31st International Conference on Distributed Computing Systems
None
2011
Epidemic gossip has proven a reliable and efficient technique for sharing information in a distributed network. Much of the reliability and efficiency derives from processes collaborating, sharing the work of distributing information. As a result of this collaboration, processes may receive information that was not originally intended for them. For example, a process may act as an intermediary, aggregating and forwarding messages from some set of sources to some set of destinations. But what if rumors are confidential? In that case, only processes that were originally intended to receive a rumor should be allowed to learn the rumor. This blatantly contradicts the basic premise of epidemic gossip, which assumes that processes can collaborate. In fact, if only processes in a rumor's "destination set" participate in gossiping that rumor, we show that high message complexity is unavoidable. We propose a scheme in which each rumor is broken into multiple fragments using a simple coding scheme: any given fragment provides no information about the rumor, while together, they allow the original rumor to be reassembled. The processes collaborate in disseminating the rumor fragments while ensuring that no process receives all the fragments of a rumor unless it is in that rumor's destination set. Our solution operates in an environment where rumors are dynamically and continuously injected into the system and processes are subject to crashes and restarts. In addition, the scheme presented can tolerate a moderate amount of collusion among curious processes without too large an increase in cost.
[Collusion, Protocols, information sharing, Computational modeling, distributed processing, Computer crashes, Complexity theory, coding scheme, encoding, Dynamic rumor injection, Privacy, curious processes, Confidentiality, Collaboration, Randomized gossip, epidemic gossip, Fault-tolerance, Cryptography, Message complexity, distributed network]
Understanding the Network and User-Targeting Properties of Web Advertising Networks
2011 31st International Conference on Distributed Computing Systems
None
2011
Advertising has become an integral and inseparable part of the World Wide Web. However, neither public auditing nor monitoring mechanisms still exist in this emerging area. In this paper, we present our initial efforts on building a network and content-level auditing service for Web-based ad networks. Our network-level measurements -- charting the network infrastructure and quantifying the ad platforms' delay performance -- can help commissioners to evaluate their networks from end users' perspective, and let advertisers choose commissioners that better fit their needs. Our content-level measurements -- understanding the ad distribution mechanisms and evaluating location-based and behavioral targeting approaches -- bring useful auditing information to all entities involved in the on line advertising business. We extensively evaluate Google's, AOL's, and Ad blade's ad networks and demonstrate how their different design philosophies dominantly affect their performance at both network and content levels.
[ad distribution mechanisms, Companies, World Wide Web, network-level measurements, Servers, Content-level, Delay, user-targeting properties, Google ad networks, AOL ad networks, content-level auditing service, network-level auditing service, Network-level, IP networks, online advertising business, Adblade ad networks, Advertising, Google, Auditing service, Web-based Ad Network, advertising data processing, Web advertising networks, location-based approaches, behavioral targeting approaches, content-level measurements, Internet, Web sites]
Cache Miss Analysis for GPU Programs Based on Stack Distance Profile
2011 31st International Conference on Distributed Computing Systems
None
2011
Using the graphics processing unit (GPU) to accelerate the general purpose computation has attracted much attention from both the academia and industry due to GPU's powerful computing capacity. Thus optimization of GPU programs has become a popular research direction. In order to support the general purpose computing more efficiently, GPU has integrated the general data cache to replace the existing software-managed on-chip memory. Consequently, improving the usage of the data cache becomes of vital importance to improve the performance of the GPU programs. The foundation of cache locality optimizations is efficient analysis and prediction of the cache behavior. Unfortunately, existing cache miss analysis models are based on sequential programs and thus cannot be used to analyze the GPU programs directly. In this paper, based on the deep analysis of GPU's execution model, we propose, for the first time, a cache miss analysis model for the GPU programs. We divide the problem into two subproblems: stack distance profile analysis of single thread block and cache contention analysis of multiple thread blocks. The experimental results from nine typical application kernels in the scientific computing field illustrate that our method is efficient and can be used to guide the cache locality optimizations for the GPU programs.
[stack distance profile, cache miss analysis model, computer graphic equipment, multi-threading, Instruction sets, Computational modeling, cache locality optimizations, cache miss analysis models, cache storage, coprocessors, general purpose computing, cache contention analysis, GPU program optimisation, GPU, Optimization, Graphics processing unit, Analytical models, data cache, graphics processing unit, multiple thread blocks, Silicon, Kernel, stack distance profile analysis, software-managed on-chip memory]
BloomFlash: Bloom Filter on Flash-Based Storage
2011 31st International Conference on Distributed Computing Systems
None
2011
The bloom filter is a probabilistic data structure that provides a compact representation of a set of elements. To keep false positive probabilities low, the size of the bloom filter must be dimensioned a priori to be linear in the maximum number of keys inserted, with the linearity constant ranging typically from one to few bytes. A bloom filter is most commonly used as an in memory data structure, hence its size is limited by the availability of RAM space on the machine. As datasets have grown over time to Internet scale, so have the RAM space requirements of bloom filters. If sufficient RAM space is not available, we advocate that flash memory may serve as a suitable medium for storing bloom filters, since it is about one-tenth the cost of RAM per GB while still providing access times orders of magnitude faster than hard disk. We present BLOOMFLASH, a bloom filter designed for flash memory based storage, that provides a new dimension of trade off with bloom filter access times to reduce RAM space usage (and hence system cost). The simple design of a single flat bloom filter on flash suffers from many performance bottlenecks, including in-place bit updates that are inefficient on flash and multiple reads and random writes spread out across many flash pages for a single lookup or insert operation. To mitigate these performance bottlenecks, BLOOMFLASH leverages two key design innovations: (i) buffering bit updates in RAM and applying them in bulk to flash that helps to reduce random writes to flash, and (ii) a hierarchical bloom filter design consisting of component bloom filters, stored one per flash page, that helps to localize reads and writes on flash. We use two real-world data traces taken from representative bloom filter applications to drive and evaluate our design. BLOOMFLASH achieves bloom filter access times in the range of few tens of microseconds, thus allowing up to order of tens of thousands operations per sec.
[Performance evaluation, flash-based storage, Buffer storage, Random access memory, RAM space usage reduction, Solid State Disk (SSD), Servers, flash memory based storage, storage management, flash memories, Bloom Filter, Ash, data structures, Hierarchical Design, BloomFlash, RAM space, random-access storage, Internet scale, hierarchical bloom filter design, Data structures, probabilistic data structure, buffering bit updates, Data Center Applications, Games, memory data structure, Flash Memory]
Understanding Vicious Cycles in Server Clusters
2011 31st International Conference on Distributed Computing Systems
None
2011
In this paper, we present an automated on-line service for troubleshooting performance problems in server clusters caused by unintended vicious cycles. The tool complements a large volume of prior performance troubleshooting and diagnostic literature for server farms that identifies problems arising due to resource bottlenecks or failed components. We show that unintended interactions between components in large-scale systems can cause performance problems even in the absence of bottlenecks or failures. Our tool leverages discriminative sequence mining to identify anomalous sequences of events that are candidates for blame for the performance problem. The tool looks for patterns consistent with "vicious cycles" or unstable behavior, as such patterns, when present, are most likely to be problematic. It highlights candidates that are semantically conflicting, such as those arising when different performance management mechanisms make adjustments in conflicting directions. Our approach offers two key advantages in performance troubleshooting. First, it does not require detailed prior knowledge of the underlying system to diagnose the problem. Second, contrary to simple statistical techniques, such as correlation analysis, that work well for continuous variables, our scheme can also identify chains of events (labels) that may explain the root cause of a problem. Our service is deployed on a web server testbed of 17 machines. To make the comparison of our scheme to prior work more concrete, we first reproduce two real-life problem scenarios reported in earlier literature, then explore a third, new case study. In all cases, our tool reports the patterns that explain the cause of the problem without requiring detailed a priori knowledge.
[adaptive components, server clusters, Servers, discriminative sequence mining, Voltage control, Degradation, correlation analysis, vicious cycles, file servers, computer facilities, failed components, Web server, Monitoring, program diagnostics, Color, performance evaluation, performance problems, interactive complexity, large-scale systems, continuous variables, diagnostics, Web services, data center, automated on-line service, Computer bugs, performance troubleshooting, anomalous sequences, troubleshooting, Software, performance management]
Searching for Bandwidth-Constrained Clusters
2011 31st International Conference on Distributed Computing Systems
None
2011
Data-intensive distributed applications can increase their performance by running on a cluster of hosts connected via high-bandwidth interconnections. However, there is no effective method to find such a bandwidth-constrained cluster in a decentralized fashion. Our work is inspired by prior work that treats Internet bandwidth as an approximate tree metric space. This paper presents a decentralized, accurate, and efficient method to find a cluster of Internet hosts, given the desired cluster size and minimum interconnection bandwidth. We describe a centralized polynomial time algorithm for a tree metric space, along with a proof of correctness. We then provide a decentralized version of the algorithm. Simulation experiments with two real-world datasets confirm that our clustering approach achieves high accuracy and scalability. We also discuss the costs of decentralization and how the treeness of the dataset affects clustering accuracy.
[Algorithm design and analysis, cluster, Peer to peer computing, polynomials, Internet host, bandwidth, trees (mathematics), high-bandwidth interconnection, bandwidth constrained cluster, tree, Extraterrestrial measurements, centralized polynomial time algorithm, real-world dataset, pattern clustering, Clustering algorithms, Bandwidth, prediction, Prediction algorithms, Internet bandwidth, Internet, tree metric space]
Smart Redundancy for Distributed Computation
2011 31st International Conference on Distributed Computing Systems
None
2011
Many distributed software systems allow participation by large numbers of untrusted, potentially faulty components on an open network. As faults are inevitable in this setting, these systems utilize redundancy and replication to achieve fault tolerance. In this paper, we present a novel "smart" redundancy technique called iterative redundancy, which ensures efficient replication of computation and data given finite processing and storage resources, even when facing Byzantine faults. Iterative redundancy is more efficient and more adaptive than comparable state-of-the-art techniques that operate in environments with unknown system resource reliability. We show how systems that solve computational problems using a network of independent nodes can benefit from iterative redundancy. We present a formal analytical analysis and an empirical analysis, demonstrate iterative redundancy on a real-world volunteer-computing system, and compare it to existing methods.
[iterative methods, distributed software system, real-world volunteer-computing system, reliability, distributed processing, autonomic distribution, Servers, smart redundancy technique, storage management, resource allocation, computational grid, Fault tolerant systems, redundancy, Iterative methods, formal analytical analysis, storage resources, fault tolerance, Computational modeling, Redundancy, open network, distributed computation, resource reliability, Byzantine faults, software fault tolerance, autonomic grid, data replication, iterative redundancy]
Fused Data Structures for Handling Multiple Faults in Distributed Systems
2011 31st International Conference on Distributed Computing Systems
None
2011
The paper describes a technique to correct crash faults in large data structures hosted on distributed servers, based on the concept of fused backups. The prevalent solution to this problem is replication. To correct f crash faults among n distinct data structures, replication requires nf additional replicas. If each of the primaries contains O(m) nodes of O(s) size each, this translates to O(nmsf) total backup space. Our technique uses a combination of erasure correcting codes and selective replication to correct f crash faults using just f additional backups consuming O(msf) total backup space, while incurring minimal overhead during normal operation. Since the data is maintained in the coded form, recovery is costly as compared to replication. However, in a system with infrequent faults, the savings in space outweighs the cost of recovery. We explore the theory and algorithms for these fused backups and provide a library of such backups for all the data structures in the Java 6 Collection framework. Our experimental evaluation confirms that fused backups are space-efficient as compared to replication (almost n times), while they cause very little overhead for updates. Many real world distributed systems such as Amazon's Dynamo data store use replication to achieve reliability. An alternate, fusion-based design can result in significant savings in space as well as other resources such as power.
[crash fault, fusion-based design, distributed processing, distributed system, sensor fusion, distributed server, Servers, Distributed Systems, system recovery, real world distributed system, Fault tolerance, fused data structure, Fault tolerant systems, data structures, Amazon dynamo data store, selective replication, multiple fault handling, Data Structures, erasure correcting code, Computer crashes, Indexes, space outweighs saving, space efficient fused backup library, data handling, Arrays, computational complexity, Fault Tolerance]
Low-Overhead Fault Tolerance for High-Throughput Data Processing Systems
2011 31st International Conference on Distributed Computing Systems
None
2011
The MapReduce programming paradigm proved to be a useful approach for building highly scalable data processing systems. One important reason for its success is simplicity, including the fault tolerance mechanisms. However, this simplicity comes at a price: efficiency. MapReduce's fault tolerance scheme stores too much intermediate information on disk. This inefficiency negatively affects job completion time. Furthermore, this inefficiency in particular forbids the application of MapReduce in near real-time scenarios where jobs need to produce results quickly. In this paper, we discuss an alternative fault tolerance scheme that is inspired by virtual synchrony. The key feature of our approach is a low-overhead deterministic execution. Deterministic execution reduces the amount of persistently stored information. In addition, because persisting intermediate results are no longer required for fault tolerance, we use more efficient communication techniques that considerably improve job completion time and throughput. Our contribution is twofold: (i) we enable the use of MapReduce for jobs ranging from seconds to a few tens of seconds, satisfying these deadlines even in the case of failures, (ii) we considerably reduce the fault tolerance overhead and as such the overhead of MapReduce in general. Our modifications are transparent to the application.
[low overhead fault tolerance, fault tolerance, fault tolerance mechanism, high throughput data processing system, Programming, virtual synchrony, MapReduce programming, Computer crashes, Synchronization, deterministic algorithms, parallel programming, communication technique, Fault tolerance, real time scenario, Aggregates, Fault tolerant systems, low overhead deterministic execution, data handling, Monitoring]
Guaranteeing High Availability Goals for Virtual Machine Placement
2011 31st International Conference on Distributed Computing Systems
None
2011
The placement of virtual machines (VMs) on a cluster of hosts under multiple constraints, including administrative (security, regulations) resource-oriented (capacity, energy), and QoS-oriented (performance) is a highly complex task. We define a new high-availability property for a VM; when a VM is marked as k-resilient, as long as there are up to k host failures, it should be guaranteed that it can be relocated to a non-failed host without relocating other VMs. Together with Hardware Predictive Failure Analysis and live migration, which enable VMs to be evacuated from a host before it fails, this property allows the continuous running of YMs on the cluster despite host failures. The complexity of the constraints associated with k-resiliency, which are naturally expressed by Second Order logic statements, prevented their integration into the placement computation until now. We present a novel algorithm which enables this integration by transforming the k-resiliency constraints to rules consumable by a generic Constraint Programming engine, prove that it guarantees the required resiliency and describe the implementation. We provide some preliminary results and compare our high availability support with naive solutions.
[Transforms, Engines, generic constraint programming engine, constraint handling, administrative constraints, Availability, High Availability, numbering, virtual machine placement, performance evaluation, resource oriented constraints, Virtual machining, high availability goals, Virtual, quality of service, Indexes, Equations, machine, failure, virtual machines, k-resiliency, hardware predictive failure analysis, constraint, placement, Resource management, QoS oriented constraints, second order logic statements]
Efficient and Private Access to Outsourced Data
2011 31st International Conference on Distributed Computing Systems
None
2011
As the use of external storage and data processing services for storing and managing sensitive data becomes more and more common, there is an increasing need for novel techniques that support not only data confidentiality, but also confidentiality of the accesses that users make on such data. In this paper, we propose a technique for guaranteeing content, access, and pattern confidentiality in the data outsourcing scenario. The proposed technique introduces a shuffle index structure, which adapts traditional B+-trees. We show that our solution exhibits a limited performance cost, thus resulting effectively usable in practice.
[pattern classification, outsourced data private access, data management, shuffle index structure, private access, trees (mathematics), external storage, information retrieval, Observers, Data structures, content confidentiality, Encryption, Servers, Indexes, shuffle index, data privacy, pattern confidentiality, data outsourcing, Outsourcing, data processing service, data storing, access confidentiality]
Towards Fine-Grained Access Control in JavaScript Contexts
2011 31st International Conference on Distributed Computing Systems
None
2011
A typical Web 2.0 application usually includes JavaScript from various sources with different trust. It is critical to properly regulate JavaScript's access to web application resources. Unfortunately, existing protection mechanisms in web browsers do not provide enough granularity in JavaScript access control. Specifically, existing solutions partially mitigate this sort of threat by only providing access control for certain types of JavaScript objects, or by unnecessarily restricting the functionality of untrusted JavaScript. In this paper, we systematically analyze the complete access control requirements in a web browser's JavaScript environment and identify the fundamental lack of fine-grained JavaScript access control mechanisms in modern web browsers. As our solution, we propose a reference monitor called JCShadow that enables fine-grained access control in JavaScript contexts without unnecessarily restricting the functionality of JavaScript. We have developed a proof-of-concept prototype in the Mozilla Firefox browser and the evaluation with real-world attacks indicates that JCShadow effectively prevents such attacks with low performance overhead.
[Context, Access control, JC Shadow, Java, fine grained JavaScript access control mechanism, reference monitor, Browsers, Engines, proof-of-concept prototype, Web browser, Web pages, authorisation, online front-ends, Mozilla Firefox browser, Libraries, Internet, Web 2.0 application resources, JavaScript contexts]
A New Class of Buffer Overflow Attacks
2011 31st International Conference on Distributed Computing Systems
None
2011
In this paper, we focus on a class of buffer overflow vulnerabilities that occur due to the "placement new'' expression in C++. "Placement new'' facilitates placement of an object/array at a specific memory location. When appropriate bounds checking is not in place, object overflows may occur. Such overflows can lead to stack as well as heap/data/bss overflows, which can be exploited by attackers in order to carry out the entire range of attacks associated with buffer overflow. Unfortunately, buffer overflows due to "placement new'' have neither been studied in the literature nor been incorporated in any tool designed to detect and/or address buffer overflows. In this paper, we show how the "placement new'' expression in C++ can be used to carry out buffer overflow attacks - on the stack as well as heap/data/bss. We show that overflowing objects and arrays can also be used to carry out virtual table pointer subterfuge, as well as function and variable pointer subterfuge. Moreover, we show how "placement new" can be used to leak sensitive information, and how denial of service attacks can be carried out via memory leakage.
[Decision support systems, storage allocation, heap-data-bss overflows, Placement new, Security, Servers, heap overflow, memory location, virtual table pointer subterfuge, buffer storage, Stack overflow, service attacks, C++ language, buffer overflow attack, Web services, security of data, Memory management, bounds checking, placement new expression, object overflows, Buffer overflow, Arrays, buffer overflow vulnerability, Attacks, memory leakage]
Location Cheating: A Security Challenge to Location-Based Social Network Services
2011 31st International Conference on Distributed Computing Systems
None
2011
Location-based mobile social network services such as foursquare and Gowalla have grown exponentially over the past several years. These location-based services utilize the geographical position to enrich user experiences in a variety of contexts, including location-based searching and location-based mobile advertising. To attract more users, the location-based mobile social network services provide real-world rewards to the user, when a user checks in at a certain venue or location. This gives incentives for users to cheat on their locations. In this paper, we investigate the threat of location cheating attacks, find the root cause of the vulnerability, and outline the possible defending mechanisms. We use foursquare as an example to introduce a novel location cheating attack, which can easily pass the current location verification mechanism (e.g., cheater code of foursquare). We also crawl the foursquare website. By analyzing the crawled data, we show that automated large scale cheating is possible. Through this paper, we aim to call attention to location cheating in mobile social network services and provide insights into the defending mechanisms.
[location-based mobile advertising, location verification mechanism, foursquare Web site, Social network services, Online mobile social networks, Crawlers, Location-based services, Web site crawling, Mobile communication, Servers, computer network security, Global Positioning System, Location cheating, Gowalla, location-based mobile social network service security, mobile computing, Databases, social networking (online), location cheating attacks, location-based searching, Business]
A Monte Carlo Method for Mobile Target Counting
2011 31st International Conference on Distributed Computing Systems
None
2011
This paper addresses the problem of target counting based on the Monte Carlo simulation. We rely on an Accept-Reject process to guide the placement of virtual targets in a virtual sensor field, which has exactly the same sensor layout as the real one. The objective of this construction is to generate a virtual target energy landscape whose shape is close enough to an energy landscape estimated from the real sensor readings. Based on the number of virtual targets placed on the virtual field and the total virtual and real target energy volumes, the number of real targets can be estimated. We consider both single-epoch and multi-epoch sensor readings and our theoretical analysis indicates that by exploiting the information from multiple epochs, our approach yields a target count that approximately converges to the true target count when the number of epochs is large enough. Extensive comparison based simulation study has been performed and the results verify the effectiveness of our target counting algorithms.
[Monte Carlo method, accept-reject process, Shape, wireless sensor networks, Mobile communication, mobile target counting, multiepoch sensor readings, Monte Carlo methods, Accuracy, Clustering algorithms, Approximation algorithms, virtual sensor field, Sensors, single epoch sensor readings, virtual target energy landscape]
Whistle: Synchronization-Free TDOA for Localization
2011 31st International Conference on Distributed Computing Systems
None
2011
Localization is of great importance in mobile and wireless network applications. TDOA is one of the widely used localization schemes, in which a to-be-located object emits a signal and a number of receivers record the arriving time of the signal. By calculating the time difference of different receivers, the location of the object is estimated. In such a scheme, receivers must be precisely synchronized, even slight noises are completely unacceptable for centimeter-level localization. Previous studies have shown that existing time synchronization approaches for low-cost devices are insufficiently accurate and basically infeasible for high accuracy localization. In our scheme (called Whistle), several asynchronous receivers record a target signal and a successive signal that is generated artificially. By two-signal sensing and sample counting techniques, high time resolution can be achieved. This design fundamentally changes TDOA in the sense of releasing the synchronization requirement and avoiding many sources of inaccuracy. We implement Whistle on commercial off-the-shelf (COTS) cell phones. Through extensive real-world experiments in indoor and outdoor, quiet and noisy environments, the mean error is 10~20 centimeters in a 9m &#x00D7; 9m &#x00D7; 4m3 3D space.
[high-time resolution, time-of-arrival estimation, Whistle scheme, asynchronous receivers, Microwave integrated circuits, Accuracy, Three dimensional displays, mobile radio, signal resolution, Receivers, sample counting techniques, commercial off-the-shelf cell phones, synchronization-free TDOA, Synchronization, Microphones, synchronisation, centimeter-level localization, wireless network applications, time synchronization approaches, mobile network applications, receiver time difference, two-signal sensing, to-be-located object, high-accuracy localization, COTS cell phones, signal arriving time, synchronization requirement, Clocks]
Bubble Trace: Mobile Target Tracking under Insufficient Anchor Coverage
2011 31st International Conference on Distributed Computing Systems
None
2011
As an essential requirement for surveillance systems, target tracking has been studied extensively. Most of the tracking schemes are based on trilateration, which requires each point in the monitoring area to be covered by at least three anchors. However, due to the inadequate deployment of costly anchors and environment constraints, the target might not always be detected by three or more anchors simultaneously, resulting in intermittent localization failures and performance degradation. To address this issue, this paper proposes a tracking method called Bubble Trace (BT) for insufficient anchor coverage and asynchronous networks. By fully extracting the location information embedded in dual, single and zero anchor coverage, we develop a bidirectional bounding algorithm to offer the bubble-shaped regions that indicate the possible locations of the target. Moreover, instead of separately estimating each position point of the target, we construct the trace by finding a maximum-likelihood path in a graph. The design is evaluated through extensive simulation and a test-bed experiment with 20 MicaZ nodes. Results show that the proposed scheme improves the tracking accuracy without using additional hardware under insufficient anchor coverage.
[maximum-likelihood path, tracking accuracy, graph theory, Mobile communication, anchor coverage, object detection, anchor deployment, maximum likelihood estimation, MicaZ nodes, Accuracy, bubble-shaped regions, telecommunication network reliability, surveillance systems, intermittent localization failures, surveillance, Monitoring, mobile target tracking, Target tracking, mobile radio, signal detection, Bubble Trace tracking method, environment constraints, bidirectional bounding algorithm, performance degradation, Synchronization, graph, test-bed experiment, trilateration, target detection, target tracking, asynchronous networks, BT tracking method]
Efficient and Robust Localization of Multiple Radiation Sources in Complex Environments
2011 31st International Conference on Distributed Computing Systems
None
2011
We present a robust localization algorithm for multiple radiation sources using a network of sensors under random underlying physical processes and measurement errors. The proposed solution uses a hybrid formulation of particle filter and mean-shift techniques to achieve several important features that address major challenges faced by existing localization algorithms. First, our algorithm is able to maintain a constant number of estimation (source) parameters even as the number of radiation sources K increases. In existing algorithms, the number of estimation parameters is proportional to K and thus the algorithm complexity grows exponentially with K. Second, to decide the number of sources K, existing algorithms either require the information to be known in advance or rely on expensive statistical estimations that do not scale well with K. Instead, our algorithm efficiently learns the number of sources from the estimated source parameters. Third, when obstacles are present, our algorithm can exploit the obstacles to achieve better isolation between the source signatures, thereby increasing the localization accuracy in complex deployment environments. In contrast, incompletely specified obstacles will significantly degrade the accuracy of existing algorithms due to their unpredictable effects on the source signatures. We present extensive simulation results to demonstrate that our algorithm has robust performance in complex deployment environments, and its efficiency is scalable to many radiation sources in these environments.
[statistical estimation, Computational modeling, robust localization algorithm, measurement errors, multiple radiation source, Estimation, particle filtering (numerical methods), sensors network, mean-shift technique, radiation detection, measurement error, Atmospheric measurements, complex environment, Particle measurements, parameter estimation, particle filter formulation, Silicon, Sensors]
Towards Approximate Event Processing in a Large-Scale Content-Based Network
2011 31st International Conference on Distributed Computing Systems
None
2011
Event matching is a critical component of large-scale content-based publish/subscribe systems. However, most existing methods suffer from a dramatic performance degradation when the system scales up. In this paper, we present TAMA (Table Match), a highly efficient content-based event matching and forwarding engine. We consider range-based attribute constraints that are widely used in real-world applications. TAMA employs approximate matching to provide fast event matching against an enormous amount of subscriptions. To this end, TAMA uses a hierarchical indexing table to store subscriptions. Event matching in TAMA becomes the query to this table, which is substantially faster than traditional methods. In addition, the false positive rate of matching events in TAMA can be adjusted by tuning the size of the matching table, which makes TAMA favorable in practice. We implement TAMA as a forwarding component in Siena and conduct extensive experiments with realistic settings. The results demonstrate that TAMA has a significantly faster event matching speed compared to existing methods, and only incurs a small fraction of false positives.
[Content-based publish/subscribe, message passing, pattern matching, indexing, Subscriptions, TAMA, attribute constraint, Complexity theory, performance degradation, Engines, content-based event matching, approximate event processing, Boolean expression, Impedance matching, hierarchical indexing table, approximate event matching, Memory management, large-scale content-based network, large-scale content-based publish/subscribe systems, table match, forwarding engine, Indexing, middleware]
Foundations for Highly Available Content-Based Publish/Subscribe Overlays
2011 31st International Conference on Distributed Computing Systems
None
2011
Content-based publish/subscribe overlays offer a scalable messaging substrate for various event-based distributed systems. In an enterprise environment where service level agreements(SLAs) are strictly enforced, maintaining high availability and efficiency of the broker overlay is critical. To support these requirements, a set of three primitive operations are proposed to allow arbitrary transformations of an overlay to an optima lone, and two additional primitives are developed to enable ondemand adjustments when there are permanent or transient failures. Both sets of primitive operations minimize disruption by preserving message delivery guarantees even as the overlay topology changes, requiring no overhead when the overlay is not being modified, operating on a fixed neighborhood of brokers regardless of the size of the overlay, and completing quickly under a variety of conditions.
[replication, service level agreements, message passing, arbitrary transformations, failures, Subscriptions, Transforms, content-based publish-subscribe overlays, scalable messaging substrate, distributed processing, Routing, Topology, event-based distributed systems, content-based publish/subscribe, content-based retrieval, Delay, high availability, message delivery, Routing protocols]
Green Resource Allocation Algorithms for Publish/Subscribe Systems
2011 31st International Conference on Distributed Computing Systems
None
2011
A popular trend in large enterprises today is the adoption of green IT strategies that use resources as efficiently as possible to reduce IT operational costs. With the publish/subscribe middleware playing a vital role in seamlessly integrating applications at large enterprises including Google and Yahoo, our goal is to search for resource allocation algorithms that enable publish/subscribe systems to use system resources as efficiently as possible. To meet this goal, we develop methodologies that minimize system-wide message rates, broker load, hop count, and the number of allocated brokers, while maximizing the resource utilization of allocated brokers to achieve maximum efficiency. Our contributions consist of developing a bit vector supported resource allocation framework, designing and comparing four different classes with a total of ten variations of subscription allocation algorithms, and developing a recursive overlay construction algorithm. A compelling feature of our work is that it works under any arbitrary workload distribution and is independent of the publish/subscribe language, which makes it easily applicable to any topic and content-based publish/subscribe system. Experiments on a cluster testbed and a high performance computing platform show that our approach reduces the average broker message rate by up to 92% and the number of allocated brokers by up to 91%.
[Measurement, publish-subscribe middleware, subscriber relocation, load minimization, Subscriptions, Complexity theory, Optimization, green IT strategies, load estimation, green IT, resource allocation, arbitrary workload distribution, resource allocation algorithms, Clustering algorithms, Bandwidth, recursive estimation, publisher relocation, system resources, broker load minimization, middleware, publish/subscribe, Google, content-based routing, message passing, system-wide message rates minimization, hop count minimization, subscription allocation algorithms, recursive overlay construction algorithm, bit vector, Yahoo, overlay construction, content-based publish-subscribe system, green resource allocation algorithms, Resource management]
Split and Subsume: Subscription Normalization for Effective Content-Based Messaging
2011 31st International Conference on Distributed Computing Systems
None
2011
Content-based publish/subscribe networks (CPSNs) scale to large numbers of publishers and subscribers by having brokers summarize subscriptions from subscribers and down-stream brokers based on coverage relationships ("subsumption") between subscriptions. A broker forwards the summary to brokers which are upstream on the routes to the publishers. Current summarization and event processing mechanisms induce heavy event processing load on brokers, leading to low event throughput and high latency and further sharp performance degradation under high rates of churn, i.e., addition, deletion, or modification of subscriptions. This paper describes Beretta, a novel CPSN that leverages a simple model of typed events, enabling a succinct and uniform normalized representation of subscriptions. This in turn supports highly effective subsumption and attribute-wise split filtering with matching complexity logarithmic in the number of subscriptions, and enables the systematic introduction of parameters into subscriptions to support both parametric and structural updates. We empirically demonstrate that our techniques significantly improve throughput and latency of event propagation and reduce response times to subscription updates.
[summarization, Subscriptions, Complexity theory, downstream broker, subscription, Systematics, Boolean functions, normalization, CPSN, subsumption, Argon, matching complexity logarithmic, subscription updates, middleware, event processing mechanism, message passing, content-based messaging, attribute wise split filtering, Data structures, Grammar, messaging, content-based, event propagation, subscription normalization, content based publish-subscribe network, sharp performance degradation, computational complexity]
One More Weight is Enough: Toward the Optimal Traffic Engineering with OSPF
2011 31st International Conference on Distributed Computing Systems
None
2011
Traffic Engineering (TE) leverages information of network traffic to generate a routing scheme optimizing the traffic distribution so as to advance network performance. However, optimizing the link weights for OSPF to the offered traffic is an known NP-hard problem. In this paper, we model the optimal TE as the utility maximization of multi-commodity flows and theoretically prove that any given set of optimal routes corresponding to a particular objective function can be converted to shortest paths with respect to a set of positive link weights, which can be explicitly formulated using the optimal distribution of traffic and objective function. This can be directly configured on OSPF-based protocols. On these bases, we employ the Network Entropy Maximization (NEM) framework and develop a new OSPF-based routing protocol, SPEF, to realize a flexible way to split traffic over shortest paths in a distributed fashion. Actually, comparing to OSPF, SPEF only needs one more weight for each link and provably achieves optimal TE. Numerical experiments have been done to compare SPEF with the current version of OSPF, showing the effectiveness of SPEF in terms of link utilization and network load distribution.
[telecommunication links, Traffic engineering, traffic distribution optimization, Delay, Optimization, optimisation, OSPF based routing protocol, Utility, entropy, network entropy maximization framework, Routing protocols, Convex functions, open shortest path first, objective function, optimal traffic engineering, utility maximization, Routing, network traffic, positive link weights, NP-hard problem, Aggregates, routing protocols, multicommodity flow, network load distribution, telecommunication traffic, OSPF]
The Routing Continuum from Shortest-Path to All-Path: A Unifying Theory
2011 31st International Conference on Distributed Computing Systems
None
2011
Routing is a critical operation in networks. In the context of data and sensor networks, routing strategies such as shortest-path, multi-path and potential-based ("all-path") routing have been developed. Based on the connection between routing and flow optimization in a network, in this paper we develop a unifying theoretical framework by considering flow optimization with mixed (weighted) L1/L2-norms. We obtain a surprising result: as we vary the trade-off parameter, the routing graphs induced by the optimal flow solutions span from shortest-path to multi-path to all-path routing - this entire sequence of routing graphs is referred to as the routing continuum. Our theory subsumes the earlier results showing the shortest path and all-path routing can be obtained from L1 and L2 flow optimization, respectively. We also develop an efficient iterative algorithm for computing the entire routing continuum. Several generalizations are also considered, with applications to traffic engineering and wireless sensor networks.
[L1-L2-norms, Electric potential, iterative methods, wireless sensor networks, graph theory, Routing, optimal flow solution span, iterative algorithm, Optimization, Wireless communication, Resistance, Wireless sensor networks, flow optimization, unifying theoretical framework, traffic engineering, Energy dissipation, telecommunication network routing, routing continuum, multipath routing, shortest-path-to-all-path routing, routing graphs, telecommunication traffic]
Greedy Distance Vector Routing
2011 31st International Conference on Distributed Computing Systems
None
2011
Greedy Distance Vector (GDV) is the first geographic routing protocol designed to optimize end-to-end path costs using any additive routing metric, such as: hop count, latency, ETX, ETT, etc. GDV requires no node location information. Instead, GDV uses estimated routing costs to destinations which are locally computed from node positions in a virtual space. GDV makes use of VPoD, a new virtual positioning protocol for wireless networks. Prior virtual positioning systems (e.g., Vivaldi and GNP) were designed for Internet hosts and require that each host measures latencies (routing costs) to distant hosts or landmarks. VPoD does not have this requirement and uses only routing costs between directly connected nodes. Experimental results show that the routing performance of GDV is better than prior geographic routing protocols when hop count is used as metric and much better when ETX is used as metric. As a geographic protocol, the storage cost of GDV per node remains low as network size increases. GDV provides guaranteed delivery for nodes placed in 2D, 3D, and higher dimensions. We also show that GDV and VPoD are highly resilient to dynamic topology changes.
[Measurement, geographic routing protocols, Additives, Economic indicators, geographic routing, VPoD, Routing, routing metrics, virtual positioning systems, wireless routing protocol, routing costs, greedy distance vector routing, Wireless networks, telecommunication network routing, GDV, end-to-end path costs, Routing protocols, Internet, virtual coordinates]
Making CDN and ISP Routings Symbiotic
2011 31st International Conference on Distributed Computing Systems
None
2011
Internet Service Providers (ISPs) route traffic at the IP layer with the preference of less inter-carrier payments while Content Distribution Networks (CDNs) route traffic at the application layer with the preference of better application performance. Such mismatch of routing preferences leads to conflicts that eventually result in higher operational cost for both ISPs and CDNs. In this paper, we propose to make CDN and ISP routing mutually beneficial through ISP's non-uniform bandwidth charging and CDN's bandwidth cost-aware request routing. More specifically, ISPs charge different prices for traffic that traverses different types of inter domain links and CDNs, in routing user requests to their servers, try to minimize their ISP payments by taking the pricing information into consideration. We evaluate the solution in large scale simulations. The greedy solution presents the lowest bandwidth cost for CDNs but at the expense of network performance for users. With end-to-end delay introduced as a constraint in the optimization process, the solution maintains good network performance for users while achieving significant savings in bandwidth cost. Compared with conventional nearest-available policy in CDN request routing, our solution moves significant amount of inter domain traffic from provider routes to peer or customer routes, reducing operational costs for ISPs and CDNs.
[telecommunication links, Biological system modeling, CDN routing symbiotic, nearest-available policy, Servers, computer network performance evaluation, Delay, optimization process, optimisation, ISP routing symbiotic, network performance, telecommunication network routing, Bandwidth, Pricing, CDN bandwidth cost-aware request routing, inter-carrier payments, Internet, IP layer, content distribution networks route traffic, IP networks, telecommunication traffic, Internet service provider route traffic, interdomain links]
SID: Ship Intrusion Detection with Wireless Sensor Networks
2011 31st International Conference on Distributed Computing Systems
None
2011
Surveillance is a vital problem for harbor protection, border control or the security of other commercial facilities. It is particularly challenging to protect the vast near-coast sea surface and busy harbor areas from intrusions of unauthorized marine vessels, such as trespassing boats and ships. In this paper, we present an innovative solution for ship intrusion detection. Equipped with three-axis accelerometer sensors, we deploy an experimental wireless sensor network on the sea surface to detect ships. Using signal processing techniques and cooperative signal processing, we can detect the passing ships by distinguishing the ship-generated waves and the ocean waves. We design an intrusion detection system in which we propose to exploit spatial and temporal correlations of the intrusion to increase detection reliability. We conduct evaluations with real data collected by our initial experiments, and provide quantitative analysis on the detection system, such as the successful detection ratio and the estimation of the intruding ship velocity.
[telecommunication security, wireless sensor networks, intruding ship velocity estimation, signal processing, ship intrusion detection, cooperative signal processing, commercial facility security, spatial correlations, border control, ship-generated waves, detection reliability, harbor protection, Intrusion detection, Ocean waves, Sensors, surveillance, Marine vehicles, three-axis accelerometer sensors, ocean waves, Accelerometers, Wavelet transforms, Ship detection, marine communication, Sea measurements, SID, near-coast sea surface protection, signal processing techniques, accelerometers, temporal correlations, marine vessel intrusion]
Compressive Sensing Approach to Urban Traffic Sensing
2011 31st International Conference on Distributed Computing Systems
None
2011
Traffic sensing is crucial to a number of tasks such as traffic management and city road network engineering. We build a traffic sensing system with probe vehicles for metropolitan scale traffic sensing. Each probe vehicle senses its instant speed and position periodically and sensory data of probe vehicles can be aggregated for traffic sensing. However, there is a critical issue that the sensory data contain spatiotemporal vacancies with no reports. This is a result of the naturally uneven distribution of probe vehicles in both spatial and temporal dimensions since they move at their own wills. This paper proposes a new approach based on compressive sensing to large-scale traffic sensing in urban areas. We mine the extensive real trace datasets of taxies in an urban environment with principal component analysis and reveal the existence of hidden structures with sensory traffic data that underpins the compressive sensing approach. By exploiting the hidden structures, an efficient algorithm is proposed for finding the best estimate traffic condition matrix by minimizing the rank of the estimate matrix. With extensive trace-driven experiments, we demonstrate that the proposed algorithm outperforms a number of alternative algorithms. Surprisingly, we show that our algorithm can achieve an estimation error of as low as 20% even when more than 80% of sensory data are not present.
[Algorithm design and analysis, traffic matrices, sensory data, Roads, hidden structures, large-scale urban traffic sensing, traffic management, traffic sensing, taxies, Vehicles, spatiotemporal vacancies, Sensors, Probes, compressive sensing approach, data compression, metropolitan scale traffic sensing, estimate matrix rank minimization, trace datasets, sensory traffic data, traffic engineering computing, Matrix decomposition, city road network engineering, matrix algebra, interpolation, trace-driven experiments, traffic condition matrix, probe vehicles, Compressed sensing, principal component analysis, compressive sensing]
Towards Optimal Sensor Placement for Hot Server Detection in Data Centers
2011 31st International Conference on Distributed Computing Systems
None
2011
Recent studies have shown that a significant portion of the total energy consumption of many data centers is caused by the inefficient operation of their cooling systems. Without effective thermal monitoring with accurate location information, the cooling systems often use unnecessarily low temperature set points to over cool the entire room, resulting in excessive energy consumption. Sensor network technology has recently been adopted for data-center thermal monitoring because of its non-intrusive nature for the already complex data center facilities and robustness to instantaneous CPU or disk activities. However, existing solutions place sensors in a simplistic way without considering the thermal dynamics in data centers, resulting in unnecessarily degraded hot server detection probability. In this paper, we first formulate the problem of sensor placement for hot server detection in a data center as a constrained optimization problem. We then propose a novel placement scheme based on Computational Fluid Dynamics (CFD) to take various factors, such as cooling systems and server layout, as inputs to analyze the thermal conditions of the data center. Based on the CFD analysis in various server overheating scenarios, we apply data fusion and advanced optimization techniques to find a near-optimal sensor placement solution, such that the probability of detecting hot servers is significantly improved. Our empirical results in a real server room demonstrate the detection performance of our placement solution. Extensive simulation results also show that the proposed solution outperforms a commonly used placement solution in terms of detection probability.
[data fusion, optimal sensor placement, temperature sensors, data centers, sensor fusion, sensor network technology, Servers, thermal dynamics, power consumption, Temperature sensors, optimisation, sensor placement, file servers, Mathematical model, CFD analysis, nonintrusive nature, Monitoring, constrained optimization, computational fluid dynamics (CFD), hot server detection, Computational fluid dynamics, computational fluid dynamics, probability, disk activities, total energy consumption, computer centres, thermal monitoring, CPU activities, Temperature measurement, Data centers, mechanical engineering computing, cooling, cooling systems]
E-Shadow: Lubricating Social Interaction Using Mobile Phones
2011 31st International Conference on Distributed Computing Systems
None
2011
In this paper, we propose E-Shadow, a distributed mobile phone-based local social networking system. E-Shadow has two main components: (1) Local profiles. They enable EShadow users to record and share their names, interests, and other information with fine-grained privacy controls. (2) Mobile phone based local social interaction tools. E-Shadow provides mobile phone software that enables rich social interactions. The software maps proximate users' local profiles to their human owners and enables user communication and content sharing. We have designed and implemented E-Shadow on mobile phones. In our E-Shadow system, we allow users to perform dynamic and layered information publishing, making use of interpersonal relevance. Our system also provides a mechanism to help users perform direction-driven localization of an E-Shadow and match it with its owner. Experiments on real world Windows Mobile phones and large-scale simulations show that our system disseminates information efficiently and helps receivers find the direction of a specific E-Shadow with accuracy. We believe our E-Shadow concept and system can lead to a more tightly-knit temporary community in one's physical vicinity.
[layered information publishing, Bluetooth, distributed processing, Mobile handsets, user interfaces, Wireless communication, Publishing, Windows Mobile phones, social interaction lubrication, publishing, fine-grained privacy controls, mobile radio, IEEE 802.11 Standards, information dissemination, Direction-driven Matching, Receivers, mobile phone software, Wireless sensor networks, direction-driven localization, social networking (online), human computer interaction, E-Shadow, Layered Publishing, distributed mobile phone-based local social networking system, Mobile Phone]
[Publisher Information]
2011 31st International Conference on Distributed Computing Systems
None
2011
Provides a listing of current committee members and society officers.
[]
Message from the General Chairs
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Presents the welcome message from the conference proceedings.
[]
Message from the Program Chairs
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Presents the welcome message from the conference proceedings.
[]
eTransform: Transforming Enterprise Data Centers by Automated Consolidation
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Modern day enterprises have a large IT infrastructure comprising thousands of applications running on servers housed in tens of data centers geographically spread out. These enterprises periodically perform a transformation of their entire IT infrastructure to simplify, decrease operational costs and enable easier management. However, the large number of different kinds of applications and data centers involved and the variety of constraints make the task of data center transformation challenging. The state-of-the-art technique for performing this transformation is simplistic, often unable to account for all but the simplest of constraints. We present eTransform, a system for generating a transformation and consolidation plan for the IT infrastructure of large scale enterprises. We devise a linear programming based approach that simultaneously optimizes all the costs involved in enterprise data centers taking into account the constraints of applications groups. Our algorithm handles the various idiosyncrasies of enterprise data centers like volume discounts in pricing, wide-area network costs, traffic matrices, latency constraints, distribution of users accessing the data etc. We include a disaster recovery (DR) plan, so that eTransform, thus provides an integrated disaster recovery and consolidation plan to transform the enterprise IT infrastructure. We use eTransform to perform case studies based on real data from three different large scale enterprises. In our experiments, eTransform is able to suggest a plan to reduce the operational costs by more than 50% from the "as-is" state of these enterprise to the consolidated enterprise IT environment. Even including the DR capability, eTransform is still able to reduce the operational costs by more than 25% from the simple "as-is" state. In our experiments, eTransform is able to simultaneously optimize multiple parameters and constraints and discover solutions that are 7x cheaper than other solutions.
[Algorithm design and analysis, traffic matrix, latency constraint, information technology, eTransform system, enterprise data center, linear programming, Servers, automated consolidation, Business, user distribution, disaster recovery plan, Wide area networks, cost reduction, wide-area network cost, computer centres, disaster consolidation plan, large scale enterprise, volume discount, IT infrastructure, Economies of scale, business continuity, cost optimization, Planning, Virtual private networks, data center transformation, business data processing, pricing, operational cost reduction, DR capability]
VirtualKnotter: Online Virtual Machine Shuffling for Congestion Resolving in Virtualized Datacenter
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Our measurements on production data center traffic together with recently reported results suggest that data center networks suffer from long-lived congestion caused by core network over subscription and unbalanced workload placement. In contrast to traditional traffic engineering approaches that optimize flow routing, in this paper, we explore the opportunity to address the continuous congestion via optimizing VM placement in virtualized data centers. To this end, we present Virtual Knotter, an efficient online VM placement algorithm to reduce congestion with controllable VM migration traffic as well as low time complexity. Our evaluation with both real and synthetic traffic patterns shows that Virtual Knotter performs close to the baseline algorithm in terms of link unitization, with only 5%-10% migration traffic of the baseline algorithm. Furthermore, Virtual Knotter decreases link congestion time by 53% for the production data center traffic.
[virtual reality, Heuristic algorithms, VM migration traffic, virtualized datacenter, link congestion, Servers, Optimization, synthetic traffic pattern, online VM placement algorithm, production data center traffic, traffic engineering, Clustering algorithms, continuous congestion, Production, VirtualKnotter, data center networks, unbalanced workload placement, baseline algorithm, time complexity, Routing, Partitioning algorithms, computer centres, online virtual machine shuffling, flow routing, virtual machines, core network, virtualized data center]
Provably-Efficient Job Scheduling for Energy and Fairness in Geographically Distributed Data Centers
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Decreasing the soaring energy cost is imperative in large data centers. Meanwhile, limited computational resources need to be fairly allocated among different organizations. Latency is another major concern for resource management. Nevertheless, energy cost, resource allocation fairness, and latency are important but often contradicting metrics on scheduling data center workloads. In this paper, we explore the benefit of electricity price variations across time and locations. We study the problem of scheduling batch jobs, which originate from multiple organizations/users and are scheduled to multiple geographically-distributed data centers. We propose a provably-efficient online scheduling algorithm -- Gre Far -- which optimizes the energy cost and fairness among different organizations subject to queueing delay constraints. Gre Far does not require any statistical information of workload arrivals or electricity prices. We prove that it can minimize the cost (in terms of an affine combination of energy cost and weighted fairness) arbitrarily close to that of the optimal offline algorithm with future information. Moreover, by appropriately setting the control parameters, Gre Far achieves a desirable tradeoff among energy cost, fairness and latency.
[Availability, Algorithm design and analysis, fairness, Scheduling, Data center, Servers, Delay, delay, Electricity, batch jobs, Distributed databases, scheduling, energy cost]
DARD: Distributed Adaptive Routing for Datacenter Networks
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Data center networks typically have many paths connecting each host pair to achieve high bisection bandwidth for arbitrary communication patterns. Fully utilizing the bisection bandwidth may require flows between the same source and destination pair to take different paths. However, existing routing protocols have little support for load-sensitive adaptive routing. We propose DARD, a Distributed Adaptive Routing architecture for Data center networks. DARD allows each end host to move traffic from overloaded paths to under loaded paths without central coordination. We use an Open Flow implementation and simulations to show that DARD can effectively use a data center network's bisection bandwidth under both static and dynamic traffic patterns. It outperforms previous solutions based on random path selection by 10%, and performs similarly to previous work that assigns flows to paths using a centralized controller. We use competitive game theory to show that DARD's path selection algorithm makes progress in every step and converges to a Nash equilibrium in finite steps. Our evaluation results suggest that DARD can achieve a close-to-optimal solution in practice.
[Distributed Adaptive Routing, Network topology, Bandwidth, Datacenter, Routing, Load management, Topology, Monitoring, Oscillators]
Skeleton Extraction from Incomplete Boundaries in Sensor Networks Based on Distance Transform
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
We study the problem of skeleton extraction for large-scale sensor networks using only connectivity information. Existing solutions for this problem heavily depend on an algorithm that can accurately detect network boundaries. This dependence may seriously affect the effectiveness of skeleton extraction. For example, in low density networks, boundary detection algorithms normally do not work well, potentially leading to an incorrect skeleton being generated. This paper proposes a novel approach, named DIST, to skeleton extraction from incomplete boundaries using the idea of distance transform, a concept in the computer graphics area. The main contribution is a distributed and low-cost algorithm that produces accurate network skeletons without requiring that the boundaries be complete or tight. The algorithm first establishes the network's distance transform - the hop distance of each node to the network's boundaries. Based on this, some critical skeleton nodes are identified. Next, a set of skeleton arcs are generated by controlled flooding; connecting these skeleton arcs then gives us a coarse skeleton. The algorithm finally refines the coarse skeleton by building shortest path trees, followed by a prune phase. The obtained skeletons are robust to boundary noise and shape variations.
[connectivity information, low-cost algorithm, skeleton nodes, wireless sensor networks, Noise, DIST, Transforms, skeleton arcs, transforms, distance transform, large-scale sensor networks, skeleton, prune phase, shape variations, boundary detection algorithms, hop distance, Skeleton, boundary noise, Computer vision, Radiation detectors, computer graphics area, trees (mathematics), shortest path trees, controlled flooding, distributed algorithm, skeleton extraction, network boundary, incomplete boundaries, distributed algorithms, coarse skeleton, Sensor networks, low density networks, Joining processes, Detection algorithms]
Connectivity-based and Boundary-Free Skeleton Extraction in Sensor Networks
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
In sensor networks, skeleton (also known as medial axis) extraction is recognized as an appealing approach to support many applications such as load-balanced routing and location free segmentation. Existing solutions in the literature rely heavily on the identified boundaries, which puts limitations on the applicability of the skeleton extraction algorithm. In this paper, we conduct the first work of a connectivity-based and boundary free skeleton extraction scheme, in sensor networks. In detail, we propose a simple, distributed and scalable algorithm that correctly identifies a few skeleton nodes and connects them into a meaningful representation of the network, without reliance on any constraint on communication radio model or boundary information. The key idea of our algorithm is to exploit the necessary (but not sufficient) condition of skeleton points: the intersection area of the disk centered at a skeleton point x should be the largest one as compared to other points on the chord generated by x, where the chord is referred to as the line segment connecting x and the tangent point in the boundary. To that end, we present the concept of &#x03B5;-centrality of a point, quantitatively measuring how "central" a point is. Accordingly, a skeleton point should have the largest value of &#x03B5;-centrality as compared to other points on the chord generated by this point. Our simulation results show that the proposed algorithm works well even for networks with low node density or skewed nodal distribution, etc. In addition, we obtain two by-products, the boundaries and the segmentation result of the network.
[Computer aided software engineering, location free segmentation, wireless sensor networks, wireless sensor network, Neighborhood Size, Routing, boundary-free skeleton extraction, skeleton extraction algorithm, Indexes, Data mining, Wireless Sensor Networks, Voronoi Cell, boundary information, medial axis, Wireless sensor networks, skewed nodal distribution, Skeleton Extraction, telecommunication network routing, load-balanced routing, communication radio model, &#x03B5;-centrality, Skeleton, Routing protocols, connectivity-based extraction]
A Kautz-based Real-Time and Energy-Efficient Wireless Sensor and Actuator Network
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Wireless Sensor and Actuator Networks (WSANs) are composed of sensors and actuators to perform distributed sensing and actuating tasks. Most WSAN applications (e.g., fire detection) demand that actuators rapidly respond to events under observation. Therefore, real-time and fault-tolerant transmission is a critical requirement in WSANs to enable sensed data to reach actuators reliably and quickly. Due to limited power resources, energy-efficiency is another crucial requirement. Such requirements become formidably challenging in large-scale WSANs. However, existing WSANs fall short in meeting these requirements. To this end, we first theoretically study the Kautz graph for its applicability in WSANs to meet these requirements. We then propose a Kautz-based Real-time, Fault-tolerant and Energy-efficient WSAN (REFER). REFER has a protocol that embeds Kautz graphs into the physical topology of a WSAN for real-time communication and connects the graphs using Distributed Hash Table (DHT) for high scalability. We also theoretically study routing paths in the Kautz graph, based on which we develop an efficient fault-tolerant routing protocol. It enables a relay node to quickly and efficiently identify the next shortest path from itself to the destination only based on node IDs upon routing failure. REFER is advantageous over previous Kautz graph based works in that it does not need an energy-consuming protocol to find the next shortest path and it can maintain the consistency between the overlay and physical topology. Experimental results demonstrate the superior performance of REFER in comparison with existing systems in terms of real-time communication, energy-efficiency, fault-tolerance and scalability.
[Real time systems, routing paths, Actuators, wireless sensor networks, REFER, graph theory, Kautz Graph, physical topology, fault tolerant WSAN, Fault tolerance, fault tolerant routing protocol, wireless sensor and actuator networks, real time communication, actuators, Fault tolerant systems, telecommunication network reliability, energy efficiency, fault tolerant, Routing protocols, power resource, Kautz based real time wireless sensor and actuator network, fault tolerance, telecommunication network topology, Routing, Topology, energy efficient WSAN, shortest path, Kautz graphs, distributed hash table, routing protocols, routing failure, distributed sensing, real-time routing]
EQS: Neighbor Discovery and Rendezvous Maintenance with Extended Quorum System for Mobile Sensing Applications
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
In many mobile sensing applications devices need to discover new neighbors and maintain the rendezvous with known neighbors continuously. Due to the limited energy supply, these devices have to cycle their radios to conserve energy, making neighbor discovery and rendezvous maintenance even more challenging. To date, the main mechanism for device discover and rendezvous maintenance in existing solutions is pair wise, direct one-hop communication. We argue that such pair wise direct communication is sufficient but not necessary: there exist unnecessary active slots that can be eliminated, without affecting discovery and rendezvous. In this work, we propose a novel concept of extended quorum system, which leverages indirect discovery to further conserve energy. Specifically, we use quorum graph to capture all possible information flow paths where knowledge about known-neighbors can propagate among devices. By eliminating redundant paths, we can reduce the number of active slots significantly. Since a quorum graph can characterize arbitrary active schedules of mobile devices, our work can be broadly used to improve many existing quorum based discovery and rendezvous solutions. The simulation and test bed experimental results show that our solution can reduce as much as 55% energy consumption with a maximal 5% increase in latency for existing solutions.
[Context, quorum graph, information flow path, Schedules, Protocols, wireless sensor networks, graph theory, neighbor discovery, Maintenance engineering, Minimization, Mobile communication, direct one hop communication, rendezvous maintenance, mobile sensing application device, quorum-based discovery, energy conservation, Sensors, protocols, extended quorum system, arbitrary active schedule, mobile handsets]
4D TeleCast: Towards Large Scale Multi-site and Multi-view Dissemination of 3DTI Contents
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
3D Tele-immersive systems create real-time multi-stream and multi-view 3D collaborative contents from multiple sites to allow interactive shared activities in virtual environments. Applications of 3DTI include online sports, tele-health, remote learning and collaborative arts. In addition to interactive participants in 3DTI environments, we envision a large number of passive non-interactive viewers that (a) watch the interactive activities in 3DTI shared environments, and (b) select views of the activities at run time. To achieve this vision, we present 4D Tele Cast, a novel multi-stream 3D content distribution framework for non-interactive viewers providing the functionality of multi-view selection. It addresses the following challenges: (1) supporting a large number of concurrent multi-stream viewers as well as multi-views, (2) preserving the unique nature of 3DTI multi-stream and multi-view dependencies at the viewers, and (3) allowing dynamic viewer behavior such as view changes and large-scale simultaneous viewer arrivals or departures. We divide the problem space into two: (1) multi-stream overlay construction problem that aims to minimize the cost of distribution of multi-stream contents, and maximize the number of concurrent viewers with sufficient viewer dynamism in terms of their resources and availabilities, and (2) effective resource utilization problem that aims to preserve the multi-stream dependencies in a view considering the heterogeneous resource constraints at the viewers. We evaluate 4D Tele Cast using extensive simulations with 3DTI activity data and Planet Lab traces.
[virtual reality, large scale multisite dissemination, passive noninteractive viewers, real-time multistream 3D collaborative content, online sports, Delay, 4D TeleCast, heterogeneous resource constraint, multiview 3D collaborative content, Multi-stream P2P, distribution cost minimization, Topology, Resource Utilization, viewer dynamism, P2P-CDN, Planet Lab trace, virtual environment, Logic gates, Cameras, remote learning, concurrent multistream viewers, Subscriptions, 3DTI shared environment, multimedia systems, multiview dependency, multistream 3D content distribution, collaborative arts, Content Dissemination, multistream overlay construction problem, Multi-view, telehealth, Bandwidth, groupware, 3DTI contents, view change, interactive shared activity, health care, interactive participants, run time activity view selection, art, dynamic viewer behavior, 3D teleimmersive system, concurrent viewer number maximization, resource utilization problem, Tele-immersion, multiview selection, Channel allocation, computer aided instruction, sport, multiview dissemination, 3DTI multistream dependency]
A Novel En-route Filtering Scheme against False Data Injection Attacks in Cyber-Physical Networked Systems
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
In Cyber-Physical Networked Systems (CPNS), attackers could inject false measurements to the controller through compromised sensor nodes, which not only threaten the security of the system, but also consumes network resources. To deal with this issue, a number of en-route filtering schemes have been designed for wireless sensor networks. However, these schemes either lack resilience to the number of compromised nodes or depend on the statically configured routes and node localization, which are not suitable for CPNS. In this paper, we propose a Polynomial-based Compromised-Resilient En-route Filtering scheme (PCREF), which can filter false injected data effectively and achieve a high resilience to the number of compromised nodes without relying on static routes and node localization. Particularly, PCREF adopts polynomials instead of MACs (message authentication codes) for endorsing measurement reports to achieve the resilience to attacks. Each node stores two types of polynomials: authentication polynomial and check polynomial derived from the primitive polynomial, and used for endorsing and verifying the measurement reports. Via extensive theoretical analysis and simulation experiments, our data show that PCREF achieves better filtering capacity and resilience to the large number of compromised nodes in comparison to the existing schemes.
[telecommunication security, false measurement, Actuators, polynomial-based en-route filtering, wireless sensor networks, wireless sensor network, polynomial-based compromised-resilient en-route filtering scheme, authentication polynomial, sensor networks, Cyber-Physical networked system, system security, attack resilience, Polynomials, Sensors, check polynomial, Monitoring, false data injection attack, polynomials, filtering theory, PCREF, false measurement report, cyber-physical networked system, CPNS, MAC, Resilience, network resource, primitive polynomial, Authentication, message authentication, telecommunication network routing, filtering capacity, message authentication code, sensor node]
Scaling Down Off-the-Shelf Data Compression: Backwards-Compatible Fine-Grain Mixing
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Pu and Singaravelu presented Fine-Grain Mixing, an adaptive compression system which aimed to maximize CPU and network utilization simultaneously by splitting a network stream into a mixture of compressed and uncompressed blocks. Blocks were compressed opportunistically in a send buffer, they compressed as many blocks as they could without becoming a bottleneck. They successfully utilized all available CPU and network bandwidth even on high speed connections. In addition, they noted much greater throughput than previous adaptive compression systems. Here, we take a different view of FG-Mixing than was taken by Pu and Singaravelu and give another explanation for its high performance: that fine-grain mixing of compressed and uncompressed blocks enables off-the-shelf compressors to scale down their degree of compression linearly with decreasing CPU usage. Exploring the scaling behavior in-depth allows us to make a variety of improvements to fine-grain mixed compression: better compression ratios for a given level of CPU consumption, a wider range of data reduction and CPU cost options, and parallelized compression to take advantage of multi-core CPUs. We make full compatibility with the ubiquitous deflate decompress or (as used in many network protocols directly, or as the back-end of the gzip and Zip formats) a primary goal, rather than using a special, incompatible protocol as in the original implementation of FG-Mixing. Moreover, we show that the benefits of fine-grain mixing are retained by our compatible version.
[CPU consumption, parallelized compression, data compression, fine-grain mixed compression, CPU cost option, Switches, network utilization maximization, Throughput, off-the-shelf data compression, off-the-shelf compressor, Compressors, CPU utilization maximization, History, network bandwidth, Standards, scaling behavior, data reduction, Image coding, Bandwidth, backwards-compatible fine-grain mixing, multicore CPU, adaptive compression system]
Distributed Incomplete Pattern Matching via a Novel Weighted Bloom Filter
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
In this paper, we first propose a very interesting and practical problem, pattern matching in a distributed mobile environment. Pattern matching is a well-known problem and extensive research has been conducted for performing effective and efficient search. However, previous proposed approaches assume that data are centrally stored, which is not the case in a mobile environment (e.g., mobile phone networks), where one person's pattern could be separately stored in a number of different stations, and such a local pattern is incomplete compared with the global pattern. A simple solution to pattern matching over a mobile environment is to collect all the data distributed in base stations to a data center and conduct pattern matching at the data center afterwards. Clearly, such a simple solution will raise huge amount of communication traffic, which could cause the communication bottleneck brought by the limited wireless bandwidth to be even worse. Therefore, a communication efficient and search effective solution is necessary. In our work, we present a novel solution which is based on our well-designed Weighted Bloom Filter (WBF), called, Distributed Incomplete pattern matching (DI-matching), to find target patterns over a distributed mobile environment. Specifically, to save communication cost and ensure pattern matching in distributed incomplete patterns, we use WBF to encode a query pattern and disseminate the encoded data to each base station. Each base station conducts a local pattern search according to the received WBF. Only qualified IDs and corresponding weights in each base station are sent to the data center for aggregation and verification. Through extensive empirical experiments on a real city-scale mobile networks data set, we demonstrate the effectiveness and efficiency of our proposed solutions.
[pattern matching, Incomplete pattern matching, encoded data dissemination, Weighted Bloom Filter, Mobile communication, Mobile handsets, person pattern, data aggregation, query pattern encoding, query processing, global pattern, Distributed databases, communication bottleneck, data structures, target pattern finding, city-scale mobile network, Base stations, mobile radio, data analysis, mobile phone network, local pattern search, Time series analysis, distributed mobile environment, time series, distributed incomplete pattern matching, communication cost, data verification, distributed data collection, search effective solution, DI-matching, Matched filters, data center, limited wireless bandwidth, weighted bloom filter, space-efficient randomized data structure, communication traffic, Pattern matching, base stations]
Distributed Maintenance of Cache Freshness in Opportunistic Mobile Networks
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Opportunistic mobile networks consist of personal mobile devices which are intermittently connected with each other. Data access can be provided to these devices via cooperative caching without support from the cellular network infrastructure, but only limited research has been done on maintaining the freshness of cached data which may be refreshed periodically and is subject to expiration. In this paper, we propose a scheme to efficiently maintain cache freshness. Our basic idea is to let each caching node be only responsible for refreshing a specific set of caching nodes, so as to maintain cache freshness in a distributed and hierarchical manner. Probabilistic replication methods are also proposed to analytically ensure that the freshness requirements of cached data are satisfied. Extensive trace driven simulations show that our scheme significantly improves cache freshness, and hence ensures the validity of data access provided to mobile users.
[Peer to peer computing, caching nodes, probability, probabilistic replication methods, extensive trace driven simulations, Mobile communication, cache storage, cache freshness, Delay, cooperative communication, personal mobile devices, mobile communication, Aggregates, Distributed databases, Feeds, distributed maintenance, cooperative caching, opportunistic mobile networks, Mobile computing]
NEAT: Road Network Aware Trajectory Clustering
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Mining trajectory data has been gaining significant interest in recent years. However, existing approaches to trajectory clustering are mainly based on density and Euclidean distance measures. We argue that when the utility of spatial clustering of mobile object trajectories is targeted at road network aware location based applications, density and Euclidean distance are no longer the effective measures. This is because traffic flows in a road network and the flow-based density characterization become important factors for finding interesting trajectory clusters of mobile objects travelling in road networks. In this paper, we propose NEAT-a road network aware approach for fast and effective clustering of spatial trajectories of mobile objects travelling in road networks. Our method takes into account the physical constraints of the road network, the network proximity and the traffic flows among consecutive road segments to organize trajectories into spatial clusters. The clusters discovered by NEAT are groups of sub-trajectories which describe both dense and highly continuous traffic flows of mobile objects. We perform extensive experiments with mobility traces generated using different scales of real road network maps. Our experimental results demonstrate that the NEAT approach is highly accurate and runs orders of magnitude faster than existing density-based trajectory clustering approaches.
[Roads, Trajectory clustering, road network, Clustering algorithms, traffic flow, Mobile communication, Silicon, location-based services, Trajectory, map matching, Junctions, Mobile computing]
Dynamic Activation Policies for Event Capture with Rechargeable Sensors
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
We consider the problem of event capture by a rechargeable sensor network. We assume that the events of interest follow a renewal process whose event inter-arrival times are drawn from a general probability distribution, and that a stochastic recharge process is used to provide energy for the sensors' operation. Dynamics of the event and recharge processes make the optimal sensor activation problem highly challenging. In this paper we first consider the single-sensor problem. Using dynamic control theory, we consider a full-information model in which, independent of its activation schedule, the sensor will know whether an event has occurred in the last time slot or not. In this case, the problem is framed as a Markov decision process (MDP), and we develop a simple and optimal policy for the solution. We then further consider a partial-information model where the sensor knows about the occurrence of an event only when it is active. This problem falls into the class of partially observable Markov decision processes (POMDP). Since the POMDP's optimal policy has exponential computational complexity and is intrinsically hard to solve, we propose an efficient heuristic clustering policy and evaluate its performance. Finally, our solutions are extended to handle a network setting in which multiple sensors collaborate to capture the events. We provide extensive simulation results to evaluate the performance of our solutions.
[dynamic activation, partial-information model, probability, dynamic control theory, Sensor phenomena and characterization, Sensor systems, Discharges (electric), rechargeable sensor network, optimal sensor activation, single-sensor problem, sensors, event capture, exponential computational complexity, POMDP, Markov processes, dynamic activation policies, probability distribution, MDP, Markov decision process, rechargeable sensors, Monitoring, computational complexity]
FindingHuMo: Real-Time Tracking of Motion Trajectories from Anonymous Binary Sensing in Smart Environments
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
In this paper we have proposed and designed FindingHuMo (Finding Human Motion), a real-time user tracking system for Smart Environments. FindingHuMo can perform device-free tracking of multiple (unknown and variable number of) users in the Hallway Environments, just from non-invasive and anonymous (not user specific) binary motion sensor data stream. The significance of our designed system are as follows: (a) fast tracking of individual targets from binary motion data stream from a static wireless sensor network in the infrastructure. This needs to resolve unreliable node sequences, system noise and path ambiguity, (b) Scaling for multi-user tracking where user motion trajectories may crossover with each other in all possible ways. This needs to resolve path ambiguity to isolate overlapping trajectories, FindingHumo applies the following techniques on the collected motion data stream: (i) a proposed motion data driven adaptive order Hidden Markov Model with Viterbi decoding (called Adaptive-HMM), and then (ii) an innovative path disambiguation algorithm (called CPDA). Using this methodology the system accurately detects and isolates motion trajectories of individual users. The system performance is illustrated with results from real-time system deployment experience in a Smart Environment.
[Real time systems, multiuser tracking, Adaptation models, wireless sensor networks, Tracking, anonymous binary sensing, hidden Markov model, system noise, binary motion sensor, hidden Markov models, noninvasive binary motion sensor data stream, Employment, innovative path disambiguation algorithm, multi-access systems, Trajectory, Hidden Markov Model, static wireless sensor network, finding human motion, real-time system deployment, Target tracking, device-free tracking, Wireless Sensor Networks, FindingHuMo, adaptive-HMM, path ambiguity, sensors, hallway environment, motion data driven adaptive order, CPDA, Hidden Markov models, real-time systems, target tracking, anonymous binary motion sensor data stream, system performance, user motion trajectory, smart environment, real-time user tracking system, Human localization, Viterbi decoding, Smart Environments, node sequence]
Tiresias: Online Anomaly Detection for Hierarchical Operational Network Data
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Operational network data, management data such as customer care call logs and equipment system logs, is a very important source of information for network operators to detect problems in their networks. Unfortunately, there is lack of efficient tools to automatically track and detect anomalous events on operational data, causing ISP operators to rely on manual inspection of this data. While anomaly detection has been widely studied in the context of network data, operational data presents several new challenges, including the volatility and sparseness of data, and the need to perform fast detection (complicating application of schemes that require offline processing or large/stable data sets to converge). To address these challenges, we propose Tiresias, an automated approach to locating anomalous events on hierarchical operational data. Tiresias leverages the hierarchical structure of operational data to identify high-impact aggregates (e.g., locations in the network, failure modes) likely to be associated with anomalous events. To accommodate different kinds of operational network data, Tiresias consists of an online detection algorithm with low time and space complexity, while preserving high detection accuracy. We present results from two case studies using operational data collected at a large commercial IP network operated by a Tier-1 ISP: customer care call logs and set-top box crash logs. By comparing with a reference set verified by the ISP's operational group, we validate that Tiresias can achieve &gt;;94% accuracy in locating anomalies. Tiresias also discovered several previously unknown anomalies in the ISP's customer care cases, demonstrating its effectiveness.
[time series analysis, hierarchical operational data, anomaly detection, Tiresias, Accuracy, detection accuracy, manual inspection, IP networks, log analysis, online anomaly detection, Charge coupled devices, high-impact aggregate, Time series analysis, hierarchical structure, ISP operator, data sparseness, time complexity, network problem detection, Computer crashes, data volatility, Forecasting, online detection algorithm, IP network, equipment system logs, anomalous event detection, hierarchical operational network data, customer care call logs, security of data, Aggregates, Vegetation, operational network data, anomalous event tracking, management data, space complexity, Tier-1 ISP]
Limiting Byzantine Influence in Multihop Asynchronous Networks
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
We consider the problem of reliably broadcasting information in a multi hop asynchronous network that is subject to Byzantine failures. That is, some nodes of the network can exhibit arbitrary (and potentially malicious) behavior. Existing solutions provide deterministic guarantees for broadcasting between all correct nodes, but require that the communication network is highly-connected (typically, 2k+1 connectivity is required, where k is the total number of Byzantine nodes in the network). In this paper, we investigate the possibility of Byzantine tolerant reliable broadcast between most correct nodes in low-connectivity networks (typically, networks with constant connectivity). In more details, we propose a new broadcast protocol that is specifically designed for low-connectivity networks. We provide sufficient conditions for correct nodes using our protocol to reliably communicate despite Byzantine participants. We present experimental results that show that our approach is especially effective in low-connectivity networks when Byzantine nodes are randomly distributed.
[Protocols, low-connectivity networks, multihop asynchronous networks, Multihop networks, broadcast communication, Distributed computing, broadcast protocol, Random failures, Authorization, sufficient conditions, Fault tolerance, telecommunication network reliability, Broadcasting, protocols, Protocol, Reliable broadcast, Byzantine tolerant reliable broadcast, fault tolerance, Peer to peer computing, Byzantine failures, constant connectivity, Topology, Standards, Reliability, Asynchronous networks]
When You Don't Trust Clients: Byzantine Proposer Fast Paxos
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
We derive a consensus protocol for a hybrid failure model. In this model, clients are Byzantine faulty and servers are crash faulty. We argue that this model is well suited to environments where the servers run within one administrative domain, and the clients run outside of this domain. Our consensus protocol, which is derived from crash Paxos, provides low latency for client requests, tolerates any number of (Byzantine) faulty clients, up to 1/3 (crash) faulty servers, and does not rely on computing costly signatures in the common case. It can be used to build state machine replication that provides a highly available service.
[crash Paxos, Protocols, Law, administrative domain, client request latency, Servers, Proposals, system recovery, Detectors, Safety, Byzantine proposer fast Paxos, protocols, highly available service, client-server systems, state machine replication, consensus protocol, crash faulty server, Computer crashes, hybrid failure model, computer network security, Consensus, Byzantine faulty clients, fault tolerant computing, Byzantine Fault Tolerance, Paxos]
SybilRes: A Sybil-resilient Flow-Based Decentralized Reputation Mechanism
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Due to the possibility of cheap identity creation, decentralized online reputation mechanisms are susceptible to sybil attacks. Barter Cast is a reputation mechanism used in the Internet-deployed Tribler file-sharing client. In this paper we study the opportunities for sybil attacks in Barter Cast and we devise a method for making Barter Cast sybil resilient, which is incorporated in a protocol called Sybil Res. Like in Barter Cast, in Sybil Res each peer maintains a local subjective weighted directed graph reflecting data transfer actions in Tribler, from which it computes the reputations of other peers using a flow based algorithm taking the edge weights as flows. In Sybil Res, after an upload action, the uploading peer discounts the weights of the edges on the paths from the down loader to itself. As a consequence, due to the way reputations are computed, the reputation of a peer performing a sybil attack decreases fast. To mitigate the negative impact of edge weight discounting on the reputations of honest peers, after a download action, the downloading peer increases the weights of the edges on the paths from the up loader to itself. We demonstrate that Sybil Res is effective in practice by means of trace-driven simulations using data collected from the Tribler network. The results show that Sybil Res effectively marginalizes attackers while having a minimal effect on the reputations of honest peers.
[Measurement, honest peers, Protocols, Sybil Attack, subjective weighted directed graph, Distributed Systems, BarterCast sybil resilient, downloading peer, Robustness, peer-to-peer computing, Peer to peer computing, Social network services, Buildings, Reputation Systems, Tribler network, sybil attacks, SybilRes, Mechanical factors, cheap identity creation, Internet-deployed tribler file-sharing client, flow based algorithm, trace-driven simulations, electronic data interchange, security of data, decentralized online reputation mechanisms, sybil-resilient flow-based decentralized reputation mechanism, data transfer, Internet, uploading peer, edge weight discounting]
Privacy Preserving Group Ranking
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Group ranking is a necessary process used to find the best participant from a group. Group ranking has many applications, including online marketing, personal interests matching and proposal ranking. In an online virtual environment, participants want to do group ranking without leaking any of their private information. In this work, we generalize this scenario as a privacy preserving group ranking problem and formulate the privacy requirements of this problem. We propose a fully distributed privacy preserving group ranking framework and prove its security in the honest but curious model. The core of our framework is a novel multiparty sorting protocol, which guarantees that an adversary cannot link the private information to its owner's identity as long as the owner's final ranking is hidden from the adversary. Our protocol is efficient in computational overhead and communication rounds compared to existing works, as demonstrated by our analysis and simulation.
[Privacy, Protocols, secure multiparty computing, group ranking, Companies, Vectors, anonymity, Encryption, distributed protocol, Sorting]
PAAS: A Privacy-Preserving Attribute-Based Authentication System for eHealth Networks
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Recently, eHealth systems have replaced paper based medical system due to its prominent features of convenience and accuracy. Also, since the medical data can be stored on any kind of digital devices, people can easily obtain medical services at any time and any place. However, privacy concern over patient medical data draws an increasing attention. In the current eHealth networks, patients are assigned multiple attributes which directly reflect their symptoms, undergoing treatments, etc. Those life-threatened attributes need to be verified by an authorized medical facilities, such as hospitals and clinics. When there is a need for medical services, patients have to be authenticated by showing their identities and the corresponding attributes in order to take appropriate healthcare actions. However, directly disclosing those attributes for verification may expose real identities. Therefore, existing eHealth systems fail to preserve patients' private attribute information while maintaining original functionalities of medical services. To solve this dilemma, we propose a framework called PAAS which leverages users' verifiable attributes to authenticate users in eHealth systems while preserving their privacy issues. In our system, instead of letting centralized infrastructures take care of authentication, our scheme only involves two end users. We also offer authentication strategies with progressive privacy requirements among patients or between patients and physicians. Based on the security and efficiency analysis, we show our framework is better than existing eHealth systems in terms of privacy preservation and practicality.
[privacy preservation, non-interactive witness-indistinguishable, eHealth networks, centralized infrastructures, healthcare action, Privacy, patient medical data, privacy concern, medical system, Cryptography, health care, authorized medical facilities, efficiency analysis, medical services, privacy-preserving attribute-based authentication system, Diseases, Hospitals, patient private attribute information, homomorphic encryption, Authentication, message authentication, data privacy, non-interactive zero-knowledge proof, medical computing, eHealth systems]
Robust Overlays for Privacy-Preserving Data Dissemination over a Social Graph
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
A number of recently proposed systems provide secure and privacy-preserving data dissemination by leveraging pre-existing social trust relations and effectively mapping them into communication links. However, as we show in this paper, the underlying trust graph may not be optimal as a communication overlay. It has relatively long path lengths and it can be easily partitioned in scenarios where users are unavailable for a fraction of time. Following this observation, we present a method for improving the robustness of trust-based overlays. Essentially, we start with an overlay derived from the trust graph and evolve it in a privacy-preserving fashion into one that lends itself to data dissemination. The experimental evaluation shows that our approach leads to overlays that are significantly more robust under churn, and exhibit lower path lengths than the underlying trust graph.
[Protocols, Peer to peer computing, graph theory, trust-based overlays, social graph, online social networks, Observers, Maintenance engineering, communication links, privacy, robust overlays, Relays, peer-to-peer, Privacy, secure data dissemination, security of data, privacy-preserving data dissemination, trust graph, social networking (online), Robustness, data privacy, communication overlay, trusted computing, social trust relations]
Optimal Distributed Data Collection for Asynchronous Cognitive Radio Networks
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
As a promising communication paradigm, Cognitive Radio Networks (CRNs) have paved a road for Secondary Users (SUs) to opportunistically exploit unused licensed spectrum without causing unacceptable interference to Primary Users (PUs). In this paper, we study the distributed data collection problem for asynchronous CRNs, which has not been addressed before. First, we study the Proper Carrier-sensing Range (PCR) for SUs. By working with this PCR, an SU can successfully conduct data transmission without disturbing the activities of PUs and other SUs. Subsequently, based on the PCR, we propose an Asynchronous Distributed Data Collection (ADDC) algorithm with fairness consideration for CRNs. ADDC collects data of a snapshot to the base station in a distributed manner without any time synchronization requirement. The algorithm is scalable and more practical compared with centralized and synchronized algorithms. Through comprehensive theoretical analysis, we show that ADDC is order-optimal in terms of delay and capacity, as long as an SU has a positive probability to access the spectrum. Finally, extensive simulation results indicate that ADDC can effectively finish a data collection task and significantly reduce data collection delay.
[unused licensed spectrum, asynchronous distributed data collection algorithm, Delay, cognitive radio networks, capacity, asynchronous cognitive radio network, cognitive radio, Distributed databases, Sensors, Data communication, secondary users, base station, fairness consideration, Base stations, proper carrier-sensing range, Interference, Synchronization, data collection, optimal distributed data collection, distributed algorithm, delay, primary user, data transmission, communication paradigm, distributed data collection problem, asynchronous wireless network]
Game Theoretic Analysis of Distributed Spectrum Sharing with Database
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
According to FCC's ruling for white-space spectrum access, white-space devices are required to query a database to determine the spectrum availability. In this paper, we adopt a game theoretic approach for the database-assisted white-space access point (AP) network design. We first model the channel selection problem among the APs as a distributed AP channel selection game, and design a distributed AP channel selection algorithm that achieves a Nash equilibrium. We then propose a state-based game formulation for the distributed AP association problem of the secondary users by taking the cost of mobility into account. We show that the state-based distributed AP association game has the finite improvement property, and design a distributed AP association algorithm can converge to a state-based Nash equilibrium. Numerical results show that the algorithm is robust to the perturbation by secondary users' dynamical leaving and entering the system.
[Algorithm design and analysis, TV, database, Databases, Heuristic algorithms, Games, white space, game theory, Nash equilibrium, Manganese, distributed spectrum sharing]
Byte Caching in Wireless Networks
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
The explosion of data consumption has led to a renewed interest in byte caching. With studies showing potential reductions in network traffic of 50%, this fine grained caching technique looks like a very good and attractive solution for mobile wireless operators. However, properties of wireless networks actually present new challenges. We first show that a single packet loss, re-ordering or corruption -- all common conditions over the air interface -- can result in circular dependencies and cause existing byte caching algorithms to loop endlessly. To remedy the problem, we then explore a new set of encoding algorithms. Third, we assess the impact of packet losses on byte caching performances, both in terms of byte savings and delay reduction. We found that a mere 1% packet loss can already nullify any delay reduction and instead cause significant increases that users may not be willing to tolerate. Finally, we shared several insights, including interactions between transport layer protocol's mechanisms (e.g., TCP window congestion) and byte caching operations that can cause sophisticated encoding algorithms to perform poorly. We believe that these insights are important for designing more efficient and robust byte caching encoding algorithms.
[Redundancy, Logic gates, Encoding, Decoding, IP networks, Servers, Payloads]
Failure Detectors in Homonymous Distributed Systems (with an Application to Consensus)
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
This paper is on homonymous distributed systems where processes are prone to crash failures and have no initial knowledge of the system membership (&#x201C;homonymous&#x201D; means that several processes may have the same identifier). New classes of failure detectors suited to these systems are first defined. Among them, the classes H&#x03A9; and H&#x03A3; are introduced that are the homonymous counterparts of the classes &#x03A9; and &#x03A3;, respectively. (Recall that the pair &#x2329;&#x03A9;, &#x03A3;&#x232A; defines the weakest failure detector to solve consensus.) Then, the paper shows how H&#x03A9; and H&#x03A3; can be implemented in homonymous systems without membership knowledge (under different synchrony requirements). Finally, two algorithms are presented that use these failure detectors to solve consensus in homonymous asynchronous systems where there is no initial knowledge of the membership. One algorithm solves consensus with &#x2329;H&#x03A9;, H&#x03A3;&#x232A;, while the other uses only H&#x03A9;, but needs a majority of correct processes. Observe that the systems with unique identifiers and anonymous systems are extreme cases of homonymous systems from which follows that all these results also apply to these systems. Interestingly, the new failure detector class H&#x03A9; can be implemented with partial synchrony, while the analogous class A&#x03A9; defined for anonymous systems can not be implemented (even in synchronous systems). Hence, the paper provides us with the first proof showing that consensus can be solved in anonymous systems with only partial synchrony (and a majority of correct processes).
[process failure, failure detectors, crash failure, Asynchrony, Distributed computing, system recovery, Homonymous system, failure detector, Detectors, Safety, crash-prone message-passing distributed system, Face, synchrony requirement, Context, homonymous distributed system, message passing, Process crash, Nominations and elections, membership knowledge, homonymous asynchronous system, Computer crashes, anonymous systems, partial synchrony, Consensus, consensus, Agreement problem, Message-passing, fault tolerant computing, Distributed computability, unique identifiers, system membership]
PREPARE: Predictive Performance Anomaly Prevention for Virtualized Cloud Systems
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Virtualized cloud systems are prone to performance anomalies due to various reasons such as resource contentions, software bugs, and hardware failures. In this paper, we present a novel Predictive Performance Anomaly Prevention (PREPARE) system that provides automatic performance anomaly prevention for virtualized cloud computing infrastructures. PREPARE integrates online anomaly prediction, learning-based cause inference, and predictive prevention actuation to minimize the performance anomaly penalty without human intervention. We have implemented PREPARE on top of the Xen platform and tested it on the NCSU's Virtual Computing Lab using a commercial data stream processing system (IBM System S) and an online auction benchmark (RUBiS). The experimental results show that PREPARE can effectively prevent performance anomalies while imposing low overhead to the cloud infrastructure.
[Measurement, Cloud computing, virtual reality, RUBiS, Predictive models, data stream processing system, virtualized cloud systems, human intervention, Benchmark testing, cloud computing, learning (artificial intelligence), online auction benchmark, Monitoring, learning-based cause inference, virtual computing lab, program diagnostics, virtualized cloud computing infrastructures, IBM System S, inference mechanisms, predictive prevention actuation, Bayesian methods, predictive performance anomaly prevention, PREPARE, Markov processes, performance anomaly prevention, online anomaly prediction]
Optimal Recovery from Large-Scale Failures in IP Networks
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Quickly recovering IP networks from failures is critical to enhancing Internet robustness and availability. Due to their serious impact on network routing, large-scale failures have received increasing attention in recent years. We propose an approach called Reactive Two-phase Rerouting (RTR) for intra-domain routing to quickly recover from large-scale failures with the shortest recovery paths. To recover a failed routing path, RTR first forwards packets around the failure area to collect information on failures. Then, in the second phase, RTR calculates a new shortest path and forwards packets along it through source routing. RTR can deal with large-scale failures associated with areas of any shape and location, and is free of permanent loops. For any failure area, the recovery paths provided by RTR are guaranteed to be the shortest. Extensive simulations based on ISP topologies show that RTR can find the shortest recovery paths for more than 98.6% of failed routing paths with reachable destinations. Compared with prior works, RTR achieves better performance for recoverable failed routing paths and uses much less network resources for irrecoverable failed routing paths.
[Network topology, Routing, Routing protocols, Topology, Internet, IP networks, Convergence]
Spammer Behavior Analysis and Detection in User Generated Content on Social Networks
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Spam content is surging with an explosive increase of user generated content (UGC) on the Internet. Spammers often insert popular keywords or simply copy and paste recent articles from the Web with spam links inserted, attempting to disable content-based detection. In order to effectively detect spam in user generated content, we first conduct a comprehensive analysis of spamming activities on a large commercial UGC site in 325 days covering over 6 million posts and nearly 400 thousand users. Our analysis shows that UGC spammers exhibit unique non-textual patterns, such as posting activities, advertised spam link metrics, and spam hosting behaviors. Based on these non-textual features, we show via several classification methods that a high detection rate could be achieved offline. These results further motivate us to develop a runtime scheme, BARS, to detect spam posts based on these spamming patterns. The experimental results demonstrate the effectiveness and robustness of BARS.
[advertised spam link metrics, user generated content, spam links, classification methods, Blogs, Unsolicited electronic mail, social networks, unique nontextual patterns, content-based detection, unsolicited e-mail, spammer behavior analysis, spammer behavior detection, classification, spam content, UGC, runtime scheme, Runtime, spam hosting behaviors, Feature extraction, social networking (online), Software, Internet, BARS, Bars]
Securing Virtual Coordinates by Enforcing Physical Laws
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Virtual coordinate systems (VCS) provide accurate estimations of latency between arbitrary hosts on a network, while conducting a small amount of actual measurements and relying on node cooperation. While these systems have good accuracy under benign settings, they suffer a severe decrease of their effectiveness when under attack by compromised nodes acting as insider attackers. Previous defenses mitigate such attacks by using machine learning techniques to differentiate good behavior (learned over time) from bad behavior. However, these defense schemes have been shown to be vulnerable to advanced attacks that make the schemes learn malicious behavior as good behavior. We present Newton, a decentralized VCS that is robust to a wide class of insider attacks. Newton uses an abstraction of a real-life physical system, similar to that of Vivaldi, but in addition uses safety invariants derived from Newton's laws of motion. As a result, Newton does not need to learn good behavior and can tolerate a significantly higher percentage of malicious nodes. We show through simulations and real-world experiments on the Planet Lab test bed that Newton is able to mitigate all known attacks against VCS while providing better accuracy than Vivaldi, even in benign settings.
[node cooperation, real-life physical system abstraction, Force, virtual coordinate system, insider attack, Vectors, Newton laws of motion, physical laws, Planet Lab test bed, Oscillators, Accuracy, Coordinate measuring machines, security of data, Safety, decentralized VCS, Springs, safety invariant, latency estimation]
Octopus: A Secure and Anonymous DHT Lookup
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Distributed Hash Table (DHT) lookup is a core technique in structured peer-to-peer (P2P) networks. Its decentralized nature introduces security and privacy vulnerabilities for applications built on top of them, we thus set out to design a lookup mechanism achieving both security and anonymity, heretofore an open problem. We present the design of Octopus, which uses attacker identification mechanisms to discover and remove malicious nodes, severely limiting an adversary's ability to carry out active attacks, and splits lookup queries over separate anonymous paths and introduces dummy queries to achieve high levels of anonymity. We analyze the security of Octopus by developing an event-based simulator to show that the attacker discovery mechanisms can rapidly identify malicious nodes with low error rate. We calculate the anonymity of Octopus using probabilistic modeling and show that Octopus can achieve near-optimal anonymity. We evaluate Octopus's efficiency on Planet lab and show that Octopus has reasonable lookup latency and low bandwidth overhead.
[Octopus anonymity, decentralized nature, anonymous DHT lookup, Octopus security, probabilistic modeling, Security, secure DHT lookup, privacy vulnerability, P2P networks, malicious nodes, structured peer-to-peer networks, query processing, table lookup, DHT, Pollution, Accuracy, adversary ability, near-optimal anonymity, security, attacker discovery mechanisms, Fingers, dummy query, attacker identification mechanisms, distributed hash table lookup, lookup query, reasonable lookup latency, low error rate, error statistics, Octopus efficiency, lookup, peer-to-peer computing, Peer to peer computing, probability, Anonymity, Routing, bandwidth overhead, security of data, Surveillance, Planet lab, data privacy, Octopus design, core technique, security vulnerability, lookup mechanism, event-based simulator]
Total Order in Content-Based Publish/Subscribe Systems
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Total ordering is a messaging guarantee increasingly required of content-based pub/sub systems, which are traditionally focused on performance. The main challenge is the uniform ordering of streams of publications from multiple publishers within an overlay broker network to be delivered to multiple subscribers. Our solution integrates total ordering into the pub/sub logic instead of offloading it as an external service. We show that our solution is fully distributed and relies only on local broker knowledge and overlay links. We can identify and isolate specific publications and subscribers where synchronization is required: the overhead is therefore contained to the affected subscribers. Our solution remains safe under the presence of failure, where we show total order to be impossible to maintain. Our experiments demonstrate that our solution scales with the number of subscriptions and has limited overhead for the non-conflicting cases. A holistic comparison with group communication systems is offered to evaluate their relative scalability.
[Algorithm design and analysis, publish/subscribe, total ordering, uniform ordering, Protocols, content-based pub/sub systems, Subscriptions, total order, Topology, content-based publish/subscribe systems, overlay links, quality of service, local broker knowledge, content-based, external service, Semantics, messaging guarantee, Safety, Monitoring, middleware]
Publiy+: A Peer-Assisted Publish/Subscribe Service for Timely Dissemination of Bulk Content
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Publish/Subscribe (P/S) systems and file sharing applications traditionally share the common goal of disseminating data among large populations of users. Despite this similarity, the former focuses on timely dissemination of small-sized notification messages, while the latter presumes larger types of bulk content with less emphasis on the time needed between release and delivery of data. In this paper, we develop a peer-assisted content dissemination mechanism to bridge this gap by adopting the P/S model. We propose a hybrid two-layer architecture in which P/S brokers act as coordinators and guide their clients with interest in similar content to engage in direct exchange of data blocks in a peer-to-peer and cooperative fashion. Furthermore, we use network coding in order to facilitate data exchange among clients. Our peer-assisted scheme offloads the burden of disseminating huge volumes of data from P/S brokers to subscribers themselves. As an added advantage of our approach, brokers employ strategies that help shape traffic flows in multi-domain network settings. Finally, we have implemented our approach and carried out extensive large-scale experimental evaluation on a cluster with aggregate data transfers of up to 1 TB and involving up to 1000 subscribers. Our results demonstrate good scalability and faster content delivery compared to file sharing protocols such as BitTorrent.
[publish/subscribe, network coding, message passing, peer-to-peer computing, data exchange, Subscriptions, Routing, data dissemination, Encoding, Servers, peer-assisted content dissemination mechanism, Content Dissemination, multidomain network, peer-assisted publish-subscribe service, Network Coding, Bandwidth, Network coding, Internet, traffic flows, Publiy, bulk content dissemination, middleware, file sharing]
G-COPSS: A Content Centric Communication Infrastructure for Gaming Applications
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Information-Centric Networking provides substantial flexibility for users to obtain information without knowing the source of information or its current location. With users increasingly focused on an online world, an emerging challenge for the network infrastructure is to support Massively Multiplayer Online Role Playing Game (MMORPG). Currently, MMORPG is built on IP infrastructure with the primary responsibility resting on servers for disseminating control messages and predicting/retrieving objects belonging to each player's view. Scale and timeliness are major challenges of such a server-oriented gaming architecture. Limited server resources significantly impair the user's interactive experience, requiring game implementations to limit the number of players in a single game instance. We propose Gaming over COPSS (G-COPSS), a distributed communication infrastructure using a Content-Oriented Pub/Sub System (COPSS) to enable efficient decentralized information dissemination in MMORPG, jointly exploiting the network and end-systems for player management and information dissemination. G-COPSS aims to scale well in the number of players in a single game, while still meeting users' response time requirements. We have implemented G-COPSS on top of the open-source CCNx implementation. We use a simple game with a hierarchical map to carefully micro benchmark the implementation and the processing involved in managing game dynamics. We have also micro benchmarked the game based on NDN and a server with an IP infrastructure. We emulate an application that is particularly emblematic of MMORPG -- Counter-Strike -- but one in which all players share a hierarchical structured map. Using trace-driven simulation, we demonstrate that G-COPSS can achieve high scalability and tight timeliness requirements of MMORPG. The simulator is parameterized based on micro benchmarks of our implementation. Our evaluations show that G-COPSS provides orders of magnitude improvement in update latency and a factor of two reduction in aggregate network load compared to a server-based implementation.
[Subscriptions, Games, Servers, Face, IP networks, Engines, Optimization]
FMTCP: A Fountain Code-Based Multipath Transmission Control Protocol
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Ideally, the throughput of a Multipath TCP (MPTCP) connection should be as high as that of multiple disjoint single-path TCP flows. In reality, the throughput of MPTCP is far lower than expected. This is fundamentally caused by the fact that a sub flow with high delay and loss affects the performance of other sub flows, and thus becomes the bottleneck of the MPTCP connection and significantly degrades the aggregate good put. To tackle this problem, we propose Fountain code-based Multipath TCP (FMTCP), which effectively mitigates the negative impact of the heterogeneity of different paths. FMTCP takes advantage of the random nature of the fountain code to flexibly transmit encoded symbols from the same or different data blocks over different sub flows. Moreover, we design a data allocation algorithm based on the expected packet arriving time and decoding demand to coordinate the transmissions of different sub flows. Quantitative analyses are provided to show the benefit of FMTCP. We also evaluate the performance of FMTCP through ns-2 simulations and demonstrate that FMTCP can outperform IETF-MPTCP, a typical MPTCP approach, when the paths have diverse loss and delay in terms of higher total good put, lower delay and jitter. In addition, FMTCP achieves much more stable performance under abrupt changes of path quality.
[MPTCP connection, packet arriving time, codes, Protocols, FMTCP, Receivers, Throughput, Encoding, Decoding, transmission control protocol, multiple disjoint single-path TCP flows, decoding demand, decoding, quantitative analyses, jitter, delay, transport protocols, data allocation algorithm, Propagation losses, Resource management, fountain code-based multipath TCP]
Enforcing High-Performance Operation of Multi-hop Wireless Networks with MIMO Relays
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
In multi-hop wireless networks where links are prone to be broken or degraded, it is important to guarantee the network connectivity as well as satisfy the performance requirements. Observing the promising features of Multiple-Input Multiple-Output (MIMO) techniques for improving the transmission capacity and reliability, in this paper, we make the very first attempt to deploy MIMO nodes as relays to assist weak links in wireless networks, with the aim of reducing the number of relay nodes and providing performance provisioning. We identify the specific constraints of MIMO relay nodes for assisting weak links, and take advantage of the MIMO ability to flexibly select among different transmission strategies. The constrains and flexibility, however, make the MIMO deployment problem different from conventional single-antenna deployment schemes and much more challenging. Based on the constraints, we formulate the MIMO relay deployment problem, and provide a polynomial-time approximation scheme (PTAS) algorithm, as well as a distributed heuristic algorithm. The performance of the proposed algorithms is evaluated through simulations and demonstrated to be very effective.
[Multiplexing, approximation theory, radio networks, polynomial-time approximation scheme, reliability, Downlink, enforcing high-performance operation, multihop wireless networks, transmission capacity, Relays, MIMO deployment problem, Wireless sensor networks, Wireless networks, multiple-input multiple-output techniques, network connectivity, Spread spectrum communication, single-antenna deployment, distributed heuristic algorithm, MIMO, MIMO relays, MIMO relay deployment problem, MIMO communication, weak links]
Receiver Consensus: On-time Warning Delivery for Vehicular Ad-hoc Networks
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
To improve safety, a warning message in VANETs should be delivered both reliably and urgently. Existing solutions either tend to compromise propagation delay or do not reach high reliability due to broadcast storm problem caused by excessive retransmissions. We propose ReC, which exploits geographical information to help nodes autonomously achieve agreement on forwarding strategies. Each forwarding candidate ranks itself and its neighbors (who affirmatively or potentially received the message already) by distance to the centroid of neighbors in need of message, to assign different priority in forwarding among neighboring nodes and remarkably suppress unnecessary retransmission, while enabling best nodes to transmit the packet without waiting. The effectiveness and efficiency of this method is validated through extensive simulations under 802.11p settings. The results demonstrate that the proposed protocol achieves the high reliability of leading state-of-the-art solutions, while at the same time significantly enhances timeliness, dedicating itself to disseminating emergency messages in 2D vehicular networks.
[Protocols, 2D vehicular network, propagation delay, Roads, reliability, neighboring node, emergency message dissemination, Relays, Delay, forwarding strategy, Vehicles, ReC, protocol, safety, telecommunication network reliability, geographical information, protocols, packet transmission, vehicular ad hoc networks, Receivers, excessive retransmission, VANET, warning message, on-time warning delivery, radio receivers, radiowave propagation, vehicular ad-hoc network, 802.11p setting, receiver consensus, broadcast storm problem, Reliability]
Joint Optimization of Computing and Cooling Energy: Analytic Model and a Machine Room Case Study
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Total energy minimization in data centers (including both computing and cooling energy) requires modeling the interactions between computing decisions (such as load distribution) and heat transfer in the room, since load acts as heat sources whose distribution in space affects cooling energy. This paper presents the first closed-form analytic optimal solution for load distribution in a machine rack that minimizes the sum of computing and cooling energy. We show that by considering actuation knobs on both computing and cooling sides, it is possible to reduce energy cost comparing to state of the art solutions that do not offer holistic energy optimization. The above can be achieved while meeting both throughput requirements and maximum CPU temperature constraints. Using a thorough evaluation on a real test bed of 20 machines, we demonstrate that our simple model adequately captures the thermal behavior and energy consumption of the system. We further show that our approach saves more energy compared to the state of the art in the field.
[joint optimization, thermal behavior, data centers, machine room case study, thorough evaluation, computing decisions, machine rack, Servers, throughput requirements, closed-form analytic optimal solution, energy optimization, space cooling, heat sources, testbed, cooling energy, analytic model, Mathematical model, energy consumption, Load modeling, cost reduction, Cooling, practical, Computational modeling, load distribution, computing energy, holistic energy optimization, CPU temperature constraints, computer centres, Equations, data-center, energy cost reduction, optimal load allocation, Heating, actuation knobs, minimisation, space distribution, heat transfer, energy minimization]
v-Bundle: Flexible Group Resource Offerings in Clouds
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Traditional Infrastructure-as-a-Service offerings provide customers with large numbers of fixed-size virtual machine (VM) instances with resource allocations that are designed to meet application demands. With application demands varying over time, cloud providers gain efficiencies through resource consolidation and over-commitment. For cloud customers, however, this leads to inefficient use of the cloud resources they have purchased. To address cloud customers' dynamic application requirements, we present a new cloud resource offering, called v-Bundle, which makes flexible the exchange of resource capacity among multiple VM instances belonging to the same customer. Specifically targeting network resources, for each customer application, we first use DHT-based techniques to achieve an initial VM placement that minimizes its use of the data center network's bi-section bandwidth. When VMs' networking requirements change, the customer can then use v-Bundle to trade the networking resources allocated to her application. v-Bundle maintains information about network resources with any-cast tree-based methods implemented as extensions of the Pastry pub-sub core. Experimental evaluations show that the approach can scale well to thousands of hosts and VMs, and that v-Bundle can provide customers with better bandwidth utilization and improved application quality of service through borrowing extra bandwidth when needed, at no additional cost in terms of the total resources allocated to the customer.
[cloud resources, Switches, clouds, bi-section bandwidth, pastry overlay, Servers, resource allocations, DHT-based techniques, Pastry pub-sub core, resource allocation, Infrastructure-as-a-Service, cloud customers, Bandwidth, v-bundle, cloud computing, customer application, v-Bundle, Peer to peer computing, virtualization, flexible group resource offerings, Receivers, resource consolidation, Routing, quality of service, cloud resource offering, cloud customer dynamic application requirements, virtual machines, any-cast tree-based methods, network resources, Resource management, fixed-size virtual machine]
Dynamic Control of Electricity Cost with Power Demand Smoothing and Peak Shaving for Distributed Internet Data Centers
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Internet based service providers, such as Amazon, Google, Yahoo etc, build their data centers (IDC) across multiple regions to provide reliable and low latency of services to clients. Ever-increasing service demand, complexity of services and growing client population cause enormous power consumptions by these IDCs incurring a major part of their running costs. Modern electric power grid provides a feasible way to dynamically and efficiently manage the electricity cost of distributed IDCs based on the Locational Marginal Pricing (LMP) policy. While recent works exploit LMP by electricity-price based geographic load distribution, the dynamic workload and high volatility of electricity prices induce highly volatile power demand and critical power peak problem. The benefit of cost minimization via geographic load distribution is counterbalanced with the high cost incurred by violating the peak power. In this paper, we study the dynamic control of electricity cost to provide low volatility in power demand and shaving of power peaks. To this end, a Model Predictive Control (MPC) electricity cost minimization problem is formulated based on a time-continuous differential model. The proposed solution minimizes electricity costs, provides low variation in power demand by penalizing the change in workload and alleviates the power peaks by tracking the available power budget. By providing extensive simulation results based on real-life electricity price traces we show the effectiveness of our approach.
[Power demand, Electricity, Power system dynamics, Predictive models, Internet, Servers, Portals]
Towards Optimal Capacity Segmentation with Hybrid Cloud Pricing
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Cloud resources are usually priced in multiple markets with different service guarantees. For example, Amazon EC2 prices virtual instances under three pricing schemes -- the subscription option (a.k.a., Reserved Instances), the pay-as-you-go offer (a.k.a., On-Demand Instances), and an auction-like spot market (a.k.a., Spot Instances) -- simultaneously. There arises a new problem of capacity segmentation: how can a provider allocate resources to different categories of pricing schemes, so that the total revenue is maximized? In this paper, we consider an EC2-like pricing scheme with traditional pay-as-you-go pricing augmented by an auction market, where bidders periodically bid for resources and can use the instances for as long as they wish, until the clearing price exceeds their bids. We show that optimal periodic auctions must follow the design of m+1-price auction with seller's reservation price. Theoretical analysis also suggests the connections between periodic auctions and EC2 spot market. Furthermore, we formulate the optimal capacity segmentation strategy as a Markov decision process over some demand prediction window. To mitigate the high computational complexity of the conventional dynamic programming solution, we develop a near-optimal solution that has significantly lower complexity and is shown to asymptotically approach the optimal revenue.
[multiple markets, Amazon EC2, Subscriptions, virtual instances, cloud resources, auction-like spot market, optimal capacity segmentation strategy, Optimization, hybrid cloud pricing, subscription option, resource allocation, pricing schemes, Pricing, reserved instances, cloud computing, service guarantees, demand prediction window, Economics, Computational modeling, spot instances, dynamic programming, pay-as-you-go offer, on-demand instances, Markov processes, Markov decision process, Resource management, pricing, computational complexity]
Dash: A Novel Search Engine for Database-Generated Dynamic Web Pages
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Database-generated dynamic web pages (db-pages, in short), whose contents are created on the fly by web applications and databases, are now prominent in the web. However, many of them cannot be searched by existing search engines. Accordingly, we develop a novel search engine named Dash, which stands for Db-pAge Search, to support db-page search. Dash determines db-pages possibly generated by a target web application and its database through exploring the application code and the related database content and supports keyword search on those db-pages. In this paper, we present its system design and focus on the efficiency issue. To minimize costs incurred for collecting, maintaining, indexing and searching a massive number of db-pages that possibly have overlapped contents, Dash derives and indexes db-page fragments in place of db-pages. Each db-page fragment carries a disjointed part of a db-page. To efficiently compute and index db-page fragments from huge datasets, Dash is equipped with MapReduce based algorithms for database crawling and db-page fragment indexing. Besides, Dash has a top-k search algorithm that can efficiently assemble db-page fragments into db-pages relevant to search keywords and return the k most relevant ones. The performance of Dash is evaluated via extensive experimentation.
[search engines, db-page search, database-generated dynamic Web page, searching, database management systems, keyword search, MapReduce, database crawling, top-k search algorithm, Dash, database content, MapReduce based algorithm, Top-k Search, db-page fragment indexing, Search engines, Search Engine, Hadoop and Performance, cost minimization, search problems, indexing, search engine, Keyword search, information retrieval, system design, Educational institutions, huge dataset, Database Crawling, application code, Web pages, Web application, Internet, Database-Generated Dynamic Web Pages, Indexing]
MOVE: A Large Scale Keyword-Based Content Filtering and Dissemination System
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
The Web 2.0 era is characterized by the emergence of a very large amount of live content. A real time and fine grained content filtering approach can precisely keep users up-to-date the information that they are interested. The key of the approach is to offer a scalable match algorithm. One might treat the content match as a special kind of content search, and resort to the classic algorithm [5]. However, due to blind flooding, [5] cannot be simply adapted for scalable content match. To increase the throughput of scalable match, we propose an adaptive approach to allocate (i.e, replicate and partition) filters. The allocation is based on our observation on real datasets: most users prefer to use short queries, consisting of around 2-3 terms per query, and web content typically contains tens and even thousands of terms per article. Thus, by reducing the number of processed documents, we can reduce the latency of matching large articles with filters, and have chance to achieve higher throughput. We implement our approach on an open source project, Apache Cassandra. The experiment with real datasets shows that our approach can achieve around folds of better throughput than two counterpart state-of-the-arts solutions.
[dissemination system, Web 2.0, information dissemination, Throughput, Apache Cassandra, information filtering, Registers, large scale keyword-based content filtering, Indexes, Web content, Equations, Optimization, scalable match algorithm, MOVE, Clustering algorithms, Internet, Resource management]
When Scalability Meets Consistency: Genuine Multiversion Update-Serializable Partial Data Replication
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
In this article we introduce GMU, a genuine partial replication protocol for transactional systems, which exploits an innovative, highly scalable, distributed multiversioning scheme. Unlike existing multiversion-based solutions, GMU does not rely on a global logical clock, which represents a contention point and can limit system scalability. Also, GMU never aborts read-only transactions and spares them from distributed validation schemes. This makes GMU particularly efficient in presence of read-intensive workloads, as typical of a wide range of real-world applications. GMU guarantees the Extended Update Serializability (EUS) isolation level. This consistency criterion is particularly attractive as it is sufficiently strong to ensure correctness even for very demanding applications (such as TPC-C), but is also weak enough to allow efficient and scalable implementations, such as GMU. Further, unlike several relaxed consistency models proposed in literature, EUS has simple and intuitive semantics, thus being an attractive, scalable consistency model for ordinary programmers. We integrated the GMU protocol in a popular open source in-memory transactional data grid, namely Infinispan. On the basis of a large scale experimental study performed on heterogeneous experimental platforms and using industry standard benchmarks (namely TPC-C and YCSB), we show that GMU achieves linear scalability and that it introduces negligible overheads (less than 10%), with respect to solutions ensuring non-serializable semantics, in a wide range of workloads.
[transaction processing, Protocols, grid computing, transactional system, Proposals, History, consistency, scalability, Semantics, extended update serializability isolation level, Distributed databases, Partial Data Replication, industry standard benchmark, Multiversioning, GMU protocol, genuine multiversion update-serializable partial data replication, genuine partial replication protocol, configuration management, YCSB, Transactional Systems, heterogeneous experimental platform, open source in-memory transactional data grid, distributed multiversioning scheme, read-only transaction, TPC-C, Data models, Infinispan, Clocks, Fault Tolerance]
Clustering Streaming Graphs
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
In this paper, we propose techniques for clustering large-scale "streaming" graphs where the updates to a graph are given in form of a stream of vertex or edge additions and deletions. Our algorithm handles such updates in an online and incremental manner and it can be easily parallel zed. Several previous graph clustering algorithms fall short of handling massive and streaming graphs because they are centralized, they need to know the entire graph beforehand and are not incremental, or they incur an excessive computational overhead. Our algorithm's fundamental building block is called graph reservoir sampling. We maintain a reservoir sample of the edges as the graph changes while satisfying certain desired properties like bounding number of clusters or cluster-sizes. We then declare connected components in the sampled sub graph as clusters of the original graph. Our experiments on real graphs show that our approach not only yields clusterings with very good quality, but also obtains orders of magnitude higher throughput, when compared to offline algorithms.
[graph reservoir sampling, Image edge detection, Heuristic algorithms, fundamental building block, graph theory, large-scale streaming graphs clustering, Throughput, Partitioning algorithms, Complexity theory, graph clustering algorithm, graph, computational overhead, streaming, pattern clustering, Clustering algorithms, Reservoirs, online, clustering]
Competitive Self-Stabilizing k-Clustering
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
In this paper, we propose a silent self-stabilizing asynchronous distributed algorithm for constructing a kclustering of any connected network with unique IDs. Our algorithm stabilizes in O(n) rounds, using O(log n) space per process, where n is the number of processes. In the general case, our algorithm constructs O(n/k) k-clusters. If the network is a Unit Disk Graph (UDG), then our algorithm is 7.2552k+O(1)competitive, that is, the number of k-clusters constructed by the algorithm is at most 7.2552k + O(1) times the minimum possible number of k-clusters in any k-clustering of the same network. More generally, if the network is an Approximate Disk Graph (ADG) with approximation ratio &#x03BB;, then our algorithm is 7.2552&#x03BB;2k + O(&#x03BB;)-competitive. Our solution is based on the self-stabilizing construction of a data structure called the MIS Tree, a spanning tree of the network whose processes at even levels form a maximal independent set of the network. The MIS tree construction is the time bottleneck of our k-clustering algorithm, as it takes &#x0398;(n) rounds in the worst case, while the rest of the algorithm takes O(D) rounds, where V is the diameter of the network. We would like to improve that time to be O(D), but we show that our distributed MIS tree construction is a P-complete problem.
[Algorithm design and analysis, ADG, k-clustering algorithm, self-stabilization, graph theory, distributed MIS tree construction, data structure, maximal independent set, competitiveness, Complexity theory, k-clustering, approximate disk graph, connected network, self-stabilizing construction, time bottleneck, Clustering algorithms, network spanning tree, UDG, MIS tree, tree data structures, approximation ratio, approximation theory, unit disk graph, network diameter, Data structures, Encoding, self-stabilizing asynchronous distributed algorithm, P-complete problem, pattern clustering, distributed algorithms, competitive self-stabilizing k-clustering, Vegetation, Approximation algorithms, computational complexity]
Neighbor Knowledge of Mobile Nodes in a Road Network
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
A key challenge for wireless networks in which nodes can move is for each node to keep track of its dynamically changing set of nearby nodes (neighbors). We present a solution for nodes to maintain neighbor knowledge where nodes communicate via wireless broadcast and are restricted to move on a two-dimensional road network. A road network is a collection of one-dimensional lines that may intersect each other. For nodes to exchange neighbor information, we construct a deterministic collision-free broadcast schedule which utilizes time division multiplexing and geographical segmentation. Under a certain node density requirement and assuming initial neighbor knowledge, our broadcast schedule tolerates node movement on the road network while providing deterministic guarantees in maintaining neighbor knowledge. We also provide a lower bound on the speed of a message propagation given our broadcast schedule. In addition, we consider grouping nodes into clusters and show that, under certain conditions, neighbor knowledge is maintained when two different clusters move close to each other. Finally, we address the issue of obtaining initial neighbor knowledge.
[Knowledge engineering, Schedules, Roads, vehicular ad-hoc networks, Color, Interference, Ad hoc networks, Trajectory, maintaining neighbor knowledge, distributed computing]
Local Reasoning for Global Convergence of Parameterized Rings
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
This paper presents a method that can generate Self-Stabilizing (SS) parameterized protocols that are generalizable, i.e., correct for arbitrary number of finite-state processes. Specifically, we present necessary and sufficient conditions specified in the local state space of the representative process of parameterized rings for deadlock-freedom in their global state space. Moreover, we introduce sufficient conditions that guarantee live lock-freedom in arbitrary-sized unidirectional rings. We illustrate the proposed approach in the context of several classic examples including a maximal matching protocol and an agreement protocol. More importantly, the proposed method lays the foundation of an approach for automated design of global convergence in the local state space of the representative process.
[finite-state processes, Protocols, Ring topology, Self-Stabilization, global convergence, agreement protocol, Cognition, Topology, global state space, Cost accounting, Convergence, deadlock-freedom, local reasoning, sufficient conditions, Reactive power, maximal matching protocol, necessary conditions, parameterized rings, System recovery, self-stabilizing parameterized protocols, local state space, unidirectional rings, protocols, Parameterized Protocols]
The Power of Lights: Synchronizing Asynchronous Robots Using Visible Bits
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
In this paper we study the power of using lights, i.e. visible external memory, for distributed computation by autonomous robots moving in LookCompute-Move (LCM) cycles. With respect to the LCM cycles, the most common models studied in the literature are the fully-synchronous (FSYNC), the semisynchronous (SSYNC), and the asynchronous (ASYNC). In this paper we introduce in the ASYNC model, the weakest of the three, the availability of visible external memory: each robot is equipped with a light bulb that is visible to all other robots, and that can display a constant numbers of different colors; the colors are persistent, that is they are not automatically reset at the end of each cycle. We first study the relationship between ASYNC with visible bits and SSYNC. We prove hat asynchronous robots, when equipped with a constant number of colors, are strictly more powerful than traditional semisynchronous robots. We also show that, when enhanced with visible lights, the difference between asynchrony and semi-synchrony disappears; this result must be contrasted with the strict dominance ASYNC &lt;;SSYNC between the models without lights. We then study the relationship between ASYNC with visible bits and FSYNC. We prove that asynchronous robots with a constant number of visible bits, if they can remember a single snapshot, are strictly more powerful than fully-synchronous robots. This is to be contrasted with the fact that, without lights, ASYNC robots are not even as powerful as SSYNC, even if they remember an unlimited number of previous snapshots. These results demonstrate the power of using visible external memory for distributed computation with autonomous robots. In particular, asynchrony can be overcome with the power of lights.
[semisynchronous robots, Protocols, Computational modeling, FSYNC model, asynchronous robots, Color, autonomous robots, visible bits, distributed computation, mobile robots, Mobile robots, power of lights, ASYNC model, look-compute-move cycles, Robot kinematics, LCM cycles, Robot sensing systems, SSYNC model]
ADAPT: Availability-Aware MapReduce Data Placement for Non-dedicated Distributed Computing
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
The MapReduce programming paradigm is gaining more and more popularity recently due to its merits of ease of programming, data distribution and fault tolerance. The low barrier of adoption of MapReduce makes it a promising framework for non-dedicated distributed computing environments. However, the variability of hosts resources and availability could substantially degrade the performance of MapReduce applications. The replication-based fault tolerance mechanism helps to alleviate some problems at the cost of inefficient storage space utilization. Intelligent solutions that guarantee the performance of MapReduce applications with low data replication degree are needed to promote the idea of running MapReduce applications in non-dedicated environment at lower costs. In this research, we propose an Availability-aware Data Placement (ADAPT) strategy to improve the application performance without extra storage cost. The basic idea of ADAPT is to dispatch data based on the availability of each node, reduce network traffic, improve data locality, and optimize the application performance. We implement the prototype of ADAPT within the Hadoop framework, an open-source implementation of MapReduce. The performance of ADAPT is evaluated in an emulated non-dedicated distributed environment. The experimental results show that ADAPT can improve the performance by more than 30%. ADAPT achieves high reliability without the need for additional data replication. ADAPT has also been evaluated for large-scale computing environment through simulations, with promising results.
[Adaptation models, Interrupters, data distribution, resource availability, MapReduce adoption, parallel programming, Hadoop framework, MapReduce, storage space utilization, data locality, large-scale computing environment, Distributed databases, network traffic reduction, open-source implementation, node availability, Availability, availability-aware MapReduce data placement, fault tolerance, Computational modeling, replication-based fault tolerance mechanism, storage cost, MapReduce programming paradigm, data replication degree, ADAPT strategy, nondedicated distributed computing, Data models, fault tolerant computing, Reliability, Performance, resource variability]
Dynamic Service Placement in Geographically Distributed Clouds
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Large-scale online service providers have been increasingly relying on geographically distributed cloud infrastructures for service hosting and delivery. In this context, a key challenge faced by service providers is to determine the locations where service applications should be placed such that the hosting cost is minimized while key performance requirements (e.g. response time) are assured. Furthermore, the dynamic nature of both demand pattern and infrastructure cost favors a dynamic solution to this problem. Currently most of the existing solutions for service placement have either ignored dynamics, or provided inadequate solutions that achieve both objectives at the same time. In this paper, we present a framework for dynamic service placement problems based on control- and game-theoretic models. In particular, we present a solution that optimizes the desired objective dynamically over time according to both demand and resource price fluctuations. We further consider the case where multiple service providers compete for resource in a dynamic manner, and show that there is a Nash equilibrium solution which is socially optimal. Using simulations based on realistic topologies, demand and resource prices, we demonstrate the effectiveness of our solution in realistic settings.
[Cloud computing, Adaptation models, Servers, Resource Management, Cloud Computing, service delivery, resource price fluctuations, key performance requirements, Mathematical model, cloud computing, demand pattern, Context, hosting cost, control-theoretic models, dynamic service placement, topology, Nash equilibrium solution, realistic topology, game theory, geographically distributed cloud infrastructures, socially optimal, service hosting, Model Predictive Control, large-scale online service providers, inadequate solutions, demand price fluctuations, ignored dynamics, response time, infrastructure cost, geographically distributed clouds, Data models, Resource management, game-theoretic models]
Attributed-Based Access Control for Multi-authority Systems in Cloud Storage
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Cipher text-Policy Attribute-base Encryption (CP-ABE) is regarded as one of the most suitable technologies for data access control in cloud storage. In almost all existing CP-ABE schemes, it is assumed that there is only one authority in the system responsible for issuing attributes to the users. However, in many applications, there are multiple authorities co-exist in a system and each authority is able to issue attributes independently. In this paper, we design an access control framework for multi-authority systems and propose an efficient and secure multi-authority access control scheme for cloud storage. We first design an efficient multi-authority CP-ABE scheme that does not require a global authority and can support any LSSS access structure. Then, we prove its security in the random oracle model. We also propose a new technique to solve the attribute revocation problem in multi-authority CP-ABE systems. The analysis and simulation results show that our multi-authority access control scheme is scalable and efficient.
[Access control, Cloud computing, LSSS access structure, multiauthority CP-ABE scheme, attribute revocation problem, Attribute Revocation, global authority, multiauthority systems, Encryption, attributed-based access control, Servers, cipher text-policy attribute-base encryption, Cloud Storage, Multi-Authority, authorisation, random oracle model, Access Control, cloud computing, secure multiauthority access control, access control framework, cryptography, data access control, CP-ABE, Public key, cloud storage]
Growing Secure Distributed Systems from a Spore
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
This paper describes the design and evaluation of Spore, a secure cloud-based file system that minimizes trust and functionality assumptions on underlying servers. Spore differs from other systems in that system relationships are formalized only through signed data objects, rather than in complicated protocols executed between clients and servers. This approach allows Spore to bootstrap a file system from a single object, providing integrity and security guarantees while storing all data as simple, immutable objects on untrusted servers. We use simulation to characterize the performance of this system, focusing primarily on the cost incurred in compensating for the minimal server support. We show that while a naive approach is quite inefficient, a series of simple optimizations can enable the system to perform well in real-world scenarios.
[Protocols, Buildings, file system bootstrap, signed data object, secure cloud-based file system, Servers, security guarantee, storage management, Spore evaluation, data storage, security of data, integrity guarantee, Public key, Distributed databases, secure distributed system, server support, Spore system, Spore design, cloud computing]
Achieving Full View Coverage with Randomly-Deployed Heterogeneous Camera Sensors
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
A brand-new concept about the coverage problem of camera sensor networks, full view coverage, has been proposed recently to judge whether an object's face is guaranteed to be captured. It is specially significant for camera networks since image shot at the frontal viewpoint considerably increases the possibility to recognize the object. In this paper, we investigate the necessary and sufficient conditions to achieve full view coverage under two random deployment schemes, uniform deployment and Poisson deployment. In uniform deployment, we define a centralized parameter - critical sensing area (CSA) - to evaluate the total requirements to reach symptotic full view coverage for all heterogeneous sensors in the network. In Poisson deployment, we develop the probability for a point to be full view covered. Our results reveal that under uniform deployment, whether full view coverage is achieved depends largely on the area of the sensing region, rather than its shape.
[Image recognition, probability, Poisson deployment, Sensor phenomena and characterization, Sensor systems, randomly-deployed heterogeneous camera sensors network, cameras, image sensors, CSA, heterogeneous sensors, random deployment schemes, critical sensing area, Thermal sensors, Cameras, random deployment, Face, stochastic processes, sensing region, camera sensor, full view coverage]
LAACAD: Load Balancing k-Area Coverage through Autonomous Deployment in Wireless Sensor Networks
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Although the problem of k-area coverage has been intensively investigated for dense wireless sensor networks (WSNs), how to arrive at a k-coverage sensor deployment that optimizes certain objectives in relatively sparse WSNs still faces both theoretical and practical difficulties. In this paper, we present a practical algorithm LAACAD (Load balancing k-Area Coverage through Autonomous Deployment) to move sensor nodes toward k-area coverage, aiming at minimizing the maximum sensing range required by the nodes. LAACAD enables purely autonomous node deployment as it only entails localized computations. We prove the convergence of the algorithm, as well as the (local) optimality of the output. We also show that our optimization objective is closely related to other frequently considered objectives. Therefore, our practical algorithm design also contributes to the theoretical understanding of the k-area coverage problem. Finally, we use extensive simulation results both to confirm our theoretical claims and to demonstrate the efficacy of LAACAD.
[Algorithm design and analysis, autonomous deployment, autonomous node deployment, wireless sensor networks, load balancing, k-coverage, Generators, localized computations, Convergence, Wireless sensor networks, sensor nodes, optimisation, resource allocation, WSN, Chebyshev approximation, LAACAD, Load management, optimization objective, Sensors, load balancing k-area coverage deployment, area coverage]
Localizing Multiple Objects in an RF-based Dynamic Environment
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Radio Frequency (RF) based technologies play an important role in indoor localization, since Radio Signal Strength (RSS) is easily achieved by various wireless devices without additional cost. Among these, radio map based technologies (also referred as fingerprinting technologies) are attractive. They are able to accurately localize the targets without introducing many reference nodes. Therefore, their hardware cost is low. However, this technology has two fatal limitations. First, it is hard to localize multiple objects, since radio map has to collect all the RSS information when targets are at different possible positions. But due to the multipath phenomenon, different number of target nodes at different positions often generates different multipath signals. So when the target object number is unknown, constructing a radio map of multiple objects is almost impossible. Second, environment changes will generate different multipath signals and severely disturb the RSS measurement, making laborious retraining inevitable. In this paper, we propose a novel method, called Line-Of-Sight (LOS) map matching. It leverages frequency diversity of wireless nodes to eliminate the multipath behavior, making RSS more reliable than before. These reliable RSS signals are able to construct the radio map, which only reserves the LOS signal among nodes. We call it LOS radio map. The number of objects and environment changes will not affect the LOS signal between the targets and reference nodes. Such map is able to be constructed easily and require no training if reference nodes are carefully redeployed. Our basic idea is to utilize the frequency diversity of each wireless node to transmit data in different spectrum channel. Then it solves the optimization problem to get the LOS signal. Our experiments are based on TelosB sensor platform with three reference nodes. It shows that the accuracy will not decrease when localizing multiple targets in a dynamic environment. It outperforms the traditional methods by about60%. More importantly, no calibration is required in such environment. Furthermore, our approach presents attractive flexibility, making it more appropriate for general RF-based localization studies than just the radio map based localization.
[signal processing, indoor localization, radio map based technologies, LOS map matching, Training, Wireless communication, Accuracy, optimisation, line-of-sight map matching, wireless devices, optimization, radio signal strength, multipath signals, spectrum channel, Hardware, multipath phenomenon, Localization, RSS measurement, radio frequency based technologies, Radio transmitters, Receivers, multipath channels, Calibration, Multiple Objects, Dynamic Environment, multiple objects, indoor communication, RF-based dynamic environment, target object number]
Energy-Efficient and Fault-Tolerant Distributed Mobile Execution
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Although battery capacities keep increasing, the execution demands of modern mobile devices continue to outstrip their battery lives. As a result, battery life is bound to remain a key constraining factor in the design of mobile applications. To save battery power, mobile applications are often partitioned to offload parts of their execution to a remote server. However, partitioning an application renders it unusable in the face of network outages. In this paper, we present a novel approach that reduces the energy consumption of mobile applications through server offloading without partitioning. The functionality that consumes energy heavily is executed in the cloud, with the program's state check pointed and transferred across the mobile device and the cloud. Our approach is portable, as it introduces the offloading functionality through byte code enhancement, without any changes to the runtime system. The check pointed state's size is minimized through program analysis. In the case of a network outage, the offloading interrupts and the application reverts to executing locally from the latest checkpoint. Our case studies demonstrate how our approach can reduce energy consumption for third-party Android applications. Transformed through our approach, the applications consume between 30% and 60% fewer Joules than their original versions. Our results indicate that portable offloading can improve the battery life of modern mobile applications while maintaining their resilience to network outages.
[checkpointing, mobile application design, Energy consumption, fault-tolerance, offloading, Mobile communication, Mobile handsets, Batteries, Servers, battery life, cloud, mobile computing, power aware computing, network outages, remote server, file servers, program analysis, bytecode enhancement, cloud computing, third-party Android applications, constraining factor, program diagnostics, battery capacities, fault-tolerant distributed mobile execution, Optical character recognition software, Synchronization, energy saving, software fault tolerance, checkpointed state, energy-efficient distributed mobile execution, mobile devices, operating systems (computers), server offloading]
Software-Directed Data Access Scheduling for Reducing Disk Energy Consumption
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Most existing research in disk power management has focused on exploiting idle periods of disks. Both hardware power-saving mechanisms (such as spin-down disks and multi-speed disks) and complementary software strategies (such as code and data layout transformations to increase the length of idle periods) have been explored. However, while hardware power-saving mechanisms cannot handle short idle periods of high-performance parallel applications, prior code/data reorganization strategies typically require extensive code modifications. In this paper, we propose and evaluate a compiler-directed data access (I/O call) scheduling framework for saving disk energy, which groups as many data requests as possible in a shorter period, thus creating longer disk idle periods for improving the effectiveness of hardware power-saving mechanisms. As compared to prior software based efforts, it requires no code or data restructuring. We evaluate our approach using six application programs in a cluster-based simulation environment. The experimental results show that it improves the effectiveness of both spin-down disks and multi-speed disks with doubled power savings on average.
[code-data reorganization strategy, Schedules, Energy consumption, multispeed disk, compiler-directed data access scheduling, parallel processing, program compilers, disk energy consumption reduction, data layout transformation, Runtime, power aware computing, Velocity control, code modification, scheduling, spin-down disk, Hardware, Libraries, software-directed data access scheduling, code layout transformation, Power demand, data restructuring, compiler-directed data access, disk power management, code restructuring, cluster-based simulation environment, software strategy, hardware power-saving mechanism, data request, energy conservation, high-performance parallel application, disk idle period, I/O call scheduling, multi-speed disk]
Providing Fair Share Scheduling on Multicore Cloud Servers via Virtual Runtime-based Task Migration Algorithm
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
While Linux is the most favored operating system for an open source-based cloud data center, it falls short of expectations when it comes to fair share multicore scheduling. The primary task scheduler of the mainline Linux kernel, CFS, cannot provide a desired level of fairness in a multicore system. CFS uses a weight-based load balancing mechanism to evenly distribute task weights among all cores. Contrary to expectations, this mechanism cannot guarantee fair share scheduling since balancing loads among cores has nothing to do with bounding differences in the virtual runtimes of tasks. To make matters worse, CFS allows a persistent load imbalance among cores. This paper presents a virtual runtime-based task migration algorithm which directly bounds the maximum virtual runtime difference among tasks. For a given pair of cores, our algorithm periodically partitions run able tasks into two groups depending on their virtual runtimes and assigns each group to a dedicated core. In doing so, it bounds the load difference between two cores by the largest weight in the task set and makes the core with larger virtual runtimes receive a larger load and thus run more slowly. It bounds the virtual runtime difference of any pair of tasks running on these cores by a constant. We have implemented the algorithm into the Linux kernel 2.6.38.8. Experimental results show that the maximal virtual runtime difference is 50.53 time units while incurring only 0.14% more run-time overhead than CFS.
[load balancing, public domain software, Servers, weight-based load balancing mechanism, CFS, Runtime, Scheduling algorithms, resource allocation, multicore cloud servers, Linux kernel 2.6.38.8, file servers, scheduling, multi-core scheduling, cloud computing, virtual runtime-based task migration algorithm, operating system kernels, multiprocessing systems, Multicore processing, operating system, cloud server computing, multicore scheduling, fair share scheduling, computer centres, Linux, open source-based cloud data center, Load management]
Combining Partial Redundancy and Checkpointing for HPC
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Today's largest High Performance Computing (HPC) systems exceed one Petaflops (1015 floating point operations per second) and exascale systems are projected within seven years. But reliability is becoming one of the major challenges faced by exascale computing. With billion-core parallelism, the mean time to failure is projected to be in the range of minutes or hours instead of days. Failures are becoming the norm rather than the exception during execution of HPC applications. Current fault tolerance techniques in HPC focus on reactive ways to mitigate faults, namely via checkpoint and restart (C/R). Apart from storage overheads, C/R-based fault recovery comes at an additional cost in terms of application performance because normal execution is disrupted when checkpoints are taken. Studies have shown that applications running at a large scale spend more than 50% of their total time saving checkpoints, restarting and redoing lost work. Redundancy is another fault tolerance technique, which employs redundant processes performing the same task. If a process fails, a replica of it can take over its execution. Thus, redundant copies can decrease the overall failure rate. The downside of redundancy is that extra resources are required and there is an additional overhead on communication and synchronization. This work contributes a model and analyzes the benefit of C/R in coordination with redundancy at different degrees to minimize the total wallclock time and resources utilization of HPC applications. We further conduct experiments with an implementation of redundancy within the MPI layer on a cluster. Our experimental results confirm the benefit of dual and triple redundancy - but not for partial redundancy - and show a close fit to the model. At &#x2248; 80, 000 processes, dual redundancy requires twice the number of processing resources for an application but allows two jobs of 128 hours wallclock time to finish within the time of just one job without redundancy. For narrow ranges of processor counts, partial redundancy results in the lowest time. Once the count exceeds &#x2248; 770, 000, triple redundancy has the lowest overall cost. Thus, redundancy allows one to trade-off additional resource requirements against wallclock time, which provides a tuning knob for users to adapt to resource availabilities.
[Checkpointing, checkpointing, partial redundancy, Protocols, multiprocessing systems, fault tolerance techniques, MPI layer, application program interfaces, Peer to peer computing, Redundancy, wallclock time, billion-core parallelism, HPC applications, high performance computing systems, checkpoint and restart, resource availabilities, C-R-based fault recovery, failure time, Libraries, fault tolerant computing, Kernel, exascale computing]
SOS: A Distributed Mobile Q&amp;amp;A System Based on Social Networks
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Recently, emerging research efforts have been focused on question and answer (Q\\&amp;amp; A) systems based on social networks. The social-based Q\\&amp;amp; A systems can answer non-factual questions, which cannot be easily resolved by web search engines. These systems either rely on a centralized server for identifying friends based on social information or broadcast a user's questions to all of its friends. Mobile Q\\&amp;amp; A systems, where mobile nodes access the Q\\&amp;amp; A systems through Internet, are very promising considering a rapid increase of mobile users and the convenience of practical use. However, such systems cannot directly use the previous centralized methods or broadcasting methods, which generate high cost of mobile Internet access, node overload, and high server bandwidth cost with the tremendous number of mobile users. We propose a distributed Social-based mobile Q\\&amp;amp; A System (SOS) with low overhead and system cost as well as quick response to question askers. SOS enables mobile users to forward questions to potential answerers in their friend lists in a decentralized manner for a number of hops and then resort to the server. It leverages lightweight knowledge engineering techniques to accurately identify friends who are able to and willing to answer questions, thus reducing the search and computation costs of mobile nodes. The trace-driven simulation results show that SOS can achieve a high query precision and recall rate, a short response latency and low overhead. We have also deployed a pilot version of SOS for use in a small group in Clemson University. The feedback from the users shows that SOS can provide high-quality answers.
[Question and answer system, Databases, Social network services, distributed system, on-line social network, Mobile communication, Motion pictures, Internet, Servers, Engines]
Geology: Modular Georecommendation in Gossip-Based Social Networks
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Geolocated social networks, combining traditional social networking features with geolocation information, have grown tremendously over the last few years. Yet, very few works have looked at implementing geolocated social networks in a fully distributed manner, a promising avenue to handle the growing scalability challenges of these systems. In this paper, we focus on georecommendation, and show that existing decentralized recommendation mechanisms perform in fact poorly on geodata. We propose a set of novel gossip-based mechanisms to address this problem, in a modular similarity framework called GEOLOGY. The resulting platform is lightweight, efficient, and scalable, and we demonstrate its advantages in terms of recommendation quality and communication overhead on a real dataset of 15,694 users from Foursquare, a leading geolocated social network.
[Measurement, Geolocated social networks, geolocation information, Geology, Peer to peer computing, social networks, Twitter, Vectors, Compaction, communication overhead, recommender systems, georecommendation, GEOLOGY modular similarity framework, recommendation quality, social networking feature, gossip-based social network, social networking (online), distributed systems, decentralized recommendation mechanism, gossip protocols, Foursquare, geolocation]
SEER: A Secure and Efficient Service Review System for Service-Oriented Mobile Social Networks
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
In this paper, we consider service-oriented mobile social networks (S-MSNs) and propose a Secure and Efficient service Review (SEER) system to enable user feedback. Each service provider independently maintains a SEER system for itself, which collects and stores user reviews about its services without requiring any central trusted authority. The service reviews can then be made available to interested users in making wise service selection decisions. We identify three unique service review attacks and then develop sophisticated security mechanisms for SEER to deal with these attacks. Specifically, SEER enables users to distributedly and cooperatively submit their reviews in an integrated chain form by using hierarchical and aggregate signature techniques. It discourages service providers to reject, modify or delete their reviews. The integrity of reviews is therefore improved. Through security analysis and performance evaluation, we show that SEER effectively resists the service review attacks and achieves significantly better performance in terms of submission rate and delay than a service review system that does not adopt user cooperation or the chain review structure.
[Mobile communication, S-MSN, Security, wise service selection decisions, Delay, secure review system, Secure and Efficient service Review, Privacy, mobile computing, Location based services, hierarchical signature techniques, SEER system, sophisticated security mechanisms, service review system, aggregate signature techniques, cooperation, security analysis, unique service review attacks, Social network services, central trusted authority, service-oriented mobile social networks, security of data, adopt user cooperation, Aggregates, Resists, social networking (online), user feedback, service review]
Explaining BGP Slow Table Transfers
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Although there have been a plethora of studies on TCP performance in supporting of various applications, relatively little is known about the interaction between TCP and BGP, which is a specific application running on top of TCP. This paper investigates BGP's slow route propagation by analyzing packet traces collected from a large ISP and Route Views Oregon collector. In particular we focus on the prolonged periods of BGP routing table transfers and examine in detail the interplay between TCP and BGP. In addition to the problems reported in previous literature, this study reveals a number of new TCP transport problems, that collectively induce significant delays. Furthermore, we develop a tool, named T-DAT, that can be deployed together with BGP data collectors to infer various factors behind the observed delay, including BGP's sending and receiving behavior, TCP's parameter settings, TCP's flow and congestion control, and network path limitation. Identifying these delay contributing factors makes an important step for ISPs and router vendors to diagnose and improve the BGP performance.
[TCP, Measurement, telecommunication congestion control, TCP transport problem, sending behavior, BGP performance, T-DAT tool, Delay, receiving behavior, slow route propagation, Route Views Oregon collector, Routing protocols, Delay Analysis, packet traces, Monitoring, TCP performance, Estimation, BGP slow table transfer, Receivers, ISP, congestion control, Routing, BGP, TCP parameter setting, border gateway protocol, BGP data collector, network path limitation, delay, transport protocols, BGP routing table transfer, routing protocols, TCP flow, Internet]
Studying Impacts of Prefix Interception Attack by Exploring BGP AS-PATH Prepending
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
The AS path prep ending approach in BGP is commonly used to perform inter-domain traffic engineering, such as inbound traffic load-balancing for multi-homed ASes. It artificially increases the length of the AS level path in BGP announcements by inserting its local AS number multiple times into outgoing announcements. In this work, we study how the AS path prep ending mechanism can be exploited to launch a BGP prefix interception attack. Our work is motivated by a recent routing anomaly related to AS Path prepending behavior, i.e., Facebook's traffic being redirected to Korea and China due to a shorter path with fewer prep ending ASNs. In order to measure the possible impact of the attack, we develop a simulator to quantify the damage of the attack under a diverse set of attacker/victim combinations. Our main contribution is to quantify how many ASes may be susceptible to the attack, and analyze how effective the attack may be through simulation. Furthermore, we propose an algorithm to detect the interception attack by exploiting inconsistencies via collaborative monitoring from multiple vantage points. Our evaluation shows up to 99% accuracy with 150 vantage points.
[inter-domain traffic engineering, Routing, BGP, Telecommunications, Topology, Security, computer network security, attacker-victim combinations, traffic load-balancing, routing protocols, China, BGP AS-path prepending, social networking (online), prefix interception attack, Routing protocols, Internet, ASN, Facebook, Monitoring, telecommunication traffic, multihomed AS, Korea]
CLUE: Achieving Fast Update over Compressed Table for Parallel Lookup with Reduced Dynamic Redundancy
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
The sizes of routing table in backbone routers continue to keep a rapid growth and some of them currently increase up to 400K entries [1]. An effective solution to deflate the large table is the routing table compression. Meanwhile, there is an increasingly urgent demand for fast routing update mainly due to the change of network topology and new emerging Internet functionalities. Furthermore, the Internet link transmission speed has scaled up to 100Gbps commercially and towards 400Gbps Ethernet for laboratory experiments, resulting in a raring need of ultra-fast routing lookup. To achieve high performance, backbone routers must gracefully handle the three issues simultaneously: routing table Compression, fast routing Lookup, and fast incremental Update (CLUE), while previous works often only concentrate on one of the three dimensions. To address these issues, we propose a complete set of solutions-CLUE, by improving previous works and adding a novel incremental update mechanism. CLUE consists of three parts: a routing table compression algorithm, an improved parallel lookup mechanism, and a new fast incremental update mechanism. The routing table compression algorithm is based on ONRTC algorithm [2], a base for fast TCAM parallel lookup and fast update of TCAM. The second part is the improvement of the logical caching scheme for dynamic load balancing parallel lookup mechanism [3]. The third one is the conjunction of the trie, TCAM and redundant prefixes update algorithm. We analyze the performance of CLUE by mathematical proof, and draw the conclusion that speedup factor is proportional to the hit rate of redundant prefixes in the worst case, which is also confirmed by experimental results. Large-scale experimental results show that, compared with the mechanism in [3], CLUE only needs about 71% TCAM entries, 4.29% update time, and 3/4 dynamic redundant prefixes for the same throughput when using four TCAMs. In addition, CLUE has another advantage over the mechanism in [3] - the frequent interactions between control plane and data plane caused by redundant prefixes update can be avoided.
[reduced dynamic redundancy, local area networks, ternary content addressable memory, Compression algorithms, ONRTC algorithm, speedup factor, IP networks, Internet functionality, hit rate, Redundancy, fast incremental update mechanism, backbone router, redundant prefixes update algorithm, telecommunication network topology, Routing, control plane, Partitioning algorithms, routing table compression, fast routing update, network topology, TCAM fast update, telecommunication network routing, Ethernet, TCAM parallel lookup, Load management, parallel lookup mechanism, Internet, routing table compression algorithm, data plane, CLUE solution, Internet link transmission speed]
Scalable Name Lookup in NDN Using Effective Name Component Encoding
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Name-based route lookup is a key function for Named Data Networking (NDN). The NDN names are hierarchical and have variable and unbounded lengths, which are much longer than IPv4/6 address, making fast name lookup a challenging issue. In this paper, we propose an effective Name Component Encoding (NCE) solution with the following two techniques: (1) A code allocation mechanism is developed to achieve memory-efficient encoding for name components, (2) We apply an improved State Transition Arrays to accelerate the longest name prefix matching and design a fast and incremental update mechanism which satisfies the special requirements of NDN forwarding process, namely to insert, modify, and delete name prefixes frequently. Furthermore, we analyze the memory consumption and time complexity of NCE. Experimental results on a name set containing 3,000,000 names demonstrate that compared with the character trie NCE reduces overall 30% memory. Besides, NCE performs a few millions lookups per second (on an Intel 2.8 GHz CPU), a speedup of over 7 times compared with the character trie. Our evaluation results also show that NCE can scale up to accommodate the potential future growth of the name sets.
[NDN forwarding process, code allocation mechanism, Named Data Networking, Routing, Encoding, Complexity theory, name-based route lookup, Indexes, encoding, Name Component Encoding, telecommunication network routing, Name Prefix Longest Matching, state transition arrays, Internet, IP networks, Resource management, Acceleration, scalable name lookup, memory-efficient encoding, named data networking, prefix matching, IPv4-6 address, name component encoding]
DMap: A Shared Hosting Scheme for Dynamic Identifier to Locator Mappings in the Global Internet
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
This paper presents the design and evaluation of a novel distributed shared hosting approach, DMap, for managing dynamic identifier to locator mappings in the global Internet. DMap is the foundation for a fast global name resolution service necessary to enable emerging Internet services such as seamless mobility support, content delivery and cloud computing. Our approach distributes identifier to locator mappings among Autonomous Systems (ASs) by directly applying K&gt;;1 consistent hash functions on the identifier to produce network addresses of the AS gateway routers at which the mapping will be stored. This direct mapping technique leverages the reachability information of the underlying routing mechanism that is already available at the network layer, and achieves low lookup latencies through a single overlay hop without additional maintenance overheads. The proposed DMap technique is described in detail and specific design problems such as address space fragmentation, reducing latency through replication, taking advantage of spatial locality, as well as coping with inconsistent entries are addressed. Evaluation results are presented from a large-scale discrete event simulation of the Internet with ~26,000 ASs using real-world traffic traces from the DIMES repository. The results show that the proposed method evenly balances storage load across the global network while achieving lookup latencies with a mean value of ~50 ms and 95th percentile value of ~100 ms, considered adequate for support of dynamic mobility across the global Internet.
[telecommunication security, future internet architecture, Protocols, distributed shared hosting, AS gateway router, address space fragmentation, content delivery, storage load balancing, Mobile communication, GNRS, Mobile handsets, reachability information, routing mechanism, global Internet, MobilityFirst, Proposals, large-scale discrete event simulation, Naming resolution service, table lookup, network layer, mobile computing, resource allocation, hash function, seamless mobility support, lookup latencies, DMap, DIMES repository, IP networks, cloud computing, dynamic mobility, discrete event simulation, Internet service, autonomous system, direct mapping, cryptography, global name resolution service, network address, telecommunication network routing, Logic gates, Internet, locator mapping, global network, dynamic identifier]
Error Tolerant Address Configuration for Data Center Networks with Malfunctioning Devices
2012 IEEE 32nd International Conference on Distributed Computing Systems
None
2012
Address auto-configuration is a key problem in data center networks, where servers and switches encode topology information into their addresses for routing. A recent work DAC [2] has been introduced to address this problem. Without malfunctions, DAC can auto-configure all the devices quickly. But in case of malfunctions, DAC requires significant human efforts to correct malfunctions and it can cause substantial operation delay of the whole data center. In this paper, we further optimize address auto-configuration process even in the presence of malfunctions. Instead of waiting for all the malfunctions to be corrected, we could first configure the devices that are not involved in malfunctions and let them work first. This idea can be translated to considerable practical benefits because in most cases malfunctions in data centers only account for a very small portion. To realize the idea, we conceptually remove the malfunctions from the physical data center topology graph and mathematically convert the address configuration problem into induced sub graph isomorphism problem, which is NP-complete. We then introduce an algorithm that can solve the induced sub graph isomorphism quickly by taking advantage of data center topology characteristics and induced sub graph properties. We extensively evaluate our design on representative data center structures with various malfunction scenarios. The evaluation results demonstrate that the proposed framework and algorithm are efficient and labor-free to deal with the mapping task in the presence of error devices.
[Algorithm design and analysis, data center topology characteristics, malfunctioning devices, Data Center Networks, Manuals, switches, Induced Subgraph Isomorphism Problem, Servers, routing, Network topology, servers, error tolerant address configuration, data center networks, computer networks, DAC, telecommunication network topology, Educational institutions, Topology, Partitioning algorithms, encoding, NP-complete problem, computer centres, Address Configuration, data center topology graph, induced sub graph isomorphism problem, computational complexity, topology information encoding, auto-configuration process]
Message from General Chair
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Presents the introductory welcome message from the conference proceedings.
[]
Message from Program Chair
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Presents the introductory welcome message from the conference proceedings.
[]
Organizing Committee
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Provides a listing of current committee members and society officers.
[]
Program Committee
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Provides a listing of current committee members and society officers.
[]
Volley: Violation Likelihood Based State Monitoring for Datacenters
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Distributed state monitoring plays a critical role in Cloud datacenter management. One fundamental problem in distributed state monitoring is to minimize the monitoring cost while maximizing the monitoring accuracy at the same time. In this paper, we present Volley, a violation likelihood based approach for efficient distributed state monitoring in Cloud datacenters. Volley achieves both efficiency and accuracy with a flexible monitoring framework which uses dynamic monitoring intervals determined by the likelihood of detecting state violations. Volley consists of three unique techniques. It utilizes efficient node-level adaptation algorithms that minimize monitoring cost with controlled accuracy. Volley also employs a distributed scheme that coordinates the adaptation on multiple monitor nodes of the same task for optimal task- level efficiency. Furthermore, it enables multi-task level cost reduction by exploring state correlation among monitoring tasks. We perform extensive experiments to evaluate Volley with system, network and application monitoring tasks in a virtualized datacenter environment. Our results show that Volley can reduce considerable monitoring cost and still deliver user specified monitoring accuracy under various scenarios.
[Correlation, Volley, Heuristic algorithms, Likelihood, violation likelihood based state monitoring, distributed scheme, multitask level cost reduction, Servers, Computer crime, node-level adaptation algorithms, Accuracy, dynamic monitoring intervals, state correlation, computer crime, monitor nodes, optimal task- level efficiency, cloud computing, Monitoring, Adaptation, monitoring cost minimization, virtualized datacenter environment, Estimation, distributed state monitoring, cloud datacenter management, computer centres, tasks monitoring, State, Distributed, state violations detection, Datacenter, system monitoring, violation likelihood based approach]
Diagnosing Data Center Behavior Flow by Flow
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Multi-tenant data centers are complex environments, running thousands of applications that compete for the same infrastructure resources and whose behavior is guided by (sometimes) divergent configurations. Small workload changes or simple operator tasks may yield unpredictable results and lead to expensive failures and performance degradation. In this paper, we propose a holistic approach for detecting operational problems in data centers. Our framework, FlowDiff, collects information from all entities involved in the operation of a data center -- applications, operators, and infrastructure -- and continually builds behavioral models for the operation. By comparing current models with pre-computed, known-to-be-stable models, FlowDiff is able to detect many operational problems, ranging from host and network failures to unauthorized access. FlowDiff also identifies common system operations (e.g., VM migration, software upgrades) to validate the behavior changes against planned operator tasks. We show that using passive measurements on control traffic from programmable switches to a centralized controller is sufficient to build strong behavior models; FlowDiff does not require active measurements or expensive server instrumentation. Our experimental results using NEC data center testbed, Amazon EC2, and simulations demonstrate that FlowDiff is effective and robust in detecting anomalous behavior. FlowDiff scales well with the number of applications running in the data center and their traffic volume.
[NEC data center testbed, Amazon EC2, data center behavior diagnosis, OpenFlow, Control systems, Servers, Data Center, multitenant data centers, expensive failures, network failures, EC2, Diagnosis, anomalous behavior, Learning automata, Infrastructure Signature, performance evaluation, unauthorized access, performance degradation, Passive Monitoring, planned operator tasks, computer centres, Application Signature, Task Signature, Automata, operational problems, FlowDiff, Data models, Delays, Time factors, host failures]
FChain: Toward Black-Box Online Fault Localization for Cloud Systems
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Distributed applications running inside cloud systems are prone to performance anomalies due to various reasons such as resource contentions, software bugs, and hardware failures. One big challenge for diagnosing an abnormal distributed application is to pinpoint the faulty components. In this paper, we present a black-box online fault localization system called FChain that can pinpoint faulty components immediately after a performance anomaly is detected. FChain first discovers the onset time of abnormal behaviors at different components by distinguishing the abnormal change point from many change points caused by normal workload fluctuations. Faulty components are then pinpointed based on the abnormal change propagation patterns and inter-component dependency relationships. FChain performs runtime validation to further filter out false alarms. We have implemented FChain on top of the Xen platform and tested it using several benchmark applications (RUBiS, Hadoop, and IBM System S). Our experimental results show that FChain can quickly pinpoint the faulty components with high accuracy within a few seconds. FChain can achieve up to 90% higher precision and 20% higher recall than existing schemes. FChain is non-intrusive and light-weight, which imposes less than 1% overhead to the cloud system.
[Measurement, RUBiS, black-box online fault localization system, runtime validation, abnormal change propagation patterns, intercomponent dependency relationships, distributed applications, resource contentions, FChain, Runtime, Accuracy, cloud systems, performance anomaly detection, Xen platform, Benchmark testing, cloud computing, normal workload fluctuations, abnormal distributed application diagnosis, hardware failures, software bugs, abnormal behaviors, Hadoop, IBM System S, Web servers, software fault tolerance, faulty components, security of data]
Detecting Transient Bottlenecks in n-Tier Applications through Fine-Grained Analysis
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Identifying the location of performance bottlenecks is a non-trivial challenge when scaling n-tier applications in computing clouds. Specifically, we observed that an n-tier application may experience significant performance loss when there are transient bottlenecks in component servers. Such transient bottlenecks arise frequently at high resource utilization and often result from transient events (e.g., JVM garbage collection) in an n-tier system and bursty workloads. Because of their short lifespan (e.g., milliseconds), these transient bottlenecks are difficult to detect using current system monitoring tools with sampling at intervals of seconds or minutes. We describe a novel transient bottleneck detection method that correlates throughput (i.e., request service rate) and load (i.e., number of concurrent requests) of each server in an n-tier system at fine time granularity. Both throughput and load can be measured through passive network tracing at millisecond-level time granularity. Using correlation analysis, we can identify the transient bottlenecks at time granularities as short as 50ms. We validate our method experimentally through two case studies on transient bottlenecks caused by factors at the system software layer (e.g., JVM garbage collection) and architecture layer (e.g., Intel SpeedStep).
[system software layer, fine-grained analysis, Throughput, millisecond-level time granularity, Servers, scalability, correlation analysis, resource allocation, component servers, file servers, computing clouds, fine time granularity, bottleneck, transient events, cloud computing, resource utilization, Transient analysis, Monitoring, Web-facing applications, Time measurement, transient bottleneck detection, Passive networks, system monitoring tools, passive network tracing, Performance evaluations, n-tier applications, Time factors, architecture layer, n-tier system]
Cache Privacy in Named-Data Networking
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Content-Centric Networking (CCN) is an alternative to host-centric networking exemplified by today's Internet. CCN emphasizes content distribution by making content directly addressable. Named-Data Networking (NDN) is an example of CCN being considered as a candidate next-generation Internet architecture. One key NDN feature is router-side content caching that optimizes bandwidth consumption, reduces congestion and provides fast fetching for popular content. Unfortunately, the same feature is also detrimental to privacy of both consumers and producers of content. As we show in this paper, simple and difficult-to-detect timing attacks can exploit NDN routers as "oracles" and allow the adversary to learn whether a nearby consumer recently requested certain content. Similarly, probing attacks that target adjacent content producers can be used to discover whether certain content has been recently fetched. After analyzing the scope and feasibility of such attacks, we propose and evaluate some efficient countermeasures that offer quantifiable privacy guarantees while retaining key features of NDN.
[cache privacy, next generation networks, host-centric networking, Educational institutions, cache storage, Topology, YouTube, computer network security, next-generation Internet architecture, Privacy, named-data networking, content distribution, Network topology, content-centric networking, difficult-to-detect timing attacks, router-side content caching, data privacy, Internet, Delays, timing attacks]
Cache Topology Aware Mapping of Stream Processing Applications onto CMPs
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Data Stream Processing is an important class of data intensive applications in the "Big Data" era. Chip Multi-Processors (CMPs) are the standard hosting platforms in modern data centers. Gaining high performance for stream processing applications on CMPs is therefore of great interest. Since the performance of stream processing applications largely depends on their effective use of the complex cache structure present on CMPs, this paper proposes the StreamMap approach for tuning streaming applications' use of cache. Our major idea is to map application threads to CPU cores to facilitate data sharing AND mitigate memory resource contention among threads in a holistic manner. Applying StreamMap to the IBM's System S middleware leads to improvements of up to 1.8x in the performance of realistic applications over standard Linux OS scheduler on three different CMP platforms.
[Instruction sets, Ports (Computers), complex cache structure, Thread Mapping, cache storage, Data Stream Processing, modern data centers, Runtime, resource allocation, very large databases, Benchmark testing, memory resource contention, big data, Message systems, middleware, cache topology aware mapping, multiprocessing systems, CMP, CPU cores, microprocessor chips, standard hosting platforms, Topology, network topology, Middleware, chip multiprocessors, data stream processing applications, IBM System S middleware, Cache Topology, StreamMap, data sharing, IBM Infosphere Streams, data intensive applications]
Coordinating In-Network Caching in Content-Centric Networks: Model and Analysis
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
In-network content storage has become an inherent capability of routers in the content-centric networking architecture. This raises new challenges in utilizing and provisioning the in-network caching capability, namely, how to optimally provision individual routers' storage to cache contents, so as to balance the trade-offs between the network performance and the provisioning cost. To address this problem, we first propose a holistic model to characterize the network performance of routing contents to clients and the network cost incurred by globally coordinating the in-network storage capability. We then derive the optimal strategy for provisioning the storage capability that optimizes the overall network performance and cost, and analyze the performance gains via numerical evaluations on real network topologies. Our results reveal interesting phenomena; for instance, different ranges of the Zipf exponent can lead to opposite optimal strategies, and the trade-offs between the network performance and the provisioning cost have great impacts on the stability of the optimal strategy. We also demonstrate that the optimal strategy can achieve significant gain on both the load reduction at origin servers and the improvement on the routing performance.
[Measurement, in-network caching coordination, content-centric networks, in-network caching, Performance gain, telecommunication network topology, Routing, Educational institutions, cache storage, Servers, in-network content storage, content-centric networking architecture, storage capability, Analytical models, coordinated caching, network performance, telecommunication network routing, Zipf exponent, network topologies, numerical evaluations, Internet, routers, load reduction]
Proteus: Power Proportional Memory Cache Cluster in Data Centers
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
In this paper, we describe the design, implementation and evaluation of Proteus, a power-proportional cache cluster which eliminates the delay penalty during server provisioning dynamics. To speed up data center services, a cache cluster is used in front of the database tier, providing fast in-cache data access. Since the number of cache servers is large, building power-proportional cache clusters can lead to considerable monetary savings. Dynamic server provisioning, one common methodology for realizing power proportionality in data centers, calls for agile load balancing schemes and smart in-cache data migration algorithms when applied to cache clusters. Otherwise, it induces unacceptable delay spikes due to data re-allocation among cache servers. Proteus addresses both challenges by using a specifically designed virtual nodes placement algorithm and an amortized data migration policy. We implement Proteus, and evaluate it on a 40-server cluster using real Wikipedia data and workload traces. The results show that, with Proteus, the load distribution is much more evenly balanced compared to the case of applying unmodified consistent hashing. At the same time, Proteus induces almost no extra delay during provisioning transitions, which is a significant advantage over other state-of-the-art solutions.
[Algorithm design and analysis, data center services, Heuristic algorithms, cache storage, Data center, delay penalty, Proteus, database tier, power aware computing, Databases, resource allocation, server provisioning dynamics, Clustering algorithms, amortized data migration policy, Energy proportionality, Load balancing, power proportional memory cache cluster, agile load balancing schemes, Bloom filter, Web servers, virtual nodes, in-cache data access, computer centres, workload traces, dynamic server provisioning, Wikipedia data, Delays, smart in-cache data migration algorithms, delay spikes, power proportionality, Memcached]
Dynamic Energy Estimation of Query Plans in Database Systems
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Data centers are well known to consume large amounts of energy. Since database is one of the major applications in a typical data center, building energy-aware database systems has become an active research topic recently. The quantification of the energy cost of database systems is an important task in designing such systems. In this paper, we report our recent efforts on this topic, with a focus on the energy cost estimation of query plans during query optimization. We start from building a series of physical models for energy estimation of individual relational operators based on their resource consumption patterns. Since the execution of individual queries is a combination of relational operators, we use the physical models as a basis for a comprehensive energy cost estimation model for entire query plans. To further improve model accuracy under system dynamics and the variations of workload characteristics, we develop an online model estimation scheme that dynamically corrects the static model based on advanced modeling techniques adopted from control engineering. The models are implemented in a real database and evaluated on a physical test bed with a comprehensive set of experimental workloads. The results show that our solution achieves a high accuracy (above 90%) in energy estimation despite noises from the system and workloads.
[static model, Query Optimization, Energy consumption, Adaptation models, query plan, resource consumption patterns, Database Management System, data centers, costing, energy cost estimation model, database management systems, Energy Cost Estimation, energy cost quantification, query processing, workload characteristics variation, advanced modeling techniques, power aware computing, energy estimation physical models, energy-aware database systems, Power demand, online model estimation scheme, Energy Efficient Database, Estimation, individual relational operators, computer centres, system dynamics, query optimization, Query processing, dynamic energy estimation]
Content-Based Scheduling of Virtual Machines (VMs) in the Cloud
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Organizations of all sizes are shifting their IT infrastructures to the cloud because of its cost efficiency and convenience. Because of the on-demand nature of the Infrastructure as a Service (IaaS) clouds, hundreds of thousands of virtual machines (VMs) may be deployed and terminated in a single large cloud data center each day. In this paper, we propose a content-based scheduling algorithm for the placement of VMs in data centers. We take advantage of the fact that it is possible to find identical disk blocks in different VM disk images with similar operating systems by scheduling VMs with high content similarity on the same hosts. That allows us to reduce the amount of data transferred when deploying a VM on a destination host. In this paper, we first present our study of content similarity between different VMs, based on a large set of VMs with different operating systems that represent the majority of popular operating systems in use today. Our analysis shows that content similarity between VMs with the same operating system and close version numbers (e.g., Ubuntu 12.04 vs. Ubuntu 11.10) can be as high as 60%. We also show that there is close to zero content similarity between VMs with different operating systems. Second, based on the above results, we designed a content-based scheduling algorithm that lowers the network traffic associated with transfer of VM disk images inside data centers. Our experimental results show that the amount of data transfer associated with deployment of VMs and transfer of virtual disk images can be lowered by more than 70%, resulting in significant savings in data center network utilization and congestion.
[infrastructure as a service, IaaS clouds, IT infrastructures, Scheduling, Data center, Servers, content-based scheduling, network traffic, Scheduling algorithms, Operating systems, virtual machines, VM, scheduling, Peer-to-peer computing, cloud computing, Virtualization, telecommunication traffic, Manganese, Cloud-computing]
HybridMR: A Hierarchical MapReduce Scheduler for Hybrid Data Centers
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Virtualized environments are attractive because they simplify cluster management, while facilitating cost-effective workload consolidation. As a result, virtual machines in public clouds or private data centers, have become the norm for running transactional applications like web services and virtual desktops. On the other hand, batch workloads like MapReduce, are typically deployed in a native cluster to avoid the performance overheads of virtualization. While both these virtual and native environments have their own strengths and weaknesses, we demonstrate in this work that it is feasible to provide the best of these two computing paradigms in a hybrid platform. In this paper, we make a case for a hybrid data center consisting of native and virtual environments, and propose a 2-phase hierarchical scheduler, called HybridMR, for the effective resource management of interactive and batch workloads. In the first phase, HybridMR classifies incoming MapReduce jobs based on the expected virtualization overheads, and uses this information to automatically guide placement between physical and virtual machines. In the second phase, HybridMR manages the run-time performance of MapReduce jobs collocated with interactive applications in order to provide best effort delivery to batch jobs, while complying with the Service Level Agreements (SLAs) of interactive applications. By consolidating batch jobs with over-provisioned foreground applications, the available unused resources are better utilized, resulting in improved application performance and energy efficiency. Evaluations on a hybrid cluster consisting of 24 physical servers and 48 virtual machines, with diverse workload mix of interactive and batch MapReduce applications, demonstrate that HybridMR can achieve up to 40% improvement in the completion times of MapReduce jobs, over the virtual-only case, while complying with the SLAs of interactive applications. Compared to the native-only cluster, at the cost of minimal performance penalty, HybridMR boosts resource utilization by 45%, and achieves up to 43% energy savings. These results indicate that a hybrid data center with an efficient scheduling mechanism can provide a cost-effective solution for hosting both batch and interactive workloads.
[cluster management, SLA, virtualisation, transactional applications, Hadoop MapReduce, Servers, contracts, Resource Management, cost-effective workload consolidation, batch workloads, over-provisioned foreground applications, interactive workloads, Energy, Clustering algorithms, Benchmark testing, scheduling, HybridMR, hybrid data centers, service level agreements, virtualized environments, Interference, Virtual machining, hierarchical MapReduce scheduler, private data centers, Scheduling, computer centres, virtual desktops, 2-phase hierarchical scheduler, scheduling mechanism, Web services, virtual machines, Hybrid Data Center, public clouds, Resource management, Performance, Virtualization, MapReduce jobs]
AUTOVAC: Automatically Extracting System Resource Constraints and Generating Vaccines for Malware Immunization
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Malware often contains many system-resource-sensitive condition checks to avoid any duplicate infection, make sure to obtain required resources, or try to infect only targeted computers, etc. If we are able to extract the system resource constraints from malware code, and manipulate the environment state as vaccines, we would then be able to immunize a computer from infections. Towards this end, this paper provides the first systematic study and presents a prototype system, AUTOVAC, for automatically extracting the system resource constraints from malware code and generating vaccines based on the system resource conditions. Specifically, through monitoring the data propagation from system-resource-related system calls, AUTOVAC automatically identifies the environment related state of a computer. Through analyzing the environment state, AUTOVAC automatically generates vaccines. Such vaccines can be then injected into other computers, thereby being immune from future infections from the same malware or its polymorphic variants. We have evaluated AUTOVAC on a large set of real-world malware samples and successfully extracted working vaccines for many families including high-profile Conficker, Sality and Zeus. We believe AUTOVAC represents an appealing technique to complement existing malware defenses.
[Computers, Algorithm design and analysis, invasive software, vaccine., Zeus, vaccines, Conficker, duplicate infection, Vaccines, AUTOVAC prototype system, polymorphic variants, Malware, Sality, malware defenses, system resource constraints, Immune system, Context, malware code, malware immunization, Dynamic malware analysis, system-resource-sensitive condition checks, environment state, environment constraint, system-resource-related system calls, Software, system resource conditions, data propagation monitoring]
Storing Shared Data on the Cloud via Security-Mediator
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Nowadays, many organizations outsource data storage to the cloud such that a member (owner) of an organization can easily share data with other members (users). Due to the existence of security concerns in the cloud, both owners and users are suggested to verify the integrity of cloud data with Provable Data Possession (PDP) before further utilization on data. However, previous methods either unnecessarily reveal the identity of a data owner to the untrusted cloud or any public verifiers, or introduce significant overheads on verification metadata to preserve anonymity. In this paper, we propose a simple and efficient publicly verifiable approach to ensure cloud data integrity without sacrificing the anonymity of data owners nor requiring significant verification metadata. Specifically, we introduce a security-mediator (SEM), which is able to generate verification metadata (i.e., signatures) on outsourced data for data owners. Our approach decouples the anonymity protection mechanism from the PDP. Thus, an organization can employ its own anonymous authentication mechanism, and the cloud is oblivious to that since it only deals with typical PDP-metadata, Consequently, there is no extra storage overhead when compared with existing non-anonymous PDP solutions. The distinctive features of our scheme also include data privacy, such that the SEM does not learn anything about the data to be uploaded to the cloud at all, which is able to minimize the requirement of trust on the SEM. In addition, we can also extend our scheme to work with the multi-SEM model, which can avoid the potential single point of failure existing in the single-SEM scenario. Security analyses prove our scheme is secure, and experiment results demonstrate our scheme is efficient.
[anonymous authentication mechanism, Cloud computing, Data privacy, publicly verifiable approach, data integrity, Servers, verification metadata, data storage, formal verification, Public key, message authentication, security-mediator, Organizations, shared data, anonymity protection mechanism, provable data possession, data privacy, anonymity, cloud computing, cloud data integrity, PDP]
pVOCL: Power-Aware Dynamic Placement and Migration in Virtualized GPU Environments
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Power-hungry Graphics processing unit (GPU) accelerators are ubiquitous in high performance computing data centers today. GPU virtualization frameworks introduce new opportunities for effective management of GPU resources by decoupling them from application execution. However, power management of GPU-enabled server clusters faces significant challenges. The underlying system infrastructure shows complex power consumption characteristics depending on the placement of GPU workloads across various compute nodes, power-phases and cabinets in a datacenter. GPU resources need to be scheduled dynamically in the face of time-varying resource demand and peak power constraints. We propose and develop a power-aware virtual OpenCL (pVOCL) framework that controls the peak power consumption and improves the energy efficiency of the underlying server system through dynamic consolidation and power-phase topology aware placement of GPU workloads. Experimental results show that pVOCL achieves significant energy savings compared to existing power management techniques for GPU-enabled server clusters, while incurring negligible impact on performance. It drives the system towards energy-efficient configurations by taking an optimal sequence of adaptation actions in a virtualized GPU environment and meanwhile keeps the power consumption below the peak power budget.
[Power Management, Dynamic Placement and Migration, Graphics processing units, energy savings, GPU enabled server clusters, Servers, power management techniques, time varying resource demand, system infrastructure, power aware virtual OpenCL, peak power consumption, Power measurement, power aware computing, complex power consumption, GPU resources, power aware dynamic placement and migration in virtualized GPU, power hungry graphics processing unit, energy efficient configurations, Kernel, Monitoring, power phase topology aware placement, optimal sequence, Power demand, peak power constraints, peak power budget, GPU workloads, high performance computing data centers, GPU accelerators, Topology, graphics processing units, virtualized GPU environment, GPU virtualization frameworks, energy conservation, GPU Accelerators, OpenCL, Virtualization, pVOCL framework, dynamic consolidation]
Mobility-Assisted Energy-Aware User Contact Detection in Mobile Social Networks
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Many practical problems in mobile social networks such as routing, community detection, and social behavior analysis, rely on accurate user contact detection. The frequently used method for detecting user contact is through Bluetooth on smartphones. However, Bluetooth scans consume lots of power. Although increasing the scan duty cycle can reduce the power consumption, it also reduces the accuracy of contact detection. In this paper, we address this problem based on the observation that user contact changes (i.e., starts and ends of user contacts) are mainly caused by user movement. Since most smartphones have accelerometers, we can use them to detect user movement with much less energy and then start Bluetooth scans to detect user contacts. By conducting experiments on smartphones, we discover three relationships between user movement and user contact changes. According to these relationships, we propose a Mobility-Assisted User Contact detection algorithm (MAUC), which triggers Bluetooth scans only when user movements have a high possibility to cause contact changes. Moreover, we propose energy-aware MAUC (E-MAUC) to further reduce energy consumption during Bluetooth discovery, while keeping the same detection accuracy as MAUC. Via trace driven simulations, we show that MAUC can reduce the number of Bluetooth scans by half while maintaining similar contact detection rates compared to existing algorithms, and E-MAUC can further reduce the energy consumption by 45% compared to MAUC.
[Accelerometers, Bluetooth, Accuracy, Power demand, mobile computing, Educational institutions, social networking (online), mobile social networks, smartphones, mobility-assisted energy-aware user contact detection, Detection algorithms, Smart phones]
Energy-Aware Web Browsing in 3G Based Smartphones
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Smartphone based web browsing wastes a lot of power when downloading webpages due to the special characteristics of the 3G radio interface. In this paper, we identify these special characteristics, and address power consumption issues through two novel techniques. First, we reorganize the computation sequence of the web browser when loading a webpage, so that the web browser can first run the computations that will generate new data transmissions and retrieve these data from the web server. Then, the web browser can put the 3G radio interface into low power state, release the radio resource, and then run the remaining computations. Second, we introduce a practical data mining based method to predict the user reading time of webpages, based on which the smartphone can switch to low power state when the reading time is longer than a threshold. To demonstrate the effectiveness of our energy-aware approaches, we develop a testbed with Android phones on T-Mobile UMTS network. Experimental results show that our approach can reduce the power consumption of smartphone by more than 30% during web browsing, and reduce the webpage loading time by 17%.
[energy-aware Web browsing, 3G mobile communication, data mining, Switches, HTML, T-Mobile UMTS network, computation sequence, mobile computing, power aware computing, Loading, data transmissions, 3G radio interface, Web page loading, Data communication, Web server, energy-aware approaches, telecommunication power management, 3G based smartphones, smart phones, Browsers, Layout, Web browser, power consumption reduction, user reading time prediction, Android phones, Internet, Smart phones]
Datacenters as Controllable Load Resources in the Electricity Market
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Data centers, being major consumers of power, can play an important role in the efficient operation of electrical grids. This paper develops an optimization framework to allow data centers to operate as controllable load resources within the demand dispatch regime, a demand response (DR) program in which incentives are designed to induce lower electricity use not just during times of high prices but also when the reliability of the local grid is jeopardized or when the electricity supply and demand are unbalanced. Assuming the availability of geographically distributed and virtualized data centers situated in multiple regional electrical markets, the basic idea is to migrate the workload in the form of virtual machines (VMs) between these centers to maximize the expected payoff. The proposed framework addresses issues specific to the demand dispatch of data centers such as timeliness of VM migrations and the impact of geographic distance on migration times. It also explicitly incorporates risks that may cause the load curtailment operation to be ultimately unsuccessful and result in monetary losses to data center operators; specifically, variability in network bandwidth that can cause uncertainty in VM migration times as well as the uncertain payoff when participating in DR markets. A set of case studies involving datacenters participating in an economic DR program is used to validate the framework.
[electricity supply, reliability, virtualisation, Servers, load dispatching, Optimization, monetary loss, deflation (monetary), optimisation, incentive, Electricity, optimization, controllable load resource, Real-time systems, power markets, demand response market, power grids, geographic distance, data center operator, demand response program, risk-aware optimization, electrical grid, demand dispatch, VM migration time, incentive schemes, multiple regional electrical market, expected payoff maximization, network bandwidth, computer centres, power engineering computing, DR market, virtual machine, load curtailment operation, virtual machines, geographically distributed data center, Electricity supply industry, Load management, Datacenters, demand side management, Reliability, demand dispatch regime, pricing, virtualized data center]
A Scalable Conflict-Free Replicated Set Data Type
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Replication of state is the fundamental approach to achieve scalability and availability. In order to maintain or restore replica consistency under updates, some form of synchronization is needed. Conflict-free Replicated Data Types (CRDTs) ensure eventual consistency, such that replicas converge to a common state, equivalent to a correct sequential execution without foreground synchronization. A particular CRDT is the set data type, which is a pervasive abstraction for storing collections of unique elements and constitutes an important building block for other, more complex data structures. Since the original specification is not scalable, we improve it by introducing an efficient algorithm for sending deltas of updates between replicas and by partitioning a set replica into disjunctive subsets. We further add support for limited-lifetime elements, which, in turn, enable simple garbage collection strategies to address the problem of unbounded database growth. Lastly, implementation details and evaluation results of a client library for this data structure are presented.
[data scalability, replicated databases, Radiation detectors, Scalability, Data structures, Vectors, Synchronization, synchronisation, scalable conflict-free replicated set data type, data replication, Distributed databases, data availability, eventual consistency, synchronization, distributed systems, data structures, Payloads]
No Hot Spot Non-blocking Skip List
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
This paper presents a new non-blocking skip list algorithm. The algorithm alleviates contention by localizing synchronization at the least contended part of the structure without altering consistency of the implemented abstraction. The key idea lies in decoupling a modification to the structure into two stages: an eager abstract modification that returns quickly and whose update affects only the bottom of the structure, and a lazy selective adaptation updating potentially the entire structure but executed continuously in the background. On SPECjbb as well as on micro-benchmarks, we compared the performance of our new non-blocking skip list against the performance of the JDK non-blocking skip list. The results indicate that our implementation can me more than twice as fast as the JDK skip list.
[lazy selective adaptation, lock-freedom, Java, Dictionaries, eager abstract modification, Poles and towers, data structure, Data structures, Complexity theory, contention, Synchronization, database management systems, nonblocking skip list algorithm, synchronisation, concurrency control, Abstracts, synchronization, data structures, SPECjbb nonblocking skip list, JDK nonblocking skip list]
Mindicators: A Scalable Approach to Quiescence
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
We introduce the Mindicator, a new shared object that is optimized for querying the minimum value of a set of values proposed by several processes. A mindicator may hold at most one value per process. This interface is designed for use in shared memory runtime systems, such as garbage collectors, software transactional memory (TM), and operating system kernels. We introduce linearizable and relaxed mindicator implementations, both of which are lock-free. Our algorithms employ a tree structure, where querying the minimum element takes constant time, and adding and removing elements from the set does not hinder scalability. In microbenchmarks and a synthetic TM workload, we show that both provide good scalability on the x86 and SPARC platforms.
[transaction processing, lock-freedom, Scalability, SPARC platform, operating system kernel, tree structure, concurrent data structures, garbage collector, Data structures, Educational institutions, scalable approach, Synchronization, linearizability, software transactional memory, storage management, Runtime, shared memory runtime system, mindicator, x86, shared memory systems, quiescence, synchronization, tree data structures, Time complexity]
Non-blocking Patricia Tries with Replace Operations
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
This paper presents a non-blocking Patricia trie implementation for an asynchronous shared-memory system using Compare&amp;Swap. The trie implements a linearizable set and supports three update operations: insert adds an element, delete removes an element and replace replaces one element by another. The replace operation is interesting because it changes two different locations of trie atomically. If all update operations modify different parts of the trie, they run completely concurrently. The implementation also supports a wait-free find operation, which only reads shared memory and never changes the data structure. Empirically, we compare our algorithms to some existing set implementations.
[Algorithm design and analysis, concurrent data structure, Java, multiprocessing programs, set, lock-free, replace operations, data structure, Binary search trees, Search problems, non-blocking, Compare&amp;Swap, Patricia trie, dictionary, asynchronous shared-memory system, Vegetation, shared memory systems, nonblocking Patricia tries, tree data structures, Arrays, shared memory]
MoLoc: On Distinguishing Fingerprint Twins
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Indoor localization has enabled a great number of mobile and pervasive applications, attracting attentions from researchers worldwide. Most of current solutions rely on Received Signal Strength (RSS) of wireless signals as location fingerprint, to discriminate locations of interest. Fingerprint uniqueness with respect to locations is a basic requirement in these fingerprinting-based solutions. However, due to insufficient number of signal sources, temporal variations of wireless signals, and rich multipath effects, such requirement is not always met in complex indoor environments, which we refer to as fingerprint ambiguity. In this work, we explore the potential of leveraging user motion against fingerprint ambiguity. Our basic idea is that user motion patterns collected by built-in sensors of mobile phones add to the diversity built by RSS fingerprints. On this basis, we propose MoLoc, a motion-assisted localization scheme implemented on mobile phones. MoLoc can easily be integrated in existing localization systems by simply adding a motion database that is constructed automatically by crowdsourcing. We conducted experiments in a large office hall. The experiment results show that MoLoc doubles the localization accuracy achieved by the fingerprinting method, and limits the mean localization error to less than 1m.
[fingerprint identification, wireless signals, mobile phones, Fingerprint recognition, localization accuracy, Mobile handsets, indoor localization, database management systems, Indoor Localization, RSS Fingerprint, Accuracy, mobile computing, Databases, multipath effects, mobile applications, received signal strength, User Motion, indoor environments, Sensors, motion-assisted localization scheme, Crowdsourcing, IEEE 802.11 Standards, motion database, pervasive applications, fingerprint ambiguity, MoLoc scheme, fingerprinting-based solutions, built-in sensors, temporal variations, mean localization error, user motion pattern collection, signal sources, sensors, crowdsourcing approach, Hidden Markov models, RSS fingerprints, mobile handsets, office hall]
Pilot: Passive Device-Free Indoor Localization Using Channel State Information
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Many emerging applications such as intruder detection and border protection drive the fast increasing development of device-free passive (DfP) localization techniques. In this paper, we present Pilot, a Channel State Information (CSI)-based DfP indoor localization system in WLAN. Pilot design is motivated by the observations that PHY layer CSI is capable of capturing the environment variance due to frequency diversity of wideband channel, such that the position where the entity located can be uniquely identified by monitoring the CSI feature pattern shift. Therefore, a ``passive'' radio map is constructed as prerequisite which include fingerprints for entity located in some crucial reference positions, as well as clear environment. Unlike device-based approaches that directly percepts the current state of entities, the first challenge for DfP localization is to detect their appearance in the area of interest. To this end, we design an essential anomaly detection block as the localization trigger relying on the CSI feature shift when entity emerges. Afterwards, a probabilistic algorithm is proposed to match the abnormal CSI to the fingerprint database to estimate the positions of potential existing entities. Finally, a data fusion block is developed to address the multiple entities localization challenge. We have implemented Pilot system with commercial IEEE 802.11n NICs and evaluated the performance in two typical indoor scenarios. It is shown that our Pilot system can greatly outperform the corresponding best RSS-based scheme in terms of anomaly detection and localization accuracy.
[radio direction-finding, Wireless LAN, anomaly detection block, Correlation, channel state information, sensor fusion, IEEE 802.11n NIC, Channel State Information, Device-free Indoor localization, pilot system, CSI feature pattern shift monitoring, Radio frequency, fingerprint database, device-free passive indoor localization technique, DfP indoor localization system, passive radio map, wireless channels, Channel state information, Estimation, probability, performance evaluation, WLAN, position estimation, monitoring, PHY layer, data fusion block, RSS, intruder detection, probabilistic algorithm, Physical Layer, Feature extraction, Frequency diversity, frequency diversity wideband channel, wireless LAN, RSS-based scheme, border protection drive]
Harnessing Mobile Multiple Access Efficiency with Location Input
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Benefiting from the abundant sensor hints of current mobile devices, their location information has become pervasively available and easy to access. As smartphones and tablets have become recently the main access devices of 802.11-based WLAN, location information provides large opportunity to improve the performance of underlying wireless communication. This paper presents CO-MAP (Co-Occurrence MAP) which leverages the position of devices to handle exposed and hidden terminal problems in mobile WLANs so as to improve the multiple access efficiency. With the location information, CO-MAP rapidly builds a co-occurrence map showing which two links can occur concurrently. Meanwhile, it selects the best settings of frame transmissions based on a novel analytical network model when hidden terminals are distinguished. CO-MAP improves the goodputs of both downlinks and uplinks in instant and distributed manner. Our implementation on a testbed of 6 laptops demonstrates that CO-MAP can accurately detect potential hidden and exposed interferers, and provide a large gain of goodput for both exposed terminal and hidden terminal scenarios. The simulation results of large scale networks on NS-2 also suggest that imperfect position hints can still bring substantial improvement on the multiple access efficiency.
[frame transmissions, Wireless LAN, Noise, location information, Mobile communication, Multiaccess communication, NS-2, wireless communication, mobile computing, multiple access, CO-MAP, smartphones, discrete event simulation, mobile radio, IEEE 802.11 Standards, location input, sensor hints, Receivers, Interference, exposed terminal, smart phones, mobile WLAN, hidden terminal, mobile devices, location, wireless LAN, tablets, mobile multiple access efficiency, 802.11-based WLAN, co-occurrence MAP]
Location Privacy Preserving Dynamic Spectrum Auction in Cognitive Radio Network
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Dynamic spectrum auction offers the flexibility and capability for bidders to request and acquire unoccupied channels from spectrum license holders. Compared with the conventional auction, spectrum auction allows various buyers to utilize the same channel simultaneously based on their locations, which is denoted as spectrum reusability. In this paper, we consider a novel kind of attack, which could compromise location privacy of bidders by observing the bid items as well as bid price. To thwart this attack, we introduce a new Location Privacy Preserving Dynamic Spectrum Auction (LPPA) scheme which consists of two components: Privacy Preserving Bid Submission protocol (PPBS) and Private Spectrum Distribution protocol (PSD). Based on the prefix membership verification scheme, PPBS allows the auctioneer to construct the conflict relationship between different users and obtain the maximum value of bids on various channels without leaking users' location information. Furthermore, PSD is proposed to efficiently distribute the spectrum among bidders and securely charge the winners with the help of periodically available TTP (Trusted Third Party). To demonstrate the effectivenss of the proposed scheme, we implement our attack and scheme on data extracted from Google Earth Coverage Maps released by FCC. The experiment results show the efficacy and efficiency of our approach.
[bidders, Protocols, dynamic spectrum auction, Heuristic algorithms, private spectrum distribution protocol, trusted third party, Privacy, TTP, Databases, cognitive radio, security of data, Google Earth Coverage Maps, Prefix Membership Verification, Channel estimation, Dynamic Spectrum Auction, location privacy preserving, data privacy, Location Privacy, Cryptography, privacy preserving bid submission protocol, cognitive radio network]
Achieving High-Throughput State Machine Replication in Multi-core Systems
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
The traditional architecture used by implementations of Replicated State Machines (RSM) does not fully exploit modern multi-core CPUs. This is increasingly the limiting factor in their performance, because network speeds are increasing much faster than the single-thread performance of CPUs. Thus, when deployed on Gigabit-class networks and exposed to a workload of small to medium size client requests, RSMs are often CPU-bound, as they are only able to leverage a few cores, even though many more may be available. In this work, we revisit the traditional architecture of a RSM implementation, showing how it can be parallelized so that its performance scales with the number of cores in the nodes. We do so by applying several good practices of concurrent programming to the specific case of state machine replication, including staged execution, workload partitioning, actors, and non-blocking data structures. We describe and test a Java prototype of our architecture, based on the Paxos protocol. With a workload consisting of small requests, we achieve a six times improvement in throughput using eight cores. More generally, in all our experiments we have consistently reached the limits of the network subsystem by using up to 12 cores, and do not observe any degradation when using up to 24 cores. Furthermore, the profiling results of our implementation show that even at peak throughput contention between threads is minimal, suggesting that the throughput would continue scaling given a faster network.
[Protocols, limiting factor, concurrent programming, Throughput, Servers, finite state machines, Java prototype, small to medium size client requests, multicore, scalability, software architecture, eight cores, Gigabit-class networks, Paxos protocol, RSM implementation, Computer architecture, Parallel processing, network subsystem, staged execution, data structures, protocols, multicore CPU, Message systems, Java, multiprocessing systems, state machine replication, CPU-bound, network speeds, multicore systems, workload partitioning, peak throughput contention, concurrency control, replicated state machines, nonblocking data structures, Paxos]
Order-Preserving Renaming in Synchronous Systems with Byzantine Faults
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Renaming is a fundamental problem in distributed computing. It consists in having a set of processes with unique ids from a large namespace pick distinct names from a smaller namespace. Order-preserving renaming is a stronger variant of the renaming problem where the new names are required to preserve the ordering of the initial ids. This paper addresses order-preserving renaming in synchronous message passing systems with Byzantine failures. Although in this model order-preserving renaming can be solved by using consensus, it is known that this problem is "weaker" than consensus. Therefore, we are interested in designing algorithms that are more efficient than consensus-based solutions. This paper makes three contributions in this direction. We present an order-preserving renaming algorithm with N &gt; 3t resiliency, a target namespace of size N + t -- 1, and O(log N ) round complexity (N is the number of processes and t is an upper bound on the number of faults). We also show that with N &gt; t2 +2t our algorithm can be modified to have constant round complexity while achieving tight namespace of size N. Finally, we present an algorithm that solves order-preserving renaming in just 2 communication rounds with N &gt; 2t2 + t.
[Algorithm design and analysis, message passing, order-preserving renaming algorithm, Byzantine failures, Computer crashes, Byzantine faults, Complexity theory, Approximation methods, synchronous systems, O(log N) round complexity, Message passing, consensus-based solutions, Approximation algorithms, fault tolerant computing, synchronous message passing systems, Arrays, computational complexity]
Hybrid Replication: State-Machine-Based and Deferred-Update Replication Schemes Combined
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
We propose a novel algorithm for hybrid transactional replication (HTR) of highly dependable services. It combines two schemes: a transaction is executed either optimistically by only one service replica in the deferred update mode (DU), or deterministically by all replicas in the state machine mode (SM); the choice is made by an oracle. The DU mode allows for parallelism and thus takes advantage of multicore hardware. In contrast to DU, the SM mode guarantees abort-free execution, so it is suitable for irrevocable operations and transactions generating high contention. For expressiveness, transactions can be discarded or retried on demand. We developed HTR-enabled Paxos STM, an object-based distributed transactional memory system, and evaluated it using several benchmarks: Bank, Distributed STMBench7, and Twitter Clone. We tested our system under various workloads and three oracle types: DU and SM, which execute all transactions in one mode, and Hybrid -- tailored specifically for each benchmark -- which selects a mode for each transaction dynamically based on various parameters. In all our tests, the Hybrid oracle is not worse than DU and SM and outperforms them when the number of replicas grows.
[transaction processing, Bank system, Protocols, state-machine-based update replication scheme, object-based distributed transactional memory system, HTR, Programming, finite state machines, hybrid transactional replication, deferred-update replication scheme, HTR-enabled Paxos STM, Parallel processing, Message systems, Context, state machine mode, deferred update mode, deferred update, Object oriented modeling, state machine replication, SM mode, Computer crashes, distributed transactional memory, Distributed STMBench7 system, transactional replication, DU mode]
RBFT: Redundant Byzantine Fault Tolerance
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Byzantine Fault Tolerant state machine replication (BFT) protocols are replication protocols that tolerate arbitrary faults of a fraction of the replicas. Although significant efforts have been recently made, existing BFT protocols do not provide acceptable performance when faults occur. As we show in this paper, this comes from the fact that all existing BFT protocols targeting high throughput use a special replica, called the primary, which indicates to other replicas the order in which requests should be processed. This primary can be smartly malicious and degrade the performance of the system without being detected by correct replicas. In this paper, we propose a new approach, called RBFT for Redundant-BFT: we execute multiple instances of the same BFT protocol, each with a primary replica executing on a different machine. All the instances order the requests, but only the requests ordered by one of the instances, called the master instance, are actually executed. The performance of the different instances is closely monitored, in order to check that the master instance provides adequate performance. If that is not the case, the primary replica of the master instance is considered malicious and replaced. We implemented RBFT and compared its performance to that of other existing robust protocols. Our evaluation shows that RBFT achieves similar performance as the most robust protocols when there is no failure and that, under faults, its maximum performance degradation is about 3%, whereas it is at least equal to 78% for existing protocols.
[Byzantine fault tolerance, Protocols, fault tolerance, robust protocols, Throughput, arbitrary faults, finite state machines, Degradation, redundant byzantine fault tolerance, state machine replication protocols, RBFT, Robustness, Delays, Spinning, protocols, Monitoring, replicated state machine]
Characterizing Information Diffusion in Online Social Networks with Linear Diffusive Model
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Mathematical modeling is an important approach to study information diffusion in online social networks. Prior studies have focused on the modeling of the temporal aspect of information diffusion. A recent effort introduced the spatiotemporal diffusion problem and addressed the problem with a theoretical framework built on the similarity between information propagation in online social networks and biological invasion in ecology [1]. This paper examines the spatio-temporal characteristics in further depth and reveals that there exist regularities in information diffusion in temporal and spatial dimensions. Furthermore, we propose a simpler linear partial differential equation that takes account of the influence of spatial population density and temporal decay of user interests in the information. We validate the proposed linear model with Digg news stories which received more than 3000 votes during June 2009, and show that the model can describe nearly 60% of the news stories with over 80% accuracy. We also use the most popular news story as a case study and find that the linear diffusive model can achieve an accuracy as high as 97:41% for this news story. Finally, we discuss the potential applications of this model towards finding super spreaders and classifying news story into groups.
[linear partial differential equation, online social networks, Predictive models, PDE, Accuracy, ecology, news story classifications, Mathematical model, linear diffusive model, information diffusion, online social network, Digg news stories, information propagation, information diffusion characterization, spatiotemporal diffusion problem, Social network services, Biological system modeling, information dissemination, Diffusion processes, information diffusion temporal aspect, biological invasion, mathematical modeling, social networking (online), partial differential equations, user interests, Logistics, spatio-temporal]
Maximizing the Spread of Positive Influence in Online Social Networks
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Online social networks (OSNs) provide a new platform for product promotion and advertisement. Influence maximization problem arisen in viral marketing has received a lot of attentions recently. Most of the existing diffusion models rely on one fundamental assumption that an influenced user necessarily adopts the product and encourages his/her friends to further adopt it. However, an influenced user may be just aware of the product. Due to personal preference, neutral or negative opinion can be generated so that product adoption is uncertain. Maximizing the total number of influenced users is not the uppermost concern, instead, letting more activated users hold positive opinions is of first importance. Motivated by above phenomenon, we proposed a model, called Opinion-based Cascading (OC) model. We formulate an opinion maximization problem on the new model to take individual opinion into consideration as well as capture the change of opinions at the same time. We show that under the OC model, opinion maximization is NP-hard and the objective function is no longer submodular. We further prove that there does not exist any approximation algorithm with finite ratio unless P=NP. We have designed an efficient algorithm to compute the total positive influence based on this new model. Comprehensive experiments on real social networks are conducted, and results show that previous methods overestimate the overall positive influence, while our model is able to distinguish between negative opinions and positive opinions, and estimate the overall influence more accurately.
[Greedy algorithms, Algorithm design and analysis, Opinion, online social networks, OC model, optimisation, influence maximization problem, viral marketing, product awareness, opinion maximization problem, advertisement, approximation algorithm, Polynomials, positive influence spread, objective function, opinion-based cascading model, advertising data processing, Diffusion Model, Social network services, Computational modeling, Viral marketing, OSN, negative opinions, Algorithm, NP-hard problem, Social Networks, directed graphs, positive opinions, Approximation algorithms, social networking (online), Time complexity, diffusion models, product promotion, computational complexity]
Message in a Sealed Bottle: Privacy Preserving Friending in Social Networks
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Many proximity-based mobile social networks are developed to facilitate connections between any two people, or to help a user to find people with a matched profile within a certain distance. A challenging task in these applications is to protect the privacy of the participants' profiles and personal interests. In this paper, we design novel mechanisms, when given a preference-profile submitted by a user, that search persons with matching-profile in decentralized multi-hop mobile social networks. Our mechanisms also establish a secure communication channel between the initiator and matching users at the time when the matching user is found. Our rigorous analysis shows that our mechanism is privacy-preserving (no participants' profile and the submitted preference-profile are exposed), verifiable (both the initiator and the unmatched user cannot cheat each other to pretend to be matched), and efficient in both communication and computation. Extensive evaluations using real social network data, and actual system implementation on smart phones show that our mechanisms are significantly more efficient than existing solutions.
[personal interest, Protocols, pattern matching, privacy preserving friending, profile matching, Social network services, Private Profile Matching, Lattices, participant profile privacy protection, smart phone, Vectors, smart phones, Secure Communication, decentralized multihop mobile social network, Relays, Privacy, mobile computing, social networking (online), data privacy, Decentralized Mobile Social Networks, proximity-based mobile social network, preference profile, Cryptography, secure communication channel]
Gathering of Mobile Robots Tolerating Multiple Crash Faults
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
We study distributed coordination among autonomous mobile robots, focussing on the problem of gathering the robots at a single location. The gathering problem has been solved previously using deterministic algorithms even for robots that are anonymous, oblivious, disoriented, and operate in the semi-synchronous ATOM model. However these solutions require all robots to be fault-free. The recent results of Agmon and Peleg [1] show how to gather all correct robots when one of the robots may crash permanently. We study gathering in n-robot systems with f crashes for any f &lt;; n. In such a scenario, no robot can wait for another robot, i.e., the algorithm must be wait-free. We provide such a wait-free algorithm to gather all correct robots assuming the capabilities of strong multiplicity detection and chirality. Unlike previous solutions, our algorithm does not impose the requirement of initially distinct locations, and works for any arbitrary initial configuration of robots (except the bivalent configuration where deterministic gathering is not possible).
[Algorithm design and analysis, Oblivious, autonomous mobile robots gathering, multi-robot systems, mobile robots, Deterministic, Anonymous, Mobile robots, multiplicity detection, Mobile Robots, Robot sensing systems, multiple crash fault toleration, fault tolerance, Computational modeling, wait-free algorithm, chirality, Distributed Coordination, Computer crashes, deterministic algorithms, n-robot systems, Robot kinematics, Gathering, distributed coordination, semisynchronous ATOM model, Fault Tolerance]
Ring Exploration by Oblivious Agents with Local Vision
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
The problem of exploring a discrete environment by identical oblivious asynchronous agents (or robots) devoid of direct means of communication has been well investigated so far. The (terminating) exploration requires that starting from a configuration where no two agents occupy the same node, every node needs to be visited by at least one agent, with the additional constraint that all agents eventually stop moving. Agents have sensors that allow them to see their environment and move accordingly. The previous works on this problem assume agents having an unlimited visibility, that is, they can sense the agents on every node of the ring, whatever the ring size. In this paper, we address deterministic exploration in an anonymous, unoriented ring using oblivious, and myopic agents. By myopic, we mean that their visibility is limited in terms of sensing distance. We consider the strongest possible myopia that is, an agent can only sense agents located at its own and at its immediate neighboring nodes. Our contribution is threefold. We first prove that within such settings, no deterministic exploration is possible in the semi-synchronous model. The result is also valid for the (fully) asynchronous model and holds for any k 6. Finally, we provide optimal (in terms of number of agents) deterministic algorithms in the fully synchronous model for both cases 3 6.
[Protocols, multi-robot systems, deterministic exploration, unlimited visibility, Limited Visibility, sense agents, identical oblivious asynchronous agents, Anonymous Oblivious Agents, Abstracts, Bismuth, Sensors, robots, Mirrors, ring exploration, terminating exploration, asynchronous model, Discrete Environment, Computational modeling, sensing distance, Educational institutions, robot vision, deterministic algorithms, oblivious agents, Deterministic Exploration, neighboring nodes, local vision, discrete environment, semisynchronous model, myopic agents]
Zigzag: Local-Information-Based Self-Optimizing Routing in Virtual Grid Networks
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
In this paper, we present a local-information-based self-optimizing routing protocol Zigzag in virtual grid networks. A virtual grid network is obtained by virtually dividing a wireless network into a grid of geographical square regions called cells, and is used in MANETs and sensor networks to reduce energy consumption. A single node is selected as a router at each cell and inter-cell communication is realized by using the routers. Other nodes in the cell have no responsibility for inter-cell communication and can become inactive to save energy consumption. We consider maintenance of an inter-cell communication path to a destination node from its source node. When the destination node moves to a cell next to the current one, the path can be simply updated by extending it to the next cell. But, if the destination node moves around the network, the path becomes redundantly long and needs to be shortened. In this paper, we propose a self-optimizing routing protocol Zigzag in virtual grid networks, which can transform any given inter-cell path to a shortest (or minimum-hop) one by repeatedly applying local updates on the path. The routers locally and asynchronously update the path based only on local information and require no global information of the path such as the locations of the destination and the source nodes. We also show that the convergence time to a shortest path from any given path P is O(|P|) in the synchronous execution where |P| is the length (or the number of hops) of P.
[Self-optimization, Zigzag, Redundancy, local-information-based self-optimizing routing protocol, Virtual grid network, Routing, Routing protocol, Optimization, Convergence, MANET, Wireless networks, reduce energy consumption, Local information, mobile ad hoc networks, routing protocols, intercell communication, Wireless network, Routing protocols, geographical square regions, virtual grid networks]
Distributed Ranked Data Dissemination in Social Networks
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
The amount of content served on social networks can overwhelm users, who must sift through the data for relevant information. To facilitate users, we develop and implement dissemination of ranked data in social networks. Although top-k computation can be performed centrally at the user, the size of the event stream can constitute a significant bottleneck. Our approach distributes the top-k computation on an overlay network to reduce the number of events flowing through. Experiments performed using real Twitter and Facebook datasets with 5K and 30K query subscriptions demonstrate that social workloads exhibit properties that are advantageous for our solution.
[Algorithm design and analysis, event stream size, query subscriptions, publish/subscribe, data analysis, Social network services, information dissemination, Subscriptions, top-k, social networks, overlay network, distributed processing, top-k computation, Facebook datasets, data dissemination, query processing, Overlay networks, social workloads, Semantics, Distributed databases, Twitter datasets, distributed ranked data dissemination, social networking (online), Feeds]
Optimistic Atomic Multicast
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Message ordering is one of the cornerstones of reliable distributed systems. However, some ordering guarantees, such as atomic order, are expensive to implement in terms of message delays. This paper presents Optimistic Atomic Multicast, a protocol that combines reduced latency and increased throughput. Messages can be delivered optimistically in a single communication step and conservatively in three communication steps. Differently from previous optimistic group communication protocols, Optimistic Atomic Multicast does not rely on spontaneous message ordering for fast delivery. In addition to presenting Optimistic Atomic Multicast, we provide detailed performance results comparing it to other ordering protocols in both local-area and wide-area networks.
[message delays, atomic order, reduced latency, Multicast communication, Multicast protocols, Throughput, ordering guarantees, wide-area network, spontaneous message ordering, ordering protocols, local-area network, reliable distributed systems, optimistic atomic multicast, optimistic delivery, quasi-genuine, atomic multicast, multicast protocols, telecommunication network reliability, optimistic group communication protocols, fifo reliable multicast, Delays, Reliability, Clocks]
ImageElves: Rapid and Reliable System Updates in the Cloud
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Virtualization has significantly reduced the cost of creating a new virtual machine and cheap storage allows VMs to be turned down when unused. This has led to a rapid proliferation of virtual machine images, both active and dormant, in the data center. System management technologies have not been able to keep pace with this growth and the management cost of keeping all virtual machines images, active as well as dormant, updated is significant. In this work, we present ImageElves, a system to rapidly, reliably and automatically propagate updates (e.g., patches, software installs, compliance checks) in a data center. ImageElves analyses all target images and creates reliable image patches using a very small number of online updates. Traditionally, updates are applied by taking the application offline, applying updates, and then restoring the application, a process that is unreliable and has an unpredictable downtime. With ImageElves, we propose a two phase process. In the first phase, images are analyzed to create an update signature and update manifest. In the second phase, downtime is taken and the manifest is applied offline on virtual images in a parallel, reliable and automated manner. This has two main advantages, (i) spontaneously apply updates to already dormant VMs, and (ii) all updates following this process are guaranteed to work reliably leading to reduced and predictable downtimes. ImageElves uses three key ideas: (i) a novel per-update profiling mechanism to divide VMs into equivalence classes, (ii) a background logging mechanism to convert updates on live instances into patches for dormant images, and (iii) a cross-difference mechanism to filter system-specific or random information (e.g., host name, IP address), while creating equivalence classes. We evaluated the ability of ImageElves to speed up mix of popular system management activities and observed upto 80% smaller update times for active instances and upto 90% reduction in update time for dormant instances.
[ImageElves, software reliability, Manuals, virtualisation, Servers, image analysis, cloud, random information filtering, system management activities, Operating systems, cloud computing, Testing, cost reduction, virtualization, Virtual machining, computer centres, system update reliability, virtual machine, data center, cross-difference mechanism, per-update profiling mechanism, virtual machines, VM, virtual images, background logging mechanism, Reliability, system-specific information filtering, Virtualization, equivalence classes, system management technologies]
Dynamic Cloud Resource Reservation via Cloud Brokerage
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Infrastructure-as-a-Service clouds offer diverse pricing options, including on-demand and reserved instances with various discounts to attract different cloud users. A practical problem facing cloud users is how to minimize their costs by choosing among different pricing options based on their own demands. In this paper, we propose a new cloud brokerage service that reserves a large pool of instances from cloud providers and serves users with price discounts. The broker optimally exploits both pricing benefits of long-term instance reservations and multiplexing gains. We propose dynamic strategies for the broker to make instance reservations with the objective of minimizing its service cost. These strategies leverage dynamic programming and approximate algorithms to rapidly handle large volumes of demand. Our extensive simulations driven by large-scale Google cluster-usage traces have shown that significant price discounts can be realized via the broker.
[Cloud Brokerage, approximation theory, infrastructure-as-a-service clouds, Heuristic algorithms, pricing options, dynamic programming, dynamic cloud resource reservation, Equations, approximate algorithms, Cloud Computing, Aggregates, large-scale Google cluster-usage traces, Pricing, Cloud Resource Reservation, Prediction algorithms, Approximation algorithms, cloud brokerage service, Dynamic programming, cloud providers, cloud computing, pricing]
Present or Future: Optimal Pricing for Spot Instances
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
The recent years witnessed rapid emergence and proliferation of cloud computing. To fully utilize the compute resources, some cloud operators provide spot resources. Spot resources allow customers to bid on unused capacity. However, pricing policy of spot resources should be carefully designed and the impact on both present and future should be considered. For the present, the cloud provider can set a higher price to gain extra revenue. For the future, higher price will shift more requests with lower prices to later time and reduce the revenue of future. Meanwhile, the quality of service should be considered either since bad QoS will incur loss of potential users. In this paper, we present a demand curve to model the impact of pricing on the present and future revenue. Then we formulate the revenue maximization problem as a time-average optimization problem. Next, since this basic model fails to provide information of service delay, we extend it to a more generalized one that ensures the worst-case delay of user requests. While the future knowledge of arrival requests is unknown, it is necessary to design online algorithms for the optimization problems. We apply Lyapunov optimization framework and design an efficient online algorithm which dose not require any future knowledge of requests arrival. Evaluations based on real-life datacenter workload and Amazon EC2 Spot Price illustrate efficiency of our algorithms.
[Algorithm design and analysis, Cloud computing, user requests, real-life datacenter workload, worst-case delay, cloud provider, Quality of service, pricing policy, Optimization, spot resources, optimisation, QoS, Pricing, cloud computing, spot instances, revenue maximization problem, Educational institutions, quality of service, computer centres, online algorithms, cloud operators, optimal pricing, time-average optimization problem, unused capacity, Delays, Amazon EC2 spot price, pricing]
SmartDPSS: Cost-Minimizing Multi-source Power Supply for Datacenters with Arbitrary Demand
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
To tackle soaring power costs, significant carbon emission and unexpected power outage, Cloud Service Providers (CSPs) typically equip their Datacenters with a Power Supply System (DPSS) nurtured by multiple sources: (1) smart grid with time-varying electricity prices, (2) uninterrupted power supply (UPS), and (3) renewable energy with intermittent and uncertain supply. It remains a significant challenge how to operate among multiple power supply sources in a complementary manner, to deliver reliable energy to datacenter users with arbitrary demand over time, while minimizing a CSP's operation cost over the long run. This paper proposes an efficient, online control algorithm for DPSS, SmartDPSS, based on the two-timescale Lyapunov optimization techniques. Without requiring a priori knowledge of system statistics, SmartDPSS allows CSPs to make online decisions on how much power demand, including delay-sensitive demand and delay-tolerant demand, to serve at each time, the amount of power to purchase from the long-term-ahead and realtime grid markets, and charging and discharging of UPS over time, in order to fully leverage the available renewable energy and time-varying prices from the grid markets, for minimum operational cost. We thoroughly analyze the performance of our online control algorithm with rigorous theoretical analysis. We also demonstrate its optimality in terms of operational cost, demand service delay, datacenter availability, system robustness and scalability, using extensive simulations based on one-month worth of traces from live power systems.
[online decisions making, carbon emission, SmartDPSS, system robustness, smart power grids, Batteries, online control algorithm, Optimization, power outage, computer power supplies, renewable energy, power aware computing, datacenters-with-a-power supply system, intermittent supply, system scalability, UPS, cloud computing, time-varying electricity prices, CSP operation cost minimization, Power demand, cloud service providers, smart grid, renewable energy sources, live power systems, datacenter availability, power supply sources, uninterrupted power supply, computer centres, Uninterruptible power systems, realtime grid markets, Renewable energy sources, demand service delay, uncertain supply, delay-tolerant demand, delay-sensitive demand, cost-minimizing multisource power supply, Delays, two-timescale Lyapunov optimization techniques, power costs, power demand]
Longer Is Better: Exploiting Path Diversity in Data Center Networks
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Data Center (DC) networks exhibit much more centralized characteristics than the legacy Internet, yet they are operated by similar distributed routing and control algorithms that fail to exploit topological redundancy to deliver better and more sustainable performance. Multipath protocols, for example, use node-local and heuristic information to only exploit path diversity between shortest paths. In this paper, we use a measurement-based approach to schedule flows over both shortest and non-shortest paths based on temporal network-wide utilization. We present the Baatdaat flow scheduling algorithm which uses spare DC network capacity to mitigate the performance degradation of heavily utilized links. Results show that Baatdaat achieves close to optimal Traffic Engineering by reducing network-wide maximum link utilization by up to 18% over Equal-Cost Multi-Path (ECMP) routing, while at the same time improving flow completion time by 41% - 95%.
[Schedules, Data Center Networks, multipath protocol, Ports (Computers), Control systems, Throughput, heuristic information, Unequal-cost Multipath, path diversity, Network topology, spare DC network capacity, node-local information, network-wide maximum link utilization, temporal network-wide utilization, Hardware, optimal traffic engineering, measurement-based approach, Baatdaat flow scheduling algorithm, Topology, distributed routing, Traffic Engineering, routing protocols, topological redundancy, Software Defined Networking, Multipath Scheduling, telecommunication traffic, data center network]
Signal-Aware Green Wireless Relay Network Design
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Small cell network is the new trend for next generation mobile network design. One feasible model is using Relay stations (RS) as small cell providers to achieve extended coverage, lower cost, and higher network capacity. This paper studies Signal-aware relay station placement and power allocation problem in wireless relay networks with multiple base stations in the field. This problem consists of both subscriber coverage problem and relay power optimization problem, which have not been extensively studied together in previous works. This work takes into account physical constraints such as channel capacity, signal to noise ratio (SNR) requirement of subscribers, relay power cost and network topology. We set up a two-step goal that is firstly to find minimum number of RS in order to cover all the subscribers meeting each SNR requirement, and then to ensure communications built between any subscriber to a base station. In order to ensure each subscriber's SNR, transmission power of each RS should be adjustable. Thus, minimizing power cost of RSs is our goal in the second step. We divide the problem into two sub-problems, Lower-tier Coverage Relay Allocation (LCRA) problem and Upper-tier Connectivity Relay Allocation (UCRA) problem. For the LCRA problem, we present two approximation solutions based on minimum hitting set and maximum independent set. For the UCRA problem, an approximation algorithm and an optimal algorithm are proposed. At the end, an approximation solution for our original problem, which combines the approaches of the two sub-problems, is provided. Numerical results are presented to confirm the theoretical analysis of our schemes, and to show strong performances of our solutions.
[next generation mobile network design, next generation networks, channel capacity, linear programming, power allocation problem, Relays, Wireless communication, optimisation, resource allocation, UCRA problem, LCRA problem, signal to noise ratio requirement, approximation solutions, small cell network, wireless relay networks, minimum hitting set, optimal algorithm, relay networks (telecommunication), relay power optimization problem, signal-aware relay station placement, subscriber coverage problem, Base stations, approximation theory, Channel capacity, relay power cost, lower-tier coverage relay allocation problem, Topology, maximum independent set, network topology, SNR requirement, Resource management, base stations, upper-tier connectivity relay allocation problem, Signal to noise ratio]
Ease the Queue Oscillation: Analysis and Enhancement of DCTCP
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Because of the terrible performance of TCP protocol in data center environment, DCTCP has been proposed as a TCP replacement, which uses a simple marking mechanism at switches and a few amendments at end hosts to adjust congestion window based on the extent of the congestion in networks. Thus, DCTCP can make a proper tradeoff between high throughput and low latency. However, through our observation, we discover that DCTCP causes severe oscillation of queue under some parameters and network configuration. Our perceptual analysis concludes that the rough single-threshold marking mechanism may be the essential reason. Therefore, we propose Double-Threshold DCTCP as an improvement of DCTCP. Then, by applying describing function method in nonlinear control theory, we analyze the stability of both DCTCP and Double-Threshold DCTCP, and theoretically explain why Double-Threshold DCTCP is more stable than DCTCP. At last, we validate theoretical analysis and conclude that the Double- Threshold DCTCP can achieve smaller queue, and the queue length of Double-Threshold DCTCP is less sensitive to the growing number of flows. Further, Double-Threshold DCTCP can postpone the throughput collapse caused by Incast traffic and reduce the tail latency in completion time experiment.
[DF approach, telecommunication congestion control, Throughput, Incast traffic, data center environment, Analytical models, TCP replacement, Double-Threshold DCTCP, Mathematical model, stability, queueing theory, marking mechanism, TCP protocol, network congestion, stability criterion, function method, nonlinear control theory, Oscillators, queue oscillation, transport protocols, Stability criteria, nonlinear control systems, congestion window, tail latency reduction, Queueing analysis, DCTCP]
Ripple: Improved Architecture and Programming Model for Bulk Synchronous Parallel Style of Analytics
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
We present Ripple, an architecture and a programming model for a broad set of data analytics. Ripple builds on the ideas of iterated MapReduce and adds two innovations. First it has a richer programming model, including more ideas from the Bulk Synchronous Parallel (BSP) model of computation and others. By doing so, Ripple creates a flexible and higher-level platform that is easier for both application programmers and platform implementors. Second, Ripple is based on a limited interface for key/value storage making it portable among many different key/value store implementations. By building on these two ideas Ripple improves the scope, performance, and openness of the data analytics platform. We evaluate Ripple using three representative, and non-trivial, data analysis scenarios requiring iterative computation. Using these examples, we show how Ripple achieves clear performance advantages over iterated MapReduce.
[iterative methods, Programming, parallel programming, MapReduce, BSP model, software architecture, bulk synchronous parallel model, key/value storage, programming model, Distributed databases, Computer architecture, distributed databases, Trademarks, architecture, distributed database, data analysis, data analytics, Computational modeling, iterative computation, Synchronization, Distributed programming, Ripple, Data models, platform implementors, application programmers]
Efficient Geo-distributed Data Processing with Rout
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Big data processing undoubtedly represents a major challenge of this era. While several programming models and supporting systems have been proposed to deal with such data in so-called &#x201C;cloud&#x201D; infrastructures, they all exhibit the same limitation: all data is assumed to be located in one datacenter. This limitation results from cloud vendors promoting the abstraction of omnipresent computing and storage resources. When dealing with data distributed across datacenters, programmers currently have two options: (1) copying all data to a single datacenter easily becomes tedious if done manually as the original dataset is updated, leads to repetitive copying if performed as part of a program, and is sometimes impossible; (2) writing multiple variants of the same program, with consolidation occurring at different points varying by characteristics of the task (e.g., input sub-dataset sizes) is laborious and does not help determining the most appropriate one for a given run. This paper introduces geo-distributed data structures and operations for expressing data processing tasks taking place across datacenters. We describe the design and implementation of such data structures and operations for the PigLatin language. We illustrate the performance benefits of our geodistributed data structures and operations through several benchmarks, showing up to 2&#x00D7; faster response times.
[Schedules, Programming, programming models, supporting systems, Distributed databases, efficient geo-distributed data processing, cloud infrastructures, data structures, Rout, cloud computing, Java, storage resources, data processing tasks, geo-distributed data structures, Big Data, geophysics computing, Data structures, Data processing, datacenter, PigLatin language, computer centres, cloud vendors, Syntactics, big data processing, omnipresent computing abstraction]
Towards an Efficient Online Causal-Event-Pattern-Matching Framework
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Event monitoring and logging, that is, recording the communication events between processes, is a critical component in many highly reliable distributed systems. The event logs enable the identification of certain safety-condition violations, such as race conditions and mutual-exclusion violations, as safety is generally contingent on processes communicating in a specific causally ordered pattern. Previous e orts at finding such patterns have often focused on online techniques, which are unable to identify operational problems as they occur. Online monitoring tools exist but they are often restricted to identifying a specific violation condition, such as a deadlock or a race condition, using dedicated data structures. We address the more general problem of detecting causally related event patterns that can be used to identify various undesired behaviours in the system. The main challenge for online pattern matching is the need to store the partial matches to the pattern, as they may combine with future events to form a complete match. Unlike pattern matching in most other domains, causally ordered patterns can span a potentially unbounded number of events and efficiently searching through this large collection poses a significant challenge. In this paper, we introduce OCEP, an efficient online causalevent- pattern-matching framework that bounds the number of partial matches it stores by reporting only a representative subset of pattern matches. We define a subset of matches as representative if it has at least one occurrence of each event in the pattern on each process, which is applicable for a large class of distributed applications. With this definition, OCEP introduces a backtracking algorithm to efficiently find a representative subset from the history of events. An evaluation of the framework shows that OCEP is capable of handling several frequently occurring violation patterns at the event rates of some representative distributed applications.
[pattern matching, safety-condition violations, Event-Based System, Distributed System Monitoring., distributed processing, Compounds, History, online causal-event-pattern-matching framework, Runtime, mutual-exclusion violations, online pattern matching, backtracking algorithm, distributed systems, Safety, OCEP, online monitoring tools, Monitoring, Distributed System, Causal Ordering, event logging, computerised monitoring, event monitoring, backtracking, race conditions, Pattern matching, Clocks]
S3: Characterizing Sociality for User-Friendly Steady Load Balancing in Enterprise WLANs
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Traffic load is often unevenly distributed among the access points (APs) in enterprise WLANs. Such load imbalance results in sub-optimal network throughput and unfair bandwidth allocation among users. In this paper, we collect real traces from over twelve thousand WiFi users in Shanghai Jiao Tong University. Through intensive data analysis, we find that user behavior like leaving together may cause significant AP load imbalance problem. We also observe from the trace that users with similar application usage have the potential to leave together. Inspired by those observations, we propose an innovative scheme, Social-aware AP Selection Scheme(S3), which can actively learn the sociality information among users trained with their history application profiles and elegantly assign users based on the obtained knowledge. Both real prototype implementation and simulation results show that S3 is feasible and can achieve 41.2% balancing performance gain on average.
[balancing performance gain, IEEE 802.11, Conferences, access points, social-aware AP selection scheme, enterprise WLANs, Distributed computing, telecommunication computing, Shanghai Jiao Tong University, load imbalance, suboptimal network throughput, resource allocation, traffic load, sociality information, user-friendly steady load balancing, history application profiles, social behavior based AP selection, data analysis, bandwidth allocation, real prototype implementation, enterprise WLAN, WiFi users, human computer interaction, innovative scheme, wireless LAN, user behavior, load balanc- ing, telecommunication traffic]
On Server Provisioning for Distributed Interactive Applications
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Increasing geographical spreads of modern distributed interactive applications (DIAs) make distributed server deployment vital for combating network latency and improving the interactivity among participants. In this paper, we investigate the server provisioning problem that concerns where to place servers in DIAs. We formulate the server provisioning problem with an objective of reducing the network latency involved in the interaction between participants. We prove that the problem is NP-hard under any one of the following three scenarios that may be common in practice: (a) the network latency does not satisfy the triangle inequality; or (b) the choices of server locations in the network are restricted; or (c) the number of server locations to select is limited. Then, we propose an efficient greedy server provisioning heuristic, analyze its approximation ratio and give a tight example. Experiments using real Internet latency data show that our proposed algorithm significantly outperforms traditional k-median and k-center server placements.
[approximation ratio, network servers, NP-hard, greedy algorithms, distributed processing, Servers, Approximation methods, communication complexity, distributed interactive applications, Internet latency data, network latency reduction, Collaboration, distributed server deployment, Approximation algorithms, Polynomials, Peer-to-peer computing, Internet, greedy server provisioning heuristic]
Harmony: Dynamic Heterogeneity-Aware Resource Provisioning in the Cloud
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Data centers today consume tremendous amount of energy in terms of power distribution and cooling. Dynamic capacity provisioning is a promising approach for reducing energy consumption by dynamically adjusting the number of active machines to match resource demands. However, despite extensive studies of the problem, existing solutions for dynamic capacity provisioning have not fully considered the heterogeneity of both workload and machine hardware found in production environments. In particular, production data centers often comprise several generations of machines with different capacities, capabilities and energy consumption characteristics. Meanwhile, the workloads running in these data centers typically consist of a wide variety of applications with different priorities, performance objectives and resource requirements. Failure to consider heterogenous characteristics will lead to both sub-optimal energy-savings and long scheduling delays, due to incompatibility between workload requirements and the resources offered by the provisioned machines. To address this limitation, in this paper we present HARMONY, a Heterogeneity-Aware Resource Management System for dynamic capacity provisioning in cloud computing environments. Specifically, we first use the K-means clustering algorithm to divide the workload into distinct task classes with similar characteristics in terms of resource and performance requirements. Then we present a novel technique for dynamically adjusting the number of machines of each type to minimize total energy consumption and performance penalty in terms of scheduling delay. Through simulations using real traces from Google's compute clusters, we found that our approach can improve data center energy efficiency by up to 28% compared to heterogeneity-oblivious solutions.
[Energy consumption, dynamic capacity provisioning, Energy Management, heterogenous characteristics, Containers, k-means clustering algorithm, task classes, Resource Management, Google compute clusters, power aware computing, Cloud Computing, scheduling, heterogeneity-oblivious solutions, cloud computing, energy consumption, power cooling, suboptimal energy-savings, scheduling delays, Google, heterogeneity-aware resource management system, resource requirements, performance penalty, Harmony, data center energy efficiency, Dynamic scheduling, active machines, computer centres, Model Predictive Control, production data centers, production environments, performance objectives, cloud computing environments, Processor scheduling, incompatibility, pattern clustering, delays, resource demands, workload requirements, power distribution, Delays, dynamic heterogeneity-aware resource provisioning, energy consumption characteristics, machine hardware]
RAC: A Freerider-Resilient, Scalable, Anonymous Communication Protocol
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Enabling anonymous communication over the Internet is crucial. The first protocols that have been devised for anonymous communication are subject to freeriding. Recent protocols have thus been proposed to deal with this issue. However, these protocols do not scale to large systems, and some of them further assume the existence of trusted servers. In this paper, we present RAC, the first anonymous communication protocol that tolerates freeriders and that scales to large systems. Scalability comes from the fact that the complexity of RAC in terms of the number of message exchanges is independent from the number of nodes in the system. Another important aspect of RAC is that it does not rely on any trusted third party. We theoretically prove, using game theory, that our protocol is a Nash equilibrium, i.e, that freeriders have no interest in deviating from the protocol. Further, we experimentally evaluate RAC using simulations. Our evaluation shows that, whatever the size of the system (up to 100.000 nodes), the nodes participating in the system observe the same throughput.
[Protocols, Scalability, anonymous communication protocol, game theory, Throughput, Routing, Nash equilibrium, Servers, Relays, freerider-resilient communication protocol, RAC, Cryptography, protocols, scalable communication protocol]
Recursive Fact-Finding: A Streaming Approach to Truth Estimation in Crowdsourcing Applications
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
This paper presents a streaming approach to solve the truth estimation problem in crowdsourcing applications. We consider a category of crowdsourcing applications where a group of individuals volunteer (or are recruited to) share certain observations or measurements about the physical world. Examples include reporting locations of gas stations that remain operational after a natural disaster or reporting locations of potholes on city streets. We call such applications social sensing. Ascertaining the correctness of reported observations is a key challenge in such applications, referred to as the truth estimation problem. This problem is made difficult by the fact that the reliability of individual sources is usually unknown a priori, since any concerned citizen may, in principle, participate. Moreover, the timescales of crowdsourcing campaigns of interest can be as small as a few hours or days, which does not offer enough history for a reputation system to converge. Instead, recent prior work, including our own, developed fact-finding algorithms to solve this problem by iteratively assessing the credibility of sources and their claims in the absence of reputation scores. Such algorithms, however, operate on the entire dataset of reported observations in a batch fashion, which makes them less suited to applications where new observations arrive continually. In this paper, we describe a streaming fact-finder that recursively updates previous estimates based on new data. The recursive algorithm solves an expectation maximization (EM) problem to determine the odds of correctness of different observations. We compare the performance of our recursive EM algorithm to a batch EM algorithm, as well as to several state-of-art fact-finders through extensive simulations. We also demonstrate convergence of the recursive algorithm to the results of the batch version through a real social sensing experiment. Our evaluation shows that the proposed approach can process data streams much more efficiently while keeping the truth estimation accuracy close to that of the (much slower) batch algorithm. Ours is therefore the first fact-finder developed with explicit consideration to the continuous update needs of crowd-sourcing applications.
[estimation theory, streaming data, Estimation, data mining, crowdsourcing application, social sensing, expectation maximization problem, Equations, Atmospheric measurements, streaming fact-finder, EM problem, real-time, truth estimation problem, truth discovery, expectation-maximisation algorithm, Particle measurements, social networking (online), Sensors, Reliability, Mathematical model, recursive expectation maximization, recursive fact-finding]
Least Cost Rumor Blocking in Social Networks
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
In many real-world scenarios, social network serves as a platform for information diffusion, alongside with positive information (truth) dissemination, negative information (rumor) also spread among the public. To make the social network as a reliable medium, it is necessary to have strategies to control rumor diffusion. In this article, we address the Least Cost Rumor Blocking (LCRB) problem where rumors originate from a community Cr in the network and a notion of protectors are used to limit the bad influence of rumors. The problem can be summarized as identifying a minimal subset of individuals as initial protectors to minimize the number of people infected in neighbor communities of Cr at the end of both diffusion processes. Observing the community structure property, we pay attention to a kind of vertex set, called bridge end set, in which each node has at least one direct in-neighbor in Cr and is reachable from rumors. Under the OOAO model, we study LCRB-P problem, in which &#x03B1; (0 &lt;; &#x03B1; &lt;; 1) fraction of bridge ends are required to be protected. We prove that the objective function of this problem is submodular and a greedy algorithm is adopted to derive a (1-1/e)-approximation. Furthermore, we study LCRB-D problem over the DOAA model, in which all the bridge ends are required to be protected, we prove that there is no polynomial time o(ln n)-approximation for the LCRB-D problem unless P = NP, and propose a Set Cover Based Greedy (SCBG) algorithm which achieves a O(ln n)-approximation ratio. Finally, to evaluate the efficiency and effectiveness of our algorithm, we conduct extensive comparison simulations in three real-world datasets, and the results show that our algorithm outperforms other heuristics.
[Greedy algorithms, approximation ratio, approximation theory, Social network services, greedy algorithms, Communities, social networks, SCBG algorithm, set theory, Approximation methods, Bridges, least cost rumor blocking, LCRB problem, community structure property, opportunistic One-Activate-One model, approximation algorithm, Approximation algorithms, social networking (online), set cover based greedy, deterministic One-Activate-Many model, diffusion process, greedy algorithm, Integrated circuit modeling, information diffusion]
Identifying and Addressing Protocol Manipulation Attacks in "Secure" BGP
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Over more than a decade, researchers have studied a number of control and data plane attacks on BGP, the Internet's interdomain routing protocol, in the presence of malicious ASes. These prior efforts have largely focused on attacks that can be addressed using traditional cryptographic mechanisms to ensure authentication or integrity (e.g., S-BGP). Although augmenting BGP with authentication and integrity mechanisms is critical, it is far from sufficient to prevent attacks based on manipulating the complex BGP protocol itself. In this paper, we identify two serious protocol manipulation attacks that undermine the two most fundamental goals of the BGP control plane -- to ensure reachability and enable ASes to pick routes according to their routing policies -- despite the presence of S-BGP-like mechanisms. Our key contributions are to (1) formalize two critical security properties, (2) experimentally validate using commodity router implementations that BGP fails to achieve them, (3) quantify the extent of the resulting vulnerabilities in the Internet's AS topology, and (4) design and implement simple modifications to provably ensure that those properties are satisfied. Our experiments show that, a single malicious AS can cause thousands of other ASes to become disconnected from thousands of other ASes for arbitrarily long, while our proposed modifications almost completely eliminates such attacks.
[S-BGP-like mechanisms, commodity router implementations, reachability analysis, internetworking, telecommunication network topology, protocol manipulation attack address, Routing, cryptography, protocol manipulation attack identification, Steady-state, malicious ASes, integrity mechanism, authentication mechanism, computer network security, border gateway protocol, Internet interdomain routing protocol, Authentication, routing protocols, Routing protocols, Internet, cryptographic mechanisms, reachability, secure BGP control plane, Erbium]
A Formal Framework for Network Security Design Synthesis
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Due to the extensive use of Internet services and emerging security threats, most enterprise networks deploy varieties of security devices for controlling resource access based on organizational security requirements. These requirements are becoming more fine-grained, where access control depends on heterogeneous isolation patterns like access deny, trusted communication, and payload inspection. However, organizations are looking to design usable and optimal security configurations that can harden the network security within enterprise budget constraints. This requires analyzing various alternative security architectures in order to find a security design that satisfies the organizational security requirements as well as the business constraints. In this paper, we present ConfigSynth, an automated framework for synthesizing network security configurations by exploring various security design alternatives to provide an optimal solution. The main design alternatives include different kinds of isolation patterns for traffic flows in different segments of the network. ConfigSynth takes security requirements and business constraints along with the network topology as inputs. Then it synthesizes optimal and cost-effective security configurations satisfying the constraints. ConfigSynth also provides optimal placements of different security devices in the network according to the given network topology. ConfigSynth uses Satisfiability Modulo Theories (SMT) for modeling this synthesis problem. We demonstrate the scalability of the tool using simulated experiments.
[Internet services, resource access control, computability, enterprise networks, Security, constraints, formal logic, Network topology, security threats, synthesis problem modeling, authorisation, alternative security architectures, formal framework, traffic flows, Mathematical model, Business, satisfiability modulo theories, enterprise budget constraints, business constraints, network security configuration synthesis, automatic synthesis, telecommunication network topology, network security design synthesis, trusted communication, security configuration, network topology, Equations, computer network security, payload inspection, ConfigSynth, optimal security configurations, heterogeneous isolation patterns, Internet, organizational security requirements, SMT, Usability, business data processing, telecommunication traffic, Payloads, access deny]
Peer Pressure: Exerting Malicious Influence on Routers at a Distance
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Both academic research and historical incidents have shown that unstable BGP speakers can have extreme, undesirable impacts on network performance and reliability. Large amounts of time and energy have been invested in improving router stability. In this paper, we show how an adversary in control of a BGP speaker in a transit AS can cause a victim router in an arbitrary location on the Internet to become unstable. Through experimentation with both hardware and software routers, we examine the behavior of routers under abnormal conditions and come to three conclusions. First, that unexpected but perfectly legal BGP messages can place routers into those states with troubling ease. Second, that an adversary can implement attacks using these messages to disrupt the function of victim routers in arbitrary locations in the network. And third, modern best practices do not blunt the force of these attacks sufficiently. These conclusions lead us to recommend more rigorous testing of BGP implementations, focusing as much on protocol correctness as on software correctness.
[hardware router, Communities, Security, malicious influence, peer pressure, network performance, BGP speakers, Routing protocols, Hardware, protocols, peer-to-peer computing, software correctness, software router, Routing, Router, BGP, computer network security, network reliability, border gateway protocol, protocol correctness, Memory management, router stability, telecommunication network routing, Software, Internet]
Efficient Identification of Additive Link Metrics via Network Tomography
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
We investigate the problem of identifying individual link metrics in a communication network from accumulated end-to-end metrics over selected measurement paths, under the assumption that link metrics are additive and constant during the measurement, and measurement paths cannot contain cycles. We know from linear algebra that all link metrics can be uniquely identified when the number of linearly independent measurement paths equals u, the number of links. It is, however, inefficient to collect measurements from all possible paths, whose number can grow exponentially in u, as the number of useful measurements (from linearly independent paths) is at most u. The aim of this paper is to develop efficient algorithms for constructing linearly independent measurement paths and calculating link metrics. We show that whenever there exists a set of u linearly independent measurement paths, there must exist a set of three pairwise independent spanning trees. We exploit this property to develop an algorithm that can construct u linearly independent, cycle-free paths between monitors without examining all candidate paths, whose complexity is quadratic in u. A further benefit of the proposed algorithm is that the generated paths satisfy a nested structure that allows linear-time computation of link metrics without explicitly inverting the measurement matrix. Our evaluations on both synthetic and real network topologies verify the superior efficiency of the proposed algorithms, which are orders of magnitude faster than benchmark solutions for large networks.
[Linear systems, linearly independent measurement paths, linear time computation, Simple Paths, telecommunication links, Additives, Network Tomography, communication network, trees (mathematics), measurement matrix, telecommunication network topology, individual link metrics, Complexity theory, Algorithm, accumulated end to end metrics, Atmospheric measurements, Tomography, Path Construction, additive link metrics, network tomography, selected measurement paths, Monitoring, linear algebra]
On the k-Atomicity-Verification Problem
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Modern Internet-scale storage systems often provide weak consistency in exchange for better performance and resilience. An important weak consistency property is k-atomicity, which bounds the staleness of values returned by read operations. The k-atomicity-verification problem (or k-AV for short) is the problem of deciding whether a given history of operations is k-atomic. The 1-AV problem is equivalent to verifying atomicity/linearizability, a well-known and solved problem. However, for k &#x2265; 2, no polynomial-time k-AV algorithm is known. This paper makes the following contributions towards solving the k-AV problem. First, we present a simple 2-AV algorithm called LBT, which is likely to be efficient (quasilinear) for histories that arise in practice, although it is less efficient (quadratic) in the worst case. Second, we present a more involved 2-AV algorithm called FZF, which runs efficiently (quasilinear) even in the worst case. To our knowledge, these are the first algorithms that solve the 2-AV problem fully. Third, we show that the weighted k-AV problem, a natural extension of the k-AV problem, is NP-complete.
[read operations, weighted k-AV problem, Terminology, weak consistency property, Containers, data integrity, NP-complete, Registers, History, atomicity/linearizability verification, Resilience, Internet-scale storage systems, data consistency, formal verification, FZF, Clustering algorithms, 2-AV algorithm, 1-AV problem, k-atomicity-verification problem, computational complexity, LBT]
Safety of Deferred Update in Transactional Memory
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Transactional memory allows the user to declare sequences of instructions as speculative transactions that can either commit or abort. If a transaction commits, it appears to be executed sequentially, so that the committed transactions constitute a correct sequential execution. If a transaction aborts, none of its instructions can affect other transactions. The popular criterion of opacity requires that the views of aborted transactions must also be consistent with the global sequential order constituted by committed ones. This is believed to be important, since inconsistencies observed by an aborted transaction may cause a fatal irrecoverable error or waste of the system in an infinite loop. Intuitively, an opaque implementation must ensure that no intermediate view a transaction obtains before it commits or aborts can be affected by a transaction that has not started committing yet, so called deferred-update semantics. In this paper, we intend to grasp this intuition formally. We propose a variant of opacity that explicitly requires the sequential order to respect the deferred-update semantics. Unlike opacity, our property also ensures that a serialization of a history implies serializations of its prefixes. Finally, we show that our property is equivalent to opacity if we assume that no two transactions commit identical values on the same variable, and present a counter-example for scenarios when the &#x201C;unique-write&#x201D; assumption does not hold.
[transaction processing, deferred update safety, sequential execution, Law, Transactional Memory, unique-write assumption, transactional memory, committed transactions, History, opacityc riterion, Semantics, concurrency control, Limit-closure, Silicon, Real-time systems, prefix serializations, Safety, speculative transactions, deferred-update semantics, sequential order]
Parallel Consensus is Harder than Set Agreement in Message Passing
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
In the traditional consensus task, processes are required to agree on a common value chosen among the initial values of the participating processes. It is well known that consensus cannot be solved in crash-prone, asynchronous distributed systems. Two generalizations of the consensus tasks have been introduced: k-set agreement and k-parallel consensus. The k-set agreement task has the same requirements as consensus except that processes are allowed to decide up to k distinct values. In the k-parallel consensus task, each process participates simultaneously in k instances of consensus and is required to decide in at least one of them; any two processes deciding in the same instance must decide the same value. It is known that both tasks are equivalent in the wait-free shared memory model. Perhaps surprisingly, this paper shows that this is no longer the case in the n-process asynchronous message passing model with at most t process crashes. Specifically, the paper establishes that for parameters t, n, k such that t &gt; n+k-2/2 , k-parallel consensus is strictly harder than k-set agreement. The proof compares the information on failures necessary to solve each task in the failure detector framework and relies on a result in topological combinatorics, namely, the chromatic number of Kneser graphs. The paper also introduces the new failure detector class VI&#x0302;&#x00A3;k , which is a generalization of the quorum failures detector class I&#x0302;&#x00A3; suited to k-parallel consensus.
[parallel memories, Protocols, graph theory, asynchronous distributed system, n-process asynchronous message passing model, History, task analysis, failure analysis, consensus task, Fault tolerance, Kneser graphs, quorum failure detector, topological combinatorics, Parallel consensus, Detectors, shared memory systems, message passing, fault tolerance, Set agreement, Color, Computer crashes, Kneser graph, wait free shared memory model, crash prone system, k-parallel consensus task, Message passing, k-set agreement task, fault tolerant computing, Arrays]
[Publisher's information]
2013 IEEE 33rd International Conference on Distributed Computing Systems
None
2013
Provides a listing of current committee members and society officers.
[]
Message from the General Chair
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from the Program Co-Chairs
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Committee Lists
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Provides a listing of current committee members and society officers.
[]
Program Committee Members
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Provides a listing of current committee members and society officers.
[]
On Task Assignment for Real-Time Reliable Crowdsourcing
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
With the rapid growth of mobile smartphone users, several commercial mobile companies have exploited crowd sourcing as an effective approach to collect and analyze data, to improve their services. In a crowd sourcing system, "human workers" are enlisted to perform small tasks, that are difficult to be automated, in return for some monetary compensation. This paper presents our crowd sourcing system that seeks to address the challenge of determining the most efficient allocation of tasks to the human crowd. The goal of our algorithm is to efficiently determine the most appropriate set of workers to assign to each incoming task, so that the real-time demands are met and high quality results are returned. We empirically evaluate our approach and show that our system effectively meets the requested demands, has low overhead and can improve the number of tasks processed under the defined constraints over 71% compared to traditional approaches.
[Crowdsourcing, Measurement, commercial mobile companies, mobile smart phone users, crowdsourcing, real-time reliable crowd sourcing system, human crowd, Pareto optimization, smart phones, human resource management, monetary compensation, real-time, human workers, distributed systems, Real-time systems, personnel, Bipartite graph, task assignment, Reliability, Time factors]
Towards Truthful Mechanisms for Mobile Crowdsourcing with Dynamic Smartphones
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Stimulating participation from smartphone users is of paramount importance to mobile crowd sourcing systems and applications. A few incentive mechanisms have been proposed, but most of them have made the impractical assumption that smartphones remain static in the system and sensing tasks are known in advance. The existing mechanisms fail when being applied to the realistic scenario where smartphones dynamically arrive to the system and sensing tasks are submitted at random. It is particularly challenging to design an incentive mechanism for such a mobile crowd sourcing system, given dynamic smartphones, uncertain arrivals of tasks, strategic behaviors, and private information of smartphones. We propose two truthful auction mechanisms for two different cases of mobile crowd sourcing with dynamic smartphones. For the offline case, we design an optimal truthful mechanism with an optimal task allocation algorithm of polynomial-time computation complexity of O (n+&#x03B3;)3, where n is the number of smartphones and &#x03B3; is the number of sensing tasks. For the online case, we design a near-optimal truthful mechanism with an online task allocation algorithm that achieves a constant competitive ratio of 1:2. Rigorous theoretical analysis and extensive simulations have been performed, and the results demonstrate the proposed auction mechanisms achieve truthfulness, individual rationality, computational efficiency, and low overpayment.
[Crowdsourcing, mobile crowdsourcing, Computational modeling, Truthful mechanisms, private information, Online mechanisms, information retrieval, incentive mechanism, Mobile communication, truthful auction mechanisms, smart phones, competitive ratio, mobile computing, polynomial-time computation complexity, Bismuth, online task allocation algorithm, Sensors, Resource management, trusted computing, Smart phones, computational complexity, dynamic smart phones]
Community-Based Identity Validation on Online Social Networks
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Identity management in online social networks (OSNs) is a challenging, yet important requirement for effective privacy protection and trust management. Literature offers several proposals addressing issues related to identity breaches and/or identity related attacks on OSNs, but only a few aim at giving means to judge users' reliability in terms of trustworthiness of their claimed identities. In this paper, we propose an identity validation process that relies on OSN community feedback to assign to OSN users identity trustworthiness levels. For this purpose, we define a community based supervised learning process to detect the set of attributes in a user profile for which it is expected to see a correlation among their values (e.g., job and salary). Once these correlated attribute sets are identified, the profile of a target user is judged by a selected group of raters to estimate her identity trustworthiness level. We demonstrate the effectiveness of our proposal through experimentation under two different scenarios and using real data. The experiments' results under the two scenarios demonstrate the effectiveness and meaningfulness of our proposal.
[user profile, Identity validation, OSN community feedback, Correlation, correlated attribute sets, Communities, online social networks, Online Social Networks, Remuneration, Training, identity management, Community-based Validation, security of data, privacy protection, Coherence, community- based supervised learning process, identity trustworthiness level estimation, social networking (online), trust management, data privacy, Reliability, learning (artificial intelligence), community-based identity validation, trusted computing]
Efficient Data Forwarding in Mobile Social Networks with Diverse Connectivity Characteristics
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Mobile Social Network (MSN) with diverse connectivity characteristics is a combination of opportunistic network and mobile ad hoc network. Since the major difficulty of data forwarding is the opportunistic part, techniques designed for opportunistic networks are commonly used to forward data in MSNs. However, this may not be the best solution since they do not consider the ubiquitous existences of Transient Connected Components (TCCs), where nodes inside a TCC can reach each other by multi-hop wireless communications. In this paper, we first identify the existence of TCCs and analyze their properties based on five real traces. Then, we propose TCC-aware data forwarding strategies which exploit the special characteristics of TCCs to increase the contact opportunities and then improve the performance of data forwarding. Trace-driven simulations show that our TCC-aware data forwarding strategies outperform existing data forwarding strategies in terms of data delivery ratio and network overhead.
[Measurement, mobile ad hoc network, data delivery ratio, Bluetooth, Exponential distribution, MSN, TCC-aware data forwarding strategies, Mobile ad hoc networks, Wireless communication, trace-driven simulations, network overhead, opportunistic network, mobile computing, TCC, social networking (online), mobile social networks, diverse network connectivity characteristics, Peer-to-peer computing, data handling, transient connected components, multihop wireless communications, Transient analysis]
Modeling WiFi Active Power/Energy Consumption in Smartphones
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
We conduct the first detailed measurement study of the properties of a class of WiFi active power/energy consumption models based on parameters readily available to smartphone app developers. We first consider a number of parameters used by previous models and show their limitations. We then focus on a recent approach modeling the active power consumption as a function of the application layer throughput. Using a large dataset and an 802.11n-equipped smartphone, we build four versions of a previously proposed linear power-throughput model, which allow us to explore the fundamental trade off between accuracy and simplicity. We study the properties of the model in relation to other parameters such as the packet size and/or the transport layer protocol, and we evaluate its accuracy under a variety of scenarios which have not been considered in previous studies. Our study shows that the model works well in a number of scenarios but its accuracy drops with high throughput values or when tested on different hardware. We further show that a non-linear model can greatly improve the accuracy in these two cases.
[Energy consumption, Power demand, IEEE 802.11 Standards, telecommunication power management, application layer throughput, nonlinear model, Throughput, smart phones, packet size, 802.11n-equipped smartphone, transport layer protocol, Accuracy, linear power-throughput model, smartphones, power, Mathematical model, wireless LAN, WiFi active power consumption models, WiFi, Smart phones, WiFi active energy consumption models, energy]
Fuel Cell Generation in Geo-Distributed Cloud Services: A Quantitative Study
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
The demand for capping carbon emission has promoted the use of fuel cell energy in cloud computing, yet it is unclear what and how much benefit it may bring. This paper, for the first time, attempts to quantitatively examine the benefits brought by fuel cell generation, and to illustrate how such benefits can be realized with an intelligent coordination between grid power and fuel cell generation. Specifically, we propose UFC, a quantitative index called the utility of the cloud using fuel cells, which captures the level of the data enters operator's overall satisfaction from energy cost, carbon emission, and workload performance. We formulate the UFC maximization problem to jointly optimize both fuel cell generation and geographical request routing. In order to avoid centralized solutions with high complexity and low scalability, we develop a distributed algorithm blending the advantages of Alternating Direction Method of Multipliers (ADMM) and the auxiliary variable method, whose performance is evaluated and verified through our extensive simulations based on real-world data enter workload traces, electricity prices and generation data sets.
[fuel cell generation, geodistributed cloud services, Carbon dioxide, Routing, Servers, Indexes, computer centres, carbon emission capping, Convergence, distributed algorithm, optimisation, alternating direction method of multipliers, Electricity, distributed algorithms, Fuel cells, fuel cells, green computing, cloud computing, ADMM]
Optimal Energy Cost for Strongly Stable Multi-hop Green Cellular Networks
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
With the ever increasing user adoption of mobile devices like smart phones and tablets, the cellular service providers' energy consumption and cost are fast-growing and have received tremendous attention. How to effectively reduce the energy cost of cellular networks and achieve green communications while satisfying cellular users' rocketing traffic demands has become an urgent and challenging problem. In this paper, we investigate the minimization of the long-term time-averaged expected energy cost of a cellular service provider while guaranteeing the strong stability of the network. We first formulate an offline optimization problem with a joint consideration of flow routing, link scheduling, and energy (i.e., renewable energy resource, energy storage unit, etc.) constraints. Since the formulated problem is a time-coupling stochastic Mixed-Integer Non-Linear Programming (MINLP) problem, it is prohibitively expensive to solve. Then, we reformulate the problem by employing Lyapunov optimization theory. A decomposition based algorithm is developed to solve the problem, which is proved to guarantee the network strong stability. Both the lower and upper bounds on the optimal result of the original problem are derived and proven. Simulation results demonstrate that the obtained lower and upper bounds are very tight, and that the proposed scheme results in noticeable energy cost savings.
[network stability, link scheduling, integer programming, nonlinear programming, time-coupling stochastic problem, energy storage unit, Power grids, optimal energy cost, Optimization, Spread spectrum communication, MINLP problem, decomposition based algorithm, multi-hop green cellular networks, stochastic processes, Base stations, offline optimization problem, telecommunication power management, long-term time-averaged expected energy cost, Minimization, renewable energy resource, Renewable energy sources, mixed-integer non-linear programming problem, flow routing, telecommunication network routing, Lyapunov optimization theory, minimisation, cellular service provider, Energy storage, cellular radio, Lyapunov methods]
Thermal Modeling for a HVAC Controlled Real-Life Auditorium
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
The largest source of energy consumption in buildings is heating, ventilation, and air conditioning (HVAC). For an HVAC system to provide comfort and minimize energy consumption, it is crucial to understand the spatiotemporal thermal dynamics, especially in large open spaces. To optimize HVAC control, it is important to establish accurate dynamic thermal models. For this purpose, we constructed a real-world test bed by instrumenting an HVAC-controller auditorium using multiple types of sensors. Based on the dataset, we develop and evaluate a novel data-driven approach to model the complex thermal dynamics in a large space through a combination of data clustering and system identification techniques. Real-world data shows that our approach achieves low estimation errors. Our modeling approach therefore provides a practical foundation for HVAC control and optimization for large open spaces.
[Atmospheric modeling, air conditioning, Cyber physical systems, ventilation, Predictive models, data clustering, Modeling, dynamic thermal modeling, building, Temperature sensors, Temperature measurement, HVAC controlled real-life auditorium, HVAC, data-driven approach, Sensor networks, system identification technique, Data models, spatiotemporal thermal dynamics, heating, building management systems, energy consumption]
Exploring the Use of Diverse Replicas for Big Location Tracking Data
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
The value of large amount of location tracking data has received wide attention in many applications including human behavior analysis, urban transportation planning, and various location-based services (LBS). Nowadays, both scientific and industrial communities are encouraged to collect as much location tracking data as possible, which brings about two issues: 1) it is challenging to process the queries on big location tracking data efficiently, and 2) it is expensive to store several exact data replicas for fault-tolerance. So far, several dedicated storage systems have been proposed to address these issues. However, they do not work well when the query ranges vary widely. In this paper, we present the design of a storage system using diverse replica scheme which improves the query processing efficiency with reduced cost of storage space. To the best of our knowledge, we are the first to investigate the data storage and processing in the context of big location tracking data. Specifically, we conduct in-depth theoretical and empirical analysis of the trade-offs between different spatio-temporal partitioning schemes as well as data encoding schemes. Then we propose an effective approach to select an appropriate set of diverse replicas, which is optimized for the expected query loads while conforming to the given storage space budget. The experiment results confirm that using diverse replicas can significantly improve the overall query performance. The results also demonstrate that the proposed algorithms for the replica selection problem is both effective and efficient.
[Algorithm design and analysis, dedicated storage systems, storage space cost reduction, fault-tolerance, query performance, storage system design, urban transportation planning, query processing, storage management, data storage, replica selection problem, diverse replica scheme, Big data, location-based services, scientific communities, LBS, Context, replicated databases, spatio-temporal partitioning schemes, data processing, Linear programming, diverse replicas, query loads, Encoding, data replicas, human behavior analysis, data encoding schemes, Query processing, Organizations, big location tracking data, query processing efficiency, industrial communities, storage space budget]
WOHA: Deadline-Aware Map-Reduce Workflow Scheduling Framework over Hadoop Clusters
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
In this paper, we present WOHA, an efficient scheduling framework for deadline-aware Map-Reduce workflows. In data centers, complex backend data analysis often utilizes a workflow that contains tens or even hundreds of interdependent Map-Reduce jobs. Meeting deadlines of these workflows is usually of crucial importance to businesses (for example, workflows tightly linked to time-sensitive advertisement placement optimizations can directly affect revenue). Popular Map-Reduce implementations, such as Hadoop, deal with independent Map-Reduce jobs rather than workflows of jobs. In order to simplify the process of submitting workflows, solutions like Oozie emerge, which take a workflow configuration file as input and automatically submit its Hadoop jobs at the right time. The information separation that Hadoop only handles resource allocation and Oozie workflow topology, although preventing the Hadoop master node from getting involved with complex workflow analysis, may unnecessarily lengthen the workflow spans and thus cause more deadline misses. To address this problem and at the same time honor the efficiency of Hadoop master node, WOHA allows client nodes to locally generate scheduling plans which are later used as resource allocation hints by the master node. Under this framework design, we propose a novel scheduling algorithm that improves deadline satisfaction ratio by dynamically assigning priorities among workflows based on their progresses. We implement WOHA by extending Hadoop-1.2.1. Our experiments over an 80-server cluster show that WOHA manages to increase the deadline satisfaction ratio by 10% compared to state-of-the-art solutions, and scales up to tens of thousands of concurrently running workflows.
[Hadoop clusters, Schedules, Job shop scheduling, data analysis, data centers, Hadoop, Deadline, Oozie workflow topology, complex workflow analysis, Workflow, Generators, Scheduling, scheduling algorithm, parallel programming, deadline-aware Map-Reduce workflow scheduling framework, MapReduce, complex backend data analysis, Scheduling algorithms, resource allocation, Clustering algorithms, scheduling, WOHA]
SOR: An Objective Ranking System Based on Mobile Phone Sensing
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Currently, a few online review and recommendation systems (such as Yelp and Trip Advisor) have attracted millions of users and are gaining increasing popularity. They usually rate and rank places and attractions based on subjective ratings provided by users. In this paper, we present design, implementation and evaluation of a mobile phone Sensing based Objective Ranking (SOR) system, which ranks a target place based on data collected via mobile phone sensing. Our system has the following desirable features: 1) it is easy to use, 2) its architecture is so scalable that various embedded and external sensors can be easily integrated into it, 3) an online scheduling algorithm is proposed and used to schedule sensing activities for coverage maximization, which has a constant approximation ratio of 1/2, 4) a personalizable ranking algorithm is developed and used to rank target places based on various sensor readings and user preferences. We validate and evaluate SOR via both field tests (using real hiking trails and coffee shops in Syracuse, NY as target places) and simulation. The field-testing results show that data collected and processed by SOR can well capture characteristics of target places, and personalizable rankings produced by SOR can well match user preferences. In addition, simulation results well justify effectiveness of the proposed scheduling algorithm.
[objective ranking system, Schedules, maximization, Mobile communication, Mobile handsets, Servers, Temperature sensors, online review, mobile computing, optimisation, Databases, Ranking, Yelp, approximation theory, Trip Advisor, smart phones, Scheduling, SOR system, Mobile Computing, recommendation systems, personalizable ranking algorithm, recommender systems, constant approximation ratio, online scheduling algorithm, subjective ratings, Mobile Phone Sensing, mobile phone sensing, external sensors]
The Tempo-Spatial Information Dissemination Properties of Mobile Opportunistic Networks with Levy Mobility
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Mobile opportunistic networks make use of a new networking paradigm that takes advantage of node mobility to distribute information. Studying their inherent properties of information dissemination can provide a straightforward explanation on the potentials of mobile opportunistic networks to support emerging applications such as mobile commerce, emergency services, and so on. In this paper, we investigate the inherent properties of information dissemination using the Le&#x0301;vy mobility model to characterize the movement pattern of the nodes. Because Le&#x0301;vy mobility can closely mimic human walk, the analysis model we adopt is practical. Our analyses are taken from the perspectives of small- and large-scales. From the perspective of small-scale, the distribution of the minimum time needed by the information to spread to a given region is investigated, from the perspective of large-scale, the bounds of the probability of the earliest time at which the information arrives in a region that is sufficiently farther away are obtained. We also provide the rate that such probability approaches zero as the distance to the region increases to infinity. Finally, our main results are validated by the numerical simulations.
[numerical simulations, mobile opportunistic networks, Le&#x0301;vy mobility model, probability, Mobile communication, Ad hoc networks, mobility management (mobile radio), mobile commerce, Wireless communication, node mobility, Analytical models, Wireless sensor networks, numerical analysis, tempo-spatial information dissemination properties, networking paradigm, emergency services, Mobile computing, Business]
Cooperative and Efficient Real-Time Scheduling for Automotive Communications
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
FlexRay is an automotive network communication protocol. It provides support to transmit time-sensitive messages in automobiles. FlexRay transmits periodic messages in a static segment and a periodic messages in a dynamic segment. To improve transmission reliability, FlexRay offers hybrid data management schemes for both static and dynamic segments. However, existing approaches only schedule static segment and dynamic segment separately, leading to poor bandwidth utilization and transmission delay. Moreover, due to the bandwidth limitation, existing best-effort retransmission for all segments fails to achieve high reliability. To address these two concerns, we propose a novel and efficient scheduling scheme, called Coefficient. The idea behind Coefficient is to cooperatively schedule the static and dynamic segments, while judiciously stealing the selective slacks for reliable transmission based on practical fault models. Coefficient schedules both static and dynamic segments in the dual-channel manner based on practical fault models. Extensive experiments based on real-world case studies demonstrate that Coefficient meets the needs of both real-time transmission and reliability requirements, and delivers significant performance improvements.
[Schedules, periodic messages, mobility management (mobile radio), Vehicle dynamics, static segment, coefficient schedules, fault models, Bandwidth, FlexRay, scheduling, Real-time systems, protocols, real-time scheduling, best-effort retransmission, bandwidth limitation, hybrid data management schemes, Dynamic scheduling, cooperative communication, automotive network communication protocol, Processor scheduling, mobile communication, cooperative scheduling, time-sensitive messages, dynamic segment, bandwidth utilization, Reliability, transmission delay, transmission reliability]
Adaptive Partitioning for Large-Scale Dynamic Graphs
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
In the last years, large-scale graph processing has gained increasing attention, with most recent systems placing particular emphasis on latency. One possible technique to improve runtime performance in a distributed graph processing system is to reduce network communication. The most notable way to achieve this goal is to partition the graph by minimizing the number of edges that connect vertices assigned to different machines, while keeping the load balanced. However, real-world graphs are highly dynamic, with vertices and edges being constantly added and removed. Carefully updating the partitioning of the graph to reflect these changes is necessary to avoid the introduction of an extensive number of cut edges, which would gradually worsen computation performance. In this paper we show that performance degradation in dynamic graph processing systems can be avoided by adapting continuously the graph partitions as the graph changes. We present a novel highly scalable adaptive partitioning strategy, and show a number of refinements that make it work under the constraints of a large-scale distributed system. The partitioning strategy is based on iterative vertex migrations, relying only on local information. We have implemented the technique in a graph processing system, and we show through three real-world scenarios how adapting graph partitioning reduces execution time by over 50% when compared to commonly used hash-partitioning.
[Adaptive systems, adaptive partitioning, Heuristic algorithms, Computational modeling, graph theory, distributed processing, Partitioning algorithms, Topology, performance degradation, distributed graph processing system, Convergence, dynamic graphs, Graph processing, Finite element analysis, large-scale graph processing, adaptive graph partitioning]
Sim-Watchdog: Leveraging Temporal Similarity for Anomaly Detection in Dynamic Graphs
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Graphs are widely used to characterize relationships or information flows among entities in large networks or distributed systems. In this work, we propose a systematic framework that leverages temporal similarity inherent in dynamic graphs for anomaly detection. This framework relies on the Neyman-Pearson criterion to choose similarity measures with high discriminative power for online anomaly detection in dynamic graphs. We formulate the problem rigorously, and after establishing its inapproximibility result, we develop a greedy algorithm for similarity measure selection. We apply this framework to dynamic graphs generated from email communications among thousands of employees in a large research institution and demonstrate that it works effectively on a set of more than 100 candidate graph similarity measures.
[Computers, Greedy algorithms, Image edge detection, greedy algorithms, graph theory, sim-watchdog, Electronic mail, graph similarity, Optimization, Anomaly detection, Training, temporal similarity, dynamic graphs, similarity measures, Systematics, security of data, similarity measure selection, Neyman-Pearson criterion, information flows, distributed systems, greedy algorithm, email communications, online anomaly detection]
On Limits of Travel Time Predictions: Insights from a New York City Case Study
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
The proliferation of location sensors has resulted in the wide availability of historical location and time data. A prominent use of such data is to develop models to estimate travel-times (between arbitrary points in a city) accurately. The problem of travel-time estimation/prediction has been well studied in the past, where the proposed techniques span a spectrum of statistical methods, such as k-nearest neighbors, Gaussian regression, Artificial Neural Networks, and Support Vector Machines. In this paper, we demonstrate that, contrary to popular intuition, empirical data suggests that simple travel time predictors come very close to the fundamental error bounds achievable in delay prediction. We derive such bounds by estimating entropy that remains in travel time distributions, even after all spatio-temporal delay-influencing factors have been accounted for. Our results are based on analysis of cab traces from New York City, that feature 15 million trips. While we cannot claim generalizability to other cities, the results suggest the diminishing return of complex travel-time predictors due to the inherent nature of uncertainty in trip delays. We demonstrate a simple travel-time predictor, whose error approaches the uncertainty bound. It predicts delay based only on total distance traveled and time-of-day and is close to the optimal solution.
[Measurement, Uncertainty, New York city case study, Travel time prediction, location based services, historical location, regression analysis, fundamental error bounds, delay prediction, mobile computing, Cities and towns, Probability density function, location sensors, travel time predictions, cab traces, support vector machines, error bounds, Estimation, Gaussian regression, time data, Standards, Equations, k-nearest neighbors, artificial neural networks, trip delays, spatio-temporal delay-influencing factors, Gaussian processes, neural nets, statistical methods, travel time distributions, Information theory]
Learning from the Past: Intelligent On-Line Weather Monitoring Based on Matrix Completion
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Matrix completion has emerged very recently and provides a new venue for low cost data gathering in WSNs. Existing schemes often assume that the data matrix has a known and fixed low-rank, which is unlikely to hold in a practical monitoring system such as weather data gathering. Weather data varies in temporal and spatial domain with time. By analyzing a large set of weather data collected from 196 sensors in ZhuZhou, China, we reveal that weather data have the features of low-rank, temporal stability, and relative rank stability. Taking advantage of these features, we propose an on-line data gathering scheme based on matrix completion theory, named MC-Weather, to adaptively sample different locations according to environmental and weather conditions. To better schedule sampling process while satisfying the required reconstruction accuracy, we propose several novel techniques, including three sample learning principles, an adaptive sampling algorithm based on matrix completion, and a uniform time slot and cross sample model. With these techniques, our MC-Weather scheme can collect the sensory data at required accuracy while largely reduce the cost for sensing, communication and computation. We perform extensive simulations based on the real weather data sets and the simulation results validate the efficiency and efficacy of the proposed scheme.
[sample learning principle, wireless sensor networks, matrix completion, MC-Weather, temporal stability, weather data gathering, wireless sensor network, Stability analysis, Vectors, Sparse matrices, spatial domain, matrix algebra, intelligent online weather monitoring, online data gathering scheme, Wireless sensor networks, weather forecasting, WSN, relative rank stability, temporal domain, data gathering, adaptive sampling algorithm, Sensors, Monitoring, Meteorology]
e-PPI: Locator Service in Information Networks with Personalized Privacy Preservation
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
In emerging information networks, having a privacy preserving index (or PPI) is critically important for locating information of interest for data sharing across autonomous providers while preserving privacy. An understudied problem for PPI techniques is how to provide controllable privacy preservation, given the innate difference of privacy concerns regarding different data owners. In this paper we present a personalized privacy preserving index, coined &#x03B5;-PPI, which guarantees quantitative privacy preservation differentiated by personal identities. We devise a new common-identity attack that breaks existing PPI's and propose an identity-mixing protocol against the attack in &#x03B5;-PPI. The proposed &#x03B5;-PPI construction protocol is the first without any trusted third party and/or trust relationships between providers. We have implemented our &#x03B5;-PPI construction protocol by using generic MPC techniques (secure multi-party computation) and optimized the performance to a practical level by minimizing the expensive MPC part.
[Measurement, Data privacy, Protocols, information sharing, Noise, privacy preservation, PPI, expensive MPC part, secure computation, trusted third party, trust relationships, construction protocol, Privacy, database, common-identity attack, generic MPC techniques, health informatics, protocols, secret sharing, mpc, personalized privacy preserving index, secure multiparty computation, record locator service, personalized privacy preservation, data sharing across autonomous providers, Indexes, information networks, controllable privacy preservation, Hospitals, security of data, identity-mixing protocol, locator service, data privacy, quantitative privacy preservation]
Enabling Privacy-Preserving Image-Centric Social Discovery
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
The increasing popularity of images at social media sites is posing new opportunities for social discovery applications, i.e., suggesting new friends and discovering new social groups with similar interests via exploring images. To effectively handle the explosive growth of images involved in social discovery, one common trend for many emerging social media sites is to leverage the commercial public cloud as their robust backend data center. While extremely convenient, directly exposing content-rich images and the related social discovery results to the public cloud also raises new acute privacy concerns. In light of the observation, in this paper we propose a privacy-preserving social discovery service architecture based on encrypted images. As the core of such social discovery is to compare and quantify similar images, we first adopt the effective Bag-of-Words model to extract the "visual similarity content" of users' images into image profile vectors, and then model the problem as similarity retrieval of encrypted high-dimensional image profiles. To support fast and scalable similarity search over hundreds of thousands of encrypted images, we propose a secure and efficient indexing structure. The resulting design enables social media sites to obtain secure, practical, and accurate social discovery from the public cloud, without disclosing the encrypted image content. We formally prove the security and discuss further extensions on user image update and the compatibility with existing image sharing social functionalities. Extensive experiments on a large Flickr image dataset demonstrate the practical performance of the proposed design. Our qualitative social discovery results show consistency with human perception.
[Cloud computing, Visualization, encrypted images, bag-of-words model, image profile vectors, Privacy-preserving, Image, commercial public cloud, Cryptography, service-oriented architecture, backend data center, image sharing social functionalities, encrypted high-dimensional image profile similarity retrieval, privacy-preserving image-centric social discovery service architecture, Media, cryptography, Vectors, Indexes, Social discovery, vectors, Flickr image dataset, social media sites, visual similarity content extraction, image retrieval, social networking (online), data privacy, secure indexing structure]
Providing Efficient Privacy-Aware Incentives for Mobile Sensing
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Mobile sensing relies on data contributed by users through their mobile device (e.g., smart phone) to obtain useful information about people and their surroundings. However, users may not want to contribute due to lack of incentives and concerns on possible privacy leakage. To effectively promote user participation, both incentive and privacy issues should be addressed. Existing work on privacy-aware incentive is limited to special scenario of mobile sensing where each sensing task needs only one data report from each user, and thus not appropriate for generic scenarios in which sensing tasks may require multiple reports from each user (e.g., in environmental monitoring applications). In this paper, we propose a privacy-aware incentive scheme for general mobile sensing, which allows each sensing task to collect one or multiple reports from each user as needed. Besides being more flexible in task management, our scheme has much lower computation and communication cost compared to the existing solution. Evaluations show that, when each node only contributes data for a small fraction of sensing tasks (e.g, due to the incapability or disqualification to generate sensing data for other tasks), our scheme runs at least one order of magnitude faster.
[task management, Protocols, Mobile communication, Mobile handsets, Incentive schemes, mobile device, Privacy, mobile computing, mobile sensing, privacy leakage, data privacy, Sensors, user participation, Cryptography, privacy-aware incentives]
No NAT'd User Left Behind: Fingerprinting Users behind NAT from NetFlow Records Alone
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
It is generally recognized that the network traffic generated by an individual acts as his biometric signature. Several tools exploit this fact to fingerprint and monitor users. Often, though, these tools access the entire traffic, including IP addresses and payloads. In general, this is not feasible on the grounds that both performance and privacy would be negatively affected. In reality, most ISPs convert user traffic into Net Flow records for a concise representation that does not include the payload. More importantly, a single IP address belonging to a large and distributed network is usually masked using Network Address Translation techniques, thus a few IP addresses may be associated to thousands of individuals (NAT'd IPs). We devised a new fingerprinting framework that overcomes these hurdles. Our system is able to analyze a huge amount of network traffic represented as Net Flows, with the intent to track people. It does so by accurately inferring when users are connected to the network and which IP addresses they are using, even though thousands of users are hidden behind NAT. Our prototype implementation was deployed and tested within an existing large metropolitan WiFi network serving about 200,000 users, with an average load of more than 1,000 users simultaneously connected behind 2 NAT'd IP addresses only. Our solution turned out to be very effective, with an accuracy greater than 90%. We also devised new tools and refined existing ones that may be applied to other contexts related to Net Flow analysis.
[Protocols, biometric signature, masked IP addresses, NAT user, network address translation techniques, biometrics (access control), Net Flow analysis, Training, metropolitan area networks, user privacy, Hidden Markov models, Net Flow records, metropolitan WiFi network, data privacy, digital signatures, Internet, IP networks, wireless LAN, user network traffic, user fingerprinting framework, Monitoring, telecommunication traffic, Payloads, distributed network]
OpenSample: A Low-Latency, Sampling-Based Measurement Platform for Commodity SDN
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
In this paper we propose, implement and evaluate OpenSample: a low-latency, sampling-based network measurement platform targeted at building faster control loops for software-defined networks. OpenSample leverages sFlow packet sampling to provide near-real-time measurements of both network load and individual flows. While OpenSample is useful in any context, it is particularly useful in an SDN environment where a network controller can quickly take action based on the data it provides. Using sampling for network monitoring allows OpenSample to have a 100 millisecond control loop rather than the 1-5 second control loop of prior polling-based approaches. We implement OpenSample in the Floodlight Open Flow controller and evaluate it both in simulation and on a test bed comprised of commodity switches. When used to inform traffic engineering, OpenSample provides up to a 150% throughput improvement over both static equal-cost multi-path routing and a polling-based solution with a one second control loop.
[Floodlight open flow controller, Measurement, network load, Ports (Computers), Control systems, Throughput, near-real-time measurements, Data Center, Packet Sampling, software-defined networks, Network topology, traffic engineering, static equal-cost multipath routing, Monitoring, SDN environment, polling-based approaches, Maximum likelihood estimation, program control structures, Radiation detectors, low-latency sampling-based network measurement platform, computer networks, commodity switches, network controller, sFlow packet sampling, control loops, Traffic Engineering, network monitoring, telecommunication network routing, sFlow, commodity SDN, Software Defined Networking, telecommunication traffic, OpenSample, throughput improvement]
Scalable Traffic-Aware Virtual Machine Management for Cloud Data Centers
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Virtual Machine (VM) management is a powerful mechanism for providing elastic services over Cloud Data Centers (DC)s. At the same time, the resulting network congestion has been repeatedly reported as the main bottleneck in DCs, even when the overall resource utilization of the infrastructure remains low. However, most current VM management strategies are traffic-agnostic, while the few that are traffic-aware only concern a static initial allocation, ignore bandwidth oversubscription, or do not scale. In this paper we present S-CORE, a scalable VM migration algorithm to dynamically reallocate VMs to servers while minimizing the overall communication footprint of active traffic flows. We formulate the aggregate VM communication as an optimization problem and we then define a novel distributed migration scheme that iteratively adapts to dynamic traffic changes. Through extensive simulation and implementation results, we show that S-CORE achieves significant (up to 87%) communication cost reduction while incurring minimal overhead and downtime.
[optimization problem, Scalable, Migration, scalable traffic-aware virtual machine management, Servers, Traffic-Aware, optimisation, Consolidation, scalable VM migration algorithm, Bandwidth, IP networks, cloud computing, Data Center Network, S-CORE, Communication Cost, network congestion, Dynamic scheduling, Topology, elastic services, Virtual machine monitors, VM management, cloud data center, virtual machines, Resource management, distributed migration scheme, Virtual Machine]
Energy-Efficient Flow Scheduling and Routing with Hard Deadlines in Data Center Networks
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
The power consumption of enormous network devices in data centers has emerged as a big concern to data center operators. Despite many traffic-engineering-based solutions, very little attention has been paid on performance-guaranteed energy saving schemes. In this paper, we propose a novel energy-saving model for data center networks by scheduling and routing "deadline-constrained flows" where the transmission of every flow has to be accomplished before a rigorous deadline, being the most critical requirement in production data center networks. Based on speed scaling and power-down energy saving strategies for network devices, we aim to explore the most energy efficient way of scheduling and routing flows on the network, as well as determining the transmission speed for every flow. We consider two general versions of the problem. For the version of only flow scheduling where routes of flows are pre-given, we show that it can be solved polynomially and we develop an optimal combinatorial algorithm for it. For the version of joint flow scheduling and routing, we prove that it is strongly NP-hard and cannot have a Fully Polynomial-Time Approximation Scheme (FPTAS) unless P=NP. Based on a relaxation and randomized rounding technique, we provide an efficient approximation algorithm which can guarantee a provable performance ratio with respect to a polynomial of the total number of flows.
[Schedules, NP-hard, combinatorial mathematics, Switches, Optimal scheduling, power consumption, performance-guaranteed energy saving schemes, energy-efficient flow scheduling, power aware computing, hard deadlines, polynomial approximation, energy efficiency, scheduling, transmission speed, power-down energy saving strategies, cloud computing, traffic-engineering-based solutions, data center networks, data center operators, speed scaling, Power demand, production data center networks, optimal combinatorial algorithm, Routing, deadline-constrained flows, randomized rounding technique, computer centres, Approximation algorithms, Data models, network devices, FPTAS, fully polynomial-time approximation scheme, computational complexity]
Bandwidth Guarantee under Demand Uncertainty in Multi-tenant Clouds
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
The shared multi-tenant nature of cloud network infrastructures has caused poor application performance in the clouds due to unpredictable network performance. To provide bandwidth guarantee, several virtual network abstractions have been proposed which allow the tenants to specify and reserve virtual clusters with required network bandwidth between the VMs. However, all of these existing proposals require the tenants to deterministically characterize the exact bandwidth demands in the abstractions, which can be difficult and result in inefficient bandwidth reservation due to the demand uncertainty. In this paper, we propose a virtual cluster abstraction with stochastic bandwidth requirements between VMs, called Stochastic Virtual Cluster (SVC), which probabilistically models the bandwidth demand uncertainty. Based on SVC, we propose a network sharing framework and efficient VM allocation algorithms to ensure that the bandwidth demands of tenants on any link are satisfied with a high probability, while minimizing the bandwidth occupancy cost on links. Using simulations, we demonstrate the effectiveness of SVC for accommodating cloud application workloads with highly volatile bandwidth demands, in the way of achieving the trade-off between the job concurrency and average job running time.
[SVC, Uncertainty, stochastic bandwidth requirements, Stochastic processes, Network Reservation, bandwidth occupancy cost minimization, bandwidth reservation, virtual cluster abstraction, Cloud Computing, Allocation, Static VAr compensators, Clustering algorithms, Bandwidth, cloud computing, stochastic processes, shared multitenant cloud network infrastructures, bandwidth, probability, Probabilistic logic, network sharing framework, bandwidth allocation, unpredictable network performance, stochastic virtual cluster, virtual machines, virtual network abstractions, Datacenter, VM allocation algorithms, Resource management, network bandwidth guarantee, probabilistic bandwidth demand uncertainty modeling]
Measuring and Evaluating Live Content Consistency in a Large-Scale CDN
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Content Delivery Networks (CDNs) play a central role of today's Internet infrastructure, and have seen a sharp increment in scale. More and more internet sites are armed with dynamic (or live) content (such as live sports game statistics, e-commerce and online auction), and there is a need to deliver dynamic content freshly in scale. To achieve high scalability, the consistency maintenance problem for dynamic content (contents with frequent updates) served by CDNs is non-trivial. The large number of widely scattered replicas guarantee the service QoS of end-users, meanwhile largely increase the complexity of consistency maintenance. Current consistency maintenance infrastructures and methods cannot simultaneously satisfy the two requirements: scalability and consistency. In this paper, we first analyze our crawled trace data of a cached sports game content on thousands of content servers of a major CDN. We analyze the content consistency from different perspectives, from which we try to break down the reasons for inconsistency among content servers. Finally, we further evaluate the performance in consistency, scalability and overhead for different infrastructures with different update methods. We itemize the advantages and disadvantages of different methods and infrastructures in different scenarios through the evaluation. We aim to give guidance for appropriate selections of consistency maintenance infrastructures and methods for a CDN, and for choosing a CDN service with different considerations.
[Scalability, live content consistency, Maintenance engineering, Extraterrestrial measurements, quality of service, Servers, large-scale CDN, cached sports game content, Consistency Maintenance, consistency maintenance problem, Content Distribution Network, Games, content delivery network, Internet, IP networks, service QoS, Propagation delay]
MOLStream: A Modular Rapid Development and Evaluation Framework for Live P2P Streaming
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
We present MOL Stream, a modular framework for rapid development and evaluation of P2P live streaming systems. MOL Stream allows P2P streaming protocols to be decomposed into basic blocks, each associated with a standard functional specification. By exposing structural commonalities between these components, MOL Stream enables specific implementations of these building blocks to be combined in order to devise, refine and evaluate new P2P live streaming protocols. Our approach offers several benefits. First, block encapsulation entails that more advanced individual components, e.g., the overlay, can seamlessly replace existing ones without affecting the rest of the system. As a case study, we show how MOL Stream can seamlessly substitute the overlay used by DONet/Coolstreaming, a popular P2P live streaming implementation, for an improved version. Second, MOL Stream facilitates the comparison between various protocols over local clusters or wide-area test beds such as Planet Lab. The combination of rapid prototyping and minimum effort valuation enables researchers and students to faster understand how various design choices at different levels impact the performance and scalability of the protocol, as shown through several examples in this paper. MOL Stream is written in Java and is freely available as an open-source project at https://sourceforge.net/projects/molstream/.
[Measurement, Java, Protocols, peer-to-peer computing, public domain software, software prototyping, live P2P streaming, rapid prototyping, Evaluation Framework, minimum effort valuation, Development Framework, Modular middleware, MOLStream, Live Streaming, P2P, Bandwidth, Cost function, Libraries, Peer-to-peer computing, open-source project, protocols, peer-to-peer live streaming protocols]
Polystyrene: the Decentralized Data Shape That Never Dies
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Decentralized topology construction protocols organize nodes along a predefined topology (e.g. a torus, ring, or hypercube). Such topologies have been used in many contexts ranging from routing and storage systems, to publish-subscribe and event dissemination. Since most topologies assume no correlation between the physical location of nodes and their positions in the topology, they do not handle catastrophic failures well, in which a whole region of the topology disappears. When this occurs, the overall shape of the system typically gets lost. This is highly problematic in applications in which overlay nodes are used to map a virtual data space, be it for routing, indexing or storage. In this paper, we propose a novel decentralized approach that maintains the initial shape of the topology even if a large (consecutive) portion of the topology fails. Our approach relies on the dynamic decoupling between physical nodes and virtual ones enabling a fast reshaping. For instance, our results show that a 51,200-node torus converges back to a full torus in only 10 rounds after 50% of the nodes have crashed. Our protocol is both simple and flexible and provides a novel form of collective survivability that goes beyond the current state of the art.
[ring topology, Protocols, Shape, fault-tolerance, decentralized topology construction protocols, Electronic mail, distributed computing, virtual data space map, routing, Network topology, topology shape, dynamic decoupling, event dissemination, publish-subscribe, protocols, decentralized approach, decentralized data shape, overlay nodes, hypercube topology, indexing, polystyrene, telecommunication network topology, Routing, Topology, virtual nodes, decentralized computing, physical nodes, epidemic protocols, overlay networks, telecommunication network routing, storage systems, Peer-to-peer computing, torus topology]
An Interest-Based Per-Community P2P Hierarchical Structure for Short Video Sharing in the YouTube Social Network
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
The past few years have seen an explosion in the popularity of online short-video sharing in You Tube. As the number of users continued to grow, the bandwidth required to maintain acceptable quality of service (QoS) has greatly increased. Peer-to-peer (P2P) architectures have shown promise in reducing the bandwidth costs, however, the previous works build one P2P overlay for each video, which provides limited availability of video providers and produces high overlay maintenance overhead. To handle these problems, in this work, we novelly leverage the existing social network in You Tube, where a user subscribes to another user's channel to track all his uploaded videos. The subscribers of a channel tend to watch the channel's videos and common-interest nodes tend to watch the same videos. Also, the popularity of videos in one channel varies greatly. We study real trace data to confirm these properties. Based on these properties, we propose Social Tube that builds the subscribers of one channel into a P2P overlay and also clusters common-interest nodes in a higher level. It also incorporates a prefetching algorithm that prefetches higher-popularity videos. Extensive trace-driven simulation results and Planet Lab real world experimental results verify the effectiveness of Social Tube at reducing server load and overlay maintenance overhead and at improving QoS for users.
[PlanetLab real-world experimental results, Video on demand, Quality of service, Servers, P2P networks, P2P overlay, QoS, video on demand, Bandwidth, video-on-demand services, quality-of-service, YouTube social network, cost reduction, peer-to-peer computing, high overlay maintenance overhead, Social networks, quality of service, YouTube, trace-driven simulation, server load reduction, SocialTube, peer-to-peer architectures, bandwidth cost reduction, interest-based per-community P2P hierarchical structure, online short-video sharing, common-interest nodes, Streaming media, social networking (online), Peer-to-peer computing]
Two-Party Fine-Grained Assured Deletion of Outsourced Data in Cloud Systems
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
With clients losing direct control of their data, this paper investigates an important problem of cloud systems: When clients delete data, how can they be sure that the deleted data will never resurface in the future if the clients do not perform the actual data removal themselves? How to guarantee inaccessibility of deleted data when the data is not in their possession? Using a novel key modulation function, we design a solution for two-party fine-grained assured deletion. The solution does not rely on any third-party server. Each client only keeps one or a small number of keys, regardless of how big its file system is. The client is able to delete any individual data item in any file without causing significant overhead, and the deletion is permanent - no one can recover already-deleted data, not even after gaining control of both the client device and the cloud server. We validate our design through experimental evaluation.
[modulation function, cloud server, two-party fine-grained assured deletion, Servers, data removal, client device, outsourced data, Radio frequency, storage management, already-deleted data, cloud systems, outsourcing, Modulation, Distributed databases, file servers, Outsourcing, Cryptography, cloud computing, third-party server]
Towards Online, Accurate, and Scalable QoS Prediction for Runtime Service Adaptation
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Service-based cloud applications are typically built on component services to fulfill certain application logic. To meet quality-of-service (QoS) guarantees, these applications have to become resilient against the QoS variations of their component services. Runtime service adaptation has been recognized as a key solution to achieve this goal. To make timely and accurate adaptation decisions, effective QoS prediction is desired to obtain the QoS values of component services. However, current research has focused mostly on QoS prediction of the working services that are being used by a cloud application, but little on QoS prediction of candidate services that are also important for making adaptation decisions. To bridge this gap, in this paper, we propose a novel QoS prediction approach, namely adaptive matrix factorization (AMF), which is inspired from the collaborative filtering model used in recommender systems. Specifically, our AMF approach extends conventional matrix factorization into an online, accurate, and scalable model by employing techniques of data transformation, online learning, and adaptive weights. Comprehensive experiments have been conducted based on a real-world large-scale QoS dataset of Web services to evaluate our approach. The evaluation results provide good demonstration for our approach in achieving accuracy, efficiency, and scalability.
[Adaptation models, AMF approach, Quality of service, Predictive models, runtime service adaptation, quality-of-service guarantees, service-based cloud applications, matrix decomposition, scalable QoS prediction, Accuracy, QoS prediction, data transformation, cloud computing, Recommender systems, matrix factorization, QoS dataset, QoS variations, quality of service, Service adaptation, Web services, adaptive matrix factorization, data communication, Data models, Time factors, online learning]
Columbus: Configuration Discovery for Clouds
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Low-cost, accurate and scalable software configuration discovery is the key to simplifying many cloud management tasks. However, the lack of standardization across software configuration techniques has prevented the development of a fully automated and application independent configuration discovery solution. In this work, we present Columbus, an application-agnostic system to automatically discover environmental configuration parameters or Points of Variability (PoV) in clustered applications with high accuracy. Columbus uses the insight that even though configuration mechanisms and files vary across different software, the PoVs are encoded using a few common patterns. It uses a novel rule framework to annotate file content with PoVs and a Bayesian network to estimate confidence for annotated PoVs. Our experiments confirm that Columbus can accurately discover configuration for a diverse set of enterprise and cloud applications. It has subsequently been integrated in three real-world systems that analyze this information for discovery of distributed application dependencies, enterprise IT migration and virtual application configuration.
[distributed application dependencies, estimation theory, application-agnostic system, Ports (Computers), Servers, rule framework, Semantics, IP networks, belief networks, cloud computing, Bayesian network, enterprise IT migration, Context, confidence estimation, enterprise application, software configuration discovery, automatic environmental configuration parameter discovery, cloud application, points-of-variability, configuration management, XML, virtual machines, Columbus, Software, virtual application configuration, Bayes methods, file content annotation]
Counting in Anonymous Dynamic Networks under Worst-Case Adversary
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
In this paper we investigate the problem of counting the size of a network where processes are anonymous (i.e, they share the same identifier) and the network topology constantly changes controlled by an adversary able to look internal process states and add and remove edges in order to contrast the convergence of the algorithm to the correct count. It is easy to show that, if the adversary can generate graphs without any constraint on the connectivity (i.e. it can generate topologies where there exist nodes not able to influence the others), counting is impossible. In this paper we consider a synchronous round based computation and the dynamicity is governed by a worst-case adversary that generates a sequence of graphs, one for each round, with the only constraint that each graph must be connected (1-interval connectivity property). It has been conjectured that counting in a finite time against such adversary is impossible and the existing solutions consider that each process has some knowledge about network topologies generated by the adversary, i.e. at each round, each node has a degree lesser than D. Along the path of proving the validity (or not) of the conjecture, this paper presents an algorithm that counts in a finite time against the worst-case adversary assuming each process is equipped with an oracle. The latter provides a process at each round r with an estimation of the process degree in the graph generated by the adversary at round r. To the best of our knowledge, this is the first counting algorithm (terminating in a finite time) where processes exploit the minimal knowledge about the behavior of the adversary. Interestingly, such oracle can be implemented in a wide range of real systems.
[Heuristic algorithms, graph theory, Estimation, first counting algorithm, Color, distributed processing, Topology, network topology, synchronous round based computation, Upper bound, Network topology, worst-case adversary, graph sequence, anonymous dynamic networks, Peer-to-peer computing]
The LevelArray: A Fast, Practical Long-Lived Renaming Algorithm
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
The long-lived renaming problem appears in shared-memory systems where a set of threads need to register and deregister frequently from the computation, while concurrent operations scan the set of currently registered threads. Instances of this problem show up in concurrent implementations of transactional memory, flat combining, thread barriers, and memory reclamation schemes for lock-free data structures. In this paper, we analyze a randomized solution for long-lived renaming. The algorithmic technique we consider, called the Level Array, has previously been used for hashing and one-shot (single-use) renaming. Our main contribution is to prove that, in long-lived executions, where processes may register and deregister polynomially many times, the technique guarantees constant steps on average and O (log log n) steps with high probability for registering, unit cost for deregistering, and O (n) steps for collect queries, where n is an upper bound on the number of processes that may be active at any point in time. We also show that the algorithm has the surprising property that it is self-healing: under reasonable assumptions on the schedule, operations running while the data structure is in a degraded state implicitly help the data structure re-balance itself. This subtle mechanism obviates the need for expensive periodic rebuilding procedures. Our benchmarks validate this approach, showing that, for typical use parameters, the average number of steps a process takes to register is less than two and the worst-case number of steps is bounded by six, even in executions with billions of operations. We contrast this with other randomized implementations, whose worst-case behavior we show to be unreliable, and with deterministic implementations, whose cost is linear in n.
[transaction processing, Schedules, renaming, Instruction sets, memory reclamation scheme, transactional memory, Registers, Complexity theory, Indexes, flat combining, periodic rebuilding procedures, randomized algorithms, LevelArray, thread barriers, shared memory systems, long-lived renaming problem, long-lived data structures, data structures, Arrays, lock-free data structures, shared-memory system, shared memory, computational complexity]
Deterministic Blind Rendezvous in Cognitive Radio Networks
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Blind rendezvous is a fundamental problem in cognitive radio networks. The problem involves a collection of agents (radios) that wish to discover each other (i.e., rendezvous) in the blind setting where there is no shared infrastructure and they initially have no knowledge of each other. Time is divided into discrete slots and spectrum is divided into discrete channels, [n] = 1, 2, ..., n. Each agent may access (or hop on) a single channel in a single time slot and two agents rendezvous when they hop on the same channel in the same time slot. The goal is to design deterministic channel hopping schedules for each agent so as to guarantee rendezvous between any pair of agents with access to overlapping sets of channels. The problem has three complicating considerations: first, the agents are asymmetric, i.e., each agent Ai only has access to a particular subset S<sub>i</sub> &#x2282; [n] of the channels and different agents may have access to different subsets of channels (clearly, two agents can rendezvous only if their channel subsets overlap), second, the agents are synchronous, i.e., they do not possess a common sense of absolute time, so different agents may commence their channel schedules at different times (they do have a common sense of slot duration), lastly, agents are anonymous i.e., they do not possess an identity, and hence the schedule for Ai must depend only on S<sub>i</sub>. Whether guaranteed blind rendezvous in the asynchronous model was even achievable was an open problem. In a recent breakthrough, two independent sets of authors, Shin et al. (Communications Letters, 2010) and Lin et al. (INFOCOM, 2011), gave the first constructions guaranteeing asynchronous blind rendezvous in O (n2) and O (n3) time, respectively. We present a substantially improved and conceptually simpler construction guaranteeing that any two agents, A<sub>i</sub>, A<sub>j</sub>, will rendezvous in O (|S<sub>i</sub>||S<sub>j</sub>| log log n) time. Our results are the first that achieve nontrivial dependence on |S<sub>i</sub>|, the sizes of the sets of available channels. This allows us, for example, to save roughly a quadratic factor over the best previous results in the important case when channel subsets have constant size. We also achieve the best possible bound of O (1) rendezvous time for the symmetric situation, previous works could do no better than O (n). Using techniques from the probabilistic method and Ramsey theory we establish that our construction is nearly optimal: we show both an &#x03A9; (|S<sub>i</sub>||S<sub>j</sub>|) lower bound and an &#x03A9;(log log n) lower bound when |S<sub>i</sub>|, |S<sub>j</sub>| &#x2264; n/2.
[Schedules, communications letters, Dynamic spectrum allocation, Communities, subsets overlap, Ramsey theory, cognitive radio networks, cognitive radio, discrete channels, discrete spectrum, deterministic blind rendezvous, Silicon, channel hopping, discrete slots, deterministic channel hopping schedules, nontrivial dependence, asynchronous model, asynchronous blind rendezvous, probability, Color, symmetric situation, Educational institutions, channel subsets, Cognitive radio, Deterministic blind rendezvous, Computer science, quadratic factor, probabilistic method, cognitive radio network]
Rethinking State-Machine Replication for Parallelism
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
State-machine replication, a fundamental approach to designing fault-tolerant services, requires commands to be executed in the same order by all replicas. Moreover, command execution must be deterministic: each replica must produce the same output upon executing the same sequence of commands. These requirements usually result in single-threaded replicas, which hinders service performance. This paper introduces Parallel State-Machine Replication (P-SMR), a new approach to parallelism in state-machine replication. P-SMR scales better than previous proposals since no component plays a centralizing role in the execution of independent commands-those that can be executed concurrently, as defined by the service. The paper introduces P-SMR, describes a "commodified architecture" to implement it, and compares its performance to other proposals using a key-value store and a networked file system.
[Multicore Architectures, command execution, Instruction sets, key-value store, networked file system, parallelism, Fault-Tolerance, Servers, Distributed Systems, parallel processing, single-threaded replicas, Concurrent computing, fault-tolerant service design, parallel state-machine replication, Semantics, commodified architecture, Parallel processing, System recovery, Parallelism, Libraries, fault tolerant computing, State-Machine Replication, P-SMR approach]
Propeller: A Scalable Real-Time File-Search Service in Distributed Systems
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
File-search service is a valuable facility to accelerate many analytics applications, because it can drastically reduce the scale of the input data. The main challenge facing the design of large-scale and accurate file-search services is how to support real-time indexing in an efficient and scalable way. To address this challenge, we propose a distributed file-search service, called Propeller, which utilizes a special file-access pattern, called access-causality, to partition file-indices in order to expose substantial access locality and parallelism to accelerate the file-indexing process. The extensive evaluations of Propeller show that it is real-time in file-indexing operations, accurate in file-search results, and scalable in large datasets. It achieves significantly better file-indexing and file-search performance (up to 250x) than a centralized solution (MySQL) and much higher accuracy and substantially lower query latency (up to 22x than a state-of-the-art desktop search engine (Spotlight).
[metadata, Propellers, data management, distributed processing, distributed file-search service, access-causality, real-time indexing, Accuracy, Propeller, Prototypes, scalable real-time file-search service, file-access pattern, file organisation, file search, Real-time systems, file-indexing process, Acceleration, Indexing]
Compiler Driven Automatic Kernel Context Migration for Heterogeneous Computing
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Computer systems provide different heterogeneous resources (e.g., GPUs, DSPs and FPGAs) that accelerate applications and that can reduce the energy consumption by using them. Usually, these resources have an isolated memory and a require target specific code to be written. There exist tools that can automatically generate target specific codes for program parts, so-called kernels. The data objects required for a target kernel execution need to be moved to the target resource memory. It is the programmers' responsibility to serialize these data objects used in the kernel and to copy them to or from the resource's memory. Typically, the programmer writes his own serializing function or uses existing serialization libraries. Unfortunately, both approaches require code modifications, and the programmer needs knowledge of the used data structure format. There is a need for a tool that is able to automatically extract the original kernel data objects, serialize them, and migrate them to a target resource without requiring intervention from the programmer. In this paper, we present a tool collection ConSerner that automatically identifies, gathers, and serializes the context of a kernel and migrates it to a target resource's memory where a target specific kernel is executed with this data. This is all done transparently to the programmer. Complex data structures can be used without making a modification of the program code by a programmer necessary. Predefined data structures in external libraries (e.g., the STL's vector) can also be used as long as the source code of these libraries is available.
[Context, source code (software), ConSerner, Instruments, Graphics processing units, compiler driven automatic kernel context migration, data structure format, source code, program compilers, heterogeneous computing, Memory management, data structures, Arrays, Kernel, energy consumption]
BOND: Exploring Hidden Bottleneck Nodes in Large-Scale Wireless Sensor Networks
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
In a large-scale wireless sensor network, thousands of sensor nodes periodically generate and forward data back to the sink. In our recent outdoor deployment, we observe that some bottleneck nodes can greatly determine other nodes' data collection ratio, and thus affect the whole network performance. To figure out the importance of a node in data collection, the manager needs to understand the interactive behaviors among the parent and child nodes. To address this issue, we present a management tool BOND (Bottleneck Node Detector). We introduce the concept of Node Dependence to characterize how much a node relies on each of its parent nodes. BOND models the routing process as a Hidden Markov Model, and uses a machine learning approach to learn the state transition probabilities in this model based on the observed traces. BOND utilizes Node Dependence to explore the hidden bottleneck nodes in the network. Moreover, we can predict how adding or removing the sensor nodes would impact the data flow, thus avoid data loss and flow congestion in redeployment. We implement our tool on real hardware and deploy it in an outdoor system. Our extensive experiments show that BOND infers the Node Dependence with an average accuracy of more than 85%.
[routing process, wireless sensor networks, Network diagnosis, hidden Markov model, child nodes, Bottleneck detection, large-scale wireless sensor networks, hidden Markov models, bottleneck node detector, sensor nodes, Network topology, Routing protocols, learning (artificial intelligence), BOND, Routing, node dependence, machine learning, state transition probability, Wireless sensor networks, Hidden Markov models, telecommunication network routing, hidden bottleneck nodes, management tool, Markov processes, Arrays, parent nodes, interactive behaviors]
Enhancing Visibility of Network Performance in Large-Scale Sensor Networks
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Being embedded in the physical world, wireless sensor networks (WSNs) present a wide range of failures, due to environment conditions, hardware limitations and software uncertainties, and so on. Once deployed, the interactivity of a WSN greatly decreases, which leads to limited visibility of network performance for managers to investigate sensor behaviors. Existing evidence-based approaches aim to explain particular network symptoms based on expert knowledge and heuristic experiences, which degrade diagnosis accuracy and perform unreliably. These diagnosis models define a limited group of network failures, emphasizing on expert knowledge too much, and thus fail to be adopted to different applications. In this work, we propose VN2, a novel tool to enhance the visibility of network performance. VN2 quantifies a node's state in terms of variation of 43 metrics, and trains a representative matrix of network exceptions with Non-negative Matrix Factorization (NMF) model. With this matrix, when a new network state coming up, VN2 automatically attributes abnormal symptoms to one or more root causes. We implement VN2 on test bed and real system traces. Experimental results show that VN2 models network exceptions involving small subsets of root causes, and the interpretation of root causes help us understand network behaviors in details.
[Measurement, software uncertainties, Correlation, network symptoms, wireless sensor networks, representative matrix training, large-scale sensor networks, matrix decomposition, evidence-based approaches, History, nonnegative matrix factorization model, physical world, network performance, network failures, heuristic experiences, Monitoring, diagnosis accuracy, representative matrix, expert knowledge, Receivers, NMF model, network diagnosis, WSNs, Vectors, sensor behaviors, Wireless sensor networks, root cause, VN2]
Domo: Passive Per-Packet Delay Tomography in Wireless Ad-hoc Networks
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
In multi-hop wireless ad-hoc networks, packet delivery delay is one of the most important performance metrics. While a lot of research efforts have been spent on measuring and optimizing the end-to-end delay performance, there usually lack accurate and lightweight methods for decomposing the end-to-end delay into the per-hop delay for each packet. Knowledge on the per-hop per-packet delay can greatly improve the network visibility and facilitate network measurement and management. In this paper, we propose Domo, a passive, lightweight and accurate delay tomography approach to decomposing the packet end-to-end delay into each hop. The basic idea is to formulate the problem into a set of optimization problems by carefully considering the constraints among various timing quantities. At the network side, Domo attaches a small overhead to each packet for constructing constraints of the optimization problems. At the PC side, Domo employs semi-definite relaxation and several other methods to efficiently solve the optimization problems. We implement Domo and evaluate its performance extensively using large-scale simulations. Results show that Domo significantly outperforms two existing methods, nearly tripling the accuracy of the state-of-the-art.
[per-hop per-packet delay, telecommunication network management, PC side, multihop wireless ad hoc networks, Educational institutions, Ad hoc networks, Domo, Optimization, synchronisation, network management, Wireless communication, semidefinite relaxation, timing quantities, Accuracy, optimisation, end-to-end delay performance, delays, network measurement, Tomography, packet delivery delay, Delays, passive per-packet delay tomography, ad hoc networks, network visibility]
Energy Efficient HVAC System with Distributed Sensing and Control
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
This paper presents our implementation experience in building an energy efficient HVAC system for cooling and air conditioning. The system exercises the "low exergy" theory and leverages high temperature water (18&#x00B0;C) cooling for better energy efficiency. In order to achieve this, the system decomposes the cooling and dehumidification functionalities, and employs decentralized air control for on-demand dehumidification and ventilation. The system comprises two control modules, namely, radiant cooling module and distributed ventilation module, cooperating with each other to provide the HVAC control. Abundant sensors and embedded control devices are customized and instrumented, and we develop a wireless sensor network to support control data exchange among those devices. Our experimental evaluation demonstrates that the system achieves accurate control targets and promptly responses to environment dynamics. The wireless sensor network effectively supports the system needs with long system lifespan. Compared with traditional HVAC systems, our system is of much higher energy efficiency, as measured by the standard Coefficient of Performance (COP) metric.
[energy efficient HVAC system, wireless sensor networks, air conditioning, HVAC systems, ventilation, wireless sensor network, on-demand dehumidification, building, Temperature sensors, HVAC, embedded systems, Humidity, building management systems, control engineering computing, low exergy, coefficient of performance metric, distributed control, COP, embedded control devices, abundant sensors, radiant cooling module, Temperature measurement, dehumidification functionalities, distributed sensing, Distributed control, Ventilation, Energy efficiency, control data exchange, Wireless Sensing]
SCAPE: Safe Charging with Adjustable Power
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Wireless power transfer technology is considered as one of the promising solutions to address the energy limitation problems for end-devices, but its incurred potential risk of electromagnetic radiation (EMR) exposure is largely overlooked by most existing works. In this paper, we consider the Safe Charging with Adjustable PowEr (SCAPE) problem, namely, how to adjust the power of chargers to maximize the charging utility of devices, while assuring that EMR intensity at any location in the field does not exceed a given threshold R<sub>t</sub>. We present novel techniques to reformulate SCAPE into a traditional linear programming problem, and then remove its redundant constraints as much as possible to reduce computational effort. Next, we propose a distributed algorithm with provable approximation ratio (1 - &#x03B5;). Through extensive simulation and testbed experiments, we demonstrate that our (1 - &#x03B5;)-approximation algorithm outperforms the Set-Cover algorithm by up to 23%, and has an average performance gain of 41.1% over the SCP algorithm in terms of the overall charging utility.
[linear programming problem, SCAPE problem, linear programming, Approximation methods, EMR intensity, electromagnetic radiation exposure, EMR exposure, safe charging, Wireless communication, electromagnetic waves, wireless power transfer technology, provable approximation ratio, Safety, Sensors, inductive power transmission, Distributed algorithms, approximation theory, Linear programming, charging utility, chargers, Wireless sensor networks, distributed algorithm, set-cover algorithm, safe charging with adjustable power problem, distributed algorithms, adjustable power, wireless power transfer]
Building Green Cloud Services at Low Cost
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Interest in powering data enters at least partially using on-site renewable sources, e.g. solar or wind, has been growing. In fact, researchers have studied distributed services comprising networks of such "green" data centers, and load distribution approaches that "follow the renewables" to maximize their use. However, prior works have not considered where to site such a network for efficient production of renewable energy, while minimizing both data center and renewable plant building costs. Moreover, researchers have not built real load management systems for follow-the-renewables services. Thus, in this paper, we propose a framework, optimization problem, and solution approach for sitting and provisioning green data centers for a follow-the-renewables HPC cloud service. We illustrate the location selection tradeoffs by quantifying the minimum cost of achieving different amounts of renewable energy. Finally, we design and implement a system capable of migrating virtual machines across the green data centers to follow the renewables. Among other interesting results, we demonstrate that one can build green HPC cloud services at a relatively low additional cost compared to existing services.
[green HPC cloud services, optimization problem, Batteries, Servers, virtual machine migration, parallel processing, Optimization, renewable energy, optimisation, power aware computing, green cloud services, renewable energy production, cloud computing, on-site renewable sources, Power generation, Availability, load management systems, renewable energy sources, Buildings, follow-the-renewables HPC cloud service, green data centers, datacenter, computer centres, follow-the-renewables services, Green products, virtual machines, renewable plant building costs, green computing, load distribution approaches, distributed services, location selection tradeoffs]
Almost Optimal Channel Access in Multi-Hop Networks with Unknown Channel Variables
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
We consider the problem of online dynamic channel accessing in multi-hop cognitive radio networks. Previous works on online dynamic channel accessing mainly focus on single-hop networks that assume complete conflicts among all secondary users. In the multi-hop multi-channel network settings studied here, there is more general competition among different communication pairs. A simple application of models for single-hop case to multi-hop case with N nodes and M channels leads to exponential time/space complexity O (MN), and poor theoretical guarantee on throughput performance. We thus novelly formulate the problem as a linearly combinatorial multi-armed bandits (MAB) problem that involves a maximum weighted independent set (MWIS) problem with unknown weights. To efficiently address the problem, we propose a distributed channel access algorithm that can achieve 1/&#x03C1; of the optimum averaged throughput where each node has communication complexity O (r2+D) and space complexity O (m) in the learning process, and time complexity O (D m&#x03C1;r) in strategy decision process for an arbitrary wireless network. Here &#x03C1; = 1 + &#x03B5; is the approximation ratio to MWIS for a local r-hop network with m &lt;; N nodes, and D is the number of mini-rounds inside each round of strategy decision.
[unknown channel variable, online dynamic channel access, multihop cognitive radio network, combinatorial mathematics, learning process, Interference, time complexity, Throughput, multichannel network, Complexity theory, Approximation methods, communication complexity, linearly combinatorial multiarmed bandits problem, optimisation, cognitive radio, multihop network, Spread spectrum communication, Approximation algorithms, Robustness, maximum weighted independent set problem, channel allocation, space complexity, optimal channel access]
Shared Memory Buffer Management for Heterogeneous Packet Processing
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Packet processing increasingly involves heterogeneous requirements. We consider the well-known model of a shared memory switch with bounded-size buffer and generalize it in two directions. First, we consider unit-sized packets labeled with an output port and a processing requirement (i.e., packets with heterogeneous processing), maximizing the number of transmitted packets. We analyze the performance of buffer management policies under various characteristics via competitive analysis that provides uniform guarantees across traffic patterns (Borodin and El-Yaniv, 1998). We propose the Longest-Work-Drop policy and show that it is at most 2-competitive and at least sqrt 2}-competitive. Second, we consider another generalization, posed as an open problem in [10], where each unit-sized packet is labeled with an output port and intrinsic value, and the goal is to maximize the total value of transmitted packets. We show first results in this direction and define a scheduling policy that, as we conjecture, may achieve constant competitive ratio. We also present a comprehensive simulation study that validates our results.
[Optimized production technology, Ports (Computers), Switches, Educational institutions, admission control, heterogeneous packet processing, shared memory switch, longest-work-drop policy, storage management, Upper bound, bounded-size buffer, scheduling policy, Memory management, buffer management, scheduling, shared memory systems, shared memory buffer management, unit-sized packets, packet scheduling, buffer management policies, traffic patterns, network processor, competitive analysis]
Robust Network Tomography in the Presence of Failures
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
In this paper, we study the problem of selecting paths to improve the performance of network tomography applications in the presence of network element failures. We model the robustness of paths in network tomography by a metric called expected rank. We formulate an optimization problem to cover two complementary performance metrics: robustness and probing cost. The problem aims at maximizing the expected rank under a budget constraint on the probing cost. We prove that the problem is NP-Hard. Under the assumption that the failure distribution is known, we propose an algorithm called RoMe with guaranteed approximation ratio. Moreover, since evaluating the expected rank is generally hard, we provide a bound which can be evaluated efficiently. We also consider the case in which the failure distribution is not known, and propose a reinforcement learning algorithm to solve our optimization problem, using RoMe as a subroutine. We run a wide range of simulations under realistic network topologies and link failure models to evaluate our solution against a state-of-the-art path selection algorithm. Results show that our approaches provide significant improvements in the performance of network tomography applications under failures.
[Measurement, optimization problem, NP-hard, budget constraint, failure distribution, state-of-art path selection algorithm, Approximation methods, Optimization, link failure models, optimisation, Tomography, Network Analytics, Robustness, learning (artificial intelligence), Erbium, Monitoring, expected rank, Network Monitoring, telecommunication network topology, realistic network topologies, robust network tomography, network element failures, computer network management, RoMe, probing cost, Network Measurements, computational complexity, reinforcement learning algorithm]
Client-Centric Benchmarking of Eventual Consistency for Cloud Storage Systems
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Eventually-consistent key-value storage systems sacrifice the ACID semantics of conventional databases to achieve superior latency and availability. However, this means that client applications, and hence end-users, can be exposed to stale data. The degree of staleness observed depends on various tuning knobs set by application developers (customers of key-value stores) and system administrators (providers of key-value stores). Both parties must be cognizant of how these tuning knobs affect the consistency observed by client applications in the interest of both providing the best end-user experience and maximizing revenues for storage providers. Quantifying consistency in a meaningful way is a critical step toward both understanding what clients actually observe, and supporting consistency-aware service level agreements (SLAs) in next generation storage systems. This paper proposes a novel consistency metric called Gamma that captures client-observed consistency. This metric provides quantitative answers to questions regarding observed consistency anomalies, such as how often they occur and how bad they are when they do occur. We argue that Gamma is more useful and accurate than existing metrics. We also apply Gamma to benchmark the popular Cassandra key-value store. Our experiments demonstrate that Gamma is sensitive to both the workload and client-level tuning knobs, and is preferable to existing techniques which focus on worst-case behavior.
[staleness, key-value storage system, benchmarking, application developers, cloud storage system, client-observed consistency, system administrators, key-value storage, contracts, generation storage system, Distributed computing, Tuning, client-centric benchmarking, consistency-aware service level agreements, Databases, Clustering algorithms, information retrieval systems, eventual consistency, Benchmark testing, distributed systems, cloud computing, ACID semantics, Cassandra key-value store, Clocks]
MicroFuge: A Middleware Approach to Providing Performance Isolation in Cloud Storage Systems
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Most cloud providers improve resource utilization by having multiple tenants share the same resources. However, this comes at the cost of reduced isolation between tenants, which can lead to inconsistent and unpredictable performance. This performance variability is a significant impediment for tenants running services with strict latency deadlines. Providing predictable performance is particularly important for cloud storage systems. The storage system is the performance bottleneck for many cloud-based services and therefore often determines their overall performance characteristics. In this paper, we introduce MicroFuge, a new distributed caching and scheduling middleware that provides performance isolation for cloud storage systems. MicroFuge addresses the performance isolation problem by building an empirically-driven performance model of the underlying storage system based on measured data. Using this model, MicroFuge reduces deadline misses through adaptive deadline-aware cache eviction, scheduling and load-balancing policies. MicroFuge can also perform early rejection of requests that are unlikely to make their deadlines. Using workloads from the YCSB benchmark on an EC2 deployment, we show that adding MicroFuge to the storage stack substantially reduces the deadline miss rate of a distributed storage system compared to using a deadline oblivious distributed caching middleware such as Memcached.
[Cloud computing, cloud storage system, load-balancing policy, cache storage, distributed storage system, storage, Servers, cloud-based services, caching, MicroFuge, scheduling middleware, Admission control, scheduling, Time factors, Resource management, cloud computing, adaptive deadline-aware cache eviction, distributed caching, performance isolation, Load modeling, middleware, Memcached]
S4D-Cache: Smart Selective SSD Cache for Parallel I/O Systems
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Parallel file systems (PFS) are widely-used in modern computing systems to mask the ever-increasing performance gap between computing and data access. PFSs favor large requests, and do not work well for small requests, especially small random requests. Newer Solid State Drives (SSD) have excellent performance on small random data accesses, but also incur a high monetary cost. In this study, we propose a hybrid architecture named the Smart Selective SSD Cache (S4D-Cache), which employs a small set of SSD-based file servers as a selective cache of conventional HDD-based file servers. A novel scheme is introduced to identify performance-critical data, and conduct selective cache admission to fully utilize the hybrid architecture in terms of data-access parallelism and randomness. We have implemented an S4D-Cache under the MPI-IO and PVFS2 parallel file system. Our experiments show that S4D-Cache can significantly improve I/O throughput, and is a promising approach for parallel applications.
[hybrid architecture, S4D-Cache, selective cache admission, Throughput, File servers, performance gap, cache storage, Servers, parallel processing, Parallel I/O System, hard discs, performance-critical data identification, Computer architecture, solid state drives, Parallel processing, parallel I/O systems, PFS, MPI-IO, Solid State Drive, HDD-based file servers, S4D-cache, SSD-based file servers, Middleware, smart selective SSD cache, I/O throughput improvement, Data models, PVFS2 parallel file system, parallel file systems, data-access parallelism, I/O Middleware, disc drives]
Will They Blend?: Exploring Big Data Computation Atop Traditional HPC NAS Storage
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
The Apache Hadoop framework has rung in a new era in how data-rich organizations can process, store, and analyze large amounts of data. This has resulted in increased potential for an infrastructure exodus from the traditional solution of commercial database ad-hoc analytics on network-attached storage (NAS). While many data-rich organizations can afford to either move entirely to Hadoop for their Big Data analytics, or to maintain their existing traditional infrastructures and acquire a new set of infrastructure solely for Hadoop jobs, most supercomputing centers do not enjoy either of those possibilities. Too much of the existing scientific code is tailored to work on massively parallel file systems unlike the Hadoop Distributed File System (HDFS), and their datasets are too large to reasonably maintain and/or ferry between two distinct storage systems. Nevertheless, as scientists search for easier-to-program frameworks with a lower time-to-science to post-process their huge datasets after execution, there is increasing pressure to enable use of MapReduce within these traditional High Performance Computing (HPC) architectures. Therefore, in this work we explore potential means to enable use of the easy-to-program Hadoop MapReduce framework without requiring a complete infrastructure overhaul from existing HPC NAS solutions. We demonstrate that retaining function-dedicated resources like NAS is not only possible, but can even be effected efficiently with MapReduce. In our exploration, we unearth subtle pitfalls resultant from this mash-up of new-era Big Data computation on conventional HPC storage and share the clever architectural configurations that allow us to avoid them. Last, we design and present a novel Hadoop File System, the Reliable Array of Independent NAS File System (RainFS), and experimentally demonstrate its improvements in performance and reliability over the previous architectures we have investigated.
[Apache Hadoop framework, easy-to-program Hadoop MapReduce framework, high performance computing architectures, reliable array of independent NAS file system, parallel processing, MapReduce, Big Data analytics, storage management, HPC storage, File systems, Computer architecture, distributed databases, supercomputing centers, HPC architectures, Big data, Big Data computation, data analysis, Network Attached Storage, Big Data, HPC NAS storage, Encoding, massively parallel file systems, File Systems, function-dedicated resources, network-attached storage, RainFS, Organizations, Writing, Reliability, commercial database ad-hoc analytics, data-rich organizations, Hadoop distributed file system]
T-Storm: Traffic-Aware Online Scheduling in Storm
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Storm has emerged as a promising computation platform for stream data processing. In this paper, we first show inefficiencies of the current practice of Storm scheduling and challenges associated with applying traffic-aware online scheduling in Storm via experimental results and analysis. Motivated by our observations, we design and implement a new stream data processing system based on Storm, namely, T-Storm. Compared to Storm, T-Storm has the following desirable features: 1) based on runtime states, it accelerates data processing by leveraging effective traffic-aware scheduling for assigning/re-assigning tasks dynamically, which minimizes inter-node and inter-process traffic while ensuring no worker nodes are overloaded, 2) it enables fine-grained control over worker node consolidation such that T-Storm can achieve better performance with even fewer worker nodes, 3) it allows hot-swapping of scheduling algorithms and adjustment of scheduling parameters on the fly, and 4) it is transparent to Storm users (i.e., Storm applications can be ported to run on T-Storm without any changes). We conducted real experiments in a cluster using well-known data processing applications for performance evaluation. Extensive experimental results show that compared to Storm (with the default scheduler), T-Storm can achieve over 84% and 27% speedup on lightly and heavily loaded topologies respectively (in terms of average processing time) with 30% less number of worker nodes.
[Schedules, Storm, interprocess traffic minimization, Fasteners, Big Data, performance evaluation, Data processing, Scheduling, Topology, traffic-aware online scheduling, Storm scheduling, internode traffic minimization, T-Storm, Resource Management, Storms, Stream Data Processing, scheduling, stream data processing, Internet, worker node consolidation, scheduling algorithm hot-swapping, Monitoring, telecommunication traffic]
AJIRA: A Lightweight Distributed Middleware for MapReduce and Stream Processing
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Currently, MapReduce is the most popular programming model for large-scale data processing and this motivated the research community to improve its efficiency either with new extensions, algorithmic optimizations, or hardware. In this paper we address two main limitations of MapReduce: one relates to the model's limited expressiveness, which prevents the implementation of complex programs that require multiple steps or iterations. The other relates to the efficiency of its most popular implementations (e.g., Hadoop), which provide good resource utilization only for massive volumes of input, operating sub optimally for smaller or rapidly changing input. To address these limitations, we present AJIRA, a new middleware designed for efficient and generic data processing. At a conceptual level, AJIRA replaces the traditional map/reduce primitives by generic operators that can be dynamically allocated, allowing the execution of more complex batch and stream processing jobs. At a more technical level, AJIRA adopts a distributed, multi-threaded architecture that strives at minimizing overhead for non-critical functionality. These characteristics allow AJIRA to be used as a single programming model for both batch and stream processing. To this end, we evaluated its performance against Hadoop, Spark, Esper, and Storm, which are state of the art systems for both batch and stream processing. Our evaluation shows that AJIRA is competitive in a wide range of scenarios both in terms of processing time and scalability, making it an ideal choice where flexibility, extensibility, and the processing of both large and dynamic data with a single programming model are either desirable or even mandatory requirements.
[Storm, dynamic data, Scalability, Programming, complex batch, lightweight distributed middleware, Esper, MapReduce, Fault tolerance, resource allocation, Fault tolerant systems, Computer architecture, AJIRA, generic data processing, generic operators, multithreaded architecture, resource utilization, stream processing jobs, middleware, multi-threading, Computational modeling, Hadoop, noncritical functionality, single programming model, Data models, Spark, algorithmic optimizations]
Cost-Effective Resource Allocation for Deploying Pub/Sub on Cloud
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Publish/subscribe (pub/sub) is a popular communication paradigm in the design of large-scale distributed systems. A fundamental challenge in deploying pub/sub systems on a data center or a cloud infrastructure is efficient and cost-effective resource allocation that would allow delivery of notifications to all subscribers. In this paper, we provide answers to the following three fundamental questions: Given a pub/sub workload, (1) what is the minimum amount of resources needed to satisfy all the subscribers, (2) what is a cost-effective way to allocate resources for the given workload, and (3) what is the cost of hosting it on a public Infrastructure-as-a-Service (IaaS) provider like Amazon EC2. To answer these questions, we formulate a problem coined Minimum Cost Subscriber Satisfaction (MCSS). We prove MCSS to be NP-hard and provide an efficient heuristic solution based on a combination of optimizations. We evaluate the solution experimentally using real traces from Spotify and Twitter along with a pricing model from Amazon. We show the impact of each optimization using a naive solution as the baseline. Using a variety of practical scenarios for each dataset, we also show that our solution scales well for millions of subscribers and runs fast.
[TV, Amazon EC2, NP-hard, pub/sub workload, Twitter, pub/sub systems, Spotify, Optimization, Engines, optimisation, IaaS provider, MCSS, resource allocation, Bandwidth, public infrastructure-as-a-service provider, social interaction, cloud computing, middleware, publish/subscribe, heuristic solution, pricing model, Virtual machining, large-scale distributed systems, computer centres, data center, minimum cost subscriber satisfaction, pub/sub, cost-effective resource allocation, cloud infrastructure, communication paradigm, Resource management, optimizations]
Elastic Scaling of a High-Throughput Content-Based Publish/Subscribe Engine
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Publish/subscribe (pub/sub) infrastructures running as a service on cloud environments offer simplicity and flexibility for composing distributed applications. Provisioning them appropriately is however challenging. The amount of stored subscriptions and incoming publications varies over time, and the computational cost depends on the nature of the applications and in particular on the filtering operation they require (e.g., content-based vs. topic-based, encrypted vs. non-encrypted filtering). The ability to elastically adapt the amount of resources required to sustain given throughput and delay requirements is key to achieving cost-effectiveness for a pub/sub service running in a cloud environment. In this paper, we present the design and evaluation of an elastic content-based pub/sub system: E-STREAMHUB. Specific contributions of this paper include: (1) a mechanism for dynamic scaling, both out and in, of stateful and stateless pub/sub operators, (2) a local and global elasticity policy enforcer maintaining high system utilization and stable end-to-end latencies, and (3) an evaluation using real-world tick workload from the Frankfurt Stock Exchange and encrypted content-based filtering.
[Cloud computing, Subscriptions, high-throughput content-based publish-subscribe engine, elasticity policy enforcer, Elasticity, elastic scaling, E-STREAMHUB system, distributed applications, Engines, dynamic scaling, nonencrypted filtering, Frankfurt Stock Exchange, Runtime, publish-subscribe infrastructures, encrypted filtering, cloud environment, topic-based filtering, encrypted content-based filtering, Delays, content-based filtering, Cryptography, cloud computing, filtering operation, middleware]
TiM: Fine-Grained Rate Adaptation in WLANs
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Channel condition varies frequently in wireless networks. To achieve good performance, devices need rate adaptation. In rate adaptation, choosing proper modulation schemes based on channel conditions is vital to the transmission performance. However, due to the natural character of discrete modulation types and continuous varied link conditions, we cannot make a one-to-one mapping from modulation schemes to channel conditions. This matching gap causes either over-select or under-select modulation schemes which limits throughput performance. To fill-in the gap, we propose TiM (Time-line Modulation), a novel 3-Dimensional modulation scheme by adding time dimension into current amplitude-phase domain schemes. With estimation of channel condition, TiM changes base-band data transmission time by artificially interpolating values between original data points without changing amplitude-phase domain modulation type. We implemented TiM on USRP2 and conducted comprehensive simulations. Results show that, compared with rate adaptation choosing from traditional modulation schemes, TiM can improve channel utilization up to 200%.
[discrete modulation type, USRP2, modulation, 3-dimensional modulation, Time-domain analysis, Modulation, TiM, Rate Adaptation, Mathematical model, wireless network, time-line modulation, Receivers, WLAN, fine-grained rate adaptation, continuous varied link condition, baseband data transmission, Equations, Interpolation, Modulation Scheme, channel estimation, transmission performance, Adapting Interpolation Rate, wireless LAN, channel condition estimation, Signal to noise ratio]
NomLoc: Calibration-Free Indoor Localization with Nomadic Access Points
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Newly popular indoor location-based services (ILBS), when integrated with commerce and public safety, offer a promising land for wireless indoor localization technologies. WLAN is suggested to be one of the most potential candidates owing to its prevalent infrastructure (i.e., access points (APs)) and low cost. However, the overall performance can be greatly degraded by the spatial localizability variance problem, i.e., the localization accuracy across various locations may have significant differences given any fixed AP deployment. As a result, it brings in user experience inconsistency which is unfavorable for ILBS. In this paper, we propose NomLoc - an indoor localization system using nomadic APs to address the performance variance problem. The key insight of NomLoc is to leverage the mobility of nomadic APs to dynamically adjust the WLAN network topology. A space partition (SP)-based localization algorithm is tailored for NomLoc to perform calibration-free positioning. Moreover, fine-grained channel state information (CSI) is employed to mitigate the performance degradation of the SP-based method due to multipath and none-line-of-sight (NLOS) effects. We have implemented the NomLoc system with off-the-shelf devices and evaluated the performance in two typical indoor environments. The results show that NomLoc can greatly mitigate spatial localizability variance and improve localization accuracy with the assistance of nomadic APs as compared with the corresponding static AP deployment. Moreover, it is robust to the position error of nomadic APs.
[Wireless LAN, Estimation, Indoor environments, telecommunication network topology, Calibration, mobility management (mobile radio), fine grained channel state information, Servers, wireless indoor localization technologies, calibration free positioning, indoor location based services, Accuracy, space partition based localization algorithm, calibration free indoor localization, WLAN network topology, Delays, indoor radio, wireless LAN, nomadic access points]
Generic Composite Counting in RFID Systems
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Counting the number of RFID tags is a fundamental issue and has a wide range of applications in RFID systems. Most existing protocols, however, only apply to the scenario where a single reader counts the number of tags covered by its radio, or at most the union of tags covered by multiple readers. They are unable to achieve more complex counting objectives, i.e., counting the number of tags in a composite set expression such as (S_1 big cup S_2) - (S_3 big cap S_4). This type of counting has realistic significance since it provides more diversity than existing counting scenario, and can be applied in various applications. In this paper, we formally introduce the RFID composite counting problem, which aims at counting the tags in arbitrary set expression. We obtain strong lower bounds on the communication cost of composite counting. We then propose a generic Composite Counting Framework (CCF) that provides estimates for any set expression with desired accuracy. The communication cost of CCF is proved to be within a small factor from the optimal. We build a prototype system for CCF using USRP software defined radio and Intel WISP computational tags. Also, extensive simulations are conducted to evaluate the performance of CCF. The experimental results show that CCF is generic, accurate and time-efficient.
[Protocols, radiofrequency identification, USRP software defined radio, Estimation, CCF, Complexity theory, Servers, RFID tags, Integrated circuits, Intel WISP computational tags, Accuracy, software radio, protocols, generic composite counting framework, Radiofrequency identification]
Competitive FIB Aggregation without Update Churn
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
This paper attends to the well-known problem of compressing the Forwarding Information Base of a router or switch, while preserving a correct forwarding. In contrast to related work, we study an online variant of the problem where BGP routes can change over time, and where the number of updates to the FIB are taken into account explicitly. Minimizing the number of FIB updates is important, especially when they are sent across the network (e.g., from the network-attached SDN controller). This paper pursues a competitive analysis approach and introduces a formal model which is an interesting generalization of several classic online aggregation problems. The main contribution is a O (w)-competitive algorithm, where w is the length of an IP address. We also derive a lower bound which shows that our result is asymptotically optimal within a natural class of algorithms.
[Algorithm design and analysis, correct forwarding, online aggregation problems, competitive FIB aggregation, FIB updates, Networking, Ports (Computers), Prefix Aggregation, competitive analysis approach, switch, router, Image color analysis, competitive algorithm, IP networks, forwarding information base compression, Competitive Analysis, Radiation detectors, Color, competitive algorithms, Routing, IP address, Ski Rental, formal model, BGP routes, telecommunication network routing, network-attached SDN controller]
Achieving Absolutely Optimal Block Pipelining in Organized Network Coded Gossip
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
We use random linear network coding with simple connection topology control to approach the theoretical limit on finish time of disseminating k blocks in a server cluster of n nodes. Unlike existing gossip schemes which rely on completely random contact, we prove that with random linear network coding, any receiver selection following a simple permutation rule can achieve a broadcast finish time of k + n and that a time-varying random permutation topology achieves a finish time of k + o (k) + O (log n), both with high probability. Since the theoretical limit on finish time is k + log2 n, our simple permutation algorithms achieve absolutely optimal (not only order-optimal) block pipelining for k blocks. Our results hold for both one-to-all (broadcast) and all-to-all transfers. We demonstrate the usefulness of the proposed organized network coded gossip with an application to content distribution in cluster computing systems like MapReduce.
[workstation clusters, time-varying random permutation topology, cluster computing systems, Servers, completely random contact, MapReduce, content distribution, all-to-all transfers, Clustering algorithms, file servers, permutation algorithms, broadcast finish time, connection topology control, one-to-all transfers, network coding, random linear network coding, telecommunication network topology, Vectors, optimal block pipelining, Topology, gossip schemes, Pipeline processing, server cluster, organized network coded gossip, absolutely optimal block pipelining, randomized algorithms, Data transfer, gossip, Peer-to-peer computing, pipeline processing]
Lock-Free Cuckoo Hashing
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
This paper presents a lock-free cuckoo hashing algorithm, to the best of our knowledge this is the first lock-free cuckoo hashing in the literature. The algorithm allows mutating operations to operate concurrently with query ones and requires only single word compare-and-swap primitives. Query of items can operate concurrently with others mutating operations, thanks to the two-round query protocol enhanced with a logical clock technique. When an insertion triggers a sequence of key displacements, instead of locking the whole cuckoo path, our algorithm breaks down the chain of relocations into several single relocations which can be executed independently and concurrently with other operations. A fine tuned synchronization and a helping mechanism for relocation are designed. The mechanisms allow high concurrency and provide progress guarantees for the data structure's operations. Our experimental results show that our lock-free cuckoo hashing performs consistently better than two efficient lock-based hashing algorithms, the chained and the hopscotch hash-map, in different access pattern scenarios.
[Algorithm design and analysis, cuckoo hashing, lock-based hashing algorithms, lock-free, lock-free cuckoo hashing algorithm, two-round query protocol, Concurrent computing, query processing, fine tuned synchronization, Semantics, logical clock technique, data structures, mutating operations, key displacements, data structure operations, hash table, Radiation detectors, concurrent data structures, compare-and-swap primitives, Synchronization, Indexes, access pattern scenarios, hopscotch hashmap, CAS, file organisation, Clocks]
Enforcing Location and Time-Based Access Control on Cloud-Stored Data
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Recent incidents of data-breaches from the cloud suggest that users should not trust the cloud provider to enforce access control on their data. We focus on mitigating trust to the cloud in scenarios where granting access to data not only considers user identities (as in conventional access policies), but also contextual information such as the user's location and time of access. Previous work in this context assumes a fully trusted cloud that is further capable of locating users. We introduce LoTAC, a novel framework that seamlessly integrates the operation of a cloud provider and a localization infrastructure to enforce location- and time-based access control to cloud-stored data. In LoTAC, the two entities operate independently and are only trusted to offer their basic services: the cloud provider is used and trusted only to reliably store data, the localization infrastructure is used and trusted only to accurately locate users. Furthermore, neither the cloud provider nor the localization infrastructure can access the data, even if they collude. LoTAC protocols require no changes to the cloud provider and minimal changes to the localization infrastructure. We evaluate our protocols using a cellular network as the localization infrastructure and show that they incur in low communication and computation costs and scale well with a large number of users and policies.
[Access control, Protocols, cloud provider, cellular network, Encryption, cloud-stored data, Servers, contextual information, Zinc, cloud, time-based access control, Public key, authorisation, LoTAC protocols, access control, location-based, localization infrastructure, cloud computing]
Impact Analysis of Topology Poisoning Attacks on Economic Operation of the Smart Power Grid
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
The Optimal Power Flow (OPF) routine used in energy control centers allocates individual generator outputs by minimizing the overall cost of generation subject to system level operating constraints. The OPF relies on the outputs of two other modules, namely topology processor and state estimator. The topology processor maps the grid topology based on statuses received from the switches and circuit breakers across the system. The state estimator computes the system state, i.e., voltage magnitudes with phase angles, transmission line flows, and system loads based on real-time meter measurements. However, topology statuses and meter measurements are vulnerable to false data injection attacks. Recent research has shown that such cyber attacks can be launched against state estimation where adversaries can corrupt the states but still remain undetected. In this paper, we show how the stealthy topology poisoning attacks can compromise the integrity of OPF, and thus undermine economic operation. We describe a formal verification based framework to systematically analyze the impact of such attacks on OPF. The proposed framework is illustrated with an example. We also evaluate the scalability of the framework with respect to time and memory requirements.
[state estimator, memory requirements, smart power grids, optimal power flow, switches, system state, smart power grid, time requirements, Power Grid, OPF routine, phase angles, Power measurement, formal verification, system loads, Transmission line measurements, circuit breakers, Mathematical model, State estimation, power meters, system level operating constraints, false data injection attacks, grid topology, cyber attacks, State Estimation, Formal Method, transmission line flows, Vectors, Topology, Impact of Stealthy Attacks, power engineering computing, voltage magnitudes, economic operation, Equations, power measurement, power system economics, power system security, impact analysis, Optimal Power Flow, security of data, real-time meter measurements, energy control centers, topology processor, state estimation, generator outputs]
Turret: A Platform for Automated Attack Finding in Unmodified Distributed System Implementations
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Security and performance are critical goals for distributed systems. The increased design complexity, incomplete expertise of developers, and limited functionality of existing testing tools often result in bugs and vulnerabilities that prevent implementations from achieving their design goals in practice. Many of these bugs, vulnerabilities, and misconfigurations manifest after the code has already been deployed making the debugging process difficult and costly. In this paper, we present Turret, a platform for automatically finding performance attacks in unmodified implementations of distributed systems. Turret does not require the user to provide any information about vulnerabilities and runs the implementation in the same operating system setup as the deployment, with an emulated network. Turret uses a new attack finding algorithm and several optimizations that allow it to find attacks in a matter of minutes. We ran Turret on 5 different distributed system implementations specifically designed to tolerate insider attacks, and found 30 performance attacks, 24 of which were not previously reported to the best of our knowledge.
[Algorithm design and analysis, program debugging, Protocols, automated attack finding, distributed processing, Turret, Optimization, operating system setup, security, design complexity, security of data, Operating systems, debugging process, unmodified distributed system, operating systems (computers), distributed systems, Libraries, Real-time systems, automatic attack finding, Testing]
Incremental Deployment Strategies for Effective Detection and Prevention of BGP Origin Hijacks
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
A variety of solutions have been proposed for detecting and preventing IP hijack attacks. Despite potentially serious consequences these solutions have not been widely deployed, partially because many ISPs do not view their risk as large enough to warrant investment. Nevertheless, a number of organizations such as critical national infrastructure are at a very high risk level and require a deployed solution. Is it possible for these sites to be protected despite the majority apathy, given that a critical mass of ISPs is generally required to participate in the solution? We examine this conflict by presenting an approach which determines AS vulnerability based on topological location. We next examine the effectiveness of incremental security deployment. We separately examine BGP hijack detection which, if improperly peered, may completely miss a hijack. Finally, we address a pessimistic view with respect to deployment and propose an approach in which an autonomous system can act in its own self-interest to determine a minimal threshold for hijack detection or prevention.
[Measurement, IP Hijacking, simulation, Security, BGP hijack prevention, ROVER, Analytical models, hijack prevention, IP networks, BGP Security, hijack detection, IP hijack attacks, Internet protocol, incremental deployment strategies, Routing, BGP origin hijacks, Internet service providers, BGP, critical national infrastructure, topological location, incremental security deployment, computer network security, border gateway protocol, Resistance, AS vulnerability, Internet]
[Publisher's information]
2014 IEEE 34th International Conference on Distributed Computing Systems
None
2014
Provides a listing of current committee members and society officers.
[]
Message from the General Chair
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from the Program Chair
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Organizing Committee
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Provides a listing of current committee members and society officers.
[]
Technical Program Committee
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Provides a listing of current committee members and society officers.
[]
Crowd Map: Accurate Reconstruction of Indoor Floor Plans from Crowdsourced Sensor-Rich Videos
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Lack of an accurate and low-cost method to reconstruct indoor maps is the main reason behind the current sporadic availability of digital building floor plans. The conventional approach using professional equipment is very costly and only available in the most popular areas. In this paper, we propose and demonstrate CrowdMap, a crowd sourcing system utilizing sensor-rich video data from mobile users for indoor floor plan reconstruction with low-cost. The key idea of CrowdMap is to first jointly leverage crowd sourced sensory and video data to track user movements, then use the inferred user motion traces and context of the image to produce an accurate floor plan. In particular, we exploit the sequential relationship between each consecutive frame abstracted from the video to improve system performance. Our experiments in three college buildings show that CrowdMap achieves a precision of hallway shape around 88%, a recall around 93% and a F-measure around 90%. In addition, we achieve on average 9.8% room area error and on average 6.5% room aspect ratio error. The evaluation result demonstrates a significant improvement of accuracy compared with other crowd sourcing floor plan reconstruction systems.
[user movement tracking, video, design engineering, Image reconstruction, Videos, crowdsourced sensor-rich videos, CrowdMap, mobile sensing, Skeleton, Trajectory, college buildings, building management systems, video signal processing, Crowdsourcing, crowdsourcing, Buildings, cartography, image reconstruction, civil engineering computing, digital building floor plans, indoor floor plan reconstruction, F-measure, crowdsourcing floor plan reconstruction systems, system, Layout, Floorplan, reconstruction, professional equipment, video data]
Crowd-ML: A Privacy-Preserving Learning Framework for a Crowd of Smart Devices
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Smart devices with built-in sensors, computational capabilities, and network connectivity have become increasingly pervasive. Crowds of smart devices offer opportunities to collectively sense and perform computing tasks at an unprecedented scale. This paper presents Crowd-ML, a privacy-preserving machine learning framework for a crowd of smart devices, which can solve a wide range of learning problems for crowd sensing data with differential privacy guarantees. Crowd-ML endows a crowd sensing system with the ability to learn classifiers or predictors online from crowd sensing data privately with minimal computational overhead on devices and servers, suitable for practical large-scale use of the framework. We analyze the performance and scalability of Crowd-ML and implement the system with off-the-shelf smartphones as a proof of concept. We demonstrate the advantages of Crowd-ML with real and simulated experiments under various conditions.
[Performance evaluation, Data privacy, Scalability, Noise, Crowd-ML, computational capabilities, smart phones, built-in sensors, proof of concept, Servers, crowdsensing data, off-the-shelf smartphones, Privacy, privacy-preserving machine learning framework, network connectivity, data privacy, smart devices, Sensors, learning (artificial intelligence), minimal computational overhead]
Urban Traffic Monitoring with the Help of Bus Riders
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Real-time urban traffic conditions are critical to wide populations in the city and serve the needs of many transportation dependent applications. This paper presents our experience of building a participatory urban traffic monitoring system that exploits the power of bus riders' mobile phones. The system takes lightweight sensor hints and collects minimum set of cellular data from the bus riders' mobile phones. Based on such a participatory sensing framework, the system turns buses into dummy probes, monitors their travel statuses, and derives the instant traffic map of the city. Unlike previous works that rely on intrusive detection or full cooperation from "probe vehicles\
[bus rider mobile phones, fine-grained traffic estimation, Roads, participatory sensing framework, intrusive detection, Poles and towers, transportation dependent applications, urban traffic monitoring system, Mobile handsets, Global Positioning System, Vehicles, mobile computing, sensors, lightweight sensor, traffic information systems, real-time urban traffic conditions, cellular data, instant traffic map, information source providers, Sensors, probe vehicles, Monitoring, mobile handsets]
Privacy-Preserving Compressive Sensing for Crowdsensing Based Trajectory Recovery
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Location based services have experienced an explosive growth and evolved from utilizing a single location to the whole trajectory. Due to the hardware and energy constraints, there are usually many missing data within a trajectory. In order to accurately recover the complete trajectory, crowdsensing provides a promising method. This method resorts to the correlation among multiple users' trajectories and the advanced compressive sensing technique, which significantly outperforms conventional interpolation methods on accuracy. However, as trajectories exposes users' daily activities, the privacy issue is a major concern in crowdsensing. While existing solutions independently tackle the accurate trajectory recovery and privacy issues, yet no single design is able to address these two challenges simultaneously. Therefore in this paper, we propose a novel Privacy Preserving Compressive Sensing (PPCS) scheme, which encrypts a trajectory with several other trajectories while maintaining the homomorphic obfuscation property for compressive sensing. Under PPCS, adversaries can only capture the encrypted data, so the user privacy is preserved. Furthermore, the homomorphic obfuscation property guarantees that the recovery accuracy of PPCS is comparable to the state-of-the-art compressive sensing design. Based on two publicly available traces with numerous users and long durations, we conduct extensive simulations to evaluate PPCS. The results demonstrate that PPCS achieves a high accuracy of &lt;;53 m and a large distortion between the encrypted and the original trajectories (a commonly adopted metric of privacy strength) of &gt;9,000 m even when up to 50% original data are missing.
[user daily activities, privacy issue, compressed sensing, location based services, trajectory encryption, interpolation methods, Encryption, PPCS scheme, homomorphic obfuscation property, Servers, Privacy, Accuracy, interpolation, mobile computing, privacy preserving compressive sensing scheme, compressive sensing technique, crowdsensing based trajectory recovery, user trajectories, energy constraints, data privacy, Trajectory, Compressed sensing]
Optimizing Roadside Advertisement Dissemination in Vehicular Cyber-Physical Systems
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
In this paper, we address a promising application in the Vehicular Cyber-Physical Systems (VCPS) called roadside advertisement dissemination. Its application involves three elements: the drivers in the vehicles, Roadside Access Points (RAPs), and shopkeepers. The shopkeeper wants to attract as many customers as possible, through using RAPs to disseminate advertisements to the passing vehicles. Upon receiving an advertisement, the driver may detour towards the shop, depending on the detour distance. Given a fixed number of RAPs and the traffic distribution, our goal is to optimize the RAP placement for the shopkeeper to maximally attract potential customers. This application is a non-trivial extension of traditional coverage problems, the difference being that we use RAPs to cover the traffic flows. RAP placement algorithms may pose complex trade-offs. If we place RAPs at locations that can provide small detour distances to attract more customers, these locations may not necessarily be located in heavy traffic regions. While heavy traffic regions cover more flows, they can cause large detour distances, making shopping less attractive to customers. To balance this trade off, novel RAP placement algorithms are proposed. Since real-world traffic distributions exhibit unique patterns, here we further consider the Manhattan grid scenario and then propose corresponding near-optimal solutions. Real trace-driven experiments validate the competitive performance of the proposed algorithms.
[Greedy algorithms, Computers, advertising data processing, roadside access points, Conferences, Distributed computing, Vehicular cyber-physical systems, Vehicles, roadside advertisement dissemination, Manhattan grid scenario, shopkeepers, vehicle drivers, driver information systems, vehicular cyber-physical systems, RAP placement algorithms, advertisement dissemination, Approximation algorithms, placement, coverage problem]
Point-to-Point Traffic Volume Measurement through Variable-Length Bit Array Masking in Vehicular Cyber-Physical Systems
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
In this paper, we consider an important problem of privacy-preserving point-to-point traffic volume measurement in vehicular cyber physical systems (VCPS), whose focus is utilizing VCPS to enable automatic traffic data collection, and measuring point-to-point traffic volume while preserving the location privacy of all participating vehicles. The novel scheme that we propose tackles the efficiency, privacy, and accuracy problems encountered by previous solutions. Its applicability is demonstrated through both mathematical and numerical analysis. The simulation results also show its superior performance.
[privacy-preserving point-to-point traffic volume measurement, variable-length bit array masking, vehicles, traffic engineering computing, mathematical analysis, Servers, automatic traffic data collection, Vehicles, Privacy, Accuracy, Volume measurement, location privacy, vehicular cyber-physical systems, numerical analysis, VCPS, data privacy, Arrays]
Community-Based Bus System as Routing Backbone for Vehicular Ad Hoc Networks
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Low delivery latency and high delivery ratio are two key goals in the design of routing schemes in Vehicular Ad Hoc Networks (VANETs). The existing routing schemes utilize real-time information (e.g., Geographical position and vehicle density) and historical information (e.g., Contacts of vehicles), which usually suffer from a long delivery latency and a low delivery ratio. Inspired by the unique features of bus systems such as wide coverage, fixed routes and regular service, we propose to use the bus systems as routing backbones of VANETs. In this work, we present a Community-based Bus System (CBS) which consists of two components: a community-based backbone and a routing scheme over the backbone. We collect real traces of 2515 buses in Beijing and build a community-based backbone by applying community detection techniques in the Beijing bus system. A two-level routing scheme is proposed to operate over the backbone. The proposed routing scheme performs sequentially in the inter-community level and the intra-community level, and is able to support message delivery to both mobile vehicles and specific locations/areas. Extensive experiments are conducted on the real trace data of the Beijing bus system and the results show that CBS can significantly lower the delivery latency and improve the delivery ratio. CBS is applicable to any bus-based VANETs.
[vehicular ad hoc networks, signal detection, Social network services, Communities, electronic messaging, intracommunity level, intercommunity level, mobile vehicles, Routing, Mobile communication, VANET routing backbone, community detection technique, Vehicles, Global Positioning System, Beijing, routing, Vehicular ad hoc networks, telecommunication network routing, vehicular ad hoc network, bus systems, message delivery, community-based bus system, VANETs, backbone]
Asynchronous Adaptive Task Allocation
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
We present a randomized algorithm for asynchronous task allocation, also known as the write-all or do-all problem. Our algorithm has work complexity O(n+k2 log3 k) with high probability, where n the number of tasks and k the number of processes that participate in the computation. Our solution uses O(n) shared memory space that supports atomic test-and-set operations and with high probability each participating process uses O(k) internal memory space. This is the first adaptive solution for the write-all problem that has work n plus some additive term which depends only on the number of participating processes k and not the size of the problem n.
[Additives, Computational modeling, write-all, asynchronous adaptive task allocation, Computer crashes, Complexity theory, randomized algorithm, Indexes, distributed computing, internal memory space, task allocation, Program processors, asynchronous shared memory, randomized algorithms, shared memory systems, atomic test-and-set operations, write-all problem, Resource management, shared memory space, computational complexity]
Competitive Strategies for Online Cloud Resource Allocation with Discounts: The 2-Dimensional Parking Permit Problem
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Cloud computing heralded an era where resources can be scaled up and down elastically and in an online manner. This paper initiates the study of cost-effective cloud resource allocation algorithms under price discounts, using a competitive analysis approach. We show that for a single resource, the online resource renting problem can be seen as a 2-dimensional variant of the classic online parking permit problem, and we formally introduce the PPP2 problem accordingly. Our main contribution is an online algorithm for PPP2 which achieves a deterministic competitive ratio of k (under a certain set of assumptions), where k is the number of resource bundles. This is almost optimal, as we also prove a lower bound of k/3 for any deterministic online algorithm. Our online algorithm makes use of an optimal offline algorithm, which may be of independent interest since it is the first optimal offline algorithm for the 1D and 2D versions of the parking permit problem. Finally, we show that our algorithms and results also generalize to multiple resources (i.e., Multi-dimensional parking permit problems).
[Algorithm design and analysis, multidimensional parking permit problems, online cloud resource allocation algorithm, Heuristic algorithms, optimal offline algorithm, online resource renting problem, deterministic algorithms, competitive analysis approach, Upper bound, resource allocation, Cloud Computing, Resource Allocation, deterministic online algorithm, Online Algorithm, online parking permit problem, price discounts, Resource management, cloud computing, Contracts, pricing, PPP2 problem, Competitive Analysis]
Cost-Effective Low-Delay Cloud Video Conferencing
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
The cloud computing paradigm has been advocated in recent video conferencing system design, which exploits the rich on-demand resources spanning multiple geographic regions of a distributed cloud, for better conferencing experience. A typical architectural design in cloud environment is to create video conferencing agents, i.e., Virtual machines, in each cloud site, assign users to the agents, and enable inter-user communication through the agents. Given the diversity of devices and network connectivities of the users, the agents may also transcode the conferencing streams to the best formats and bitrates. In this architecture, two key issues exist on how to effectively assign users to agents and how to identify the best agent to perform a Transco ding task, which are nontrivial due to the following: (1) the existing proximity-based assignment may not be optimal in terms of inter-user delay, which fails to consider the whereabouts of the other users in a conferencing session, (2) the agents may have heterogeneous bandwidth and processing availability, such that the best Transco ding agents should be carefully identified, for cost minimization while best serving all the users requiring the transcoded streams. To address these challenges, we formulate the user-to-agent assignment and Transco ding-agent selection problems, which targets at minimizing the operational cost of the conferencing provider while keeping the conferencing delay low. The optimization problem is combinatorial in nature and difficult to solve. Using Markov approximation framework, we design a decentralized algorithm that provably converges to a bounded neighborhood of the optimal solution. An agent ranking scheme is also proposed to properly initialize our algorithm so as to improve its convergence. The results from a prototype system implementation show that our design in a set of Internet-scale scenarios reduces the operational cost by 77% as compared to a commonly-adopted alternative, while simultaneously yielding lower conferencing delays.
[heterogeneous bandwidth, Heuristic algorithms, convergence, Combinatorial Network Problem, Approximation methods, Parallel Algorithm, transcoding-agent selection problems, Markov approximation framework, decentralized algorithm, Cloud Computing, optimization, operational cost, Bandwidth, cloud environment, cloud computing, cloud video conferencing agent, cost minimization, user-to-agent assignment, Transcoding, Video Conferencing, virtual machine, video conferencing system design, virtual machines, Markov processes, inter-user communication, Approximation algorithms, Delays, Internet, on-demand resource spanning multiple geographic region]
eTrain: Making Wasted Energy Useful by Utilizing Heartbeats for Mobile Data Transmissions
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
With the rapid proliferation of smartphones, hundreds of millions of mobile users are attracted to Instant Messaging (IM) apps. While such apps have brought convenience to our life, it comes with the price of great energy consumption, as these apps keep sending heartbeat messages to the server periodically in order to maintain an always-online connection. These frequent and fragmented transmissions result in a considerable amount of energy waste. In this paper, we investigate the "cost and potential of heartbeats". We quantify power consumption of heartbeats of real-world IM apps through extensive measurements. The measurement results confirm that huge power consumption is induced by heartbeats. The goal of this paper is to save energy by turning the energy wastage of heartbeats into transmitting useful data. Thus, we develop eTrain, a transmission management system running on Android phones, which takes advantage of IM heartbeats (as trains) to piggyback aggregated delay-tolerant apps' data such as e-mail and Weibo (as cargoes) via an online transmission strategy, so as to minimize the cumulative tail energy without sacrificing user-specified deadlines. Compared to other existing works, eTrain can reduce more energy consumption under the same settings. Experiments conducted on smartphones show that eTrain can achieve 12%-33% energy saving in various application scenarios.
[energy wastage, Energy consumption, IM heartbeats, mobile data transmissions, Mobile communication, smart phones, instant messaging apps, mobility management (mobile radio), delay-tolerant apps, eTrain, power consumption, cargoes, Weibo, mobile computing, Heart beat, transmission management system, e-mail, Android phones, smartphones, Delays, mobile users, Data communication, Smart phones, Monitoring]
TIDE: A User-centric Tool for Identifying Energy Hungry Applications on Smartphones
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Today, many smartphone users are unaware of what applications (apps) they should stop using to prevent their battery from running out quickly. The problem is identifying such apps is hard due to the fact that there exist hundreds of thousands of apps and their impact on the battery is not well understood. We show via extensive measurement studies that the impact of an app on battery consumption depends on both environmental (wireless) factors and usage patterns. Based on this, we argue that there exists a critical need for a tool that allows a user to (a) identify apps that are energy hungry, and (b) understand why an app is consuming energy, on her phone. Towards addressing this need, we present TIDE, a tool to detect high energy apps on any particular smartphone. TIDE's key characteristic is that it accounts for usage-centric information while identifying energy hungry apps from among a multitude of apps that run simultaneously on a user's phone. Our evaluation of TIDE on a testbed of Android-based smartphones, using weeklong smartphone usage traces from 17 real users, shows that TIDE correctly identifies over 94% of energy-hungry apps and has a false positive rate of &lt;; 6%.
[Energy consumption, environmental factor, environmental factors, Energy measurement, energy hungry application, smart phones, Batteries, wireless factor, Tides, Videos, high energy application, TIDE, Android (operating system), power aware computing, usage-centric information, Android-based smartphone, user-centric tool, energy management, user-centric, smartphones, Smart phones, Monitoring]
Improve Charging Capability for Wireless Rechargeable Sensor Networks Using Resonant Repeaters
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Wireless charging has provided a convenient alternative to renew sensors' energy in wireless sensor networks. Due to physical limitations, previous works have only considered recharging a single node at a time, which has limited efficiency and scalability. Recent advance on multi-hop wireless charging is gaining momentum to provide fundamental support to address this problem. However, existing single-node charging designs do not consider and cannot take advantage of such opportunities. In this paper, we propose a new framework to enable multi-hop wireless charging using resonant repeaters. First, we present a realistic model that accounts for detailed physical factors to calculate charging efficiencies. Second, to achieve balance between energy efficiency and data latency, we propose a hybrid data gathering strategy that combines static and mobile data gathering to overcome their respective drawbacks and provide theoretical analysis. Then we formulate multi-hop recharge schedule into a bi-objective NP-hard optimization problem. We propose a two-step approximation algorithm that first finds the minimum charging cost and then calculates the charging vehicles' moving costs with bounded approximation ratios. Finally, upon discovering more room to reduce the total system cost, we develop a post-optimization algorithm that iteratively adds more stopping locations for charging vehicles to further improve the results. Our extensive simulations show that the proposed algorithms can handle dynamic energy demands effectively, and can cover at least three times of nodes and reduce service interruption time by an order of magnitude compared to the single-node charging scheme.
[Coils, Energy consumption, wireless sensor networks, mobile data gathering, mobile energy replenishment, Approximation methods, service interruption time reduction, Relays, post-optimization algorithm, optimisation, multi-hop wireless charging, radio repeaters, energy efficiency, hybrid data gathering, hybrid data gathering strategy, two-step approximation algorithm, cost reduction, approximation theory, Inductive charging, multihop wireless charging, data latency, Repeaters, biobjective NP-hard optimization problem, Wireless sensor networks, energy conservation, wireless Rechargeable sensor network, Approximation algorithms, resonant repeater]
Ostro: Scalable Placement Optimization of Complex Application Topologies in Large-Scale Data Centers
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
A complex cloud application consists of virtual machines (VMs) running software such as web servers and load balancers, storage in the form of disk volumes, and network connections that enable communication between VMs and between VMs and disk volumes. The application is also associated with various requirements, including not only quantities such as the sizes of the VMs and disk volumes, but also quality of service (QoS) attributes such as throughput, latency, and reliability. This paper presents Ostro, an Open Stack-based scheduler that optimizes the utilization of data center resources, while satisfying the requirements of the cloud applications. The novelty of the approach realized by Ostro is that it makes holistic placement decisions, in which all the requirements of an application -- described using an application topology abstraction -- are considered jointly. Specific placement algorithms for application topologies are described including an estimate-based greedy algorithm and a time-bounded A algorithm. These algorithms can deal with complex topologies that have heterogeneous resource requirements, while still being scalable enough to handle the placement of hundreds of VMs and volumes across several thousands of host servers. The approach is evaluated using both extensive simulations and realistic experiments. These results show that Ostro significantly improves resource utilization when compared with naive approaches.
[complex cloud application, scalable placement optimization, Switches, Quality of service, heterogeneous resource requirements, scalability, cloud, Network topology, QoS, optimization, network connections, Bandwidth, cloud computing, Ostro, application topology abstraction, greedy algorithms, large-scale data centers, host servers, Web servers, Topology, quality of service, estimate-based greedy algorithm, complex application topologies, time-bounded A* algorithm, computer centres, OpenStack-based scheduler, performance, Heating, virtual machines, VM, disk volumes, Reliability, load balancers]
eBay in the Clouds: False-Name-Proof Auctions for Cloud Resource Allocation
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
The paradigm of cloud computing has spontaneously prompted a wide interest in auction-based mechanisms for cloud resource allocation. To eliminate market manipulation, a number of strategy-proof (a.k.a. Truthful) cloud auction mechanisms have been recently proposed by enforcing bidders to bid their true valuations of the cloud resources. However, as discovered in this paper, they would suffer from a new cheating pattern, named false-name bids, where a bidder can gain profit by submitting bids under multiple fictitious names (e.g, Multiple e-mail addresses). Such false-name cheating is easy to make but hard to detect in cloud auctions. To tackle this issue, we propose FAITH, a new False-name-proof Auction for virtual machine instance allocation, that is proven both strategy-proof and false-name proof by our theoretical analysis. When N users compete for M different types of computing instances with multiple units, FAITH achieves a lower time complexity of O(N log N+NM) compared to exiting cloud auction designs. We further extend FAITH to support range-based requests as desired in practice for flexible auction. Through extensive simulation experiments, we show that FAITH highly improves auction efficiency, outperforming the extended mechanisms of conventional false-name-proof auctions in terms of generated revenue and social welfare by up to 220% and 140%, respectively.
[revenue generation, Cloud computing, false-name-proof auctions, virtual machine instance allocation, auction efficiency improvement, range-based requests, cheating pattern, Cost accounting, cloud resource valuations, resource allocation, social welfare, Cloud Computing, cloud resource allocation, Pricing, false-name bids, strategy-proof cloud auction mechanisms, eBay, FAITH, cloud computing, electronic commerce, profit gain, multiple fictitious names, Computational modeling, time complexity, Virtual machining, multiple e-mail addresses, virtual machines, Resource management, Mechanism Design, computational complexity]
T-Chain: A General Incentive Scheme for Cooperative Computing
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
In this paper, we propose a simple, distributed, but highly efficient fairness-enforcing incentive mechanism for cooperative computing. The proposed incentive scheme, called Triangle Chaining (T-Chain), enforces reciprocity to minimize the exploitable aspects of other schemes that allow free-riding. In T-Chain, symmetric key cryptography provides the basis for a lightweight, almost-fair exchange protocol, which is coupled with a pay-it-forward mechanism. This combination increases the opportunity for multi-lateral exchanges and further maximizes the resource utilization of participants, each of whom is assumed to operate solely for his or her own benefit. T-Chain also provides barrier-free entry to newcomers with flexible resource allocation, providing them with immediate benefits, and therefore is suitable for dynamic environments with high churn (i.e., Turnover). TChain is distributed and simple to implement, as no trusted third party is required to monitor or enforce the scheme, nor is there any reliance on reputation information or tokens.
[flexible resource allocation, Protocols, cryptographic protocols, Pay it forward, triangle chaining, barrier-free entry, Encryption, cooperative computing, almost-fair exchange protocol, Incentive schemes, fairness-enforcing incentive mechanism, resource allocation, Peer to Peer, Bandwidth, cooperative systems, Free-riding, Monitoring, T-Chain, Incentive Scheme, general incentive scheme, incentive schemes, pay-it-forward mechanism, Cooperative Computing, Resource management, symmetric key cryptography]
Data Center Sprinting: Enabling Computational Sprinting at the Data Center Level
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Microprocessors may need to keep most of their cores off in the era of dark silicon due to thermal constraints. Recent studies have proposed Computational Sprinting, which allows a chip to temporarily exceed its power and thermal limits by turning on all its cores for a short time period, such that its computing performance is boosted for bursty computation demands. However, conducting sprinting in a data center faces new challenges due to power and thermal constraints at the data center level, which are exacerbated by recently proposed power infrastructure under-provisioning and reliance on renewable energy, as well as the increasing server density. In this paper, we propose Data Center Sprinting, a methodology that enables a data center to temporarily boost its computing performance by turning on more cores in the era of dark silicon, in order to handle occasional workload bursts. We demonstrate the feasibility of this approach by analyzing the tripping characteristics of data center circuit breakers and the discharging characteristics of energy storage devices, in order to realize safe sprinting without causing undesired server overheating or shutdown. We evaluate a prototype of Data Center Sprinting on a hardware testbed and in data enter-level simulations. The experimental results show that our solution can improve the average computing performance of a data center by a factor of 1.62 to 2.45 for 5 to 30 minutes.
[power constraints, occasional workload bursts, Batteries, Servers, renewable energy, hardware testbed, dark silicon, energy storage devices, circuit breakers, microprocessors, Partial discharges, data center-level simulations, Cooling, computing performance, power infrastructure, computer centres, power engineering computing, Uninterruptible power systems, data center sprinting, discharging characteristics, Upper bound, tripping characteristics, thermal constraints, Circuit breakers, time 5 min to 30 min, computational sprinting, server density]
Phase Detection with Hidden Markov Models for DVFS on Many-Core Processors
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
The energy concerns of many-core processors are increasing with the number of cores. We provide a new method that reduces energy consumption of an application on many-core processors by identifying unique segments to apply dynamic voltage and frequency scaling (DVFS). Our method, phase-based voltage and frequency scaling (PVFS), hinges on the identification of phases, i.e., Segments of code with unique performance and power attributes, using hidden Markov Models. In particular, we demonstrate the use of this method to target hardware components on many-core processors such as Network-on-Chip (NoC). PVFS uses these phases to construct a static power schedule that uses DVFS to reduce energy with minimal performance penalty. This general scheme can be used with a variety of performance and power metrics to match the needs of the system and application. More importantly, the flexibility in the general scheme allows for targeting of the unique hardware components of future many-core processors. We provide an in-depth analysis of PVFS applied to five threaded benchmark applications, and demonstrate the advantage of using PVFS for 4 to 32 cores in a single socket. Empirical results of PVFS show a reduction of up to 10.1% of total energy while only impacting total time by at most 2.7% across all core counts. Furthermore, PVFS outperforms standard coarse-grain time-driven DVFS, while scaling better in terms of energy savings with increasing core counts.
[coarse-grain time-driven DVFS, multiprocessing systems, static power schedule, Conferences, energy savings, Distributed computing, Statistical Models, hidden Markov models, power aware computing, Phases, PVFS, dynamic voltage and frequency scaling, energy conservation, phase detection, many-core processors, energy consumption reduction, phase-based voltage and frequency scaling, DVFS]
Low Radiation Efficient Wireless Energy Transfer in Wireless Distributed Systems
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Rapid technological advances in the domain of Wireless Energy Transfer (WET) pave the way for novel methods for energy management in Wireless Distributed Systems and recent research efforts have already started considering network models that take into account these new technologies. In this paper, we follow a new approach in studying the problem of efficiently charging a set of rechargeable nodes using a set of wireless energy chargers, under safety constraints on the electromagnetic radiation incurred. In particular, we define a new charging model that greatly differs from existing models in that it takes into account real technology restrictions of the chargers and nodes of the system, mainly regarding energy limitations. Our model also introduces non-linear constraints (in the time domain), that radically change the nature of the computational problems we consider. In this charging model, we present and study the Low Radiation Efficient Charging Problem (LREC), in which we wish to optimize the amount of "useful" energy transferred from chargers to nodes (under constraints on the maximum level of imposed radiation). We present several fundamental properties of this problem and provide indications of its hardness. Finally, we propose an iterative local improvement heuristic for LREC, which runs in polynomial time and we evaluate its performance via simulation. Our algorithm decouples the computation of the objective function from the computation of the maximum radiation and also does not depend on the exact formula used for the computation of the electromagnetic radiation in each point of the network, achieving good trade-offs between charging efficiency and radiation control, it also exhibits good energy balance properties. We provide extensive simulation results supporting our claims and theoretical results.
[iterative methods, time domain, Energy exchange, iterative local improvement, WET, rechargeable nodes, LREC, Wireless communication, electromagnetic waves, network models, low radiation efficient wireless energy transfer, charging efficiency, wireless distributed systems, energy management, safety constraints, polynomial time, wireless energy chargers, objective function, Computational modeling, Electromagnetic radiation, time-domain analysis, Linear programming, nonlinear constraints, Wireless sensor networks, energy management systems, electromagnetic radiation, Energy storage, computational complexity]
Modeling Propagation Dynamics and Developing Optimized Countermeasures for Rumor Spreading in Online Social Networks
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
The spread of rumors in Online Social Networks (OSNs) poses great challenges to the social peace and public order. It is imperative to model propagation dynamics of rumors and develop corresponding countermeasures. Most of the existing works either overlook the heterogeneity of social networks or do not consider the cost of countermeasures. Motivated by these issues, this paper proposes a heterogeneous network based epidemic model that incorporates both the network heterogeneity and various countermeasures. Through analyzing the existence and stability of equilibrium solutions of the proposed ODE (Ordinary Differential Equation) system, the critical conditions that determine whether a rumor continuously propagates or becomes extinct are derived. Moreover, we concern about the cost of the main two types of countermeasures, i.e., Blocking rumors at influential users and spreading truth to clarify rumors. Employing the Pontryagin's maximum principle, we obtain the optimized countermeasures that ensures a rumor can become extinct at the end of an expected time period with lowest cost. Both the critical conditions and the optimized countermeasures provide a real-time decision reference to restrain the rumor spreading. Experiments based on Digg2009 dataset are conducted to evaluate the effectiveness of the proposed dynamic model and the efficiency of the optimized countermeasures.
[Transforms, online social networks, epidemic model, Pontryagin maximum principle, blocking rumors, maximum principle, real-time decision reference, differential equations, Asymptotic stability, ODE system, rumor spreading, heterogeneous network, Digg2009 dataset, Mathematical model, public order, spreading truth, network heterogeneity, modeling propagation dynamics, Social network services, Stability analysis, OSN, social peace, real-time systems, Differential equations, social networking (online), ordinary differential equation system]
Optimizing Inter-server Communication for Online Social Networks
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Distributed storage systems are the key infrastructures for hosting the user data of large-scale Online Social Networks (OSNs). The amount of inter-server communication is an important scalability indicator for these systems. Data partitioning and replication are two inter-related issues affecting the inter-server traffic caused by user-initiated read and write operations. This paper investigates the problem of minimizing the total inter-server traffic among a cluster of OSN servers through joint partitioning and replication optimization. We propose a Traffic-Optimized Partitioning and Replication (TOPR) method based on an analysis of how replica allocation affects the inter-server communication. Lightweight algorithms are developed to adjust partitioning and replication dynamically according to data read and write rates. Evaluations with real Facebook and Twitter social graphs show that TOPR significantly reduces the inter-server communication compared with state-of-the-art methods.
[Algorithm design and analysis, scalability indicator, Heuristic algorithms, distributed storage systems, Servers, Optimization, storage management, Twitter social graphs, resource allocation, file servers, data partitioning, Joints, Facebook, TOPR method, replica allocation, inter-server traffic, inter-server communication optimization, data replication, computer mediated communication, social networking (online), large-scale online social networks, user-initiated read-write operations, Resource management, traffic-optimized partitioning and replication method, OSN servers]
Efficient Customized Privacy Preserving Friend Discovery in Mobile Social Networks
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Mobile social networks have been increasingly popular with the explosive growth of mobile devices. Mobile users are allowed to interact with potential friends within a certain distance. Motivated by this feature, many exciting applications have been developed, yet the challenge of privacy protection is thus aroused. In this paper, we propose an efficient customized privacy preserving friend discovery mechanism, which not only protects the privacy of users' profile, but also establishes a verifiable secure communication channel between matched users. Besides, the initiator has the freedom to set a customized request profile by choosing the interested attributes and giving each attribute a specific value. Moreover, the request profile's privacy protection level is customized by the initiator according to his/her own privacy requirements. We also consider the collusion attacks among unmatched users. To the best of our knowledge, this is the first work to address such a security threat. Our protocol guarantees that only exactly matched users are able to communicate with the initiator securely, while little information can be obtained by other participants. To increase the matching efficiency, our design adopts the Bloom filter to efficiently exclude most unmatched users. As a result, our design effectively protects the profile privacy and efficiently decreases the computational overhead. Security analysis and performance evaluation are conducted to justify the superiority of our protocol.
[user profile, Protocols, privacy preservation, Mobile communication, Entropy, Privacy, mobile computing, Mobile social networks, profile privacy, data protection, secure friend discovery, data structures, Attribute Based Encryption, mobile users, Cryptography, security analysis, customized privacy preserving friend discovery mechanism, Social network services, Receivers, performance evaluation, Bloom filter, customized request profile, privacy protection, mobile devices, social networking (online), mobile social networks, collusion attacks, secure communication channel, privacy requirements]
Combating Friend Spam Using Social Rejections
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Unwanted friend requests in online social networks (OSNs), also known as friend spam, are among the most evasive malicious activities. Friend spam can result in OSN links that do not correspond to social relationship among users, thus pollute the underlying social graph upon which core OSN functionalities are built, including social search engine, ad targeting, and OSN defense systems. To effectively detect the fake accounts that act as friend spammers, we propose a system called Rejecto. It stems from the observation on social rejections in OSNs, i.e., Even well-maintained fake accounts inevitably have their friend requests rejected or they are reported by legitimate users. Our key insight is to partition the social graph into two regions such that the aggregate acceptance rate of friend requests from one region to the other is minimized. This design leads to reliable detection of a region that comprises friend spammers, regardless of the request collusion among the spammers. Meanwhile, it is resilient to other strategic manipulations. To efficiently obtain the graph cut, we extend the Kernighan-Lin heuristic and use it to iteratively detect the fake accounts that send out friend spam. Our evaluation shows that Rejecto can discern friend spammers under a broad range of scenarios and that it is computationally practical.
[Friend spam detection, Kernighan-Lin heuristic, Unsolicited electronic mail, social graph, online social networks, Sybil defense, fake account detection, unsolicited e-mail, social rejections, Partitioning algorithms, OSN, friend requests aggregate acceptance rate, Approximation methods, friend spam, Rejecto, security of data, Aggregates, Approximation algorithms, social networking (online), social rejection, Reliability, manipulation resistance, Facebook]
Prebaked &#x00B5;VMs: Scalable, Instant VM Startup for IaaS Clouds
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
IaaS clouds promise instantaneously available resources to elastic applications. In practice, however, virtual machine (VM) start up times are in the order of several minutes, or at best, several tens of seconds, negatively impacting the elasticity of applications like Web servers that need to scale out to handle dynamically increasing load. VM start up time is strongly influenced by booting the VM's operating system. In this work, we propose using so-called prebaked uVMs to speed up VM start up. Uvms are snapshots of minimal VMs that can be quickly resumed and then configured to application needs by hot-plugging resources. To serve uVMs, we extend our VM boot cache service, Squirrel, allowing to store uVMs for large numbers of VM images on the hosts of a data center. Our experiments show that uVMs can start up in less than one second on a standard file system. Using 1000+ VM images from a production cloud, we show that the respective uVMs can be stored in a compressed and deduplicated file system within 50GB storage per host, while starting up within 2 -- 3 seconds on average.
[hot-plugging resources, Scalability, scalable instant VM startup, Booting, cache storage, Servers, virtual machine startup times, VM operating system, VM images, VM boot cache service, deduplicated file system, prebaked VMs, cloud computing, Kernel, IaaS clouds, elastic applications, Virtual machining, Web servers, computer centres, data center, Instant VM Startup, Linux, Memory management, virtual machines, Resource Hot-plugging, operating systems (computers), Squirrel]
ECO-DNS: Expected Consistency Optimization for DNS
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
The flexibility of the current Domain Name System (DNS) has been stretched to its limits to accommodate new applications such as content delivery networks and dynamic DNS. In particular, maintaining cache consistency has become a much larger problem, as emerging technologies require increasingly-frequent updates to DNS records. Though Time-To-Live (TTL) is the most widely used method of controlling cache consistency, it does not offer the fine-grained control necessary for handling these frequent changes. In addition, TTLs are too static to handle sudden changes in traffic caused by Internet failures or social media trends, demonstrating their inflexibility in the face of unforeseen events. To address these problems, we first propose a metric called Expected Aggregate Inconsistency (EAI), which allows us to consider important factors such as a record's update frequency and popularity when quantitatively measuring inconsistency. We then design ECO-DNS, a lightweight system that leverages the information provided by EAI to optimize a record's TTL. This value can be tuned to individual cache servers' preferences between better consistency and bandwidth overhead. Further-more, our optimization model's flexibility allows us to easily adapt ECO-DNS to handle various caching hierarchies such as multi-level caching while considering the trade off among consistency, overhead, latency, and server load.
[optimization model flexibility, Consistency, DNS, Frequency measurement, Servers, TTL, Optimization, dynamic DNS, expected aggregate inconsistency, Bandwidth, domain name system, Mathematical model, cache consistency, expected consistency optimization, legislation, government policies, bandwidth overhead, EAI, server load, multilevel caching, ECO, time-to-live, Aggregates, caching hierarchies, Internet]
An Online Method for Minimizing Network Monitoring Overhead
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Network monitoring is an essential component of network operation and, as the network size increases, it usually generates a significant overhead in large scale networks such as sensor and data center networks. In this paper, we show that measurement correlation often exhibited in real networks can be successfully exploited to reduce the network monitoring overhead. In particular, we propose an online adaptive measurement technique with which a subset of nodes are dynamically chosen as monitors while the measurements of the remaining nodes are estimated using the computed correlations. We propose an estimation framework based on jointly Gaussian distributed random variables, and formulate an optimization problem to select the monitors which minimize the estimation error under a total cost constraint. We show that the problem is NP-Hard and propose three efficient heuristics. In order to apply our framework to real-world networks, in which measurement distribution and correlation may significantly change over time, we also develop a learning based approach that automatically switches between learning and estimation phases using a change detection algorithm. Simulations carried out on two real traces from sensor networks and data centers show that our algorithms outperforms previous solutions based on compressed sensing and it is able to reduce the monitoring overhead by 50% while incurring a low estimation error. The results further demonstrate that applying the change detection algorithm reduces the estimation error up to two orders of magnitude.
[optimization problem, NP-hard, compressed sensing, learning based approach, Conferences, online adaptive measurement technique, large scale networks, Gaussian distribution, sensor networks, network operation, Distributed computing, jointly Gaussian distributed random variables, estimation phases, real-world networks, optimisation, network monitoring overhead minimization, change detection algorithm, measurement correlation, learning (artificial intelligence), data center networks, computational complexity, correlation methods]
Thread Migration in a Replicated-Kernel OS
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Chip manufacturers continue to increase the number of cores per chip while balancing requirements for low power consumption. This drives a need for simpler cores and hardware caches. Because of these trends, the scalability of existing shared memory system software is in question. Traditional operating systems (OS) for multiprocessors are based on shared memory communication between cores and are symmetric (SMP). Contention in SMP OSes over shared data structures is increasingly significant in newer generations of many-core processors. We propose the use of the replicated-kernel OS design to improve scalability over the traditional SMP OS. Our replicated-kernel design is an extension of the concept of the multikernel. While a multikernel appears to application software as a distributed network of cooperating micro kernels, we provide the appearance of a monolithic, single-system image, task-based OS in which application software is unaware of the distributed nature of the underlying OS. In this paper we tackle the problem of thread migration between kernels in a replicated-kernel OS. We focus on distributed thread group creation, context migration, and address space consistency for threads that execute on different kernels, but belong to the same distributed thread group. This concept is embodied in our prototype OS, called Popcorn Linux, which runs on multicore x86 machines and presents a Linux-like interface to application software that is indistinguishable from the SMP Linux interface. By doing this, we are able to leverage the wealth of existing Linux software for use on our platform while demonstrating the characteristics of the underlying replicated-kernel OS. We show that a replicated-kernel OS scales as well as a multikernel OS by removing the contention on shared data structures. Popcorn, Barr elfish, and SMP Linux are compared on selected benchmarks. Popcorn is shown to be competitive to SMP Linux, and up to 40% faster.
[thread migration, Instruction sets, Popcorn Linux, low power consumption, distributed thread group creation, distributed thread group, single-system image, prototype OS, SMP OSes, microkernels, shared memory systems, Hardware, data structures, multiprocessors, many-core processors, Kernel, shared memory system software, Message systems, shared memory communication, multi-threading, operating systems, Data structures, replicated-kernel OS, context migration, hardware caches, Linux, scalability improvement, replicated-kernel OS design, address space consistency, Barrelfish Linux]
Towards Understanding the Advertiser's Perspective of Smartphone User Privacy
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Many smartphone apps routinely gather various private user data and send them to advertisers. Despite recent study on protection mechanisms and analysis on apps' behavior, the understanding about the consequences of such privacy losses remains limited. In this paper we investigate how much an advertiser can infer about users' social and community relationships by combining data from multiple applications and across many users. After one month's user study involving about 200 most popular Android apps, we find that an advertiser can infer 90% of the social relationships. We further propose a privacy leakage inference framework and use real mobility traces and Foursquare data to quantify the consequences of privacy leakage. We find that achieving 90% inference accuracy of the social and community relationships requires merely 3 weeks' user data. The discoveries underscore the importance of early adoption of privacy protection mechanisms.
[advertiser, Data privacy, Google, privacy leakage inference framework, Communities, smart phones, social relationships, Android apps, Global Positioning System, Privacy, Social relationship, data protection, smartphone user privacy, privacy protection mechanisms, social sciences computing, community relationships, Smartphone, IEEE 802.11 Standard, Testing]
Advertiser and Publisher-centric Privacy Aware Online Behavioral Advertising
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Online behavioral advertising (OBA) has become one of the most successful advertising models on the Internet. Nevertheless, all existing OBA systems are broker-centric in the billing phase, which means it is the broker who exclusively determines advertisers' expenses and publishers' revenues. Consequently, a malicious broker may cheat in their tallying of ad clicks to overcharge advertisers or underpay publishers. Furthermore, as the broker cannot justify the bills, malicious advertisers may deny actual clicks to ask for refunds, and malicious publishers may claim non-existing clicks to demand extra revenue shares. This paper solves these problems by reversing the priority between the broker and the advertisers and publishers. Specifically, when users click on ads, it makes corresponding advertisers and publishers forward click reports of clients to the broker after checking, anonymizing and signing them. The broker then settles accounts with advertisers and publishers fully based on these reports. To guarantee the interests of the broker after the priority reversal, we further propose effective mechanisms for detecting underreporting advertisers and over reporting publishers, respectively.
[Online behavioral advertising, advertising data processing, publisher-centric privacy aware online behavioral advertising, publisher revenues, Encryption, Proposals, advertiser-centric privacy aware online behavioral advertising, Privacy, billing phase, malicious broker, advertiser expenses, ad clicks, malicious publishers, privacyawareness, Hardware, Internet, advertising models, behavioural sciences computing, Advertising, billing security, broker-centric OBA systems]
POP: Privacy-Preserving Outsourced Photo Sharing and Searching for Mobile Devices
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Facing a large number of personal photos and limited resource of mobile devices, cloud plays an important role in photo storing, sharing and searching. Meanwhile, some recent reputation damage and stalk events caused by photo leakage increase people's concern about photo privacy. Though most would agree that photo search function and privacy are both valuable, few cloud system supports both of them simultaneously. The center of such an ideal system is privacy-preserving outsourced image similarity measurement, which is extremely challenging when the cloud is untrusted and a high extra overhead is disliked. In this work, we introduce a framework POP, which enables privacy-seeking mobile device users to outsource burdensome photo sharing and searching safely to untrusted servers. Unauthorized parties, including the server, learn nothing about photos or search queries. This is achieved by our carefully designed architecture and novel non-interactive privacy-preserving protocols for image similarity computation. Our framework is compatible with the state-of-the-art image search techniques, and it requires few changes to existing cloud systems. For efficiency and good user experience, our framework allows users to define personalized private content by a simple check-box configuration and then enjoy the sharing and searching services as usual. All privacy protection modules are transparent to users. The evaluation of our prototype implementation with 31,772 real-life images shows little extra communication and computation overhead caused by our system.
[Protocols, image search techniques, Photo Privacy, privacy protection modules, privacy-preserving outsourced photo sharing, Search problems, photo leakage, Encryption, Servers, privacy-seeking mobile device users, POP, Privacy, mobile computing, Privacy-preserving Photo Searching, novel noninteractive privacy-preserving protocols, mobile devices, privacy-preserving outsourced image similarity measurement, Feature extraction, data privacy, photo privacy, simple check-box configuration, cloud computing, photo search function]
Privacy-Preserving Machine Learning Algorithms for Big Data Systems
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Machine learning has played an increasing important role in big data systems due to its capability of efficiently discovering valuable knowledge and hidden information. Often times big data such as healthcare systems or financial systems may involve with multiple organizations who may have different privacy policy, and may not explicitly share their data publicly while joint data processing may be a must. Thus, how to share big data among distributed data processing entities while mitigating privacy concerns becomes a challenging problem. Traditional methods rely on cryptographic tools and/or randomization to preserve privacy. Unfortunately, this alone may be inadequate for the emerging big data systems because they are mainly designed for traditional small-scale data sets. In this paper, we propose a novel framework to achieve privacy-preserving machine learning where the training data are distributed and each shared data portion is of large volume. Specifically, we utilize the data locality property of Apache Hadoop architecture and only a limited number of cryptographic operations at the Reduce() procedures to achieve privacy-preservation. We show that the proposed scheme is secure in the semi-honest model and use extensive simulations to demonstrate its scalability and correctness.
[Protocols, cryptographic operations, privacy-preserving machine learning, Big Data, data locality property, Data mining, parallel processing, Support vector machines, Training, Big Data systems, Training data, Big data, data privacy, learning (artificial intelligence), Kernel, Apache Hadoop architecture]
Fault-Tolerant and Elastic Streaming MapReduce with Decentralized Coordination
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
The MapReduce programming model, due to its simplicity and scalability, has become an essential tool for processing large data volumes in distributed environments. Recent Stream Processing Systems (SPS) this model to provide low-latency analysis of high-velocity continuous data streams. However, integrating MapReduce with streaming poses challenges: first, the runtime variations in data characteristics such as data-rates and key-distribution cause resource overload, that in-turn leads to fluctuations in the Quality of the Service (QoS), and second, the stateful reducers, whose state depends on the complete tuple history, necessitates efficient fault-recovery mechanisms to maintain the desired QoS in the presence of resource failures. We propose an integrated streaming MapReduce architecture leveraging the concept of consistent hashing to support runtime elasticity along with locality-aware data and state replication to provide efficient load-balancing with low-overhead fault-tolerance and parallel fault-recovery from multiple simultaneous failures. Our evaluation on a private cloud shows up to 2.8&#x00D7; improvement in peak throughput compared to Apache Storm SPS, and a low recovery latency of 700 - 1500 ms from multiple failures.
[Checkpointing, Distributed Stream processing, tuple history, fault-tolerance, fault-recovery mechanisms, Elasticity, elastic streaming, parallel fault-recovery, high-velocity continuous data streams, parallel processing, Fault tolerance, data-rates, Runtime, resource allocation, QoS, Fault tolerant systems, MapReduce programming model, stream processing systems, resource failures, integrated streaming architecture, big data, key-distribution, runtime elasticity, state replication, load-balancing, locality-aware data, quality of service, software fault tolerance, low-overhead fault-tolerance, Streaming mapreduce, Storms, Apache Storm SPS, quality of the service, private cloud, Peer-to-peer computing, data handling, low-latency analysis, decentralized coordination, runtime variations]
Task-Cloning Algorithms in a MapReduce Cluster with Competitive Performance Bounds
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Job scheduling for a MapReduce cluster has been an active research topic in recent years. However, measurement traces from real-world production environment show that the duration of tasks within a job vary widely. The overall elapsed time of a job, i.e. The so-called flow time, is often dictated by one or few slowly-running tasks within a job, generally referred as the "stragglers". The cause of stragglers include tasks running on partially/intermittently failing machines or the existence of some localized resource bottleneck(s) within a MapReduce cluster. To tackle this online job scheduling challenge, we adopt the task cloning approach and design the corresponding scheduling algorithms which aim at minimizing the weighted sum of job flow times in a MapReduce cluster based on the Shortest Remaining Processing Time scheduler (SRPT). To be more specific, we first design a 2-competitive offline algorithm when the variance of task-duration is negligible. We then extend this offline algorithm to yield the so-called SRPTMS+C algorithm for the online case and show that SRPTMS+C is (1 + &#x03F5;) - speed o (1/&#x03F5;2) - competitive in reducing the weighted sum of job flow times within a cluster. Both of the algorithms explicitly consider the precedence constraints between the two phases within the MapReduce framework. We also demonstrate via trace-driven simulations that SRPTMS+C can significantly reduce the weighted/unweighted sum of job flow times by cutting down the elapsed time of small jobs substantially. In particular, SRPTMS+C beats the Microsoft Mantri scheme by nearly 25% according to this metric.
[competitive bound, Conferences, SRPT, weighted job flowtime, stragglers, localized resource bottleneck, job Scheduling, Distributed computing, parallel processing, competitive performance bounds, MapReduce, online job scheduling challenge, Microsoft Mantri scheme, pattern clustering, MapReduce cluster, shortest remaining processing time scheduler, task-cloning algorithms, SRPTMS+C algorithm, data handling, failing machines, real-world production environment, offline algorithm, cloning]
FLOWPROPHET: Generic and Accurate Traffic Prediction for Data-Parallel Cluster Computing
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Data-parallel computing frameworks (DCF) such as MapReduce, Spark, and Dryad etc. Have tremendous applications in big data and cloud computing, and throw tons of flows into data center networks. In this paper, we design and implement FLOWPROPHET, a general framework to predict traffic flows for DCFs. To this end, we analyze and summarize the common features of popular DCFs, and gain a key insight: since application logic in DCFs is naturally expressed by directed acyclic graphs (DAG), DAG contains necessary time and data dependencies for accurate flow prediction. Based on the insight, FLOWPROPHET extracts DAGs from user applications, and uses the time and data dependencies to calculate flow information 4-tuple, (source, destination, flow_size, establish_time), ahead-of-time for all flows. We also provide generic programming interface to FLOWPROPHET, so that current and future DCFs can deploy FLOWPROPHET readily. We implement FLOWPROPHET on both Spark and Hadoop, and perform extensive evaluations on a testbed with 37 physical servers. Our implementation and experiments demonstrate that, with time in advance and minimal cost, FLOWPROPHET can achieve almost 100% accuracy in source, destination, and flow size predictions. With accurate prediction from FLOWPROPHET, the job completion time of a Hadoop TeraSort benchmark is reduced by 12.52% on our cluster with a simple network scheduler.
[job completion time, 4-tuple, physical servers, Data mining, parallel processing, Optimization, establish-time, MapReduce, Parallel processing, Prediction algorithms, directed acyclic graphs, source predictions, big data, cloud computing, Dryad, data center networks, Group Communication, traffic prediction, FLOWPROPHET, Context, generic programming interface, Data Center Network, Calculators, data dependencies, data analysis, network scheduler, user applications, DCF, DAG, Sparks, Hadoop TeraSort benchmark, pattern clustering, directed graphs, data-parallel cluster computing, Spark, flow size predictions, destination predictions, Data-parallel Computing, time dependencies, telecommunication traffic]
Towards Energy Efficiency in Heterogeneous Hadoop Clusters by Adaptive Task Assignment
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
The cost of powering servers, storage platforms and related cooling systems has become a major component of the operational costs in big data deployments. Hence, the design of energy-efficient Hadoop clusters has attracted significant research attentions in recent years. However, existing studies do not consider the impact of the complex interplay between workload and hardware heterogeneity on energy efficiency. In this paper, we find that heterogeneity-oblivious task assignment approaches are detrimental to both performance and energy efficiency of Hadoop clusters. Importantly, we make a counterintuitive observation that even heterogeneity-aware techniques that focus on reducing job completion time do not necessarily guarantee energy efficiency. We propose a heterogeneity-aware task assignment approach, E-Ant, that aims to minimize the overall energy consumption in a heterogeneous Hadoop cluster without sacrificing job performance. It adaptively schedules heterogeneous workloads on energy-efficient machines, without a priori knowledge of the workload properties. Furthermore, it provides the flexibility to trade off energy efficiency and job fairness in a Hadoop cluster. E-Ant employs an ant colony optimization approach that generates task assignment solutions based on the feedback of each task's energy consumption reported by Hadoop Task Trackers in an agile way. Experimental results on a heterogeneous cluster with varying hardware capabilities show that E-Ant improves the overall energy savings for a synthetic workload from Microsoft by 17% and 12% compared to Fair Scheduler and Tarazu, respectively.
[Energy consumption, Adaptation models, adaptive task assignment, Hadoop task trackers, heterogeneous Hadoop clusters, operational costs, heterogeneous workload scheduling, Servers, parallel processing, Accuracy, power aware computing, task energy consumption, energy efficiency, Benchmark testing, scheduling, Hardware, heterogeneity-aware task assignment approach, e-ant, ant colony optimisation, energy consumption, energy-efficient Hadoop cluster design, Power demand, hardware heterogeneity, synthetic workload, ant colony optimization approach, Big Data, energy conservation, Big Data deployments, data handling, storage platforms, cooling systems]
Experimental Study for Multi-layer Parameter Configuration of WSN Links
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Many applications of wireless sensor networks (WSNs) need to balance multiple yet often conflicting performance requirements such as high energy efficiency, high throughput, low delay and low loss. Finding appropriate WSN parameter configuration to achieve the best trade-off requires in depth understanding of the joint effect of key parameters residing at different layers on the performance. In this paper, we present an extensive experimental study on the data delivery performance of aWSN link, where 4 major performance metrics, namely energy, throughput, delay and loss, were measured over 6 months under around 50 thousand parameter configurations of 7 key stack parameters. Different from existing work, rich observations are made out of the extensive measurement data, with the focus on the joint effect of these parameters on the performance. Specifically, for each of the four performance metrics, a set of guidelines is derived for parameter optimization. In addition, we propose empirical models for each performance metric to quantify the joint effects, which enable finding optimal settings for parameters such as payload size or retransmissions, in consideration of link quality and other parameter settings, to achieve better performance trade-offs. To demonstrate the potential of this work, the obtained joint parameter optimization results are applied to an example. The outcome is compared with those achieved by following representative single-parameter tuning guidelines from the literature. The comparison reveals that by considering the joint effect of multi-layer parameters together, a WSN application can obtain a much improved performance trade-off.
[radio links, Energy consumption, wireless sensor networks, wireless sensor network, experimental study, packet loss, multi-layer, Tuning, WSN link quality, parameter configuration, Wireless sensor networks, delay, multilayer parameter configuration, empirical models, parameter optimization, throughput, Joints, energy consumption, Power generation, performance analysis, Signal to noise ratio, Payloads]
PTZ Camera Scheduling for Selected Area Coverage in Visual Sensor Networks
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Visual sensor networks (VSNs) can track multiple pedestrians and capture high-quality videos of the monitored area. Therefore, VSNs is ideal for providing good broadcast service. In sports broadcasting, a basic requirement for broadcasters is to report the significant events as quickly as possible when they take place. To meet this requirement, we propose the Camera Scheduling for selected area coverage problem (CamS). Considering that Pan-Tilt-Zoom (PTZ) camera sensor has the flexibility of configuring its angle of view in both horizontal and vertical dimensions, we apply PTZ camera sensors to solve CamS. A polynomial time optimal algorithm that schedules PTZ camera sensors elegantly is devised for CamS. We set many realistic application scenarios in simulation and thoroughly study how our algorithm's performance is affected by different environmental parameters, including angle velocity, the number of camera sensors and the number of sub-areas.
[Visualization, Schedules, angle velocity, visual perception, Visual sensor networks, Selected Area Coverage, multiple pedestrians, polynomial time optimal algorithm, cameras, PTZ camera scheduling, high-quality videos, sensors, visual sensor networks, sports broadcasting, scheduling, selected area coverage, Cameras, Cams, Sensors, Bipartite graph, pan-tilt-zoom camera sensor, PTZ camera, Monitoring, Maximum Weight Perfect Matching]
On Multipath Link Characterization and Adaptation for Device-Free Human Detection
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Wireless-based device-free human sensing has raised increasing research interest and stimulated a range of novel location-based services and human-computer interaction applications for recreation, asset security and elderly care. A primary functionality of these applications is to first detect the presence of humans before extracting higher-level contexts such as physical coordinates, body gestures, or even daily activities. In the presence of dense multipath propagation, however, it is non-trivial to even reliably identify the presence of humans. The multipath effect can invalidate simplified propagation models and distort received signal signatures, thus deteriorating detection rates and shrinking detection range. In this paper, we characterize the impact of human presence on wireless signals via ray-bouncing models, and propose a measurable metric on commodity WiFi infrastructure as a proxy for detection sensitivity. To achieve higher detection rate and wider sensing coverage in multipath-dense indoor scenarios, we design a lightweight sub carrier and path configuration scheme harnessing frequency diversity and spatial diversity. We prototype our scheme with standard WiFi devices. Evaluations conducted in two typical office environments demonstrate a detection rate of 92.0% with a false positive of 4.5%, and almost 1x gain in detection range given a minimal detection rate of 90%.
[radio links, human-computer interaction application, OFDM, wireless based device-free human sensing, received signal signature, multipath dense indoor scenario, ray bouncing model, location-based service, frequency diversity, dense multipath propagation, Wireless communication, path configuration scheme, Sensors, indoor radio, multipath link characterization, diversity reception, higher-level context extraction, shrinking detection range, multipath channels, Wi-Fi infrastructure, radiowave propagation, lightweight subcarrier, Shadow mapping, Wireless sensor networks, Sensitivity, multipath link adaptation, device-free human detection, spatial diversity, human computer interaction, wireless LAN, IEEE 802.11 Standard]
Elastic Stream Processing with Latency Guarantees
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Many Big Data applications in science and industry have arisen, that require large amounts of streamed or event data to be analyzed with low latency. This paper presents a reactive strategy to enforce latency guarantees in data flows running on scalable Stream Processing Engines (SPEs), while minimizing resource consumption. We introduce a model for estimating the latency of a data flow, when the degrees of parallelism of the tasks within are changed. We describe how to continuously measure the necessary performance metrics for the model, and how it can be used to enforce latency guarantees, by determining appropriate scaling actions at runtime. Therefore, it leverages the elasticity inherent to common cloud technology and cluster resource management systems. We have implemented our strategy as part of the Nephele SPE. To showcase the effectiveness of our approach, we provide an experimental evaluation on a large commodity cluster, using both a synthetic workload as well as an application performing real-time sentiment analysis on real-world social media data.
[scalable stream processing engine, appropriate scaling action, social media data flow, Nephele SPE, latency guarantee, Quality of service, Throughput, parallel processing, Engines, Stream Processing, Runtime, Big Data application, resource allocation, Parallel processing, Real-time systems, cluster resource management system, elasticity inherent, Elastic Scaling, Streaming, elastic stream processing, Stream Processing Engine, data flow analysis, Big Data, application performing real-time sentiment analysis, Latency Constraint, scalable SPE, Storms, Autoscaling, social networking (online), cloud technology, Latency Guarantee]
DRS: Dynamic Resource Scheduling for Real-Time Analytics over Fast Streams
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
In a data stream management system (DSMS), users register continuous queries, and receive result updates as data arrive and expire. We focus on applications with real-time constraints, in which the user must receive each result update within a given period after the update occurs. To handle fast data, the DSMS is commonly placed on top of a cloud infrastructure. Because stream properties such as arrival rates can fluctuate unpredictably, cloud resources must be dynamically provisioned and scheduled accordingly to ensure real-time response. It is essential, for the existing systems or future developments, to possess the ability of scheduling resources dynamically according to the current workload, in order to avoid wasting resources, or failing in delivering correct results on time. Motivated by this, we propose DRS, a novel dynamic resource scheduler for cloud-based DSMSs. DRS overcomes three fundamental challenges: (a) how to model the relationship between the provisioned resources and query response time (b) where to best place resources, and (c) how to measure system load with minimal overhead. In particular, DRS includes an accurate performance model based on the theory of Jackson open queueing networks and is capable of handling arbitrary operator topologies, possibly with loops, splits and joins. Extensive experiments with real data confirm that DRS achieves real-time response with close to optimal resource consumption.
[open queueing networks, Computational modeling, DRS, dynamic resource scheduling, fast streams, cloud resources, real-time analytics, Dynamic scheduling, dynamic scheduling, resource scheduling, Program processors, Processor scheduling, resource allocation, DSMS, arbitrary operator topologies, Feature extraction, cloud infrastructure, data stream analytics, Real-time systems, data stream management system, Delays, data handling, cloud computing, scheduling resources]
An Application-Aware Scheduling Policy for Real-Time Traffic
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
The pervasiveness of mobile applications stimulates more eager demand for Quality of Experience (QoE) than Quality of Service (QoS), especially on the aspect of link scheduling in wireless networks. In many applications, end users concern more about transmission quality of an individual task rather than an individual packet. A task may correspond to a piece of video, music, etc. And may include many packets. This paper proposes a new network model aiming at improving users' experience that pushes the scheduling problem to the task layer. We first introduce a QoE requirement that can generalize the QoS requirement in link scheduling, the partial result requirement. Subsequently, a novel scheduling policy is proposed which can capture this requirement for each task, and then performs an application-aware scheduling. We theoretically analyze the performance of the novel scheduling policy, and discuss the impact of the QoE requirements and network settings. Finally, the simulation results indicate that our scheduling policy can significantly improve QoE.
[Schedules, application-aware scheduling policy, Correlation, link scheduling, QoE, Optimal scheduling, Quality of service, Wireless Networks, transmission quality, mobile computing, network settings, Wireless networks, network model, mobile applications, QoS requirement, task layer, Link Scheduling, wireless networks, Scheduling, quality of service, quality of experience, QoE requirement, end users, real-time systems, Real-time Traffic, Time complexity, telecommunication traffic, real-time traffic]
UStore: A Low Cost Cold and Archival Data Storage System for Data Centers
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Recent trend in cloud computing demands vast and ever increasing storage capacity for data centers. For many cloud service providers, much of the storage capacity demand is driven by cold and archival data, such as user uploaded contents, system logs, and backups. In this paper, we describe UStore, a hard disk based storage system designed for such workloads. We make the assumption that most data centers are already populated with computer servers and networking gears, and propose a solution to attach additional disks to these servers reliably at extremely low cost. The main component of UStore is a novel fat tree interconnect fabric to connect hard disks to existing servers and network infrastructure. To reduce cost, UStore leverages the mature commodity USB 3.0 technology to build the fabric, which has extremely low amortized cost per disk while still providing sufficient throughput to satisfy cold and archival workload. The software of the UStore system abstracts the system's physical topology and provides a consistent view of the storage capacity to the upper layer services such as distributed file systems or backup services. In a sense, UStore can be regarded as external USB hard disks designed for data centers.
[archival storage, USB 3.0 technology, UStore system, data centers, external USB hard disks, Control systems, Topology, Servers, computer centres, fat tree interconnect fabric, USB interconnect, hard discs, storage management, cold data, Universal Serial Bus, Hard disks, Fabrics, Software, cloud computing, hard disk based storage system]
Aurora: Adaptive Block Replication in Distributed File Systems
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Distributed file systems such as Google File System and Hadoop Distributed File System have been used to store large volumes of data in Cloud data centers. These systems divide data sets in blocks of fixed size and replicate them over multiple machines to achieve both reliability and efficiency. Recent studies have shown that data blocks tend to have a wide disparity in data popularity. In this context, the naive block replication schemes used by these systems often cause an uneven load distribution across machines, which reduces the overall I/O throughput of the system. While many replication algorithms have been proposed, existing solutions have not carefully studied the placement of data blocks that balances the load across machines, while ensuring node and rack-level reliability requirements are satisfied. In this paper, we study the dynamic data replication problem with the goal of balancing machine load while ensuring machine and rack-level reliability requirements are met. We propose several local search algorithms that provide constant approximation guarantees, yet simple and practical for implementation. We further present Aurora, a dynamic block placement mechanism that implements these algorithms in the Hadoop Distributed File System with minimal overhead. Through experiments using workload traces from Yahoo! and Facebook, we show Aurora reduces machine load imbalance by up to 26.9% compared to existing solutions, while satisfying node and rack-level reliability requirements.
[Naive block replication schemes, Heuristic algorithms, machine load balancing, reliability, I/O throughput, approximation algorithms, parallel processing, Google File System, local search, adaptive block replication, Fault tolerance, Aurora, rack-level reliability requirements, Fault tolerant systems, network operating systems, Clustering algorithms, Distributed databases, Distributed file system, distributed databases, cloud computing, dynamic data replication problem, Facebook, search problems, machine-level reliability requirements, dynamic block placement mechanism, Yahoo!, Hadoop, HDFS, load distribution, computer centres, data block placement, local search algorithms, Approximation algorithms, data handling, Hadoop distributed file system, cloud data centers]
Fast Compaction Algorithms for NoSQL Databases
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Compaction plays a crucial role in NoSQL systems to ensure a high overall read throughput. In this work, we formally define compaction as an optimization problem that attempts to minimize disk I/O. We prove this problem to be NP-Hard. We then propose a set of algorithms and mathematically analyze upper bounds on worst-case cost. We evaluate the proposed algorithms on real-life workloads. Our results show that our algorithms incur low I/O costs and that a compaction approach using a balanced tree is most preferable.
[balanced tree, Schedules, optimization problem, np-hard, NP-hard, NoSQL databases, trees (mathematics), Compaction, Approximation methods, database management systems, nosql, greedy approximation algorithm, worst-case cost, Vegetation, Binary trees, Bismuth, Cost function, compaction, real-life workloads, computational complexity, fast compaction algorithms]
Minimizing the Communication Cost of Aggregation in Publish/Subscribe Systems
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Modern applications for distributed publish/subscribe systems often require stream aggregation capabilities along with rich data filtering. When compared to other distributed systems, aggregation in pub/sub differentiates itself as a complex problem which involves dynamic dissemination paths that are difficult to predict and optimize for a priori, temporal fluctuations in publication rates, and the mixed presence of aggregated and non-aggregated workloads. In this paper, we propose a formalization for the problem of minimizing communication traffic in the context of aggregation in pub/sub. We present a solution to this minimization problem by using a reduction to the well-known problem of minimum vertex cover in a bipartite graph. This solution is optimal under the strong assumption of complete knowledge of future publications. We call the resulting algorithm "Aggregation Decision, Optimal with Complete Knowledge" (ADOCK). We also show that under a dynamic setting without full knowledge, ADOCK can still be applied to produce a low, yet not necessarily optimal, communication cost. We also devise a computationally cheaper dynamic approach called "Aggregation Decision with Weighted Publication" (WAD). We compare our solutions experimentally using two real datasets and explore the trade-offs with respect to communication and computation costs.
[ADOCK, distributed publish/subscribe systems, Subscriptions, graph theory, aggregation decision with weighted publication, Routing, aggregation decision optimal with complete knowledge, communication cost minimization, bipartite graph, Optimization, Publish/Subscribe system, Aggregation, Aggregates, Distributed databases, communication traffic minimization, Bipartite graph, WAD, minimisation, minimum vertex cover, Distributed event based system, Monitoring, middleware]
Weighted Overlay Design for Topic-Based Publish/Subscribe Systems on Geo-Distributed Data Centers
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
We incorporate underlay information into overlay design for topic-based publish/subscribe (pub/sub) systems on geo-distributed data centers. We propose the MinAvg-WTCO problem that optimizes the weighted average node degree while constructing a topic-connected overlay (TCO), i.e., Each topic induces a connected sub-overlay among all nodes interested in this topic. Most existing TCO designs are oblivious to the low-level network infrastructure and assume edge equivalence. We prove that MinAvg-WTCO is NP-complete and difficult to approximate within a logarithmic factor with regard to the number of nodes. We devise several approximation algorithms for MinAvg-WTCO using different design techniques. Both theoretical analysis and empirical evaluation show that our designed algorithms tread the balance between overlay quality and runtime cost. Our algorithms significantly outperform the state of the art for TCO design that ignores edge differences.
[Algorithm design and analysis, weighted graph, MinAvg-WTCO problem, network theory (graphs), Approximation methods, empirical evaluation, optimisation, Network topology, geo-distributed data center, weighted overlay design, approximation algorithm, low-level network infrastructure, topic-based publish-subscribe system, Routing, weighted average node degree, Topology, NP-complete problem, computer centres, theoretical analysis, TCO design, pub/sub, Approximation algorithms, topic-connected overlay, Peer-to-peer computing, edge equivalence]
Dynamoth: A Scalable Pub/Sub Middleware for Latency-Constrained Applications in the Cloud
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
This paper presents Dynamoth, a dynamic, scalable, channel-based pub/sub middleware targeted at large scale, distributed and latency constrained systems. Our approach provides a software layer that balances the load generated by a high number of publishers, subscribers and messages across multiple, standard pub/sub servers that can be deployed in the Cloud. In order to optimize Cloud infrastructure usage, pub/sub servers can be added or removed as needed. Balancing takes into account the live characteristics of each channel and is done in an hierarchical manner across channels (macro) as well as within individual channels (micro) to maintain acceptable performance and low latencies despite highly varying conditions. Load monitoring is performed in an unintrusive way, and rebalancing employs a lazy approach in order to minimize its temporal impact on performance while ensuring successful and timely delivery of all messages. Extensive real-world experiments that illustrate the practicality of the approach within a massively multiplayer game setting are presented. Results indicate that with a given number of servers, Dynamoth was able to handle 60% more simultaneous clients than the consistent hashing approach, and that it was properly able to deal with highly varying conditions in the context of large workloads.
[Measurement, load monitoring, Subscriptions, Servers, channel-based middleware, Middleware, Standards, scalability, standard pub-sub servers, pub/sub, scalable pub-sub middleware, Games, Bandwidth, Cloud, software layer, cloud infrastructure usage, cloud computing, latency-constrained applications, massively multiplayer game setting, topic-based pub/sub, middleware, Dynamoth]
Discrete Load Balancing in Heterogeneous Networks with a Focus on Second-Order Diffusion
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
In this paper we consider a wide class of discrete diffusion load balancing algorithms. The problem is defined as follows. We are given an interconnection network and a number of load items, which are arbitrarily distributed among the nodes of the network. The goal is to redistribute the load in iterative discrete steps such that at the end each node has (almost) the same number of items. In diffusion load balancing, nodes are only allowed to balance their load with their direct neighbors. We show three main results. Firstly, we present a general framework for randomly rounding the flow generated by continuous diffusion schemes over the edges of a graph in order to obtain corresponding discrete schemes. Compared to the results of Rabani, Sinclair, and Wanka, FOCS'98, which are only valid w.r.t. The class of homogeneous first order schemes, our framework can be used to analyze a larger class of diffusion algorithms, such as algorithms for heterogeneous networks and second order schemes. Secondly, we bound the deviation between randomized second order schemes and their continuous counterparts. Finally, we provide a bound for the minimum initial load in a network that is sufficient to prevent the occurrence of negative load at a node during the execution of second order diffusion schemes. Our theoretical results are complemented with extensive simulations on different graph classes. We show empirically that second order schemes, which are usually much faster than first order schemes, will not balance the load completely on a number of networks within reasonable time. However, the maximum load difference at the end seems to be bounded by a constant value, which can be further decreased if first order scheme is applied once this value is achieved by second order scheme.
[continuous diffusion schemes, iterative methods, heterogeneous networks, diffusion, graph theory, iterative discrete steps, Diffusion processes, second-order scheme, network theory (graphs), randomized second order schemes, homogeneous first order schemes, Program processors, resource allocation, discrete diffusion load balancing algorithms, interconnection network, Load management, diffusion load balancing, Eigenvalues and eigenfunctions, Mathematical model, second order diffusion schemes, Load modeling]
Weighted Restless Bandit and Its Applications
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Motivated by many applications such as cognitive radio spectrum scheduling, downlink fading channel scheduling, and unmanned aerial vehicle dynamic routing, we study two restless bandit problems. Given a bandit consisting of multiple restless arms, the state of each arm evolves as a Markov chain. Assume each arm is associated with a positive weight. At each step, we select a subset of arms to play such that the weighted sum of the selected arms cannot exceed a limit. The reward of playing each arm varies according to the arm's state. The exact state of each arm is only revealed when the arm is played. The problem weighted restless bandit aims to maximize the expected average reward over the infinite horizon. We also study an extended problem called multiply-constrained restless bandit where each time there are two simultaneous constraints on the selected arms. First, the weighted sum of the selected arms cannot exceed a limit, Second, the number of the selected arms is at most a constant K. The objective of multiply-constrained restless bandit is to maximize the long term average reward. Both problems are partially observable Markov decision processes and have been proved to be PSPACE-hard even in their special cases. We propose constant approximation algorithms for both problems. Our method involves solving a semi-infinite program, converting back to a low-complexity policy, and accounting for the average reward via a Lyapunov function analysis.
[long term average reward maximization, Downlink, Approximation methods, multi-armed bandits, Markov chain, multiply-constrained restless bandit problems, optimisation, partially observable Markov decision process, restless bandits, observability, Fading, approximation theory, Lyapunov function analysis, Dynamic scheduling, Routing, Indexes, expected average reward maximization, semiinfinite program, decision making, Markov processes, Approximation algorithms, weighted restless bandit problem, PSPACE-hard processes, infinite horizon, Lyapunov methods, constant approximation algorithms]
Deterministic Symmetry Breaking in Ring Networks
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
We study a distributed coordination mechanism for uniform agents located on a circle. The agents perform their actions in synchronised rounds. At the beginning of each round an agent chooses the direction of its movement from clockwise, anticlockwise, or idle, and moves at unit speed during this round. Agents are not allowed to overpass, i.e., When an agent collides with another it instantly starts moving with the same speed in the opposite direction (without exchanging any information with the other agent). However, at the end of each round each agent has access to limited information regarding its trajectory of movement during this round. We assume that n mobile agents are initially located on a circle unit circumference at arbitrary but distinct positions unknown to other agents. The agents are equipped with unique identifiers from a fixed range. The location discovery task to be performed by each agent is to determine the initial position of every other agent. Our main result states that, if the only available information about movement in a round is limited to distance between the initial and the final position, then there is a superlinear lower bound on time needed to solve the location discovery problem. Interestingly, this result corresponds to a combinatorial symmetry breaking problem, which might be of independent interest. If, on the other hand, an agent has access to the distance to its first collision with another agent in a round, we design an asymptotically efficient and close to optimal solution for the location discovery problem.
[location discovery task, combinatorial mathematics, deterministic symmetry breaking problem, circle unit circumference, Nominations and elections, Complexity theory, mobile robots, ring networks, bouncing, combinatorial symmetry breaking problem, Robot kinematics, superlinear lower bound, distributed coordination mechanism, mobile agents, uniform agents, Trajectory, location discovery, Clocks, Testing]
DISCO: A Distributed Localization Scheme for Mobile Networks
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Localization is one of the key operations in mobile networks. Due to the limitations of GPS, many researchers have devised a variety of different range-free and range-based localization schemes. Range-free schemes utilize the connectivity information to localize mobile nodes. However, the use of the connectivity information allows a high degree of freedom in terms of pinpointing the location of mobile nodes, which leads to low localization precision. Range-based schemes can achieve high localization precision because they require the fine-granularity distance information. Nevertheless, they normally result in high computation complexity and do not work well when part of the distance measurements are missing. In this paper, we propose a distributed range-based localization scheme, DISCO, that uses a series of minimization problems that only involve convex optimization to arrive at high localization precision and low computation complexity. In addition, when some distance measurements are not available, DISCO utilizes the partial distance information to achieve satisfactory localization results. Furthermore, DISCO is a distributed algorithm, which means that it scales well. The performance of DISCO is analyzed through simulation experiments. An in-depth analysis of the time complexity of DISCO is also included in this paper.
[range-free based localization schemes, Temporal Stability, mobility management (mobile radio), Approximation methods, distributed range-based localization scheme, Mobile Networks, Localization, Low Rank, fine-granularity distance information, Minimization, convex programming, Mobile nodes, in-depth analysis, distributed algorithm, computation complexity, distributed algorithms, minimization problem, Convex Optimization, convex optimization, Distance measurement, DISCO, minimisation, Mobile computing, computational complexity, mobile network]
Rain Bar: Robust Application-Driven Visual Communication Using Color Barcodes
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Color barcode-based visible light communication (VLC) over screen-camera links has attracted great research interest in recent years due to its many desirable properties, including free of charge, free of interference, free of complex network configuration and well-controlled communication security. To achieve high-throughput barcode streaming, previous systems separately address design challenges such as image blur, imperfect frame synchronization and error correction etc., without being investigated as an interrelated whole. This does not fully exploit the capacity of color barcode streaming, and these solutions all have their own limitations from a practical perspective. This paper proposes RainBar, a new and improved color barcode-based visual communication system, which features a carefully-designed high-capacity barcode layout design to allow flexible frame synchronization and accurate code extraction. A progressive code locator detection and localization scheme and a robust color recognition scheme are proposed to enhance system robustness and hence the decoding rate under various working conditions. An extensive experimental study is presented to demonstrate the effectiveness and flexibility of RainBar. Results on Android smartphones show that our system achieves higher average throughput than previous systems, under various working environments.
[telecommunication security, RainBar, color barcode, visual communication, flexible frame synchronization, Visible light communication, cameras, Image color analysis, bar codes, robust color recognition scheme, Android smartphone, Robustness, smartphones, image colour analysis, optical communication, high-throughput barcode streaming, color barcode-based visible light communication, Receivers, robustness, smart phones, Synchronization, decoding, progressive code locator detection, synchronisation, screen-camera link, Streaming media, Smart phones, Bars]
ABCCC: An Advanced Cube Based Network for Data Centers
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
A new network structure called BCube Connected Crossbars (BCCC) was recently proposed. Its short diameter, good expandability and low cost make it a very promising topology for data center networks. However, it can utilize only two NIC ports of each server, which is suitable for nowadays technology, even when more ports are available. Due to technology advances, servers with more NIC ports are emerging and they will become low-cost commodities some time later. In this paper, we propose a more general server-centric data center network structure, called Advanced BCube Connected Crossbars (ABCCC), which can utilize inexpensive commodity off-the-shelf switches and servers with any fixed number of NIC ports and provide good network properties. Like BCCC, ABCCC has good expandability. When doing expansion, there is no need to alter the existing system but only to add new components into it. Thus the expansion cost that BCube suffers from can be significantly reduced in ABCCC. We also introduce an addressing scheme and an efficient routing algorithm for one-to-one communication in ABCCC. We make comprehensive comparisons between ABCCC and some popular existing structures in terms of several critical metrics, such as diameter, network size, bisection bandwidth and capital expenditure. We also conduct extensive simulations to evaluate ABCCC, which show that ABCCC achieves the best trade off among all these critical metrics and it suits for many different applications by fine tuning its parameters.
[advanced BCube connected crossbar, network diameter, Ports (Computers), computer networks, topology, Routing, server-centric data center network structure, Topology, Servers, computer centres, ABCCC, server-centric, Network topology, routing algorithm, Data center networks, one-to-one communication, NIC port, expandability, Hypercubes, Hardware, off-the-shelf switch]
Congestion Avoidance with Incremental Filter Aggregation in Content-Based Routing Networks
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
The subscription covering optimization, whereby a general subscription quenches the forwarding of more specific ones, is a common technique to reduce network traffic and routing state in content-based routing networks. Such optimizations, however, leave the system vulnerable to unsubscriptions that trigger the immediate forwarding of all the subscriptions they had previously quenched. These subscription bursts can severely congest the network, and destabilize the system. This paper presents techniques to retain much of the benefits of subscription covering while avoiding bursty subscription traffic. Heuristics are used to estimate the similarity among subscriptions, and a distributed algorithm determines the portions of a subscription propagation tree that should be preserved. Evaluations show that these mechanisms avoid subscription bursts while maintaining relatively compact routing tables.
[Measurement, subscription propagation tree, Corporate acquisitions, Subscriptions, Merging, filtering theory, trees (mathematics), content-based routing networks, subscription covering, Routing, incremental filter aggregation, Optimization, distributed algorithm, congestion avoidance, Databases, heuristics, distributed algorithms, telecommunication network routing, network traffic reduction, general subscription, routing tables, telecommunication traffic]
Rewiring 2 Links Is Enough: Accelerating Failure Recovery in Production Data Center Networks
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Failures are not uncommon in production data center networks (DCNs) nowadays, and it takes long time for the network to recover from a failure and find new forwarding paths, significantly impacting real time and interactive applications at the upper layer. The slow failure recovery is due to two primary reasons. First, there lacks immediate backup paths for downward links in DCN with multi-rooted tree topology. Second, distributed routing protocols in DCN take time to converge after failures. In this paper, we present a fault-tolerant DCN solution, called F2Tree, that can significantly improve the failure recovery time in current DCNs, only through a small amount of link rewiring and switch configuration changes. Because F2Tree does not change any existing software or hardware, it is readily deployed in production DCNs, where other existing proposals fail to achieve. Through testbed and emulation experiments, we show that F2Tree can greatly reduce the time of failure recovery by 78%. Our experimental results also show that, for partition-aggregate applications (popular in DCN) under various failure conditions, F2Tree reduces the ratio of deadline-missing requests by more than 96% compared to current DCNs.
[failure recovery acceleration, production data center networks, Redundancy, Ports (Computers), Switches, forwarding paths, downward links, partition-aggregate applications, fault-tolerant DCN solution, Routing, Topology, computer centres, failure analysis, realtime applications, backup paths, F2Tree, real-time systems, Data center networks, Production, Failure recovery, Routing protocols, fault tolerant computing, interactive applications, multirooted tree topology]
Synthesizing Self-Stabilizing Protocols under Average Recovery Time Constraints
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
A self-stabilizing system is one that converges to a legitimate state from any arbitrary state. Such an arbitrary state may be reachable due to wrong initialization or the occurrence of transient faults. Average recovery time of self-stabilizing systems is a key factor in evaluating their performance, especially in the domain of network and robotic protocols. This paper introduces a groundbreaking result on automated repair and synthesis of self-stabilizing protocols whose average recovery time is required to satisfy certain constraints. We show that synthesizing and repairing weak-stabilizing protocols under average recovery time constraints is NP-complete. To cope with the exponential complexity (unless P = NP), we propose a polynomial-time heuristic.
[Protocols, automated self-stabilizing protocol repair, self-stabilizing system, Probability distribution, Complexity theory, set theory, Recovery, arbitrary state, Convergence, robotic protocols, legitimate state, average recovery time constraints, Synthesis, protocols, Repair, Transient analysis, average recovery time, Maintenance engineering, self-adjusting systems, automated self-stabilizing protocol synthesis, exponential complexity, Fault-tolerance, Time factors, polynomial-time heuristics, computational complexity]
Space-Optimal Time-Efficient Silent Self-Stabilizing Constructions of Constrained Spanning Trees
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Self-stabilizing algorithms are distributed algorithms supporting transient failures. Starting from any configuration, they allow the system to detect whether the actual configuration is legal, and, if not, they allow the system to eventually reach a legal configuration. In the context of network computing, it is known that, for every task, there is a self-stabilizing algorithm solving that task, with optimal space-complexity, but converging in an exponential number of rounds. On the other hand, it is also known that, for every task, there is a self-stabilizing algorithm solving that task in a linear number of rounds, but with large space-complexity. It is however not known whether for every task there exists a self-stabilizing algorithm that is simultaneously space-efficient and time-efficient. In this paper, we make a first attempt for answering the question of whether such an efficient algorithm exists for every task, by focussing on constrained spanning tree construction tasks. We present a general roadmap for the design of silent space-optimal self-stabilizing algorithms solving such tasks, converging in polynomially many rounds under the unfair scheduler. By applying our roadmap to the task of constructing minimum-weight spanning tree (MST), and to the task of constructing minimum-degree spanning tree (MDST), we provide algorithms that outperform previously known algorithms designed and optimized specifically for solving each of these two tasks.
[Algorithm design and analysis, Law, minimum-degree spanning tree, network computing, silent space-optimal self-stabilizing algorithm, Registers, Complexity theory, minimum-weight spanning tree, MST, spanning tree construction task, distributed algorithm, optimal space-complexity, distributed algorithms, Vegetation, space-optimal time-efficient silent self-stabilizing construction, Polynomials, fault tolerant computing, MDST, computational complexity]
Replica Placement for Availability in the Worst Case
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
We explore the problem of placing object replicas on nodes in a distributed system to maximize the number of objects that remain available when node failures occur. In our model, failing (the nodes hosting) a given threshold of replicas is sufficient to disable each object, and the adversary selects which nodes to fail to minimize the number of objects that remain available. We specifically explore placement strategies based on combinatorial structures called t-packings, provide a lower bound for the object availability they offer, show that these placements offer availability that is c-competitive with optimal, propose an efficient algorithm for computing combinations of t-packings that maximize their availability lower bound, and provide parameter selection strategies to concretely instantiate our schemes for different system sizes. We compare the availability offered by our approach to that of random replica placement, owing to the popularity of the latter approach in previous work. After quantifying the availability offered by random replica placement in our model, we show that our combinatorial strategy yields placements with better availability than random replica placement for many realistic parameter values.
[Context, replication, parameter selection strategies, Conferences, Buildings, distributed system, nodes hosting, Probabilistic logic, Virtual machining, availability, random replica placement, t-packings, network operating systems, object replicas, replica placement, data structures, placement strategies, combinatorial structures, Peer-to-peer computing]
Privacy Preserving String Matching for Cloud Computing
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Cloud computing has become indispensable in providing highly reliable data services to users. But, there are major concerns about the privacy of the data stored on cloud servers. While encryption of data provides sufficient protection, it is challenging to support rich querying functionality, such as string matching, over the encrypted data. In this work, we present the first ever symmetric key based approach to support privacy preserving string matching in cloud computing. We describe an efficient and accurate indexing structure, the PASS tree, which can execute a string pattern query in logarithmic time complexity over a set of data items. The PASS tree provides strong privacy guarantees against attacks from a semi-honest adversary. We have comprehensively evaluated our scheme over large real-life data, such as Wikipedia and Enron documents, containing up to 100000 keywords, and show that our algorithms achieve pattern search in less than a few milliseconds with 100% accuracy. Furthermore, we also describe a relevance ranking algorithm to return the most relevant documents to the user based on the pattern query. Our ranking algorithm achieves 90%+ above precision in ranking the returned documents.
[Enron documents, Cloud computing, PASS tree, relevance ranking algorithm, Secure Index, Wikipedia, logarithmic time complexity, Complexity theory, Servers, query processing, Cloud Storage, Privacy, indexing structure, returned document ranking, cloud servers, tree data structures, Cryptography, cloud computing, IND-CKA, document handling, data services, data encryption, semi honest adversary, indexing, SSE, String Matching, Prefix Matching, cryptography, string pattern query, Indexes, privacy preserving string matching, data items, symmetric key based approach, data privacy, string matching, Pattern matching, Data Privacy, computational complexity]
Zeus Milker: Circumventing the P2P Zeus Neighbor List Restriction Mechanism
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
The emerging trend of highly-resilient P2P botnets poses a huge security threat to our modern society. Carefully designed countermeasures as applied in sophisticated P2P botnets such as P2P Zeus impede botnet monitoring and successive takedown. These countermeasures reduce the accuracy of the monitored data, such that an exact reconstruction of the botnet's topology is hard to obtain efficiently. However, an accurate topology snapshot, revealing particularly the identities of all bots, is crucial to execute effective botnet takedown operations. With the goal of obtaining the required snapshot in an efficient manner, we provide a detailed description and analysis of the P2P Zeus neighbor list restriction mechanism. As our main contribution, we propose ZeusMilker, a mechanism for circumventing the existing anti-monitoring countermeasures of P2P Zeus. In contrast to existing approaches, our mechanism deterministically reveals the complete neighbor lists of bots and hence can efficiently provide a reliable topology snapshot of P2P Zeus. We evaluated ZeusMilker on a real-world dataset and found that it outperforms state-of-the-art techniques for botnet monitoring with regard to the number of queries needed to retrieve a bot's complete neighbor list. Furthermore, ZeusMilker is provably optimal in retrieving the complete neighbor list, requiring at most 2n queries for an n-elemental list. Moreover, we also evaluated how the performance of ZeusMilker is impacted by various protocol changes designed to undermine its provable performance bounds.
[Algorithm design and analysis, invasive software, Protocols, Crawlers, security threat, Complexity theory, P2P Zeus neighbor list restriction mechanism, anti-monitoring countermeasures, n-elemental list, Anti-monitoring countermeasures, effective botnet takedown operations, botnet topology exact reconstruction, P2P Zeus, Monitoring, peer-to-peer computing, highly-resilient P2P botnets, topology snapshot, ZeusMilker mechanism, telecommunication network topology, Topology, computer network security, P2P Zeus impede botnet monitoring, Peer-to-peer computing, botnet, XOR metric, milking]
Systematic Mining of Associated Server Herds for Malware Campaign Discovery
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
HTTP is a popular channel for malware to communicate with malicious servers (e.g., Command &amp; Control, drive-by download, drop-zone), as well as to attack benign servers. By utilizing HTTP requests, malware easily disguises itself under a large amount of benign HTTP traffic. Thus, identifying malicious HTTP activities is challenging. We leverage an insight that cyber criminals are increasingly using dynamic malicious infrastructures with multiple servers to be efficient and anonymous in (i) malware distribution (using redirectors and exploit servers), (ii) control (using C&amp;C servers) and (iii) monetization (using payment servers), and (iv) being robust against server takedowns (using multiple backups for each type of servers). Instead of focusing on detecting individual malicious domains, we propose a complementary approach to identify a group of closely related servers that are potentially involved in the same malware campaign, which we term as Associated Server Herd (ASH). Our solution, SMASH (Systematic Mining of Associated Server Herds), utilizes an unsupervised framework to infer malware ASHs by systematically mining the relations among all servers from multiple dimensions. We build a prototype system of SMASH and evaluate it with traces from a large ISP. The result shows that SMASH successfully infers a large number of previously undetected malicious servers and possible zero-day attacks, with low false positives. We believe the inferred ASHs provide a better global view of the attack campaign that may not be easily captured by detecting only individual servers.
[invasive software, HTTP traffic, network servers, HTTP requests, Communities, data mining, dynamic malicious infrastructures, hypermedia, malicious HTTP activities, ISP, SMASH, undetected malicious servers, zero-day attacks, Servers, cyber criminals, malware distribution, Ash, systematic mining of associated server herds, Malware, Silicon, Robustness, IP networks, malware campaign discovery]
MAVR: Code Reuse Stealthy Attacks and Mitigation on Unmanned Aerial Vehicles
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
As embedded systems have increased in performance and reliability, their applications have expanded into new domains such as automated drone-based delivery mechanisms. Security of these drones, also referred to as unmanned aerial vehicles (UAVs), is crucial due to their use in many different domains. In this paper, we present a stealthy attack strategy that allows the attacker to change sensor values and modify the UAV navigation path. As the attack is stealthy, the system will continue to execute normally and thus the ground station or other monitoring entities and systems will not be able to detect that an attack is undergoing. With respect to defense, we propose a strategy that combines software and hardware techniques. At software level, we propose a fine grained randomization based approach that modifies the layout of the executable code and hinders code-reuse attack. To strengthen the security of our defense, we leverage a custom hardware platform designed and built by us. The platform isolates the code binary and randomized binary in such a way that the actual code being executed is never exposed for an attacker to analyze. We have implemented a prototype of this defense technique and present results to demonstrate the effectiveness and efficiency of this defense strategy.
[defense technique, Microcontrollers, unmanned aerial vehicles, automated drone-based delivery mechanisms, ground station, software techniques, randomized binary, Registers, code binary, autonomous aerial vehicles, code reuse stealthy attacks, custom hardware platform, MAVR, embedded systems, Ash, Computer architecture, Hardware, Software, sensor values, control engineering computing, fine grained randomization based approach, Payloads]
Shuttle: Intrusion Recovery for PaaS
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
The number of applications being deployed using the Platform as a Service (PaaS) cloud computing model is increasing. Despite the security controls implemented by cloud service providers, we expect intrusions to strike such applications. We present Shuttle, a novel intrusion recovery service. Shuttle recovers from intrusions in applications deployed in PaaS platforms. Our approach allows undoing changes to the state of PaaS applications due to intrusions, without loosing the effect of legitimate operations performed after the intrusions take place. We combine a record-and-replay approach with the elasticity provided by cloud offerings to recover applications deployed on various instances and backed by distributed databases. The service loads a database snapshot taken before the intrusion and replays subsequent requests, as much in parallel as possible, while continuing to execute incoming requests. We present an experimental evaluation of Shuttle on Amazon Web Services. We show Shuttle can replay 1 million requests in 10 minutes and that it can duplicate the number of requests replayed per second by increasing the number of application servers from 1 to 3.
[database snapshot, Elasticity, platform as a service, Servers, Security, Intrusion Tolerance, Amazon Web services, Cloud Computing, Distributed Database Systems, Distributed databases, distributed databases, time 10 min, Intrusion Recovery, application servers, cloud computing, Shuttle, intrusion recovery service, cloud service providers, Computational modeling, PaaS platforms, Dependability, Platform as a Service, security of data, Web services, security controls, cloud computing model, record-and-replay approach, Software]
Accelerating Apache Hive with MPI for Data Warehouse Systems
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Data warehouse systems, like Apache Hive, have been widely used in the distributed computing field. However, current generation data warehouse systems have not fully embraced High Performance Computing (HPC) technologies even though the trend of converging Big Data and HPC is emerging. For example, in traditional HPC field, Message Passing Interface (MPI) libraries have been optimized for HPC applications during last decades to deliver ultra-high data movement performance. Recent studies, like DataMPI, are extending MPI for Big Data applications to bridge these two fields. This trend motivates us to explore whether MPI can benefit data warehouse systems, such as Apache Hive. In this paper, we propose a novel design to accelerate Apache Hive by utilizing DataMPI. We further optimize the DataMPI engine by introducing enhanced non-blocking communication and parallelism mechanisms for typical Hive workloads based on their communication characteristics. Our design can fully and transparently support Hive workloads like Intel HiBench and TPC-H with high productivity. Performance evaluation with Intel HiBench shows that with the help of light-weight DataMPI library design, efficient job start up and data movement mechanisms, Hive on DataMPI performs 30% faster than Hive on Hadoop averagely. And the experiments on TPC-H with ORCFile show that the performance of Hive on DataMPI can improve 32% averagely and 53% at most more than that of Hive on Hadoop. To the best of our knowledge, Hive on DataMPI is the first attempt to propose a general design for fully supporting and accelerating data warehouse systems with MPI.
[application program interfaces, TPC-H, MPI, DataMPI library design, HPC applications, parallel processing, Engines, data warehouse systems, Data Warehouse Systems, Data MPI, ORCFile, Benchmark testing, Big data, nonblocking communication, message passing, message passing interface, Hadoop, Data warehouses, Generators, Aggregates, Intel HiBench, high performance computing, Acceleration, data warehouses, Apache Hive]
RStore: A Direct-Access DRAM-based Data Store
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Distributed DRAM stores have become an attractive option for providing fast data accesses to analytics applications. To accelerate the performance of these stores, researchers have proposed using RDMA technology. RDMA offers high bandwidth and low latency data access by carefully separating resource setup from IO operations, and making IO operations fast by using rich network semantics and offloading. Despite recent interest, leveraging the full potential of RDMA in a distributed environment remains a challenging task. In this paper, we present RDMA Store or RStore, a DRAM-based data store that delivers high performance by extending RDMA's separation philosophy to a distributed setting. RStore achieves high aggregate bandwidth (705 Gb/s) and close-to-hardware latency on our 12-machine testbed. We developed a distributed graph processing framework and a Key-Value sorter using RStore's unique memory-like API. The graph processing framework, which relies on RStore for low-latency graph access, outperforms state-of-the-art systems by margins of 2.6 -- 4.2&#x00D7; when calculating Page Rank. The Key-Value sorter can sort 256 GB of data in 31.7 sec, which is 8&#x00D7; better than Hadoop TeraSort in a similar setting.
[magnetic disc storage, RDMA Store, distributed graph processing framework, application program interfaces, Random access memory, remote direct memory access, memory-like API, PageRank, Data processing, Servers, direct-access DRAM-based data store, Next generation networking, Data storage systems, Memory management, Distributed databases, key-value sorter, Bandwidth, low-latency graph access, DRAM chips, file organisation, RStore, Resource management]
Cache Serializability: Reducing Inconsistency in Edge Transactions
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Read-only caches are widely used in cloud infrastructures to reduce access latency and load on backend databases. Operators view coherent caches as impractical at genuinely large scale and many client-facing caches are updated asynchronously with best-effort pipelines. Existing solutions that support cache consistency are inapplicable to this scenario since they require a round trip to the database on every cache transaction. Existing incoherent cache technologies are oblivious to transactional data access, even if the backend database supports transactions. We propose T-Cache, a novel caching policy for read-only transactions in which inconsistency is tolerable (won't cause safety violations) but undesirable (has a cost). T-Cache improves cache consistency despite asynchronous and unreliable communication between the cache and the database. We define cache-serializability, a variant of serializability that is suitable for incoherent caches, and prove that with unbounded resources T-Cache implements this new specification. With limited resources, T-Cache allows the system manager to choose a trade-off between performance and consistency. Our evaluation shows that T-Cache detects many inconsistencies with only nominal overhead. We use synthetic workloads to demonstrate the efficacy of T-Cache when data accesses are clustered and its adaptive reaction to workload changes. With workloads based on the real-world topologies, T-Cache detects 43 -- 70% of the inconsistencies and increases the rate of consistent transactions by 33 -- 58%.
[transaction processing, transactional data access, Companies, cache technologies, Throughput, cache storage, Servers, client-facing caches, backend databases, Distributed databases, Prototypes, read-only caches, cache serializability, cloud infrastructures, T-Cache, cloud computing, cache consistency, cache, Social network services, real-world topologies, transactions, read-only transactions, cache transaction, system manager, edge transaction inconsistency reduction]
A Novel Approximation for Multi-hop Connected Clustering Problem in Wireless Sensor Networks
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Wireless sensor networks (WSNs) have been widely used in plenty of applications. To achieve higher efficiency for data collection, WSNs are often partitioned into several disjointed clusters, each with a representative cluster head in charge of the data gathering and routing process. Such a partition is balanced and effective if the distance between each node and its cluster head can be bounded within a constant number of hops, and any two cluster heads are connected. Finding such a cluster partition with minimum number of clusters and connectors between cluster heads is defined as minimum connected d-hop dominating set (d-MCDS) problem, which is proved to be NP-complete. In this paper, we propose a distributed approximation algorithm, named CS-Cluster, to address the d-MCDS problem. CS-Cluster constructs a sparser d-hop maximal independent set (d-MIS), connects the d-MIS and finally checks and removes redundant nodes. We prove the approximation ratio of CS-Cluster is (2d + l)&#x03BB;, where &#x03BB; is a parameter related with d but is no more than 18.4. Compared with the previous best result O(d2), our approximation ratio is a great improvement. Our evaluation results demonstrate the outstanding performance of our algorithm compared with previous works.
[multihop connected clustering problem, CS-cluster algorithm, routing process, wireless sensor networks, Color, wireless sensor network, NP-complete, Approximation methods, Connectors, cluster head, distributed approximation algorithm, Wireless sensor networks, optimisation, WSN, pattern clustering, d-MCDS problem, Clustering algorithms, telecommunication network routing, data gathering, Approximation algorithms, data communication, sparser d-hop maximal independent set, Distributed algorithms, computational complexity]
Multi-path Based Avoidance Routing in Wireless Networks
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
The speedy advancement in computer hardware has caused data encryption to no longer be a 100% safe solution for secure communications. To battle with adversaries, a countermeasure is to avoid message routing through certain insecure areas, e.g., Malicious countries and nodes. To this end, avoidance routing has been proposed over the past few years. However, the existing avoidance protocols are single-path-based, which means that there must be a safe path such that no adversary is in the proximity of the whole path. This condition is difficult to satisfy. As a result, routing opportunities based on the existing avoidance schemes are limited. To tackle this issue, we propose an avoidance routing framework, namely Multi-Path Avoidance Routing (MPAR). In our approach, a source node first encodes a message into k different pieces, and each piece is sent via k different paths. The destination can assemble the original message easily, while an adversary cannot recover the original message unless she obtains all the pieces. We prove that the coding scheme achieves perfect secrecy against eavesdropping under the condition that an adversary has incomplete information regarding the message. The simulation results validate that the proposed MPAR protocol achieves its design goals.
[network coding, radio networks, cryptographic protocols, speedy advancement, data encryption, electronic messaging, Routing, Encoding, multipath channels, Encryption, coding scheme, secure communication, multipath message avoidance routing protocol, routing protocols, computer hardware, MPAR protocol, Routing protocols, wireless network]
Tele Adjusting: Using Path Coding and Opportunistic Forwarding for Remote Control in WSNs
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
On-air access of individual sensor node (called remote control) is an indispensable function in operational wireless sensor networks, for purposes like network management and real-time information delivery. To realize reliable and efficient remote control in a wireless sensor network (WSN), however, is extremely challenging, due to the stringent resource constraints and intrinsically unrealizable wireless communication. In this paper, we propose TeleAdjusting, a ready-to-use protocol to remotely control any individual node in a WSN. We develop a coding scheme for addressing on the cost-optimal reverse routing tree. In the address of each node, all its upstream relaying nodes are implicitly encoded. Then through a distributed prefix matching process between the local address and the destination address, a packet used for remote control is forwarded along a cost-optimal path. Moreover, TeleAdjusting incorporates opportunistic forwarding into the addressing process, so as to improve the network performance in terms of reliability and energy efficiency. We implement TeleAdjusting with TinyOS and evaluate its performance through extensive simulations and experiments. The results demonstrate that compared to the existing protocols, TeleAdjusting can provide high performance of remote control, which is as reliable as network-wide flooding and much more efficient than remote control through a pre-determined path.
[Protocols, wireless sensor networks, distributed prefix matching process, tree codes, Relays, wireless communication, Wireless communication, coding, TinyOS, telecommunication network reliability, energy efficiency, network coding, telecommunication power management, path coding, remote control, wireless sensor network reliability, trees (mathematics), Routing, WSNs, TeleAdjusting, opportunistic forwarding, Wireless sensor network, Wireless sensor networks, cost-optimal reverse routing tree, telecontrol, ready-to-use protocol, routing protocols, energy conservation, low power, Reliability, Resource management]
Towards Redundancy-Aware Data Utility Maximization in Crowdsourced Sensing with Smartphones
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
This paper studies the critical problem of maximizing the aggregate data utility under the practical constraint on budget in mobile crowd sourced sensing. This problem is particularly challenging given the redundancy in sensing data, self-interested and strategic user behaviors, private cost information of smartphones and budget constraint. In this paper, we propose a combinatorial auction mechanism based on a redundancy-aware reverse auction framework. It consists of an approximation algorithm for winning bids determination and a critical payment scheme. Our mechanism achieves truthfulness, individual rationality, computational efficiency, budget feasibility and high redundancy-aware data utility.
[Algorithm design and analysis, individual rationality, mobile crowdsourced sensing, strategic user behaviors, combinatorial mathematics, Noise, budget constraint, reverse auction framework, computational efficiency, Approximation methods, commerce, private cost information, winning bids determination, Budget Feasibility, approximation algorithm, smartphones, Sensors, combinatorial auction mechanism, truthfulness, critical payment scheme, budget feasibility, Auction, smart phones, Mobile Crowdsourcing, sensing data, self-interested behaviors, Truthful, outsourcing, redundancy-aware data utility maximization, Approximation algorithms, Nickel, Redundancy-aware, Smart phones]
Improving the Energy Benefit for 802.3az Using Dynamic Coalescing Techniques
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
In this work, we propose a dynamic coalescing algorithm for IEEE 802.3az standard, which dynamically adapts coalescing operations based on the current load and on the delay experienced in the link. Our results show that our algorithm almost doubles the energy efficiency of EEE with static coalescing while keeping packet delay bounded.
[Algorithm design and analysis, Energy consumption, 802.3az, Heuristic algorithms, telecommunication power management, EPON, local area networks, Servers, Energy Efficiency, dynamic coalescing technique, Coalescing, IEEE 802.3 Standard, energy efficiency, energy conservation, Delays, energy benefit improvement, IEEE 802.3az standard]
Policy-Aware Virtual Machine Management in Data Center Networks
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Policies play an important role in network configuration and, therefore, in offering secure and high performance services, especially over multi-tenant Cloud Data Center (DC) environments. At the same time, elastic resource provisioning through virtualization often disregards policy requirements, assuming that the policy implementation is handled by the underlying network infrastructure. In this paper, we define PLAN, a Policy-Aware virtual machine management scheme to jointly consider DC communication cost reduction through Virtual Machine (VM) migration while meeting network policy requirements.
[Computers, policy-aware virtual machine management, cost reduction, government policies, multitenant cloud data center environments, Middleboxes, Virtual machining, Servers, PLAN, computer centres, Computer science, elastic resource provisioning, network configuration, Virtual machine monitors, virtual machines, DC environments, Resource management, cloud computing, network policy requirements]
Lightitude: Indoor Positioning Using Ubiquitous Visible Lights and COTS Devices
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
In this paper, we propose a novel indoor localization scheme, Lightitude, by exploiting ubiquitous visible lights, which are necessarily and densely deployed in almost all indoor environments. Different from existing positioning systems that exploit special LEDs, ubiquitous visible lights lack fingerprints that can uniquely identify the light source, which results in an ambiguity problem that an RLS may correspond to multiple candidate positions. Moreover, received light strength (RLS) is not only determined by device's position, but also seriously affected by its orientation, which causes great complexity in site-survey. To address these challenges, we first propose and validate a realistic light strength model to avoid the expensive site-survey, then harness user's mobility to generate spatial-related RLS to tackle single RLS's position-ambiguity problem. Experiment results show that Lightitude achieves mean accuracy 1.93m and 2.24m in office (720m2) and library scenario (960m2) respectively.
[Performance evaluation, indoor navigation, COTS device, Indoor localization, ubiquitous visible light, Light emitting diodes, LED, lightitude, light emitting diodes, Light sources, Computer science, received light strength model, Robustness, Libraries, Ubiquitous visible light, Sensors, optical communication, indoor positioning, RLS position-ambiguity problem]
Leveraging Fog to Extend Cloud Gaming for Thin-Client MMOG with High Quality of Experience
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
With the increasing popularity of Massively Multiplayer Online Game (MMOG) and fast growth of mobile gaming, cloud gaming exhibits great promises over the conventional MMOG gaming model as it frees players from the requirement of hardware and game installation on their local computers. However, as the graphics rendering is offloaded to the cloud, the data transmission between the end-users and the cloud significantly increases the response latency and limits the user coverage, thus preventing cloud gaming to achieve high user Quality of Experience (QoE). To solve this problem, previous research suggested deploying more data centers, but it comes at a prohibitive cost. We propose a lightweight system called Cloud Fog, which incorporates "fog" consisting of super nodes that are responsible for rendering game videos and streaming them to their nearby players. Fog enables the cloud to be only responsible for the intensive game state computation and sending update information to super nodes, which significantly reduce the traffic hence the latency and bandwidth consumption. Experimental results from PeerSim and Planet Lab show the effectiveness and efficiency of Cloud Fog in increasing user coverage, reducing response latency and bandwidth consumption.
[Cloud computing, graphics rendering, P2P network, QoE, Servers, Online gaming, cloud fog, intensive game state computation, computer games, Bandwidth, Planet Lab, cloud computing, rendering (computer graphics), game video rendering, bandwidth consumption, peer-to-peer computing, Computational modeling, PeerSim, massively multiplayer online game, mobile gaming, Quality of experience, cloud gaming, quality of experience, thin-client MMOG, game installation, Games, data transmission, Streaming media, Cloud gaming, Delays]
EcoFlow: An Economical and Deadline-Driven Inter-datacenter Video Flow Scheduling System
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
As video streaming applications are deployed on the cloud, cloud providers are charged by ISPs for inter-data enter transfers under the dominant percentile-based charging models. In order to minimize the payment costs, existing works aim to keep the traffic on each link under the charging volume (i.e., 95th percentile traffic volume from the beginning of a charging period up to current time). However, these methods cannot fully utilize each link's available bandwidth capacity, and may increase the charging volumes. To further reduce the bandwidth payment cost by fully utilizing link bandwidth, we propose an economical and deadline-driven video flow scheduling system, called EcoFlow. Considering different video flows have different transmission deadlines, EcoFlow transmits videos in the order of their deadline tightness and postpones the deliveries of later-deadline videos to later time slots so that the charging volume at current time interval will not increase. The flows that are expected to miss their deadlines are divided into sub flows to be rerouted to other underutilized links in order to meet their deadlines without increasing charging volumes. Experimental results on Planet Lab and EC2 show that compared to existing methods, EcoFlow achieves the least bandwidth costs for cloud providers.
[Conferences, EcoFlow, Distributed computing, time slots, charging period, EC2, Bandwidth, scheduling, Planet Lab, Bandwidth cost, video streaming, time interval, cloud computing, Percentile-based charging models, inter datacenter transfers, percentile traffic volume, cost reduction, video streaming applications, charging volume, Biological system modeling, economical inter datacenter video flow scheduling system, bandwidth payment cost reduction, Video streaming, link bandwidth utilization, Routing, computer centres, payment cost minimization, bandwidth allocation, percentile-based charging models, deadline-driven inter datacenter video flow scheduling system, Streaming media, Inter-datacenter traffic, High definition video, telecommunication traffic]
Harnessing the Power of Multiple Cloud Service Providers: An Economical and SLA-Guaranteed Cloud Storage Service
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
It is critical for a cloud service broker to minimize its payment cost to cloud service providers for serving the broker's customers while providing SLA-guaranteed services. In this paper, we propose an Economical and SLA-guaranteed cloud Storage Service (ES3), which finds a data allocation and resource reservation schedule with cost minimization. ES3 incorporates (1) a data allocation and reservation algorithm, which allocates each data item to a datacenter and determines the reservation amount on data centers by leveraging all the pricing policies, (2) a genetic algorithm based data allocation adjustment approach, which makes data Get/Put rates stable in each datacenter to maximize the reservation benefit. Our trace-driven experiments show the superior performance of ES3.
[Cloud computing, Schedules, ES3, data reservation algorithm, Payment cost minimization, Optimal scheduling, SLA, contracts, storage management, resource allocation, Pricing, genetic algorithm based data allocation adjustment approach, cloud computing, resource reservation scheduling, cost minimization, economical and SLA-guaranteed cloud storage service, Minimization, cloud service broker, genetic algorithms, computer centres, Get-Put rates, SLA-guaranteed services, Web services, data center, Aggregates, broker customer, multiple cloud service providers, data allocation algorithm, Cloud storage, Data availability, Resource management, pricing]
Approximate Holistic Aggregation in Wireless Sensor Networks
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Holistic aggregation results are important for users to obtain summary information from Wireless Sensor Networks (WSNs). Holistic aggregation requires all the sensory data to be sent to the sink, which costs a huge amount of energy. Fortunately, in most applications, approximate results are acceptable. We study the approximated holistic aggregation algorithms based on uniform sampling. In this paper, four holistic aggregation operations are investigated. The mathematical methods to construct their estimators and determine the optional sample size are proposed, and the correctness of these methods is proved. Four corresponding distributed holistic algorithms are presented. The theoretical analysis and simulation results show that the algorithms have high performance.
[Algorithm design and analysis, approximation theory, wireless sensor networks, uniform sampling, Electronic mail, Computer science, Wireless sensor networks, WSN, Simulation, distributed algorithms, mathematical methods, Clustering algorithms, Approximation algorithms, optional sample size determination, approximate holistic aggregation, distributed holistic algorithms]
Planning Battery Swapping Stations for Urban Electrical Taxis
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Despite the clear benefits of electric vehicles (EVs) in terms of reducing greenhouse gas emissions and traditional energy consumptions, the popularization of EVs remains a challenge in the short run. When considering electric taxis, urban planners must face the additional issue of providing battery swapping services. While previous studies focused on planning battery swapping stations for private EVs, we investigate ways of supporting the upgrade of an entire urban taxi system, with demands differing both in scale and nature. With this insight, we analyze the historical sensing data of taxi routes, and evaluate the battery swapping demand profile, as well as the driving time between positions in the road network. Based on these inputs, we propose a method to calculate an optimized battery swapping station scheme. Our strategies are then evaluated via a real world 366-day, 3,976-taxi dataset. The results show that compared to uniform deployment, our planning scheme reduces the average time-cost by 67.2%.
[power system planning, electric vehicles, Urban areas, Hydrogen, automobiles, greenhouse gas emission reduction, Batteries, battery swapping station planning, Computer science, urban electrical taxis, electric vehicle, Electric vehicles, Planning, energy consumption, historical sensing data analysis, air pollution control]
Interference-Aware Component Scheduling for Reducing Tail Latency in Cloud Interactive Services
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Large-scale interactive services usually divide requests into multiple sub-requests and distribute them to a large number of server components for parallel execution. Hence the tail latency (i.e. The slowest component's latency) of these components determines the overall service latency. On a cloud platform, each component shares and competes node resources such as caches and I/O bandwidths with its co-located jobs, hence inevitably suffering from their performance interference. In this paper, we study the short-running jobs in a 12k-node Google cluster to illustrate the dynamic resource demands of these jobs, resulting in both individual components' latency variability over time and across different nodes and hence posing a major challenge to maintain low tail latency. Given this motivation, this paper introduces a dynamic and interference-aware scheduler for large-scale, parallel cloud services. At each scheduling interval, it collects workload and resource contention information of a running service, and predicts both the component latency on different nodes and the overall service performance. Based on the predicted performance, the scheduler identifies straggling components and conducts near-optimal component-node allocations to adapt to the changing workloads and performance interferences. We demonstrate that, using realistic workloads, the proposed approach achieves significant reductions in tail latency compared to the basic approach without scheduling.
[interference-aware component scheduling, resource contention information, Google, workload information, parallel cloud services, Interference, parallel execution, component latency, Dynamic scheduling, Servers, component latency variability, interference-aware scheduler, Cloud interactive services, tail latency, server components, cloud interactive services, dynamic scheduler, Bandwidth, Resource management, cloud computing, tail latency reduction, large-scale interactive services, dynamic resource demands, Monitoring]
Foreseer: Workload-Aware Data Storage for MapReduce
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Inter-job Write once read many (WORM) scenario is ubiquitous in MapReduce applications that are widely deployed on enterprise production systems. However, traditional MapReduce auto-tuning techniques can not address the inter-job WORM scenario. To address the shortcomings in existing works, this work presents a novel online cross-layer solution, FORESEER. It can automatically predict workloads' data access information and tune data placement parameters to optimize the over-all performance for an inter-job WORM scenario. In our experiments, we observe that FORESEER can achieve significant performance speedup (up to 37%) compared with previous work.
[Greedy algorithms, MapReduce auto-tuning techniques, inter-job WORM scenario, Foreseer, data placement parameters, Partitioning algorithms, Grippers, parallel processing, inter-job write once read many scenario, Optimization, online cross-layer solution, storage management, Distributed databases, Clustering algorithms, workload-aware data storage, Writing, enterprise production systems, workload data access information prediction, data handling]
Operational Transformation for Real-Time Synchronization of Cloud Storage
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
This paper presents an Operational Transformation (OT) technique, named CSOT (Cloud Storage OT), that supports real-time file synchronization in cloud storage and achieve well-defined consistent combined-effects of concurrent file manipulation operations. We have developed and used a comprehensive suite of concurrency testing cases to derive and compare the results produced by CSOT and three industrial cloud storage systems and made interesting discoveries.
[Cloud computing, concurrent file manipulation operations, Maintenance engineering, operational transformation technique, Synchronization, real-time file synchronization, CSOT technique, synchronisation, Concurrent computing, storage management, Collaboration, Benchmark testing, cloud storage, Real-time systems, real-time synchronization, cloud computing, industrial cloud storage systems]
A Route Scheduling Algorithm for the Sweep Coverage Problem
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
In order to decrease the sweep cycle and the number of mobile sensors required, we propose a route scheduling problem in this paper which is the first to consider the effect of sensing range. We prove that the Distance-Sensitive-Route Scheduling(DSRS) problem is NP-hard, and consider two different scenarios: the single kissing-point case and the general case. For different cases, We propose three corresponding approximation algorithms ROSE, G-ROSE, D-ROSE.
[approximation theory, wireless sensor networks, mobile sensors, Conferences, sweep cycle, Mobile communication, telecommunication scheduling, distance sensitive route scheduling, ROSE, Scheduling, mobility management (mobile radio), Approximation methods, DSRS problem, optimisation, D-ROSE, Scheduling algorithms, sweep coverage problem, NP-hard problem, telecommunication network routing, approximation algorithm, Approximation algorithms, Sensors, G-ROSE]
WhiFind: A Matrix Completion Approach for Indoor White Space Identification
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
In this paper, we introduce WhiFind, which is an indoor white space identification system. It leverages matrix completion techniques to provide accurate indoor white space availability information with only a small number of spectrum sensors deployed.
[TV, Correlation, indoor white space identification, matrix completion approach, Matrix decomposition, WhiFind, matrix algebra, Databases, sensor placement, White spaces, Sensors, indoor radio, Matrix converters, spectrum sensor deployment]
mQual: A Mobile Peer-to-Peer Network Framework Supporting Quality of Service
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Mobile peer-to-peer applications require devices to network themselves on-the-fly to communicate directly with each another. This paper presents mQual, a framework to help create such networks that meet different application requirements, and is able to adjust the network to ensure that these requirements are met in dynamic environments. Our prototype mQual extends the current WiFi-Direct in Android, and the experimental results suggests that mobile apps built using mQual outperform those built using WiFi-Direct.
[mobile apps, mobile peer-to-peer network, peer-to-peer computing, mobile peer-to-peer, Mobile communication, WiFi-Direct, quality of service, Servers, mobile P2P, Android, mobile computing, D2D, mQual, Peer-to-peer computing, Mobile computing, IEEE 802.11 Standard, Smart phones]
An Efficient Anonymous Authentication Protocol for RFID Systems Using Dynamic Tokens
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Radio frequency identification (RFID) technologies are widely used in many applications. The widespread use of tags in traditional ways of deployment raises a privacy concern: They make their carriers track able. This paper studies the problem of anonymous authentication. Due to resource constraints of low-cost tags, we develop a new technique to generate dynamic tokens for anonymous authentication by following an asymmetric design principle that pushes most complexity to more powerful RFID readers. Instead of implementing complicated cryptographic hash functions, our authentication protocol only requires tags to perform several simple hardware-efficient operations such as bitwise XOR, one-bit left circular shift and bit flip. Moreover, our protocol reduces the communication overhead and online computation overhead to O(1) per authentication for both tags and readers, which compares favorably with the prior art.
[hardware-efficient operation, Protocols, cryptographic protocols, radiofrequency identification, online computation overhead reduction, cryptographic hash function, Servers, Indexes, RFID system, dynamic token, Integrated circuits, communication overhead reduction, Authentication, anonymous authentication protocol, Arrays, radiofrequency identification technology, Radiofrequency identification, computational complexity]
On Privacy Preserving Partial Image Sharing
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Sharing photos through Online Social Networks becomes an increasingly popular fashion. However, users' privacy may be at stake when sensitive photos are shared improperly. This paper presents a dynamic privacy protection technique (named PuPPIeS) for image data where the data owner stipulates small private regions for sensitive objects (faces, SSN numbers, etc.) of a photo/image and sets different sharing policies for these partial regions with respect to different individuals. PuPPIeS is based on optimized reversible matrix perturbation of compressed image data. Hence it can naturally support frequently used image transformations. Our experiments show that our solution is effective for privacy protection and incurs only a small overhead for partial image sharing.
[Cloud computing, perturbation techniques, photo sharing policies, image transformations, online social networks, optimized reversible matrix perturbation, privacy preserving partial image sharing, compressed image data, Standards, matrix algebra, SSN numbers, Privacy, Image coding, PuPPIeS, user privacy, privacy protection, image retrieval, social networking (online), dynamic privacy protection technique, data privacy, Cryptography, Discrete cosine transforms, Facebook, image coding]
Inspecting Coding Dependency in Layered Video Coding for Efficient Unequal Error Protection
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
To improve the quality of video streaming subject to video bitrate or communication channel capacity, a high-quality video is encoded into multiple layers of unequal importance. Layers that provide higher quality rely on the previous layers for successful reconstruction of transmitted video packets. Hence, if a video packet in a reference layer is corrupted or lost during transmission, the dependent layers cannot be reconstructed successfully, and the resources consumed to transmit them are wasted. To address this problem, unequal error protection (UEP) techniques have been proposed to provide appropriate level of protection to each layer according to their importance. Nonetheless, the importance of a piece of video content is determined by not only the layering structure, but also coding dependency imposed by encoding decisions. In this paper, based on a deep inspection of coding and prediction in SVC (a layered video coding standard) and an analysis of seven real SVC videos, we conclude that macro block-level coding dependency will provide a more accurate importance measure when applying UEP to protection video packets in noisy channels.
[Video coding, video packet reconstruction, Visualization, SVC, unequal error protection, video bitrate, video packet protection, channel capacity, macroblock-level coding dependency, Encoding, image reconstruction, video coding, high-quality video encoding, communication channel capacity, Standards, video streaming quality improvement, UEP technique, Static VAr compensators, block codes, Streaming media, noisy channel, video streaming, Error correction codes, layered video coding dependency inspection]
Fast Total Ordering for Modern Data Centers
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Data center applications rely on messaging services that guarantee reliable, ordered message delivery for a wide range of distributed coordination tasks. Totally ordered multicast, which (informally) guarantees that all processes receive messages in exactly the same order, is particularly useful for maintaining consistent distributed state in systems as diverse as financial systems, distributed storage systems, cloud management, and big data analytics platforms.
[Protocols, big data analytics platforms, Switches, distributed coordination tasks, financial systems, Multicast communication, Big Data, Throughput, distributed storage systems, Servers, computer centres, data center applications, messaging services, Prototypes, cloud management, fast total ordering, Acceleration, cloud computing]
On Preserving Data Integrity of Transactional Applications on Multicore Architectures
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Multicore architectures are increasingly becoming prone to transient faults. In this paper we briefly present Shield, a middleware to provide transactional applications with resiliency to those faults that can happen anytime during the execution of a processor but do not cause any hardware interruption. Shield is inspired by the state machine replication approach, where computational resources are partitioned, the shared state is fully replicated, and requests are executed by all partitions in the same order. Shield embeds a set of algorithmic and system innovations to limit the overhead with respect to non-fault-tolerant solutions. They include a fast total order layer that lets application threads and computational nodes co-operate in order to fast deliver.
[transaction processing, multiprocessing systems, Multicore processing, parallel architectures, state machine replication approach, data integrity, Shield middleware, Registers, transactional applications, finite state machines, transient faults, Soft Errors, data integrity preservation, Transactions, multicore architectures, nonfault-tolerant solutions, Hardware, Reliability, Transient analysis, hardware interruption, Message systems, middleware, Fault Tolerance]
On Exploiting Locality for Generalized Consensus
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Single leader-based Consensus protocols are known to stop scaling once the leader reaches its saturation point. On the other hand, establishing Consensus of commands by taking into account only their dependencies (as specified by Generalized Consensus) is appealing because of the potentially higher parallelism and lower latency. However, current solutions have well-known pitfalls due to the higher quorum size, which is required to exploit low-latency fast decisions, and the need for tracking dependency relations. In this paper we briefly introduce M2PAXOS, a new implementation of Generalized Consensus that provides a fast decision of commands by leveraging a classic quorum size, which matches just the majority of nodes deployed. M2PAXOS does not establish command dependencies based on conflicts, rather it associates accessed objects with nodes, so that the delivery decision of commands operating on the same objects is made by a common node. The evaluation study of M2PAXOS confirms its effectiveness by showing an improvement up to 7&#x00D7; over state-of-the-art (Generalized) Consensus protocols.
[Protocols, low-latency fast decisions, generalized consensus protocol locality, Scalability, Instruction sets, Buildings, Locality, Throughput, Generalized Consensus, tracking dependency relations, Distributed databases, classic quorum size, Delays, protocols, M2PAXOS, single leader-based consensus protocols]
Less Transmissions, More Throughput: Bringing Carpool to Public WLANs
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
The proliferation of WiFi hotspots in public places enables ubiquitous Internet access. These public WiFi hotspots usually serve scores of mobile devices and suffer from extremely poor performance in terms of low good put and severe delay. In this paper, we first study the traffic characteristics in public WiFi networks, and demonstrate that the main causes of such poor performance are media access control (MAC) inefficiency and downlink-uplink traffic asymmetry. To cope with these issues, we call attention to transmission carpool, which facilitates an access point (AP) to send multiple frames for different mobile stations (STAs) in a single transmission. It reduces contention and conveys more frames in each channel access. As such, each downlink transmission carries more payload and thus improves efficiency and solves traffic asymmetry simultaneously.
[Wireless LAN, public WLAN, Conferences, Downlink, Throughput, access protocols, ubiquitous Internet access, Libraries, wireless channels, IEEE 802.11n Standard, contention reduces, Frame Aggregation, access point, mobile radio, Wi-Fi hotspot, MAC efficiency, media access control, MAC, AP, mobile device, channel access, downlink-uplink traffic asymmetry, PHY/MAC Design, carpool, Internet, wireless LAN, public WiFi network, transmission carpool, telecommunication traffic]
A Mechanism Approach to Reduce New Seller Ramp-Up Time in eBay-Like Reputation Systems
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
We present a novel insurance mechanism consisting of an insurance protocol and a transaction mechanism to reduce new seller ramp up time in eBay-like reputation mechanisms. We conduct experiments on an eBay's dataset and show that our insurance mechanism reduces ramp up time by 90%.
[insurance, Protocols, insurance mechanism, eBay-like reputation systems, Companies, reputation, Product design, ramp up time, eBay dataset, Negative feedback, Insurance, new seller ramp up time reduction, Quality assessment, Internet, insurance protocol, electronic commerce, retail data processing, transaction mechanism]
RahasNym: Protecting against Linkability in the Digital Identity Ecosystem
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Unlink ability and accountability are conflicting yet critical requirements for on-line transactions that need to be addressed in order to preserve users' privacy as well as to protect service providers in today identity ecosystems. In this poster paper we introduce a pseudonymous identity management system in which users can carry out unlink able on-line transactions without having to disclose their actual identity to the service providers. At the same time, the service providers have strong assurance about the authenticity of the identity and credentials. In our approach, users' identity is cryptographically encoded in pseudonymous identity tokens issued by trusted identity providers. Our system includes a lightweight policy language which enables users and service providers to express their requirements pertaining to pseudonymous identity verification and a suite of protocols based on zero-knowledge-proofs which enables the fulfillment of these requirements.
[Performance evaluation, transaction processing, linkability, Protocols, cryptographic protocols, data mining, authenticity, on-line transaction, Credit cards, lightweight policy language, cryptographically encoded identity, Electronic mail, Identity management systems, Privacy, protocol, pseudonymous identity verification, privacy protection, RahasNym, pseudonymous identity token, zero-knowledge-proof, data privacy, Cryptography, pseudonymous identity management system, digital identity ecosystem]
Piros: Pushing the Limits of Partially Concurrent Transmission in WiFi Networks
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Partially overlapped channels are barely used for concurrent transmission in WiFi networks, since they lead to collisions where the collided packets cannot be decoded successfully. In this paper, we observe that the actual corrupted symbols by partial-channel interference in OFDM-based WiFi networks are not as severe as we expected. There remains extra coding redundancy that can be exploited from the corrupted symbols, and utilized for packet recovery. Accordingly, we present a novel paradigm termed Piros, in order to Push the lImits of partially concurrent transmission in WiFi networks. Piros strategically leverages the coding redundancy according to the overlap portion in a distributed manner, and extracts useful decoding information from the corrupted symbols to decode the packet with partial-channel interference.
[channel coding, Bit error rate, Piros, Coding Redundancy, OFDM-based Wi-Fi network, packet recovery, radiofrequency interference, packet radio networks, coding redundancy, OFDM modulation, computer network reliability, redundancy, network coding, modulation coding, collided packet decoding, OFDM Modulation, Redundancy, Interference, Encoding, partial overlapped channel, Decoding, decoding, Partially Overlapped Channel, Symbol Recovery, IEEE 802.11g Standard, partial channel interference, push the limits of partially concurrent transmission, wireless LAN]
Structured Encryption with Non-interactive Updates and Parallel Traversal
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Searchable Symmetric Encryption (SSE) encrypts data in such a way that they can be searched efficiently. Some recent SSE schemes allow modification of data, yet they may incur storage overhead to support parallelism in searching, or additional computation to minimize the potential leakage incurred by the update, both penalize the performance. Moreover, most of them consider only keyword search and not applicable to arbitrary structured data. In this work, we propose the first parallel and dynamic symmetric-key structured encryption, which supports query of encrypted data structure. Our scheme leverages the rather simple randomized binary search tree to achieve non-interactive queries and updates.
[searchable symmetric encryption, non-interactive, randomized binary search tree, noninteractive update, Encryption, Complexity theory, potential leakage, Servers, parallel processing, query processing, Databases, encrypted data structure, parallel, SSE scheme, data structures, dynamic, structured encryption, parallel traversal, arbitrary structured data, noninteractive query, Keyword search, Binary search trees, cryptography, symmetric searchable encryption, parallel symmetric-key structured encryption, dynamic symmetric-key structured encryption]
Soft Quorums: A High Availability Solution for Service Oriented Stream Systems
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Large-scale information gathering becomes more and more common with the increasing popularity of smartphones, GPS, social networks, and sensor networks. Services based on this real-time data are the logical next step. Service Oriented Stream Systems (SOSS) have a focus on one-time ad hoc queries as op-posed to continuous queries. High availability is crucial in these services. However, data replication has inherent costs, which are particularly burdensome for high rate, often overloaded, SOSS. To provide high availability and to cope with the problem of over-loading the system, we propose a mechanism called soft quorums. Soft quorums incorporate a tuning knob that provides a trade off between query result accuracy and performance. Thus, in essence, soft quorums simultaneously offer high availability and per-query load shedding as needed. This is done in a system-wise optimal way. The parameter choices of soft quorums automatically adapt to dynamic data stream rates and query rates, and minimize the overall system load, given an accuracy requirement. We devise a recovery algorithm and study data quality after recovery. Finally, we conduct a comprehensive experimental study using two real-world and some synthetic datasets.
[Computers, service oriented stream system, one-time ad hoc queries, Web 2.0, soft quorums, service oriented stream systems, quorum, Tuning, Global Positioning System, query processing, dynamic data stream rates, high availability, Accuracy, Query processing, Real-time systems, SOSS, overall system load minimisation, service-oriented architecture, large-scale information gathering, per-query load shedding]
Improve Quality of Experience for Mobile Instant Video Clip Sharing
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
With the rapid development of mobile networking and end-terminals, anytime and anywhere data access becomes readily available nowadays. Given the crowd sourced content capturing and sharing, the preferred length becomes shorter and shorter, even for such multimedia content as video. A representative is Twitter's Vine service, which, mainly targeting mobile users, enables them to create ultra-short video clips, and instantly post and share them with their followers. In this paper, we present an initial study on this new generation of instant video clip sharing service enabled by mobile platforms and explore the potentials for its further enhancement. Taking Vine as a case study, we closely investigate its unique user behaviors, revealing how such Vine-enabled anytime anywhere data access patterns differentiate mobile instant video clip sharing from its traditional counterparts. We then formulate a generic scheduling problem to maximize the user watching experience as well as the efficiency on the monetary and energy costs. To better solve it, we divide the problem into two sub problems, specifically, the pre-fetching scheduling problem and the watch-time download scheduling problem, and conquer them separately. We further demonstrate the preliminary evaluation result to show the superiority of our solution. To the best of our knowledge, this is the first work on modeling and optimizing the instant video clip sharing on mobile devices.
[crowdsourced content capturing, crowdsourced content sharing, Watches, Mobile communication, mobile networking, Electronic mail, mobile platforms, Twitter Vine service, storage management, mobile computing, prefetching scheduling problem, energy costs, Bandwidth, scheduling, data access patterns, mobile instant video clip sharing service, video signal processing, watch-time download scheduling problem, Social network services, end-terminals, multimedia content, quality of experience, Streaming media, mobile devices, social networking (online), generic scheduling problem, Mobile computing]
Towards Planning the Transformation of Overlays
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Reconfiguring a topology is an important management technique to sustain high efficiency and robustness of an overlay. But, the problem of transforming the overlay from an old topology to a newly refined topology, at runtime, has received relatively little attention. The key challenge is to minimize the disruption that can be caused by topology transformation operations. Excessive disruption can be costly and harmful and thus it may hamper the decision to migrate to a better topology. To address this issue, we solve a problem of finding an appropriate sequence of steps to transform a topology that incurs the least service disruption. We refer to this problem as an incremental topology transformation (ITT) problem. The ITT problem can be formulated as an automated planning problem and can be solved with numerous off-the-shelf planning techniques. However, we found that state-of-the-art domain-independent planning techniques did not scale to solve large ITT problem instances. This shortcoming motivated us to develop a suite of planners that use novel domain-specific heuristics to guide the search for a solution. We empirically evaluated our planners on a wide range of topologies. Our results illustrate that our planners offer a viable solution to a diversity of ITT problems. We envision that our approach could eventually provide a compelling addition to the arsenal of techniques currently employed by the administrators of distributed overlay networks.
[topology reconfiguration, telecommunication network management, telecommunication network planning, distributed overlay networks, telecommunication network topology, transforms, Routing, Topology, off-the-shelf planning techniques, Jacobian matrices, disruption minimization, management technique, Overlay networks, Network topology, domain-specific heuristics, overlay networks, incremental topology transformation problem, Planning, overlay transformation, Reliability, automated planning problem, ITT problem]
Towards Scalable Publish/Subscribe Systems
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Despite suffering from inefficiency and flexibility limitations, the filter-based routing (FBR) algorithm is widely used in content-based publish/subscribe (pub/sub) systems. To address its limitations, we propose a dynamic destination-based routing algorithm called D-DBR, which decomposes pub/sub into two independent parts: Content-based matching and destination based multicasting. D-DBR exhibits low event matching cost and high efficiency, flexibility, and robustness for event routing in small-scale overlays. To improve its scalability to large-scale overlays, we further extend D-DBR to a new routing algorithm called MERC. MERC divides the overlay into interconnected clusters and applies content-based and destination-based mechanisms to route events inter- and intra-cluster, respectively. We implemented all algorithms in the PADRES pub/sub system. Experimental results show that our algorithms outperform the FBR algorithm.
[Heuristic algorithms, Subscriptions, Multicast communication, Event Processing, Engines, Clustering algorithms, D-DBR, content-based pub-sub system, content-based matching, middleware, FBR algorithm, topology, Routing, destination-based mechanisms, Content-based Publish/Subscribe, Publish/Subscribe, Topology, PADRES System, MERC, PADRES pub-sub system, destination-based multicasting, event matching cost, content-based publish-subscribe system, filter-based routing algorithm, dynamic destination-based routing algorithm, Content-based Routing]
The Reachability Query over Distributed Uncertain Graphs
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Reachability, one of the most fundamental queries over uncertain graphs, which asks the probability that two given query vertices are reachable over an uncertain graph. Although this problem has been widely studied, the existing works are all processed in a single server. However, as graph data becomes larger, it usually cannot be stored in a single server. Moreover, processing probabilistic reachability queries is #P-complete, so the calculation is very expensive even on small graphs. Thus, in this paper, our purpose is to develop efficient distributed strategies to firstly pick out all the maximal subgraphs whose reachability probabilities can be calculated in polynomial time efficiently. After this step, only a small graph remains, and we provide an approximate method. Extensive experimental studies show that our distributed algorithms are efficient and have a low communication cost.
[reachability analysis, Conferences, distributed uncertain graphs, graph theory, probability, graph data, query vertices, Probability, Master-slave, Servers, query processing, distributed algorithms, Distributed databases, maximal subgraphs, probabilistic reachability queries, Polynomials, polynomial time, Distributed algorithms, #P-complete]
Lazy Ctrl: Scalable Network Control for Cloud Data Centers
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
The advent of software defined networking enables flexible, reliable and feature-rich control planes for data center networks. However, the tight coupling of centralized control and complete visibility leads to a wide range of issues among which scalability has risen to prominence. We observe that data center traffic is usually highly skewed and thus edge switches can be grouped according to traffic locality. As a result, the workload of the central controller could be highly reduced if we carry out distributed control inside those groups. Based on the above observation, we present LazyCtrl, a novel hybrid control plane design for data center networks. LazyCtrl aims at bringing laziness to the central controller by dynamically devolving most of the control tasks to independent switch groups to process frequent intra-group events using distributed control mechanisms, while handling rare inter-group or other specified events by the controller. We implement LazyCtrl and build a prototype based on Open vSwich and Floodlight. Trace-driven experiments on our prototype show that an effective switch grouping is easy to maintain in multi-tenant clouds and the central controller can be significantly shielded by staying lazy, with its workload reduced by up to 82%.
[Algorithm design and analysis, Heuristic algorithms, distributed control, telecommunication control, computer networks, Open vSwich, Floodlight, computer centres, Centralized control, LazyCtrl, multitenant clouds, Decentralized control, Prototypes, Distributed databases, hybrid network control plane design, cloud computing, data center networks, central controller, cloud data centers]
Multi-tenant Latency Optimization in Erasure-Coded Storage with Differentiated Services
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
The effect of coding on content retrieval latency in data center storage system is drawing more and more significant attention these days, and customizing elastic service latency for the tenants is undoubtedly appealing to cloud storage, but it also comes with great technical challenges: due to the lack of analytic latency models for erasure-coded storage, most of the literature is limited to the analysis of average service latency, e.g., [1], [2], having assumptions like homogeneous files, exponential service time distribution [3], fixed erasure codes [4], which is unsuitable for a multi-tenant cloud environment where each tenant has a different latency requirement for accessing files in an erasure-coded, online cloud storage. Optimizing differentiated service delay in an erasure-coded storage system is an open problem. This work considers an erasure-coded storage with multiple tenants and differentiated delay demands, studies two types of service policies, non-preemptive priority queue and weighted queue, quantifying service latency of these policies, propose a novel optimization framework that provides differentiated service latency to meet heterogeneous application requirements in cloud storage.
[Algorithm design and analysis, Cloud computing, erasure-coded storage system, Encoding, fixed erasure code, content retrieval latency, computer centres, content-based retrieval, exponential service time distribution, Optimization, storage management, elastic service latency, differentiated service delay, multitenant latency optimization, Bandwidth, online cloud storage, Delays, cloud computing, data-center storage system, Queueing analysis, multitenant cloud environment]
A Self-Stabilizing Algorithm for CDS Construction with Constant Approximation in Wireless Networks under SINR Model
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
As a distributed system, a wireless network, usually faces a complex environment (transient faults and topology changes occur frequently). The connected dominating set (CDS) problem has been widely studied due to its important applications in wireless communication and networks, especially the important role as a virtual backbone for efficient routing. In this paper, under SINR (Signal-to-Interference-plus-Noise-Ratio) model, we propose a distributed self-stabilizing maximal independent set (MIS) algorithm (DSSMIS). Based on DSSMIS, we design a distributed self-stabilizing algorithm (DSSCDS) for CDS construction with constant approximation within O(log n) rounds. To best of our knowledge, this is the first self-stabilizing CDS algorithm under SINR model.
[Decision support systems, Algorithm design and analysis, DSS CDS algorithm, radio networks, self-stabilizing CDS algorithm, connected dominating set problem, Interference, distributed system, Approximation methods, Wireless networks, telecommunication network routing, DSS MIS algorithm, connected dominating set, distributed self-stabilizing MIS algorithm, Approximation algorithms, maximal independent set algorithm, signal-to-interference-plus-noise-ratio, wireless network, SINR, self-stabilizing, SINR model, Signal to noise ratio, wireless communication network routing]
Circular Range Search on Encrypted Spatial Data
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Searchable encryption is a promising technique enabling meaningful search operations to be performed on encrypted databases while protecting user privacy from untrusted third-party service providers. However, while most of the existing works focus on common SQL queries, geometric queries on encrypted spatial data have not been well studied. Especially, circular range search is an important type of geometric query on spatial data which has wide applications, such as proximity testing in Location-Based Services and Delaunay triangulation in computational geometry. In this poster, we propose two novel symmetric-key searchable encryption schemes supporting circular range search. Informally, both of our schemes can correctly verify whether a point is inside a circle on encrypted spatial data without revealing data privacy or query privacy to a semi-honest cloud server. We formally define the security of our proposed schemes, prove that they are secure under Selective Chosen-Plaintext Attacks, and evaluate their performance through experiments in a real-world cloud platform (Amazon EC2). To the best of our knowledge, this work represents the first study in secure circular range search on encrypted spatial data.
[Data privacy, Amazon EC2, query privacy, SQL query, Companies, computational geometry, encrypted spatial data, Encryption, Servers, geometric query, location-based service, query processing, user privacy protection, proximity testing, encrypted database, mesh generation, Delaunay triangulation, Spatial databases, SQL, symmetric-key searchable encryption scheme, semi-honest cloud server, circular range search, private key cryptography, data privacy, selective chosen-plaintext attack]
Inside Attack Filtering for Robust Sensor Localization
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Several solutions have recently been proposed to securely estimate sensor positions even when there is malicious location information which distorts the estimate. Some of those solutions are based on the Minimum Mean Square Estimation (MMSE) methods which efficiently estimate sensor positions. Although such solutions can filter out most of malicious information, if an attacker knows the position of a target sensor, the attacker can significantly alter the position information. In this paper, we introduce such a new attack, called Inside-Attack, and a technique that is able to detect and filter out malicious location information.
[telecommunication security, Filtering, wireless sensor networks, sensor position estimation, Estimation, robust sensor localization, Receivers, minimum mean square estimation methods, malicious location information, Global Positioning System, MMSE methods, Wireless sensor networks, Accuracy, inside attack filtering, sensor placement, Distortion measurement, mean square error methods]
Sparsity Exploiting Erasure Coding for Resilient Storage and Efficient I/O Access in Delta Based Versioning Systems
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
In this work, we study the problem of storing reliably an archive of versioned data. Specifically, we focus on systems where the differences (deltas) between subsequent versions rather than the whole objects are stored - a typical model for storing versioned data. For reliability, we propose erasure encoding techniques that exploit the sparsity of information in the deltas while storing them reliably in a distributed back-end storage system, resulting in improved I/O read performance to retrieve the whole versioned archive. Along with the basic techniques, we propose a few optimization heuristics, and evaluate the techniques' efficacy analytically and with numerical simulations.
[Conferences, information retrieval, reliability, Encoding, Generators, distributed back-end storage system, delta-based versioning system, information sparsity, Distributed Storage Systems, Fault tolerance, storage management, optimisation, Versioning Systems, Fault tolerant systems, optimization heuristics, Distributed databases, sparsity exploiting erasure coding, efficient I/O access, Erasure Codes, resilient storage, Fault Tolerance]
Shuffle to Baffle: Towards Scalable Protocols for Secure Multi-party Shuffling
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
In secure multi-party shuffling, multiple parties, each holding an input, want to agree on a random permutation of their inputs while keeping the permutation secret. This problem is important as a primitive in many privacy-preserving applications such as anonymous communication, location-based services, and electronic voting. Known techniques for solving this problem suffer from poor scalability, load-balancing issues, trusted party assumptions, and/or weak security guarantees. In this paper, we propose an unconditionally-secure protocol for multi-party shuffling that scales well with the number of parties and is load-balanced. In particular, we require each party to send only a polylogarithmic number of bits and perform a polylogarithmic number of operations while incurring only a logarithmic round complexity. We show security under universal compos ability against up to about n/3 fully-malicious parties. We also provide simulation results in the full version of this paper showing that our protocol improves significantly over previous work. For example, for one million parties, when compared to the state of the art, our protocol reduces the communication and computation costs by at least three orders of magnitude and slightly decreases the number of communication rounds.
[Protocols, cryptographic protocols, unconditionally-secure protocol, Secure Shuffling, anonymous communication, polylogarithmic number, Security, Mobile radio mobility management, random permutation, Privacy, resource allocation, logarithmic round complexity, location-based services, Electronic voting, trusted party assumptions, load-balancing, Privacy-Preserving Applications, Multi-Party Computation, privacy-preserving, Sorting, scalable protocols, secure multiparty shuffling, Logic gates, electronic voting, data privacy, permutation secret, computational complexity]
Privacy-Preserving Publication of Mobility Data with High Utility
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
An increasing amount of mobility data is being collected every day by different means, e.g., By mobile phone operators. This data is sometimes published after the application of simple anonymization techniques, which might lead to severe privacy threats. We propose in this paper a new solution whose novelty is two-fold. Firstly, we introduce an algorithm designed to hide places where a user stops during her journey (namely points of interest), by enforcing a constant speed along her trajectory. Secondly, we leverage places where users meet to take a chance to swap their trajectories and therefore confuse an attacker.
[Data privacy, data publication, trajectories swapping, Distortion, time distortion, privacy threats, Privacy, Accuracy, mobile computing, Publishing, Databases, location privacy, anonymization techniques, data privacy, mobile phone operators, Trajectory, mobile handsets, privacy-preserving mobility data publication]
TAFTA: A Truthful Auction Framework for User Data Allowance Trading in Mobile Networks
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
User data allowance trading is emerging as a promising field in mobile data networks. Mobile operators are establishing data trading platforms to attract more users. To date, there has been no coherent study on user data allowance trading. In this paper, we develop a truthful framework that allows users to bid for data allowance. We focus on preventing price cheating, guaranteeing fairness and minimizing trading maintenance cost. We model the data trading process as a double auction problem. We develop algorithms to solve the problem. The algorithms use a uniform price based on a competitive equilibrium to defend against price cheating and provide fairness, and use linear programming to minimize trading maintenance cost. We conduct extensive simulations to testify the proposed mechanism. Results show that our mechanism is truthful, fair and can minimize the cost of trading.
[Conferences, double auction problem, uniform price, Maintenance engineering, Mobile communication, mobile operators, linear programming, commerce, truthful auction framework for user data allowance trading, data trading platforms, mobile computing, Distributed databases, mobile data networks, Writing, TAFTA, Mobile computing, pricing]
[Publishers information]
2015 IEEE 35th International Conference on Distributed Computing Systems
None
2015
Provides a listing of current committee members and society officers.
[]
Message from the General Chairs and Program Chair
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Organizing Committee
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Provides a listing of current committee members and society officers.
[]
Program Committee
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Provides a listing of current committee members and society officers.
[]
Keynotes
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Provides an abstract for each of the keynote presentations and may include a brief professional biography of each
[]
Timely, Reliable, and Cost-Effective Internet Transport Service Using Dissemination Graphs
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Emerging applications such as remote manipulation and remote robotic surgery require communication that is both timely and reliable, but the Internet natively supports only communication that is either completely reliable with no timeliness guarantees (e.g. TCP) or timely with best-effort reliability (e.g. UDP). We present an overlay transport service that can provide highly reliable communication while meeting stringent timeliness guarantees (e.g. 130ms round-trip latency across the US) over the Internet. To enable routing schemes that can support the necessary timeliness and reliability, we introduce dissemination graphs, providing a unified framework for specifying routing schemes ranging from a single path, to multiple disjoint paths, to arbitrary graphs. We conduct an extensive analysis of real-world network data, finding that a routing approach using two disjoint paths performs well in most cases, and that cases where two disjoint paths do not perform well typically involve problems around a source or destination. Based on this analysis, we develop a timely dissemination-graph-based routing method that can add targeted redundancy in problematic areas of the network. This approach can cover over 99% of the performance gap between a traditional single-path approach and an optimal (but prohibitively expensive) scheme, while two dynamic disjoint paths cover about 70% of this gap, and two static disjoint paths cover about 45%. This performance improvement is obtained at a cost increase of about 2% over two disjoint paths.
[Node-disjoint paths, Protocols, data analysis, arbitrary graphs, routing schemes, graph theory, Reliability theory, network theory (graphs), dissemination graphs, Routing, Dissemination graphs, overlay transport service, Reliable messaging, Overlay networks, network data analysis, Low-latency messaging, Internet transport service, communication reliability, Forward error correction, disjoint paths, Internet]
Pronto: Efficient Test Packet Generation for Dynamic Network Data Planes
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Computer networks are becoming increasingly complex today and thus prone to various network faults. Traditional testing tools (e.g., ping, traceroute) that often involve substantial manual effort to uncover faults are inefficient. This paper focuses on fault detection of the network data plane using test packets. Existing solutions of test packet generation either take very long time (e.g., more than one hour) to complete or generate too many test packets that may hurt regular traffic. In this paper, we present Pronto, an automated test packet generation tool that generates test packets to exercise data plane rules in the entire network in a short time (e.g., several seconds) and can quickly react to rule changes due to network dynamics. In addition, Pronto minimizes the number of test packets by allowing a packet to test multiple rules at different switches. The performance evaluation using two real network data plane rule sets shows that Pronto is faster than a recently developed tool by more than two orders of magnitude. Pronto can update the probes for rule changes using less than 1ms while existing methods have no such update function.
[fault diagnosis, Pronto, Dynamic Network, Ports (Computers), automated test packet generation, Tools, computer networks faults, performance evaluation, computer network performance evaluation, Data Plane, Probing Packet, Bandwidth, dynamic network data planes fault detection, Software, Hardware, Probes, Testing]
Agar: A Caching System for Erasure-Coded Data
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Erasure coding is an established data protection mechanism. It provides high resiliency with low storage overhead, which makes it very attractive to storage systems developers. Unfortunately, when used in a distributed setting, erasure coding hampers a storage system's performance, because it requires clients to contact several, possibly remote sites to retrieve their data. This has hindered the adoption of erasure coding in practice, limiting its use to cold, archival data. Recent research showed that it is feasible to use erasure coding for hot data as well, thus opening new perspectives for improving erasure-coded storage systems. In this paper, we address the problem of minimizing access latency in erasure-coded storage. We propose Agar-a novel caching system tailored for erasure-coded content. Agar optimizes the contents of the cache based on live information regarding data popularity and access latency to different data storage sites. Our system adapts a dynamic programming algorithm to optimize the choice of data blocks that are cached, using an approach akin to "Knapsack" algorithms. We compare Agar to the classical Least Recently Used and Least Frequently Used cache eviction policies, while varying the amount of data cached between a data chunk and a whole replica of the object. We show that Agar can achieve 16% to 41% lower latency than systems that use classical caching policies.
[Algorithm design and analysis, least frequently used cache eviction policy, Heuristic algorithms, erasure coding, cache storage, data protection mechanism, Electronic mail, erasure-coded data, distributed storage systems, erasure-coded content, Agar system, caching, data popularity, dynamic programming algorithm, Distributed databases, Bandwidth, erasure-coded storage system, Redundancy, least recently used cache eviction policy, dynamic programming, Encoding, erasure-coding, caching system, knapsack algorithm, access latency]
High Performance Recovery for Parallel State Machine Replication
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
State machine replication is a fundamental approach to high availability. Despite the vast literature on the topic, relatively few studies have considered the issues involved in recovering faulty replicas. Recovering a replica requires (a) retrieving and installing an up-to-date replica checkpoint, and (b) restoring and re-executing the log of commands not reflected in the checkpoint. Parallel techniques to state machine replication render recovery particularly challenging since throughput under normal execution (i.e., in the absence of failures) is very high. Consequently, the log of commands that need to be applied until the replica is available is typically large, which delays recovery. In this paper, we present two techniques to optimize recovery in parallel state machine replication. The first technique allows new commands to execute concurrently with the execution of logged commands, before replicas are completely updated. The second technique introduces on-demand state recovery, which allows segments of a checkpoint to be recovered concurrently.
[checkpointing, Protocols, command log restoration, state machine replication, Throughput, high performance recovery, replica checkpoint installation, recovery, Servers, finite state machines, parallel processing, recovery optimization, Concurrent computing, parallel techniques, optimisation, Semantics, Prototypes, replica checkpoint retrieval, Delays, parallel state machine replication, high performance, replica recovery]
On Data Parallelism of Erasure Coding in Distributed Storage Systems
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Deployed in various distributed storage systems, erasure coding has demonstrated its advantages of low storage overhead and high failure tolerance. Typically in an erasure-coded distributed storage system, systematic maximum distance seperable (MDS) codes are chosen since the optimal storage overhead can be achieved and meanwhile data can be read directly without decoding operations. However, data parallelism of existing MDS codes is limited, because we can only read data from some specific servers in parallel without decoding operations. In this paper, we propose Carousel codes, designed to allow data to be read from an arbitrary number of servers in parallel without decoding, while preserving the optimal storage overhead of MDS codes. Furthermore, Carousel codes can achieve the optimal network traffic to reconstruct an unavailable block. We have implemented a prototype of Carousel codes on Apache Hadoop. Our experimental results have demonstrated that Carousel codes can make MapReduce jobs finish with almost 50% less time and reduce data access latency significantly, with a comparable throughput in the encoding and decoding operations and no additional sacrifice of failure tolerance or the network overhead to reconstruct unavailable data.
[codes, distributed storage, maximum distance separable codes, Throughput, erasure coding, Encoding, Decoding, distributed storage systems, Servers, parallel processing, MDS codes, Apache Hadoop, MapReduce, Reed-Solomon codes, minimum-storage regenerating codes, storage management, Systematics, data parallelism, Distributed databases, Parallel processing, Carousel codes, data handling]
MeteorShower: Minimizing Request Latency for Majority Quorum-Based Data Consistency Algorithms in Multiple Data Centers
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
With the increasing popularity of serving and storing data in multiple data centers, we investigate the efficiency of majority quorum-based data consistency algorithms under this scenario. Because of the failure-prone nature of distributed storage systems, majority quorum-based data consistency algorithms become one of the most widely adopted approaches. In this paper, we propose the MeteorShower framework, which provides fault-tolerant read/write key-value storage service across multiple data centers with sequential consistency guarantees. A major feature is that most read operations are executed locally within a single data center. This results in lowering read latency from hundreds of milliseconds to tens of milliseconds. The data consistency algorithm in MeteorShower augments majority quorum-based algorithms. Thus, it keeps all the desirable properties of majority quorums, such as fault tolerance, balanced load, etc. An implementation of MeteorShower on top of Cassandra is deployed and evaluated in multiple data centers using the Google Cloud Platform. Evaluations of MeteorShower framework have shown that it can consistently serve read requests without paying the communication delays among replicas maintained in multiple data centers. As a result, we are able to improve the latency of read requests from hundreds of milliseconds to tens of milliseconds while achieving the same latency on write requests and the same fault tolerance guarantee. Thus, MeteorShower is optimized for read intensive workloads.
[Algorithm design and analysis, MeteorShower framework, fault-tolerant read/write key-value storage service, Cassandra, Servers, Distributed time, majority quorum-based data consistency algorithms, Fault tolerance, Synchronized clocks, Data consistency, Distributed databases, sequential consistency guarantees, cloud computing, Majority quorum, NoSQL databases, data integrity, Synchronization, computer centres, Google Cloud platform, data center, Data models, fault tolerant computing, request latency minimization, Geo-distributed storage systems, Clocks]
LSbM-tree: Re-Enabling Buffer Caching in Data Management for Mixed Reads and Writes
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
LSM-tree has been widely used in data management production systems for write-intensive workloads. However, as read and write workloads co-exist under LSM-tree, data accesses can experience long latency and low throughput due to the interferences to buffer caching from the compaction, a major and frequent operation in LSM-tree. After a compaction, the existing data blocks are reorganized and written to other locations on disks. As a result, the related data blocks that have been loaded in the buffer cache are invalidated since their referencing addresses are changed, causing serious performance degradations. In order to re-enable high-speed buffer caching during intensive writes, we propose Log-Structured buffered-Merge tree (simplified as LSbM-tree) by adding a compaction buffer on disks, to minimize the cache invalidations on buffer cache caused by compactions. The compaction buffer efficiently and adaptively maintains the frequently visited data sets. In LSbM, strong locality objects can be effectively kept in the buffer cache with minimum or without harmful invalidations. With the help of a small on-disk compaction buffer, LSbM achieves a high query performance by enabling effective buffer caching, while retaining all the merits of LSM-tree for write-intensive data processing, and providing high bandwidth of disks for range queries. We have implemented LSbM based on LevelDB. We show that with a standard buffer cache and a hard disk, LSbM can achieve 2x performance improvement over LevelDB. We have also compared LSbM with other existing solutions to show its strong effectiveness.
[high-speed buffer caching, Buffer storage, buffer cache, Random access memory, query performance, Throughput, cache storage, Compaction, Servers, intensive write, data management production system, disk location, query processing, LSbM-tree, Key-Value store, tree data structures, referencing address, write-intensive workload, Google, on-disk compaction buffer, trees (mathematics), range queries, performance degradation, write-intensive data processing, cache invalidation minimization, LSM-tree, data access, Hard disks, data handling, read-and-write workload, log-structured buffered-merge tree, frequently visited data set, data blocks reorganization, strong locality objects]
Incremental Topology Transformation for Publish/Subscribe Systems Using Integer Programming
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Distributed overlay-based publish/subscribe systems provide a selective, scalable, and decentralized approach to data dissemination. Due to the dynamic communication flows between data producers and consumers, the overlay topology of such systems can become inefficient over time and therefore requires adaptation to the existing load. Existing studies propose algorithms to design overlay topologies which are optimized for specific workloads. However, the problem of generating a plan to incrementally transform the current topology to an optimized one has been largely ignored. In this paper, we present IPITT, an approach based on integer programming for the incremental topology transformation (ITT) problem. Given the current topology and a target topology, IPITT generates a transformation plan with a minimal number of steps in order to lessen service disruption. Furthermore, we introduce a plan execution mechanism and evaluate our approach on an existing publish/subscribe system. Based on our evaluation, IPITT can reduce plan computation time by a factor of 10 and generates plans with an execution time up to 55% shorter than those of existing approaches.
[data producers, data consumers, integer programming, dynamic communication flow, incremental topology transformation, Network topology, overlay topology design, IP networks, plan execution mechanism, distributed overlay-based publish/subscribe system, middleware, Client-server systems, telecommunication network topology, IPITT, Routing, Linear programming, data dissemination, Topology, Middleware, service disruption, overlay networks, Publish-subscribe, Integer linear programming, data communication, Planning, data handling, Reliability]
milliScope: A Fine-Grained Monitoring Framework for Performance Debugging of n-Tier Web Services
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Modern distributed systems are often considered to be black boxes that greatly limit the potential to understand behaviors at the level of detail necessary to diagnose some of the most important types of performance problems. Recently researchers have found abnormal response time delays, one to two orders of magnitude longer than the average response time, that exist in short periods and cause economic loss for service providers. These very short bottlenecks are hard to detect due to their short life spans and their variety of possible reasons. In this paper, we propose milliScope (mScope), the first millisecond-granularity software-based resource and event monitoring for distributed systems that achieves both performance, low overhead at high frequency, and high accuracy matched with other firmware monitoring tool. More specifically, milliScope is a fine-grained monitoring framework to collaborate multiple mScopeMonitors for event and resource monitoring to reconstruct the flow of each client request and profile execution performance in a distributed system. We utilize the resource mScopeMonitors for system resource monitoring, and we develop our own event mScopeMonitors to identify the execution boundary in a lightweight, precise and systematic methodology. The semantic and syntactic of these monitoring logs with arbitrary formats are enriched by our multistage data transformation tool, mScopeDataTransformer, which unifies the diverse monitoring logs into a dynamic data warehouse, mScopeDB, for advanced analysis. We conduct several illustrative scenarios in which milliScope successfully diagnoses the response time anomalies caused by very short bottlenecks using a representative web application benchmark (RUBBoS).
[program debugging, dynamic data warehouse, fine-grained monitoring framework, response time anomalies, distributed processing, performance debugging, mScopeMonitors, Frequency measurement, Servers, millisecond-granularity software-based resource, Semantics, firmware monitoring tool, distributed systems, Monitoring, mScopeDB, milliScope, Tools, Data warehouses, system resource monitoring, multistage data transformation tool, Web services, representative Web application benchmark, event monitoring, system monitoring, profile execution performance, Time factors, data warehouses, RUBBoS, mScopeDataTransformer, n-tier Web services]
Stark: Optimizing In-Memory Computing for Dynamic Dataset Collections
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Emerging distributed in-memory computing frameworks, such as Apache Spark, can process a huge amount of cached data within seconds. This remarkably high efficiency requires the system to well balance data across tasks and ensure data locality. However, it is challenging to satisfy these requirements for applications that operate on a collection of dynamically loaded and evicted datasets. The dynamics may lead to time-varying data volume and distribution, which would frequently invoke expensive data re-partition and transfer operations, resulting in high overhead and large delay. To address this problem, we present Stark, a system specifically designed for optimizing in-memory computing on dynamic dataset collections. Stark enforces data locality for transformations spanning multiple datasets (e.g., join and cogroup) to avoid unnecessary data replications and shuffles. Moreover, to accommodate fluctuating data volume and skewed data distribution, Stark delivers elasticity into partitions to balance task execution time and reduce job makespan. Finally, Stark achieves bounded failure recovery latency by optimizing the data checkpointing strategy. Evaluations on a 50-server cluster show that Stark reduces the job makespan by 4X and improves system throughput by 6X compared to Spark.
[dynamic dataset collections, data repartition, task execution time, data analysis, transfer operations, data checkpointing strategy, Elasticity, distributed processing, cached data, Sparks, Servers, skewed data distribution, Distributed databases, time-varying data volume, data replications, Computer architecture, distributed in-memory computing frameworks, Delays, Stark, Bars]
CRESON: Callable and Replicated Shared Objects over NoSQL
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In a Cloud environment, the ability to share and persist objects simplifies the design of applications. Storing objects in a NoSQL database ensures their availability and provides scalability to applications. When Object-NoSQL Mapping is performed at the client side, objects that are accessed by several clients are repeatedly converted between their in-memory and serialized representations. This negatively impacts performance and increases replication costs. In this paper, we describe the design of CRESON, a system supporting callable objects over NoSQL, in which application objects are mapped and instantiated directly on the storage nodes. CRESON supports composition by reference and ensures strong consistency. Objects are replicated and maintained coherent using State Machine Replication. The implementation of CRESON leverages the support of a listenable key-value store (LKVS), a novel NoSQL storage abstraction that we introduce in this paper. We discuss the performance and complexity of CRESON with the example of the portage of a personal cloud storage service, initially developed using an object-relational mapping over a sharded PostgreSQL database. Our results show that CRESON offers a simpler programming experience both in terms of learning time and lines of code, while performing better on average and being more scalable.
[Cloud computing, Java, replicated databases, NoSQL databases, Scalability, CRESON, callable shared objects, state machine replication, Elasticity, replicated shared objects, sharded PostgreSQL database, finite state machines, NoSQL database, object-NoSQL, personal cloud storage service, object-relational mapping, listenable key-value store, Impedance, cloud computing, storage nodes, LKVS]
Consensus Robustness and Transaction De-Anonymization in the Ripple Currency Exchange System
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Distributed financial systems are radically changing the way we do business and spend our money. Ripple, in particular, is unique in its kind. It is built on consensus and trust among its users and it allows to exchange both fiat currencies and goods over its network. It does so by storing the accounts of its users, their balances, and all the transactions in a distributed ledger, publicly accessible. In this paper we perform an in-depth study of the Ripple exchange system and its public distributed ledger. We analyze payments, the structure of payment paths, and the role of the entities in the system such as Gateways (the equivalent of banks) and Market Makers. We also analyze the internal stream of events and show that Ripple relies on a surprisingly small number of active validators, raising concerns on the actual robustness and fairness of the system. Moreover, we consider the degree of anonymity that Ripple is able to guarantee. By examining the first three years of Ripple history (more than 500 GB worth of data), we show that even approximate information on a single payment can uncover, with incredible accuracy, the entire financial life of the user. For example, anyone who overhears our order of a Latte at our favourite bar can easily get complete and unlimited access to our balance, our previous and future payments, our monthly income, as well as critical information about the places where we shop and the people we trust.
[payment path structure, Protocols, transaction de-anonymization, distributed financial systems, Ripple currency exchange system, Bitcoin, Banking, payments, distributed processing, privacy, consensus., credit networks, transactions, market makers, Privacy, distributed ledger, Logic gates, Ripple, distributed systems, Robustness, anonymity, foreign exchange trading, consensus robustness]
Learning Privacy Habits of PDS Owners
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The concept of Personal Data Storage (PDS) has recently emerged as an alternative and innovative way of managing personal data w.r.t. the service-centric one commonly used today. The PDS offers a unique logical repository, allowing individuals to collect, store, and give access to their data to third parties. The research on PDS has so far mainly focused on the enforcement mechanisms, that is, on how user privacy preferences can be enforced. In contrast, the fundamental issue of preference specification has been so far not deeply investigated. In this paper, we do a step in this direction by proposing different learning algorithms that allow a fine-grained learning of the privacy aptitudes of PDS owners. The learned models are then used to answer third party access requests. The extensive experiments we have performed show the effectiveness of the proposed approach.
[Data privacy, Correlation, user privacy preferences, Tools, personal data storage, Proposals, data collection, Training, Privacy, storage management, personal data management, privacy habit learning, PDS owners, Computer architecture, data access, data privacy, enforcement mechanisms, logical repository, learning (artificial intelligence)]
City-Hunter: Hunting Smartphones in Urban Areas
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The security issue of public WiFi is gaining more and more concern. By listening to probe requests, an adversary can obtain the SSID list of the APs to which a smartphone previously connected, and utilizes this information to trick the smartphone into associating to it. However, with the enhancement of security level, most smartphones now do not proactively disclose their SSID lists, making these attacks obsolete. In this paper, we propose City-Hunter, an attacker that can lure nearby smartphones without knowing their SSID information. City-Hunter establishes and maintains an SSID database by integrating both offline and online information. Meanwhile, it smartly chooses some SSIDs to hit a smartphone according to the past record and freshness. We evaluate the performance of City-Hunter in different public places. The results demonstrate that City-Hunter is able to successfully hit 12% ~ 18% smartphones without knowing their SSID information, which is about 4 ~ 8 times improvement compared to the similar attacks like KARMA and MANA.
[SSID database, WiFi security, Mobile communication, smart phones, urban areas, Security, computer network security, Databases, public administration, SSID list, city-hunter, smartphones hunting, wireless LAN, Probes, Wireless fidelity, Smart phones]
When Seeing Isn't Believing: On Feasibility and Detectability of Scapegoating in Network Tomography
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Network tomography is a vital tool to estimate link qualities from end-to-end network measurements. An implicit assumption in network tomography is that observed measurements indeed reflect the aggregate of link performance (i.e., seeing is believing). However, it is not guaranteed today that there exists no anomaly (e.g., malicious autonomous systems and insider threats) in large-scale networks. Malicious nodes can intentionally manipulate link metrics via delaying or dropping packets to affect measurements. Will such an assumption render a vulnerability when facing attackers? The problem is of essential importance in that network tomography is developed towards effective network diagnostics and failure recovery. In this paper, we demonstrate that the vulnerability is real and propose a new attack strategy, called scapegoating, in which malicious nodes can substantially damage a network (e.g., delaying packets) and at the same time maliciously manipulate end-to-end measurement results such that a legitimate node is misleadingly identified as the root cause of the damage (thereby becoming a scapegoat) under network tomography. We formulate three basic scapegoating approaches and show under what conditions attacks can be successful. We also reveal conditions to detect such attacks. Our theoretical and experimental results show that simply trusting measurements leads to scapegoating vulnerabilities. Thus, existing methods should be revisited accordingly for security in various applications.
[scapegoating vulnerabilities, large-scale networks, attack feasibility, failure recovery, trusted measurements, Loss measurement, Security, scapegoating detectability, effective network diagnostics, security, end-to-end network measurements, legitimate node, scapegoating, end-to-end measurement, Tomography, attack detection, link performance, computer network reliability, insider threats, Network tomography, Monitoring, malicious autonomous systems, link quality estimation, delaying packets, Routing, computer network security, network tomography, dropping packets, Delays]
You Can Hear But You Cannot Steal: Defending Against Voice Impersonation Attacks on Smartphones
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Voice, as a convenient and efficient way of information delivery, has a significant advantage over the conventional keyboard-based input methods, especially on small mobile devices such as smartphones and smartwatches. However, the human voice could often be exposed to the public, which allows an attacker to quickly collect sound samples of targeted victims and further launch voice impersonation attacks to spoof those voice-based applications. In this paper, we propose the design and implementation of a robust software-only voice impersonation defense system, which is tailored for mobile platforms and can be easily integrated with existing off-the-shelf smart devices. In our system, we explore magnetic field emitted from loudspeakers as the essential characteristic for detecting machine-based voice impersonation attacks. Furthermore, we use a state-of-the-art automatic speaker verification system to defend against human imitation attacks. Finally, our evaluation results show that our system achieves simultaneously high accuracy (100%) and low equal error rates (EERs) (0%) in detecting the machine-based voice impersonation attack on smartphones.
[magnetic field, magnetic fields, voice impersonation attacks, software-only voice impersonation defense system, Mobile communication, smart phones, Mobile applications, Defense system, loudspeakers, off-the-shelf smart devices, Impersonation Attacks, mobile platforms, Loudspeakers, mobile computing, automatic speaker verification system, Authentication, Speech, Robustness, Voice, Smartphone, Smart phones, speaker recognition]
Flow Reconnaissance via Timing Attacks on SDN Switches
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
When encountering a packet for which it has no matching forwarding rule, a software-defined networking (SDN) switch requests an appropriate rule from its controller; this request delays the routing of the flow until the controller responds. We show that this delay gives rise to a timing side channel in which an attacker can test for the recent occurrence of a target flow by judiciously probing the switch with forged flows and using the delays they encounter to discern whether covering rules were previously installed in the switch. We develop a Markov model of an SDN switch to permit the attacker to select the best probe (or probes) to infer whether a target flow has recently occurred. Our model captures practical challenges related to rule evictions to make room for other rules; rule timeouts due to inactivity; the presence of multiple rules that apply to overlapping sets of flows; and rule priorities. We show that our model enables detection of target flows with considerable accuracy in many cases.
[timing side channel, software-defined networking, matching forwarding rule, software defined networking, Control systems, Markov model, computer network security, flow reconnaissance, SDN switches, Reconnaissance, Markov processes, Delays, timing attacks, target flow, Probes]
A Study of Long-Tail Latency in n-Tier Systems: RPC vs. Asynchronous Invocations
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Long-tail latency of web-facing applications continues to be a serious problem. Most of the previously published research addresses two classes of long latency problems: uneven workloads such as web search, and resource saturation in single nodes. We describe an experimental study of a third class of long tail latency problems that are specific to distributed systems: Cross-Tier Queue Overflow (CTQO) due to a combination of millibottlenecks (with sub-second duration) and tightly-coupled servers in n-tier systems (e.g., Apache, Tomcat, and MySQL) using RPC-style request-response communications. Our experiments show that the appearance of millibottlenecks (e.g., created by short workload bursts) in one server often causes another server (which has no saturated resources) in the synchronous invocation chain to fill up its queues (CTQO) and drop packets, creating very long response time queries. CTQO can be reduced or avoided by replacing the server dropping packets with an asynchronous server. In synchronous n-tier system experiments, long tail latency due to CTQO can be reproduced consistently at utilization as low as 43%. In contrast, when all n-tier servers are replaced by asynchronous versions, CTQO and consequent dropped packets remain absent at utilization levels as high as 83%, despite the same millibottlenecks.
[request-response communications, RPC, long-tail latency, asynchronous invocations, cross-tier queue overflow, Throughput, server dropping packets, Servers, n-tier systems, response time queries, query processing, CTQO, Asynchronous communication, millibottlenecks, remote procedure calls, distributed systems, Software, Delays, Internet, Time factors, Web search, Web-facing applications]
Rain or Shine? &#x2014; Making Sense of Cloudy Reliability Data
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Cloud datacenters must ensure high availability for the hosted applications and failures can be the bane of datacenter operators. Understanding the what, when and why of failures can help tremendously to mitigate their occurrence and impact. Failures can, however, depend on numerous spatial and temporal factors spanning hardware, workloads, support facilities, and even the environment. One has to rely on failure data from the field to quantify the influence of these factors on failures. Towards this goal, we collect failures data along with many parameters that might influence failures from two large production datacenters with very diverse characteristics. We show that multiple factors simultaneously affect failures, and these factors may interact in non-trivial ways. This makes conventional approaches that study aggregate characteristics or single parameter influences, rather inaccurate. Instead, we build a multi-factor analysis framework to systematically identify influencing factors, quantify their relative impact, and help in more accurate decision making for failure mitigation. We demonstrate this approach for three important decisions: spare capacity provisioning, comparing the reliability of hardware for vendor selection, and quantifying flexibility in datacenter climate control for cost-reliability trade-offs.
[Cloud computing, Temperature, Cooling, cloudy reliability data, reliability, datacenter climate control, costing, failure data, cloud datacenters, Servers, computer centres, system recovery, measurement, multifactor analysis, datacenters, Production, failure mitigation, cost reliability, Hardware, Reliability, climate mitigation, cloud computing]
Right-Sizing Geo-distributed Data Centers for Availability and Latency
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
We show cloud developers how to right size data center (DC) capacity for geo-distributed applications deployed on several multi-megawatt DCs, possibly also using many smaller edge DCs. Note that capacity considerations for a geo-distributed infrastructure do not decompose into individual DC capacity planning. When edge DCs are used, heterogeneous availability and costs affect the capacity split between the edge and core DCs. Non-uniform spatial distribution of clients and interdependence between latency and availability constraints make it non-trivial to provision the right capacity at each DC. We develop a geo-distributed capacity planning framework to capture the key factors that influence capacity, ranging from application demand patterns, latency and availability requirements, DC cost-availability trade-offs, and data replication overheads. We apply our framework to a realistic application and DC infrastructure setting to gather insights into how capacity should be provisioned and allocated across DCs for a representative set of requirements and costs.
[Cloud computing, latency constraint, geo-distributed infrastructure, DC capacity planning, Servers, Optimization, application demand pattern, join latency and availability SLA, resource allocation, capacity planning, Bandwidth, cloud computing, geo-distributed data center right-sizing, client nonuniform spatial distribution, Cooling, Biological system modeling, geo-distributed application, availability constraint, cloud development, computer centres, heterogeneous availability, datacenters, geo-distributed capacity planning framework, data replication overhead, availability requirement, geo-distribution, data center capacity, Capacity planning]
Performance Driven Resource Sharing Markets for the Small Cloud
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Small-scale clouds (SCs) often suffer from resource under-provisioning during peak demand, leading to inability to satisfy service level agreements (SLAs) and consequent loss of customers. One approach to address this problem is for a set of autonomous SCs to share resources among themselves in a cost-induced cooperative fashion, thereby increasing their individual capacities (when needed) without having to significantly invest in more resources. In this context, a central problem is how to properly share resources for a price in order to achieve profitable service, while maintaining customer SLAs. To address this problem, we propose the SC-Share framework that utilizes two interacting models: (i) a stochastic performance model that estimates the achieved performance characteristics under given SLA requirements, and (ii) a market-based game-theoretic model that (as shown empirically) converges to efficient resource sharing decisions at market equilibrium. Our results include extensive evaluations that illustrate the utility of the proposed framework.
[Measurement, Cloud computing, service level agreements, market-based game-theoretic model, Computational modeling, Biological system modeling, data centers, game theory, SC-share framework, SLA, small-scale clouds, Complexity theory, contracts, small cloud, markets, performance, performance driven resource sharing markets, Silicon, Resource management, cloud computing]
Fault-Scalable Virtualized Infrastructure Management
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Large-scale virtualized datacenters require considerable automation in infrastructure management in order to operate efficiently. Automation is impaired, however, by the fact that deployments are prone to multiple types of subtle faults due to hardware failures, software bugs, misconfiguration, crashes, performance degraded hardware, etc. Existing Infrastructure-as-a-Service (IaaS) management stacks incorporate little to no resilience measures to shield end users from such cloud providerlevel failures and poor performance. This paper proposes and evaluates extensions to IaaS stacks that mask faults in a fault-agnostic manner while ensuring that the overheads can be proportional to observed failure rates. We also demonstrate that infrastructure automation services and end-user applications can use service-specific knowledge, together with our new interface, to achieve better outcomes.
[Cloud computing, fault-scalable virtualized infrastructure management, infrastructure automation service, virtualisation, availability, Servers, large-scale virtualized datacenters, system recovery, crash, misconfiguration, performance degraded hardware, Fault tolerance, Fault tolerant systems, Hardware, cloud computing, Automation, software bugs, deployment faults, IaaS management stacks, resilience measure, computer centres, cloud provider level failure, infrastructure-as-a-service, virtual machines, end-user application, fault tolerant computing, hardware failure, service-specific knowledge]
DeltaCFS: Boosting Delta Sync for Cloud Storage Services by Learning from NFS
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Cloud storage services, such as Dropbox, iCloud Drive, Google Drive, and Microsoft OneDrive, have greatly facilitated users' synchronizing files across heterogeneous devices. Among them, Dropbox-like services are particularly beneficial owing to the delta sync functionality that strives towards greater network-level efficiency. However, when delta sync trades computation overhead for network-traffic saving, the tradeoff could be highly unfavorable under some typical workloads. We refer to this problem as the abuse of delta sync. To address this problem, we propose DeltaCFS, a novel file sync framework for cloud storage services by learning from the design of conventional NFS (Network File System). Specifically, we combine delta sync with NFS-like file RPC in an adaptive manner, thus significantly cutting computation overhead on both the client and server sides while preserving the network-level efficiency. DeltaCFS also enables a neat design for guaranteeing causal consistency and fine-grained version control of files. In our FUSE-based prototype system (which is open-source), DeltaCFS outperforms Dropbox by generating up to 11x less data transfer and up to 100x less computation overhead under concerned workloads.
[Cloud computing, Fuses, network file system, RPC, FUSE-based prototype system, Mobile communication, cloud storage services, Encoding, Synchronization, Servers, Noise measurement, network-level efficiency, storage management, file sync framework, delta encoding, delta sync functionality, DeltaCFS, cloud storage, NFS, user synchronizing files, cloud computing, learning (artificial intelligence), Dropbox, data synchronization]
Cachier: Edge-Caching for Recognition Applications
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Recognition and perception based mobile applications, such as image recognition, are on the rise. These applications recognize the user's surroundings and augment it with information and/or media. These applications are latency-sensitive. They have a soft-realtime nature - late results are potentially meaningless. On the one hand, given the compute-intensive nature of the tasks performed by such applications, execution is typically offloaded to the cloud. On the other hand, offloading such applications to the cloud incurs network latency, which can increase the user-perceived latency. Consequently, edge computing has been proposed to let devices offload intensive tasks to edge servers instead of the cloud, to reduce latency. In this paper, we propose a different model for using edge servers. We propose to use the edge as a specialized cache for recognition applications and formulate the expected latency for such a cache. We show that using an edge server like a typical web cache, for recognition applications, can lead to higher latencies. We propose Cachier, a system that uses the caching model along with novel optimizations to minimize latency by adaptively balancing load between the edge and the cloud, by leveraging spatiotemporal locality of requests, using offline analysis of applications, and online estimates of network conditions. We evaluate Cachier for image-recognition applications and show that our techniques yield 3x speedup in responsiveness, and perform accurately over a range of operating conditions. To the best of our knowledge, this is the first work that models edge servers as caches for compute-intensive recognition applications, and Cachier is the first system that uses this model to minimize latency for these applications.
[cache latency, Cloud computing, cache, load balancing, Image edge detection, Computational modeling, edge computing, mobile, Cachier system, cache storage, Servers, edge caching, edge servers, caching model, Edge computing, mobile computing, resource allocation, file servers, mobile applications, Feature extraction, minimisation, latency minimization, image recognition, optimizations]
Content Centric Peer Data Sharing in Pervasive Edge Computing Environments
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The proliferation and daily congregation of modern mobile devices have created abundant opportunities for peer edge devices to share valuable data with each other. The short contact durations, relatively small sharing sizes, and uncertain data availability, demand agile, light weight peer based data sharing. In this paper, we propose Peer Data Sharing (PDS) that enables edge devices to discover which data exist in nearby peers, and retrieve interested data robustly and efficiently. PDS uses novel lingering queries, mixedcast and en-route message rewriting techniques to minimize redundant transmissions and maximize opportunistic overhearing thus caching in data discovery and retrieval. Extensive evaluations based on an Android prototype show that PDS discovers and retrieves almost 100% data in tens of seconds, and remains robust despite wireless contention, simultaneous consumer requests and user mobility.
[content centric PDS, data retrieval, Data Retrieval, Metadata, mixedcast, Mobile handsets, cache storage, caching, Wireless communication, query processing, peer edge devices, mobile computing, content centric peer data sharing, data discovery, Robustness, Mobile Sensing, peer-to-peer computing, enroute message rewriting, Data Discovery, Receivers, Ad hoc networks, queries, Peer Data Sharing, mobile devices, pervasive edge computing, Peer-to-peer computing, Internet]
FLARE: Coordinated Rate Adaptation for HTTP Adaptive Streaming in Cellular Networks
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Fog computing is an emerging architecture that aims to run applications on multiple devices that lie on a continuum from cloud servers to personal user smartphones. These architectures allow applications to optimize over the information stored at and functionalities run on each device, based on individual device capabilities. We demonstrate the benefits of this approach for mobile video streaming. Existing HAS (HTTP adaptive streaming) techniques often suffer from problems like unstable video quality and suboptimal resource utilization. We find that a lack of coordination prevents both clientand network-side HAS techniques from solving them. However, our fog approach can exploit existing telecommunication APIs, which expose network capabilities to applications, in order to coordinate between clients and the network. Our coordinated HAS solution, FLARE, optimizes the total utility of all clients in a cell while maintaining stable video quality and supporting user- and device-specific needs. We implement FLARE on a commodity LTE femtocell and use the implementation to conduct the first comparison of HAS players on an LTE femtocell. By conducting extensive experiments using the ns-3 simulator, we also demonstrate that FLARE (i) enhances the average video bitrate, (ii) achieves stable video quality, and (iii) balances the throughput of simultaneous video and data flows, compared to other representative HAS solutions.
[Fog computing, data flows, MPEG DASH, Servers, HTTP adaptive streaming, suboptimal resource utilization, FLARE, Bit rate, Computer architecture, Bandwidth, cloud servers, video streaming, cellular networks, Long Term Evolution, coordinated rate adaptation, device-specific needs, video quality, Video recording, network-side HAS techniques, commodity LTE femtocell, individual device capabilities, 5G, transport protocols, mobile video streaming, femtocellular radio, Streaming media, Quality assessment, telecommunication API, LTE, user-specific needs]
Networked Drone Cameras for Sports Streaming
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
A network of drone cameras can be deployed to cover live events, such as high-action sports game played on a large field, but managing networked drone cameras in real-time is challenging. Distributed approaches yield suboptimal solutions from lack of coordination but coordination with a centralized controller incurs round-trip latencies of several hundreds of milliseconds over a wireless channel. We propose a fog-networking based system architecture to automatically coordinate a network of drones equipped with cameras to capture and broadcast the dynamically changing scenes of interest in a sports game. We design both optimal and practical algorithms to balance the tradeoff between two metrics: coverage of the most important scenes and streamed video bitrate. To compensate for network round-trip latencies, the centralized controller uses a predictive approach to predict which locations the drones should cover next. The controller maximizes video bitrate by associating each drone to an optimally matched server and dynamically re-assigns drones as relay nodes to boost the throughput in low-throughput scenarios. This dynamic assignment at centralized controller occurs at slower time-scale permitted by round-trip latencies, while the predictive approach and drones' local decision ensures that the system works in real-time. Experimental results over tens of flights on the field suggest our system can achieve really good performance, for example, 8 drones can achieve a tradeoff of 94% coverage and (on average) 2K video support at 20 Mbps by optimizing between coverage and throughput. By dynamically allocating drones to cover the game or act as relays, our system also demonstrates a 2x gain over systems maximizing static coverage alone that achieves only 9 Mbps video throughput.
[fog-networking based system architecture, centralised control, predictive controller, Throughput, Servers, Relays, bit rate 9 Mbit/s, cameras, centralized controller, video streaming, wireless channel, sports game, round-trip latency, image matching, bit rate 20 Mbit/s, distributed approach, autonomous aerial vehicles, video bit rate streaming, sport streaming, Games, Streaming media, Cameras, predictive control, networked drone camera, Drones]
Chronus: Consistent Data Plane Updates in Timed SDNs
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Software-Defined Networks (SDNs) introduce interesting new opportunities in how network routes can be defined, verified, and changed over time. Yet despite the logically-centralized perspective offered, an SDN still needs to be considered a distributed system: rule updates communicated from the controller to the individual switches traverse an asynchronous network and may arrive out-of-order, and hence lead to (temporary or permanent) inconsistencies. Accordingly, the consistent network update problem has recently received much attention. Motivated by the advent of tightly synchronized SDNs, we in this paper initiate the study of algorithms for consistent network updates in &#x201C;timed SDNs&#x201D;-SDNs in which individual node updates can be scheduled at specific times. This paper presents Chronus, which is based on provably congestion- and loop-free update scheduling algorithms, and avoids the flow table space headroom required by existing two-phase update approaches. We formulate the Minimum Update Time Problem (MUTP) as an optimization program. We propose a tree algorithm to check the feasibility and a greedy algorithm to find a update sequence in polynomial time. Extensive experiments on Mininet and numerical simulations show that Chronus can substantially reduce transient congestion by 75% and save over 60% of the rules compared to the state of the art.
[MUTP, transient congestion reduction, congestion-free, telecommunication congestion control, flow table space headroom, clock synchronization, two-phase update approach, network updates, distributed system, Control systems, telecommunication scheduling, tightly-synchronized SDN, History, Security, Optimization, consistent data plane updates, software-defined networks, Distributed databases, logically-centralized perspective, asynchronous network, polynomial time, individual switches, greedy algorithm, SDN, Transient analysis, consistent network update problem, numerical simulation, congestion-free loop-free update scheduling algorithms, network routes, software defined networking, loop-free, Routing, Mininet, mathematical programming, rule updates, telecommunication network routing, minimum update time problem, Chronus algorithms, timed SDN, optimization program]
Distributed Deep Neural Networks Over the Cloud, the Edge and End Devices
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
We propose distributed deep neural networks (DDNNs) over distributed computing hierarchies, consisting of the cloud, the edge (fog) and end devices. While being able to accommodate inference of a deep neural network (DNN) in the cloud, a DDNN also allows fast and localized inference using shallow portions of the neural network at the edge and end devices. When supported by a scalable distributed computing hierarchy, a DDNN can scale up in neural network size and scale out in geographical span. Due to its distributed nature, DDNNs enhance sensor fusion, system fault tolerance and data privacy for DNN applications. In implementing a DDNN, we map sections of a DNN onto a distributed computing hierarchy. By jointly training these sections, we minimize communication and resource usage for devices and maximize usefulness of extracted features which are utilized in the cloud. The resulting system has built-in support for automatic sensor fusion and fault tolerance. As a proof of concept, we show a DDNN can exploit geographical diversity of sensors to improve object recognition accuracy and reduce communication cost. In our experiment, compared with the traditional method of offloading raw sensor data to be processed in the cloud, DDNN locally processes most sensor data on end devices while achieving high accuracy and is able to reduce the communication cost by a factor of over 20x.
[Performance evaluation, object recognition, Cloud computing, Computational modeling, dnn, edge computing, Artificial neural networks, sensor fusion, embedded dnn, ddnn, distributed deep neural networks, Training, scalable distributed computing hierarchy, system fault tolerance, feature extraction, distributed computing hierarchies, data privacy, cloud computing, neural nets, deep neural networks, DDNN]
Dynamic Control of Flow Completion Time for Power Efficiency of Data Center Networks
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Data center network (DCN) can consume a significant amount of power (e.g., 10% to 20%) in large-scale data centers. To reduce the power consumption of DCN, traffic consolidation has been recently proposed as an effective approach to reduce the number of DCN devices in use. However, existing consolidation approaches do not sufficiently consider the flow completion time (FCT) requirement. On one hand, missing the FCT deadlines can cause serious violation of service-level agreement, especially for delay-sensitive networking services, such as web search and E-commerce. On the other hand, keeping all the devices on to make FCTs much shorter than the desired requirements is unnecessary because 1) users may not be able to perceive the difference, and 2) such a greedy strategy can lead to unnecessarily high DCN power consumption and thus more electricity costs. In this paper, we propose FCTcon, a dynamic FCT control strategy for DCN power optimization. FCTcon is designed rigorously based on control theory to dynamically control the FCT of delay-sensitive traffic flows exactly to requirements, such that the desired FCT performance is guaranteed while the maximum amount of DCN power savings can be achieved. Results from both hardware experiments and simulation evaluations demonstrate that, compared to the state-of-the-art DCN power optimization schemes, FCTcon can improve the DCN FCT performance, while achieving nearly the same or even more power savings. Consequently, FCTcon can result in more than 22.0% to 62.2% extra net profits for a data center with 50K servers.
[Electronic publishing, service-level agreement, control theory, dynamic control, Encyclopedias, E-commerce, contracts, Optimization, power consumption, greedy strategy, FCT deadlines, electricity costs, DCN devices, data center networks, delay-sensitive networking services, power efficiency, FCTcon, Power demand, computer networks, large-scale data centers, dynamic FCT control strategy, DCN power optimization, traffic consolidation, web search, computer centres, DCN power savings, flow completion time, energy conservation, DCN FCT performance, Internet, Control theory, FCT requirement, delay-sensitive traffic flows, DCN power consumption]
On Energy-Efficient Congestion Control for Multipath TCP
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Multipath TCP (MPTCP) enables transmission via multiple routes for an end-to-end connection to improve resource usage of regular TCP. Due to the increasing concern in green computing, there has been significant interest in designing energy-efficient multipath transport. For existing MPTCP congestion control algorithms, the research community still lacks a comprehensive understanding of which components in such an algorithm play the fundamental role in energy efficiency, how various algorithms compare against each other from energy-consuming perspective, or whether there exist potentially better solutions for energy saving. In this paper, we take a first step to answer these questions. Based on the MPTCP Linux kernel experiments, we first summarize that the energy consumption is related to three aspects: average throughput, path delay and different network scenarios. In order to bridge congestion control to the three aspects, we analyze the existing algorithms and capture the essential parameters of multipath congestion control model related to MPTCP's energy-efficiency. Then we design a window increase factor to shift traffic to low-delay energy-efficient paths. We further extend this design by using an energy-aware compensative parameter to fit the general hierarchical Internet topology. We evaluate the performance of existing multipath congestion control algorithms and our proposed algorithm in different network scenarios. The results successfully validate the improved energy efficiency of our design.
[Algorithm design and analysis, Energy consumption, energy-aware compensative parameter, telecommunication congestion control, Throughput, multiple routes, end-to-end connection, different network scenarios, low-delay energy-efficient paths, Kernel, energy consumption, Wireless fidelity, multipath congestion control model, operating system kernels, Power demand, path delay, telecommunication power management, MPTCP Linux kernel experiments, telecommunication network topology, multipath channels, MPTCP congestion control algorithms, energy saving, resource usage, energy-efficient congestion control, Linux, transport protocols, multipath TCP, telecommunication network routing, energy conservation, Internet, green computing, energy-efficient multipath transport, regular TCP, telecommunication traffic, average throughput, general hierarchical Internet topology]
A Mechanism for Cooperative Demand-Side Management
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Demand-side management (DSM) is an important theme in studies of the Smart Grid and offers the possibility of leveling power consumption with its attendant benefits of reducing capital expenses. This paper develops an algorithmic mechanism that reduces peak total power consumption and encourages prosocial behavior, such as expressing flexibility in one's power consumption and reporting preferences truthfully. The objective is to provide a tractable, budget-balanced mechanism that promotes truth-telling from households. The resulting mechanism is theoretically and empirically proven to be ex ante budget-balanced, weakly Pareto-efficient, and weakly Bayesian incentive-compatible. A simulation study verifies that the mechanism could largely reduce the computational complexity that the optimal allocation requires, while maintaining approximately the same performance. A user study with 20 subjects further shows the effectiveness of the mechanism in preventing participants from defecting and incentivizing them to reveal flexible preferences.
[Companies, smart power grids, Cost accounting, power consumption, Home appliances, ex ante budget balanced compatible, weakly Bayesian incentive compatible, truth-telling, algorithmic mechanism, Pricing, weakly Pareto-efficient compatible, Demand-side management, cooperative demand-side management, Pareto analysis, Load modeling, Power demand, smart grid, prosocial behavior, incentive schemes, Prosocial behaviors, budget-balanced mechanism, optimal allocation, Bayes methods, demand side management, Resource management, computational complexity]
A Hierarchical Framework of Cloud Resource Allocation and Power Management Using Deep Reinforcement Learning
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Automatic decision-making approaches, such as reinforcement learning (RL), have been applied to (partially) solve the resource allocation problem adaptively in the cloud computing system. However, a complete cloud resource allocation framework exhibits high dimensions in state and action spaces, which prohibit the usefulness of traditional RL techniques. In addition, high power consumption has become one of the critical concerns in design and control of cloud computing systems, which degrades system reliability and increases cooling cost. An effective dynamic power management (DPM) policy should minimize power consumption while maintaining performance degradation within an acceptable level. Thus, a joint virtual machine (VM) resource allocation and power management framework is critical to the overall cloud computing system. Moreover, novel solution framework is necessary to address the even higher dimensions in state and action spaces. In this paper, we propose a novel hierarchical framework for solving the overall resource allocation and power management problem in cloud computing systems. The proposed hierarchical framework comprises a global tier for VM resource allocation to the servers and a local tier for distributed power management of local servers. The emerging deep reinforcement learning (DRL) technique, which can deal with complicated control problems with large state space, is adopted to solve the global tier problem. Furthermore, an autoencoder and a novel weight sharing structure are adopted to handle the high-dimensional state space and accelerate the convergence speed. On the other hand, the local tier of distributed server power managements comprises an LSTM based workload predictor and a model-free RL based power manager, operating in a distributed manner. Experiment results using actual Google cluster traces show that our proposed hierarchical framework significantly saves the power consumption and energy usage than the baseline while achieving no severe latency degradation. Meanwhile, the proposed framework can achieve the best trade-off between latency and power/energy consumption in a server cluster.
[Cloud computing, cloud computing system, autoencoder, state space, Aerospace electronics, model-free RL based power manager, Servers, hierarchical framework, LSTM based workload predictor, Degradation, power aware computing, resource allocation, cloud resource allocation, Deep reinforcement learning, deep reinforcement learning, cloud computing, learning (artificial intelligence), Power demand, distributed server power managements, DPM policy, automatic decision-making, dynamic power management, distributed algorithm, power consumption minimization, virtual machine, Learning (artificial intelligence), decision making, virtual machines, VM, Resource management, state-space methods, DRL, weight sharing structure]
SunChase: Energy-Efficient Route Planning for Solar-Powered EVs
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Electric vehicles (EVs) play a significant role in the current transportation systems. The main factor that affects the acceptance of existing EV models is the range anxiety problem caused by limited charging stations and long recharge times. Recently, the solar-powered EV has drawn many attentions due to being free of charging limitations. However, the solarpowered EVs may still struggle with the limited use because of unpredictable solar availability. For example, shadings causedby buildings and trees also possibly decrease the solar panel cell efficiency. To address this, we propose a route planning method for solar-powered EVs to balance the energy harvesting and consumption subject to time constraint. The idea behindour solution is to offer power-aware optimal routing, which maximizes the on-road energy input given solar availability on each road segment. We first build a solar access estimation model using 3D geographic data and then employ a multi-criteriasearch method to generate a set of Pareto candidate routes. In order to reduce the size of the set, we leverage the bisect kmeans clustering algorithm to extract the most representative Pareto routes with better solar availability. In the evaluation, wedeveloped a validation platform on the vehicle and leveraged mobile sensing techniques to examine our proposed model in real road environments. We conducted simulations to evaluate our proposed route planning algorithm using real life scenarios. Experimental results demonstrate that our solar input model is robust to real road scenarios, and the routing algorithm has great potential to provide efficient services for solar-powered EV in the future.
[estimation theory, Roads, Heuristic algorithms, solar powered vehicles, Solar Power, Vehicles, mobile computing, power aware computing, Pareto candidate routes, mobile sensing, Electric Vehicle, control engineering computing, energy consumption, kmeans clustering, Pareto optimisation, solar powered electric vehicles, transportation systems, 3D geographic data, energy harvesting, Routing, energy efficiency route planning, Path planning, Route Planning, SunChase, pattern clustering, solar powered EVs, vehicle routing, power aware optimal routing, Solar energy, Planning, solar access estimation]
Persistent Traffic Measurement Through Vehicle-to-Infrastructure Communications
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Measuring point traffic volume and point-to-point traffic volume in a road system has important applications in transportation engineering. The connected vehicle technologies integrate wireless communications and computers into transportation systems, allowing wireless data exchanges between vehicles and road-side equipment, and enabling large-scale, sophisticated traffic measurement. This paper investigates the problems of persistent point traffic measurement and persistent point-to-point traffic measurement, which were not adequately studied in the prior art, particularly in the context of intelligent vehicular networks. We propose two novel estimators for privacy-preserving persistent traffic measurement: one for point traffic and the other for point-to-point traffic. The estimators are mathematically derived from the join result of traffic records, which are produced by the electronic roadside units with privacy-preserving data structures. We evaluate our estimation methods using simulations based on both real transportation traffic data and synthetic data. The numerical results demonstrate the effectiveness of the proposed methods in producing high measurement accuracy and allowing accuracy-privacy tradeoff through parameter setting.
[telecommunication security, wireless communications, connected vehicle technologies, large-scale traffic measurement, Roads, traffic measurement, privacy, road-side equipment, Wireless communication, Privacy, Volume measurement, persistent point traffic measurement, road system, computers, point-to-point traffic volume, mobile radio, Estimation, vehicle-to-infrastructure communications, privacy-preserving persistent traffic measurement, Probabilistic logic, sophisticated traffic measurement, electronic roadside units, accuracy-privacy tradeoff, persistent traffic, Vehicular networks, data privacy, wireless data exchanges, intelligent vehicular networks, telecommunication traffic, transportation engineering]
TagBreathe: Monitor Breathing with Commodity RFID Systems
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Breath monitoring helps assess the general personal health and gives clues to chronic diseases. Yet current breath monitoring technologies are inconvenient and intrusive. For instance, typical breath monitoring devices need to attach nasal probes or chest bands to users. Wireless sensing technologies have been applied to monitor breathing using radio waves without physical contact. Those wireless sensing technologies however require customized radios which are not readily available. More importantly, due to interference, such technologies do not work well with multiple users. With multiple users in presence, the detection accuracy of existing systems decreases dramatically. In this paper, we propose to monitor users' breathing using commercial-off-the-shelf (COTS) RFID systems. In our system, passive lightweight RFID tags are attached to users' clothes and backscatter radio waves, and commodity RFID readers report low level data (e.g., phase values). We track periodic body movement due to inhaling and exhaling by analyzing the low level data reported by commodity readers. To enhance the measurement robustness, we synthesize data streams from an array of multiple tags to improve the monitoring accuracy. Our design follows the standard EPC protocol which arbitrates collisions in the presence of multiple tags. We implement a prototype the breath monitoring system with commodity RFID systems. The experiment results show that the prototype system can simultaneously monitor breathing with high accuracy even with the presence of multiple users.
[Phase measurement, radiofrequency identification, wireless sensor networks, periodic body movement, data streams, Frequency measurement, breath monitoring technologies, Doppler effect, pneumodynamics, backscatter radio waves, chest bands, phase values, commodity RFID systems, passive lightweight RFID tags, protocols, Monitoring, radio waves, Backscatter, commercial-off-the-shelf, chronic diseases, backscatter, TagBreathe, EPC protocol, wireless sensing technologies, nasal probes, COTS, commodity RFID readers, Biomedical monitoring, Radiofrequency identification]
Double-Edged Sword: Incentivized Verifiable Product Path Query for RFID-Enabled Supply Chain
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Querying the path information of individual products in a supply chain is key to many applications. RFID (Radio-Frequency IDentification) is a main technology to enable product path information query today. With RFID technology, supply chain participants can efficiently track products in transit and record their production information in databases. In this paper, we investigate the following question: how can we conduct privacy-preserving product path information query with verifiability on an RFID-enabled distributed supply chain? We address this question with Double Edged(DE)-Sword, an incentivized verifiable query system. DE-Sword introduces a novel double-edged reputation incentive mechanism to encourage supply chain participants to behave; and couples it with cryptographic primitives and careful protocol design. We evaluate DE-Sword through security analysis and performance experiments. The security analysis shows that DE-Sword guarantees both verifiability and privacy. The experiment results show that DE-Sword achieves low overhead in RFID-enabled supply chain applications.
[double-edged sword, incentivized verifiable query system, cryptographic protocols, radiofrequency identification, Supply chains, production engineering computing, reputation, RFID technology, production information, query processing, Privacy, Databases, Cryptography, incentivized verifiable product path query, DE-Sword, security analysis, cryptographic primitives, path query, RFID, privacy-preserving product path information query, RFID-enabled distributed supply chain, double-edged reputation incentive mechanism, careful protocol design, radio-frequency identification, supply chain management, data privacy, Radiofrequency identification]
Towards Accurate Corruption Estimation in ZigBee Under Cross-Technology Interference
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Cross-Technology Interference affects the operation of low-power ZigBee networks, especially under severe WiFi interference. Accurate corruption estimation is very important to improve the resilience of ZigBee transmissions. However, there are many limitations in existing approaches such as low accuracy, high overhead, and requiring hardware modification. In this paper, we propose an accurate corruption estimation approach, AccuEst, which utilizes per-byte SINR (Signal-to-Interference-and-Noise Ratio) to detect corruption. We combine the use of pilot symbols with per-byte SINR to improve corruption detection accuracy, especially in highly noisy environments (i.e., noise and interference are at the same level). In addition, we design an adaptive pilot instrumentation scheme to strike a good balance between accuracy and overhead. We implement AccuEst on the TinyOS 2.1.1/TelosB platform and evaluate its performance through extensive experiments. Results show that AccuEst improves corruption detection accuracy by 78.6% on average compared with state-of-the-art approach (i.e., CARE) in highly noisy environments. In addition, AccuEst reduces pilot overhead by 53.7% on average compared to the traditional pilot-based approach. We implement AccuEst in a coding-based transmission protocol, and results show that with AccuEst, the packet delivery ratio is improved by 20.3% on average.
[ZigBee transmissions, estimation theory, Bit error rate, Zigbee, packet delivery ratio, pilot overhead, TinyOS 2.1.1/TelosB platform, Hardware, Wireless fidelity, corruption detection accuracy, ZigBee, per-byte SINR, low-power ZigBee networks, Estimation, Interference, Noise measurement, encoding, corruption estimation approach, interference (signal), WiFi interference, coding-based transmission protocol, transport protocols, signal-to-interference and-noise ratio, accurate corruption estimation, adaptive pilot instrumentation scheme, cross-technology interference, AccuEst]
Unseen Activity Recognitions: A Hierarchical Active Transfer Learning Approach
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Human activity recognition (AR) is an essential element for user-centric and context-aware applications. While previous studies showed promising results using various machine learning algorithms, most of them can only recognize the activities that were previously seen in the training data. We investigate the challenges of improving the recognition of unseen daily activities in smart home environment, by better exploiting the hierarchical taxonomy of complex daily activities. We first (a) design a hierarchical representation of complex activity taxonomy in terms of human-readable semantic attributes, and (b) develop a hierarchy of classifiers which incorporates a cluster tree built on the domain knowledge from training samples. Though this model is rich in recognizing complex activities that are previously seen in training data, it is not well versed to recognize unseen complex activities without new training samples. To tackle this challenge, we extend Hierarchical Active Transfer Learning (HATL) approach that exploits semantic attribute cluster structure of complex activities shared between seen (source) and unseen (target) activity domains. Our approach employs transfer and active learning to help label target domain unlabeled data by spawning the most effective queries. We evaluated our approach with two real-time smart home systems (IRB #HP-00064387) which corroborates radical improvements in recognizing unseen complex activities.
[hierarchical active transfer learning, pattern classification, domain knowledge, Uncertainty, human-readable semantic attributes, Taxonomy, Smart homes, real-time smart home systems, Training, home automation, complex activity taxonomy, pattern clustering, Semantics, semantic attribute cluster structure, unseen activity recognitions, classifiers, cluster tree, Sensors, Labeling, learning (artificial intelligence)]
RFIPad: Enabling Cost-Efficient and Device-Free In-air Handwriting Using Passive Tags
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
An important function of smart environments is the ubiquitous access of computing devices. In public areas such as hospitals, libraries, and airports, people may want to interact with nearby computing systems to get information, such as directions to a hospital room, locations of books, and flight departure/arrival information. Touch screen based displays and kiosks, which are commonly used today, may incur extra hardware cost or even possible germ and bacteria infection. This work provides a new solution: users can make queries and inputs by performing in-air handwriting to an array of passive RFID tags, named RFIPad. This input method does not require human hands to carry any device and hence is convenient for applications in public areas. Besides the mobile and contactless property, this system is a cost-efficient extension to current RFID systems: an existing reader can monitor multiple RFIPads while performing its regular applications such as identification and tracking. We implement a prototype of RFIPad using commercial off-the-shelf UHF RFID devices. Experimental results show that RFIPad achieves &gt;91% accuracy in recognizing basic touch-screen operations and English letters.
[Performance evaluation, bacteria infection, hospitals, radiofrequency identification, touch-screen operation recognition, device-free in-air handwriting, Doppler effect, handwriting recognition, Radio frequency, touch screen-based displays, English letter recognition, RFIPad, Prototypes, cost-efficient in-air handwriting, touch sensitive screens, passive tags, book location, germ infection, Touch sensitive screens, libraries, flight departure-arrival information, passive RFID tags, UHF RFID devices, airports, cost-efficient extension, Hospitals, contactless property, hospital room, kiosks, computing systems, image recognition, Radiofrequency identification]
Robust Incentive Tree Design for Mobile Crowdsensing
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
With the proliferation of smart mobile devices (smart phone, tablet, and wearable), mobile crowdsensing becomes a powerful sensing and computation paradigm. It has been put into application in many fields, such as spectrum sensing, environmental monitoring, healthcare, and so on. Driven by promising incentives, the power of the crowd grants crowdsensing an advantage in mobilizing users who perform sensing tasks with the embedded sensors on the smart devices. Auction is one of the commonly adopted crowdsensing incentive mechanisms to incentivize users for participation. However, it does not consider the incentive for user solicitation, where in crowdsensing, such incentive would ease the tension when there is a lack of crowdsensing users. To deal with this issue, we aim to design an auction-based incentive tree to offer rewards to users for both participation and solicitation. Meanwhile, we want the incentive mechanism to be robust against dishonest behavior such as untruthful bidding and sybil attacks, to eliminate malicious price manipulations. We design RIT as a Robust Incentive Tree mechanism for mobile crowdsensing which combines the advantages of auctions and incentive trees. We prove that RIT is truthful and sybil-proof with probability at least H, for any given H &#x2208; (0, 1). We also prove that RIT satisfies individual rationality, computational efficiency, and solicitation incentive. Simulation results of RIT further confirm our analysis.
[telecommunication security, individual rationality, smart mobile devices, environmental monitoring, Crowdsensing, incentive mechanism, Truthfulness., Mobile communication, sensor fusion, computational efficiency, Mobile handsets, Wireless Networks, healthcare, wearable device, tablet, user solicitation, mobile computing, embedded sensors, Mobile Networks, mobile crowdsensing, Robustness, dishonest behavior, Sensors, solicitation incentive, sybil-proof, Crowdsourcing, probability, trees (mathematics), sybil attacks, smart phone, Mechanical factors, smart phones, Sybil-proofness, robust incentive tree design, malicious price manipulation, Incentive Mechanism, crowdsensing user incentivization, security of data, untruthful bidding, Vegetation, spectrum sensing, auction-based incentive tree]
WearLock: Unlocking Your Phone via Acoustics Using Smartwatch
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Smartphone lock screens are implemented to reduce the risk of data loss or compromise given the fact that increasing amount of person data are accessible on smartphones nowadays. Unfortunately, many smartphone users abandon lock screens due to the inconvenience of unlocking their phones many times a day. With the wide adoption of wearables, token-based approaches have gained popularity in simplifying unlocking and retaining security at the same time. To this end, we propose to take advantage of the smartwatch for easy smartphone unlocking. In this paper, we have designed WearLock, a system that uses acoustic tones as tokens to automate the unlocking securely. We build a sub-channel selection and an adaptive modulation in the acoustic modem to maximize unlocking success rate against ambient noise only when those two devices are nearby. We leverage the motion sensor on the smartwatch to reduce the unlock frequency. We offload smartwatch tasks to the smartphone to speed up computation and save energy. We have implemented the WearLock prototype and conducted extensive evaluations. Results achieved a low average bit error rate (BER) as 8% in various experiments. Compared to traditional manual personal identification numbers (PINs) entry, WearLock achieves at least 18% unlock speedup without any manual effort.
[smartwatch, adaptive modulation, smartphone, OFDM, smartphone lock screens, Acoustics, wearables, user interfaces, Communication system security, Wireless communication, security, acoustics, Modems, error statistics, smartphone unlocking, motion sensor, WearLock, smart phones, bit error rate, token-based approaches, Wireless sensor networks, security of data, acoustic tones, Authentication, acoustic modem, subchannel selection, Smart phones]
Modeling Mobile Code Acceleration in the Cloud
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The quality of service of a mobile application is critical to ensure user satisfaction. Techniques have been proposed to accomplish adaptation of quality of service dynamically. However, there is still a limited understanding about how to provide a utility model for code execution. One key challenge is modeling the level of quality in the code execution that can be provisioned by the cloud. Since the allocation of cloud resources has a cost, it is important to optimize cloud usage. We propose a software-defined networking approach that allows modeling and controlling code acceleration of a mobile application deployed across multiple type of devices. By segregating the computational requirements of the mobile application into groups, we were able to define the acceleration needed by each group of devices. As the computational requirements of a device can change across time, a mobile device can be re-assigned to another group based on demand. Our SDN approach implements a model that allows the system to predict workload based on acceleration groups. Evaluating our system in a real testbed showed that it is possible to predict workload and allocate optimal resources to handle that workload with 87.5% accuracy.
[Cloud computing, computational requirements, Software-defined, cloud usage, cloud resources, Mobile communication, Code Offload, Servers, optimal resources allocation, mobile computing, QoS, software-defined networking approach, acceleration groups, cloud computing, SDN approach, Computational modeling, software defined networking, Mobile applications, quality of service, Mobile Cloud, mobile device, mobile code acceleration modeling, workload prediction, Acceleration, Smart phones, code execution]
E-Android: A New Energy Profiling Tool for Smartphones
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
As the limited battery lifetime remains a major factor restricting the applicability of a smartphone, significant research efforts have been devoted to understand the energy consumption in smartphones. Existing energy modeling methods can account energy drain in a fine-grained manner and provide well designed human-battery interfaces for users to characterize energy usage of every app in smartphones. However, in this paper, we demonstrate that there are still pitfalls in current Android energy modeling approaches, leaving collateral energy consumption unaccounted. The existence of collateral energy consumption becomes a serious energy bug. In particular, those energy bugs could be exploited to launch a new class of energy attacks, which deplete battery life and sidestep the supervision of current energy accounting. To unveil collateral energy bugs, we propose E-Android to accurately profile energy consumption of a smartphone in a comprehensive manner. E-Android monitors collateral energy related events and maintains energy consumption maps for relevant apps. We evaluate the effectiveness of E-Android under six different collateral energy attacks and two normal scenarios, and compare the results with those of Android. While Android fails to disclose collateral energy bugs, E-Android can accurately profile energy consumption and reveal the existence of energy bugs with minor overhead.
[E-Android, Energy consumption, human-battery interfaces, Humanoid robots, Android energy modeling approaches, smart phones, Batteries, energy bugs, Android (operating system), power aware computing, Computer bugs, battery lifetime, collateral energy attacks, Malware, smartphones, Androids, collateral energy consumption, energy consumption, Smart phones, energy profiling tool]
Local and Low-Cost White Space Detection
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
White spaces are portions of the TV spectrum that are allocated but not used locally. Ifaccurately detected, white spaces offer a valuable new opportunity for highspeed wireless communications. We propose a new method for white space detection that allows a node to actlocally, based on a centrally constructed model, and at low cost, whiledetecting more spectrum opportunities than best known approaches. Weleverage two ideas. First, we demonstrate that low-cost spectrum monitoringhardware can offer "good enough" detection capabilities. Second, we develop amodel that combines locally-measured signal features and location to more efficiently detect white space availability. We incorporate these ideas into the design,implementation, and evaluation of a complete system we call Waldo. We deployWaldo on a laptop in the Atlanta metropolitan area in the US covering 700 km2. Our results show that usingsignal features, in addition to location, can improve detection accuracy by up to10x for some channels. We also deploy Waldo on an Android smartphone,demonstrating the feasibility of real-time white space detection with efficientuse of smartphone resources.
[radio spectrum management, TV, white space detection, high-speed wireless communication, Urban areas, FCC, TV spectrum, low-cost spectrum monitoring hardware, smart phones, local low-cost white space detection, low cost spectrum sensing, Databases, Android smartphone, measurement-augmented white space databases, White spaces, Sensors, Safety, US]
General Analysis of Incentive Mechanisms for Peer-to-Peer Transmissions: A Quantum Game Perspective
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The peer-to-peer transmission is a mainstream in challenged network environments. Yet, the free rider phenomenon in peer-to peer transmissions presses a need for incentive mechanisms to stimulate contributions of data transmission. As a result, it is imperative to answer the questions: whether and to what extent an incentive mechanism can invoke such contributions? To answer these questions, we employ an n-player continuous quantum game model to analyze extrinsic incentive mechanisms (promoting cooperative behaviors by offering rewards), and use the quantum prisoner's dilemma model to analyze intrinsic incentive mechanisms (encouraging reciprocal cooperation by exploiting internal bounds). To the best of our knowledge, we are the first to analyze incentive mechanisms for peer-to-peer transmissions from a quantum game perspective. Such a perspective is adopted because the extended strategy space in the quantum game broadens the range for searching optimal strategies and the introduction of entanglement makes the proposed analytical frameworks more practical due to the consideration of the peers' relationships in decision-making. Our proposed quantum game-based analytical frameworks are generic because they are compatible with classic game-based schemes. Our analytical results can provide straightforward insights on evaluating the potential of incentive mechanisms and can serve as important references for designing new incentive mechanisms.
[peer-to-peer computing, Conferences, decision making, game theory, data transmission, quantum prisoner dilemma model, quantum game-based analytical framework, free rider phenomenon, incentive schemes, general incentive mechanism analysis, Distributed computing, peer-to-peer transmission]
High-Performance and Resilient Key-Value Store with Online Erasure Coding for Big Data Workloads
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Distributed key-value store-based caching solutions are being increasingly used to accelerate Big Data applications on modern HPC clusters. This has necessitated incorporating fault-tolerance capabilities into high-performance key-value stores such as Memcached that are otherwise volatile in nature. In-memory replication is being used as the primary mechanism to ensure resilient data operations. However, this incurs increased network I/O with high remote memory requirements. On the other hand, Erasure Coding is being extensively explored for enabling data resilience, while achieving better storage efficiency. In this paper, we first perform an in-depth modeling-based analysis of the performance trade-offs of In-Memory Replication and Erasure Coding schemes for key-value stores, and explore the possibilities of employing Online Erasure Coding for enabling resilience in high-performance key-value stores for HPC clusters. We then design a non-blocking API-based engine to perform efficient Set/Get operations by overlapping the encoding/decoding involved in enabling Erasure Coding-based resilience with the request/response phases, by leveraging RDMA on high performance interconnects. Performance evaluations show that the proposed designs can outperform synchronous RDMA-based replication by about 2.8&#x00D7;, and can improve YCSB throughput and average read/write latencies by about 1.34&#x00D7; - 2.6&#x00D7; over asynchronous replication for larger key-value pair sizes (&gt;16KB). We also demonstrate its benefits by incorporating it into a hybrid and resilient key-value store-based burst-buffer system over Lustre for accelerating Big Data I/O on HPC clusters.
[RDMA, HPC clusters, application program interfaces, high-performance clusters, nonblocking API-based engine, Big Data, Encoding, cache storage, Servers, parallel processing, Resilience, set/get operations, pattern clustering, Erasure Coding, Distributed databases, distributed key-value store-based caching solutions, Big Data workloads, in-memory replication, Mathematical model, Acceleration, online erasure coding, Memcached]
Modeling and Analyzing Latency in the Memcached system
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Memcached is a widely used in-memory caching solution in large-scale searching scenarios. The most pivotal performance metric in Memcached is latency, which is affected by various factors including the workload pattern, the service rate, the unbalanced load distribution and the cache miss ratio. To quantitate the impact of each factor on latency, we establish a theoretical model for the Memcached system. Specially, we formulate the unbalanced load distribution among Memcached servers by a set of probabilities, capture the burst and concurrent key arrivals at Memcached servers in form of batching blocks, and add a cache miss processing stage. Based on this model, algebraic derivations are conducted to estimate latency in Memcached. The latency estimation is validated by intensive experiments. Moreover, we obtain a quantitative understanding of how much improvement of latency performance can be achieved by optimizing each factor and provide several useful recommendations to optimal latency in Memcached.
[Measurement, concurrent key arrivals, burst key arrivals, Estimation, workload pattern, in-memory caching solution, Servers, Modeling, memcached system, Latency, Quantitative Analysis, Analytical models, Databases, large-scale searching scenarios, service rate, unbalanced load distribution, cache miss ratio, Web sites, Facebook, pivotal performance metric, latency estimation, Load modeling, Memcached]
Speculative Slot Reservation: Enforcing Service Isolation for Dependent Data-Parallel Computations
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Priority scheduling is a fundamental tool to provide service isolation for different jobs in shared clusters. Ideally, the performance of a high-priority job should not be dragged down by another with a lower priority. However, we show in this paper that simply assigning a high priority provides no isolation for jobs with dependent computations. A job, even receiving the highest priority, may give up compute slots to another before proceeding to the downstream computation, which is because of barrier, i.e., that the downstream computation cannot start until all the upstream tasks have completed. Such an interruption of execution inevitably results in a significant delay. In this paper, we propose speculative slot reservation that judiciously reserves slots for downstream computations, so as to retain service isolation for high-priority jobs. To mitigate the utilization loss due to slot reservation, we analyze the trade-off between utilization and isolation, and expose a tunable knob to navigate the trade-off. We also propose a complementary straggler mitigation strategy that uses the reserved slots to run extra copies of slow tasks. We have implemented speculative slot reservation in Spark. Evaluations based on both cluster deployment and trace-driven simulations show that our approach enforces strict service isolation for high-priority jobs, without slowing down the other jobs with a lower priority.
[workstation clusters, slot reservation, Navigation, cluster scheduling, Resumes, dependent data-parallel computations, straggler mitigation strategy, service isolation, Sparks, parallel processing, Support vector machines, speculative slot reservation, Processor scheduling, resource allocation, data analytics systems, Production, Delays, data handling, Spark, data-parallel clusters]
Optimizing Shuffle in Wide-Area Data Analytics
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
As increasingly large volumes of raw data are generated at geographically distributed datacenters, they need to be efficiently processed by data analytic jobs spanning multiple datacenters across wide-area networks. Designed for a single datacenter, existing data processing frameworks, such as Apache Spark, are not able to deliver satisfactory performance when these wide-area analytic jobs are executed. As wide-area networks interconnecting datacenters may not be congestion free, there is a compelling need for a new system framework that is optimized for wide-area data analytics. In this paper, we design and implement a new proactive data aggregation framework based on Apache Spark, with a focus on optimizing the network traffic incurred in shuffle stages of data analytic jobs. The objective of this framework is to strategically and proactively aggregate the output data of mapper tasks to a subset of worker datacenters, as a replacement to Spark's original passive fetch mechanism across datacenters. It improves the performance of wide-area analytic jobs by avoiding repetitive data transfers, which improves the utilization of inter-datacenter links. Our extensive experimental results using standard benchmarks across six Amazon EC2 regions have shown that our proposed framework is able to reduce job completion times by up to 73%, as compared to the existing baseline implementation in Spark.
[workstation clusters, Data analysis, data analysis, wide area networks, wide-area data analytics, proactive data aggregation framework, Data aggregation, Amazon EC2 regions, Sparks, data aggregation, computer centres, Apache Spark, MapReduce, data analysis shuffle stages, wide-area networks, Distributed databases, Bandwidth, Data transfer, inter-datacenter links, inter-datacenter network, shuffle]
Job Scheduling without Prior Information in Big Data Processing Systems
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Job scheduling plays an important role in improving the overall system performance in big data processing frameworks. Simple job scheduling policies, such as Fair and FIFO scheduling, do not consider job sizes and may degrade the performance when jobs of varying sizes arrive. More elaborate job scheduling policies make the convenient assumption that jobs are recurring, and complete information about their sizes is available from their prior runs. In this paper, we design and implement an efficient and practical job scheduler for big data processing systems to achieve better performance even without prior information about job sizes. The superior performance of our job scheduler originates from the design of multiple level priority queues, where jobs are demoted to lower priority queues if the amount of service consumed so far reaches a certain threshold. In this case, jobs in need of a small amount of service can finish in the topmost several levels of queues, while jobs that need a large amount of service to complete are moved to lower priority queues to avoid head-of-line blocking. Our new job scheduler can effectively mimic the shortest job first scheduling policy without knowing the job sizes in advance. To demonstrate its performance, we have implemented our new job scheduler in YARN, a popular resource manager used by Hadoop/Spark, and validated its performance with both experiments on real datasets and large-scale trace-driven simulations. Our experimental and simulation results have strongly confirmed the effectiveness of our design: our new job scheduler can reduce the average job response time of the Fair scheduler by up to 45%.
[Schedules, fair scheduling, Job shop scheduling, queueing theory, multiple level priority queues, job scheduling policies, Hadoop, Big Data, multilevel feedback queue, Sparks, Yarn, parallel processing, YARN, FIFO scheduling, resource manager, big data processing systems, Processor scheduling, scheduling, big data processing, Spark, Time factors, job scheduling]
Distributed Load Balancing in Key-Value Networked Caches
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Modern web services rely on a network of distributed cache servers to efficiently deliver content to users. Load imbalance among cache servers can substantially degrade content delivery performance. Due to the skewed and dynamic nature of real-world workloads, cache servers that serve viral content experience higher load as compared to other cache servers. We propose a novel distributed load balancing protocol called Meezan to address the load imbalance among cache servers. Meezan replicates popular objects to mitigate skewness and adjusts hash space boundaries in response to load dynamics in a novel way. Our theoretical analysis shows that Meezan achieves near perfect load balancing for a wide range of operating parameters. Our trace driven simulations shows that Meezan reduces load imbalance by up to 52% as compared to prior solutions.
[Protocols, distributed load balancing protocol, content delivery performance, key-value networked caches, Web servers, cache storage, trace driven simulations, Key-Value Networked Cache, Distributed Algorithms, Degradation, distributed cache servers, resource allocation, Web services, Distributed Load Balancing, Cloud Computing, load dynamics, Load management, Resource management, Meezan, Memcached]
Cognitive Context-Aware Distributed Storage Optimization in Mobile Cloud Computing: A Stable Matching Based Approach
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Mobile cloud storage (MCS) is being extensively used nowadays to provide data access services to various mobile platforms such as smart phones and tablets. For cross-platform mobile apps, MCS is a foundation for sharing and accessing user data as well as supporting seamless user experience in a mobile cloud computing environment. However, the mobile usage of smart phones or tablets is quite different from legacy desktop computers, in the sense that each user has his/her own mobile usage pattern. Therefore, it is challenging to design an efficient MCS that is optimized for individual users. In this paper, we investigate a distributed MCS system whose performance is optimized by exploiting the fine-grained context information of every mobile user. In this distributed system, lightweight storage servers are deployed pervasively, such that data can be stored closer to its user. We systematically optimize the data access efficiency of such a distributed MCS by exploiting three types of user context information: mobility pattern, network condition, and data access pattern. We propose two optimization formulations: a centralized one based on mixed-integer linear programming (MILP), and a distributed one based on stable matching. We then develop solutions to both formulations. Comprehensive simulations are performed to evaluate the effectiveness of the proposed solutions by comparing them against their counterparts under various network and context conditions.
[Cloud computing, network condition, integer programming, cognitive context-aware distributed storage optimization, MILP, Mobile communication, Mobile handsets, Encoding, linear programming, stable matching based approach, Optimization, storage management, mobile cloud computing, mobile computing, Distributed databases, mixed-integer linear programming, data access pattern, mobility pattern, cloud computing, distributed MCS system, Context modeling]
Fair Caching Algorithms for Peer Data Sharing in Pervasive Edge Computing Environments
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Edge devices (e.g., smartphones, tablets, connected vehicles, IoT nodes) with sensing, storage and communication resources are increasingly penetrating our environments. Many novel applications can be created when nearby peer edge devices share data. Caching can greatly improve the data availability, retrieval robustness and latency. In this paper, we study the unique issue of caching fairness in edge environment. Due to distinct ownership of peer devices, caching load balance is critical. We consider fairness metrics and formulate an integer linear programming problem, which is shown as summation of multiple Connected Facility Location (ConFL) problems. We propose an approximation algorithm leveraging an existing ConFL approximation algorithm, and prove that it preserves a 6.55 approximation ratio. We further develop a distributed algorithm where devices exchange data reachability and identify popular candidates as caching nodes. Extensive evaluation shows that compared with existing wireless network caching algorithms, our algorithms significantly improve data caching fairness, while keeping the contention induced latency similar to the best existing algorithms.
[Algorithm design and analysis, caching nodes, Caching Algorithms, cache storage, ConFL, data reachability, peer data sharing, IoT, mobile computing, Wireless networks, Distributed databases, approximation algorithm, approximation theory, connected facility location, Pervasive Edge Computing, reachability analysis, smart phones, edge devices, Internet of Things, distributed algorithm, pervasive edge computing environments, distributed algorithms, fair caching algorithms, Peer Data Sharing, Integer linear programming, Approximation algorithms, Delays, Peer-to-peer computing]
Latency-Driven Cooperative Task Computing in Multi-user Fog-Radio Access Networks
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Fog computing is emerging as one promising solution to meet the increasing demand for ultra-low latency services in wireless networks. Taking a forward-looking perspective, we propose a Fog-Radio Access Network (F-RAN) model, which utilizes the existing infrastructure, e.g., small cells and macro base stations, to achieve the ultra-low latency by joint computing across multiple F-RAN nodes and near-range communications at the edge. We treat the low latency design as an optimization problem, which characterizes the tradeoff between communication and computing across multiple F-RAN nodes. Since this problem is NP-hard, we propose a latency-driven cooperative task computing algorithm with one-for-all concept for simultaneous selection of the F-RAN nodes to serve with proper heterogeneous resource allocation for multi-user services. Considering the limited heterogeneous resources shared among all users, we advocate the one-for-all strategy for every user taking other's situation into consideration and seek for a "win-win" solution. The numerical results show that the low latency services can be achieved by F-RAN via latency-driven cooperative task computing.
[Algorithm design and analysis, Fog computing, Cloud computing, optimization problem, fog computing, heterogeneous resources, telecommunication computing, Wireless communication, ultra-low latency services, multiuser channels, optimisation, resource allocation, multiple F-RAN nodes, cloud computing, Fifth-generation (5G) cellular networks, Computational modeling, wireless networks, latency-driven cooperative task computing, radio access networks, cooperative communication, near-range communications, Ultra-low latency, NP-hard problem, multiuser services, heterogeneous resource allocation, small cells, multiuser fog-radio access networks, Delays, Resource management, macro base stations, cellular radio, computational complexity, F-RAN model]
Approximation and Online Algorithms for NFV-Enabled Multicasting in SDNs
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Multicasting is a fundamental functionality of networks for many applications including online conferencing, event monitoring, video streaming, and system monitoring in data centers. To ensure multicasting reliable, secure and scalable, a service chain consisting of network functions (e.g., firewalls, Intrusion Detection Systems (IDSs), and transcoders) usually is associated with each multicast request. Such a multicast request is referred to as an NFV-enabled multicast request. In this paper we study NFV-enabled multicasting in a Software-Defined Network (SDN) with the aims to minimize the implementation cost of each NFV-enabled multicast request or maximize the network throughput for a sequence of NFV-enabled requests, subject to network resource capacity constraints. We first formulate novel NFV-enabled multicasting and online NFV-enabled multicasting problems. We then devise the very first approximation algorithm with an approximation ratio of 2K for the NFV-enabled multicasting problem if the number of servers for implementing the network functions of each request is no more than a constant K (1). We also study dynamic admissions of NFV-enabled multicast requests without the knowledge of future request arrivals with the objective to maximize the network throughput, for which we propose an online algorithm with a competitive ratio of O(log n) when K = 1, where n is the number of nodes in the network. We finally evaluate the performance of the proposed algorithms through experimental simulations. Experimental results demonstrate that the proposed algorithms outperform other existing heuristics.
[Heuristic algorithms, data centers, Multicast communication, service chain, Middleboxes, firewalls, NFV-enabled multicasting, virtualisation, Servers, software-defined network, competitive ratio, software-defined networks, Bandwidth, multicast communication, IDS, video streaming, computer network reliability, SDN, approximation and online algorithms, online conferencing, virtualized network functions, request arrivals, approximation ratio, approximation theory, intrusion detection systems, network resource capacity constraints, multicasting scalability, Optical switches, NFV-enabled multicasting problem, online algorithm, dynamic admissions, software defined networking, network throughput maximization, first approximation algorithm, multicasting security, network fundamental functionality, NFV-enabled multicast request, multicasting reliability, multicasting, network functions, event monitoring, Approximation algorithms, system monitoring, service chains, transcoders, Network function virtualization, online NFV-enabled multicasting problem]
Distributed Auctions for Task Assignment and Scheduling in Mobile Crowdsensing Systems
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
With the emergence of Mobile Crowdsensing Systems (MCSs), many auction schemes have been proposed to incentivize mobile users to participate in sensing activities. However, in most of the existing work, the heterogeneity of MCSs has not been fully exploited. To tackle this issue, in this paper, we study the joint problem of sensing task assignment and scheduling while considering partial fulfillment, attribute diversity, and price diversity. We first elaborately model the problem as a reverse auction and design a distributed auction framework. Then, based on this framework, we propose two distributed auction schemes, cost-preferred auction scheme (CPAS) and time schedule-preferred auction scheme (TPAS), which differ on the methods of task scheduling, winner determination, and payment computation. We further rigorously prove that both CPAS and TPAS can achieve computational-efficiency, individual-rationality, budget-balance, and truthfulness. Finally, the simulation results validate the effectiveness of both CPAS and TPAS in terms of sensing task's allocation efficiency, mobile user's working time utilization and utility, and truthfulness.
[CPAS, time schedule-preferred auction scheme, truthful auction, Conferences, TPAS, task analysis, Distributed computing, winner determination, distributed auctions, distributed algorithm, mobile computing, cost-preferred auction scheme, scheduling, task assignment, task scheduling, mobile crowdsensing systems, MCS, reverse auction, payment computation, electronic commerce, Mobile crowdsensing system]
Effective Mobile Data Trading in Secondary Ad-hoc Market with Heterogeneous and Dynamic Environment
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Advances in smartphone technologies enable mobile data subscribers to resell their data allowance to other users, creating a secondary data market. The trading environment of this secondary data market is dynamic and ad-hoc: buyers and sellers join and leave the market at all times, changing the trading landscape constantly. The amount of data demanded and offered at any point in time also vary. These conditions make determining a fair transaction price, and matching buyers to sellers difficult in practice. Prior schemes utilize global description of the network and market forces to achieve good performance, but the implementation requires a high overhead cost. In this paper, we present DataMart, a data pricing and user matching platform for trading in this dynamic, ad-hoc and heterogeneous market that works in distributed manner without needing global information. Using insights from real world traces, we demonstrate via simulation that our pricing scheme is converging and consistent with the law of demand and supply. Further, our user matching scheme achieves comparable performance to the optimal solution. We implement a prototype on Android platform, and the experiment results confirm the effectiveness of DataMart.
[Heuristic algorithms, real world traces, user matching platform, Mobile communication, secondary ad-hoc market, Mobile handsets, smart phones, marketing data processing, commerce, smartphone technologies, data pricing, Distributed databases, Pricing, Android platform, heterogeneous environment, DataMart, dynamic environment, effective mobile data trading, fair transaction price, Wireless fidelity, Mobile computing]
Kalis &#x2014; A System for Knowledge-Driven Adaptable Intrusion Detection for the Internet of Things
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In this paper, we introduce Kalis, a self-adapting, knowledge-driven expert Intrusion Detection System able to detect attacks in real time across a wide range of IoT systems. Kalis does not require changes to existing IoT software, can monitor a wide variety of protocols, has no performance impact on applications on IoT devices, and enables collaborative security scenarios. Kalis is the first comprehensive approach to intrusion detection for IoT that does not target individual protocols or applications, and adapts the detection strategy to the specific network features. Extensive evaluation shows that Kalis is effective and efficient in detecting attacks to IoT systems.
[Performance evaluation, Protocols, IoT systems, knowledge-driven adaptable intrusion detection, intrusion detection, self-adapting intrusion detection, Internet of Things, Kalis, IoT, knowledge-driven, security of data, internet of things, Intrusion detection, Feature extraction, Software, Monitoring]
Fuzzy Extractors for Biometric Identification
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Fuzzy extractor provides key generation from biometrics and other noisy data. The generated key is seamlessly usable for any cryptographic applications because its information entropy is sufficient for security. Biometric authentication offers natural and passwordless user authentication in various systems where fuzzy extractors can be used for biometric information security. Typically, a biometric system operates in two modes: verification and identification. However, existing fuzzy extractors does not support efficient user identification. In this paper, we propose a succinct fuzzy extractor scheme which enables efficient biometric identification as well as verification that it satisfies the security requirements. We show that the proposed scheme can be easily used in both verification and identification modes. To the best of our knowledge, we propose the first fuzzy extractor based biometric identification protocol. The proposed protocol is able to identify a user with constant computational cost rather than linear-time computation required by other fuzzy extractor schemes. We also provide security analysis of proposed schemes to show their security levels. The implementation shows that the performance of proposed identification protocol is constant and it is close to that of verification protocols.
[Protocols, biometric identification protocol, Biometrics (access control), cryptographic protocols, cryptographic applications, fuzzy set theory, fuzzy extractors, Data mining, Servers, Noise measurement, biometrics (access control), passwordless user authentication, verification mode, identification mode, formal verification, Authentication, message authentication, authorisation, biometric information security]
Smartphone Privacy Leakage of Social Relationships and Demographics from Surrounding Access Points
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
While the mobile users enjoy the anytime anywhere Internet access by connecting their mobile devices through Wi-Fi services, the increasing deployment of access points (APs) have raised a number of privacy concerns. This paper explores the potential of smartphone privacy leakage caused by surrounding APs. In particular, we study to what extent the users' personal information such as social relationships and demographics could be revealed leveraging simple signal information from APs without examining the Wi-Fi traffic. Our approach utilizes users' activities at daily visited places derived from the surrounding APs to infer users' social interactions and individual behaviors. Furthermore, we develop two new mechanisms: the Closeness-based Social Relationships Inference algorithm captures how closely people interact with each other by evaluating their physical closeness and derives fine-grained social relationships, whereas the Behavior-based Demographics Inference method differentiates various individual behaviors via the extracted activity features (e.g., activeness and time slots) at each daily place to reveal users' demographics. Extensive experiments conducted with 21 participants' real daily life including 257 different places in three cities over a 6-month period demonstrate that the simple signal information from surrounding APs have a high potential to reveal people's social relationships and infer demographics with an over 90% accuracy when using our approach.
[user activities, fine-grained social relationships, closeness-based social relationship inference algorithm, Mobile communication, Mobile handsets, Privacy, demography, mobile users, Wireless fidelity, Demographics Inference, mobile radio, Wi-Fi services, Fine-grained Social Relationships Inference, access point deployment, smart phones, smartphone privacy leakage, social relationships, Smartphone Privacy Leakage, Global Positioning System, AP deployment, user personal information, Internet access, user social interactions, mobile devices, Feature extraction, Internet, wireless LAN, Wi-Fi traffic, behavior-based demographic inference method, Access Points]
EV-Matching: Bridging Large Visual Data and Electronic Data for Efficient Surveillance
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Visual (V) surveillance systems are extensively deployed and becoming the largest source of big data. On the other hand, electronic (E) data also plays an important role in surveillance and its amount increases explosively with the ubiquity of mobile devices. One of the major problems in surveillance is to determine human objects' identities among different surveillance scenes. Traditional way of processing big V and E datasets separately does not serve the purpose well because V data and E data are imperfect alone for information gathering and retrieval. Matching human objects in the two datasets can merge the good of the two for efficient large-scale surveillance. Yet such matching across two heterogeneous big datasets is challenging. In this paper, we propose an efficient set of parallel algorithms, called EV-Matching, to bridge big E and V data. We match E and V data based on their spatiotemporal correlation. The EV-Matching algorithms are implemented on Apache Spark to further accelerate the whole procedure. We conduct extensive experiments on a large synthetic dataset under different settings. Results demonstrate the feasibility and efficiency of our proposed algorithms.
[Algorithm design and analysis, Visualization, visual images, Electronic data, human object matching, Videos, Matching, mobile computing, electronic data, Sensors, big data, video surveillance, Heterogenous, visual data, parallel algorithms, information retrieval, mobile device ubiquity, Big Data, image matching, Apache Spark, Surveillance, visual surveillance systems, human object identities, Cameras, EV-matching algorithms, Spark, information gathering, Visual data]
Adaptive Reconnaissance Attacks with Near-Optimal Parallel Batching
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In assessing privacy on online social networks, it is important to investigate their vulnerability to reconnaissance strategies, in which attackers lure targets into being their friends by exploiting the social graph in order to extract victims' sensitive information. As the network topology is only partially revealed after each successful friend request, attackers need to employ an adaptive strategy. Existing work only considered a simple strategy in which attackers sequentially acquire one friend at a time, which causes tremendous delay in waiting for responses before sending the next request, and which lack the ability to retry failed requests after the network has changed. In contrast, we investigate an adaptive and parallel strategy, of which attackers can simultaneously send multiple friend requests in batch and recover from failed requests by retrying after topology changes, thereby significantly reducing the time to reach the targets and greatly improving robustness. We cast this approach as an optimization problem, Max-Crawling, and show it inapproximable within (1 - 1/e + &#x03B5;). We first design our core algorithm P<sub>M</sub>-AReST which has an approximation ratio of (1 - e-(1-1/e)) using adaptive monotonic submodular properties. We next tighten our algorithm to provide a nearoptimal solution, i.e. having a ratio of (1 - 1/e), via a two-stage stochastic programming approach. We further establish the gap bound of (1 - e-(1-1/e)2) between batch strategies versus the optimal sequential one. We experimentally validate our theoretical results, finding that our algorithm performs nearoptimally in practice and that this is robust under a variety of problem settings.
[Algorithm design and analysis, optimization problem, graph theory, Target Reconnaissance Attacks, stochastic programming, social graph, online social networks, near-optimal parallel batching, Optimization, Adaptive Approximation Algorithms, adaptive reconnaissance attacks, Privacy, Network topology, Social Networks Analysis, Reconnaissance, approximation ratio, Max-Crawling, Social network services, two-stage stochastic programming approach, Topology, network topology, Adaptive Stochastic Optimization, adaptive monotonic submodular properties, social networking (online), data privacy]
Achieving Strong Privacy in Online Survey
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Thanks to the proliferation of Internet access and modern digital and mobile devices, online survey has been flourishing into data collection of marketing, social, financial and medical studies. However, traditional data collection methods in online survey suffer from serious privacy issues. Existing privacy protection techniques are not adequate for online survey for lack of strong privacy. In this paper, we propose a practical strong privacy online survey scheme SPS based on a novel data collection technique called dual matrix masking (DM2), which guarantees the correctness of the tallying results with low computation overhead, and achieves universal verifiability, robustness and strong privacy. We also propose a more robust scheme RSPS, which incorporates multiple distributed survey managers. The RSPS scheme preserves the nice properties of SPS, and further achieves robust strong privacy against joint collusion attack. Through extensive analyses, we demonstrate our proposed schemes can be efficiently applied to online survey with accuracy and strong privacy.
[Data privacy, joint collusion attack, digital devices, DM2, Mobile handsets, universal verifiability, Privacy, strong privacy online survey scheme, mobile computing, low computation overhead, security of data, Internet access, Organizations, Data collection, mobile devices, multiple distributed survey managers, Robustness, Software, data privacy, Internet, RSPS scheme, dual matrix masking, data collection technique]
Service Overlay Forest Embedding for Software-Defined Cloud Networks
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Network Function Virtualization (NFV) on Software-Defined Networks (SDN) can effectively optimize the allocation of Virtual Network Functions (VNFs) and the routing of network flows simultaneously. Nevertheless, most previous studies on NFV focus on unicast service chains and thereby are not scalable to support a large number of destinations in multicast. On the other hand, the allocation of VNFs has not been supported in the current SDN multicast routing algorithms. In this paper, therefore, we make the first attempt to tackle a new challenging problem for finding a service forest with multiple service trees, where each tree contains multiple VNFs required by each destination. Specifically, we formulate a new optimization, named Service Overlay Forest (SOF), to minimize the total cost of all allocated VNFs and all multicast trees in the forest. We design a new 3&#x03C1;<sub>ST</sub>-approximation algorithm to solve the problem, where &#x03C1;<sub>ST</sub> denotes the best approximation ratio of the Steiner Tree problem, and the distributed implementation of the algorithm is also presented. Simulation results on real networks for data centers manifest that the proposed algorithm outperforms the existing ones by over 25%. Moreover, the implementation of an experimental SDN with HP OpenFlow switches indicates that SOF can significantly improve the QoE of the Youtube service.
[Algorithm design and analysis, Steiner trees, approximation ratio, Cloud computing, network function virtualization, trees (mathematics), software-defined cloud networks, software defined networking, Routing, virtualisation, Steiner tree problem, SOF, Unicast, multicast trees, 3&#x03C1;<sub>ST</sub>-approximation algorithm, HP OpenFlow switches, Vegetation, service trees, Approximation algorithms, service overlay forest, cloud computing, SDN, virtual network functions, NFV]
Joint Optimization of Chain Placement and Request Scheduling for Network Function Virtualization
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Compared with executing Network Functions (NFs) on dedicated hardwares, the recent trend of Network Function Virtualization (NFV) holds the promise for operators to flexibly deploy software-based NFs on commodity servers. However, virtual NFs (VNFs) are normally "chained" together to provide a specific network service. Thus, an efficient scheme is needed to place the VNF chains across the network and effectively schedule requests to service instances, which can maximize the average resource utilization of each node in service and simultaneously minimize the average response latency of each request. To this end, we formulate first VNF chains placement problem as a variant of bin-packing problem, which is NP-hard, and we model request scheduling problem based on the key concepts from open Jackson network. To jointly optimize the performance of NFV, we propose a priority-driven weighted algorithm to improve resource utilization and a heuristic algorithm to reduce response latency. Through extensive trace-driven simulations, we show that our methods can indeed enhance performance in diverse scenarios. In particular, we can improve the average resource utilization by 33.4% and can reduce the average total latency by 19.9% as compared with the state-of-the-art methods.
[joint optimization, Response Latency, Schedules, chain placement, NP-hard, Heuristic algorithms, Packet loss, commodity servers, open Jackson network, Chain Placement, virtualisation, Open Jackson Network, VNFs, bin packing, Network Function Virtualization (NFV), optimisation, resource allocation, scheduling, bin-packing problem, Mathematical model, resource utilization, NFV, queueing theory, network function virtualization, Computational modeling, request scheduling, software defined networking, virtual NFs, computer centres, Processor scheduling, Resource management, Request Scheduling, computational complexity]
BIG Cache Abstraction for Cache Networks
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In this paper, we advocate the notion of "BIG" cache as an innovative abstraction for effectively utilizing the distributed storage and processing capacities of all servers in a cache network. The "BIG" cache abstraction is proposed to partly address the problem of (cascade) thrashing in a hierarchical network of cache servers, where it has been known that cache resources at intermediate servers are poorly utilized, especially under classical cache replacement policies such as LRU. We lay out the advantages of "BIG" cache abstraction and make a strong case both from a theoretical standpoint as well as through simulation analysis. We also develop the dCLIMB cache algorithm to minimize the overheads of moving objects across distributed cache boundaries and present a simple yet effective heuristic for addressing the cache allotment problem in the design of "BIG" cache abstraction.
[Algorithm design and analysis, dCLIMB cache algorithm, network servers, hierarchical network, distributed storage, dCLIMB, Cache storage, cache storage, Servers, Hierarchical Caching, LRU, BIG Cache, intermediate servers, cache allotment problem, moving object overheads, Bandwidth, cache servers, distributed cache boundaries, big cache abstraction, Content Network Distribution, cache resources, Standards, classical cache replacement policy, cache networks, Caching, Cache Replacement Policies, Streaming media, Resource management]
Distributed QR Decomposition Framework for Training Support Vector Machines
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Support Vector Machines (SVM) belong to a class of supervised machine learning algorithms with applications in classification and regression analysis. SVM training is modeled as a convex optimization problem that is computationally tedious and has large memory requirements. Specifically, it is a quadratic programming problem which scales rapidly with the training set size rather than the dimensionality of the feature space. In this work, we first present a novel QR decomposition framework (QRSVM) to efficiently model and solve a large scale SVM problem by capitalizing on low-rank representations of the full kernel matrix rather than solving the problem as a sequence of smaller sub-problems. The low-rank structure of the kernel matrix is leveraged to transform the dense matrix into one with a sparse and separable structure. The modified SVM problem requires significantly lesser memory and computation. Our approach scales linearly with the training set size which makes it applicable to large datasets. This motivates towards our another contribution; exploring a distributed QRSVM framework to solve large-scale SVM classification problems in parallel across a cluster of computing nodes. We also derive an optimal step size for fast convergence of the dual ascent method which is used to solve the quadratic programming problem.
[quadratic programming problem, regression analysis, matrix decomposition, dual ascent method, Sparse matrices, Training, SVM training, QRSVM, feature space dimensionality, Static VAr compensators, full kernel matrix, Kernel, large-scale SVM classification, pattern classification, Symmetric matrices, support vector machines, supervised machine learning algorithms, optimal step size, dense matrix transformation, low-rank representations, convex programming, Matrix decomposition, quadratic programming, Support vector machines, distributed QR decomposition framework, pattern clustering, support vector machine training, convex optimization, modified SVM problem]
Distributively Computing Random Walk Betweenness Centrality in Linear Time
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Betweenness centrality of a node represents its influence over the spread of information in the network. It is normally defined as the ratio of the number of shortest paths passing through the node among all shortest paths. However, the spread of information may not just pass through the shortest paths which is captured by a new measure of betweenness centrality based on random walks [1]. The random walk betweenness centrality of a node means how often it is traversed by a random walk between all pairs of other nodes. In this paper, we propose an O(n log n) time distributed randomized approximation algorithm for calculating each node's random walk betweenness centrality with an approximation ratio (1-&#x03F5;) where n is the number of nodes and &#x03F5; is an arbitrarily small constant between 0 and 1. Our distributed algorithm is designed under the widely used CONGEST model, where each edge can only transfer O(log n) bits in each round. To our best knowledge, this is the first distributed algorithm for computing the random walk betweenness centrality. Moreover, we give a non-trivial lower bound for distributively computing the exact random walk betweenness centrality under the CONGEST model, which is &#x03A9;(n\\log n +D) where D is the network diameter. This means exactly computing random walk betweenness cannot be done in sublinear time.
[Algorithm design and analysis, approximation theory, Computational modeling, Distributed Algorithm, Probability, O(n log n) time distributed randomized approximation algorithm, Communication Complexity, Distributed computing, Random Walk Betweenness Centrality, random walk betweenness centrality, randomised algorithms, distributed algorithms, Approximation algorithms, stochastic processes, Distributed algorithms, Time complexity, CONGEST model, CONGEST Model, linear time]
DeGPar: Large Scale Topic Detection Using Node-Cut Partitioning on Dense Weighted Graphs
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Topic Detection (TD) refers to automatic techniques for locating topically related material in web documents. Nowadays, massive amounts of documents are generated by users of Online Social Networks (OSNs), in form of very short text, tweets and snippets of news. While topic detection, in its traditional form, is applied to a few documents containing a lot of information, the problem has now changed to dealing with massive number of documents with very little information. The traditional solutions, thus, fall short either in scalability (due to huge number of input items) or sparsity (due to insufficient information per input item). In this paper we address the scalability problem by introducing an efficient and scalable graph based algorithm for TD on short texts, leveraging dimensionality reduction and clustering techniques. We first, compress the input set of documents into a dense graph, such that frequent cooccurrence patterns in the documents create multiple dense topological areas in the graph. Then, we partition the graph into multiple dense sub-graphs, each representing a topic. We compare the accuracy and scalability of our solution with two state-of-the-art solutions (including the standard LDA, and BiTerm). The results on two widely used benchmark datasets show that our algorithm not only maintains a similar or better accuracy, but also performs by an order of magnitude faster than the state-of-the-art approaches.
[text analysis, DeGPar, Random Indexing, Scalability, graph theory, short texts, Topic Detection, online social networks, Twitter, Distributed Algorithms, data reduction, dimensionality reduction, Dimensionality Reduction, Semantics, topic detection, Web documents, graph based algorithm, graph partitioning, Dense Weighted Graph Partitioning, clustering techniques, Online Social Networks, Partitioning algorithms, OSN, Standards, node-cut partitioning, pattern clustering, social networking (online), scalability problem, Node-cut Graph Partitioning, Indexing, dense weighted graphs]
Networked Stochastic Multi-armed Bandits with Combinatorial Strategies
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In this paper, we investigate a largely extended version of classical MAB problem, called networked combinatorial bandit problems. In particular, we consider the setting of a decision maker over a networked bandits as follows: each time a combinatorial strategy, e.g., a group of arms, ischosen, and the decision maker receives a rewardresulting from her strategy and also receives a side bonusresulting from that strategy for each arm's neighbor. This is motivated by many real applications such as on-line social networks where friends can provide their feedback on shared content, therefore if we promote a product to a user, we can also collect feedback from her friends on that product. To this end, we consider two types of side bonus in this study: side observation and side reward. Upon the number of arms pulled at each time slot, we study two cases: single-play and combinatorial-play. Consequently, this leaves us four scenarios to investigate in the presence of side bonus: Single-play with Side Observation, Combinatorial-play with Side Observation, Single-play with Side Reward, and Combinatorial-play with Side Reward. For each case, we present and analyze a series of zero regret polices where the expect of regret over time approaches zero as time goes to infinity. Extensive simulations validate the effectiveness of our results.
[Correlation, combinatorial mathematics, Social network services, Conferences, combinatorial strategies, online social networks, MAB problem, Distributed computing, combinatorial-play, Standards, Computer science, decision making, side bonus, Nickel, networked stochastic multi-armed bandits, side observation, zero regret polices, decision maker, side reward, single-play]
Computability of Perpetual Exploration in Highly Dynamic Rings
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
We consider systems made of autonomous mobile robots evolving in highly dynamic discrete environment i.e., graphs where edges may appear and disappear unpredictably without any recurrence, stability, nor periodicity assumption. Robots are uniform (they execute the same algorithm), they are anonymous (they are devoid of any observable ID), they have no means allowing them to communicate together, they share no common sense of direction, and they have no global knowledge related to the size of the environment. However, each of them is endowed with persistent memory and is able to detect whether it stands alone at its current location. A highly dynamic environment is modeled by a graph such that its topology keeps continuously changing over time. In this paper, we consider only dynamic graphs in which nodes are anonymous, each of them is infinitely often reachable from any other one, and such that its underlying graph (i.e., the static graph made of the same set of nodes and that includes all edges that are present at least once over time) forms a ring of arbitrary size. In this context, we consider the fundamental problem of perpetual exploration: each node is required to be infinitely often visited by a robot. This paper analyzes the computability of this problem in (fully) synchronous settings, i.e., we study the deterministic solvability of the problem with respect to the number of robots. We provide three algorithms and two impossibility results that characterize, for any ring size, the necessary and sufficient number of robots to perform perpetual exploration of highly dynamic rings.
[Highly dynamic graphs, perpetual exploration, Protocols, deterministic solvability, Heuristic algorithms, graph theory, fully-synchronous robots, computability, Stability analysis, Topology, mobile robots, autonomous mobile robots, ring size, dynamic graphs, perpetual exploration problem, highly dynamic rings, static graph, Robot sensing systems, evolving graphs, persistent memory]
Locally Self-Adjusting Skip Graphs
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
We present a distributed self-adjusting algorithm for skip graphs that minimizes the average routing costs between arbitrary communication pairs by performing topological adaptation to the communication pattern. Our algorithm is fully decentralized, conforms to the CONGEST model (i.e. uses O(logn) bit messages), and requires O(logn) bits of memory for each node, where n is the total number of nodes. Upon each communication request, our algorithm first establishes communication by using the standard skip graph routing, and then locally and partially reconstructs the skip graph topology to perform topological adaptation. We propose a computational model for such algorithms, as well as a yardstick (working set property) to evaluate them. Our working set property can also be used to evaluate self-adjusting algorithms for other graph classes where multiple tree-like subgraphs overlap (e.g. hypercube networks). We derive a lower bound of the amortized routing cost for any algorithm that follows our model and serves an unknown sequence of communication requests. We show that the routing cost of our algorithm is at most a constant factor more than the amortized routing cost of any algorithm conforming to our computational model. We also show that the expected transformation cost for our algorithm is at most a logarithmic factor more than the amortized routing cost of any algorithm conforming to our computational model.
[Algorithm design and analysis, O(logn) bit messages, topological adaptation, Heuristic algorithms, graph theory, locally self-adjusting skip graphs, fully-decentralized algorithm, skip graph topology, average routing costs, standard skip graph routing, Distributed Systems and Algorithms, O(logn) memory bits, constant factor, Skip Graph, working set property, expected transformation cost, CONGEST model, amortized routing cost, communication requests, multiple-tree-like subgraph overlap, Computational modeling, logarithmic factor, Peer-to-peer Computing, distributed self-adjusting algorithm, telecommunication network topology, Self-Adjusting Computing, Distributed Data Structures, Routing, Data structures, Topology, communication request, telecommunication network routing, arbitrary communication pairs, communication pattern, Peer-to-peer computing, Self-Organizing Networks]
Online to Offline Business: Urban Taxi Dispatching with Passenger-Driver Matching Stability
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In the Online to Offline (O2O) taxi business (e.g., Uber), the interests of passengers, taxi drivers, and the company may not align with one another, since taxis do not belong to the company. To balance these interests, this paper studies the taxi dispatch problem for the O2O taxi business. The interests of passengers and taxi drivers are modeled. For non-sharing taxi dispatches (multiple passenger requests cannot share a taxi), a stable marriage approach is proposed. It can deal with unequal numbers of passenger requests and taxis through matching them to dummy partners. Given dummy partners, stable matchings are proved to exist. Three rules are presented to find out all possible stable matchings. For sharing taxi dispatches (multiple passenger requests can share a taxi), passenger requests are packed through solving a maximum set packing problem. Packed passenger requests are regarded as a single request for matching taxis. Extensive real data-driven experiments demonstrate how well our approach performs. The proposed algorithms have a limited performance gap to the literature in terms of the dispatch delay and the passenger satisfaction, but they significantly improve upon existing algorithms in terms of the taxi satisfaction.
[online to offline business, passenger-driver matching stability, Schedules, dispatching, O2O taxi business, urban taxi dispatching, Taxi dispatch schedule, Heuristic algorithms, Switched mode power supplies, matching stability, Companies, dispatch delay, stable marriage approach, taxi drivers, Stability analysis, sharing and non-sharing, Public transportation, passenger requests, customer satisfaction, public transport, passenger satisfaction]
An Optimization Framework for Online Ride-Sharing Markets
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Taxi services and product delivery services are instrumental for our modern society. Thanks to the emergence of sharing economy, ride-sharing services such as Uber, Didi, Lyft and Google's Waze Rider are becoming more ubiquitous and grow into an integral part of our everyday lives. However, the efficiency of these services are severely limited by the sub-optimal and imbalanced matching between the supply and demand. We need a generalized framework and corresponding efficient algorithms to address the efficient matching, and hence optimize the performance of these markets. Existing studies for taxi and delivery services are only applicable in scenarios of the one-sided market. In contrast, this work investigates a highly generalized model for the taxi and delivery services in the market economy (abbreviated as"taxi and delivery market") that can be widely used in two-sided markets. Further, we present efficient online and offline algorithms for different applications. We verify our algorithm with theoretical analysis and trace-driven simulations under realistic settings.
[Algorithm design and analysis, taxi services, Urban areas, product delivery services, marketing data processing, Surges, online algorithms, Public transportation, Vehicles, optimisation, offline algorithms, Pricing, public transport, optimization framework, Approximation algorithms, online ride-sharing markets, Internet]
Fast and Accurate Tracking of Population Dynamics in RFID Systems
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
RFID systems have been widely deployed for various applications such as supply chain management, indoor localization, inventory control, and access control. This paper deals with the fundamental problem of estimating the number of arriving and departing tags between any two time instants in dynamically changing RFID tag populations, which is needed in many applications such as warehouse monitoring and privacy sensitive RFID systems. In this paper, we propose a dynamic tag estimation scheme, namely DTE, that can achieve arbitrarily high required reliability, is compliant with the C1G2 standard, and works in single as well as multiple-reader environment. DTE uses the standardized frame slotted Aloha protocol and utilizes the number of slots that change their values in corresponding Aloha frames at the two time instants to estimate the number of arriving and departing tags. It is easy to deploy because it neither requires modification to tags nor to the communication protocol between tags and readers. We have extensively evaluated and compared DTE with the only prior scheme, ZDE, that can estimate the number of arriving and departing tags. Unfortunately, ZDE can not achieve arbitrarily high required reliability. In contrast, our proposed scheme always achieves the required reliability. For example, for a tag population containing 10 4 tags, a required reliability of 95%, and a required confidence interval of 5%, DTE takes 5.12 seconds to achieve the required reliability whereas ZDE achieves a reliability of only 66% in the same amount of time.
[dynamic tag estimation scheme, Protocols, radiofrequency identification, Tracking, Estimation, dynamically changing RFID tag populations, RFID, access protocols, multiple-reader environment, Statistics, RFID system, Mobile, Standards, standardized frame slotted Aloha protocol, population dynamics tracking, DTE, Sociology, C1G2 standard, Population, Reliability, Radiofrequency identification]
Robust Indoor Wireless Localization Using Sparse Recovery
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
With the multi-antenna design of WiFi interfaces, phased array has become a promising mechanism for accurate WiFi localization. State-of-the-art WiFi-based solutions using AoA (Angle-of-Arrival), however, face a number of critical challenges. First, their localization accuracy degrades dramatically when the Signal-to-Noise Ratio (SNR) becomes low. Second, they do not fully utilize coherent processing across all available domains. In this paper, we present ROArray, a Robust Array based system that accurately localizes a target even with low SNRs. In the spatial domain, ROArray can produce sharp AoA spectrums by parameterizing the steering vector based on a sparse grid. Then, to expand into the frequency domain, it jointly estimates the ToAs (Time-of-Arrival) and AoAs of all the paths using multi-subcarrier OFDM measurements. Furthermore, through multi-packet fusion, ROArray is enabled to perform coherent estimation across the spatial, frequency, and time domains. Such coherent processing not only increases the virtual aperture size, which enlarges the number of maximum resolvable paths, but also improves the system robustness to noise. Our implementation using off-the-shelf WiFi cards demonstrates that, with low SNRs, ROArray significantly outperforms state-of-the-art solutions in terms of localization accuracy; when medium or high SNRs are present, it achieves comparable accuracy.
[radio direction-finding, time-of-arrival estimation, WiFi interfaces, time domain, OFDM, localization accuracy, angle-of-arrival estimation, direction-of-arrival estimation, ROArray, spatial domain, frequency domain, sparse grid, coherent estimation, Phased arrays, target localization, AoA estimation, Robustness, OFDM modulation, Wireless fidelity, sharp AoA spectrums, phased array, steering vector, virtual aperture size, ToA estimation, time-domain analysis, sparse recovery, Multiple signal classification, frequency-domain analysis, AoA, multipacket fusion, robust indoor wireless localization, multiantenna design, signal-to-noise ratio, Sparse recovery, WiFi localization, multisubcarrier OFDM measurements, wireless LAN, antenna phased arrays, robust array-based system, Signal to noise ratio]
Max-Min Fair Resource Allocation in HetNets: Distributed Algorithms and Hybrid Architecture
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
We study the resource allocation problem in RAN-level integrated HetNets. This emerging HetNets paradigm allows for dynamic traffic splitting across radio access technologies for each client, and then for aggregating the traffic inside the network to improve the overall resource utilization. We focus on the max-min fair service rate allocation across the clients, and study the properties of the optimal solution. Based on the analysis, we design a low complexity distributed algorithm that tries to achieve max-min fairness. We also design a hybrid network architecture that leverages opportunistic centralized network supervision to augment the distributed solution. We analyze the performance of our proposed algorithms and prove their convergence. We also derive conditions under which the outcome is optimal. When the conditions are not satisfied, we provide constant upper and lower bounds on the optimality gap. Finally, we study the convergence time of our distributed solution and show that leveraging appropriate policies in its design significantly reduces the convergence time.
[Algorithm design and analysis, Heterogeneous Wireless Networks, hybrid architecture, convergence, convergence time, Throughput, Complexity theory, minimax techniques, telecommunication computing, Convergence, resource allocation, Convergence Time, Computer architecture, Wireless fidelity, mobile radio, Max-Min Fair Resource Allocation, Radio Access Technology, Hybrid Computation, distributed solution, HetNets, max-min fair resource allocation, distributed algorithms, Optimality Gap, Resource management, radio access]
Optimization of Full-View Barrier Coverage with Rotatable Camera Sensors
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In all the researches in wireless sensor networks, cameras are increasingly utilized for their surveillance capabilities. In this paper, we elaborately discuss about the problem of Full-View Barrier Coverage with Rotatable Camera Sensors (FBR), including weakly and strongly connected versions. FBR is proven to be NP-hard in this paper by reducing Group Steiner Tree problem to it. Our goal is to reduce sensor number when guaranteeing the surveillance capabilities at the same time. Correspondingly, we introduce a novel weighed graph structure called Full-View Barrier Graph. We transform weak version problem into a pseudo one-dimension one and propose W-GraProj algorithm with the help of dynamic programming; in strong version problem, we introduce two centralized algorithms (S-Dijkstra, S-Thorup), respectively aiming to save sensor num-ber and to reduce time complexity. Moreover, we rigorously analyze the correctness and time complexity for each algorithm. In addition, the mass number of experiments are conducted to validate the efficiency of all algorithms, which prove that our structures and algorithms can construct a full-view barrier with fewer camera sensors compared with previous researches.
[Algorithm design and analysis, rotatable camera sensors, NP-hard, wireless sensor networks, Heuristic algorithms, graph theory, barrier coverage, full-view coverage, Complexity theory, W-GraProj algorithm, cameras, FBR, full-view barrier graph, Sensors, S-Dijkstra, dynamic programming, time complexity, visual net- work, full-view barrier coverage, group Steiner tree problem reduction, centralized algorithms, S-Thorup, sensors, Surveillance, weighed graph structure, Cameras, surveillance capabilities, computational complexity]
Communication through Symbol Silence: Towards Free Control Messages in Indoor WLANs
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Efficient design of wireless networks benefits from the exchange of control messages. However, control message itself consumes scarce channel resources. In this paper, we propose CoS (Communication through symbol Silence), a novel communication strategy that conveys control messages for free without consuming extra channel resources. CoS inserts silence symbols in data packets and leverages the intervals between inserted silence symbols to encode information. The silence symbols can be located by energy detection at the granularity of symbols and the intervals are interpreted into transmitted control messages. Based on our key insights that the channel code is under-utilized in current wireless networks and the distribution of symbol errors within a data packet is predictable in indoor wireless transmissions, the symbols erased by silence symbols are recovered by the coding redundancy that is originally used to correct symbol errors. A rate adaptation scheme is designed to dynamically adjust the rate of free control messages according to channel conditions so that the transmission of free control messages does not harm the original data throughput. We implement CoS on our software defined radio platform to validate the feasibility of CoS. The extensive results show that the control messages are delivered with close to 100% accuracy in a large SNR range. In addition, we measure the achievable capacity of free control messages in various channel conditions.
[channel code, OFDM, channel coding, Bit error rate, CoS, rate adaptation scheme, communication strategy, Wireless communication, coding redundancy, software radio, data throughput, indoor wireless transmissions, Fading channels, free control messages, Receivers, communication through symbol silence, wireless networks, transmitted control messages, software defined radio, control message, symbol errors, indoor WLAN, channel resources, energy detection, data packets, Software, wireless LAN, Signal to noise ratio]
Secure Connectivity of Wireless Sensor Networks Under Key Predistribution with on/off Channels
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Security is an important issue in wireless sensor networks (WSNs), which are often deployed in hostile environments. The q-composite key predistribution scheme has been recognized as a suitable approach to secure WSNs. Although the q-composite scheme has received much attention in the literature, there is still a lack of rigorous analysis for secure WSNs operating under the q-composite scheme in consideration of the unreliability of links. One main difficulty lies in analyzing the network topology whose links are not independent. Wireless links can be unreliable in practice due to the presence of physical barriers between sensors or because of harsh environmental conditions severely impairing communications. In this paper, we resolve the difficult challenge and investigate k-connectivity in secure WSNs operating under the q-composite scheme with unreliable communication links modeled as independent on/off channels, where k-connectivity ensures connectivity despite the failure of any (k - 1) sensors or links, and connectivity means that any two sensors can find a path in between for secure communication. Specifically, we derive the asymptotically exact probability and a zero-one law for k-connectivity. We further use the theoretical results to provide design guidelines for secure WSNs. Experimental results also confirm the validity of our analytical findings.
[radio links, telecommunication security, key predistribution, wireless sensor networks, k-connectivity, telecommunication network topology, WSNs, sensor networks, q-composite key predistribution scheme, Security, network topology, secure connectivity, wireless links, rigorous analysis, secure communication, Wireless sensor networks, Upper bound, connectivity, Memory management, Silicon, Sensors, Channel models, link unreliability, on-off channels]
iUpdater: Low Cost RSS Fingerprints Updating for Device-Free Localization
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
While most existing indoor localization techniques are device-based, many emerging applications such as intruder detection and elderly monitoring drive the needs of device-free localization, in which the target can be localized without any device attached. Among the diverse techniques, received signal strength (RSS) fingerprint-based methods are popular because of the wide availability of RSS readings in most commodity hardware. However, current fingerprint-based systems suffer from high human labor cost to update the fingerprint database and low accuracy due to the large degree of RSS variations. In this paper, we propose a fingerprint-based device-free localization system named iUpdater to significantly reduce the labor cost and increase the accuracy. We present a novel self-augmented regularized singular value decomposition (RSVD) method integrating the sparse attribute with unique properties of the fingerprint database. iUpdater is able to accurately update the whole database with RSS measurements at a small number of reference locations, thus reducing the human labor cost. Furthermore, iUpdater observes that although the RSS readings vary a lot, the RSS differences between both the neighboring locations and adjacent wireless links are relatively stable. This unique observation is applied to overcome the short-term RSS variations to improve the localization accuracy. Extensive experiments in three different environments over 3 months demonstrate the effectiveness and robustness of iUpdater.
[cost reduction, fingerprint identification, indoor navigation, received signal strength fingerprint, device-free localization, Fingerprint recognition, RSVD, indoor localization, RSS fingerprint, Matrix decomposition, Sparse matrices, database management systems, regularized singular value decomposition, Wireless communication, fingerprint database, Microwave integrated circuits, RSSI, Databases, iUpdater, indoor radio, wireless LAN, human labor cost reduction, Wireless fidelity, singular value decomposition]
Influence Maximization in a Many Cascades World
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Online Social Networks (OSNs) are widely utilized in viral marketing campaigns exploiting the word-of-mouth effect. Various propagation models have been proposed to describe the way cascades unfold in OSNs. Based on the existing propagation models, several studies address the problem of influence maximization, where the objective is to identify an appropriate subset of users to initiate the spread of a contagion. However, existing approaches ignore an important factor in the propagation process, i.e., the correlation of multiple contagions simultaneously cascading in the social network and how these affect the users' decisions regarding the adoption of a contagion. Although recent works look into either the competition or the complementarity among a pair of contagions, a uniform model that describes the propagation of multiple cascades with varying types and degrees of correlations is lacking. This work constitutes the first attempt to fill this gap. We formulate a novel propagation model, the Correlated Contagions Dynamic Linear Threshold (CCDLT), that considers the correlation of many contagions in either competitive or complementary manner. Our proposed model allows for different degrees of competition/complementarity among cascades. We further consider that users may dynamically switch states regarding the contagion they promote during the propagation process, based on the influence of their neighborhoods. We then design a greedy seed selection algorithm that identifies the appropriate subset of users to participate in a specific contagion in order to maximize its spread and we formally prove that it approximates the best solution at a ratio of 1 - 1/e. Through an extensive experimental evaluation we demonstrate the superiority of our approach over existing schemes.
[Correlation, correlated contagions dynamic linear threshold, Biological system modeling, greedy algorithms, greedy seed selection algorithm, Diffusion processes, social networks, online social networks, Twitter, marketing data processing, OSN, correlated contagions, optimisation, viral marketing campaigns, Tagging, social networking (online), influence maximization, Informatics, word-of-mouth effect]
Expertise-Aware Truth Analysis and Task Allocation in Mobile Crowdsourcing
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Mobile crowdsourcing has received considerable attention as it enables people to collect and share large volume of data through their mobile devices. Since the accuracy of the collected data is usually hard to ensure, researchers have proposed techniques to identify truth from noisy data by inferring and utilizing the reliability of users, and allocate tasks to users with higher reliability. However, they neglect the fact that a user may only have expertise on some problems (in some domains), but not others. Neglecting this expertise diversity may cause two problems: low estimation accuracy in truth analysis and ineffective task allocation. To address these problems, we propose an Expertise-aware Truth Analysis and Task Allocation (ETA2) approach, which can effectively infer user expertise and then allocate tasks and estimate truth based on the inferred expertise. ETA2 relies on a novel semantic analysis method to identify the expertise domains of the tasks and user expertise, an expertise-aware truth analysis solution to estimate truth and learn user expertise, and an expertise-aware task allocation method to maximize the probability that tasks are allocated to users with the right expertise while ensuring the work load does not exceed the processing capability at each user. Experimental results based on two real-world datasets demonstrate that ETA2 significantly outperforms existing solutions.
[mobile crowdsourcing, probability maximization, semantic analysis method, human factors, reliability, Task allocation, Gaussian distribution, Mobile communication, sensor fusion, Servers, task analysis, mobile computing, Semantics, Truth analysis, estimation accuracy, user expertise learning, Crowdsourcing, user reliability, probability, expertise diversity, Mobile crowdsourcing, noisy data, mobile devices, expertise-aware truth analysis and task allocation, data handling, Resource management, Reliability, ETA2 approach]
MELODY: A Long-Term Dynamic Quality-Aware Incentive Mechanism for Crowdsourcing
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Crowdsourcing allows requesters to allocate tasks to a group of workers on the Internet to make use of their collective intelligence. Quality control is a key design objective in incentive mechanisms for crowdsourcing as requesters aim at obtaining answers of high quality under a given budget. However, when measuring workers' long-term quality, existing mechanisms either fail to utilize workers' historical information, or treat workers' quality as stable and ignore its temporal characteristics, hence performing poorly in a long run. In this paper we propose MELODY, a long-term dynamic quality-aware incentive mechanism for crowdsourcing. MELODY models interaction between requesters and workers as reverse auctions that run continuously. In each run of MELODY, we design a truthful, individual rational, budget feasible and quality-aware algorithm for task allocation with polynomial-time computation complexity and O(1) performance ratio. Moreover, taking into consideration the long-term characteristics of workers' quality, we propose a novel framework in MELODY for quality inference and parameters learning based on Linear Dynamical Systems at the end of each run, which takes full advantage of workers' historical information and predicts their quality accurately. Through extensive simulations, we demonstrate that MELODY outperforms existing work in terms of both quality estimation (reducing estimation error by 17.6% ~ 24.2%) and social performance (increasing requester's utility by 18.2% ~ 46.6%) in long-term scenarios.
[Crowdsourcing, parameters learning, quality inference, crowdsourcing, Heuristic algorithms, quality control, Estimation, Mechanical factors, performance ratio, incentive schemes, collective intelligence, reverse auctions, linear dynamical systems, task allocation, polynomial-time computation complexity, Quality control, feasible algorithm, MELODY models, long-term dynamic quality-aware incentive mechanism, Inference algorithms, Resource management, learning (artificial intelligence), computational complexity]
The Strong Link Graph for Enhancing Sybil Defenses
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The sybil problem is a fundamental problem in distributed systems and online social networks (OSNs). The basic problem is that an attacker can easily create multiple identities in a distributed or open online system. Popular and effective sybil defenses are usually based on properties of the network structure. However, most defenses assume that it is hard for the attacker to make many connections to honest users. However, this assumption can be invalid in real OSNs which decreases the effectiveness of many sybil defenses. We propose a graph transformation, the strong link graph, to mitigate such attacks by reducing the effect ofa large number of attack edges. Our preliminary experiments show indeed that when the attacker has many attack edges, existing algorithms such as SybilLimit, SybilRank and Gatekeeper are ineffective. After the strong link graph is applied, it deletes many of the attack edges, restoring the effectiveness of the sybil defenses.
[Image edge detection, Conferences, graph theory, online social networks, OSN, link graph, attack edges, social network, sybil attack, graph transformation, security of data, Open systems, Logic gates, distributed systems, Facebook, Detection algorithms, network structure, Sybil defenses]
Mechanism Design for Mobile Crowdsensing with Execution Uncertainty
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Mobile crowdsensing has emerged as a promising paradigm for data collection due to increasingly pervasive and powerful mobile devices. There have been extensive research works that propose incentive mechanisms for crowdsensing, but they all make the assumption that mobile users will positively complete the allocated sensing tasks. In this paper, we consider a new and practical scenario of crowdsensing, where a user may fail to complete the task with a certain probability. It is an important and emerging issue for the incentive mechanisms to ensure fault tolerance for each sensing task under such unreliable scenarios. We design reverse auctions to model the strategic interaction between the platform and mobile users, in which users' probability of success and cost to perform the tasks are private information. Considering the task execution uncertainty, the goal of the auction mechanism is to minimize the social cost of user recruitment, while guaranteeing the tasks to be completed with high probability. We prove that minimizing the social cost is an NP-hard problem, and design computationally efficient mechanisms that achieve good approximation ratio and economic-robust properties, e.g., strategy-proofness. We conduct extensive simulations to evaluate the performance of our mechanisms based on a real data set. The evaluation results show that our mechanisms outperform the heuristic algorithm and approach to the optimal solution.
[Uncertainty, task execution uncertainty, Mobile communication, sensor fusion, Mobile handsets, Recruitment, mobile computing, strategic interaction, user recruitment, Mobile Crowdsensing, design, mobile crowdsensing, Silicon, Sensors, incentive mechanisms design, fault tolerance, probability, software fault tolerance, NP-hard problem, social cost minimization, Resource management, minimisation, reverse auctions design, Mechanism Design, computational complexity, Fault Tolerance]
Towards Scalable and Dynamic Social Sensing Using A Distributed Computing Framework
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
With the rapid growth of online social media and ubiquitous Internet connectivity, social sensing has emerged as a new crowdsourcing application paradigm of collecting observations (often called claims) about the physical environment from humans or devices on their behalf. A fundamental problem in social sensing applications lies in effectively ascertaining the correctness of claims and the reliability of data sources without knowing either of them a priori, which is referred to as truth discovery. While significant progress has been made to solve the truth discovery problem, some important challenges have not been well addressed yet. First, existing truth discovery solutions did not fully solve the dynamic truth discovery problem where the ground truth of claims changes over time. Second, many current solutions are not scalable to large-scale social sensing events because of the centralized nature of their truth discovery algorithms. Third, the heterogeneity and unpredictability of the social sensing data traffic pose additional challenges to the resource allocation and system responsiveness. In this paper, we developed a Scalable Streaming Truth Discovery (SSTD) solution to address the above challenges. In this paper, we developed a Scalable Streaming Truth Discovery (SSTD) solution to address the above challenges. In particular, we first developed a dynamic truth discovery scheme based on Hidden Markov Models (HMM) to effectively infer the evolving truth of reported claims. We further developed a distributed framework to implement the dynamic truth discovery scheme using Work Queue in HTCondor system. We also integrated the SSTD scheme with an optimal workload allocation mechanism to dynamically allocate the resources (e.g., cores, memories) to the truth discovery tasks based on their computation requirements. We evaluated SSTD through real world social sensing applications using Twitter data feeds. The evaluation results on three real-world data traces (i.e., Boston Bombing, Paris Shooting and College Football) show that the SSTD scheme is scalable and outperforms the state-of-the-art truth discovery methods in terms of both effectiveness and efficiency.
[Scalability, Twitter, Truth Discovery, scalable streaming truth discovery, ubiquitous computing, Twitter data feeds, hidden Markov models, scalable dynamic social sensing, Sensors, Hidden Markov Model, Crowdsourcing, ubiquitous Internet connectivity, Aerodynamics, HMM, optimal workload allocation mechanism, dynamic truth discovery scheme, distributed computing framework, Control Theory, Socical Sensing, Hidden Markov models, SSTD, social networking (online), Distributed Computing, Reliability, online social media]
Phoenix: A Constraint-Aware Scheduler for Heterogeneous Datacenters
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Today's datacenters are increasingly becoming diverse with respect to both hardware and software architectures in order to support a myriad of applications. These applications are also heterogeneous in terms of job response times and resource requirements (eg., Number of Cores, GPUs, Network Speed) and they are expressed as task constraints. Constraints are used for ensuring task performance guarantees/Quality of Service(QoS) by enabling the application to express its specific resource requirements. While several schedulers have recently been proposed that aim to improve overall application and system performance, few of these schedulers consider resource constraints across tasks while making the scheduling decisions. Furthermore, latencycritical workloads and short-lived jobs that typically constitute about 90% of the total jobs in a datacenter have strict QoS requirements, which can be ensured by minimizing the tail latency through effective scheduling. In this paper, we propose Phoenix, a constraint-aware hybrid scheduler to address both these problems (constraint awareness and ensuring low tail latency) by minimizing the job response times at constrained workers. We use a novel Constraint Resource Vector (CRV) based scheduling, which in turn facilitates reordering of the jobs in a queue to minimize tail latency. We have used the publicly available Google traces to analyze their constraint characteristics and have embedded these constraints in Cloudera and Yahoo cluster traces for studying the impact of traces on system performance. Experiments with Google, Cloudera and Yahoo cluster traces across 15,000 worker node cluster shows that Phoenix improves the 99th percentile job response times on an average by 1.9&#x00D7; across all three traces when compared against a state-of-the-art hybrid scheduler. Further, in comparison to other distributed scheduler like Hawk, it improves the 90th and 99th percentile job response times by 4.5&#x00D7; and 5&#x00D7; respectively.
[Cloud computing, software architectures, CRV, tail latency minimization, constraint-aware hybrid scheduler, Resource Management, software architecture, resource allocation, QoS, scheduling, Phoenix, Hardware, hardware architectures, Google, resource constraints, heterogeneous data centers, resource requirements, Hybrid, Scheduling, quality of service, constraint resource vector, computer centres, Heterogeneous Data Center, Constraint-aware, Delays, Time factors, Performance]
Dual Scaling VMs and Queries: Cost-Effective Latency Curtailment
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Wimpy virtual instances equipped with small numbers of cores and RAM are popular public and private cloud offerings because of their low cost for hosting applications. The challenge is how to run latency-sensitive applications using such instances, which trade off performance for cost. In this study, we analytically and experimentally show that simultaneously scaling resources at coarse granularity and workloads, i.e., submitting multiple query clones to different servers, at fine granularity can overcome the performance disadvantages of wimpy VM instances and achieve stringent latency targets that are even lower than the average execution times of wimpy servers. To such an end, we first derive a closed-form analysis for the latency under any given VM provisioning and query replication level, considering cloning policies that can (not) terminate outstanding clones with (without) an overhead. Validated on trace-driven simulations, our analysis is able to accurately predict the latency and efficiently search for the optimal number of VMs and clones. Secondly, we develop a dual elastic scaler, DuoScale, that dynamically scales VMs and clones according to the workload dynamics so as to achieve the target latency in a cost-effective manner. The effectiveness of DuoScale lies on the observation that the application performance only scales sub-linearly with increasing vertical or horizontal resource provisioning, i.e., resources per VM or number of VMs. We evaluate DuoScale against VM-only scaling strategies via extensive trace-driven simulations as well as experimental results on a cloud test-bed. Our results show that DuoScale is able to achieve the stringent target latency by using clones on wimpy VMs with cost savings up to 50%, compared to scaling brawny VMs that have better performance at a higher unit cost.
[Cloud computing, horizontal resource provisioning, dual elastic scaler, Random access memory, dual scaling VM, Elasticity, workload dynamics, latency-sensitive applications, Servers, closed-form analysis, query processing, vertical resource provisioning, Analytical models, cloud computing, cost-effective latency curtailment, Cloning, public cloud offerings, wimpy virtual instances, multiple query clones, trace-driven simulations, virtual machines, private cloud offerings, Delays, hosting applications, DuoScale]
A Framework for Enabling Security Services Collaboration Across Multiple Domains
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Collaboration among Security Service Functions (SSF) is expected to become as essential to SECaaS (SECurity as a Service) systems as elasticity is to IaaS (Infrastructure as a Service). The virtualization opens new era in network security as new security appliances can be created on demand in appropriate places in the network. At the same time, the increasing size and diversity of attacks make it necessary to come up with new approaches for more efficient and more resilient security mechanisms. In this paper, we propose a new framework leveraging SDN (Software Defined Networking) and SFC (Service Function Chaining) to enhance the collaboration among different SSFs to mitigate large scale attacks. We describe a framework that allows SSFs from different domains to negotiate and dynamically control the amount of resources allocated for collaboration, in what we call a "best-effort" collaboration mode. This SSF collaboration framework creates a distributed mitigation system for handling large scale attacks in a dynamic and scalable manner. The efficiency and feasibility of this framework is experimentally assessed, showing that our approach incurs low overhead, increases the amount of traffic treated by SSFs and reduces the dropped traffic due to the lack of resources from the security mechanisms.
[Cloud computing, distributed processing, service function chaining, virtualisation, security service functions, Computer crime, SFC, security, resource allocation, Service Function Chaining, groupware, DDoS, cloud computing, Infrastructure as a Service, distributed mitigation system, virtualization, best-effort collaboration mode, security services collaboration, SSF, software defined networking, IaaS, Web servers, SECaaS, computer network security, Collaboration, SECurity as a Service, collaboration, Outsourcing]
Group Clustering Using Inter-Group Dissimilarities
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Various systems have natural groupings. For instance in large scale distributed system, we can have groups of virtual and/or physical devices. A system can also have groups of time series datasets collected at different time intervals. Such groups are usually characterized by multidimensional metrics (features) set. Clustering such groups using their multidimensional datasets has various applications, such as identifying different performance levels for anomaly detection and load balancing. Traditional algorithms focus on clustering a single time series dataset and not on such groups with multidimensional metrics datasets. In this paper, we present the design, implementation and analysis of two sets of group clustering algorithms. The first set is called one-to-many as it generates clusters of groups by comparing each group against all other groups. The second set of algorithms is called pairwise as it generates the clusters of groups using pairwise group dissimilarity matrix. Both sets of algorithms first generate group dissimilarity weights using metric ranking algorithms. We implemented the group clustering algorithms by extending a well known machine learning package and using a front-end visualizer.We validated the clustering algorithms using real world datasets on the VMware vSAN product. Experimental results show that 7 out of the 8 proposed algorithms can generate expected clusters in at least 4 out of the 6 detailed experiments. In 5 out of the 6 experiments, 3 out of the 8 proposed algorithms can generate the expected clusters. One of the pairwise algorithms can generate the expected clusters in all 6 of the 6 experiments.
[Algorithm design and analysis, Performance evaluation, Machine learning algorithms, Time series analysis, front-end visualizer, group dissimilarity weights, one-to-many set, set theory, machine learning package, matrix algebra, pairwise set, pairwise group dissimilarity matrix, group anomaly detection, pattern clustering, Clustering algorithms, inter-group dissimilarities, Load management, distributed systems, metric ranking algorithms, group clustering algorithms, intergroup dissimilarities, learning (artificial intelligence), Group clustering]
Comprehensive Measurement and Analysis of the User-Perceived I/O Performance in a Production Leadership-Class Storage System
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
With the increase of the scale and intensity of the parallel I/O workloads generated by those scientific applications running on high performance computing facilities, understanding the I/O dynamics, especially the root cause of the I/O performance variability and degradation in HPC environment, have become extremely critical to the HPC community. In this paper, we run extensive I/O measuring tests on a production leadership-class storage system to capture the performance variabilities of large-scale parallel I/O. Analyzing these results and its statistic correlation revealed some valuable insights into the characteristics of the storage system and the root cause of I/O performance variability. Further, we leverage these findings and propose an I/O middleware design refactoring which can improve the performance of the parallel I/O by optimizing the data striping and placement. Our preliminary evaluation results demonstrate the proposed approach can reduce the average per-process write latency by at least 80% and the maximum per-process write latency by at least 20%.
[Measurement, input-output performance, Tools, Batteries, production leadership-class storage system, HPC environment, Servers, Middleware, parallel processing, Degradation, I/O middleware design, storage management, parallel I/O workload, data placement, user-perceived I/O performance, per-process write latency, Production, high performance computing, middleware, data striping]
On the Limits of Subsampling of Location Traces
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Location data collection at a societal scale is increasingly becoming common - examples of this are call and data detail records in telecommunication companies, GPS samples collected by car companies, and GPS samples from mobile devices in mapping companies (e.g., Google, Microsoft). Such large scale mobility datasets have applications in urban planning, network planning, surveillance, and real-time traffic estimations. This paper addresses the problem of subsampling location traces while preserving the amount of information present in such datasets. We present a novel subsampling technique that is based on a hierarchical geographical encoding mechanism (geohash), that allows for efficient spatial cluster sampling. We analyze this subsampling technique through various information theoretic measures to quantify the total "amount" of information in a dataset from a location trace perspective and evaluate these metrics in the context of two large scale mobility datasets from telecommunication companies - one is that of call detail records and the second is that of data detail records. We show that subsampling data in both these cases by as much as 75% does not significantly reduce the total amount of information, i.e. the dataset can be used similar to the original version. This paves way for the creation of better space and CPU efficient models that can support various applications reliant on collective location traces.
[Measurement, real-time traffic estimations, car companies, hierarchical geographical encoding mechanism, Companies, urban planning, data detail records, markov chain, Histograms, mobile computing, mobility datasets, Sociology, telecommunication companies, geohash, Microsoft, Trajectory, surveillance, information theory, Google, mapping companies, location data collection, Encoding, GPS samples, spatial cluster sampling, Global Positioning System, subsampling location traces, mobile devices, subsampling data, location sampling, network planning]
SOM-TC: Self-Organizing Map for Hierarchical Trajectory Clustering
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Trajectory clustering techniques help discover interesting insights from moving object data, including common routes for people and vehicles, anomalous sub-trajectories, etc. Existing trajectory clustering techniques fail to take in to account the uncertainty present in location data. In this paper, we investigate the problem of clustering trajectory data and propose a novel algorithm for clustering similar full and sub-trajectories together while modeling uncertainty in this data. We describe the necessary pre-processing techniques for clustering trajectory data, namely techniques to discretize raw location data using Possible World semantics to capture the inherent uncertainty in location data, and to segment full trajectories in to meaningful sub-trajectories. As a baseline, we extend the well known K-means algorithm to cluster trajectory data. We then describe and evaluate a new trajectory clustering algorithm, SOM-TC (Self-Organizing Map Based Trajectory Clustering), that is inspired from the self-organizing map technique and is at least 4x faster than the baseline K-means and current density based clustering approaches.
[Algorithm design and analysis, Uncertainty, Urban areas, discretize raw location data, possible world semantics, Self-organizing feature maps, self organizing map, K-means algorithm, self-organizing map based trajectory clustering, pattern clustering, SOM-TC, hierarchical trajectory clustering technique, self-organising feature maps, trajectory clustering, Clustering algorithms, geohash, Data models, Trajectory]
Processing Encrypted and Compressed Time Series Data
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Numerous applications, e.g., in the industrial sector, produce large amounts of time-series data, which must be stored and made available for distributed processing. While outsourcing data storage and processing to third-party service providers offers many benefits, it raises data privacy issues. In light of this problem, techniques have been proposed to share only encrypted data with the remote service provider, yet the capability to run meaningful queries over the data is preserved. However, timeseries data is typically compressed at the server to save space, which is not easily possible when dealing with encrypted data. Moreover, data must be compressed in such a way that queries can still be executed efficiently. As a first step in this direction, we present an approach that preserves data privacy, enables compression at the server, and supports querying of the stored data. Our evaluation using realworld time-series data shows that our compression mechanism can reduce the required space drastically. Moreover, the median running time of all considered queries increases marginally, implying that compression can be introduced without sacrificing performance of query execution.
[Data privacy, data compression, query execution, query, Probabilistic logic, cryptography, time series, Encryption, Servers, encrypted time series data, compression mechanism, (partially) homomorphic encryption, data base, Databases, k-deterministic encryption, Data models, data privacy, compression, compressed time series data]
Calvin Constrained &#x2014; A Framework for IoT Applications in Heterogeneous Environments
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Calvin is an IoT framework for application development, deployment and execution in heterogeneous environments, that includes clouds, edge resources, and embedded or constrained resources. Inside Calvin, all the distributed resources are viewed as one environment by the application. The framework provides multi-tenancy and simplifies development of IoT applications, which are represented using a dataflow of application components (named actors) and their communication. The idea behind Calvin poses similarity with the serverless architecture and can be seen as Actor as a Service instead of Function as a Service. This makes Calvin very powerful as it does not only scale actors quickly but also provides an easy actor migration capability. In this work, we propose Calvin Constrained, an extension to the Calvin framework to cover resource-constrained devices. Due to limited memory and processing power of embedded devices, the constrained side of the framework can only support a limited subset of the Calvin features. The current implementation of Calvin Constrained supports actors implemented in C as well as Python, where the support for Python actors is enabled by using MicroPython as a statically allocated library, by this we enable the automatic management of state variables and enhance code re-usability. As would be expected, Python-coded actors demand more resources over C-coded ones. We show that the extra resources needed are manageable on current off-the-shelve micro-controller-equipped devices when using the Calvin framework.
[Cloud computing, Actuators, Ports (Computers), statically allocated library, Serverless Architecture, Frequency measurement, IoT applications, off-the-shelve microcontroller-equipped devices, Calvin constrained, IoT, Runtime, embedded devices, state variables, Sensors, cloud computing, application components, Distributed Cloud, Python-coded actors, heterogeneous environments, Dataflow Application Development Model, code reusability enhancement, Internet of Things, application development, Temperature measurement, resource-constrained devices, automatic management, distributed resources, actor as a service, MicroPython]
Privacy Preserving User-Based Recommender System
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
With the rapid development of the social networks, Collaborative Filtering (CF)-based recommender systems have been increasingly prevalent and become widely accepted by users. The CF-based techniques generate recommendations by collecting privacy sensitive data from users. Usually, the users are sensitive to disclosure of personal information and, consequently, there are unavoidable security concerns since private information can be easily misused by malicious third parties. In order to protect against breaches of personal information, it is necessary to obfuscate user information by means of an efficient encryption technique while simultaneously generating the recommendation by making true information inaccessible to service providers. Therefore, we propose a privacy preserving user-based CF technique based on homomorphic encryption, which is capable of determining similarities among users followed by generating recommendations without revealing any private information. We introduce different semi-honest parties to preserve privacy and to carry out intermediate computations for generating recommendations. We implement our method on publicly available datasets and show that our method is practical as well as achieves high level of security for users without compromising the recommendation accuracy.
[collaborative filtering, Protocols, user-based recommender system, privacy preservation, social networks, private information, privacy sensitive data, cryptography, recommendation, Encryption, Servers, encryption technique, Homomorphic encryption, personal information disclosure, Privacy, recommender systems, semihonest parties, homomorphic encryption, Collaboration, social networking (online), data privacy, CF-based recommender systems, Recommender systems, security concerns]
Privacy Preserving Optimization of Participatory Sensing in Mobile Cloud Computing
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
With the rapid growth of mobile cloud computing, participatory sensing emerges as a new paradigm to explore our physical world at an unprecedented ne granularity by recruiting the pervasive sensor-enabled smart phones. While extensive optimization has been performed in the literature to coordinate the sensing activity of the cloud-based sensing server (or platform) and the participating smart phones so as to maximize the efficiency of participatory sensing, the privacy issue in the optimization has been largely overlooked. In this paper, we propose a novel privacy-preserving optimization framework that allows both the cloud-based platform and mobile users to share data for the formulation and solution of the optimization, but without revealing sensitive information that may lead to privacy leakage of each other. Our method is built upon a privacy-preserving version of the well-known NP-hard weighted set-coverage problem. To accommodate privacy requirements in this framework, our solution uses a modified bloom filter along with a Dife-Hellman-type exchange protocol among all participants for data aggregation, sharing, and presentation. Through extensive simulation we evaluate the privacy strength of the proposed approach and also verify its effectiveness and low overhead.
[Data privacy, Cloud computing, participatory sensing, Mobile communication, Dife-Hellman-type exchange protocol, data aggregation, Synthetic aperture sonar, Optimization, cloud-based platform, Privacy, mobile cloud computing, mobile computing, optimisation, modified bloom filter, privacy strength, privacy leakage, privacy preserving optimization, cloud-based sensing server, data privacy, data structures, Sensors, mobile users, cloud computing, NP-hard weighted set-coverage problem]
SPHINX: A Password Store that Perfectly Hides Passwords from Itself
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Password managers (aka stores or vaults) allow a user to store and retrieve (usually high-entropy) passwords for her multiple password-protected services by interacting with a &#x201C;device&#x201D; serving the role of the manager (e.g., a smartphone or an online third-party service) on the basis of a single memorable (low-entropy) master password. Existing password managers work well to defeat offline dictionary attacks upon web service compromise, assuming the use of high-entropy passwords is enforced. However, they are vulnerable to leakage of all passwords in the event the device is compromised, due to the need to store the passwords encrypted under the master password and/or the need to input the master password to the device (as in smartphone managers). Evidence exists that password managers can be attractive attack targets. In this paper, we introduce a novel approach to password management, called SPHINX, which remains secure even when the password manager itself has been compromised. In SPHINX, the information stored on the device is information theoretically independent of the user's master password - an attacker breaking into the device learns no information about the master password or the user's site-specific passwords. Moreover, an attacker with full control of the device, even at the time the user interacts with it, learns nothing about the master password - the password is not entered into the device in plaintext form or in any other way that may leak information on it. Unlike existing managers, SPHINX produces strictly high-entropy passwords and makes it compulsory for the users to register these randomized passwords with the web services, hence fully defeating offline dictionary attack upon service compromise. The design and security of SPHINX is based on the device-enhanced PAKE model of Jarecki et al. that provides the theoretical basis for this construction and is backed by rigorous cryptographic proofs of security. While SPHINX is suitable for different device and online platforms, in this paper, we report on its concrete instantiation on smartphones given their popularity and trustworthiness as password managers (or even two-factor authentication). We present the design, implementation and performance evaluation of SPHINX, offering prototype browser plugins, smartphone apps and transparent device-client communication. Based on our inspection analysis, the overall user experience of SPHINX improves upon current managers. We also report on a lab-based usability study of SPHINX, which indicates that users' perception of SPHINX security and usability is high and satisfactory when compared to regular password-based authentication. Finally, we discuss how SPHINX may be extended to an online service for the purpose of back-up or as an independent password manager.
[password-protected services, Dictionaries, cryptography, Servers, password managers, master password, offline dictionary attacks, Resistance, SPHINX password store, device-enhanced PAKE model, SPHINX usability, Web services, Web service compromise, Authentication, high-entropy passwords, authorisation, SPHINX security, Cryptography, cryptographic proof]
When Smart TV Meets CRN: Privacy-Preserving Fine-Grained Spectrum Access
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Dynamic spectrum sharing techniques applied in the UHF TV band have been developed to allow secondary WiFi transmission in areas with active TV users. This technique of dynamically controlling the exclusion zone enables vastly increasing secondary spectrum re-use, compared to the "TV white space" model where TV transmitters determine the exclusion zone and only "idle" channels can be re-purposed. However, in current such dynamic spectrum sharing systems, the sensitive operation parameters of both primary TV users (PUs) and secondary users (SUs) need to be shared with the spectrum database controller (SDC) for the purpose of realizing efficient spectrum allocation. Since such SDC server is not necessarily operated by a trusted third party, those current systems might cause essential threatens to the privacy requirement from both PUs and SUs. To address this privacy issue, this paper proposes a privacy-preserving spectrum sharing system between PUs and SUs, which realizes the spectrum allocation decision process using efficient multi-party computation (MPC) technique. In this design, the SDC only performs secure computation over encrypted input from PUs and SUs such that none of the PU or SU operation parameters will be revealed to SDC. The evaluation of its performance illustrates that our proposed system based on efficient MPC techniques can perform dynamic spectrum allocation process between PUs and SUs efficiently while preserving users' privacy.
[TV, television transmitters, smart TV, Privacy-preserving, IPTV, set-top boxes, TV white space model, high definition television, trusted third party, Privacy, Transmitters, cognitive radio, idle channels, interactive television, secondary WiFi transmission, Cryptography, secondary users, radio spectrum management, digital television, multiparty computation technique, privacy-preserving fine-grained spectrum access, SDC server, Interference, TV transmitters, primary TV users, applied cryptography, dynamic spectrum sharing techniques, homomorphic encryption, UHF TV band, exclusion zone, privacy requirement, spectrum allocation decision process, TV receivers, Resource management, wireless LAN, spectrum database controller, CRN, secondary spectrum reuse, MPC technique]
Revisiting Security Risks of Asymmetric Scalar Product Preserving Encryption and Its Variants
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Cloud computing has emerged as a compelling vision for managing data and delivering query answering capability over the internet. This new way of computing also poses a real risk of disclosing confidential information to the cloud. Searchable encryption addresses this issue by allowing the cloud to compute the answer to a query based on the cipher texts of data and queries. Thanks to its inner product preservation property, the asymmetric scalar-product-preserving encryption (ASPE) has been adopted and enhanced in a growing number of works toperform a variety of queries and tasks in the cloud computingsetting. However, the security property of ASPE and its enhancedschemes has not been studied carefully. In this paper, we show acomplete disclosure of ASPE and several previously unknownsecurity risks of its enhanced schemes. Meanwhile, efficientalgorithms are proposed to learn the plaintext of data and queriesencrypted by these schemes with little or no knowledge beyondthe ciphertexts. We demonstrate these risks on real data sets.
[Cloud computing, confidential information, cryptography, Encryption, Servers, Indexes, product preservation property, query answering, ciphertexts, outsourcing, scalar product preserving encryption, ASPE, security risks, Internet, Mathematical model, cloud computing, asymmetric scalar-product-preserving encryption]
An Adversary-Centric Behavior Modeling of DDoS Attacks
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Distributed Denial of Service (DDoS) attacks are some of the most persistent threats on the Internet today. The evolution of DDoS attacks calls for an in-depth analysis of those attacks. A better understanding of the attackers' behavior can provide insights to unveil patterns and strategies utilized by attackers. The prior art on the attackers' behavior analysis often falls in two aspects: it assumes that adversaries are static, and makes certain simplifying assumptions on their behavior, which often are not supported by real attack data. In this paper, we take a data-driven approach to designing and validating three DDoS attack models from temporal (e.g., attack magnitudes), spatial (e.g., attacker origin), and spatiotemporal (e.g., attack inter-launching time) perspectives. We design these models based on the analysis of traces consisting of more than 50,000 verified DDoS attacks from industrial mitigation operations. Each model is also validated by testing its effectiveness in accurately predicting future DDoS attacks. Comparisons against simple intuitive models further show that our models can more accurately capture the essential features of DDoS attacks.
[distributed denial of service attack, DDoS attack model, Predictive models, Computer crime, computer network security, Analytical models, industrial mitigation operation, data-driven approach, adversary-centric behavior model, Feature extraction, Malware, Internet, Monitoring]
Anti-Malicious Crowdsourcing Using the Zero-Determinant Strategy
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Crowdsourcing is a promising paradigm to accomplish a complex task via eliciting services from a large group of contributors. However, recent observations indicate that the success of crowdsourcing is being threatened by the malicious behaviors of the contributors. In this paper, we analyze the malicious attack problem using an iterated prisoner's dilemma (IPD) game and propose a zero-determinant (ZD) strategy based scheme by rewarding a worker's cooperation or penalizing the defection for enticing his final cooperation. Both theoretical analysis and simulation study indicate that the proposed algorithm has two attractive characteristics: 1) the requestor can incentivize the worker to keep on cooperating by only increasing the short-term payment; and 2) the proposed algorithm is fair, so the requestor cannot arbitrarily penalize an innocent worker to increase her payoff even though she can dominate the game. To the best of our knowledge, we are the first to use the ZD strategy to stimulate both players to cooperate in an IPD game. Moreover, our proposed algorithm is not restricted to solve the problem of the malicious crowdsourcing - it can be employed to tackle any problem that can be formulated by an IPD game.
[Crowdsourcing, Economics, Algorithm design and analysis, ZD strategy based scheme, zero-determinant strategy based scheme, crowdsourcing, malicious behaviors, zero-determinant strategy, game theory, iterated prisoner dilemma game, Nash equilibrium, Optimization, Computer science, IPD game, security of data, innocent worker, antimalicious crowdsourcing, Games, malicious attack problem]
JPR: Exploring Joint Partitioning and Replication for Traffic Minimization in Online Social Networks
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
A scalable storage system becomes more important today for online social networks (OSNs) as the volume of user data increases rapidly. Key-value store uses consistent hashing to save data in a distributed manner. As a defacto standard, it has been widely used in production environments of many OSNs. However, the random nature of hashing always leads to high inter-server traffic. Recently, partitioning and replication are respectively proposed in many existing works where the former aims to minimize the inter-server read traffic and the latter aims to optimize the inter-server write traffic. Nevertheless, the separated manners of optimization cannot efficiently reduce the traffic. Because the inter-server read traffic is changed during replication. In this paper, we suggest that performing partitioning and replication simultaneously could provide probability to further optimize traffic. Then we formulate the problem as a revised graph partitioning with overlaps, since overlaps partitioning naturally corresponds to replication. To solve the problem, we propose a Joint Partitioning and Replication (JPR) scheme. Through extensive experiments with a real world Facebook trace, we evaluate that JPR significantly reduces inter-server traffic with slightly sacrificing storage cost compared to hashing, and preserves a good load balancing across servers as well.
[replication, inter-server traffic, probability, online social networks, OSN, Servers, JPR, Optimization, Standards, real world Facebook trace, Distributed databases, joint partitioning and replication scheme, Load management, social networking (online), traffic minimization, partitioning, Facebook, inter-server read traffic]
Optimizing Source Selection in Social Sensing in the Presence of Influence Graphs
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
This paper addresses the problem of choosing the right sources to solicit data from in sensing applications involving broadcast channels, such as those crowdsensing applications where sources share their observations on social media. The goal is to select sources such that expected fusion error is minimized. We assume that soliciting data from a source incurs a cost and that the cost budget is limited. Contrary to other formulations of this problem, we focus on the case where some sources influence others. Hence, asking a source to make a claim affects the behavior of other sources as well, according to an influence model. The paper makes two contributions. First, we develop an analytic model for estimating expected fusion error, given a particular influence graph and solution to the source selection problem. Second, we use that model to search for a solution that minimizes expected fusion error, formulating it as a zero-one integer non-linear programming (INLP) problem. To scale the approach, the paper further proposes a novel reliability-based pruning heuristic (RPH) and a similarity-based lossy estimation (SLE) algorithm that significantly reduce the complexity of the INLP algorithm at the cost of a modest approximation. The analytically computed expected fusion error is validated using both simulations and real-world data from Twitter, demonstrating a good match between analytic predictions and empirical measurements. It is also shown that our method outperforms baselines in terms of resulting fusion error.
[integer programming, nonlinear programming, similarity based lossy estimation (SLE) algorithm, graph theory, Twitter, RPH, INLP problem, source selection problem, Prediction algorithms, Silicon, Sensors, social media, fusion error, SLE algorithm, crowdsourcing, source selection optimization, reliability-based pruning heuristic, Estimation, Probability, social sensing, crowdsensing applications, expected fusion error, zero-one integer non-linear programming (INLP), similarity-based lossy estimation, influence graphs, social networking (online), Reliability, zero-one integer nonlinear programming]
Dynamic Contract Design for Heterogenous Workers in Crowdsourcing for Quality Control
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Crowdsourcing sites heavily rely on paid workers to ensure completion of tasks. Yet, designing a pricing strategies able to incentivize users' quality and retention is non trivial. Existing payment strategies either simply set a fixed payment per task without considering changes in workers' behaviors, or rule out poor quality responses and workers based on coarse criteria. Hence, task requesters may be investing significantly in work that is inaccurate or even misleading. In this paper, we design a dynamic contract to incentivize high-quality work. Our proposed approach offers a theoretically proven algorithm to calculate the contract for each worker in a cost-efficient manner. In contrast to existing work, our contract design is not only adaptive to changes in workers' behavior, but also adjusts pricing policy in the presence of malicious behavior. Both theoretical and experimental analysis over real Amazon review traces show that our contract design can achieve a near optimal solution. Furthermore, experimental results demonstrate that our contract design 1) can promote high-quality work and prevent malicious behavior, and 2) outperforms the intuitive strategy of excluding all malicious workers in terms of the requester's utility.
[Crowdsourcing, Algorithm design and analysis, crowdsourcing, quality control, pricing strategies, contract, pricing policy, heterogenous workers, Electronic mail, contracts, payment strategies, malicious behavior, dynamic contract design, Pollution, Upper bound, Amazon review traces, Pricing, personnel, Web sites, Contracts, pricing]
Joint Request Balancing and Content Aggregation in Crowdsourced CDN
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Recent years have witnessed a new content delivery paradigm named crowdsourced CDN, in which devices deployed at edge network can prefetch contents and provide content delivery service. Crowdsourced CDN offers high-quality experience to end-users by reducing their content access latency and alleviates the load of network backbone by making use of network and storage resources at millions of edge devices. In such paradigm, redirecting content requests to proper devices is critical for user experience. The uniqueness of request redirection in such crowdsourced CDN lies that: on one hand, the bandwidth capacity of the crowdsourced CDN devices is limit, hence devices located at a crowded place can be easily overwhelmed when serving nearby user requests; on the other hand, contents requested in one device can be significantly different from another one, making request redirection strategies used in conventional CDNs which only aim to balance request loads ineffective. In this paper, we explore request redirection strategies that take both workload balance of devices and content requested by users into consideration. Our contributions are as follows. First, we conduct measurement studies, coving 1.8M users watching 0.4M videos, to understand request patterns in crowdsourced CDN. We observe that the loads of nearby devices can be very different and the contents requested at nearby devices can also be significantly different. These observations lead to our design for request balancing at nearby devices. Second, we formulate the request redirection problem by taking both the content access latency and the content replication cost into consideration, and propose a request balancing and content aggregation solution. Finally, we evaluate the performance of our design using trace-driven simulations, and observe our scheme outperforms the traditional strategy in terms of many metrics, e.g., we observe a content access latency reduction by 50% over traditional mechanisms such as the Nearest/Random request routing scheme.
[Video delivery, Correlation, bandwidth capacity, Crowdsourced CDN, Servers, Videos, request redirection problem, storage management, content access latency reduction, Workload scheduling, joint request balancing and content aggregation, Sociology, Bandwidth, Wireless fidelity, storage resources, content delivery service, performance evaluation, Routing, user experience, trace-driven simulations, edge network, content prefetching, crowdsourced CDN devices, nearest-random request routing scheme, Internet, request redirection strategies, content replication cost]
Shrink: A Breast Cancer Risk Assessment Model Based on Medical Social Network
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Breast cancer risk assessment model can assess whether a people is at a high risk of developing breast cancer disease or not and confirm a breast cancer high-risk group. Because the etiology of breast cancer disease is different in different country and region, the existing risk assessment model is only adaptive to certain countries and regions. And the parameters of these models are fixed, so these models have poor generality. Aiming at these problems, the paper puts forward a new breast cancer risk assessment model named as Shrink. Using the idea of social network, Shrink constructs a medical social network to show the similarity among people, and uses group division algorithm to divide the network into breast cancer high-risk group and low-risk group. The parameters of this model can be set according to the needs of the breast census, and these parameters can be directly acquired through questionnaire, therefore Shrink has good generality. Moreover, under the uncertain classification standard, Shrink adopts a new classification method to discover breast cancer high-risk group. Based on the real data from questionnaires, we make experiments in Matlab, and obtain the evaluation index of the model. The experiment proves that the model itself has good evaluation result and is better than classic Gail model.
[Solid modeling, Adaptation models, pattern classification, Social network services, medical social network, Breast cancer, Breast Cancer, risk analysis, Gail model, Risk assessment, uncertain classification standard, Shrink, social networking (online), cancer, Group division, Risk management, Mathematical model, group division algorithm, medical computing, patient diagnosis, High-risk group, breast cancer risk assessment model, Medical social network]
Opportunistic Energy Sharing Between Power Grid and Electric Vehicles: A Game Theory-Based Pricing Policy
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Electric vehicles (EVs) have great potential to reduce dependency on fossil fuels. The recent surge in the development of online EV (OLEV) will help to address the drawbacks associated with current generation EVs, such as the heavy and expensive batteries. OLEVs are integrated with the smart grid of power infrastructure through a wireless power transfer system (WPT) to increase the driving range of the OLEV. However, the integration of OLEVs with the grid creates a tremendous load for the smart grid. The demand of a power grid changes over time and the price of power is not fixed throughout the day. There should be some congestion avoidance and load balancing policy implications to ensure quality of services for OLEVs. In this paper, first, we conduct an analysis to show the existence of unpredictable power load and congestion because of OLEVs. We use the Simulation for Urban MObility tool and hourly traffic counts of a road section of the New York City to analyze the amount of energy OLEVs can receive at different times of the day. Then, we present a game theory based on a distributed power schedule framework to find the optimal schedule between OLEVs and smart grid. In the proposed framework, OLEVs receive the amount of power charging from the smart grid based on a power payment function which is updated using best response strategy. We prove that the updated power requests converge to the optimal power schedule. In this way, the smart grid maximizes the social welfare of OLEVs, which is defined as mixed consideration of total satisfaction and its power charging cost. Finally, we verify the performance of our proposed pricing policy under different scenarios in a simulation study.
[Schedules, Roads, electric vehicles, simulation for urban mobility tool, smart power grids, Batteries, power grid, power generation scheduling, New York City, distributed power schedule framework, social welfare, load balancing policy, Pricing, Wireless power transfer, WPT, Smart grids, opportunistic energy sharing, Online electric vehicles, smart grid, Power schedule, power payment function, game theory, game theory-based pricing policy, online EV, quality of service, OLEV, Smart grid, power charging, optimal power schedule, wireless power transfer system, unpredictable power load, congestion avoidance, Games, power charging cost, Electric vehicles, pricing, quality of services, hourly traffic counts, Game theoretic pricing policy]
Energy Efficient Object Detection in Camera Sensor Networks
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
A wireless camera network can provide situation awareness information (e.g., humans in distress) in scenarios such as disaster recovery. If such camera sensors are battery operated, sending raw video feeds back to a central controller can be expensive in terms of energy consumption. Further, if all cameras were to use the optimal processing algorithm for object decision, they may also expend unnecessary energy. Stated otherwise, cameras that capture the same objects may not all have to use the optimal algorithm to achieve a desired accuracy, and this can save processing energy costs. In this paper, our objective is to design and implement a framework that can support coordination among cameras to deliver highly accurate detection of objects in an energy efficient way. The framework, which we call EECS (for energy efficient camera sensors), estimates the detection accuracy and energy costs incurred (both the processing and communication costs are taken into account) with each detection algorithm for each camera, and comes up with a choice of cameras for sending information pertaining to the object of interest. This set of cameras and the video processing algorithms that they must use, are chosen so as to minimize the energy expenditures, given a desired detection accuracy. We implement EECS on a camera network built with smartphones, and demonstrate that it reduces the energy consumption by up to 40% while ensuring a object detection accuracy of over 86%.
[Algorithm design and analysis, wireless sensor networks, object detection, situation awareness information, disaster recovery, Training, Manifolds, cameras, wireless camera sensor network, battery-operated camera sensors, EECS framework, energy consumption, video signal processing, central controller, emergency management, energy-efficient object detection, energy expenditures, video processing algorithm, optimal processing algorithm, object detection accuracy, Object detection, processing energy costs, Streaming media, energy conservation, Cameras, Feeds]
DeepOpp: Context-Aware Mobile Access to Social Media Content on Underground Metro Systems
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Accessing online social media content on underground metro systems is a challenge due to the fact that passengers often lose connectivity for large parts of their commute. As the oldest metro system in the world, the London underground represents a typical transportation network with intermittent Internet connectivity. To deal with disruption in connectivity along the sub-surface and deep-level underground lines on the London underground, we have designed a context-aware mobile system called DeepOpp that enables efficient offline access to online social media by prefetching and caching content opportunistically when signal availability is detected. DeepOpp can measure, crowdsource and predict signal characteristics such as strength, bandwidth and latency; it can use these predictions of mobile network signal to activate prefetching, and then employ an optimization routine to determine which social content should be cached in the system given real-time network conditions and device capacities. DeepOpp has been implemented as an Android application and tested on the London Underground; it shows significant improvement over existing approaches, e.g. reducing the amount of power needed to prefetch social media items by 2.5 times. While we use the London Underground to test our system, it is equally applicable in New York, Paris, Madrid, Shanghai, or any other urban underground metro system, or indeed in any situation in which users experience long breaks in connectivity.
[Crowdsourcing, mobile crowdsourcing, online social media content, Social network services, Prefetching, opportunistic networking, DeepOpp, context-aware computing, Mobile communication, Mobile handsets, cache storage, context-aware mobile access, content caching, underground metro systems, underground communication, mobile computing, context-aware mobile system, content prefetching, Bandwidth, social networking (online), Android application, Mobile computing]
PhaseBeat: Exploiting CSI Phase Data for Vital Sign Monitoring with Commodity WiFi Devices
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Vital signs, such as respiration and heartbeat, are useful to health monitoring since such signals provide important clues of medical conditions. Effective solutions are needed to provide contact-free, easy deployment, low-cost, and long-term vital sign monitoring. In this paper, we present PhaseBeat to exploit channel state information (CSI) phase difference data to monitor breathing and heartbeat with commodity WiFi devices. We provide a rigorous analysis of the CSI phase difference data with respect to its stability and periodicity. Based on the analysis, we design and implement the PhaseBeat system with off-the-shelf WiFi devices, and conduct an extensive experimental study to validate its performance. Our experimental results demonstrate the superior performance of PhaseBeat over existing approaches in various indoor environments.
[Phase measurement, OFDM, health monitoring, channel state information, phase difference, commodity 5GHz WiFi, Channel State Information, breathing monitoring, long-term vital sign monitoring, medical condition, commodity WiFi devices, indoor environments, Monitoring, Wireless fidelity, Antenna measurements, low-cost vital sign monitoring, vital sign monitoring, Receivers, biomedical telemetry, CSI phase difference data, off-the-shelf WiFi devices, PhaseBeat, contact-free vital sign monitoring, wireless LAN, Biomedical monitoring, health sensing]
REX: Rapid Ensemble Classification System for Landslide Detection Using Social Media
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
We study the problem of using Social Media to detect natural disasters, of which we are interested in a special kind, namely landslides. Employing information from Social Media presents unique research challenges, as there exists a considerable amount of noise due to multiple meanings of the search keywords, such as "landslide" and "mudslide". To tackle these challenges, we propose REX, a rapid ensemble classification system which can filter out noisy information by implementing two key ideas: (I) a new method for constructing independent classifiers that can be used for rapid ensemble classification of Social Media texts, where each classifier is built using randomized Explicit Semantic Analysis; and (II) a self-correction approach which takes advantage of the observation that the majority label assigned to Social Media texts belonging to a large event is highly accurate. We perform experiments using real data from Twitter over 1.5 years to show that REX classification achieves 0.98 in F-measure, which outperforms the standard Bag-of-Words algorithm by an average of 0.14 and the state-of-the-art Word2Vec algorithm by 0.04. We also release the annotated datasets used in the experiments as a contribution to the research community containing 282k labeled items.
[Electronic publishing, pattern classification, text analysis, geomorphology, disasters, self-correction approach, Encyclopedias, geophysics computing, Twitter, social media text, natural disasters, Terrain factors, majority label, noisy information, F-measure, Semantics, rapid ensemble classification system, randomized explicit semantic analysis, social networking (online), REX classification, landslide detection, independent classifiers]
Toward An Integrated Approach to Localizing Failures in Community Water Networks
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
We present a cyber-physical-human distributed computing framework, AquaSCALE, for gathering, analyzing and localizing anomalous operations of increasingly failure-prone community water services. Today, detection of pipe breaks/leaks in water networks takes hours to days. AquaSCALE leverages dynamic data from multiple information sources including IoT (Internet of Things) sensing data, geophysical data, human input, and simulation/modeling engines to create a sensor-simulation-data integration platform that can accurately and quickly identify vul-nerable spots. We propose a two-phase workflow that begins with robust simulation methods using a commercial grade hydraulic simulator - EPANET, enhanced with the support for IoT sensor and pipe failure modelings. It generates a profile of anomalous events using diverse plug-and-play machine learning techniques. The profile then incorporates with external observations (NOAA weather reports and twitter feeds) to rapidly and reliably isolate broken water pipes. We evaluate the two-phase mechanism in canonical and real-world water networks under different failure scenarios. Our results indicate that the proposed approach with offline learning and online inference can locate multiple simultaneous pipe failures at fine level of granularity (individual pipeline level) with high level of accuracy with detection time reduced by orders of magnitude (from hours/days to minutes).
[failure-prone community water services, EPANET, Pipelines, plug-and-play machine learning techniques, Hydraulic systems, distributed processing, failure localization, IoT sensor, failure analysis, Engines, community water networks, hydraulic simulator, cyber-physical-human distributed computing framework, learning (artificial intelligence), Water resources, pipes, Computational modeling, fault location, water supply, inference mechanisms, Internet of Things, pipe failure modelings, Temperature measurement, cyber-physical systems, offline learning, two-phase workflow, Water pollution, condition monitoring, AquaSCALE, online inference]
MobiQoR: Pushing the Envelope of Mobile Edge Computing Via Quality-of-Result Optimization
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Mobile edge computing aims at improving application response time and energy efficiency by deploying data processing at the edge of the network. Due to the proliferation of Internet of Things and interactive applications, the ever-increasing demand for low latency calls for novel approaches to further pushing the envelope of mobile edge computing beyond existing task offloading and distributed processing mechanisms. In this paper, we identify a new tradeoff between Quality-of-Result (QoR) and service response time in mobile edge computing. Our key idea is motivated by the observation that a growing set of edge applications involving media processing, machine learning, and data mining can tolerate some level of quality loss in the computed result. By relaxing the need for highest QoR, significant improvement in service response time can be achieved. Toward this end, we present a novel optimization framework, MobiQoR, which minimizes service response time and app energy consumption by jointly optimizing the QoR of all edge nodes and the offloading strategy. The proposed MobiQoR is prototyped using Parse, an open source mobile back-end tool, on Android smartphones. Using representative applications including face recognition and movie recommendation, our evaluation with real-world datasets shows that MobiQoR reduces response time and energy consumption by up to 77% (in face recognition) and 189.3% (in movie recommendation) over existing strategies under the same level of QoR relaxation.
[Energy consumption, data mining, Parse, application response time, Mobile communication, open source mobile back-end tool, real-world datasets, Optimization, Edge computing, mobile computing, media processing, MobiQoR, energy efficiency, face recognition, offloading strategy, learning (artificial intelligence), representative applications, edge applications, Image edge detection, task offloading, quality loss, quality-of-result optimization, data processing, service response time, Android smartphones, smart phones, distributed processing mechanisms, Object recognition, Internet of Things, machine learning, app energy consumption, movie recommendation, mobile edge computing, Time factors, interactive applications]
Truthful Auctions for User Data Allowance Trading in Mobile Networks
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
User data allowance trading emerges as a promising practice in mobile data networks since it can help mobile networks to attract more users. However, to date, there is no study on user data allowance trading in mobile networks. In this paper, we develop a truthful framework that allows users to bid for data allowance. We focus on preventing price cheating, guaranteeing fairness, and minimizing trading maintenance cost in trading. We formulate the data trading process as a double auction problem and develop algorithms to solve the problem. In particular, we use a uniform price auction based on a competitive equilibrium to defend against price cheating and provide fair-ness. Meanwhile, we leverage linear programming to minimize trading maintenance cost. We conduct extensive simulations to demonstrate the performance of the proposed mechanism. The simulation results show that our trading mechanism is truthful and fair, while incurring a minimized maintenance cost.
[Industries, cost reduction, mobile radio, user data allowance trading, double auction problem, maintenance engineering, Maintenance engineering, Mobile communication, Linear programming, uniform price auction, linear programming, truthful auctions, mobile networks, data trading process, mobile data networks, Pricing, Resource management, competitive equilibrium, trading maintenance cost minimization, Mobile computing, pricing, price cheating]
Online Resource Allocation for Arbitrary User Mobility in Distributed Edge Clouds
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
As clouds move to the network edge to facilitate mobile applications, edge cloud providers are facing new challenges on resource allocation. As users may move and resource prices may vary arbitrarily, %and service delays are heterogeneous, resources in edge clouds must be allocated and adapted continuously in order to accommodate such dynamics. In this paper, we first formulate this problem with a comprehensive model that captures the key challenges, then introduce a gap-preserving transformation of the problem, and propose a novel online algorithm that optimally solves a series of subproblems with a carefully designed logarithmic objective, finally producing feasible solutions for edge cloud resource allocation over time. We further prove via rigorous analysis that our online algorithm can provide a parameterized competitive ratio, without requiring any a priori knowledge on either the resource price or the user mobility. Through extensive experiments with both real-world and synthetic data, we further confirm the effectiveness of the proposed algorithm. We show that the proposed algorithm achieves near-optimal results with an empirical competitive ratio of about 1.1, reduces the total cost by up to 4x compared to static approaches, and outperforms the online greedy one-shot optimizations by up to 70%.
[Algorithm design and analysis, Cloud computing, static approach, online resource allocation, edge cloud resource allocation, distributed edge clouds, mobility management (mobile radio), Servers, gap-preserving transformation, Optimization, Edge computing, mobile computing, optimisation, resource allocation, mobile applications, logarithmic objective, cloud computing, cost reduction, resource prices, parameterized competitive ratio, online algorithm, Dynamic scheduling, service delays, edge cloud providers, arbitrary user mobility, network edge, Delays, Resource management]
Leveraging Target k-Coverage in Wireless Rechargeable Sensor Networks
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Energy remains a major hurdle in running computation-intensive tasks on wireless sensors. Recent efforts have been made to employ a Mobile Charger (MC) to deliver wireless power to sensors, which provides a promising solution to the energy problem. Most of previous works in this area aim at maintaining perpetual network operation at the expense of high operating cost of MC. In the meanwhile, it is observed that due to low cost of wireless sensors, they are usually deployed at high density so there is abundant redundancy in their coverage in the network. For such networks, it is possible to take advantage of the redundancy to reduce the energy cost. In this paper, we relax the strictness of perpetual operation by allowing some sensors to temporarily run out of energy while still maintaining target k-coverage in the network at lower cost of MC. We first establish a theoretical model to analyze the performance improvements under this new strategy. Then we organize sensors into load-balanced clusters for target monitoring by a distributed algorithm. Next, we propose a charging algorithm named &#x03BB;-GTSP Charging Algorithm to determine the optimal number of sensors to be charged in each cluster to maintain k-coverage in the network and derive the route for MC to charge them. We further generalize the algorithm to encompass mobile targets as well. Our extensive simulation results demonstrate significant improvements of network scalability and cost saving that MC can extend charging capability over 2-3 times with a reduction of 40% of moving cost without sacrificing the network performance.
[target k-coverage, wireless sensor networks, Mobile communication, wireless charging, route planning, cost saving, Wireless communication, perpetual operation, Clustering algorithms, Sensors, redundancy, Monitoring, load-balanced clusters, Inductive charging, target monitoring, Wireless sensor networks, distributed algorithm, &#x03BB;-GTSP charging algorithm, leveraging target k-coverage, distributed algorithms, network scalability, mobile targets, mobile charger, wireless rechargeable sensor networks, abundant redundancy]
Reducing Cellular Signaling Traffic for Heartbeat Messages via Energy-Efficient D2D Forwarding
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Mobile Instant Messaging (IM) apps, such as WhatsApp and WeChat, frequently send heartbeat messages to remote servers to maintain always-online status. Periodic heartbeat messages are small in size, but their transmissions incur heavy signaling traffic to frequently establish and release communication channels between base stations and smartphones, known as signaling storm. Meanwhile, smartphones also need to activate cellular data communication module frequently for transmitting short heartbeat messages, resulting in substantial energy consumption. To address these issues, we propose a Device-to-Device (D2D) based heartbeat relaying framework, in order to reduce signaling traffic and energy consumption in heartbeat transmission. The framework selects the smartphones as relays to opportunistically collect heartbeat messages from nearby smartphones using energy-efficient D2D communication. The collected heartbeat messages are transmitted to the BS in an aggregated manner to reduce cellular signaling traffic. Based on the periods and the expiration time of the collected heartbeat messages, the framework schedules the transmissions of collected heartbeat messages to minimize signaling and energy consumption while satisfying time constrains. We implement and evaluate our solution on Android smartphones. The results from real-world experiments show that our solution achieves more than 50% signaling traffic reduction and up to 36% energy saving.
[Energy consumption, WeChat, device-to-device-based heartbeat relaying framework, D2D-based heartbeat relaying framework, electronic messaging, Energy-Efficiency, communication channel, Mobile communication, telecommunication scheduling, Device-to-device communication, short-heartbeat messages, Relays, Cellular Signaling Traffic, heartbeat transmission, Smartphones, Heart beat, cellular signaling traffic reduction, smartphones, substantial energy consumption, mobile IM apps, energy consumption, relay networks (telecommunication), cellular data communication module, energy-efficient D2D forwarding, Android smartphones, biomedical communication, energy saving, Storms, mobile instant messaging, Device-to-Device (D2D), machine-to-machine communication, signaling traffic reduction, energy conservation, Smart phones, WhatsApp, base stations, cellular radio, transmission scheduling]
k-Protected Routing Protocol in Multi-hop Cognitive Radio Networks
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In cognitive radio networks (CRNs), the established communication sessions between secondary users (SUs) may be affected or even get interrupted because the SUs need to relinquish the spectrum when the licensed users (PUs) appear and reclaim the spectrum/channel. On detecting PU activities, the SUs on the affected links either switch to another available idle spectrum using the same link or the SUs seek for an alternative path/link. In either approach, the ongoing session is destined to experience delay or even gets interrupted, which is intolerable to quality of service-sensitive applications such as multimedia streaming or audio/video conferencing. In this paper, we study the problem of establishing k-protected routes in CRNs. A k-protected route consists of a set of main links with preassigned backup spectrum and backup paths and is guaranteed to sustain from k PU appearances without being interrupted. For a CRN, we find a k-protected route for each session request and maximize the number of sessions that can be supported. We propose both centralized and distributed k-protected routing algorithms for this problem. Simulation results show that our k-protected routing protocol outperforms existing opportunistic spectrum switching approaches in terms of delay and interruption rate.
[quality of service-sensitive applications, OFDM, Switches, communication sessions, audio-video conferencing, multimedia streaming, backup paths, cognitive radio, idle spectrum, distributed k-protected routing algorithm, secondary users, k-protected routing protocol, backup spectrum, centralized k-protected routing algorithm, PU activity detection, Interference, Routing, quality of service, Cognitive radio, multihop cognitive radio networks, routing protocols, session request, opportunistic spectrum switching approach, Delays, licensed users, CRN]
Multi-resource Load Balancing for Virtual Network Functions
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Middleboxes are widely deployed to perform various network functions to ensure security and improve performance. The recent trend of Network Function Virtualization (NFV) makes it easy for operators to deploy software implementations of these network functions on commodity servers. However, virtual network functions consume different amounts of resources when processing packets. Thus a multi-resource load balancing (MRLB) mechanism is needed to efficiently utilize server resources. MRLB problem in the context of NFV is fundamentally different from multi-resource allocation problems, as well as traditional single-resource load balancing and multi-resource load balancing problems in task scheduling. In this paper, we tackle the MRLB problem in NFV by first proposing dominant load-the load of the most stressed resource on a server-as the load balancing metric. We then formulate the MRLB problem as an optimization to minimize the maximum dominant load of all NFV servers given the demand. Based on proximal Jacobian ADMM, we propose an efficient algorithm to solve the problem in large scale settings. Through extensive trace-driven simulations and prototype experiments on a testbed, we show that our MRLB algorithm with dominant load performs significantly better and faster than benchmarking algorithms.
[middleboxes, MRLB mechanism, commodity servers, Middleboxes, virtualisation, Servers, minimax techniques, Network Function Virtualization (NFV), resource allocation, traditional single-resource load balancing problem, multiresource allocation problems, Bandwidth, maximum dominant load minimization, proximal Jacobian ADMM, ADMM, virtual network functions, server resources, NFV servers, computer networks, benchmarking algorithm, Multi-Resource Load Balancing, trace-driven simulation, software implementation, multiresource load balancing mechanism, Load management, Software, Network function virtualization, task scheduling, Resource management]
Learning from Failure Across Multiple Clusters: A Trace-Driven Approach to Understanding, Predicting, and Mitigating Job Terminations
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In large-scale computing platforms, jobs are prone to interruptions and premature terminations, limiting their usability and leading to significant waste in cluster resources. In this paper, we tackle this problem in three steps. First, we provide a comprehensive study based on log data from multiple large-scale production systems to identify patterns in the behaviour of unsuccessful jobs across different clusters and investigate possible root causes behind job termination. Our results reveal several interesting properties that distinguish unsuccessful jobs from others, particularly w.r.t. resource consumption patterns and job configuration settings. Secondly, we design a machine learning-based framework for predicting job and task terminations. We show that job failures can be predicted relatively early with high precision and recall, and also identify attributes that have strong predictive power of job failure. Finally, we demonstrate in a concrete use case how our prediction framework can be used to mitigate the effect of unsuccessful execution using an effective task-cloning policy that we propose.
[Linear systems, Large-Scale Systems, trace-driven approach, resource consumption patterns, job failures prediction, parallel processing, Program processors, resource allocation, large-scale computing platforms, job terminations mitigation, Parallel processing, Large-scale systems, learning (artificial intelligence), job configuration settings, Google, machine learning-based framework, Failure Mitigation, Electric breakdown, Trace Analysis, Job Failure, job terminations prediction, Computer crashes, software fault tolerance, Failure Prediction, Reliability, task-cloning policy]
RBAY: A Scalable and Extensible Information Plane for Federating Distributed Datacenter Resources
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
While many institutions, whether industrial, academic, or governmental, satisfy their computing needs through public cloud providers, many others still manage their own resources, often as geographically distributed datacenters. Spare capacity from these geographically distributed datacenters could be offered to others, provided there were a mechanism to discover, and then request these resources. Unfortunately, single datacenter administrators tend not to cooperate due to issues of scalability, diverse administrative policies, and site-specific monitoring infrastructure. This paper describes RBAY, an integrated information plane that enables secure and scalable sharing between geographically distributed datacenters. RBAY's key design features are twofold. First, RBAY employs a decentralized `hierarchical aggregation tree' structure to seamlessly aggregate spare resources from geographically distributed datacenters to a global information plane. Second, RBAY attaches to each participating server a `admin-customized' handler, which follows site-specific policy to expose, hide, add, remove resources to RBAY, and thus fulfill the task of `which resource to expose to whom, when, and how'. An experimental evaluation on eight real-world geo-distributed sites demonstrates RBAY's rapid response to composite queries, as well as its extensible, scalable, and lightweight nature.
[Cloud computing, Scalability, Graphics processing units, scalable information plane, Routing, extensible information plane, composite queries, active attribute, computer centres, RBAY, RBay, geographically distributed datacenters, global information plane, spare resources, Aggregates, real-world geo-distributed sites, decentralized hierarchical aggregation tree structure, distributed databases, Peer-to-peer computing, federating distributed datacenter resources, admin-customized handler, Monitoring, hierarchical aggregation tree]
Task-aware TCP in Data Center Networks
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In modern data centers, many flow-based and task-based schemes have been proposed to speed up the data transmission in order to provide fast, reliable services for millions of users. However, existing flow-based schemes treat all flows in isolation, contributing less to or even hurting user experience due to the stalled flows. Other prevalent task-based approaches, such as centralized and decentralized scheduling, are sophisticated or unable to share task information. In this work, we first reveal that relinquishing bandwidth of leading flows to the stalled ones effectively reduces the task completion time. We further present the design and implementation of a general supporting scheme that shares the flow-tardiness information through a receiver-driven coordination. Our scheme can be flexibly and widely integrated with the state-of-the-art TCP protocols designed for data centers, while making no modification on switches. Through the testbed experiments and simulations of typical data center applications, we show that our scheme reduces the task completion time by 70% and 50% compared with the flow-based protocols (e.g. DCTCP, L2DCT) and task-based scheduling (e.g. Baraat), respectively. Moreover, our scheme also outperforms other approaches by 18% to 25% in prevalent topologies of data center.
[TCP, Protocols, Data Center Networks, receiver-driven coordination, telecommunication scheduling, flow-tardiness information, L2DCT protocols, Servers, decentralized scheduling, TCP protocols, Distributed databases, Bandwidth, task-aware TCP, centralized scheduling, computer network reliability, data center networks, DCTCP protocols, Topology, user experience, computer centres, service reliability, flow-based scheme, flow-based protocols, transport protocols, task-based scheme, task completion time, data transmission, Delays, task-based scheduling, leading flows, task completion time reduction, DCTCP]
Limitations of Load Balancing Mechanisms for N-Tier Systems in the Presence of Millibottlenecks
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The scalability of n-tier systems relies on effective load balancing to distribute load among the servers of the same tier. We found that load balancing mechanisms (and some policies) in servers used in typical n-tier systems (e.g., Apache and Tomcat) have issues of instability when very long response time (VLRT) requests appear due to millibottlenecks, very short bottlenecks that last only tens to hundreds of milliseconds. Experiments with standard n-tier benchmarks show that during millibottlenecks, some load balancing policy/mechanism combinations make the mistake of sending new requests to the node(s) suffering from millibottlenecks, instead of the idle nodes as load balancers are supposed to do. Several of these mistakes are due to the implicit assumptions made by load balancing policies and mechanisms on the stability of system state. Our study shows that appropriate remedies at policy and mechanism levels can avoid these mistakes during millibottlenecks and remove the VLRT requests, thus improving the average response time by a factor of 12.
[client-server systems, n-tier systems scalability, load distribution, Servers, Standards, software architecture, system state stability, resource allocation, very long response time, millibottlenecks, VLRT requests, Benchmark testing, Load management, load balancing mechanisms, distributed systems, Performance analysis, Time factors, Transient analysis, Queueing analysis]
Performance Analysis of Cloud Computing Centers Serving Parallelizable Rendering Jobs Using M/M/c/r Queuing Systems
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Performance analysis is crucial to the successful development of cloud computing paradigm. And it is especially important for a cloud computing center serving parallelizable application jobs, for determining a proper degree of parallelism could reduce the mean service response time and thus improve the performance of cloud computing obviously. In this paper, taking the cloud based rendering service platform as an example application, we propose an approximate analytical model for cloud computing centers serving parallelizable jobs using M/M/c/r queuing systems, by modeling the rendering service platform as a multi-station multi-server system. We solve the proposed analytical model to obtain a complete probability distribution of response time, blocking probability and other important performance metrics for given cloud system settings. Thus this model can guide cloud operators to determine a proper setting, such as the number of servers, the buffer size and the degree of parallelism, for achieving specific performance levels. Through extensive simulations based on both synthetic data and real-world workload traces, we show that our proposed analytical model can provide approximate performance prediction results for cloud computing centers serving parallelizable jobs, even those job arrivals follow different distributions.
[blocking probability, Cloud computing, cloud computing centers, rendering service., approximate analytical model, Servers, statistical distributions, buffer size, Analytical models, real-world workload traces, cloud based rendering service platform, Parallel processing, parallel job, cloud computing, complete probability distribution, queueing theory, parallelizable rendering jobs, Computational modeling, queuing system, synthetic data traces, computer centres, number-of-servers, degree-of-parallelism, mean service response time reduction, response time, performance metrics, Rendering (computer graphics), multistation multiserver system, Queueing analysis, performance analysis]
Evaluation of Deep Learning Frameworks Over Different HPC Architectures
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Recent advances in deep learning have enabled researchers across many disciplines to uncover new insights about large datasets. Deep neural networks have shown applicability to image, time-series, textual, and other data, all of which are available in a plethora of research fields. However, their computational complexity and large memory overhead requires advanced software and hardware technologies to train neural networks in a reasonable amount of time. To make this possible, there has been an influx in development of deep learning software that aim to leverage advanced hardware resources. In order to better understand the performance implications of deep learning frameworks over these different resources, we analyze the performance of three different frameworks, Caffe, TensorFlow, and Apache SINGA, over several hardware environments. This includes scaling up and out with single-and multi-node setups using different CPU and GPU technologies. Notably, we investigate the performance characteristics of NVIDIA's state-of-the-art hardware technology, NVLink, and also Intel's Knights Landing, the most advanced Intel product for deep learning, with respect to training time and utilization. To our best knowledge, this is the first work concerning deep learning bench-marking with NVLink and Knights Landing. Through these experiments, we provide analysis of the frameworks' performance over different hardware environments in terms of speed and scaling. As a result of this work, better insight is given towards both using and developing deep learning tools that cater to current and upcoming hardware technologies.
[Knights Landing, NVIDIA, Graphics processing units, parallel processing, GPU, distributed computing, Training, deep learning, Benchmark testing, NVLink, HPC architectures, Hardware, GPU technologies, Intel product, learning (artificial intelligence), deep neural networks, CPU technologies, deep learning frameworks, TensorFlow framework, Tools, microprocessor chips, Caffe framework, graphics processing units, Apache SINGA, shown applicability, Machine learning, HPC, neural network training, performance analysis, computational complexity]
On Achieving Efficient Data Transfer for Graph Processing in Geo-Distributed Datacenters
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Graph partitioning is important for optimizing the performance and communication cost of large graph processing jobs. Recently, many graph applications such as social networks store their data on geo-distributed datacenters (DCs) to provide services worldwide with low latency. This raises new challenges to existing graph partitioning methods, due to the costly Wide Area Network (WAN) usage and the multi-levels of network heterogeneities in geo-distributed DCs. In this paper, we propose a geo-aware graph partitioning method named G-Cut, which aims at minimizing the inter-DC data transfer time of graph processing jobs in geo-distributed DCs while satisfying the WAN usage budget. G-Cut adopts two novel optimization phases which address the two challenges in WAN usage and network heterogeneities separately. G-Cut can be also applied to partition dynamic graphs thanks to its light-weight runtime overhead. We evaluate the effectiveness and efficiency of G-Cut using realworld graphs with both real geo-distributed DCs and simulations. Evaluation results show that G-Cut can reduce the inter-DC data transfer time by up to 58% and reduce the WAN usage by up to 70% compared to state-of-the-art graph partitioning methods with a low runtime overhead.
[network heterogeneities, G-cut, graph theory, distributed processing, Downlink, graph processing jobs, Runtime, optimisation, Heterogeneous network, optimization, Bandwidth, Uplink, dynamic graphs partitioning, Wide area networks, geo-distributed datacenters, Geo-distributed datacenters, computer centres, geo-distributed DC, Graph partitioning, geo-aware graph partitioning, inter-DC data transfer time, Data transfer, data handling, WAN usage budget, wide area network]
GBooster: Towards Acceleration of GPU-Intensive Mobile Applications
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The performance of GPUs on mobile devices is generally the bottleneck of multimedia mobile applications (e.g., 3D games and virtual reality). Previous attempts to tackle the issue mainly migrate GPU computation to servers residing in remote cloud centers. However, the costly network delay is especially undesirable for highly-interactive multimedia applications since a fast response time is critical for user experience. In this paper, we propose GBooster, a system that accelerates multimedia mobile applications by transparently offloading GPU tasks onto neighboring multimedia devices such as Smart TVs and Gaming Consoles. Specifically, GBooster intercepts and redirects system graphics calls by utilizing the Dynamic Linker Hooking technique, which requires no modification of the applications and the mobile systems. In addition, a major concern for offloading is the high energy consumption incurred by network transmissions. To address this concern, GBooster is designed to intelligently switch between the low-power Bluetooth and the high-throughput WiFi based on the traffic demand. We implement GBooster on the Android system and evaluate its performance. The results demonstrate that it can boost applications' frame rates by up to 85%. In terms of power consumption, GBooster can preserve up to 70% energy compared with local execution.
[Performance evaluation, Cloud computing, GBooster, Graphics processing units, Intensive, smart TV, GPU-intensive mobile applications, Multimedia communication, Servers, multimedia computing, low-power Bluetooth, GPU, power consumption, traffic demand, mobile computing, energy consumption, GPU tasks offloading, multimedia mobile applications, system graphics calls, dynamic linker hooking technique, graphics processing units, Mobile, gaming consoles, GPU computation, Games, mobile devices, graphics processing unit, high-throughput WiFi, Android system, network transmissions]
Scaling k-Nearest Neighbours Queries (The Right Way)
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Recently parallel / distributed processing approaches have been proposed for processing k-Nearest Neighbours (kNN) queries over very large (multidimensional) datasets aiming to ensure scalability. However, this is typically achieved at the expense of efficiency. With this paper we offer a novel approach that alleviates the performance problems associated with state of the art methods. The essence of our approach, which differentiates it from related research, rests on (i) adopting a coordinator-based distributed processing algorithm, instead of those employed over data-parallel executionengines (such as Hadoop/MapReduce or Spark), and (ii) on a way to organize data, to structure computation, and to index the stored datasets that ensures that only a very small number of data items are retrieved from the underlying data store, communicated over the network, and processed by the coordinatorfor every kNN query. Our approach also pays special attention to ensuring scalability in addition to low query processing times. Overall, kNN queries can be processed in just tens of milliseconds (as opposed to the tens of) seconds required by state of the art. We have implemented our approach, usinga NoSQL DB (HBase) as the data store, and we compare it against the state-of-the-art: the Hadoop-based Spatial Hadoop (SHadoop) and the Spark-based Simba methods. We employ different datasets of various sizes, showcasing the contributed performance advantages. Our approach outperforms the stateof the art, by 2-3 orders of magnitude, and consistently for dataset sizes ranging from hundreds of millions to hundreds of billions of data points. We also show that the key constituent performance overheads incurred during query processing (such as the number of data items retrieved from the data store, the required network bandwidth, and the processing time at the coordinator) scale very well, ensuring the overall scalability of the approach.
[data retrieval, Scalability, data structure, distributed processing, parallel processing, query processing, SHadoop, kNN Query, storage management, NoSQL, Distributed databases, data structures, data store, Spatial Hadoop, NoSQL databases, Hadoop, Big Data, Sparks, HBase, kNN queries, Query processing, Simba, k-nearest neighbours queries, data handling, NoSQL DB, Spark, Indexing]
Parallelizing Big De Bruijn Graph Construction on Heterogeneous Processors
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
De Bruijn graph construction is the first step in de novo assemblers to connect input reads into a complete sequence without a reference genome. This step is both time and memory space consuming. To address this problem, we develop ParaHash, a system that partitions the input data in a compact format, parallelizes the computation on both the CPUs and the GPUs in a single computer, and performs hash-based De Bruijn graph construction. This way, ParaHash utilizes all available processors to assemble big genomes that cannot fit into memory. Furthermore, we analyze the characteristics of genome data to set the hash table size, design concurrent hashing algorithms to handle the inherent multiplicity, and pipeline the data transfer and the computation for further efficiency. Our experiments on real-world genome datasets show that the workload was balanced across heterogeneous processors, and that ParaHash was able to construct billion-node graphs on a single machine with an overall performance up to 20 times faster than the state-of-the-art shared-memory assemblers.
[hash table size, graph theory, Genomics, Graphics processing units, GPUs, CPU, parallel processing, genomics, data partitioning, Bioinformatics, parallel and GPU computing, Image edge detection, Partitioning algorithms, parallelization, genome assembly, hash-based De Bruijn graph construction, genome datasets, heterogeneous processors, ParaHash, Memory management, hashing algorithms, de bruijn graph construction, bioinformatics, data transfer, file organisation, data handling]
Private, Yet Practical, Multiparty Deep Learning
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In this paper, we consider the problem of multiparty deep learning (MDL), wherein autonomous data owners jointly train accurate deep neural network models without sharing their private data. We design, implement, and evaluate &#x221D;MDL, a new MDL paradigm built upon three primitives: asynchronous optimization, lightweight homomorphic encryption, and threshold secret sharing. Compared with prior work, &#x221D;MDL departs in significant ways: a) besides providing explicit privacy guarantee, it retains desirable model utility, which is paramount for accuracy-critical domains; b) it provides an intuitive handle for the operator to gracefully balance model utility and training efficiency; c) moreover, it supports delicate control over communication and computational costs by offering two variants, operating under loose and tight coordination respectively, thus optimizable for given system settings (e.g., limited versus sufficient network bandwidth). Through extensive empirical evaluation using benchmark datasets and deep learning architectures, we demonstrate the efficacy of &#x221D;MDL.
[Privacy preservation, Computational modeling, asynchronous optimization, Encryption, private data, multiparty deep learning, threshold secret sharing, Training, lightweight homomorphic encryption, Privacy, Deep neural network, Federated learning, MDL, Machine learning, Data models, data privacy, learning (artificial intelligence)]
Fast and Flexible Networking for Message-Oriented Middleware
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Distributed applications deployed in multi-datacenter environments need to deal with network connections of varying quality, including high bandwidth and low latency within a datacenter and, more recently, high bandwidth and high latency between datacentres. In principle, for a given network connection, each message should be sent over the best available network protocol, but existing middlewares do not provide this functionality. In this paper, we present KompicsMessaging, a messaging middleware that allows for fine-grained control of the network protocol used on a per-message basis. Rather than always requiring application developers to specify the appropriate protocol for each message, we also provide an online reinforcement learner that optimises the selection of the network protocol for the current network environment. In experiments, we show how connection properties, such as the varying round-trip time, influence the performance of the application and we show how throughput and latency can be improved by picking the right protocol at the right time.
[Protocols, network protocol, Ports (Computers), Throughput, KompicsMessaging middleware, distributed application, machine learning, Middleware, round-trip time, transport protocols, message-oriented middleware, Bandwidth, Learning (artificial intelligence), multidatacenter environment, Libraries, middleware]
TailCut: Power Reduction under Quality and Latency Constraints in Distributed Search Systems
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Web search constitutes an important class of data-intensive online services in data centers. Optimizing search systems for energy efficiency, timely response and high search quality (i.e., how relevant the returned results are to a search query), however, is very challenging, as a search system involves a distributed architecture with hundreds of thousands of index serving nodes (ISNs) that return searching results to an aggregator through multiple interdependent retrieval stages in a partition-aggregate fashion. In this paper, we discover through experiments two important characteristics that can affect the system performance: (1) response time and energy consumption are greatly impacted by a small fraction of queries with long processing times; (2) the quality contribution of the ISN is independent of the query processing time. Based on our observation, we propose TailCut, which judiciously discards long query executions and enables ISN-aggregator coordination to minimize energy consumption subject to latency and quality constraints. Our experimental results show that TailCut can achieve up to 39% power saving, while satisfying the tail latency and quality constraint.
[distributed search systems, data centers, Servers, distributed architecture, index serving nodes, query processing, data-intensive online services, query processing time, ISN quality contribution, Computer architecture, Search engines, multiple interdependent retrieval stages, energy consumption, Power demand, power reduction, ISN-aggregator coordination, Indexes, computer centres, partition-aggregate fashion, energy consumption minimization, Query processing, response time, information retrieval systems, quality constraint, system performance, TailCut, Internet, Time factors, tail latency constraint, Web search]
StoArranger: Enabling Efficient Usage of Cloud Storage Services on Mobile Devices
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Cloud storage usages are becoming increasingly popular on mobile devices. Through an extensive motivation study, we find that cloud storage accesses from mobile apps suffer from several notable problems that undermine usage experiences. The root cause is that the way of cloud storage providers deploying their services onto mobile devices relies on app developers for the correct and appropriate implementations and lacks the ability of monitoring and servicing client-side cloud storage accesses. We propose StoArranger, a practical system framework that solves the problems by coordinating, rearranging, and transforming cloud storage communications on mobile devices. We have prototyped the proposed system using two different implementation approaches. We discuss our experiences of the implementations in the paper. The real-app evaluation experiments show that StoArranger can significantly improve mobile cloud storage access efficiency with little overheads.
[Cloud computing, Traffic reduction, cloud storage service usage, Humanoid robots, usage experience, Mobile communication, Mobile handsets, Mobile apps, StoArranger, Synchronization, Middleware, mobile device, storage management, mobile computing, cloud storage access, Mobile device, Androids, cloud computing, Mobile cloud storage, Monitoring]
Characterizing Performance and Energy-Efficiency of the RAMCloud Storage System
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Most large popular web applications, like Facebook and Twitter, have been relying on large amounts of in-memory storage to cache data and offer a low response time. As the main memory capacity of clusters and clouds increases, it becomes possible to keep most of the data in the main memory. This motivates the introduction of in-memory storage systems. While prior work has focused on how to exploit the low-latency of in-memory access at scale, there is very little visibility into the energy-efficiency of in-memory storage systems. Even though it is known that main memory is a fundamental energy bottleneck in computing systems (i.e., DRAM consumes up to 40% of a server's power). In this paper, by the means of experimental evaluation, we have studied the performance and energy-efficiency of RAMCloud - a well-known in-memory storage system. We reveal that although RAMCloud is scalable for read-only applications, it exhibits non-proportional power consumption. We also find that the current replication scheme implemented in RAMCloud limits the performance and results in high energy consumption. Surprisingly, we show that replication can also play a negative role in crash-recovery.
[in-memory storage systems, Performance evaluation, Power demand, random-access storage, Scalability, Random access memory, Web applications, nonproportional power consumption, Throughput, Computer crashes, cache storage, RAMCloud storage system, RAMCloud, Servers, performance characterization, storage management, power aware computing, cache data, energy efficiency, Benchmark testing, Energy efficiency, In-memory storage, cloud computing, energy consumption]
Proactively Secure Cloud-Enabled Storage
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Attacking cloud-enabled storage is becoming increasingly lucrative as more personal and enterprise data moves to the cloud. Traditional security mechanisms temporarily limit such attacks, but over a long period of time attackers will eventually find vulnerabilities; this can lead to compromising large amounts of valuable data and lead to large-scale privacy breaches. This paper addresses this problem by incorporating proactive security guarantees into cloud-enabled storage. Proactive security deals with an adversary's ability to eventually compromise all involved servers in a distributed storage or computation system. While there are several proactively secure secret sharing protocols that can be used to improve confidentiality of data stored in the cloud, their high overhead has traditionally limited them to less than ten parties and to only 100s of bytes typical for cryptographic keys. Realizing proactively secure cloud storage for larger data (e.g, MBs) requires careful design and calibration of system parameters, and faces several challenges. In this paper we design, implement and assess performance of the first system for Proactively Secure Cloud-Enabled Storage (PiSCES) of data larger than cryptographic keys. Based on our practical performance results we advocate that the high level of resilience and long-term security and confidentiality guarantees enabled by proactive security should be considered in future distributed and cloud-based storage and computing services.
[Cloud computing, Cloud Security, cryptographic protocols, distributed storage, cloud-based storage, PiSCES, cloud-enabled storage, proactively secure secret sharing protocols, security mechanisms, Encryption, Servers, Secure storage, personal data, Proactive Security, storage management, data storage confidentiality improvement, enterprise moves data, large-scale privacy breaches, System Prototype, Silicon, cloud computing, proactively secure cloud-enabled storage]
BEES: Bandwidth- and Energy- Efficient Image Sharing for Real-Time Situation Awareness
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In order to save human lives and reduce injury and property loss, Situation Awareness (SA) information is essential and important for rescue workers to perform the effective and timely disaster relief. The information is generally derived from the shared images via widely used smartphones. However, conventional smartphone-based image sharing schemes fail to efficiently meet the needs of SA applications due to two main reasons, i.e., real-time transmission requirement and application-level image redundancy, which is exacerbated by limited bandwidth and energy availability. In order to provide efficient image sharing in disasters, we propose a bandwidth-and energy-efficient image sharing system, called BEES. The salient feature behind BEES is to propose the concept of Approximate Image Sharing (AIS), which explores and exploits approximate feature extraction, redundancy detection, and image uploading to trade the slightly low quality of computation results in content-based redundancy elimination for higher bandwidth and energy efficiency. Nevertheless, the boundaries of the tradeoffs between the quality of computation results and efficiency are generally subjective and qualitative. We hence propose the energy-aware adaptive schemes in AIS to leverage the physical energy availability to objectively and quantitatively determine the tradeoffs between the quality of computation results and efficiency. Moreover, unlike existing work only for cross-batch similar images, BEES further eliminates in-batch ones via a similarity-aware submodular maximization model. We have implemented the BEES prototype which is evaluated via three real-world image datasets. Extensive experimental results demonstrate the efficacy and efficiency of BEES.
[redundancy detection, Image Sharing System, Content-based Redundancy Elimination, Servers, energy-aware adaptive schemes, Image coding, power aware computing, feature extraction, Bandwidth, energy efficiency, content-based redundancy elimination, approximate image sharing, Disaster Environments, peer-to-peer computing, similarity-aware submodular maximization model, Redundancy, Systems architecture, AIS, image uploading, BEES, approximate feature extraction, real-time situation awareness, bandwidth-and energy-efficient image sharing, Situation Awareness, energy conservation, Feature extraction, Smart phones]
Transparent Fault-Tolerance Using Intra-Machine Full-Software-Stack Replication on Commodity Multicore Hardware
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
As the number of processors and the size of the memory of computing systems keep increasing, the likelihood of CPU core failures, memory errors, and bus failures increases and can threaten system availability. Software components can be hardened against such failures by running several replicas of a component on hardware replicas that fail independently and that are coordinated by a State-Machine Replication protocol. One common solution is to replicate the physical machine to provide redundancy, and to rewrite the software to address coordination. However, a CPU core failure, a memory error, or a bus error is unlikely to always crash an entire machine. Thus, full machine replication may sometimes be an overkill, increasing resource costs. In this paper, we introduce full software stack replication within a single commodity machine. Our approach runs replicas on fault-independent hardware partitions (e.g., NUMA nodes), wherein each partition is software-isolated from the others and has its own CPU cores, memory, and full software stack. A hardware failure in one partition can be recovered by another partition taking over its functionality. We have realized this vision by implementing FT-Linux, a Linux-based operating system that transparently replicates race-free, multithreaded POSIX applications on different hardware partitions of a single machine. Our evaluations of FT-Linux on several popular Linux applications show a worst case slowdown (due to replication) by &#x2248;20%.
[multiprocessing systems, multi-threading, fault-independent hardware partitions, commodity multicore hardware, race-free multithreaded POSIX applications, software failure, replicated-kernel OS, Servers, intra-machine full-software-stack replication, multicore, state-machine replication, POSIX, Fault tolerance, Linux, Fault tolerant systems, Linux-based operating system, transparent fault-tolerance, Hardware, fault tolerant computing, software engineering, hardware failure, Kernel, FT-Linux, Hardware failure]
A Preventive Auto-Parallelization Approach for Elastic Stream Processing
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Nowadays, more and more sources (connected devices, social networks, etc.) emit real-time data with fluctuating rates over time. Existing distributed stream processing engines (SPE) have to resolve a difficult problem: deliver results satisfying end-users in terms of quality and latency without over-consuming resources. This paper focuses on parallelization of operators to adapt their throughput to their input rate. We suggest an approach which prevents operator congestion in order to limit degradation of results quality. This approach relies on an automatic and dynamic adaptation of resource consumption for each continuous query. This solution takes advantage of i) a metric estimating the activity level of operators in the near future ii) the AUTOSCALE approach which evaluates the need to modify parallelism degrees at local and global scope iii) an integration into the Apache Storm solution. We show performance tests comparing our approach to the native solution of this SPE.
[Context, Measurement, selectivity, elastic stream processing, Instruction sets, social networks, distributed stream processing, Optimal scheduling, distributed processing, operator congestion, SPE, Topology, preventive auto parallelization approach, dynamic adaptation, Parallel processing, AUTOSCALE approach, auto-parallelization, distributed systems, Central Processing Unit, connected devices, Apache Storm solution, distributed stream processing engines, middleware]
Dependable Cloud Resources with Guardian
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Despite advances in making datacenters dependable, failures still happen. This is particularly onerous for long-running "big data" applications, where partial failures can lead to significant losses and lengthy recomputations. Big data processing frameworks like Hadoop MapReduce include fault tolerance (FT) mechanisms, but these are commonly targeted at specific system/failure models, and are often redundant between frameworks. This paper proposes the paradigm of dependable resources: big data processing frameworks are typically built on top of resource management systems (RMSs), and proposing FT support at the level of such an RMS yields generic FT mechanisms, which can be provided with low overhead by leveraging constraints on resources. We demonstrate our concepts through Guardian, a robust RMS based on YARN. Guardian allows frameworks to run their applications with individually configurable FT granularity and degree, with only minor changes to their implementation. We demonstrate the benefits of our approach by evaluating Hadoop, Tez, Spark and Pig on Guardian in Amazon-EC2, improving completion time by around 68% in the presence of failures, while maintaining around 6% overhead.
[Cloud computing, fault tolerance, resource management systems, Hadoop, Big Data, Sparks, Yarn, parallel processing, FT, MapReduce, Fault tolerance, Guardian, RMS, Data models, fault tolerant computing, Resource management, cloud computing, dependable cloud resources]
A Communication-Aware Container Re-Distribution Approach for High Performance VNFs
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Containers have been used in many applications for isolation purposes due to the lightweight, scalable and highly portable properties. However, to apply containers in virtual network functions (VNFs) faces a big challenge because high-performance VNFs often generate frequent communication workloads among containers while the container communications are generally not efficient. Compared with hardware modification solutions, properly distributing containers among hosts is an efficient and low-cost way to reduce communication overhead. However, we observe that this approach yields a trade-off between the communication overhead and the overall throughput of the cluster. In this paper, we focus on the communication-aware container redistribution problem to optimize the communication overhead and the overall throughput jointly for VNF clusters. We propose a solution called FreeContainer which utilizes a novel two-stage algorithm to re-distribute containers among hosts. We implement FreeContainer in Baidu clusters with 6000 servers and 35 services deployed. Extensive experiments on real networks are conducted to evaluate the performance of the proposed approach. The results show that FreeContainer can increase the overall throughput up to 90% with significant reduction on communication overhead.
[computer networks, Containers, Throughput, communication-aware container re-distribution approach, virtualisation, high-performance virtual network function, Servers, communication overhead reduction, FreeContainer solution, two-stage algorithm, System Throughput, Hardware, high-performance VNF, Container Communication, Resource management, Transient analysis, Kernel, High-performance VNF]
Minimizing Cost in IaaS Clouds Via Scheduled Instance Reservation
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Regular diurnal patterns are often seen in the workloads of cloud-based online applications. This kind of non-stationary workloads changes the processing demands over time. To run application services with minimum costs, the number of cloud instances can be dynamically adjusted according to the workload variations. Recently, a new type of scheduled instances has emerged in the Infrastructure-as-a-Service market to facilitate such configurations. Scheduled instances can be reserved based on a recurring schedule and they offer price discounts. Meanwhile, cloud vendors require minimum scheduled durations to avoid the overhead of frequently launching and terminating cloud instances. Coupled with traditional on-demand and reserved instances, it becomes more complicated for users to find the optimal combination of these three pricing options to minimize their monetary costs. For the new scheduled instances, not only the number of instances but also their start and stop times have to be decided. In this paper, we develop a fast and effective strategy to solve this problem. Based on the hourly workload distributions, we first compute the optimal number of instances to acquire for each pricing option. Then, we design a scheduling algorithm to arrange the scheduled instances in compliance with the restriction of their scheduled durations. Using the workloads of the LOL online game and the Wikipedia Mobile service as two case studies, the efficacy of our strategy is demonstrated.
[Cloud computing, Schedules, IaaS clouds, Encyclopedias, pricing options, Optimization, LOL online game, Wikipedia mobile service, Pricing, Games, scheduling, infrastructure-as-a-service market, scheduled instance reservation, Resource management, cloud computing]
Efficient Distributed Coordination at WAN-Scale
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Traditional coordination services for distributed applications do not scale well over wide-area networks (WAN): centralized coordination fails to scale with respect to the increasing distances in the WAN, and distributed coordination fails to scale with respect to the number of nodes involved. We argue that it is possible to achieve scalability over WAN using a hierarchical coordination architecture and a smart token migration mechanism, and lay down the foundation of a novel design for a flexible-consistent coordination framework, called WanKeeper. We implemented WanKeeper based on the ZooKeeper API and deployed it over WAN as a proof of concept. Our experimental results based on the Yahoo! Cloud Serving Benchmark (YCSB), Apache BookKeeper replicated log service, and the Shared Cloud-backed File System (SCFS) show that WanKeeper provides multiple folds improvement in write/update performance in WAN compared to ZooKeeper, while keeping the same read performance.
[Wide area networks, WAN-scale, Protocols, ZooKeeper API, application program interfaces, wide area networks, centralized coordination, Scalability, Apache BookKeeper replicated log service, flexible-consistent coordination framework, Yahoo! Cloud Serving Benchmark, Shared Cloud-backed File System, hierarchical coordination architecture, Servers, WanKeeper, smart token migration mechanism, YCSB, Fault tolerance, File systems, Fault tolerant systems, wide-area networks, write/update performance, SCFS, distributed coordination]
Specifying a Distributed Snapshot Algorithm as a Meta-Program and Model Checking it at Meta-Level
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The paper proposes a new approach to model checking Chandy-Lamport Distributed Snapshot Algorithm (CLDSA). The essential of the approach is that CLDSA is specified as a meta-program in Maude such that the meta-program takes a specification of an underlying distributed system (UDS) and generates the specification of the UDS on which CLDSA is superimposed (UDS-CLDSA). To model check that a UDS-CLDSA enjoys a desired property, it suffices that human users specify the UDS for the proposed approach, while human users need to specify the UDS-CLDSA for the existing approach for each UDS. Since the proposed approach conducts model checking at meta-level, it produces a counterexample if a UDS-CLDSA does not enjoy the property, while the existing approach does not. Our method specifying CLDSA as a meta-program can be applied to formal specification of the class of distributed algorithms that are superimposed on UDSs.
[metaprogram, meta-programming, Conferences, Computational modeling, distributed snapshot algorithm, Maude, system specification, Electronic mail, UDS-CLDSA, Distributed computing, formal specification, formal verification, model checking, distributed algorithms, Chandy-Lamport distributed snapshot algorithm, underlying distributed system, Model checking, reachability, Reliability, Distributed algorithms]
Self-Evolving Subscriptions for Content-Based Publish/Subscribe Systems
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Traditional pub/sub systems cannot adequately handle workloads of applications with dynamic, short-lived subscriptions such as location-based social networks, predictive stock trading, and online games. Subscribers must continuously interact with the pub/sub system to remove and insert subscriptions, thereby inefficiently consuming network and computing resources, and sacrificing consistency. In the aforementioned applications, we recognize that the changes in the subscriptions can follow a predictable pattern over some variable (e.g., time). In this paper, we present a new type of subscription, called evolving subscription, which encapsulates these patterns and allow the pub/sub system to autonomously adapt to the dynamic interests of the subscribers without incurring an expensive re-subscription overhead. We propose a general model for expressing evolving subscriptions and a framework for supporting them in a pub/sub system. To this end, we propose three different designs to support evolving subscriptions, which are evaluated and compared to the traditional resubscription approach in the context of two use cases: online games and high-frequency trading. Our evaluation shows that our solutions can reduce subscription traffic by 96.8% and improve delivery accuracy when compared to the baseline resubscription mechanism.
[location-based social networks, content-based publish-subscribe systems, Engines, self-evolving subscriptions, delivery accuracy, Semantics, high-frequency trading, Evolving Subscriptions, online games, Meteorology, middleware, Dead reckoning, message passing, Social network services, pub-sub system, short-lived subscriptions, Routing, Publish/Subscribe, Middleware, dynamic interests, baseline resubscription mechanism, predictive stock trading, general model, Games, Dynamic Subscriptions, dynamic subscriptions]
Scalable Routing for Topic-Based Publish/Subscribe Systems Under Fluctuations
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The loose coupling and the inherent scalability make publish/subscribe systems an ideal candidate for event-driven services for wireless networks using low power protocols such as IEEE 802.15.4. This work introduces a distributed algorithm to build and maintain a routing structure for such networks. The algorithm dynamically maintains a multicast tree for each node. While previous work focused on minimizing these trees we aim to keep the effort to maintain them in case of fluctuations of subscribers low. The multicast trees are implicitly defined by a novel structure called augmented virtual ring. The main contribution is a distributed algorithm to build and maintain this augmented virtual ring. Maintenance operations after sub-and unsubscriptions require message exchange in a limited region only. We compare the average lengths of the constructed forwarding paths with an almost ideal approach. As a result of independent interest we present a distributed algorithm using messages of size O(log n) for constructing virtual rings of graphs that are on average shorter than rings based on depth first search.
[message exchange, topic-based publish/subscribe systems, Fluctuation, Virtual ring routing, maintenance operations, augmented virtual ring, Overlay networks, Wireless networks, fluctuations, routing structure, Distributed algorithms, O(log n), middleware, Fluctuations, message passing, multicast tree, trees minimization, network routing, trees (mathematics), Maintenance engineering, Routing, wireless networks, software maintenance, distributed algorithm, distributed algorithms, Distributed algorithm, Peer-to-peer computing, minimisation, event-driven services, low power protocols, computational complexity, Pub/Sub Systems]
OPPay: Design and Implementation of a Payment System for Opportunistic Data Services
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The large number of personal wireless devices in the urban areas could be used to provide various opportunistic data services, such as WiFi sharing, content-based file sharing and opportunistic networking. In order to facilitate these services, it is essential to incentivise the device owners to become service providers. However, previous research failed to deliver any practical payment systems for opportunistic data services. Inspired by smart contracts functionalities of bitcoin, this paper proposes a payment system named OPPay for opportunistic data services, which implements a micropayment communication protocol for mobile devices to perform data transactions and make payments using bitcoin. The system is designed to make incremental payments and thus resilient to interrupted communications caused by human mobility in the mobile network. By implementing and evaluating the system for three different applications, we show that the system is able to work in heterogeneous hardware and software environments and can achieve fast transactions confirmation with small fee overhead and low faulty payment value.
[Performance evaluation, Protocols, Bitcoin, micropayment, Wireless communication, cryptocurrency, mobile computing, opportunistic data service, smart contract, opportunistic data services, OPPay, payment system, micropayment communication protocol, Contracts, Wireless fidelity, finance]
Optimal Resource Allocation for Multi-user Video Streaming over mmWave Networks
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
We investigate the resource allocation problem, including time slot allocation, channel allocation, and power adaptation, in a millimeter Wave (mmWave) network with multiple transmission links, multiple channels, and a PicoNet Coordinator (PNC). Each link has a video session to transmit from the transmitter to the receiver. The objective is to minimize the number of time slots to finish the video sessions of all links by jointly optimizing channel allocation and time slot allocation for links, while considering the possible interference between different links on the same channel. The optimal solution for the formulated problem is computationally prohibitive to obtain due to the exponential complexity. We developed a column generation based method to reformulate the original problem into a main problem along with a series of sub-problems, with greatly reduced complexity. We prove that the optimal solution for the reformulated problem converges to the optimal solution of the original problem, and we derived a lower bound for the performance of the reformulated problem at each iteration, which will finally converge to the global optimal solution. The proposed scheme is validated with simulations with its superior performance over existing work is observed.
[Schedules, iterative methods, video session, power adaptation, PNC, column generation-based method, global optimal solution, resource allocation, piconet coordinator, iteration, video streaming, channel allocation, video communication, mmWave networks, 5G wireless, Interference, Receivers, mmWave communications, column generation, multiple transmission links, multi-user video streaming, picocellular radio, exponential complexity, resource allocation., time slot allocation, Streaming media, Channel allocation, optimal resource allocation, Resource management, multiuser video streaming, Signal to noise ratio, computational complexity, millimeter wave network]
A Multi-agent Parallel Approach to Analyzing Large Climate Data Sets
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Despite various cloud technologies that have parallelized and scaled up big data analysis, they target data mostly in texts which are easy to partition and thus easy to map over a cluster system. Therefore, their parallelization do not necessarily cover scientific structured data such as NetCDF or need additional, user-provided tools to convert the original data into specific formats. To facilitate user-intuitive parallelization of such scientific data analysis, this paper presents an agent-based approach that instantiates distributed arrays over a cluster system, maintains structured scientific data in these arrays, deploys many mobile agents over the arrays to perform computational actions on data, and collects necessary results. To demonstrate the practicability of our agent-based approach, we focused on climate change research and implemented a web-interfaced climate analysis, using the MASS (multi-agent spatial simulation) library. In this paper, we show practical advantages of, performance improvements by, and challenges for our agent-based approach in structured data analysis.
[multi-agent parallel approach, Data analysis, data analysis, Multi-agents, Sparks, parallel processing, climate analysis, Mobile agents, scientific data analysis, Distributed databases, mobile agents, MASS, Libraries, Arrays, cloud computing, large climate data sets, big data, Meteorology]
Energy Proportional Servers: Where Are We in 2016?
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The huge energy consumption in data centers produces not only high electricity bill but also tremendous carbon footprints. Although today's servers and data centers of leading internet companies are more energy efficient than ever before, the fluctuations in external workload and internal resource utilization calls for energy proportional computing. Insight into server energy proportionality can help improve workload placement while also reducing energy consumption. In this paper, we investigate all 477 valid published results of SPECpower_ssj benchmark from 2007 to 2016Q3 and reorganize them by hardware availability year for more accurate analysis on production servers. Through comprehensive analysis we find that: (1) The specious stagnation of energy proportionality in recent years is mainly caused by the adoption of processors of specific microarchitecture and is not the indicative trend of energy proportionality improvement. (2) Microarchitecture evolution has more influence on energy efficiency improvement than energy proportionality. (3) Today's servers' peak energy efficiencies are shifting from 100% resource utilization to 80% or 70% utilization and server energy proportionality improves with such shifting. We then conduct extensive experiments on 4 rack servers to investigate the energy efficiency variations under different hardware configurations, including memory per core installation and processor frequency scaling. Our experiments show that hardware configuration has significant impact on server's energy efficiency. Our findings presented in this paper provide useful insights and guidance to system designers, as well as data center operators for energy proportionality aware workload placement and energy savings.
[Measurement, Energy consumption, production servers, processor microarchitecture, data centers, electricity bill, energy savings, Servers, internal resource utilization, energy proportional computing, energy proportionality aware workload placement, Energy Proportionality, power aware computing, resource allocation, Benchmark testing, Market research, Data Centers, Hardware, energy consumption, memory per core installation, energy proportionality improvement, energy efficiency improvement, external workload, rack servers, energy efficiency variations, computer centres, Energy Efficiency, carbon footprints, SPECpower_ssj, energy conservation, Energy efficiency, processor frequency scaling, server energy efficiency]
Are HTTP/2 Servers Ready Yet?
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Superseding HTTP/1.1, the dominating web protocol, HTTP/2 promises to make web applications faster and safer by introducing many new features, such as multiplexing, header compression, request priority, server push, etc. Although a few recent studies examined the adoption of HTTP/2 and evaluated its impacts, little is known about whether the popular HTTP/2 servers have correctly realized the new features and how the deployed servers use these features. To fill in the gap, in this paper, we conduct the first systematic investigation by inspecting six popular implementations of HTTP/2 servers (i.e., Nginx, Apache, H2O, Lightspeed, nghttpd and Tengine) and measuring the top 1 million Alexa web sites. In particular, we propose new methods and develop a tool named H2Scope to assess the new features in those servers. The results of the large-scale measurement on HTTP/2 web sites reveal new observations and insights. This study sheds light on the current status and the future research of HTTP/2.
[Multiplexing, Water, Nginx, nghttpd, H2O, Receivers, HTTP/1.1 servers, Time measurement, Lightspeed, Web protocol, Servers, Apache, Computer crime, Alexa Web sites, transport protocols, Tengine, file servers, HTTP/2 servers, H2Scope tool, Internet, Web sites]
Data Integrity for Collaborative Applications over Hosted Services
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In this work we focus on integrity and consistency of data accessed and manipulated by multiple collaborating users, and stored in an (untrusted) hosted service. This is a problem, aspects of which have been studied in isolation in hitherto distinct communities. Consistency is one of the cardinal problems of distributed computing. Integrity of hosted data has been studied over the last decade, and numerous techniques for proof of data possession and/or retrievability have been explored. The latter line of work however have often assumed static data, and techniques to handle dynamic or versioned data have only very recently been proposed. Yet, even the existing solutions that handle mutable content do so under the assumption that only a single data owner (using a single client) manipulate and verify said data. This is a serious limitation in terms of the variety of applications that can benefit from such mechanisms for proof of data possession. The novelty, and primary contribution of this work is in filling this gap. Specifically, we extend the existing ideas of proof of possession of dynamic data, in order to support multiple users who may collaborate in real time or asynchronously. In contrast (and addition) to the challenge of an untrusted storage server that existing techniques for proof of data possession need to overcome, we had to, simultaneously account for data integrity violations that may be incurred due to all the usual challenges of maintaining consistency of collaborative data (even if the storage server was trusted).
[collaborative applications, untrusted hosted service, Servers, hosted data integrity, consistency, distributed computing, multiple collaborating users, data retrievability, cloud, storage management, data storage, integrity, Semantics, static data, Distributed databases, groupware, Real-time systems, user collaboration, dynamic, versioned data handling, data manipulation, information retrieval, Data structures, data integrity, hosted service, dynamic data handling, collaborative data consistency, mutable content handling, data owner, data possession, untrusted storage server, Collaboration, collaboration, data access, Syntactics, data integrity violation, trusted computing]
Virtual Machine Power Accounting with Shapley Value
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The ever-increasing power consumption of datacenters has eaten up a large portion of their profit. One possible solution is to charge datacenter users for their actual power usage. However, it poses a great technical challenge as the power of VMs co-existing in a physical machine cannot be measured directly. It is thus critical to develop a fair method to disaggregate the power of a physical machine to individual VMs. We tackle the above challenge by modeling the power disaggregation problem as a cooperative game and propose non-deterministic Shapley value to discover the fair power share of VMs (in the sense of satisfying four desired axiomatic principles), while compensating the negative impact of VM power variation. We demonstrate that the results from existing power model-based solution can deviate from the "ground truth" by 25.22% ~46.15%. And compared with the exact Shapley value, our non-deterministic Shapley value can achieve less than 5% error for 90% of the time.
[power model-based solution, physical machine, Power Disaggregation, Servers, power consumption, Power measurement, power aware computing, power usage, Shapley Value, Pricing, data center power consumption, Hardware, Power demand, Radiation detectors, Estimation, game theory, computer centres, Fairness, VM power variation, power disaggregation problem, fair power share, cooperative game, nondeterministic Shapley value, Power Accounting, virtual machines, virtual machine power accounting, axiomatic principles, Virtual Machine]
A General Purpose Testbed for Mobile Data Gathering in Wireless Sensor Networks and a Case Study
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In recent years, mobile data gathering in wireless sensor networks has attracted much interests in the research community. However, despite extensive efforts, many of previous work in this area lies only in theory and evaluates network performance with computer simulations, which leaves a large gap from reality. In this paper, we present the design and implementation of a general purpose, flexible platform for mobile data gathering in wireless sensor networks to evaluate network performance and algorithms in a practical setting. Instead of relying on hand-crafted theoretical models, our platform integrates both mobile data collector and sensor nodes to provide realistic performance evaluations. In addition, the platform adopts a modular design in mobile data collector and sensor nodes, and equips the mobile data collector with advanced computing capability, which makes it versatile for evaluating the performance of a wide-range of applications. Finally, as a case study, weimplement a wildlife monitoring system on our platform. Our experimental results demonstrate that real implementations can evaluate many practical performance factors which would have a great impact on the sensing results and are very difficult to fully capture by theoretical models and simulations. We expect that this platform can become a very powerful general tool for more accurate network simulations and facilitate performance optimization in wireless sensor networks.
[Protocols, wireless sensor networks, performance evaluation, Mobile communication, mobile data gathering, mobile data collector, Wireless communication, Wireless sensor networks, sensor nodes, optimisation, general purpose testbed, high-customizable computing, testbed, Robot sensing systems, Software, performance optimization, Mobile computing, high-performance computing]
On Directional Neighbor Discovery in mmWave Networks
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The directional neighbor discovery problem, i.e., spatial rendezvous, is a fundamental problem in millimeter wave (mmWave) networks. The challenge is how to let the transmitter and receiver beams meet in space under deafness caused by directional transmission and reception. In this paper, we present a Hunting-based Directional Neighbor Discovery (HDND) scheme, where a node continuously rotates its directional beam to scan its neighborhood for neighbors. Through a rigorous analysis, we derive the conditions for ensured neighbor discovery, as well as a bound for the worst case discovery time. We validate the analysis with extensive simulations, and demonstrate the superior performance of the proposed scheme over two benchmark schemes.
[radio networks, mmWave networks, directional transmission, spacial rendezvous., directional neighbor discovery, neighbor discovery, 5G Wireless, receiver beams, spatial rendezvous, Wireless communication, worst case discovery time, Transmitters, benchmark schemes, transmitter beams, HDND scheme, Receiving antennas, Directive antennas, directional antenna, Angular velocity, hunting-based directional neighbor discovery, millimeter wave networks]
Observable-by-Design
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
We present the observable-by-design principle. We believe that the new generation of services, products, and environment management systems should be designed to adapt to changes. Therefore, they should be designed to be observable, and their design should proactively and reactively adapt to the changes observed both internally and externally. Two concrete examples illustrate the application of observable-by-design principle: (1) ship building and management, and (2) river dam water flow management. We believe that the observable-by-design principle can be applied in a large scale. In the long term, a new generation of observable- by-design infrastructures can be built that incorporates the sensing and adapting capabilities in their construction.
[Computational modeling, ship building, Weather forecasting, CAD, Big Data, ship management, Rivers, Floods, Fuels, marine engineering, dams, observable-by-design principle, IoT, ships, Concrete, river dam water flow management, sensor solutions, Marine vehicles, observable-by-design, environmental management, environment management systems]
An Architectural Vision for a Data-Centric IoT: Rethinking Things, Trust and Clouds
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The Internet of Things (IoT) is producing a tidal wave of data, much of it originating at the network edge, from applications with requirements unmet by the traditional back-end Cloud architecture. To address the disruption caused by the overabundance of data, this paper offers a holistic data-centric architectural vision for the data-centric IoT. It advocates that we rethink our approach to the design and definition of key elements: that we shift our focus from Things to Smart Objects; grow Trust organically; and evolve back-end Clouds toward Edge and Fog clouds, which leverage data-centric networks and enable optimal handling of upstream data flows. Along the way, we wax poetic about several blue-sky topics, assess the status of these elements in the context of related work, and identify known gaps in meeting this vision.
[Fog computing, Cloud computing, Conferences, Trustworthiness, upstream data flows, CDN, Edge computing, IoT, Smart Objects, Trust calculus, Distributed databases, Digital Objects, Computer architecture, blue-sky topics, reverse CDN, cloud computing, data-centric IoT, information centric networks, Object Frameworks, smart objects, Internet of Things, computer centres, Trust, back-end cloud architecture, ICN, holistic architectural vision, Content Distribution Networks]
Edge Computing and IoT Based Research for Building Safe Smart Cities Resistant to Disasters
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Recently, several researches concerning with smart and connected communities have been studied. Soon the 4G / 5G technology becomes popular, and cellular base stations will be located densely in the urban space. They may offer intelligent services for autonomous driving, urban environment improvement, disaster mitigation, elderly/disabled people support and so on. Such infrastructure might function as edge servers for disaster support base. In this paper, we enumerate several research issues to be developed in the ICDCS community in the next decade in order for building safe, smart cities resistant to disasters. In particular, we focus on (A) up-to-date urban crowd mobility prediction and (B) resilient disaster information gathering mechanisms based on the edge computing paradigm. We investigate recent related works and projects, and introduce our on-going research work and insight for disaster mitigation.
[urban environment improvement, intelligent services, disaster relief, resilient disaster information gathering mechanisms, IoT based research, edge computing paradigm, Servers, disabled people, Edge computing, IoT, disaster mitigation, 5G technology, 4G technology, Sensors, ICDCS community, Smart cities, edge computing, emergency management, urban space, cellular base stations, elderly people, crowd mobility prediction, Automobiles, Internet of Things, smart communities, up-to-date urban crowd mobility prediction, smart cities, building safe smart cities]
The Internet of Things and Multiagent Systems: Decentralized Intelligence in Distributed Computing
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Traditionally, distributed computing concentrates on computation understood at the level of information exchange and sets aside human and organizational concerns as largely to be handled in an ad hoc manner. Increasingly, however, distributed applications involve multiple loci of autonomy. Research in multiagent systems (MAS) addresses autonomy by drawing on concepts and techniques from artificial intelligence. However, MAS research generally lacks an adequate understanding of modern distributed computing. In this Blue Sky paper, we envision decentralized multiagent systems as a way to place decentralized intelligence in distributed computing, specifically, by supporting computation at the level of social meanings. We motivate our proposals for research in the context of the Internet of Things (IoT), which has become a major thrust in distributed computing. From the IoT's representative applications, we abstract out the major challenges of relevance to decentralized intelligence. These include the heterogeneity of IoT components; asynchronous and delay-tolerant communication and decoupled enactment; and multiple stakeholders with subtle requirements for governance, incorporating resource usage, cooperation, and privacy. The IoT yields high-impact problems that require solutions that go beyond traditional ways of thinking. We conclude with highlights of some possible research directions in decentralized MAS, including programming models; interaction-oriented software engineering; and what we term enlightened governance.
[multi-agent systems, Multiagent systems, Computational modeling, Medical services, Norms, Internet of Things, Governance, Distributed computing, distributed computing, artificial intelligence, Temperature sensors, Sociotechnical systems, enlightened governance, Decentralization, Monitoring, MAS, Multi-agent systems, decentralized intelligence]
Internet of Things: From Small- to Large-Scale Orchestration
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The domain of Internet of Things (IoT) is rapidly expanding beyond research, and becoming a major industrial market with such stakeholders as major manufacturers of chips and connected entities (i.e., things), and fast-growing operators of wide-area networks. Importantly, this emerging domain is driven by applications that leverage an IoT infrastructure to provide users with innovative, high-value services. IoT infrastructures range from small scale (e.g., homes and personal health) to large scale (e.g., cities and transportation systems). In this paper, we argue that there is a continuum between orchestrating connected entities in the small and in the large. We propose a unified approach to application development, which covers this spectrum. To do so, we examine the requirements for orchestrating connected entities and address them with domainspecific design concepts. We then show how to map these design concepts into dedicated programming patterns and runtime mechanisms. Our work revolves around domain-specific concepts and notations, integrated into a tool-based design methodology and dedicated to develop IoT applications. We have applied our work across a spectrum of infrastructure sizes, ranging from an automated pilot in avionics, to an assisted living platform for the home of seniors, to a parking management system in a smart city.
[innovative high-value service, orchestration, runtime mechanisms, transportation system, connected entity orchestration, domain-specific design concept, seniors, Programming, Aerospace electronics, Sensor phenomena and characterization, wide-area network, MapReduce, automated pilot, Internet of things, assisted living platform, small-scale orchestration, large-scale orchestration, programming patterns, Monitoring, tool-based design, domain-specific languages, object-oriented programming, programming frameworks, Urban areas, IoT infrastructure, avionics, Internet of Things, parking management system, IoT application development, smart cities, personal health, infrastructure size, smart city]
EdgeOS_H: A Home Operating System for Internet of Everything
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The proliferation of Internet of Everything (IoE) and the success of rich Cloud services have pushed the horizon of a new computing paradigm, Edge Computing, which calls for processing the data at the edge of the network. Smart home as a typical IoE application is being widely adapted into people's lives. Edge Computing has the potential to empower the smart home, but it needs more contribution from the community before it truly benefits our lives. In this paper, we present the vision of EdgeOSH, a home operating system for Internet of Everything. We further discuss functional challenges, namely programming interface, self-management, data management, security &amp; privacy, and naming, as well as non-functional challenges, such as user experience, system cost, delay, and the lack of availability of open testbed. Within each challenge we also discuss the potential directions that are worth further investigation.
[Cloud computing, Data privacy, Edge Computing, Internet of Everything, data management, edge computing, Smart homes, Smart home, Programming, smart home, Internet of Things, self-management, Edge computing, home computing, Operating systems, operating systems (computers), Operating System, programming interface, EdgeOS_H, home operating system, IoE]
A Vision for Zero-Hop Networking (ZeN)
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
It has become increasingly important for content providers (CPs) to reach consumers with low latency. Peering links that connect CPs directly to access Internet service providers (access ISPs) have been used for this purpose thus providing one-hop AS paths from CPs to users. While providing improved latency, these peering links still do not give CPs control over the entire end-to-end path to their users. This has made it difficult for CPs to completely manage user experience. Motivated by this, we propose the deployment of Zero-Hop Networks (ZeN), where a CP's entire end-to-end path to users is under its control. We believe it is important to respond to the compelling demand for ZeN and enable its provision over the shared Internet infrastructure so that all may continue to reap its benefits. In this paper we lay out the vision for ZeN, describing its goals and challenges. We propose to deploy ZeN by allowing CPs to extend their network's control over the access ISP substrate in a way that allows the CP to control the entire end-to-end path. We develop two strawman architectures based on Software-Defined Networking ideas: one based on resource reservation and the other based on network virtualization. We also discuss some elements of a research agenda that is needed to bring ZeN deployments to realization.
[resource reservation, ZeN, software-defined networking ideas, one-hop AS paths, ISP, software defined networking, Routing, Internet service providers, virtualisation, network virtualization, CP, Substrates, content providers, shared Internet infrastructure, Bandwidth, Internet, Resource management, Virtualization, zero-hop networking, Monitoring, strawman architectures]
Structured Overlay Networks for a New Generation of Internet Services
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The dramatic success and scaling of the Internet was made possible by the core principle of keeping it simple in the middle and smart at the edge (or the end-to-end principle). However, new applications bring new demands, and for many emerging applications, the Internet paradigm presents limitations. For applications in this new generation of Internet services, structured overlay networks offer a powerful framework for deploying specialized protocols that can provide new capabilities beyond what the Internet natively supports by leveraging global state and in-network processing. The structured overlay concept includes three principles: A resilient network architecture, a flexible overlay node software architecture that exploits global state and unlimited programmability, and flow-based processing. We demonstrate the effectiveness of structured overlay networks in supporting today's demanding applications and propose forward-looking ideas for leveraging the framework to develop protocols that push the boundaries of what is possible in terms of performance and resilience.
[Internet services, Protocols, in-network processing, Network architecture, flow-based processing, Routing, multicast, low-latency reliable communication, software architecture, Overlay networks, Software architecture, structured overlay networks, overlay node software architecture, overlay networks, Computer architecture, Internet, resilient network architecture, protocols]
Ensuring Network Neutrality for Future Distributed Systems
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Network Neutrality is essential for ensuring a level playing field for the development of new applications and services on the Internet. Laws and rules alone might not be enough to protect innovation, fair competition and consumer's freedom of choice online. The research community has the responsibility to propose solutions that reveal discriminatory traffic management mechanisms on the Internet. We present the potential risks of a non-neutral Internet, identify several open challenges for designing solutions that detect traffic differentiation, and propose a model that addresses such challenges by taking advantage of distributed systems technologies.
[traffic differentiation, Technological innovation, discriminatory traffic management mechanisms, Regulators, Law, future distributed systems, Artificial neural networks, Companies, Distributed Systems, consumer online freedom-of-choice, distributed system technology, Network Neutrality, computer network management, nonneutral Internet, Innovation, innovation protection, Streaming media, network neutrality, Internet, telecommunication traffic, level playing field]
Uncovering the Useful Structures of Complex Networks in Socially-Rich and Dynamic Environments
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Many group activities can be represented as a complex network where entities (vertices) are connected in pairs by lines (edges). Uncovering a useful global structure of complex networks is important for understanding system behaviors and in providing global guidance for application designs. We briefly review existing network models, discuss several tools used in the traditional graph theory, distributed computing, distributed systems, and social network communities, and point out their limitations. We discuss opportunities to uncover the structural properties of complex networks, especially in a mobile environment, and we summarize three promising approaches for uncovering useful structures: trimming, layering, and remapping. Finally, we present some challenges in algorithmic techniques, with a focus on distributed and localized solutions, to represent various structures.
[algorithmic techniques, Social network services, graph theory, social networks, Mobile communication, complex networks, socially-rich environments, distributed computing, dynamic environments, distributed and localized solutions, structural properties, Vehicular ad hoc networks, social network communities, Complex networks, dynamic systems, social networking (online), distributed systems, Peer-to-peer computing, Mathematical model, Mobile computing]
Future Networking Challenges: The Case of Mobile Augmented Reality
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Mobile augmented reality (MAR) applications are gaining popularity due to the wide adoption of mobile and especially wearable devices. Such devices often present limited hardware capabilities while MAR applications often rely on computationally intensive computer vision algorithms with extreme latency requirements. To compensate for the lack of computing power, offloading data processing to a distant machine is often desired. However, if this process introduces new constrains in the application, especially in terms of latency and bandwidth. If current network infrastructures are not ready for such traffic, we envision that future wireless networks such as 5G will rapidly be saturated by resource hungry MAR applications. Moreover, due to the high variance of wireless networks, MAR applications should not rely only on the evolution of infrastructures. In this article, we analyze MAR applications and justify their need for accessing external infrastructure. After a review of the existing network infrastructures and protocols, we define guidelines for future real-time and multimedia transport protocols, with a focus on MAR offloading.
[Cloud computing, data processing offloading, network infrastructures, Glass, Mobile communication, augmented reality, wireless networks, Augmented reality, network protocols, mobile computing, Streaming media, data handling, mobile augmented reality, Smart phones, MAR]
Software Defined Cyberinfrastructure
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Within and across thousands of science labs, researchers and students struggle to manage data produced in experiments, simulations, and analyses. Largely manual research data lifecycle management processes mean that much time is wasted, research results are often irreproducible, and data sharing and reuse remain rare. In response, we propose a new approach to data lifecycle management in which researchers are empowered to define the actions to be performed at individual storage systems when data are created or modified: actions such as analysis, transformation, copying, and publication. We term this approach software-defined cyberinfrastructure because users can implement powerful data management policies by deploying rules to local storage systems, much as software-defined networking allows users to configure networks by deploying rules to switches.We argue that this approach can enable a new class of responsive distributed storage infrastructure that will accelerate research innovation by allowing any researcher to associate data workflows with data sources, whether local or remote, for such purposes as data ingest, characterization, indexing, and sharing. We report on early experiments with this approach in the context of experimental science, in which a simple if-trigger-then-action (IFTA) notation is used to define rules.
[if-trigger-then-action notation, Computational modeling, Instruments, Distributed databases, Tools, Metadata, software defined networking, software defined cyberinfrastructure, Feature extraction, data lifecycle management, powerful data management policies, distributed storage infrastructure]
Computing in the Continuum: Combining Pervasive Devices and Services to Support Data-Driven Applications
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The exponential growth of digital data sources has the potential to transform all aspects of society and our lives. However, to achieve this impact, the data has to be processed promptly to extract insights that can drive decision making. Further, traditional approaches that rely on moving data to remote data centers for processing are no longer feasible. Instead, new approaches that effectively leverage distributed computational infrastructure and services are necessary. Specifically, these approaches must seamlessly combine resources and services at the edge, in the core, and along the data path as needed. This paper presents our vision for enabling an approach for computing in the continuum, i.e., realizing a fluid ecosystem where distributed resources and services are programmatically aggregated on-demand to support emerging data-driven application workflows. This vision calls for novel solutions for federating infrastructure, programming applications and services, and composing dynamic workflows, which are capable of reacting in real-time to unpredictable data sizes, availabilities, locations, and rates.
[digital data sources, distributed software-defined environments, Computational modeling, data-driven workflows, edge computing, Transforms, Programming, ubiquitous computing, pervasive devices, Engines, dynamic workflow composition, distributed data-streaming, distributed computational infrastructure, Distributed databases, Streaming media, in-transit computing, remote data centers, Data models]
Decision-Driven Execution: A Distributed Resource Management Paradigm for the Age of IoT
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
This paper introduces a novel paradigm for resource management in distributed systems, called decision-driven execution. The paradigm is appropriate for mission-driven systems, where the goal is to enable faster, leaner, and more effective decision making. All resource consumption, in this paradigm, is tied to the needs of making decisions on alternative courses of action. A point of departure from traditional architectures lies in interfaces that allow applications to specify their underlying decision logic. This specification, in turn, allows the system to reason about most effective means to meet information needs of decisions, resulting in simultaneous optimization of decision accuracy, cost, and speed. The paper discusses the overall vision of decision-driven execution, outlining preliminary work and novel challenges.
[distributed resource management paradigm, simultaneous optimization, Roads, Computational modeling, Decision making, Sensor Networks, distributed processing, mission-driven systems, Internet of Things, Optimization, decision logic, IoT, Decision-driven Execution, Learning, optimisation, resource allocation, decision making, decision-driven execution, distributed systems, Distributed Computing Paradigms, Data models, Sensors, Resource management, resource consumption]
ACTiCLOUD: Enabling the Next Generation of Cloud Applications
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Despite their proliferation as a dominant computing paradigm, cloud computing systems lack effective mechanisms to manage their vast amounts of resources efficiently. Resources are stranded and fragmented, ultimately limiting cloud systems' applicability to large classes of critical applications that pose non-moderate resource demands. Eliminating current technological barriers of actual fluidity and scalability of cloud resources is essential to strengthen cloud computing's role as a critical cornerstone for the digital economy. ACTiCLOUD proposes a novel cloud architecture that breaks the existing scale-up and share-nothing barriers and enables the holistic management of physical resources both at the local cloud site and at distributed levels. Specifically, it makes advancements in the cloud resource management stacks by extending state-of-the-art hypervisor technology beyond the physical server boundary and localized cloud management system to provide a holistic resource management within a rack, within a site, and across distributed cloud sites. On top of this, ACTiCLOUD will adapt and optimize system libraries and runtimes (e.g., JVM) as well as ACTiCLOUD-native applications, which are extremely demanding, and critical classes of applications that currently face severe difficulties in matching their resource requirements to state-of-the-art cloud offerings.
[resource management, Cloud computing, cloud computing systems, ACTiCLOUD, novel cloud architecture, scale-up, Servers, rackscale hypervisor, cloud resource management stacks, hypervisor technology, Virtual machine monitors, in-memory databases, Databases, Computer architecture, resource disaggregation, Libraries, Resource management, cloud computing]
JointCloud: A Cross-Cloud Cooperation Architecture for Integrated Internet Service Customization
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Cloud computing has completely changed the economics of IT industry. Recently, the new form of "shared global economy" requires cloud services to be collaboratively provisioned by different cloud providers in a Geo-distributed manner, which brings severe challenges in service performance and cost. To address this problem, in this paper we propose JointCloud, a cross-cloud cooperation architecture for integrated Internet service customization. JointCloud borrows the idea from airline alliances and aims at empowering the cooperation among multiple clouds to provide efficient cross-cloud services. JointCloud focuses not only on the vertical integration of cloud resources but also on the horizontal cooperation among different cloud vendors. This paper describes the concept and architecture of JointCloud, as well as the initial design of the key components, namely, communication, storage, and computation.
[Phase change materials, integrated Internet service customization, shared economy, Cloud computing, service-oriented computing, Virtual machine monitors, cross-cloud cooperation architecture, Computer architecture, Virtual machining, Resource management, cloud computing, JointCloud]
Supporting Data Analytics Applications Which Utilize Cognitive Services
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
A wide variety of services are available over the Web which can dramatically improve the functionality of applications. These services include information retrieval (including data lookups from a variety of sources and Web searches), natural language understanding, visual recognition, and data storage. A key problem is how to provide support for applications which use these services. This paper presents a rich software development kit (SDK) which accesses these services and provides a variety of features applications need to use these services, optimize performance, and compare them. A key aspect of our SDK is its support for natural language understanding services. We also present a personalized knowledge base built on top of our rich SDK that uses publically available data sources as well as private information. The knowledge base supports data analysis and reasoning over data.
[cognitive services, Java, Visualization, Data analysis, data analysis, cloud service ranking, Clouds, natural language processing, Knowledge based systems, Natural languages, information retrieval, software development kit, natural language understanding services, cloud client, data analytics applications, Databases, Web, service ranking, software engineering, SDK, cognitive client]
Trillion Operations Key-Value Storage Engine: Revisiting the Mission Critical Analytics Storage Software Stack
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Data is the new natural resource of this century. As data volumes grow and applications aimed at monetizing the data continue to evolve, data processing platforms are expected to meet new scale, performance, reliability and data retention requirements. At the same time, storage hardware continues to improve in performance and price-performance. In this paper, we present TOKVS - Trillion Operation Key-Value Store, a NoSQL storage engine that redefines the storage software stack to meet the requirements of next-generation applications on next-generation hardware.
[Industries, data retention requirement, data processing platforms, performance requirement, Engines, storage management, Nonvolatile memory, Databases, NoSQL, price-performance, trillion operation key-value storage engine, Computer architecture, Hardware, next-generation hardware, Storage Engine, data analysis, NoSQL databases, NVMe, TOKVS, next-generation applications, reliability requirement, kernel bypass, Software, mission critical analytics storage software stack, NoSQL storage engine, pricing]
How Computer Science Risks to Lose Its Innocence, and Should Attempt to Take Responsibility
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Computer science is playing a driving role in transforming today's society through information technology. In this transformation we observe power shifts increasingly strengthening centralised organisations, which are negatively perceived by many people. We outline technical questions that computer science should pay attention to in order to enable individuals in preserving their interest and to take meaningful decisions based on reliable information.
[Economics, automated decision making, information technology, Decision making, centralised organisations, Tools, Big Data, privacy, Computer science, Privacy, computer science, power shifts, decision making, credibility, Internet, organisational aspects]
A Cognitive Policy Framework for Next-Generation Distributed Federated Systems: Concepts and Research Directions
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Next-generation collaborative activities and missions will be carried out by autonomous groups of devices with a large variety of cognitive capabilities. These devices will have to operate in environments characterized by uncertainty, insecurity (both physical and cyber), and instability. In such environments, communications may be fragmented. Proper policy-based management of such autonomous device groups is thus critical. However current policy management systems have many limitations, including lack of flexibility. In this paper, we articulate novel architectural approaches addressing the requirements for the effective management of autonomous groups of devices and discuss the notion of generative policies - a novel paradigm that enhances the flexibility of policy-based approaches to management. In this paper, we also survey types of policy that are essential for managing device groups. Even though many such policy types exist in conventional settings, their use in our context poses novel challenges that we articulate in the paper. We also introduce a research roadmap discussing several research directions towards the development of a cognitive and flexible policy-based approach to the management of autonomous groups of devices for collaborative missions. Finally, as our proposed policy paradigm is data-intensive, we discuss the problem of supplying the data required for policy decisions in environments characterized by mobility, uncertainly, and fragmented communications.
[Access control, next-generation collaborative activities, Autonomous systems, edge computing, distributed processing, Policy based management, Complexity theory, Distributed computing, Next generation networking, Edge computing, cyber-physical systems, cognitive policy, policy management systems, next-generation distributed federated systems, next-generation collaborative missions, security of data, Coalitions, Collaboration, Generative policies, Computer architecture, groupware, cloud computing, Drones]
Machine to Machine Trust in Smart Cities
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In the coming decades, we will live in a world surrounded by tens of billions of devices that will interoperate and collaborate in an effort to deliver personalized and autonomic services. This paradigm of smart objects and smart things interconnected and ubiquitously surrounding us is called the Internet of Things (IoT). Cities may be the first to benefit from the IoT, but reliance on these machines to make decisions has profound implications for trust, and makes mechanisms for expressing and reasoning about trust essential. This paper introduces the project funded by the Georgia Tech Research Institute to look at several dimensions of Machine to Machine Trust in the context of Smart Cities.
[trust, Smart cities, Computational modeling, personalized services, smart objects, Internet of Things, Intelligent sensors, Sensor arrays, IoT, Georgia Tech Research Institute, Machine-to-machine communications, security, machine-to-machine communication, machine-to-machine trust, smart cities, machine to machine communication, trust essential, risk, smart things, autonomic services]
Lateral Thinking for Trustworthy Apps
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The growing computerization of critical infrastructure as well as the pervasiveness of computing in everyday life has led to increased interest in secure application development. We observe a flurry of new security technologies like ARM TrustZone and Intel SGX, but a lack of a corresponding architectural vision. We are convinced that point solutions are not sufficient to address the overall challenge of secure system design. In this paper, we outline our take on a trusted component ecosystem of small individual building blocks with strong isolation. In our view, applications should no longer be designed as massive stacks of vertically layered frameworks, but instead as horizontal aggregates of mutually isolated components that collaborate across machine boundaries to provide a service. Lateral thinking is needed to make secure systems going forward.
[critical infrastructure, trusted component ecosystem, Substrates, trustworthy apps, lateral thinking, security of data, Isolation technology, secure system design, Hardware, critical infrastructures, Cryptography, Kernel, trusted computing, security technologies]
Rumor Initiator Detection in Infected Signed Networks
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In many cases, the information spread in an online network may not always be truthful or correct; such information corresponds to rumors. In recent years, signed networks have become increasingly popular because of their ability to represent diverse relationships such as friends, enemies, trust, and distrust. Signed networks are ideal for information flow in a network with varying beliefs (trust or distrust) about facts. In this paper, we will study the problem of influence analysis and diffusion models in signed networks and investigate the problem of rumor initiator detection, given the state of the network at a given moment in time. Conventional information diffusion models for unsigned networks cannot be applied to signed networks directly, and we show that the rumor initiator detection problem is NP-hard. We propose a new information diffusion model, referred to as asyMmetric Flipping Cascade (MFC), to model the propagation of information in signed networks. Based on MFC, a novel framework, Rumor Initiator Detector (RID), is introduced to determine the potential number and the identity of the rumor initiators from the state of the network at a given time. Extensive experiments conducted on real-world signed networks demonstrate that MFC works very well in modeling information diffusion in signed networks and RID can significantly outperform other comparison methods in identifying rumor initiators.
[NP-hard, data mining, infected signed networks, Data Mining, MFC, Signed Networks, Information Diffusion, information flow, social network, Analytical models, Semantics, Detectors, information propagation, Social network services, information dissemination, information spread, Diffusion processes, influence analysis, online network, RID, information diffusion models, asymmetric flipping cascade, social networking (online), Rumor Initiator Detection, rumor initiator detection, Integrated circuit modeling]
Addressing Smartphone-Based Multi-factor Authentication via Hardware-Rooted Technologies
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Multi-factor authentication is a well-recognized access control method that enhances the security of users' sensitive data and identities. A successful authentication attempt requires a user to correctly present two or more authentication factors such as knowledge factors, possession factors and inherence factors. For smartphone-based multi-factor authentication, a promising way to authenticate a user is to verify his possession of a legitimate smartphone, which calls for secure and usable device authentication schemes. In this article, we propose to authenticate a device through tracking the hardware fingerprint of its built-in sensor. We first review the existing hardware-rooted identification methods and discuss the merits of applying a hardware fingerprint as a smartphone's unique identity. Then, we analyze the security issues underlying these methods and identify two security requirements for the identification methods to be used in an authentication scheme: Fingerprint Leakage Resilience and Fingerprint Forgery Resilience. Finally, we look into a specific hardware fingerprint originally used for digital cameras. We analyze the feasibility of applying this fingerprint to differentiate off-the-shelf smartphones and list several challenging practical issues underlying this method.
[hardware-rooted identification methods, Fingerprint recognition, smart phones, fingerprint leakage resilience, Resilience, fingerprint forgery resilience, Authentication, message authentication, Cameras, Hardware, Forgery, smartphone-based multifactor authentication, digital cameras]
Enabling Wide Area Data Analytics with Collaborative Distributed Processing Pipelines (CDPPs)
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Life without the Internet is no longer possible nor thinkable. Consider the effects of a prolonged Internet outage: In the least impactful way, most of our kids and peers just would no longer be able to interact with their peers. They might severely miss out on the quality of their leisure time activities which increasingly relies on social networks, online games, YouTube, and other online entertainment offers. This may be a nuisance but still is tolerable. More seriously and economically relevant, manufacturing and trade would no longer work as all interactions inside and among companies rely on a working Internet. Indeed, just-in-time ordering mechanisms and Internet of Things-enhanced production chains within the Industry 4.0 framework would no longer be operational as old-style communication means such as phone and faxes have completely been replaced. Indeed, neither of these alternative mechanisms-faxes, phone, and also messaging-would be available either as they also rely on Internet technology. Even worse, the control of critical infrastructures would also be affected severely as they increasingly rely on the Internet for gathering input data and propagating control information. Moreover, all big data analytic applications, including financial transactions, would fail as they can no longer gather and process their input data. Even worse, the fact that there is &#x201C;no communication without energy&#x201D; nowadays also means that the reciprocal statement applies that there is no &#x201C;energy without communication&#x201D;.
[Cloud computing, data analysis, wide area data analytics, Pipelines, just-in-time ordering mechanisms, Big Data, distributed processing, Industry 4.0, Internet of Things, financial transactions, collaborative distributed processing pipelines, Big Data analytic, Distributed databases, Collaboration, groupware, financial data processing, business data processing, CDPP]
The Millibottleneck Theory of Performance Bugs, and Its Experimental Verification
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The performance of n-tier web-facing applications often suffer from response time long-tail problem. With relatively low resource utilization (less than 50%) and the majority of requests returning within a few milliseconds, a non-negligible num-ber of normally short requests may take seconds to return. We propose the millibottleneck theory of performance bugs (that lead to long-tail problems). Several case studies have confirmed the millibottlenecks (that last a few tens to hundreds of milliseconds) as causal agents of long requests. A concrete example (garbage collection) illustrates the experimental verification of millibottlenecks. An open source fine-grain monitoring toolkit is being devel-oped to facilitate the experimental research on millibottlenecks.
[Google, program debugging, program verification, public domain software, performance debugging, n-tier Web facing, Servers, millibottleneck, Diseases, performance bugs, fine-grain monitoring, Microorganisms, Web services, open source fine-grain monitoring toolkit, Computer bugs, system monitoring, experimental verification, Time factors, millibottleneck theory, resource utilization, Monitoring, response-time long-tail]
Exacution: Enhancing Scientific Data Management for Exascale
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
As we continue toward exascale, scientific data volume is continuing to scale and becoming more burdensome to manage. In this paper, we lay out opportunities to enhance state of the art data management techniques. We emphasize well-principled data compression, and using it to achieve progressive refinement. This can both accelerate I/O and afford the user increased flexibility when she interacts with the data. The formulation naturally maps onto enabling partitioning of the progressively improving-quality representations of a data quantity into different media-type destinations, to keep the highest priority information as close as possible to the computation, and take advantage of deepening memory/storage hierarchies in ways not previously possible. Careful monitoring is requisite to our vision, not only to verify that compression has not eliminated salient features in the data, but also to better understand the performance of massively parallel scientific applications. Increased mathematical rigor would be ideal,to help bring compression on a better-understood theoretical footing, closer to the relevant scientific theory, more aware of constraints imposed by the science, and more tightly error-controlled. Throughout, we highlight pathfinding research we have begun exploring related these topics, and comment toward future work that will be needed.
[media-type destinations, data compression, data management, Computational modeling, Random access memory, progressive refinement, data quantity, Throughput, memory hierarchies, Combustion, parallel processing, quality representations, storage management, high performance scientific data management, storage hierarchies, Data models, data structures, Plasmas, Numerical models, compression, exascale computing]
Hardware Acceleration Landscape for Distributed Real-Time Analytics: Virtues and Limitations
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
We are witnessing a technological revolution with a broad impact ranging from daily life (e.g., personalized medicine and education) to industry (e.g., data-driven healthcare, commerce, agriculture, and mining). At the core of this transformation lies "data". This transformation is facilitated by embedded devices, collectively known as Internet of Things (IoT), which produce real-time feeds of sensor data which are collected and processed to produce a dynamic physical model used for optimized real-time decision making. At the infrastructure level, there is a need to develop a scalable architecture for processing massive volumes of present and historical data at an unprecedented velocity to support the IoT paradigm. To cope with such extreme scale, we argue for the need to revisit the hardware and software co-design landscape in light of two key technological advancements. First is the virtualization of computation and storage over highly distributed data centers spanning across continents. Second is the emergence of a variety of specialized hardware accelerators that complement traditional general-purpose processors. Further efforts are required to unify these two trends in order to harness the power of big data. In this paper, we present a formulation and characterization of the hardware acceleration landscape geared towards real-time analytics in the cloud. Our goal is to assist both researchers and practitioners navigating the newly revived field of software and hardware co-design for building next generation distributed systems. We further present a case study to explore software and hardware interplay for designing distributed real-time stream processing.
[education, data-driven healthcare, hardware-software codesign, real-time analytics, distributed processing, virtualisation, hardware acceleration landscape, commerce, distributed real-time analytics, hardware acceleration, agriculture, software architecture, Program processors, optimized real-time decision making, fpga, general-purpose processors, stream processing, Distributed databases, Computer architecture, distributed databases, Hardware, Real-time systems, IoT paradigm, distributed data centers, software codesign landscape, virtualization, mining, Internet of Things, hardware codesign landscape, computer centres, personalized medicine, dynamic physical model, technological revolution, real-time systems, decision making, next generation distributed systems, Acceleration, Field programmable gate arrays]
Coordinating Distributed Speaking Objects
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In this paper we sketch a vision of future environments densely populated by smart sensors and actuators - possibly embedded in everyday objects - that, rather than simply producing streams of data, are capable of understanding and reporting, via factual assertions and arguments, about what is happening (for sensors) and about what they can make possibly happen (for actuators). These "speaking objects" form the nodes of a dense distributed computing infrastructure that can be exploited to monitor and control activities in our everyday environment. However, the nature of speaking objects will dramatically change the approaches to implementing and coordinating the activities of distributed processes. In fact, distributed coordination is likely to become associated with the capability of argumenting about situations and about the current "state of the affairs\
[Actuators, argumentation, intelligent actuators, distributed processing, hearing objects, smart objects, Automobiles, ubiquitous computing, pervasive computing, Distributed computing, speaking objects, Intelligent sensors, distributed computing, smart sensors, IoT, conversational coordination, Auditory system, intelligent sensors, smart actuators, distributed speaking objects, Monitoring]
Model-Driven Domain-Specific Middleware
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Middleware was introduced to facilitate the development of sophisticated applications based on a uniform methodology and industry standards. However, early research and practice suggested that no one-size-fits-all approach was suitable for all application domains and scenarios. This gave rise to industry initiatives to standardize domain-specific middleware services and profiles, as well as research efforts on configurable, reflective, and adaptive middleware. The industry's approach led to easy deployment, although with a level of flexibility limited by the extent of existing profiles. The approach of the research community, on the other hand, enabled high flexibility, allowing any middleware configuration to be defined. Nevertheless, creating sound configurations using this approach is a challenging task, limiting the target audience to expert engineers. As a consequence, both initiatives do not scale with the current proliferation of specialized application domains. In this paper, we target this problem with an approach that leverages model-driven engineering for the construction of domain-specific middleware platforms. A set of high-level, yet expressive, building blocks is defined in the form of a metamodel, which is used to create models that specify the desired middleware configuration. We argue that this approach enables the rapid development of middleware platforms to match the proliferation of application domains, at the same time as it does not require per-application middleware construction or even highly skilled middleware engineers. We present the current state of our research and discuss research directions to fully realize the approach.
[research community, adaptive middleware, Adaptation models, distributed systems middleware, metamodel, Smart cities, industry standards, Electronic mail, uniform methodology, Middleware, Engines, sound configurations, middleware engineering, Semantics, model-driven domain-specific middleware, model-driven engineering, Model driven engineering, specialized application domains, middleware]
On the Design of a Blockchain Platform for Clinical Trial and Precision Medicine
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
This paper proposes a blockchain platform architecture for clinical trial and precision medicine and discusses various design aspects and provides some insights in the technology requirements and challenges. We identify 4 new system architecture components that are required to be built on top of traditional blockchain and discuss their technology challenges in our blockchain platform: (a) a new blockchain based general distributed and parallel computing paradigm component to devise and study parallel computing methodology for big data analytics, (b) blockchain application data management component for data integrity, big data integration, and integrating disparity of medical related data, (c) verifiable anonymous identity management component for identity privacy for both person and Internet of Things (IoT) devices and secure data access to make possible of the patient centric medicine, and (d) trust data sharing management component to enable a trust medical data ecosystem for collaborative research.
[trust medical data ecosystem, Data privacy, Clinical trials, parallel processing, distributed computing, Big Data integration, identity privacy, trust data sharing management component, IoT, blockchain platform architecture, Big Data analytics, Distributed databases, Computer architecture, Parallel processing, parallel computing, clinical trial, data analysis, big data analytics, Big Data, data integrity, IoT devices, Internet of Things, distributed and parallel computing, blockchain, anonymous identity management component, patient centric medicine, data access security, precision medicine, data privacy, data integration, medical computing, trusted computing]
Towards Dataflow-Based Graph Accelerator
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Existing graph processing frameworks greatly improve the performance of memory subsystem, but they are still subject to the underlying modern processor, resulting in the potential inefficiencies for graph processing in the sense of low instruction level parallelism and high branch misprediction. These inefficiencies, in accordance with our comprehensive micro-architectural study, mainly arise out of a wealth of dependencies, serial semantic of instruction streams, and complex conditional instructions in graph processing. In this paper, we propose that a fundamental shift of approach is necessary to break through the inefficiencies of the underlying processor via the dataflow paradigm. It is verified that the idea of applying dataflow approach into graph processing is extremely appealing for the following two reasons. First, as the execution and retirement of instructions only depend on the availability of input data in dataflow model, a high degree of parallelism can be therefore provided to relax the heavy dependency and serial semantic. Second, dataflow is guaranteed to make it possible to reduce the costs of branch misprediction by simultaneously executing all branches of a conditional instruction. Consequently, we make the preliminary attempt to develop the dataflow insight into a specialized graph accelerator. We believe that our work would open a wide range of opportunities to improve the performance of computation and memory access for large-scale graph processing.
[multiprocessing systems, memory subsystem, Social network services, Pipelines, graph theory, parallelism, data flow graphs, graph processing, graph accelerator, memory access, serial semantic, storage management, modern processor, dataflow, Semantics, data flow computing, Parallel processing, Benchmark testing, Prediction algorithms, Hardware]
Towards a RISC Framework for Efficient Contextualisation in the IoT
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The Internet of Things (IoT) is a new internet evolution that involves connecting billions of internet-connected devices that we refer to as IoT things. These devices can communicate directly and intelligently over the Internet, and generate a massive amount of data that needs to be consumed by a variety of IoT applications. This paper focuses on the automatic contextualisation of IoT data, which also involves distilling information and knowledge from the IoT aiming to simplify answering the following fundamental questions that often arises in IoT applications: Which data collected by IoT are relevant to myself and the IoT Things I care for? Related work around context management and contextualisation ranges from database techniques that involve query re-writing, to semantic web and rule-based context management approaches, to machine learning and data science-based solutions in mobile and ambient computing. All such existing approaches have two main aspects in common: They are highly incompatible and horribly inefficient from a scalability and performance perspective. In this paper, we discuss a new RISC Contextualisation Framework (RCF) we have developed, implemented key aspects of, and assess its scalability. RCF provides fundamental contextualisation concepts that can be mapped to all existing contextualisation approaches for IoT data (and in this sense, it provides a common denominator that unifies the contextualisation space). RCF can be easily implemented as a cloud-based service, and provides better scalability and performance that any of the existing content management and contextualisation approaches in the IoT space.
[Scalability, semantic web, ambient computing, efficient contextualisation, content management, Vehicles, mobile computing, Agriculture, internet evolution, Context, data science-based solutions, Smart cities, automatic information, reduced instruction set computing, Knowledge representation, Internet of Things, machine learning, Intelligent sensors, IoT things, Contextualisation, RCF, cloud-based service, context management, rule-based context management approaches, internet-connected devices, Internet, query re-writing, RISC contextualisation framework]
The Future of the Semantic Web: Prototypes on a Global Distributed Filesystem
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
An important part of the Semantic Web vision is the idea that data is shared seamlessly and that world wide distributed, accessible, and interlinked knowledge bases can be created. However, the current incarnation of the Semantic Web falls short of this vision: while some necessary infrastructure (e.g., Linked Data) has been put in place, the current use of Linked Data in the Semantic Web is still happening in data silos, and sharing and reusing of knowledge is cumbersome and not straightforward. Recently the idea of prototypical objects was proposed to remedy this situation. This concept, known as Prototypes originates from early Frame systems and is also adopted in programming languages such as Javascript. In this vision paper we describe how a distributed file system forms a natural habitat for prototype knowledge representation, advancing the Semantic Web. In particular, we describe how we envision the deployment of Linked Data and Prototype Knowledge bases atop of the InterPlanetary File System (IPFS), which has several useful features matching the needs for knowledge representation based on prototypes.
[prototype knowledge bases, interplanetary file system, pattern matching, semantic Web, Knowledge based systems, distributed file system, Resource description framework, programming languages, Standards, prototypical objects, File systems, features matching, knowledge representation, Prototypes, knowledge based systems, Linked Data, IPFS, distributed databases, prototype knowledge representation]
On Broad Big Data
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
A broad data system explicitly represents a large number of concepts and properties together with its corresponding data proper. We observe the characteristics of several broad data systems and elicit three challenges we will need to research when scaling these broad data systems to become big data systems, too.
[Electronic publishing, graph theory, broad Big Data, Encyclopedias, Big Data, Complexity theory, knowledge graph, broad data systems, ontologies (artificial intelligence), Big data, Internet, Wikidata, Web sites, ontology]
SRLB: The Power of Choices in Load Balancing with Segment Routing
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Network load-balancers generally either do not take application state into account, or do so at the cost of a centralized monitoring system. This paper introduces a load-balancer running exclusively within the IP forwarding plane, i.e. in an application protocol agnostic fashion - yet which still provides application-awareness and makes real-time, decentralized decisions. To that end, IPv6 Segment Routing is used to direct data packets from a new flow through a chain of candidate servers, until one decides to accept the connection, based on its local state. This way, applications themselves naturally decide on how to share incoming connections, while incurring minimal network overhead, and no out-of-band signaling. Tests on different workloads - including realistic workloads such as replaying actual Wikipedia access traffic towards a set of replica Wikipedia instances - show significant performance benefits, in terms of shorter response times, when compared to a traditional random load-balancer.
[random load-balancer, Protocols, load balancing, centralized monitoring system, IPv6 segment routing, real-time decision, Wikipedia, IP forwarding plane, Servers, minimal network overhead, network load-balancers, decentralized decision, Distributed databases, Real-time systems, IP networks, protocols, application-awareness, Monitoring, incoming connection, load-balancing, Routing, candidate servers, segment routing, response time, telecommunication network routing, application-aware, data packets, application protocol agnostic fashion]
Improving Efficiency of Link Clustering on Multi-core Machines
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Link clustering groups different edges in a graph according to their similarities. Link clustering can reveal the overlapping and hierarchical organizations in a wide spectrum of networks. This work studies how to improve efficiency of link clustering along three dimensions, algorithm, modeling, and parallelization, on multi-core machines. We evaluate the efficiency improved due to each of the three dimensions using word association graphs extracted from a twitter dataset.
[Algorithm design and analysis, multi-core, multicore machines, multiprocessing programs, modeling, Social network services, Computational modeling, Merging, graph theory, hierarchical systems, parallelization, parallel processing, link clustering, word association graphs, Clustering algorithms, Twitter dataset, social networking (online), hierarchical organizations, Arrays, Link clustering, algorithm]
S3: Joint Scheduling and Source Selection for Background Traffic in Erasure-Coded Storage
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Erasure-coded storage systems have gained considerable adoption recently since they can provide the same level of reliability with significantly lower storage overhead compared to replicated systems. However, background traffic of such systems - e.g. repair, rebalance, backup and recovery traffic - often has large volume and consumes significant network resources. Independently scheduling such tasks and selecting their sources can easily create interference among data flows, causing severe deadline violation. We show that the well-known heuristic scheduling algorithms fail to consider important constraints, thus resulting in unsatisfactory performance. In this paper, we claim that an optimal scheduling algorithm that aims to maximizethe number of background tasks completed before deadlines must simultaneously consider deadline-aware scheduling, network topology, chunk placement, and time-varying resource availability. To solve this problem, we propose a novel algorithm, called Linear Programming for Selected Tasks (LPST) to maximize the number of successful tasks and improve overallutilization of the datacenter network. It jointly schedules tasks and selects their sources based on a notion of Remaining Time Flexibility, which measures the slackness of the starting time of a task. We evaluated the efficacy of our algorithm using extensive simulations. Our results show that, under certain scenarios, LPST can perform 7x~70x better than the heuristics which blindly treat the infrastructure as a collection of homogeneous resources, and 46.6%~65.9% better than the algorithms that take into accountthe network topology.
[Algorithm design and analysis, Schedules, data flows, deadline-aware scheduling, erasure-coded storage systems, joint scheduling, LPST, linear programming, Servers, background traffic, Optimization, storage management, Network topology, resource allocation, Bandwidth, scheduling, optimal scheduling, Linear programming, network topology, computer centres, source selection, linear programming for selected tasks, datacenter network, data flow computing, erasure-coded storage, time-varying resource availability, chunk placement]
On the Feasibility of Inter-Domain Routing via a Small Broker Set
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The current inter-domain routing protocol, namely, the Border Gateway Protocol (BGP), cannot provide end-to-end (E2E) quality-of-service (QoS) guarantees. The main reason is that an autonomous system (AS) can only receive guarantees from its first hop ASes via service level agreements (SLAs). But beyond the first hop, QoS along the path from source to destination AS is not within the source AS's control regime. In this paper, we investigate the feasibility of providing high QoS-guaranteed E2E transit services by utilizing a (small) set of ASes/IXPs to serve as "brokers" to provide supervision, control and resource negotiation. Finding an optimal set of ASes as brokers can be formulated as a Maximum Coverage with B-dominating path Guarantee (MCBG) problem, which we prove to be NP-hard. To address this problem, we design a (1-e-1/4)-approximation algorithm and also an efficient heuristic algorithm when considering additional constraints (e.g., path length). Based on the current Internet topology, we discover a "3540-alliance" subset (accounting only 6.8%) of 52,079 ASes/IXPs, which can provide high QoS guarantees for 99.29% E2E connections.
[network servers, NP-hard, Heuristic algorithms, inter-domain routing, internetworking, Quality of service, SLA, set theory, ASes/IXPs, end-to-end quality-of-service guarantees, optimisation, dominating-path, approximation algorithm, Internet topology, Economics, approximation theory, service level agreements, autonomous system, QoS-guaranteed E2E transit services, Routing, BGP, Topology, quality of service, maximum coverage with b-dominating path guarantee, border gateway protocol, efficient heuristic algorithm, E2E QoS, interdomain routing protocol, routing protocols, Approximation algorithms, Internet, small broker set, computational complexity, MCBG problem]
Subscription Covering for Relevance-Based Filtering in Content-Based Publish/Subscribe Systems
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Large-scale applications require a scalable data dissemination service with advanced filtering capabilities. We propose the use of a content-based publish/subscribe system with support for top-k filtering in the context of such applications. We focus on the problem of top-k subscription filtering, where a publication is delivered only to the k highest scoring subscribers. The naive approach to perform filtering early at the publisher edge works only if complete knowledge of the subscriptions is available, which is not compatible with the well-established covering optimization in scalable content-based publish/subscribe systems. We propose an efficient rank-cover technique to reconcile top-k subscription filtering with covering. We extend the covering model to support top-k and describe a novel algorithm for forwarding subscriptions to publishers while maintaining correctness. Finally, we compare our solutions to a baseline covering system. In a typical setting, our optimized solution is scalable and provides over 81% of the covering benefit.
[publish/subscribe, message passing, top-k filtering, Social network services, Scalability, Data dissemination, topology, top-k subscription filtering, subscription covering, relevance-based filtering, Routing, information filtering, content-based publish/subscribe systems, Optimization, Databases, rank-cover technique, Semantics, relevance feedback, covering model, middleware]
Workflow Optimization in PAW
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Many industrial applications, from domains such as telecommunication, web and sales, require to perform complex analytics across several data processing systems. The performance of such analytics is usually expressed in workflows, and it is a task that is both labor-intensive and time-consuming. At the same time, with increasing amounts of data to be analysed, the optimization of analytics workflows becomes crucial for satisfying business objectives. This paper focuses on workflow optimization with respect to time efficiency, over multiple execution engines, such as a traditional DBMS, a MapReduce engine, and a scripting engine. This configuration is emerging as a common paradigm used to combine analysis of unstructured and structured data. We propose a novel optimization technique as part of our system called PAW (Platform for Analytics Workflows). This technique creates alternative workflow structures and their execution plans based on equivalent combinations and orders of operators. The technique employs an exhaustive and a heuristic algorithm to search efficiently the space of equivalent workflow structures and select the one with the optimal execution plan. We present a thorough experimental study and we showcase the efficiency of the proposed optimization technique in a fully fledged multi-engine system, applied on three real-world applications and their data, as well as on a synthetic benchmark.
[workflow optimization, platform for analytics workflows, data analysis, Analytics Workflows, PAW, Tools, Data processing, structured data analysis, Electronic mail, Complexity theory, multiengine system, Engines, Optimization, Training, workflow structures, heuristic algorithm, optimisation, Multi-engine Systems, Workflow Optimization, industrial applications, time efficiency, workflow management software, data processing systems, optimal execution plan]
A First Look at Information Entropy-Based Data Pricing
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Distribution of intangible information goods is experiencing tremendous growth in recent years, which has facilitated a blossoming of information goods economics. As big data develops, there are more and more information goods markets for data trading. In the current of data pricing policies in data trading, there are many metrics to measure the value of data goods, such as the data generation date, data volume, and data integrity, etc. However, it is very challenging to identify the amount of data information and its distribution, and the corresponding data pricing has rarely been discussed. In this paper, we propose a new data pricing metric, i.e., the data information entropy, which helps to make a reasonable price in the data trading. We first demonstrate a data information measurement method based on information entropy, and then propose a pricing function based on the result of data information measurement. To comprehensively understand the new data pricing metric and facilitate its application in data trading, we verify the rationality of the data information measurement method and give three concrete pricing functions. It is the first time to look at the information entropy-based data pricing, which can inspire the research concerning the pricing mechanism of data goods, further promoting the development of data products business.
[Measurement, data volume, intangible information goods distribution, data trading, information entropy-based data pricing, data pricing policies, Big Data, data generation date, data integrity, data products business development, commerce, Information entropy, information goods markets, Databases, entropy, concrete pricing functions, Pricing, information goods economics, data goods value, Concrete, big data, data pricing metric, data information measurement method, pricing]
Retrospective Lightweight Distributed Snapshots Using Loosely Synchronized Clocks
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In order to take a consistent snapshot of a distributed system, it is necessary to collate and align local logs from each node to construct a pairwise concurrent cut. By leveraging NTP synchronized clocks, and augmenting them with logical clock causality information, Retroscope provides a lightweight solution for taking unplanned retrospective snapshots of past distributed system states. Instead of storing a multiversion copy of the entire system data, this is achieved efficiently by maintaining a configurable-size sliding window-log at each node to capture recent operations. In addition to retrospective snapshots, Retroscope also provides incremental and rolling snapshots that utilize an existing snapshot to reduce the cost of constructing a new snapshot in proximity. This capability is useful for performing stepwise debugging and root-cause analysis, and supporting data integrity monitoring and checkpoint-recovery. We implement Retroscope for the Voldemort distributed datastore and evaluate its performance under varying workloads.
[program debugging, retrospective lightweight distributed snapshots, distributed processing, Electronic mail, checkpoint-recovery, NTP synchronized clocks, logical clock causality information, Databases, distributed system states, distributed databases, loosely synchronized clocks, Libraries, Monitoring, data-integrity monitoring, Debugging, Synchronization, configurable-size sliding window-log, stepwise debugging, unplanned retrospective snapshots, retroscope distributed datastore, Voldemort distributed datastore, local logs, rolling snapshots, Clocks, incremental snapshots, root-cause analysis]
Power-Aware Population Protocols
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In this paper, we propose a formal energy model which allows an analytical study of energy consumption, for the first time in the context of population protocols (PP). In PP, anonymous and bounded memory agents move unpredictably and communicate in pairs. In order to illustrate the power and the usefulness of the proposed energy model, we develop a new power-aware protocol (EB-TTFM) for the task of data collection. The analytical results show that, in terms of energy consumption, EB-TTFM outperforms a known data collection protocol under certain conditions. Finally, we present a lower bound concerning energy consumption of any possible data collection protocol in PP, which also justifies the efficiency of EB-TTFM.
[power-aware population protocols, Energy consumption, Protocols, formal energy model, population protocols, anonymous agents, mobile sensor networks, Mobile communication, bounded memory agents, Statistics, data collection protocol, data collection, Sociology, power-aware protocol, Data collection, EB-TTFM, protocols, energy consumption, Mobile computing]
MultiPub: Latency and Cost-Aware Global-Scale Cloud Publish/Subscribe
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Topic-based pub/sub is a widely used communication mechanism in distributed systems for targeted information dissemination between loosely coupled entities. To scale dynamically depending on the current communication demands, pub/services can be conveniently deployed in the cloud. To provide fast dissemination, the service can be distributed across multiple cloud regions. The architectural design and run-time deployment of such a middleware is tricky, though, as it can have a significant effect on communication latency and cloud-based cost. In this paper, we propose MultiPub, a flexible pub/sub middleware for latency-constrained, world-wide distributed applications that dynamically reconfigures the communication layer to ensure a predefined maximum latency for publication dissemination while minimizing cloud-based costs. This is achieved by routing publications either through a single or across multiple cloud regions. We demonstrate the effectiveness of MultiPub by presenting a set of experiments that report on the achieved communication latency and cost savings compared to traditional approaches, as well as a performance evaluation.
[publish/subscribe, Cloud computing, Clouds, information dissemination, multiple cloud regions, Servers, cost latency, Publishing, optimization, MultiPub, Bandwidth, Games, cost-aware global-scale cloud publish-subscribe middleware, cloud computing, middleware, topic-based]
Reachability in Binary Multithreaded Programs Is Polynomial
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Automatic finding of bugs in multithreaded programs is an important but inherently difficult task, even in the finite-state interleaving-semantics case. The complexity of this task has only been partially explored so far. We measure quantities such as the diameter, which is the longest finite distance realizable in the transition graph of the program, the local diameter, which is the maximum distance from any program state to any thread-local state, and the computational complexity of bugfinding. For the subclass of so-called binary multithreaded programs, we prove new bounds: all these quantities are majorized by a polynomial and, in certain cases, by a linear, logarithmic, or even constant function. Our bounds present a preparation step towards the corresponding polynomial-bound claims for general programs. These claims contrast sharply with the common belief that the main obstacle to analyzing concurrent programs is the exponential state explosion in the number of threads.
[program debugging, logarithmic function, Instruction sets, general programs, Complexity theory, concurrent programs, binary multithreaded programs, Concurrent computing, diameter, linear function, multithreading, reachability, concurrency (computers), combinatorics, automatic bugs finding, transition graph, reachability analysis, multi-threading, polynomials, constant function, Explosions, Indexes, concurrency, Upper bound, Computer bugs, polynomial, formal methods]
An Event-Level Abstraction for Achieving Efficiency and Fairness in Network Update
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Changes of network state are a common source of instability in networks. An update event typically involves multiple flows that compete for network resources at the cost of rescheduling and migrating some existing flows. Previous network updating schemes tackle such flows independently, rather than as the entity of an update event. They only optimize the flow-level metrics for the flows involved in an update event. In this paper, we present an event-level abstraction of network update which groups flows of an update event and schedules them together to minimize the event completion time (ECT). We then study the scheduling problem of multiple update events for achieving high scheduling efficiency and preserving fairness. The designed least migration traffic first (LMTF) method schedules all update events in the FIFO order, but avoids head-of-line blocking by randomly fine-tuning the queue order of some events. It can considerably reduce the update cost, the average, and tail ECTs of all update events. In addition, we design a general parallel-LMTF (P-LMTF) method to guarantee fairness and further improve scheduling efficiency among update events. It improves the LMTF method by opportunistically updating multiple events simultaneously. The comprehensive evaluation results indicate that the average ECT of our approach is up to 10&#x00D7; faster than the flow-level scheduling method for network update events, and its tail ECT is up to 6x faster. Our P-LMTF method incurs 75% reduction in the average ECT compared with FIFO when the network utilization exceeds 70%, and it achieves a 42% reduction in tail ECT.
[fairness, network utilization, event completion time, efficiency, Conferences, Optimization methods, flow-level metrics, scheduling problem, network instability, Distributed computing, configuration management, LMTF, event-level abstraction, ECT, scheduling, network update, FIFO order, least migration traffic first]
DCM: Dynamic Concurrency Management for Scaling n-Tier Applications in Cloud
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Scaling web applications such as e-commerce in cloud by adding or removing servers in the system is an important practice to handle workload variations, with the goal of achieving both high quality of service (QoS) and high resource efficiency. Through extensive scaling experiments of an n-tier application benchmark (RUBBoS), we have observed that scaling only hardware resources without appropriate adaptation of soft resource allocations (e.g., thread or connection pool size) of each server would cause significant performance degradation of the overall system by either under- or over-utilizing the bottleneck resource in the system. We develop a dynamic concurrency management (DCM) framework which integrates soft resource allocations into the system scaling management. DCM introduces a model which determines a near-optimal concurrency setting to each tier of the system based on a combination of operational queuing laws and online analysis of fine-grained measurement data. We implement DCM as a two-level actuator which scales both hardware and soft resources in an n-tier system on the fly without interrupting the runtime system performance. Our experimental results demonstrate that DCM can achieve significantly more stable performance and higher resource efficiency compared to the state-of-the-art hardware-only scaling solutions (e.g., Amazon EC2-AutoScale) under realistic bursty workload traces.
[cloud scaling, Instruction sets, operational queuing laws, online analysis, Throughput, Servers, Concurrent computing, resource efficiency, Databases, resource allocation, QoS, system performance degradation, Hardware, cloud computing, concurrency (computers), fine-grained measurement data, dynamic concurrency management, Web application scaling, quality of service, DCM, concurrency, soft resource allocation integration, Web application, system scaling management, n-tier application scaling, Resource management, Performance, RUBBoS, n-tier system]
More Peak, Less Differentiation: Towards A Pricing-aware Online Control Framework for Inter-Datacenter Transfers
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The emerging deployment of geographically distributed data centers (DCs) incurs a significant amount of data transfers over the Internet. Such transfers are typically charged by Internet Service Providers (ISPs) with the widely adopted q-th percentile charging model. In such charging model, the time slots with top 100-q percent of data transmission do not affect the total transmission cost, and can be viewed as free. This brings the opportunity to optimize the scheduling of inter-DC transfers to minimize the entire transmission cost. However, very little work has been done to exploit those free time slots for scheduling inter-DC transfers. The crux is that existing work either lacks a mechanism to accumulate traffic to free time slots, or inevitably relies on prior knowledge of traffic arrival patterns. In this paper, we attempt to exploit those free time slots by leveraging diverse time-sensitivities among inter-DC transfers, so as to reduce or even minimize the transmission cost. Specifically, we advocate that a simple principle should be followed: more traffic peaks should be scheduled in free time slots, while less traffic differentiation should be maintained among the remaining time slots. To this end, we take advantage of the Lyapunov optimization techniques to design a pricing-aware control framework. This framework efficiently makes online decisions for inter-DC transfers without requiring a prior knowledge of traffic arrivals. To verify our proposed framework, we conduct small-scale testbed implementation. The results show that our framework can realistically reduce the transmission cost by up to 19.38%.
[inter-data center transfers, diverse time-sensitivities, small-scale testbed implementation, Percentile Pricing Model, Optimal scheduling, ISP, free time slots, Downlink, Scheduling, Lyapunov Optimization, computer centres, traffic arrival patterns, Internet Service Providers, pricing-aware online control framework, Inter-Datacenter Network, geographically distributed data centers, Bandwidth, distributed databases, Lyapunov optimization techniques, Data communication, Uplink, telecommunication traffic, DC]
Robust Multi-tenant Server Consolidation in the Cloud for Data Analytics Workloads
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Server consolidation is the hosting of multiple tenants on a server machine. Given a sequence of data analytics tenant loads defined by the amount of resources that the tenants require and a service-level agreement (SLA) between the customer and the cloud service provider, significant cost savings can be achieved by consolidating multiple tenants. Since server machines can fail causing their tenants to become unavailable, service providers can place replicas of each tenant on multiple servers and reserve capacity to ensure that tenant failover will not result in overload on any remaining server. We present the CUBEFIT algorithm for server consolidation that reduces costs by utilizing fewer servers than existing approaches for data analytics workloads. Unlike existing consolidation algorithms, CUBEFIT can tolerate multiple server failures while ensuring that no server becomes overloaded. Through theoretical analysis and experimental evaluation, we show that CUBEFIT is superior to existing algorithms and produces near-optimal tenant allocation when the number of tenants is large. Through evaluation and deployment on a cluster of 73 machines as well as through simulation studies, we experimentally demonstrate the efficacy of CUBEFIT.
[Algorithm design and analysis, Cloud computing, Data analysis, data analysis, service-level agreement, fault tolerance, data analytics, multiple tenant hosting, near-optimal tenant allocation, multi-tenancy, robustness, server machines, CUBEFIT algorithm, Servers, contracts, Analytical models, cloud server consolidation, data analytics workloads, multitenant server consolidation, file servers, data analytics tenant loads, Robustness, Silicon, cloud computing]
Flow-Aware Adaptive Pacing to Mitigate TCP Incast in Data Center Networks
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In data center networks, many network-intensive applications leverage large fan-in and many-to-one communication to achieve high performance. However, the special traffic patterns, such as micro-burst and high concurrency, easily cause TCP Incast problem and seriously degrade the application performance. To address the TCP Incast problem, we first reveal theoretically and empirically that alleviating packet burstiness is much more effective in reducing the Incast probability than controlling the congestion window. Inspired by the findings and insights from our experimental observations, we further propose a general supporting scheme Adaptive Pacing (AP), which dynamically adjusts burstiness according to the flow concurrency without any change on switch. Another feature of AP is its broad applicability. We integrate AP transparently into different TCP protocols (i.e., DCTCP, L2DCT and D2TCP). Through a series of large-scale NS2 simulations, we show that AP significantly reduces the Incast probability across different TCP protocols and the network goodput can be increased consistently by on average 7x under severe congestion.
[TCP, telecommunication congestion control, flow-aware adaptive pacing, Packet loss, Switches, network-intensive applications, Data center, Incast, Congestion control, Servers, flow concurrency, TCP protocols, packet burstiness, Bandwidth, Data communication, data center networks, Probability, many-to-one communication, TCP Incast problem, computer centres, AP, TCP incast, Incast probability, transport protocols, Pacing, congestion window, DCTCP]
Real-Time Power Cycling in Video on Demand Data Centres Using Online Bayesian Prediction
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Energy usage in data centres continues to be a major and growing concern as an increasing number of everyday services depend on these facilities. Research in this area has examined topics including power smoothing using batteries and deep learning to control cooling systems, in addition to optimisation techniques for the software running inside data centres. We present a novel real-time power-cycling architecture, supported by a media distribution approach and online prediction model, to automatically determine when servers are needed based on demand. We demonstrate with experimental evaluation that this approach can save up to 31% of server energy in a cluster. Our evaluation is conducted on typical rack mount servers in a data centre testbed and uses a recent real-world workload trace from the BBC iPlayer, an extremely popular video on demand service in the UK.
[Video on demand, power smoothing, Switches, Predictive models, multimedia systems, BBC iPlayer, Servers, cooling system control, server energy, media distribution approach, video on demand data centres, deep learning, power aware computing, online Bayesian prediction, Energy, video on demand, space cooling, UK, Real-time systems, real-time power cycling, video on demand service, online prediction model, multimedia servers, batteries, software optimisation technique, Media, data centre, real-time power-cycling architecture, computer centres, data centre energy usage, prediction, Data models, Bayes methods, rack mount servers]
A Distributed Access Control System for Cloud Federations
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Cloud federations are a new collaboration paradigm where organizations share data across their private cloud infrastructures. However, the adoption of cloud federations is hindered by federated organizations' concerns on potential risks of data leakage and data misuse. For cloud federations to be viable, federated organizations' privacy concerns should be alleviated by providing mechanisms that allow organizations to control which users from other federated organizations can access which data. We propose a novel identity and access management system for cloud federations. The system allows federated organizations to enforce attribute-based access control policies on their data in a privacy-preserving fashion. Users are granted access to federated data when their identity attributes match the policies, but without revealing their attributes to the federated organization owning data. The system also guarantees the integrity of the policy evaluation process by using block chain technology and Intel SGX trusted hardware. It uses block chain to ensure that users identity attributes and access control policies cannot be modified by a malicious user, while Intel SGX protects the integrity and confidentiality of the policy enforcement process. We present the access control protocol, the system architecture and discuss future extensions.
[Access control, Cloud computing, Protocols, access management system, data misuse, policy enforcement process, Blockchain, identity management system, Encryption, collaboration paradigm, cloud federations, federated organizations, access control protocol, authorisation, cloud computing, blockchain technology, government policies, Anonymous identities, data leakage, Intel SGX trusted hardware, Cloud federation., private cloud infrastructures, system architecture, Organizations, identity attributes, attribute-based access control policies, data privacy, privacy-preserving fashion, distributed access control system]
Voyager: Complete Container State Migration
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Due to the small memory footprint and fast startup times offerred by container virtualization, made ever more popular by the Docker platform, containers are seeing rapid adoption as a foundational capability to build PaaS and SaaS clouds. For such container clouds, which are fundamentally different from VM clouds, various cloud management services need to be revisited. In this paper, we present our Voyager - just-in-time live container migration service, designed in accordance with the Open Container Initiative (OCI) principles. Voyager is a novel filesystem-agnostic and vendor-agnostic migration service that provides consistent full-system migration. Voyager combines CRIU-based memory migration together with the data federation capabilities of union mounts to minimize migration downtime. With a union view of data between the source and target hosts, Voyager containers can resume operation instantly on the target host, while performing disk state transfer lazily in the background.
[software-as-a-service, Cloud computing, OCI principle, Containers, CRIU-based memory migration, virtualisation, Voyager, Docker platform, disk state transfer, vendor-agnostic migration service, cloud computing, cloud management service, data federation, SaaS cloud, PaaS cloud, just-in-time live container migration service, open container initiative, VM cloud, Image restoration, Noise measurement, container virtualization, filesystem-agnostic migration service, container state migration, Linux, Container Migration, Dual band, virtual machines, Cloud, platform-as-a-service, Lazy Replication, Virtualization]
Keddah: Capturing Hadoop Network Behaviour
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
As a distributed system, Hadoop heavily relies on the network to complete data processing jobs. While Hadoop traffic is perceived to be critical for job execution performance, the actual behaviour of Hadoop network traffic is still poorly understood. This lack of understanding greatly complicates research relying on Hadoop workloads. In this paper, we explore Hadoop traffic through experimentation. We analyse the generated traffic of multiple types of MapReduce jobs, with varying input sizes, and cluster configuration parameters. As a result, we present Keddah, a toolchain for capturing, modelling and reproducing Hadoop traffic, for use with network simulators. Keddah can be used to create empirical Hadoop traffic models, enabling reproducible Hadoop research in more realistic scenarios.
[Performance evaluation, Computational modeling, network simulators, Tools, Big Data, distributed system, Yarn, parallel processing, Hadoop network traffic behaviour, pattern clustering, cluster configuration parameters, Benchmark testing, Keddah, Data models, data handling, data processing jobs, MapReduce jobs, empirical Hadoop traffic models]
A Scalable and Distributed Approach for NFV Service Chain Cost Minimization
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Network function virtualization (NFV) represents the latest technology advancement in network service provisioning. Traditional hardware middleboxes are replaced by software programs running on industry standard servers and virtual machines, for service agility, flexibility, and cost reduction. NFV users are provisioned with service chains composed of virtual network functions (VNFs). A fundamental problem in NFV service chain provisioning is to satisfy user demands with minimum system-wide cost. We jointly consider two types of cost in this work: nodal resource cost and link delay cost, and formulate the service chain provisioning problem using nonlinear optimization. Through the method of auxiliary variables, we transform the optimization problem into its separable form, and then apply the alternating direction method of multipliers (ADMM) to design scalable and fully distributed solutions. Through simulation studies, we verify the convergence and efficacy of our distributed algorithm design.
[service flexibility, nonlinear programming, service agility, auxiliary variables, Servers, Optimization, VNF, nonlinear optimization, Bandwidth, Convex functions, Hardware, Distributed algorithms, network service provisioning, ADMM, hardware middlebox, virtual network functions, cost reduction, network function virtualization, computer networks, system-wide cost minimization, nodal resource cost, Minimization, software programs, industry standard servers, distributed algorithm design, NFV service chain provisioning, distributed algorithm, link delay cost, alternating direction method of multipliers, distributed algorithms, delays, virtual machines, scalable distributed approach, NFV service chain cost minimization]
Elastic Paxos: A Dynamic Atomic Multicast Protocol
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Replication is a common technique used to design reliable distributed systems by masking defective components. To cope with the requirements of modern Internet applications, replication protocols must allow for throughput scalability and dynamic reconfiguration, that is, on-demand replacement or provisioning of system resources. This paper describes Elastic Paxos, a new dynamic atomic multicast protocol that fulfills these requirements. Elastic Paxos allows to dynamically add and remove resources to an online partially replicated state machine. We implemented Elastic Paxos and evaluated its performance in OpenStack, a cloud environment. We demonstrate its practicality to dynamically scale up and down a partially replicated data store with itand to reconfigure a distributed system.
[Scalability, replication protocols, on-demand replacement, Throughput, OpenStack, Servers, scalability, on-demand provisioning, dynamic atomic multicast protocol, cloud environment, distributed systems, system resources, Silicon, dynamic reconfiguration, computer network reliability, modern Internet applications, replicated databases, Multicast protocols, Dynamic scheduling, partially replicated data store, atomic multicast, multicast protocols, online partially replicated state machine, Internet, throughput scalability, Paxos, elastic Paxos]
Boosting The Benefits of Hybrid SDN
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The upgrade of a legacy network to a full software-defined networking (SDN) deployment is usually an incremental process, during which SDN switches and legacy switches coexist in the hybrid network. However, with inappropriate deployment of SDN switches and design of hybrid control, the advantages of SDN control could not exert, and it even results in performance degradation or inconsistency (e.g., loops, black-holes). Therefore, the hybrid SDN requires considerable coordination of the centralized control and distributed routing. In this paper, we propose a solution to handle the heterogeneity caused by distinct forwarding characteristics of SDN and legacy switches, therefore boosting the benefits of hybrid SDN. We plan SDN placement to enhance the SDN controllability over the hybrid network, and conduct traffic engineering considering both the forwarding characteristics of SDN and legacy switches. The experiments with various topologies show that the SDN placement planning and hybrid forwarding yield better network performance especially in the early 70% SDN deployment.
[Access control, SDN placement planning, software-defined networking, Traffic engineering, software defined networking, Routing, distributed routing, SDN controllability, Controllability, Routing protocols, Hybrid Software-Defined Networking, Planning, centralized control, Network planning, hybrid SDN]
Adopting SDN Switch Buffer: Benefits Analysis and Mechanism Design
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
One critical issue in SDN is to reduce the communication overhead between the switches and the controller. Such overhead is mainly caused by handling miss-match packets, because for each miss-match packet, a switch will send a request to the controller asking for forwarding rule. Existing approaches to address this problem generally need to deploy intermediate proxy or authority switches to hold rule copies, so as to reduce the number of requests sent to the controller. In this paper, we argue that using the intrinsic buffer in a SDN switch can also greatly reduce the communication overhead without using additional devices. If a switch buffers each miss-match packet, only a few header fields instead of the entire packet are required to be sent to the controller. Experiment results show that this can reduce 78.7% control traffic and 37% controller overhead at the cost of increasing only 5.6% switch overhead on average. If the proposed flow-granularity buffer mechanism is adopted, only one request message needs to be sent to the controller for a new flow with many arrival packets. Thus the control traffic and controller overhead can be further reduced by 64% and 35.7% respectively on average without increasing the switch overhead.
[arrival packets, Buffer storage, Switches, Aerospace electronics, authority switches, communication overhead, switch, rule copies, SDN switch buffer, flow-granularity buffer mechanism, controller overhead, intermediate proxy, buffer, control traffic, data center networks, forwarding rule, Cloning, software defined networking, Dynamic scheduling, miss-match packets, Standards, telecommunication switching, communication overhead reduction, request message, Software Defined Networking]
IoT SENTINEL: Automated Device-Type Identification for Security Enforcement in IoT
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
With the rapid growth of the Internet-of-Things (IoT), concerns about the security of IoT devices have become prominent. Several vendors are producing IP-connected devices for home and small office networks that often suffer from flawed security designs and implementations. They also tend to lack mechanisms for firmware updates or patches that can help eliminate security vulnerabilities. Securing networks where the presence of such vulnerable devices is given, requires a brownfield approach: applying necessary protection measures within the network so that potentially vulnerable devices can coexist without endangering the security of other devices in the same network. In this paper, we present IoT Sentinel, a system capable of automatically identifying the types of devices being connected to an IoT network and enabling enforcement of rules for constraining the communications of vulnerable devices so as to minimize damage resulting from their compromise. We show that IoT Sentinel is effective in identifying device types and has minimal performance overhead.
[Internet-of-Things, Protocols, Ports (Computers), Security, Object recognition, Internet of Things, threat mitigation, device fingerprinting, IoT SENTINEL, security of data, security enforcement, IoT network, Logic gates, device identification, IoT security, IP networks, Wireless fidelity, automated device-type identification]
Efficient Z-Order Encoding Based Multi-Modal Data Compression in WSNs
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Wireless sensor networks have significant limitationsin available bandwidth and energy. The limited bandwidthin sensor networks can cause higher message delivery latencyin applications such as monitoring poisonous gas leak. In suchapplications, there are multi-modal sensors whose values such astemperature, gas concentration, location and CO2 level need tobe transmitted together for faster detection and timely assessmentof gas leak. In this paper, we propose novel Z-order based datacompression schemes (Z-compression) to reduce energy and savebandwidth without increasing the message delivery latency. Insteadof using the popular Huffman tree style based encoding, Zcompressionuses Z-order encoding to map the multidimensionalsensing data into one-dimensional binary stream transmittedusing a single packet. Our experimental evaluations using realworlddata sets show that Z-compression has a much bettercompression ratio, energy saving, streaming rate than knownschemes like LEC (and adaptive LEC), FELACS and TinyPackfor multi-modal sensor data.
[wireless sensor networks, Data compression, FELACS, gas concentration, Z-order encode, Huffman tree style based encoding, multidimensional sensing data, one-dimensional binary stream, streaming rate, TinyPack, multimodal sensor data, Bandwidth, Z-compression, multimodal data compression, message delivery latency, adaptive LEC, efficient Z-order encoding, data compression, Sensor network, encoding, energy saving, compression ratio, carbon dioxide level, Wireless sensor networks, WSN, Multimodal sensors, poisonous gas leak monitoring, Huffman coding]
PTrack: Enhancing the Applicability of Pedestrian Tracking with Wearables
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The ability to accurately track pedestrians is valuable for variant application designs. Although pedestrian tracking has been investigated excessively and owned a well-suited sensing platform, the proposed solutions are far from being mature yet. Pedestrian tracking contains step counting and stride estimation two components. Step counting already has commercial products, but the performance is still unreliable and less trustworthy in practice. Stride estimation even stays in the research stage without ready solutions released on the market. Such a non-negligible gap between long-term research investigation and technique's actual usage exists due to a series of crucial applicability issues unsolved, including design vulnerability to interfering activities, extracting purely body's movement from additive sensor signals, and parameter training without user's intervention. In this paper, we deeply analyze human's gait cycles and obtain inspiring observations to address these issues. We incorporate our techniques into existing pedestrian tracking designs and implement a prototype, PTrack, on LG smartwatch. We find PTrack effectively enhances the system applicability and achieves promising performance under very practical settings.
[Legged locomotion, Performance evaluation, Accelerometers, watches, Tracking, PTrack, Radiation detectors, Estimation, pedestrians, wearables, LG smartwatch, gait analysis, wearable computers, mobile computing, pedestrian tracking, human gait cycles analysis, Acceleration]
Source Location Privacy-Aware Data Aggregation Scheduling for Wireless Sensor Networks
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Source location privacy (SLP) is an important property for the class of asset monitoring problems in wireless sensor networks (WSNs). SLP aims to prevent an attacker from finding a valuable asset when a WSN node is broadcasting information due to the detection of the asset. Most SLP techniques focus at the routing level, with typically high message overhead. The objective of this paper is to investigate the novel problem of developing a TDMA MAC schedule that can provide SLP. We make a number of important contributions: (i) we develop a novel formalisation of a class of eavesdropping attackers and provide novel formalisations of SLP-aware data aggregation schedules (DAS), (ii) we present a decision procedure to verify whether a DAS schedule is SLP-aware, that returns a counterexample if the schedule is not, similar to model checking, and (iii) we develop a 3-stage distributed algorithm that transforms an initial DAS algorithm into a corresponding SLP-aware schedule against a specific class of eavesdroppers. Our simulation results show that the resulting SLP-aware DAS protocol reduces the capture ratio by 50% at the expense of negligable message overhead.
[Schedules, Protocols, wireless sensor networks, DAS schedule, source location privacy-aware data aggregation scheduling, telecommunication scheduling, data aggregation, WSN node, Data Aggregation Scheduling, Safety, TDMA MAC schedule, Source Location Privacy, Monitoring, TDMA, eavesdropping attackers, Data aggregation, Routing, Wireless Sensor Networks, Wireless sensor networks, SLP techniques, 3-stage distributed algorithm, asset monitoring problems, telecommunication network routing, routing level, broadcasting information, message overhead]
Velocity Optimization of Pure Electric Vehicles with Traffic Dynamics Consideration
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
As Electric Vehicles (EVs) become increasingly popular, their battery-related problems (e.g., short driving range and heavy battery weight) must be resolved as soon as possible. Velocity optimization of EVs to minimize energy consumption in driving is an effective alternative to handle these problems. However, previous velocity optimization methods assume that vehicles will pass through traffic lights immediately at green traffic signals. Actually, a vehicle may still experience a delay to pass a green traffic light due to a vehicle waiting queue in front of the traffic light. In this paper, for the first time, we propose a velocity optimization system which enables EVs to immediately pass green traffic lights without delay. We collected real driving data on a 4.0 km long road section of US-25 highway to conduct extensive trace-driven simulation studies. The experimental results from Matlab and Simulation for Urban MObility (SUMO) traffic simulator show that our velocity optimization system reduces energy consumption by up to 17.5% compared with real driving patterns without increasing trip time.
[Energy consumption, velocity optimization system, Roads, real driving patterns, heavy-battery weight, green traffic signals, Predictive models, road section, Optimization, road vehicles, battery-related problem, velocity optimization method, short-driving range, pass green traffic lights, Mathematical model, energy consumption, pure electric vehicles, simulation-for-urban mobility traffic simulator, queueing theory, battery powered vehicles, traffic dynamic consideration, SUMO traffic simulator, energy consumption minimization, US-25 highway, extensive trace-driven simulation study, Delays, Acceleration, energy consumption reduction, vehicle waiting queue, traffic light, Matlab]
PIANO: Proximity-Based User Authentication on Voice-Powered Internet-of-Things Devices
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Voice is envisioned to be a popular way for humans to interact with Internet-of-Things (IoT) devices. We propose a proximity-based user authentication method (called PIANO) for access control on such voice-powered IoT devices. PIANO leverages the built-in speaker, microphone, and Bluetooth that voice-powered IoT devices often already have. Specifically, we assume that a user carries a personal voice-powered device (e.g., smartphone, smartwatch, or smartglass), which serves as the user's identity. When another voice-powered IoT device of the user requires authentication, PIANO estimates the distance between the two devices by playing and detecting certain acoustic signals; PIANO grants access if the estimated distance is no larger than a user-selected threshold. We implemented a proof-of-concept prototype of PIANO. Through theoretical and empirical evaluations, we find that PIANO is secure, reliable, personalizable, and efficient.
[PIANO, acoustic signals, voice-powered Internet of Things devices, Protocols, Bluetooth, Biometrics (access control), proximity-based user authentication, User authentication, Estimation, Acoustics, Internet of Things, Microphones, microphones, Authentication, message authentication, voice activity detection, authorisation, built-in speaker, distance estimation, access control, microphone, acoustic signal processing]
Category Information Collection in RFID Systems
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In RFID-enabled applications, when a tag is put into use and associated with a specific object, the category-related information (e.g., the brands of clothes) about this object might be preloaded into the tag's memory as required. Since such information reflects the category attributes, all tags in the same category carry the identical category information. To collect this information, we do not need to repeatedly interrogate each tag; one tag's response in a category is sufficient. In this paper, we investigate the new problem of category information collection in a multi-category RFID system, which is referred to as information sampling. We propose an efficient two-phase sampling protocol (TPS). By quickly zooming into a category and isolating a tag from this category, TPS is able to sample a category by broadcasting only 7.5-bit polling vector (very efficient when compared to the 96-bit tag ID). We theoretically analyze the protocol performance and discuss the optimal parameter settings that minimize the overall execution time. Extensive simulations show that TPS outperforms the benchmark, greatly improving the sampling performance.
[Protocols, Dairy products, radiofrequency identification, multicategory RFID system, RFID-enabled application, TPS protocol, Indexes, category attributes, category-related information, category information collection, Temperature sensors, information sampling, identical category information, Broadcasting, Benchmark testing, protocols, efficient two-phase sampling protocol, Radiofrequency identification, tag memory, tag response]
Scalable Role-Based Data Disclosure Control for the Internet of Things
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The Internet of Things (IoT) is the latest Internet evolution that interconnects billions of devices, such as cameras, sensors, RFIDs, smart phones, wearable devices, ODBII dongles, etc. Federations of such IoT devices (or things) provides the information needed to solve many important problems that have been too difficult to harness before. Despite these great benefits, privacy in IoT remains a great concern, in particular when the number of things increases. This presses the need for the development of highly scalable and computationally efficient mechanisms to prevent unauthorised access and disclosure of sensitive information generated by things. In this paper, we address this need by proposing a lightweight, yet highly scalable, data obfuscation technique. For this purpose, a digital watermarking technique is used to control perturbation of sensitive data that enables legitimate users to de-obfuscate perturbed data. To enhance the scalability of our solution, we also introduce a contextualisation service that achieve real-time aggregation and filtering of IoT data for large number of designated users. We, then, assess the effectiveness of the proposed technique by considering a health-care scenario that involves data streamed from various wearable and stationary sensors capturing health data, such as heart-rate and blood pressure. An analysis of the experimental results that illustrate the unconstrained scalability of our technique concludes the paper.
[Access control, digital watermarking technique, Disclosure Control, Data privacy, Correlation, Scalability, Watermarking, Contextualization, Big Data, Internet of Things, Security, smart health care, watermarking, Privacy, scalable role-based data disclosure control, IoT data filtering, medical computing, health care, IoT data aggregation]
Multi-representation Based Data Processing Architecture for IoT Applications
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Internet of Things (IoT) applications like smart cars, smart cities and wearables are becoming widespread and are the future of the Internet. One of the major challenges for IoT applications is efficiently processing, storing and analyzing the continuous stream of incoming data from a large number of connected sensors. We propose a multi-representation based data processing architecture for IoT applications. The data is stored in multiple representations, like rows, columns, graphs which provides support for diverse application demands. A unifying update mechanism based on deterministic scheduling is used to update the data representations, which completely removes the need for data transfer pipelines like ETL (Extract, Transform and Load). The combination of multiple representations, and the deterministic update mechanism, provides the ability to support real-time analytics and caters to IoT applications by minimizing the latency of operations like computing pre-defined aggregates.
[Internet of Things (IoT), deterministic scheduling, Pipelines, data transfer pipelines, wearables, IoT applications, smart cars, Internet of Things, Big Data Variety, Engines, deterministic update mechanism, Deterministic Ordering, Aggregates, Computer architecture, scheduling, smart cities, Data transfer, Multi-representation processing, Sensors, ETL, multirepresentation based data processing architecture]
Long Term Sensing via Battery Health Adaptation
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Energy Neutral Operation (ENO) has created the ability to continuously operate wireless sensor networks in areas such as environmental monitoring, hazard detection and industrial IoT applications. Current ENO approaches utilise techniques such as sample rate control, adaptive duty cycling and data reduction methods to balance energy generation, storage and consumption. However, the state of the art approaches makes a strong and unrealistic assumption that battery capacity is fixed throughout the deployment time of an application. This results in scenarios where ENO systems over allocate sensing tasks, therefore as battery capacity degrades it causes the system to no longer be energy neutral and then fail unexpectedly. In this paper, we formulate the problem to maximise the quality-of-service in terms of duty cycle and the battery capacity to extend the deployment lifetime of a sensing application. In addition, we develop a lightweight algorithm to solve the formulated problem. Moreover, we evaluate the proposed method using real sensor energy consumption data captured from micro-climate sensors deployed in Queen Elizabeth Olympic Park, London. Results show that a 307% extension of deployment lifetime can be achieved when compared to a traditional ENO solution without a reduction in the duty cycle of the sensor.
[Energy consumption, environmental monitoring, wireless sensor networks, battery health, ENO, Batteries, Energy harvesting, Optimization, IoT, Degradation, battery capacity, long term sensing, industrial IoT applications, battery health adaptation, longevity, energy neutral operation, Sensors, energy neutral, duty cycle adaptation, sample rate control, quality-of-service, telecommunication power management, data reduction methods, Internet of Things, sensing application, real sensor energy consumption data, queen Elizabeth olympic park, Wireless sensor networks, hazard detection, WSN, London, microclimate sensors, adaptive duty cycling]
Detecting Time Synchronization Attacks in Cyber-Physical Systems with Machine Learning Techniques
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Recently, researchers found a new type of attacks, called time synchronization attack (TS attack), in cyber-physical systems. Instead of modifying the measurements from the system, this attack only changes the time stamps of the measurements. Studies show that these attacks are realistic and practical. However, existing detection techniques, e.g. bad data detection (BDD) and machine learning methods, may not be able to catch these attacks. In this paper, we develop a "first difference aware" machine learning (FDML) classifier to detect this attack. The key concept behind our classifier is to use the feature of "first difference\
[pattern classification, CPS, Cyber-physical systems, time synchronization attacks detection, first difference aware machine learning, Synchronization, Global Positioning System, synchronisation, cyber-physical systems, TS attacks detection, security of data, Supervised learning, Training data, Detectors, FDML classifier, learning (artificial intelligence)]
Speed-Based Location Tracking in Usage-Based Automotive Insurance
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Usage-based Insurance (UBI) is regarded as a promising way to offer more accurate insurance premium by profiling driving behaviors. Compared with traditional insurance which considers drivers' history of accidents, traffic violations and etc, UBI focuses on driving data and can give a more reasonable insurance premium based on the current driving behaviors. Insurers use sensors in smartphone or vehicle to collect driving data (e.g. mileage, speed, hark braking) and compute a risk score based on these data to recalculate insurance premium. Many insurance programs, which are advertised as being privacy-preserving, do not directly use the GPS-based tracking, but it is not enough to protect driver's location privacy. In real world, many environment factors such as real-time traffic and traffic regulations can influence driving speed. These factors provide the side-channel information about the driving route, which can be exploited to infer the vehicle's trace. Based on the observation, we propose a novel speed based trajectory inference algorithm which can track drivers only with the speed data and original location. We implement the attack on a public dataset in New Jersey. The evaluation results show that the attacker can recover the route with a high successful rate.
[smartphone, Roads, risk score, privacy preserving, GPS, tracking, Vehicles, driver information systems, Vehicular Network, vehicular network, data protection, Real-time systems, Trajectory, insurance data processing, driver location privacy protection, vehicular ad hoc networks, Location Tracking, automobiles, smart phones, New Jersey, Global Positioning System, speed-based location tracking, Usage-based Insurance, usage-based automotive insurance, Insurance, Hidden Markov models, drivers accident history, Inference algorithms, Dynamic Time Warping]
On Efficient Offloading Control in Cloud Radio Access Network with Mobile Edge Computing
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Cloud radio access network (C-RAN) and mobile edge computing (MEC) have emerged as promising candidates for the next generation access network techniques. Unfortunately, although MEC tries to utilize the highly distributed computing resources in close proximity to user equipments equipments (UE), C-RAN suggests to centralize the baseband processing units (BBU) deployed in radio access networks. To better understand and address such a conflict, this paper closely investigates the MEC task offloading control in C-RAN environments. In particular, we focus on perspective of matching problem. Our model smartly captures the unique features in both MEC and C-RAN with respect to communication and computation efficiency constraints. We divide the cross-layer optimization into the following three stages: (1) matching between remote radio heads (RRH) and UEs, (2) matching between BBUs and UEs, and (3) matching between mobile clones (MC) and UEs. By applying the Gale-Shapley Matching Theory in the duplex matching framework, we propose a multi-stage heuristic to minimize the refusal rate for user's task offloading requests. Trace-based simulation confirms that our solution can successfully achieve near-optimal performance in such a hybrid deployment.
[Cloud computing, trace-based simulation, next generation access network techniques, cloud radio access networks, Mobile communication, RRH, telecommunication computing, Optimization, Wireless communication, UE, Cloud Radio Access Network, mobile computing, BBU, Gale-Shapley Matching Theory, MC, multistage heuristic, distributed computing resources, cross-layer optimization, cloud computing, MEC task offloading control, Baseband, Computation Offloading, task offloading requests, remote radio heads, radio access networks, computation efficiency constraints, Radio access networks, baseband processing units, duplex matching framework, mobile edge computing, C-RAN environments, user equipments, mobile clones, Offloading Control, Mobile computing, Mobile Edge Computing]
Location Privacy in Mobile Edge Clouds
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In this paper, we consider user location privacy in mobile edge clouds (MECs). MECs are small clouds deployed at the network edge to offer cloud services close to mobile users, and many solutions have been proposed to maximize service locality by migrating services to follow their users. Co-location of a user and his service, however, implies that a cyber eavesdropper observing service migrations between MECs can localize the user up to one MEC coverage area, which can be fairly small (e.g., a femtocell). We consider using chaff services to defend against such an eavesdropper, with focus on strategies to control the chaffs. Assuming the eavesdropper performs maximum likelihood (ML) detection, we consider both heuristic strategies that mimic the user's mobility and optimized strategies designed to minimize the detection or tracking accuracy. We show that a single chaff controlled by the optimal strategy can drive the eavesdropper's tracking accuracy to zero when the user's mobility is sufficiently random. The efficacy of our solutions is verified through extensive simulations.
[maximum likelihood detection, Cloud computing, user location privacy, cyber eavesdropper, Optimized production technology, Mobile communication, MEC, Electronic mail, service locality maximization, ML detection, heuristic strategies, Wireless communication, Privacy, mobile computing, optimisation, security of data, services migration, location privacy, Mobile edge cloud, data privacy, Trajectory, mobile edge clouds, cloud computing, chaff service]
Approximation Designs for Cooperative Relay Deployment in Wireless Networks
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In this paper, we aim to maximize users' satisfaction by deploying limited number of relays in a target region to form a wireless relay network, and define the Deployment of Cooperative Relay (DoCR) problem, which is proved to be NP-complete. We first propose an O(&#x03B4; log n) approximation algorithm that utilizes the algorithms for budget weighted Steiner tree problem with novel position weighting assignment. We further propose a heuristic method to solve the DoCR problem releasing potential location constraint. Our extensive experiments indicate that the algorithms we propose can significantly improve the total satisfaction of the network. Furthermore, we establish a testbed using USRP to showcase our designs in real scenarios. To the best of our knowledge, we are the first to propose approximation algorithm for relay placement problem to maximize user satisfaction, which has both theoretical and practical significance in the related area.
[Algorithm design and analysis, Steiner trees, Heuristic algorithms, DoC) problem, position weighting assignment, Wireless communication, deployment of cooperative relay problem, user satisfaction maximization, approximation algorithm, O(&#x03B4; log n) approximation algorithm, relay placement problem, approximation design, relay networks (telecommunication), budget weighted Steiner tree problem, approximation theory, trees (mathematics), wireless networks, NP-complete problem, cooperative communication, wireless relay network, Relay networks (telecommunications), cooperative relay deployment, Approximation algorithms, USRP, computational complexity]
Dispersing Social Content in Mobile Crowd through Opportunistic Contacts
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Crowdsourced content sharing has become a fast-growing activity in today's online social networks, where contents of interest are created by diverse source users and conveyed over the network as friends view and reshare. The rapid and boundless propagation in a mobile crowd however often creates severe bottlenecks on the server side and incurs significant energy and monetary costs on the mobile side, particularly with the still expensive 3G/4G cellular connections. This paper presents SoCrowd, a novel framework for large-scale content sharing in a mobile crowd by exploiting contacts, i.e., users happen to move close with such short range low power communications as WiFi and bluetooth being enabled. We formulate the scheduling problem for social content propagation in a mobile crowd with contacts, and present optimal dynamic programming solution, which further motivates a series of practical heuristics. The effectiveness of SoCrowd has been demonstrated by extensive simulations driven by realworld traces and datasets.
[Schedules, crowdsourcing, Social network services, SoCrowd, dynamic programming, Mobile communication, Electronic mail, Servers, content management, social content propagation, mobile computing, opportunistic contacts, large-scale content sharing, scheduling, social networking (online), mobile crowd, Wireless fidelity, Mobile computing]
A Lightweight Recommendation Framework for Mobile User&#x2019;s Link Selection in Dense Network
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
With the proliferation of mobile devices and the development of communication technology, mobile devices have permeated every aspect of our daily lives. However, in dense network where large crowd of mobile devices try to access to the network simultaneously, the severe interference between mobile devices may incur a remarkable deterioration of the wireless communication quality. How to improve individual's experience in such scenario is a critical yet open problem. Inspired by the mobile device users' usage pattern as well as the characteristic of most wireless communication systems, we propose a framework offering uplink/downlink selection recommendation to different mobile device users to enhance their utility in this paper. The design of the framework starts with formulating the problem as a link selection game. Analysis shows that the game can be categorized as a generalized ordinal potential game whose Nash Equilibrium is guaranteed. We then devise a distributed link selection algorithm to generate a Nash Equilibrium of the game. To accommodate to the characteristic of dense network and the capacity limitation of mobile device, the design of the algorithm shows a light-weight property and does not require each mobile device user to know others' current selection. The probability of incomplete information gathering is also considered. Extensive experiments are conducted to demonstrate the effectiveness and superiority of the proposed framework. Experimental results show that the global average utility increase rate reaches above 20%, and about 70% mobile device users can benefit from using our framework.
[Algorithm design and analysis, generalized ordinal potential game, lightweight recommendation framework, downlink selection recommendation, Nash equilibrium, Downlink, Mobile handsets, mobile user link selection, Joint Fictitious, Wireless communication, Game Theory, distributed link selection algorithm, mobile computing, uplink selection recommendation, Uplink, Generalized Ordinal Potential Game, dense network, game theory, Mobile Computing, recommender systems, mobile device users, link selection game, Games, wireless communication systems, Wireless Link Selection]
Making Smartphone Smart on Demand for Longer Battery Life
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
A major concern for today's smartphones is their much faster battery drain than traditional feature phones, despite their greater battery capacities. The difference is mainly contributed by those more powerful but also much more power-consuming smartphone components, such as the multi-core application processor. While the application processor must be active when any smart apps are being used, it is also unnecessarily waken up, even during idle periods, to perform operations related to basic phone functions (i.e., incoming calls and text messages). In this paper, we investigate how to increase the battery life of smartphones by minimizing the use of the application processor during idle periods. We find that the application processor is often waken up by a process running on it, called the Radio Interface Layer Daemon (RILD), which interfaces the user and apps to the GSM/LTE cellular network. In particular, we demonstrate that a great amount of energy could be saved if RILD is stopped, such that the application processor can sleep more often. Based on this key finding, we design a Smart On Demand (SOD) configuration that reduces smartphone idle energy consumption by running RILD operations on a secondary low-power microcontroller. As a result, RILD operations can be handled at much lower energy costs and the application processor is waken up only when one needs to use any smart apps, in an on-demand manner. We have built a hardware prototype of SOD. Our results show that SOD can reduce the energy consumption by up to 42%.
[Energy consumption, smartphone idle energy consumption reduction, basic phone functions, Battery charge measurement, Batteries, multicore application processor, incoming calls, power consumption, idle period, smartphone smart on demand configuration, power aware computing, battery capacity, GSM-LTE cellular network, smartphone components, Cellular interface, secondary cells, secondary low-power microcontroller, smartphone battery life, application processor use minimization, Long Term Evolution, microcontrollers, RILD operations handling, Baseband, multiprocessing systems, Microcontrollers, smart phones, text messages, battery drain, energy saving, Cellular networks, smart apps, SOD configuration, energy conservation, Radio Interface Layer Daemon, energy cost, RILD., Smartphone, Smart phones, cellular radio]
FADEWICH: Fast Deauthentication Over the Wireless Channel
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Both authentication and deauthentication are instrumental for preventing unauthorized access to computers and other resources. While there are obvious motivating factors for using strong authentication mechanisms, convincing users to deauthenticate is not straight-forward, since deauthentication is not considered mandatory. A user who leaves a logged-in workstation unattended (especially for a short time) is typically not inconvenienced in any way; in fact, the other way around - no annoying reauthentication is needed upon return. However, an unattended workstation is trivially susceptible to the well-known "lunchtime attack" by any nearby adversary who simply takes over the departed user's log-in session. At the same time, since deauthentication does not intrinsically require user secrets, it can, in principle, be made unobtrusive. To this end, this paper designs the first automatic user deauthentication system - FADEWICH - that does not rely on biometric-or behavior-based techniques (e.g., keystroke dynamics) and does not require users to carry any devices. It uses physical properties of wireless signals and the effect of human bodies on their propagation. To assess FADEWICH's feasibility and performance, extensive experiments were conducted with its prototype. Results show that it suffices to have nine inexpensive wireless sensors deployed in a shared office setting to correctly deauthenticate all users within six seconds (90% within four seconds) after they leave their workstation's vicinity. We considered two realistic scenarios where the adversary attempts to subvert FADEWICH and showed that lunchtime attacks fail.
[telecommunication security, shared office setting, physical properties, wireless signals, wireless sensor networks, lunchtime attack, deauthentication, Communication system security, Wireless communication, wireless channel, Workstations, Sensors, wireless channels, unattended workstation, authentication, human bodies, fast deauthentication, fadewich, authentication mechanism, first automatic user deauthentication system, wireless sensors, Wireless sensor networks, Authentication, FADEWICH, wireless]
Cognitive Wireless Charger: Sensing-Based Real-Time Frequency Control For Near-Field Wireless Charging
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
A recent increase in mobile and IoT devices has led to the advancement of wireless charging. The state-of-the-art wireless charging systems operate at a particular frequency, controlled by the explicit networking from the power-receiving device (which relays the battery status information, useful for the frequency selection), but such control is not designed to cope with the variations in the power receiving device's placements and alignments (which are more significant in near-field and pseudo-tightly coupled charging applications, as more charging pads are being deployed in the public domains and serving heterogeneous clients). In this work, we analyze the impact of the power transfer performance caused by the power receiver's load, distance, and coil alignment/overlap and introduce cognitive wireless charger (CWC), which adaptively controls the operating frequency in real-time using implicit feedback from sensing for optimal operations. In addition to the theoretical and LTSpice-based simulation analysis, we build a prototype compatible to the Qi standard and analyze the performance of CWC with it. Through our analyses, we establish that frequency control achieves performance gains in inductive-coupling charging applications and is sensitive to the variations in the placement and alignment between the power-transmitting and the power-receiving coils. Our prototype, when CWC is turned off, has comparable performance to the commercial-grade Qi wireless chargers and, with CWC enabled, demonstrates significant improvement over modern wireless chargers.
[power- receiving coils, Sensing, battery chargers, Adaptive control, Frequency control, CWC, pseudo-tightly coupled charging applications, power transfer performance, Wireless communication, power-receiving device placements, inductive-coupling charging applications, Wireless charging, Transmitters, power receiver load, inductive power transmission, Qi wireless chargers, power-receiving device alignments, implicit feedback, power-transmitting coils, sensing-based real-time frequency control, coils, Qi standard, cognitive wireless charger, Inductive charging, near-field wireless charging system, Receivers, frequency control, IoT devices, power-receiving device, Wireless sensor networks, LTSpice-based simulation analysis, mobile devices, Impedance, battery status information, real-time frequency control]
Density and Mobility-Driven Evaluation of Broadcast Algorithms for MANETs
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Broadcast is a fundamental operation in Mobile Ad-Hoc Networks (MANETs). A large variety of broadcast algorithms have been proposed. They differ in the way message forwarding between nodes is controlled, and in the level of information about the topology that this control requires. Deployment scenarios for MANETs vary widely, in particular in terms of nodes density and mobility. The choice of an algorithm depends on its expected coverage and energy cost, which are both impacted by the deployment context. In this work, we are interested in the comprehensive comparison of the costs and effectiveness of broadcast algorithms for MANETs depending on target environmental conditions. We describe the results of an experimental study of five algorithms, representative of the main design alternatives. Our study reveals that the best algorithm for a given situation, such as a high density and a stable network, is not necessarily the most appropriate for a different situation such as a sparse and mobile network. We identify the algorithms characteristics that are correlated with these differences and discuss the pros and cons of each design.
[Protocols, broadcast algorithms, Probabilistic logic, Ad hoc networks, nodes density, nodes mobility, Topology, message forwarding, broadcast communication, Relays, sparse network, MANET, mobility-driven evaluation, mobile ad-hoc networks, target environmental conditions, mobile ad hoc networks, Peer-to-peer computing, Mobile computing, mobile network]
Energy-Aware CPU Frequency Scaling for Mobile Video Streaming
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The energy consumed by video streaming includes the energy consumed for data transmission and CPU processing, which are both affected by the CPU frequency. High CPU frequency can reduce the data transmission time but it consumes more CPU energy. Low CPU frequency reduces the CPU energy but increases the data transmission time and then increases the energy consumption. In this paper, we aim to reduce the total energy of mobile video streaming by adaptively adjusting the CPU frequency. Based on real measurement results, we model the effects of CPU frequency on TCP throughput and system power. Based on these models, we propose an Energy-aware CPU Frequency Scaling (EFS) algorithm which selects the CPU frequency that can achieve a balance between saving the data transmission energy and CPU energy. Since the downloading schedule of existing video streaming apps is not optimized in terms of energy, we also propose a method to determine when and how much data to download. Through trace-driven simulations and real measurement, we demonstrate that the EFS algorithm can reduce 30% of energy for the Youtube app, and the combination of our download method and EFS algorithm can save 50% of energy than the default Youtube app.
[Conferences, Mobile communication, EFS algorithm, CPU frequency effect, Video Streaming, mobile computing, power aware computing, Energy, CPU Frequency, Distributed databases, data transmission energy, video streaming, energy-aware CPU frequency scaling, energy consumption, Energy measurement, data transmission time reduction, microprocessor chips, system power, YouTube, trace-driven simulations, transport protocols, mobile video streaming, TCP throughput, Streaming media, Youtube app, data communication]
Crazy Crowd Sourcing to Mitigate Resource Scarcity
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Resource scarcity prohibits developing country population in many ways from ubiquitous services. One such service is providing information about the best route for an ambulance in a crisis situation due to lack of proper road network information and GPS data. We have worked on a routing method in Dhaka, Bangladesh and utilized the power of crowd sourcing in times of resource scarcity. We share our challenges and opportunities that opened up followed by the challenges in this paper.
[road network information, Crowd Sourcing, crowdsourcing, Roads, Urban areas, traffic route detection, routing method, Mobile handsets, traffic engineering computing, resource scarcity mitigate, ubiquitous computing, crazy crowd sourcing, ubiquitous services, Global Positioning System, Vehicles, developing country challenges, Data collection, crisis situation, Map matching, Hardware, GPS data]
Detecting Rogue AP with the Crowd Wisdom
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
WiFi networks are vulnerable to rogue AP attacks in which an attacker sets up an imposter AP to lure mobile users to connect. The attacker can eavesdrop on the communication, severely threatening users' privacy. Existing rogue AP detection solutions are confined to some specific attack scenarios (e.g., by relaying the traffic to a target AP) or require additional hardware. In this paper, we propose a crowdsensing based approach, named CRAD, to detect rogue APs in camouflage without specialized hardware requirement. CRAD exploits the spatial correlation of RSS to identify a potential imposter, which should be at a different location from the legitimate one. The RSS measurements collected from the crowd facilitate a robust profile and minimize the inaccuracy effect of a single RSS value. As a result, CRAD can filter out abnormal samples sensed in the realtime by dynamically matching the profile. We evaluate our approach with both a public dataset and a real prototype. The results show that CRAD can yield 90% detection accuracy and precision with proper crowd presence, even when the rogue AP is launched close to the legitimate one (e.g., within 1m).
[correlation theory, Rogue AP, rogue AP detection, Mobile communication, crowd wisdom, spatial correlation, Wireless Networks, Communication system security, CRAD, computer network security, rogue AP attacks, Wireless communication, Wireless sensor networks, RSS, WiFi networks, Mobile Crowdsensing, crowd presence, eavesdrop, Hardware, Sensors, wireless LAN, Monitoring]
Towards Multilingual Automated Classification Systems
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In this paper we propose and evaluate three approaches for automated classification of texts in over 60 languages without the need for a manually annotated dataset in those languages. All approaches are based on the randomized Explicit Semantic Analysis method using multilingual Wikipedia articles as their knowledge repository. We evaluate the proposed approaches by classifying a Twitter dataset in English and Portuguese into relevant and irrelevant items with respect to landslide as a natural disaster, where the highest achieved F1-score is 0.93. These approaches can be used in various applications where multilingual classification is needed, including multilingual disaster reporting using Social Media to improve coverage and increase confidence. As illustration, we present a demonstration that combines data from physical sensors and social networks to detect landslide events reported in English and Portuguese.
[Electronic publishing, text analysis, Social network services, natural language processing, multilingual automated classification systems, social networks, Encyclopedias, Wikipedia, Terrain factors, text classification, classification, Training, English, Portuguese, knowledge repository, explicit semantic analysis, Twitter dataset, social networking (online), Internet, social media]
The Joint Effects of Tweet Content Similarity and Tweet Interactions for Topic Derivation
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Interactions among tweets, i.e., mentions, retweets, replies, are important factors contributing to the quality of topic derivation on Twitter. If applied correctly, the incorporation of tweet interactions can significantly improve the quality of topic derivation in comparison with approaches that are mainly based on the content similarity analysis. However, how interactions can be measured and integrated with content similarity for topic derivation remains a challenge. In previous work, the strength of tweet-to-tweet relationship has been computed by simply adding measures for content similarity, mentions, and reply-retweets. This simple linear addition does not accurately reflect the various impacts these factors have on tweet relationships. In order to address this issue, we propose a joint probability model that can effectively integrate the effects of the content similarity, mentions, and reply-retweets to measure the tweet relationship for the purpose of topic derivation. The proposed method is based on matrix factorization techniques, which enables a flexible implementation on a distributed system in an incremental manner. Experimental results show that the proposed model results in a significant improvement in the quality of topic derivation over existing methods.
[Solid modeling, Joint Matrix Factorization, probability, Topic Derivation, Probability, Twitter, Electronic mail, matrix decomposition, tweet interactions, topic derivation, Analytical models, tweet-to-tweet relationship, Interactions of Tweets, matrix factorization techniques, Tagging, social networking (online), tweet content similarity, Australia, joint probability model]
Timed-Release of Self-Emerging Data Using Distributed Hash Tables
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Releasing private data to the future is a challenging problem. Making private data accessible at a future point in time requires mechanisms to keep data secure and undiscovered so that protected data is not available prior to the legitimate release time and the data appears automatically at the expected release time. In this paper, we develop new mechanisms to support self-emerging data storage that securely hide keys of encrypted data in a Distributed Hash Table (DHT) network that makes the encryption keys automatically appear at the predetermined release time so that the protected encrypted private data can be decrypted at the release time. We show that a straight-forward approach of privately storing keys in a DHT is prone to a number of attacks that could either make the hidden data appear before the prescribed release time (release-ahead attack) or destroy the hidden data altogether (drop attack). We develop a suite of self-emerging key routing mechanisms for securely storing and routing encryption keys in the DHT. We show that the proposed scheme is resilient to both release-ahead attack and drop attack as well as to attacks that arise due to traditional churn issues in DHT networks. Our experimental evaluation demonstrates the performance of the proposed schemes in terms of attack resilience and churn resilience.
[self-emerging data storage, distributed system, Encryption, encryption keys, encrypted private data protection, attack resilience, storage management, DHT, public key cryptography, distributed hash tables, Distributed databases, private data timed release, data protection, self-emerging key routing mechanisms, DHT networks, Receivers, Routing, Resilience, self-emerging data timed-release, distributed hash table, private key cryptography, release-ahead attack, drop attack, Data models, churn resilience, decryption]
Caching for Pattern Matching Queries in Time Evolving Graphs: Challenges and Approaches
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Pattern matching is an important class of problems related to graphs. It is a fundamental problem for many applications and has been extensively studied in literature. With the advent of huge graphs, the challenges in this domain have increased manifold. Consequently a lot of recent research has led to new architectures and approaches for optimized solutions to the pattern matching problem. A vast majority of these graphs hardly remain static and constantly evolve over time (like social networks, web graphs, etc). Recently, caching has been studied in the context of static graphs to optimize the throughput of query processing systems. In this paper, we list the challenges in caching in the context of Time Evolving Graphs (TEGs). Amongst others, one major challenge is consistency which entails to making sure the cache is consistent with the streaming changes. We propose an approach to successfully implement caching that addresses those issues and based on the initial results, we see significant gains in the overall performance of system.
[pattern matching, subgraph pattern matching, pattern matching queries, Social network services, Computational modeling, graph theory, simulation, Throughput, cache storage, caching, subgraph isomorphism, Computer science, query processing, dynamic graphs, optimisation, time evolving graphs, query processing systems, Query processing, TEG, static graphs, throughput optimization, Data models, Pattern matching]
GraphA: Adaptive Partitioning for Natural Graphs
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Large-scale graph computation is central to applications ranging from language processing to social networks. However, natural graphs tend to have skewed power-law distributions where a small subset of the vertices have a large number of neighbors. Existing graph-parallel systems suffer from load imbalance, high communication cost, or suboptimal and complex processing. In this paper we present GraphA, an Adaptive approach to efficient partitioning and computation of large-scale natural graphs. GraphA provides an adaptive and uniform graph partitioning algorithm, which partitions the datasets in a load-balanced manner by using an incremental number of hash functions. We have implemented GraphA both on Spark and on GraphLab. Extensive evaluation shows that GraphA remarkably outperforms state-of-the-art graph-parallel systems (GraphX and PowerLyra) in ingress time, execution time and storage overhead, for both real-worldand synthetic graphs.
[Algorithm design and analysis, Graph computation, Social network services, graph theory, GraphLab, graph-parallel systems, natural graphs, Partitioning algorithms, Complexity theory, large-scale graph computation, Sparks, parallel processing, GraphA, Adaptive Partitioning, Loading, adaptive uniform graph partitioning algorithm, Spark, Mirrors]
Parallel Algorithm for Core Maintenance in Dynamic Graphs
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
This paper initiates the studies of parallel algorithm for core maintenance in dynamic graphs. The core number is a fundamental index reflecting the cohesiveness of a graph, which is widely used in large-scale graph analytics. We investigate the parallelism in the core update process when multiple edges and vertices are inserted. Specifically, we discover a structure called superior edge set, the insertion of edges in which can be processed in parallel. Based on the structure of superior edge set, an efficient parallel algorithm is then devised. To the best of our knowledge, the proposed algorithm is the first parallel one for the fundamental core maintenance problem. Finally, extensive experiments are conducted on different types of real-world and synthetic datasets, and the results illustrate the efficiency, stability and scalability of the proposed algorithm. The algorithm shows a significant speedup in the processing time compared with previous results that sequentially handle edge and vertex insertions.
[Algorithm design and analysis, core update process, parallel algorithms, Multicore processing, Heuristic algorithms, graph theory, parallel algorithm, Maintenance engineering, Indexes, superior edge set, dynamic graph, Parallel algorithms, dynamic graphs, core maintenance]
DHCRF: A Distributed Conditional Random Field Algorithm on a Heterogeneous CPU-GPU Cluster for Big Data
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
As one of the most recognized models in machine learning, the conditional random fields (CRF) has been widely used in many applications. As the parameter estimation of CRF is highly time-consuming, how to improve the performance of CRF has received significant attention, in particular in the big data environment. To deal with large-scale data, CPU-based or GPU-based parallelization solutions have been proposed to improve performance. However, the problem is an ongoing one. In this paper, we focus on the big data environment and propose a distributed CRF on a heterogeneous CPU-GPU cluster called DHCRF. Our approach differs from previous work. Specifically, it leverages a three-stage heterogeneous Map and Reduce operation to improve the performance, making full use of CPU-GPU collaborative computing capabilities in a big data environment. Furthermore, by combining elastic data partition and intermediate results multiplexing method, the distributed CRF is optimized. Elastic data partition is performed to keep the load balanced, and the intermediate results multiplexing method is adopted to reduce data communication. Experimental results show that the DHCRF outperforms the baseline CRF algorithm and the CPU-based parallel CRF algorithm with notable performance improvement while maintaining competitive correctness at the same time.
[Algorithm design and analysis, Parameter estimation, conditional random fields, Graphics processing units, elastic data partition, distributed conditional random field algorithm, distributed CRF performance, resource allocation, Clustering algorithms, parameter estimation, CPU-GPU collaborative computing capabilities, intermediate results multiplexing method, Computational modeling, heterogeneous CPU-GPU cluster, Conditional Random Fields, random processes, Big Data, microprocessor chips, Heterogeneous CPU-GPU Cluster, Partitioning algorithms, DHCRF, graphics processing units, CPU-based parallelization, GPU-based parallelization, distributed algorithms, large-scale data, Distributed, data communication]
Towards New Abstractions for Implementing Quorum-Based Systems
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
This paper introduces Gorums, a novel RPC framework for building fault tolerant distributed systems. Gorums offers a flexible and simple quorum call abstraction, used to communicate with a set of processes, and to collect and process their responses. Gorums provides separate abstractions for (a) selecting processes for a quorum call and (b) processing replies. These abstractions simplify the main control flow of protocol implementations, especially for quorum-based systems, where only a subset of the replies to a quorum call need to be processed. To show that Gorums can be used in practical systems, we implemented EPaxos' latency-efficient quorum system, and ran experiments using a key-value storage. Our results show that Gorums' abstractions can provide additional performance benefits to EPaxos.
[quorum-based systems, Protocols, Buildings, Gorums, key-value storage, Servers, Fault tolerance, RPC framework, Contacts, control flow, Fault tolerant systems, EPaxos, latency-efficient quorum system, fault tolerant distributed systems, remote procedure calls, Libraries, fault tolerant computing, quorum call abstraction]
Selective Traffic Offloading on the Fly: A Machine Learning Approach
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
It has been well recognized that network transmission constitutes a large portion of smartphone energy consumption, mainly because of the tail energy caused by cellular network interface. Traffic offloading has been proposed to reduce energy by letting a smartphone offload network traffic to its neighbors in vicinity via low-power direct connections (e.g., WiFi Direct or Bluetooth). Our experiments conducted in a realistic environment reveal that energy efficiency cannot be improved or even deteriorates without a carefully designed offloading strategy. In this paper, we propose a selective traffic offloading scheme implemented as a smartphone middleware in a software-defined fashion, which consists of a packet classifier and a traffic scheduler. Using a light-weight machine learning approach exploiting unique smartphone context information, the packet classifier identifies packets generated on the fly as offloadable or not with substantially improved efficiency and feasibility on resource limited smartphones compared to traditional approaches. Both testbed and simulation based experiments are conducted and the results show that our proposal always attains the superior performance on a number of comparison metrics.
[Energy consumption, Bluetooth, network transmission, WiFi direct, Maching learning, selective traffic offloading scheme, low-power direct connections, smart phones, Network interfaces, Energy saving, Middleware, smartphone energy consumption, Cellular networks, traffic scheduler, smartphone offload network traffic, smartphone context information, software-defined fashion, Offloading, Data communication, learning (artificial intelligence), light-weight machine learning approach, Wireless fidelity, telecommunication traffic, middleware]
A Fast Heuristic Attribute Reduction Algorithm Using Spark
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Energy data, which consists of energy consumption statistics and other related data in green data centers, grows dramatically. The energy data has great value, but many attributes within it are redundant and unnecessary. Thus attribute reduction for the energy data has been conceived as a critical step. However, many existing attribute reduction algorithms are often computationally time-consuming. To address these issues, we extend the methodology of rough sets to construct data center energy consumption knowledge representation system. By taking good advantage of in-memory computing, an attribute reduction algorithm for energy data using Spark is proposed. In this algorithm, we use a heuristic formula for measuring the significance of attribute to reduce search space, and an efficient algorithm for simplifying energy consumption decision table, which further improve the computation efficiency. The experimental results show the speed of our algorithm gains up to 0.28X performance improvement over the traditional attribute reduction algorithm using Spark.
[Algorithm design and analysis, workstation clusters, Energy consumption, energy consumption statistics, Heuristic algorithms, public domain software, Heuristics, Attribute reduction, energy consumption decision table, attribute reduction algorithm, power aware computing, Rough sets, Clustering algorithms, cloud computing, in-memory computing, energy data, data center energy consumption, Energy data, Software algorithms, knowledge representation system, green data centers, Sparks, computer centres, decision tables, knowledge representation, green computing, Spark, statistics]
Profiling Users by Modeling Web Transactions
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Users of electronic devices, e.g., laptop, smartphone, etc. have characteristic behaviors while surfing the Web. Profiling this behavior can help identify the person using a given device. In this paper, we introduce a technique to profile users based on their web transactions. We compute several features extracted from a sequence of web transactions and use them with one-class classification techniques to profile a user. We assess the efficacy and speed of our method at differentiating 25 synthetic users on a benchmark dataset (from a major security vendor) representing 6 months of web traffic monitoring from a small enterprise network.
[pattern classification, smartphone, Media, one-class classification techniques, characteristic behaviors, Web transaction modeling, laptop, electronic devices, Uniform resource locators, Support vector machines, Web traffic monitoring, feature extraction, benchmark dataset, user profiling, small enterprise network, Feature extraction, Internet, behavioural sciences computing, Kernel, Monitoring]
JeCache: Just-Enough Data Caching with Just-in-Time Prefetching for Big Data Applications
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Big data clusters introduce an intermediate cache layer between the computing frameworks and the underlying distributed file systems, to enable upper-level applications or end users to efficiently access big datasets in cache and effectively share them among different computing frameworks. As caches are shared by multiple applications or end users, directly applying existing on-demand caching strategies will result in intense conflicts, when big datasets are cached as a whole. Meanwhile, big data applications usually involve massive numbers of file scans, cached-in data blocks may have little chance of being accessed before they are cached out to make way for other on-demand data blocks. Thus, it is unwise to cache data blocks long before they are actually accessed. In this paper, we propose a novel just-enough big data caching scheme for just-in-time block prefetching to improve the cache effectiveness of big data clusters. With just-in-time block prefetching, a block is cached in just before the task begins to process the block, rather than being cached in along with other blocks of the same dataset being processed. We monitor block accesses to measure the average processing time of data blocks, and then estimate the minimal number of blocks that should be kept in cache for a big dataset, so that the speed of data processing matches with that of data prefetching, and each upper-level task can obtain its input blocks from cache just in time. Our experimental results show that the proposed cache method can restrain over-requirement of cache resources in big data applications, and provides the same performance improvement as when all data blocks are cached.
[intermediate cache layer, big datasets, distributed file systems, Big Data applications, just-enough big data caching scheme, cache storage, big data clusters, storage management, on-demand caching strategies, Computer architecture, distributed databases, Data Caching, just-in-time prefetching, cached-in data blocks, Monitoring, computing frameworks, Data Prefetching, JeCache, Prefetching, Big Data, Sparks, on-demand data blocks, big data applications, end users, upper-level applications]
Proximity Awareness Approach to Enhance Propagation Delay on the Bitcoin Peer-to-Peer Network
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In the Bitcoin system, a peer-to-peer electronic currency system, the delay overhead in transaction verification prevents the Bitcoin from gaining increasing popularity nowadays as it makes the system vulnerable to double spend attacks. This paper introduces a proximity-aware extension to the current Bitcoin protocol, named Bitcoin Clustering Based Ping Time protocol (BCBPT). The ultimate purpose of the proposed protocol, that is based on how the clusters are formulated and the nodes define their membership, is to improve the transaction propagation delay in the Bitcoin network. In BCBPT, the proximity of connectivity in the Bitcoin network is increased by grouping Bitcoin nodes based on ping latencies between nodes. We show, through simulations, that the proximity base ping latency defines better clustering structures that optimize the performance of the transaction propagation delay. The reduction of the communication link cost measured by the information propagation time between nodes is mainly considered as a key reason for this improvement. Bitcoin Clustering Based Ping Time protocol is more effective at reducing the transaction propagation delay compared to the existing clustering protocol (LBC) that we proposed in our previous work.
[Bitcoin clustering based ping time protocol, Protocols, transaction verification, peer-to-peer computing, BCBPT, Bitcoin, peer-to-peer electronic currency system, History, Bitcoin peer-to-peer network, Clustering Evaluation, Peer-to-peer computing, Delays, financial data processing, protocols, proximity awareness approach, Propagation delay]
Catch Me If You Can: Detecting Compromised Users Through Partial Observation on Networks
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
People are suffering from a range of risks in the ubiquitous networks of current world, such as rumours spreading in social networks, computer viruses propagating throughout the Internet and unexpected failures happened in Smart grids. We usually monitor only a few users of detecting various risks due to the resource constraints and privacy protection. This leads to a critical problem to detect compromised users who are out of surveillance. In this paper, we propose a risk assessment method to address this problem. The aim is to assess the security status of unmonitored users according to the limited information collected from monitored users in networks. There are two innovative techniques developed: First, we identify the source of risk propagation by inversely disseminating risks from the influenced (by rumours) or infected (by viruses) monitored users. We show a new finding that the ones who synchronously receive the risk copies from all monitored users are most likely to be the sources. Second, we propose a microscopic mathematical model to present the risk propagation from the exposed sources. This model forms a discriminant to classify the compromised users from others. For evaluations, we collect three real networks on which we launch simulated risk propagation and then sample the status of monitored users. The experiment results show that our method is effective and the result of risk assessment well matches the real status of the unmonitored users.
[risk management, risk assessment, Computational modeling, resource constraints, ubiquitous networks, grid computing, compromised users detection, Security, ubiquitous computing, modelling, microscopic mathematical model, partial observation, smart grids, Source identification, privacy protection, risk propagation, data privacy, Internet, Mathematical model, Risk management, Facebook, Monitoring]
Location Privacy Breach: Apps Are Watching You in Background
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Smartphone users can conveniently install a set of apps that provide Location Based Service (LBS) from markets. These LBS-based apps facilitate users in many application scenarios, but they raise concerns on the breach of privacy related to location access. Smartphone users can hardly perceive location access, especially when it happens in background. In comparison to location access in foreground, location access in background could result in more serious privacy breach because it can continuously know a user's locations. In this paper, we study the problem of location access in background, and especially perform the first measurement of this background action on the Google app market. Our investigation demonstrates that many popular apps conduct location access in background within short intervals. This enables these apps to collect a user's location trace, from which the important personal information, Points of Interest (PoIs), can be recognized. We further extract a user's movement pattern from the PoIs, and utilize it to measure the extent of privacy breach. The measurement results also show that using the combination of movement pattern related metrics and the other PoI related metrics can help detect the privacy breach in an earlier manner than using either one of them alone.
[Measurement, Google, personal information, Humanoid robots, location based service, user movement pattern extraction, Google app market, Mobile communication, points of interest, smart phones, Electronic mail, Privacy, mobile computing, location privacy breach, background action, serious privacy breach, smartphone user location, data privacy, Androids, LBS, background location access, user location trace collection]
Android Malware Detection Using Complex-Flows
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
This paper proposes a new technique to detect mobile malware based on information flow analysis. Our approach examines the structure of information flows to identify patterns of behavior present in them and which flows are related, those that share partial computation paths. We call such flows Complex-Flows, as their structure, patterns, and relations accurately capture the complex behavior exhibited by both recent malware and benign applications. N-gram analysis is used to identify unique and common behavioral patterns present in Complex-Flows. The N-gram analysis is performed on sequences of API calls that occur along Complex-Flows' control flow paths. We show the precision of our technique by applying it to different data sets totaling 7,798 apps. These data sets consist of both recent and older generation benign and malicious apps to demonstrate the effectiveness of our approach across different generations of apps.
[partial computation path, Google, invasive software, application program interfaces, information flow structure, Android malware detection, Humanoid robots, Mobile communication, API calls sequence, mobile computing, N-gram analysis, mobile malware detection, Malware, Sensors, Internet, Androids]
Privacy Implications of DNSSEC Look-Aside Validation
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
To complement DNSSEC operations, DNSSEC Look-aside Validation (DLV) is designed for alternative off-path validation. While DNS privacy attracts a lot of attention, the privacy implications of DLV are not fully investigated and understood. In this paper, we take a first in-depth look into DLV, highlighting its lax specifications and privacy implications. By performing extensive experiments over datasets of domain names under comprehensive experimental settings, our findings firmly confirm the privacy leakages caused by DLV. We discover that a large number of domains that should not be sent to DLV servers are being leaked. We explore the root causes, including the lax specifications of DLV. We also propose two approaches to fix the privacy leakages. Our approaches require trivial modifications to the existing DNS standards and we demonstrate their cost in terms of latency and communication.
[government policies, alternative off-path validation, domain name system security extensions, Superluminescent diodes, privacy, Servers, Signal resolution, privacy implications, DLV, Privacy, security of data, Operating systems, Public key, look-aside validation, DNS standards, DNSSEC look-aside validation, Doman name system, data privacy, Internet, privacy leakages, lax specifications, legislation]
FlipNet: Modeling Covert and Persistent Attacks on Networked Resources
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Persistent and zero-day attacks have increased considerably in the recent past in terms of scale and impact. Security experts can no longer rely only on known defenses and thereby protect their resources permanently. It is increasingly common now to observe attackers being able to repeatedly break systems exploiting new vulnerabilities and defenders hardening systems with new measures. To model this phenomenon of the repeated takeover of the computing resources by system administrators and malicious attackers, a novel game framework, FlipIt, has been proposed by (Van Dijk et al. 2013) for a system consisting of a single resource. In this paper, we extend this and develop FlipNet, which is a repeated game framework for a networked system of multiple resources. This game involves two players-a defender and an attacker. Each player's objective is to maximize its gain (i.e., its control over the nodes in the network with stealthy moves), while minimizing the cost for making those moves. This leads to a novel and natural game formulation, with a very complex strategy space, that depends on the network structure. We show that finding the best response strategy for both the defender and attacker is NP-hard. In a key result in this study, we show that the attacker's gain for an instance of the game has a type of diminishing marginal return property, which leads to a near-optimal algorithm for maximizingthe attacker's gain. We examine the impact of network structure on the strategy space using simulations.
[strategy space, best response strategy, Companies, persistent attacks modeling, zero-day attacks, natural game formulation, Electronic mail, Encryption, networked resources, FlipIt, covert attacks modeling, Computer hacking, repeated game framework, near-optimal algorithm, network structure, Computational modeling, Security games, game theory, marginal return property, FlipNet, Zero-day attacks, security of data, Games, Advanced persistent threats, computational complexity]
Understanding the Market-Level and Network-Level Behaviors of the Android Malware Ecosystem
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The prevalence of malware in Android marketplaces is a growing and significant problem. Most existing studies focus on detecting Android malware or designing new security extensions to defend against specific types of attacks. In this paper, we perform an empirical study on analyzing the market-level and network-level behaviors of the Android malware ecosystem. We focus on studying whether there are interesting characteristics of those market accounts that distribute malware and specific networks that are mainly utilized by Android malware authors. We further investigate community patterns among Android malware from the perspective of their market account infrastructure and remote server infrastructure. Spurred by these analysis, we design a novel community inference algorithm to find more malicious apps by exploiting their community relationships. By using a small seed set (50) of known malicious apps, we can effectively find another extra 20 times of malicious apps, while maintaining considerable accuracy higher than 94%.
[invasive software, market-level behaviors, Android malware detection, Ecosystems, Humanoid robots, Android marketplaces, malicious apps, Servers, Android malware ecosystem, market account infrastructure, mobile computing, security, network-level behaviors, community inference algorithm, remote server infrastructure, Malware, Androids, IP networks, Smart phones]
EnGarde: Mutually-Trusted Inspection of SGX Enclaves
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Intel's SGX architecture allows cloud clients to create enclaves, whose contents are cryptographically protected by the hardware even from the cloud provider. While this feature protects the confidentiality and integrity of the client's enclave content, it also means that enclave content is completely opaque to the cloud provider. Thus, the cloud provider is unable to enforce policy compliance on enclaves. In this paper, we introduce EnGarde, a system that allows cloud providers to ensure SLA compliance on enclave content. In EnGarde, cloud providers and clients mutually agree upon a set of policies that the client's enclave content must satisfy. EnGarde executes when the client provisions the enclave, ensuring that only policy-compliant content is loaded into the enclave. EnGarde is able to achieve its goals without compromising the security guarantees offered by the SGX, and imposes no runtime overhead on the execution of enclave code. We have demonstrated the utility of EnGarde by using it to enforce a variety of security policies on enclave content.
[content protection, Cloud computing, Enclave Programs, cloud provider, cryptographic protection, EnGarde, content confidentiality, content integrity, Intel, Runtime, SGX security guarantees, data protection, mutually-trusted inspection, Hardware, Cryptography, cloud computing, cloud clients, SLA compliance, SGX architecture, Inspection, cryptography, data integrity, SGX Enclaves, Virtual machine monitors, Code Verification, Intel Software Guard Extensions (SGX), Control Flow Integrity]
Truthful Online Auction for Cloud Instance Subletting
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Despite that IaaS users are busy scaling up/out their cloud instances to meet the ever-increasing demands, the dynamics of their demands, as well as the coarse-grained billing options offered by leading cloud providers, have led to substantial instance underutilization in both temporal and spatial domains. This paper theoretically examines an instance subletting service, where underutilized instances are leased to others within user-specified periods. Serving as a secondary market that complements the existing instance market of IaaS providers,we specifically identify the theoretical challenges in instance subletting services, and design an online auction mechanism to make allocation and pricing decisions for the instances to be sublet. Our mechanism guarantees truthfulness and individual rationality with the best possible competitive ratio. Extensive trace-driven simulations show that our proposed mechanism achieves significant performance gains in both cost and social welfare.
[Cloud computing, instance underutilization, Performance gain, truthful online auction, Electronic mail, spatial domain, Cost accounting, secondary market, cloud instance subletting service, demand dynamics, competitive ratio, resource allocation, coarse-grained billing option, social welfare, Pricing, cloud computing, auction mechanism, IaaS providers, pricing decision, Indexes, performance gain, trace-driven simulation, online auction mechanism, temporal domain, invoicing, cloud providers, Resource management, allocation decision, pricing, instance subletting]
On the Power of Weaker Pairwise Interaction: Fault-Tolerant Simulation of Population Protocols
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In this paper we investigate the computational power of population protocols under some unreliable or weaker interaction models. More precisely, we focus on two features related to the power of interactions: omission failures and one-way communications. We start our investigation by providing a complete classification of all the possible models arising from the aforementioned weaknesses, and establishing the computational hierarchy of these models. We then address for each model the fundamental question of what additional power is necessary and sufficient to completely overcome the model's weakness and make it able to simulate faultless two-way protocols. We answer this question by presenting simulators that work under certain assumptions and by proving that simulation is impossible without such assumptions.
[Protocols, Computational modeling, population protocols, omission failures, one-way communications, fault-tolerant simulation, Statistics, Inductors, Standards, Sociology, fault tolerant computing, Mathematical model, protocols, weaker pairwise interaction]
Distributed Fault Tolerant Linear System Solvers Based on Erasure Coding
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
We present efficient coding schemes and distributed implementations of erasure coded linear system solvers. Erasure coded computations belong to the class of algorithmic fault tolerance schemes. They are based on augmenting an input dataset, executing the algorithm on the augmented dataset, and in the event of a fault, recovering the solution from the corresponding augmented solution. This process can be viewed as the computational analog of erasure coded storage schemes. The proposed technique has a number of important benefits: (i) as the hardware platform scales in size and number of faults, our scheme yields increasing improvement in resource utilization, compared to traditional schemes; (ii) the proposed scheme is easy to code - the core algorithms remain the same; and (iii) the general scheme is flexible - accommodating a range of computation and communication tradeoffs. We present new coding schemes for augmenting the input matrix that satisfy the recovery equations of erasure coding with high probability in the event of random failures. These coding schemes also minimize fill (non-zero elements introduced by the coding block), while being amenable to efficient partitioning across processing nodes. We demonstrate experimentally that our scheme adds minimal overhead for fault tolerance, yields excellent parallel efficiency and scalability, and is robust to different fault arrival models.
[Linear systems, Linear System Solvers, distributed fault tolerant linear system solvers, erasure coding, fill minimization, Sparse matrices, parallel processing, random failures, Fault tolerance, storage management, Program processors, resource allocation, Kruskal rank, Fault tolerant systems, input matrix augmentation, resource utilization, Computational modeling, probability, erasure coded storage schemes, Encoding, software fault tolerance, matrix algebra, algorithmic fault tolerance schemes, parallel systems, Erasure Coding, minimisation, Fault Tolerance]
Preserving Incumbent Users&#x2019; Privacy in Exclusion-Zone-Based Spectrum Access Systems
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Dynamic spectrum access (DSA) technique has emerged as a fundamental approach to mitigate the spectrum scarcity problem. As a key form of DSA, the government is proposing to release more federal spectrum for sharing with commercial wireless users. However, the flourish of federal-commercial sharing hinges upon how the federal privacy is managed. In current DSA proposals, the sensitive exclusion zone (E-Zone) information of federal incumbent users (IUs) needs to be shared with a spectrum access system (SAS) to realize spectrum allocation. However, SAS is not necessarily trust-worthy for holding the sensitive IU E-Zone data, especially considering that FCC allows some industry third parties (e.g., Google) to operate SAS for better efficiency and scalability. Therefore, the current proposals dissatisfy the IUs' privacy requirement. To address the privacy issue, this paper presents an IU-privacy-preserving SAS (IP-SAS) design, which realizes the spectrum allocation process through secure computation over ciphertext based on homomorphic encryption so that none of the IU EZone information is exposed to SAS. This paper also proposes mechanisms to prevent malicious parties from compromising IP-SAS. We prove the privacy-preserving properties of IP-SAS and demonstrate the scalability and practicality of IP-SAS using experiments based on real-world data. Evaluation results show that IP-SAS can respond an SU's spectrum request in 1.25 seconds with communication overhead of 17.8 KB.
[telecommunication security, federal incumbent users, IU-privacy-preserving SAS design, frequency allocation, privacy, Encryption, Servers, spectrum scarcity problem, spectrum allocation process, Privacy, dynamic spectrum access technique, incumbet user privacy preservation, IU EZone information, E-zone information, ciphertext, Computational modeling, federal privacy, FCC, cryptography, IP-SAS design, Synthetic aperture sonar, homomorphic encryption, Dynamic spectrum access, exclusion zone, exclusion-zone-based spectrum access systems, data privacy, Resource management, federal-commercial sharing, DSA technique]
LITMUS: Towards Multilingual Reporting of Landslides
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
LITMUS is a real-time online and openly accessible service that collects high quality information on landslide events from social media. This service uses disaster related keywords, such as "landslide" and "mudslide\
[message analysis, geomorphology, Event detection, Social network services, English speaking user, geophysics computing, Terrain factors, translated approach, LITMUS service, Training, Computer science, native approach, landslides multilingual reporting, Information services, social networking (online), Real-time systems, social media]
Pythia: A System for Online Topic Discovery of Social Media Posts
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Social media constitute nowadays one of the most common communication mediums. Millions of users exploit them daily to share information with their community in the network via messages, referred as posts. The massive volume of information shared is extremely diverse and covers a vast spectrum of topics and interests. Automatically identifying the topics of the posts is of particular interest as this can assist in a variety of applications, such as event detection, trends discovery, expert finding etc. However, designing an automated system that requires no human agent participation to identify the topics covered in posts published in Online Social Networks (OSNs) presents manifold challenges. First, posts are unstructured and commonly short, limited to just a few characters. This prevents existing classification schemes to be directly applied in such cases, due to sparseness of the text. Second, new information emerges constantly, hence building a learning corpus from past posts may fail to capture the ever evolving information emerging in OSNs. To overcome the aforementioned limitations we have designed Pythia, an automated system for short text classification that exploits the Wikipedia structure and articles to identify the topics of the posts. The topic discovery is performed in two phases. In the first step, the system exploits Wikipedia categories and articles of the corresponding categories to build the training corpus for the suppervised learning. In the second step, the text of a given post is augmented using a text enrichment mechanism that extends the post with relevant Wikipedia articles. After the initial steps are performed, we deploy k-NN classifier to determine the topic(s) covered in the original post.
[Electronic publishing, text analysis, Art, information sharing, supervised learning, Encyclopedias, Wikipedia, learning corpus, short text classification, online topic discovery, Training, text enrichment mechanism, k-NN classifier, topic discovery, Pythia, automatic topic identification, learning (artificial intelligence), pattern classification, Social network services, Wikipedia structure, social media posts, social networks, OSN, communication mediums, training corpus, Wikipedia categories, social networking (online), Internet, short-text classification]
Data-Driven Serendipity Navigation in Urban Places
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
With the proliferation of mobile computing and the ability to collect detailed data for the urban environment a number of systems that aim at providing Points of Interest (POIs) and tour recommendations have appeared. The overwhelming majority of these systems aims at providing an optimal recommendation, where optimality refers to objectives of minimizing the distance to be covered or maximizing the quality of the POIs recommended. A major problem is that by focusing on the optimization of these objectives, there remains little room to the user for serendipity. Urban and social scientists have identified serendipity, i.e., the ability to come across unexpected places, as a feature that makes a city livable. In this work, we introduce a prototype of an experimental platform for evaluating venue recommendation algorithms by providing informative tour recommendations based on the suggested venues. Our prototype system integrates the notion of serendipity in urban navigation at both the venue as well as the route recommendation level without compromising the quality and diversity of the recommended POIs. In addition, our system allows the user to upload their own algorithms and explore their performance as compared to many well-known algorithms.
[Algorithm design and analysis, Navigation, Heuristic algorithms, Urban areas, Routing, points of interest, traffic engineering computing, POI, data-driven serendipity navigation, mobile computing, recommender systems, Semantics, Prototypes, venue recommendation algorithms, urban navigation, route recommendation level]
Toward an Integrated Approach to Localizing Failures in Community Water Networks (DEMO)
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
We present a cyber-physical-human (CPHS) distributed computing framework, AquaSCALE, for gathering, analyzing and localizing anomalous operations of increasingly failure-prone community water services. Today, detection of water pipe leaks takes hours to days. AquaSCALE leverages dynamic data from multiple information sources including IoT (Internet of Things) sensing data, geophysical data, human input and simulation/modeling engines to create a sensor-simulation-data integration platform that can locate multiple simultaneous pipe failures at fine level of granularity with high level of accuracy and detection time reduced by orders of magnitude (from hours/days to minutes).
[Computational modeling, Hydraulic systems, Predictive models, Data models, Water resources, Engines, Plugs]
PrivateGraph: A Cloud-Centric System for Spectral Analysis of Large Encrypted Graphs
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Graph datasets have invaluable use in business applications and scientific research. Because of the growing size and dynamically changing nature of graphs, graph data owners may want to use public cloud infrastructures to store, process, and perform graph analytics. However, when outsourcing data and computation, data owners are at burden to develop methods to preserve data privacy and data ownership from curious cloud providers. This demonstration exhibits a prototype system for privacy-preserving spectral analysis framework for large graphs in public clouds (PrivateGraph) that allows data owners to collect graph data from data contributors, and store and conduct secure graph spectral analysis in the cloud with preserved privacy and ownership. This demo system lets its audience interactively learn the major cloud-client interaction protocols: the privacy-preserving data submission, the secure Lanczos and Nystro&#x0308;m approximate eigen-decomposition algorithms that work over encrypted data, and the outcome of an important application of spectral analysis - spectral clustering. In the process of demonstration the audience will understand the intrinsic relationship amongst costs, result quality, privacy, and scalability of the framework.
[Algorithm design and analysis, result quality, Cloud computing, Data privacy, cryptography, spectral analysis, PrivateGraph, cloud-client interaction protocols, Spectral analysis, framework scalability, eigenvalues and eigenfunctions, privacy-preserving data submission, cloud-centric system, encrypted data, Clustering algorithms, Distributed databases, eigen-decomposition algorithms, data privacy, Cryptography, cloud computing, large encrypted graphs]
IoT Sentinel Demo: Automated Device-Type Identification for Security Enforcement in IoT
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The emergence of numerous new manufacturers producing devices for the Internet-of-Things (IoT) has given rise to new security concerns. Many IoT devices exhibit security flaws making them vulnerable for attacks and manufacturers have difficulties in providing appropriate security patches to their products in a timely and user-friendly manner. In this paper, we present our implementation of IoT Sentinel, which is a system aimed at protecting the user's network from vulnerable IoT devices. IoT Sentinel automatically identifies vulnerable devices when they are first introduced to the network and enforces appropriate traffic filtering rules to protect other devices from the threats originating from the vulnerable devices.
[security flaws, IoT sentinel demo, Fingerprint recognition, Security, Communication system security, threat mitigation, device fingerprinting, Wireless communication, mobile computing, Prototypes, device identification, IoT security, SDN-based network isolation, cloud-based IoT security service, cloud computing, traffic filtering rules, Internet-of-Things, vulnerable device identification, security patches, user network protection, Object recognition, Internet of Things, computer network security, Logic gates, automated device-type identification]
Rogue Access Point Detector Using Characteristics of Channel Overlapping in 802.11n
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In this work, we introduce a powerful hardware-based rogue access point (PrAP), which can relay traffic between a legitimate AP and a wireless station back and forth, and act as a man-in-the-middle attacker. Our PrAP is built of two dedicated wireless routers interconnected physically, and can relay traffic rapidly between a station and a legitimate AP. Through extensive experiments, we demonstrate that the state-of-the-art time-based rogue AP (rAP) detectors cannot detect our PrAP, although effective against software-based rAP. To defend against PrAPs, we propose PrAP-Hunter based on intentional channel interference. PrAP-Hunter is highly accurate, even under heavy traffic scenarios. Using a high-performance (desktop) and low-performance (mobile) experimental setups of our PrAP-Hunter in various deployment scenarios, we demonstrate close to 100% of detection rate, compared to 60% detection rate by the state-of-the-art. We show that PrAP-Hunter is fast (takes 5-10 sec), does not require any prior knowledge, and can be deployed in the wild by real world experiments at 10 coffee shops.
[telecommunication security, rogue access point detector, 802.11n, Wireless LAN, Rogue AP, channel interference, time-based rogue AP, Throughput, wireless station, Wireless communication, Degradation, powerful hardware-based rogue access point, Intrusion detection, Bandwidth, Detectors, channel overlapping, man-in-the-middle attacker, subscriber loops, software-based rAP, time 5 s to 10 s, heavy traffic scenarios, legitimate AP, Interference, wireless routers, telecommunication network routing, PrAP-Hunter, Delays, wireless LAN, IEEE 802.11n, telecommunication traffic]
ReverseCloak: A Reversible Multi-level Location Privacy Protection System
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
With the fast popularization of mobile devices and wireless networks, along with advances in sensing and positioning technology, we are witnessing a huge proliferation of Location-based Services (LBSs). Location anonymization refers to the process of perturbing the exact location of LBS users as a cloaking region such that a user's location becomes indistinguishable from the location of a set of other users. However, existing location anonymization techniques focus primarily on single level unidirectional anonymization, which fails to control the access to the cloaking data to let data requesters with different privileges get information with varying degrees of anonymity. In this demonstration, we present a toolkit for ReverseCloak, a location perturbation system to protect location privacy over road networks in a multi-level reversible manner, consisting of an `Anonymizer' GUI to adjust the anonymization settings and visualize the multilevel cloaking regions over road network for location data owners and a `De-anonymizer' GUI to de-anonymize the cloaking region and display the reduced region over road network for location data requesters. With the toolkit, we demonstrate the practicality and effectiveness of the ReverseCloak approach.
[LBS users, positioning technology, location data requesters, Roads, graphical user interfaces, ReverseCloak, multilevel reversible approach, Mobile handsets, cloaking data, road networks, location perturbation system, Privacy, Information science, user location, mobile computing, multilevel cloaking regions visualize, cloaking region, Deanonymizer GUI, location privacy, authorisation, data protection, location-based services, access control, Anonymizer GUI, Graphical user interfaces, k-anonymization, sensing technology, location anonymization, wireless networks, Data visualization, mobile devices, Solids, reversible multi-level location privacy protection system]
Hopsworks: Improving User Experience and Development on Hadoop with Scalable, Strongly Consistent Metadata
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Hadoop is a popular system for storing, managing,and processing large volumes of data, but it has bare-bonesinternal support for metadata, as metadata is a bottleneck andless means more scalability. The result is a scalable platform withrudimentary access control that is neither user- nor developer-friendly. Also, metadata services that are built on Hadoop, suchas SQL-on-Hadoop, access control, data provenance, and datagovernance are necessarily implemented as eventually consistentservices, resulting in increased development effort and morebrittle software. In this paper, we present a new project-based multi-tenancymodel for Hadoop, built on a new distribution of Hadoopthat provides a distributed database backend for the HadoopDistributed Filesystem's (HDFS) metadata layer. We extendHadoop's metadata model to introduce projects, datasets, andproject-users as new core concepts that enable a user-friendly, UI-driven Hadoop experience. As our metadata service is backed bya transactional database, developers can easily extend metadataby adding new tables and ensure the strong consistency ofextended metadata using both transactions and foreign keys.
[Access control, Java, meta data, Hopsworks, Hadoop, Data Management, Metadata, Dynamic Roles, Sparks, Yarn, parallel processing, Hadoop distributed file system metadata layer, user-friendly UI-driven Hadoop experience, Distributed databases, Mutli-tenancy, data handling, transactional database]
Isolation in Docker through Layer Encryption
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Containers are constantly gaining ground in the virtualization landscape as a lightweight and efficient alternative to hypervisor-based Virtual Machines, with Docker being the most successful representative. Docker relies on union-capable file systems, where any action performed to a base image is captured as a new file system layer. This strategy allows developers to easily pack applications into Docker image layers and distribute them via public registries. However, this image creation and distribution strategy does not protect sensitive data from malicious privileged users (e.g., registry administrator, cloud provider), since encryption is not natively supported. We propose and demonstrate a mechanism for secure Docker image manipulation throughout its life cycle: The creation, storage and usage of a Docker image is backed by a data-at-rest mechanism, which maintains sensitive data encrypted on disk and encrypts/decrypts them on-the-fly in order to preserve their confidentiality at all times, while the distribution and migration of images is enhanced with a mechanism that encrypts only specific layers of the file system that need to remain confidential and ensures that only legitimate key holders can decrypt them and reconstruct the original image. Through a rich interaction with our system the audience will experience first-hand how sensitive image data can be safely distributed and remain encrypted at the storage device throughout the container's lifetime, bearing only a marginal performance overhead.
[image distribution strategy, Cloud computing, Containers, Tools, cryptography, layer encryption, Virtual machining, Encryption, virtualisation, union-capable file system, image creation strategy, Docker, secure Docker image manipulation, virtual machines, hypervisor-based virtual machines, virtualization landscape, Virtualization]
Dela &#x2014; Sharing Large Datasets between Hadoop Clusters
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Big data has, in recent years, revolutionised an ever-growing number of fields, from machine learning to climate science to genomics. The current state-of-the-art for storing large datasets is either object stores or distributed filesystems, with Hadoop being the dominant open-source platform for managing `Big Data'. Existing large-scale storage platforms, however, lack support for the efficient sharing of large datasets over the Internet. Those systems that are widely used for the dissemination of large files, like BitTorrent, need to be adapted to handle challenges such as network links with both high latency and high bandwidth, and scalable storage backends that are optimised for streaming and not random access. In this paper, we introduce Dela, a peer-to-peer data-sharing service integrated into the Hops Hadoop platform that provides an end-to-end solution for dataset sharing. Dela is designed for large-scale storage backends and data transfers that are both non-intrusive to existing TCP network traffic and provide higher network throughput than TCP on high latency, high bandwidth network links, such as transatlantic network links. Dela provides a pluggable storage layer, implementing two alternative ways for clients to access shared data: stream processing of data as it arrives with Kafka, and traditional offline access to data using the Hadoop Distributed Filesystem. Dela is the first step for the Hadoop platform towards creating an open dataset ecosystem that supports user-friendly publishing, searching, and downloading of large datasets.
[Hadoop clusters, open-source platform, Protocols, public domain software, Genomics, dataset sharing, Throughput, parallel processing, TCP network traffic, large datasets storing, storage management, large datasets sharing, Distributed databases, Bandwidth, BitTorrent, peer-to-peer data sharing, Bioinformatics, pluggable storage layer, large-scale storage backends, peer-to-peer computing, Hadoop, Big Data, data transfers, peer-to-peer, end-to-end solution, Dela, Peer-to-peer computing, Internet, Hadoop distributed filesystem]
In Vivo Evaluation of the Secure Opportunistic Schemes Middleware Using a Delay Tolerant Social Network
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Over the past decade, online social networks (OSNs) such as Twitter and Facebook have thrived and experienced rapid growth to over 1 billion users. A major evolution would be to leverage the characteristics of OSNs to evaluate the effectiveness of the many routing schemes developed by the research community in real-world scenarios. In this demonstration, we showcase the Secure Opportunistic Schemes (SOS) middleware which allows different routing schemes to be easily implemented relieving the burden of security and connection establishment. The feasibility of creating a delay tolerant social network is demonstrated by using SOS to enable AlleyOop Social, a secure delay tolerant networking research platform that serves as a real-life mobile social networking application for iOS devices. AlleyOop Social allows users to interact, publish messages, and discover others that share common interests in an intermittent network using Bluetooth, peer-to-peer WiFi, and infrastructure WiFi.
[Bluetooth, SOS middleware, Twitter, delay tolerant networking, social networking, Security, iOS (operating system), iOS devices, testbeds, peer-to-peer WiFi, mobile computing, routing scheme, security, opportunistic routing, device-to-device, secure delay tolerant networking research platform, mobile applications, connection establishment, Routing protocols, Facebook, online social network, middleware, delay tolerant social network, peer-to-peer computing, Social network services, Routing, infrastructure WiFi, Mobile applications, OSN, Middleware, peer-to-peer, security of data, delays, secure opportunistic scheme middleware, mobile social networking application, AlleyOop Social, social networking (online), message publishing, Delays, wireless LAN, intermittent network]
Scaling and Load Testing Location-Based Publish and Subscribe
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The rise of the Internet of things (IoT) poses massive scalability issues for location-based services. More particularly, location-aware publish and subscribe services are struggling to scale out the computation of matches between publications and subscriptions that continuously update their location. In this demonstration paper, we propose a novel distributed and horizontally scalable architecture for location-aware publish and subscribe. Our middleware architecture relies on a multi-step routing mechanism based on consistent hashing and range partitioning. To demonstrate its scalability, we present a traffic data generator, which, in contrast to existing generators, can be used to perform real-time load tests. Finally, we show that our architecture can be deployed on a small 10-node cluster and can process up to 80,000 location updates per second producing 25,000 matches per seconds.
[message passing, Scalability, Roads, traffic data generator, Routing, Generators, Internet of Things, location-aware publish and subscribe services, Middleware, multi-step routing mechanism, Computer architecture, Real-time systems, location-based services, middleware architecture, middleware]
A Distributed Event-Centric Collaborative Workflows Development System for IoT Application
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The rapid development of Internet of Things (IoT) attracts growing attention from both industry and academia. IoT seamlessly connects the physical world and cyberspace via various sensors. It is more worth for us to pay attention to the mechanism of the events to work collaboratively rather than those standalone sensors. In this paper, we present a Distributed Event-centric Collaborative Workflows development system for IoT application, called DECW. It supports loosely coupled event-based interaction between processes, which enables real-time response to events from the physical world. Unlike traditional centralized control flow mode, the interaction between processes in DECW is constrained by the event interface. Users could dynamically adjust the interface between processes without modifying the internal logic of the process. In addition, DECW system provides a full lifecycle for the development and operation of the IoT application, including graphical creation of processes, dynamic definition of the process interaction interfaces, logical validation, distributed packaging and deployment, parallel execution, and real-time monitoring and managing the running status of the IoT application.
[distributed data management, wireless sensor networks, graphical user interfaces, DECW system, parallel execution, graphical process creation, logical validation, distributed deployment, Servers, Internet of Things, parallel processing, Intelligent sensors, event-based interaction, IoT, event-centric workflows, Temperature sensors, distributed event-centric collaborative workflows development system, Real-time systems, distributed packaging, process interaction interfaces, Monitoring, Business]
Incentive Mechanism for Data-Centric Message Delivery in Delay Tolerant Networks
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
A key issue in delay tolerant networks (DTN) is to find the right node to store and relay messages. We consider messages annotated with the unique keywords describing the message subject, and nodes also adds keywords to describe their mission interests, priority and their transient social relationship (TSR). To offset resource costs, an incentive mechanism is developed over transient social relationships which enrich enroute message content and motivate better semantically related nodes to carry and forward messages. The incentive mechanism ensures avoidance of congestion due to uncooperative or selfish behavior of nodes.
[delay tolerant network, offset resource cost, Bluetooth, Humanoid robots, electronic messaging, incentive mechanism, Relays, resource allocation, Transient analysis, message annotation, Delay Tolerant Networks, relay networks (telecommunication), transient social relationship, Routing, delay tolerant networks, Data-centric, Incentive Mechanism, TSR, congestion avoidance, data-centric message delivery, relay messages, Delays, Androids, message storage, DTN]
Performance of Cognitive Wireless Charger for Near-Field Wireless Charging
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Wireless charging provides a convenient way to charge various mobile and IoT devices. Prior work in state-of-the-art wireless charging systems operates at a frequency controlled by explicit networking from the power-receiving devices and is designed for the environment when the participating devices are perfectly aligned with each other. The need for the finer control due to the devices' misalignment is increasing in near-field and pseudo-tightly coupled charging applications, as more charging pads, are being deployed in the public domains and serving heterogeneous clients. Because inductive-coupled charging applications are sensitive to the placement and alignment variations between the power-transmitting and the power-receiving coils, we design and build Cognitive Wireless Charger (CWC). CWC adaptively controls the operating frequency in real time using implicit feedback for optimal power transfer operations. This demo is to supplement our paper about CWC [1]. In this demo, we showcase the impact on power transfer performance caused by the variations in the placement and alignment between the charging coils of power transmitter and power receiver and demonstrate the performance improvement provided by CWC.
[Coils, power-receiving devices, optimal power transfer operations, adaptive controls, power-receiving coils, heterogeneous clients, Frequency control, CWC, pseudo-tightly coupled charging applications, Wireless communication, Prototypes, operating frequency controls, near-field wireless charging, inductive power transmission, power-transmitting coils, implicit feedback, coils, cognitive wireless charger, Inductive charging, Receivers, adaptive control, frequency control, IoT devices, inductive-coupled charging applications, device misalignment, Wireless sensor networks, charging pads, mobile devices]
Toward Vehicle Sensing: An Integrated Application with Sparse Video Vameras and Intelligent Taxicabs
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Due to the sparse distribution of road video surveillance cameras, precise trajectory tracking for vehicles remains a challenging task. To the best of our knowledge, none of the previous research considered using on-road taxicabs as mobile video surveillance cameras and road traffic flow patterns, therefore not suitable for recovering trajectories of vehicles. With this insight, we model the travel time-cost of a road segment during various time periods precisely with LNDs (Logarithmic Normal Distributions), then use LSNDs (Log Skew Normal Distributions) to approximate the time-cost of an urban trip during various time periods. We propose an approach to calculate possible location and time distribution of the vehicle, select the taxicab to verify the distribution by uploading and checking video clips of this taxicab, finally refine the restoring trajectory in a recursive manner. We evaluate our solution on real-world taxicab and road surveillance system datasets. Experimental results demonstrate that our approach outperforms alternative solutions in terms of accuracy ratio of vehicle tracking.
[log normal distribution, LND, Roads, Gaussian distribution, Mobile communication, trajectory tracking, road vehicles, sparse video cameras, on-road taxicabs, object tracking, Trajectory, video signal processing, video surveillance, road video surveillance cameras, LSND, vehicle sensing, road traffic, logarithmic normal distributions, intelligent taxicabs, log skew normal distributions, video cameras, traffic engineering computing, sparse distribution, integrated application, road traffic flow, Cameras, Video surveillance, vehicle tracking]
Segmentation of Time Series Based on Kinetic Characteristics for Storage Consumption Prediction
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The Internet services generate huge amount of data, which require large space for storage. Determining device purchase plan turns out to be very important for the service providers. Under-purchasing might lead to data loss, while over-purchasing would result in waste. In this paper, we propose a linear regression based approach to predict the storage demand according to the time series of the storage consumption. We partitioned the storage con-sumption time series into several linear segments, and perform prediction on the last segment using linear regression. Since the position of turning points between adjacent segments and the total number of the segments are both unknown, how to achieve the online segmentation becomes a big challenge. Aiming to solve this problem, we carried out the Kalman-Anova segmentation method. Experiment results show that our method has good accuracy in precision, recall and F-measure values. Moreover, the method is able to segment nonlinear time series as well, suggesting a potential wider application. The proposed method has been deployed in Baidu Inc. and saves about 45 thousand dollars in one of its device purchase program.
[Internet services, Kalman-Anova segmentation method, Segmentation, Noise reduction, Kalman filter, regression analysis, kinetic characteristics, turning points, storage demand, device purchase program, Kinetic characteristics, predict, Kalman filters, Time series analysis, Linear regression, storage consumption prediction, F-measure values, Time series, time series, Web services, linear regression based approach, Kinetic theory, Acceleration, Analysis of variance, online segmentation, linear segments]
A Multi-stage Hierarchical Window Model with Application to Real-Time Graph Analysis
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The dynamic nature of real-world networks, such as social networks and communication networks, has increased the focus towards real-time dynamic graph analysis. Observations made based on real-time analysis of dynamic graphs reflect the latest properties of the graph and have the most value in real-time analysis. Computing graph properties of large-scale, fast-evolving graphs in real-time is challenging due, not only to the high computational and memory cost, but also to the understanding of the result with respect to the data from which it was derived. This paper proposes a multi-stage hierarchical window model that can aid in rigorous understanding of complicated real-time results and we apply it to generate graphs based on real-time updates along with periodic computations on graph snapshots for processing dynamic graphs. Moreover, the paper discusses the utilization of parallel window computation. The paper evaluates the hierarchical model through analyzing graphs formed by cooccurring hashtags in a Twitter data-stream.
[text analysis, graph snapshots, cooccurring hashtags, Heuristic algorithms, graph theory, real-time analytics, network theory (graphs), real-time dynamic graph analysis, graph processing, Twitter, large-scale fast-evolving graphs, social network, dynamic graphs, graph properties, Real-time systems, memory cost, real-world network, communication network, Computational modeling, computational cost, multistage hierarchical window model, Generators, parallel window computation, Twitter data-stream, real-time graph analysis, Tagging, social networking (online)]
Dynamic Pricing at Electric Vehicle Charging Stations for Queueing Delay Reduction
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The research of electric vehicles (EVs) has gained more and more attention in recent years in both industry and academia, and new registrations of EVs increase rapidly, while the long delay at the convenient but crowded charging stations may discourage many drivers from switching to EVs. To address the problems, we propose a novel dynamic pricing policy that allows charging stations to adjust their service fees in real time based on the load at the stations. In our work, the selection of drivers is modeled by a new dissatisfaction function with multiple variables, which can be easily validated and improved by real applications, and our solution is evaluated from the real-world e-charge dataset. To the best of our knowledge, this is the first work that considers dynamic service fees among various charging stations for load balancing and reduction of queueing delay. This makes our work more realistic and beneficial.
[queueing delay reduction, dissatisfaction function, queueing theory, EVs, Roads, Charging stations, dynamic pricing policy, Dynamic pricing, Vehicle dynamics, Vehicles, notification system, electric vehicle charging, electric vehicle, Pricing, electric vehicle charging station, charging station, Load management, real-world e-charge dataset, e-charge, Delays, dynamic service fees, pricing]
Pairwise Ranking Aggregation by Non-interactive Crowdsourcing with Budget Constraints
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Crowdsourced ranking algorithms ask the crowd to compare the objects and infer the full ranking based on the crowdsourced pairwise comparison results. In this paper, we consider the setting in which the task requester is equipped with a limited budget that can afford only a small number of pairwise comparisons. To make the problem more complicated, the crowd may return noisy comparison answers. We propose an approach to obtain a good-quality full ranking from a small number of pairwise preferences in two steps, namely task assignment and result inference. In the task assignment step, we generate pairwise comparison tasks that produce a full ranking with high probability. In the result inference step, based on the transitive property of pairwise comparisons and truth discovery, we design an efficient heuristic algorithm to find the best full ranking from the potentially conflictive pairwise preferences. The experiment results demonstrate the effectiveness and efficiency of our approach.
[Crowdsourcing, Algorithm design and analysis, Smoothing methods, crowdsourcing, pairwise comparison tasks, Heuristic algorithms, probability, information retrieval, budget constraint, rank aggregation, Noise measurement, inference mechanisms, noninteractive crowdsourcing, pairwise preferences, heuristic algorithm, task requester, pairwise ranking aggregation, budget constraints, Inference algorithms, task assignment, Reliability, result inference]
Buffer-Based Reinforcement Learning for Adaptive Streaming
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Adaptive streaming improves user-perceived quality by altering the streaming bitrate depending on network conditions, trading reduced video bitrates for reduced stall times. Existing adaptation approaches, e.g., rate-based, buffer-based, either rely heavily on accurate bandwidth prediction or can be overly-conservative about video bitrates. In this work, we propose a reinforcement learning approach to choose the segment quality during playback. This approach uses only the buffer state information and optimizes for a measure of user-perceived streaming quality. Simulation results show that our proposed approach achieves better QoE than rate-, buffer-based approaches, as well as other reinforcement learning approaches.
[network conditions, Buffer storage, adaptive streaming, buffer state information, user-perceived quality, QoE, Throughput, buffer-based reinforcement learning, bandwidth prediction, Standards, video bitrates, quality of experience, Bit rate, Learning (artificial intelligence), Streaming media, streaming bitrate, user-perceived streaming quality, video streaming, learning (artificial intelligence), video signal processing]
The Case for Using Content-Centric Networking for Distributing High-Energy Physics Software
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Named Data Networking (NDN) is one of the promising future internet architectures, which focuses on the data rather than its location (IP/host-based system). NDN has several characteristics which facilitate addressing and routing the data: fail-over, in-network caching and load balancing. This makes it useful in areas such as managing scientific data. The CMS experiment on the Large Hadron Collider (LHC) has a data access problem amenable to content-centric networking. CERN Virtual Machine File System (CVMFS) is used by High Energy Physics (HEP) community for worldwide software distribution. CVMFS maintain its data by using content-addressable storage, which makes it suitable for NDN. n this paper, we investigate the possibilities of using a content-centric networking architecture such as NDN on distributing CMS software.
[Protocols, load balancing, LHC, cache storage, Servers, software architecture, resource allocation, Computer architecture, NDN, high-energy physics software, Meteorology, IP/host-based system, CVMFS, Large Hadron Collider, CERN virtual machine file system, information retrieval, in-network caching, physics computing, future Internet architectures, content-centric networking, virtual machines, large hadron collider, data access, file organisation, Software, Internet, named data networking]
LAVEA: Latency-Aware Video Analytics on Edge Computing Platform
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
We present LAVEA, a system built for edge computing, which offloads computation tasks between clients and edge nodes, collaborates nearby edge nodes, to provide low-latency video analytics at places closer to the users. We have utilized an edge-first design to minimize the response time, and compared various task placement schemes tailed for inter-edge collaboration. Our results reveal that the client-edge configuration has task speedup against local or client-cloud configurations.
[Cloud computing, fog computing, latency-aware video analytics, edge computing, Containers, Servers, computation offloading, LAVEA system, edge-first design, Edge computing, inter-edge collaboration, Collaboration, Bandwidth, low-latency video analytics, task placement scheme, Time factors, cloud computing, video signal processing]
Complete Tolerance Relation Based Filling Algorithm Using Spark
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
With the advent of cloud computing, renewable energy is integrated into data center power supply systems increasingly. The power statistics collection may not be available due to the instability of renewable energy, which results in incomplete data. The incomplete energy data will significantly disturb the management of data centers. We further propose a filling algorithm based on complete tolerance class. The algorithm expands the traditional tolerance relation, and fills the missing values of the energy data, which ensures the data integrity. By taking good advantage of in-Memory Computing, We further parallelize and optimize our algorithm using Spark. The experiment results demonstrate that our algorithm outperforms other general filling algorithms in terms of filling accuracy. The proposed algorithm also shows good performance as the missing rate rises up.
[power statistics collection, renewable energy sources, Big Data, complete tolerance relation, data integrity, Sparks, Indexes, general filling algorithms, computer centres, power engineering computing, Renewable energy sources, renewable energy, data center power supply systems, Rough sets, Computer architecture, Filling, Spark, filling accuracy, cloud computing, in-memory computing, power supplies to apparatus]
Towards Secure Public Directory for Privacy-Preserving Data Sharing
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In the age of big data, information sharing paradigm has been fundamentally changed: Data collection and consumption are increasingly decentralized (partly due to the advent of personal computing devices), and sharing personal data over the Internet becomes a prominent paradigm for new applications. One of these applications is Health Information Exchange (or HIE) where a patient's electronic medical record (EMR) was produced at one hospital, and is consumed by a physician in another hospital.
[Computational modeling, information sharing paradigm, HIE, Big Data, personal data sharing, health information exchange, electronic health records, secure public directory, data consumption, data collection, attack modeling, Privacy-enhancing, Authorization, patient electronic medical record, Privacy, Hospitals, security of data, Authentication, privacy-preserving data sharing, data privacy, Internet, hospital]
Anonymous Routing to Maximize Delivery Rates in DTNs
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In this paper, we seek to address anonymous communications in delay tolerant networks (DTNs). While many different approaches for the internet and ad hoc networks, to the best of our knowledge, only variants of onion-based routing have been tailored for DTNs. Since each type of anonymous routing protocol has its advantages and drawbacks, there is no single anonymous routing protocol for DTNs that can adapt to the different levels of security requirements. In this paper, we first design a set of anonymous routing protocols for DTNs, called anonymous epidemic and zone-based anonymous routing, based on the original anonymous routing protocols for ad hoc networks. Then, we propose a framework of anonymous routing (FAR) for DTNs, which subsumes all the aforementioned protocols. By tuning its parameters, the proposed FAR is able to outperform onion-based, anonymous Epidemic, and zone-based routing.
[Routing, delay tolerant networks, Ad hoc networks, Relays, anonymous routing protocols, onion-based routing, framework of anonymous routing, delivery rates, anonymous routing, anonymous epidemic, DTNs, routing protocols, FAR, zone-based anonymous routing, Routing protocols, Delays, Internet, Cryptography, ad hoc networks, Delay tolerant networks, DTN]
Evaluating Connection Resilience for the Overlay Network Kademlia
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Kademlia is a decentralized overlay network, up to now mainly used for highly scalable file sharing applications. Due to its distributed nature, it is free from single points of failure. Communication can happen over redundant network paths, which makes information distribution with Kademlia resilient against failing nodes and attacks. In this paper, we simulate Kademlia networks with varying parameters and analyze the number of node-disjoint paths. With our results, we show the influence of these parameters on the network connectivity and, therefore, the resilience against failing nodes and communication channels.
[Protocols, telecommunication channels, connection resilience, Kademlia networks, distributed processing, failing nodes, Distributed Systems, Overlay networks, network connectivity, Overlay Networks, communication channels, file sharing, Network Connectivity, Routing, Resilience, information distribution, redundant network paths, node-disjoint paths, overlay networks, Communication channels, decentralized overlay network, Software, Peer-to-peer computing, distributed overlay network, Fault Tolerance]
Shortfall-Based Optimal Security Provisioning for Internet of Things
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
We present a formal method for computing the best security provisioning for Internet of Things (IoT) scenarios characterized by a high degree of mobility. The security infrastructure is intended as a security resource allocation plan, computed as the solution of an optimization problem that minimizes the risk of having IoT devices not monitored by any resource. We employ the shortfall as a risk measure, a concept mostly used in the economics, and adapt it to our scenario. We show how to compute and evaluate an allocation plan, and how such security solutions address the continuous topology changes that affect an IoT environment.
[Economics, risk management, optimization problem, stochastic allocation, risk minimization, telecommunication network topology, Network security, Pareto optimization, security infrastructure, security resource allocation plan, Electronic mail, Security, Internet of Things, Optimization, computer network security, shortfall-based optimal security provisioning, optimisation, resource allocation, continuous topology changes, risk measure, Resource management, Internet-of-Things scenarios, Investment]
Group Differential Privacy-Preserving Disclosure of Multi-level Association Graphs
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Traditional privacy-preserving data disclosure solutions have focused on protecting the privacy of individual's information with the assumption that all aggregate (statistical) information about individuals is safe for disclosure. Such schemes fail to support group privacy where aggregate information about a group of individuals may also be sensitive and users of the published data may have different levels of access privileges entitled to them. We propose the notion of &#x03B5;<sub>g</sub>-Group Differential Privacy that protects sensitive information of groups of individuals at various defined privacy levels, enabling data users to obtain the level of access entitled to them. We present a preliminary evaluation of the proposed notion of group privacy through experiments on real association graph data that demonstrate the guarantees on group privacy on the disclosed data.
[Drugs, Data privacy, information privacy protection, aggregate information, Data analysis, group privacy, graph theory, graph data, multilevel association graphs, Electronic mail, &#x03B5;<sub>g</sub>-group differential privacy, group differential privacy-preserving disclosure, Privacy, Aggregates, data protection, data privacy, Bipartite graph]
Tracking Information Flow in Cyber-Physical Systems
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Cyber-Physical Systems are distributed, heterogeneous, decentralized and loosely coupled networks in which individual systems measure physical processes, exchange information, and influence processes. Sensors measure these physical processes, while aggregators process them and actuators perform resulting actions. Decisions are often based on sensor data collected by other systems. Furthermore, the aggregators also interchange information and use them to derive own decisions. Decisions must be comprehensible. However, this is only the case if all data dependencies are known. Due to the size of these networks, their loose coupling and their dynamic behavior, decisions made by a system are not always easy to understand. If an error occurs in the system, the error source must be identified. It must be known on which data a decision was based. However, since the decision can be based on information from other nodes, the search for the error source is not a trivial task. Keep in mind, that dependent nodes can have dependencies themselves as well. We present the Information Flow Monitor (IFM) that collects information about semantic data dependencies in dynamic networks. The collected dependency information is provided at a central network location. Subsequently, semantic dependencies between information can be visualized.
[Actuators, Protocols, aggregators, semantic data dependencies, information flow tracking, CPS, Cyber-physical systems, Cyber-Physical Systems, History, information flow monitor, cyber-physical systems, IFM, Semantic Dependencies, Semantics, system monitoring, Data Tracking, Sensors, Monitoring, Information Flow Monitor]
Privacy-Preserving Matchmaking in Geosocial Networks with Untrusted Servers
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
As a major branch of LBSs, geosocial networking services become popular. An important functionality of geosocial networking services is allowing people to find potential friends who have similar profile within close proximity and initiate communication with each other. However, in order to realize this functionality, most existing services require mobile users to reveal their profiles and location information to an untrusted service provider, which may expose LBSs to vulnerabilities for abuse and endanger mobile users' privacy. To address this problem, we propose to encrypt users' profile with a new searchable encryption scheme. Combining this searchable encryption scheme with other cryptographic techniques we construct a privacy- preserving matchmaking system. Compared with a previous one that aims to solve the same problem, ours is more secure, supports more flexible functionalities and moves computationally heavy key updates to resourceful service providers.
[untrusted servers, location information, Mobile communication, cryptography, Encryption, cryptographic techniques, Servers, Indexes, privacy-preserving matchmaking system, Privacy, geosocial networking services, service providers, social networking (online), data privacy, searchable encryption scheme, mobile users, LBS, Mobile computing]
You&#x2019;ve Been Tricked! A User Study of the Effectiveness of Typosquatting Techniques
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The deceitful practice of Typosquatting involves deliberately registering Internet domain names containing typographical errors that primarily target popular domain names, in an effort to redirect users to unintended destinations or steal traffic for monetary gain. Typosquatting has existed for well over two decades and continues to be a credible threat to this day. While much of the prior work has examined various typosquatting techniques and how they change over time, none have considered how effective they are in deceiving users. In this paper, we attempt to fill in this gap by conducting a user study that exposes subjects to several uniform resource locators (URLs) in an attempt to determine the effectiveness of several typosquatting techniques that are prevalent in the wild. We also attempt to determine if the security education and awareness of cybercrimes such as typosquatting will affect the behavior of Internet users.
[Correlation, Typosquatting, Internet domain names, cybercrimes, monetary gain, Internet users, Domain Names, Defenses, security education, Electronic mail, Computer crime, URL, Uniform resource locators, Education, typosquatting techniques, computer crime, Internet, typographical errors, uniform resource locators]
Real-Time Detection of Illegal File Transfers in the Cloud
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
There has been a prolific rise in the popularityof cloud storage in recent years. While cloud storage offersmany advantages such as flexibility and convenience, users arenow unable to tell or control the actual locations of their data. This limitation may affect users' confidence and trust in thestorage provider, or even be unsuitable for storing data withstrict location requirements. To address this issue, we proposean illegal file transfer detection framework that constantlymonitors the real-time file transfers in the cloud and is capableof detecting potential illegal transfers which moves sensitivedata outside the ("legal") boundaries specified by the fileowner. The main idea is to classifying multiple users' location preferences when making the data storage arrangement inthe cloud nodes. We model the legal file transfers amongnodes as a weighted graph and then maximize the probabilityof storing data items of similar privacy preferences in thesame region. Then we leverage the socket monitoring functionsprovided by LAST-HDFS (a recent location-aware Hadoop filestorage system) to monitor the real-time communication amongcloud nodes. Based on our legal file transfer graph and thedetected communication, we propose an approach to calculatethe probability of the detected transfer to be illegal.
[Cloud computing, weighted graph, Law, graph theory, detected transfe rprobability, File Transfer, parallel processing, Cloud Storage, Privacy, location-aware Hadoop filestorage system, legal file transfer graph, security of data, Sockets, LAST-HDFS, file organisation, socket monitoring functions, Real-time systems, Location-Awareness, data handling, cloud computing, illegal file transfer detection, Monitoring, Data Privacy]
Eyes of the Swarm: Streamers&#x2019; Detection in BT
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Many BitTorrent (BT) clients are using these BT networks as a video-on-demand service, taking advantage of the popularity and the large collection of media available. However, transforming the swarms into an on-demand media service can cause serious damage to the overall network performance. In this paper, we propose a methodology, using the concepts of Entropy, and present a Spy BitTorrent client that is able to identify peers streaming in a swarm. Large scale monitoring, for real swarms, were performed to detect the presence of streamers.
[real swarms, Protocols, Spy BitTorrent client, Media, Tools, large scale monitoring, Entropy, streamers detection, video-on-demand service, entropy, BT networks, video on demand, Computer architecture, Streaming media, media streaming, on-demand media service, Monitoring]
Load Prediction for Energy-Aware Scheduling for Cloud Computing Platforms
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
We address online scheduling for servers of Cloud service providers. Each server is composed of several variable-speed processors whose power function is convex. The servers may be busy, idle or switched off. The objective of our scheduling is to minimize the energy consumed by a Cloud computing platform. To achieve this goal, we try to anticipate computing demands by predicting a workload, then we modify the set of available servers to fit this prediction and finally we schedule our jobs on the available servers. To schedule jobs we have developed the POD (Predict Optimize Dispatch) algorithm. We evaluate its performance for real-life traces in the presence of different types of prediction. The analysis shows that our scheduling reduces energy consumption considerably.
[Energy consumption, Cloud computing, cloud service providers, online scheduling, Switches, power function, Servers, Program processors, power aware computing, Processor scheduling, energy-aware scheduling, POD algorithm, Cloud, scheduling, Prediction algorithms, predictor, multi-armed bandit, load prediction, cloud computing, cloud computing platforms, predict optimize dispatch algorithm, energy consumption]
Learn-as-You-Go with Megh: Efficient Live Migration of Virtual Machines
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
We propose a reinforcement learning algorithm, Megh, for live migration of virtual machines that simultaneously reduces the cost of energy consumption and enhances the performance. Megh learns the uncertain dynamics of workloads as-it-goes. Megh uses a dimensionality reduction scheme to projectthe combinatorially explosive state-action space to a polynomial dimensional space. These schemes enable Megh to be scalable and to work in real-time. We experimentally validate that Megh is more cost-effective and time-efficient than the MadVM and MMT algorithms.
[Cloud computing, Energy consumption, Heuristic algorithms, workload uncertain dynamics, state-action space, power aware computing, learn-as-you-go, dimensionality reduction, Real-time systems, cloud computing, learning (artificial intelligence), polynomials, Reinforcement learning, Virtual machining, polynomial dimensional space, Live migration, virtual machine live migration, Online algorithms, Learning (artificial intelligence), virtual machines, Markov decision process, Energy efficiency, Resource management, energy consumption reduction, Megh, reinforcement learning algorithm]
Machine-Learning Based Performance Estimation for Distributed Parallel Applications in Virtualized Heterogeneous Clusters
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In a virtualized heterogeneous cluster, for a distributed parallel application which runs in multiple virtual machines (VMs) concurrently, there are a huge number of possible ways to place its VMs. This paper investigates a performance estimation technique for distributed parallel applications in virtualized heterogeneous clusters. We first analyze the effects of different VM configurations on the performance of various distributed parallel applications. We then present a machine-learning based performance model for a distributed parallel application. Using a heterogeneous cluster with two different types of nodes, we show that our machine-learning based models can estimate the runtimes of distributed parallel applications with modest error rates.
[Measurement, Correlation, Error analysis, Estimation, Parallel and distributed applications, Interference, Quality of service, distributed parallel applications, parallel processing, virtualized heterogeneous clusters, Machine-learning based performance model, Runtime, Heterogeneous clusters, virtual machines, machine-learning based performance estimation, error rates, learning (artificial intelligence), VM configurations]
Incremental Elasticity for NoSQL Data Stores
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Elasticity actions in NoSQL data stores move large amounts of data over the network to take advantage of new resources. Here we propose incremental elasticity, a new mechanism for scheduling data transfers to a joining server, leading to smoother elasticity actions with a reduced performance impact.
[Cloud computing, Data store elasticity, elasticity actions, Elasticity, Throughput, data transfers, Distributed middleware, Servers, SQL, Computer science, NoSQL data stores, electronic data interchange, incremental elasticity, Benchmark testing, scheduling, Data transfer, joining server]
A Framework for Efficient Energy Scheduling of Spark Workloads
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Nowadays distributed processing frameworks like Apache Spark have been successfully used for the execution of big data applications. Despite their wide adoption little work has been done in terms of controlling the applications' energy consumption. Datacenters contribute over 2 % of the total US electric usage therefore minimizing the energy utilization of Spark application can be extremely helpful. Solving this energy consumption problem requires the scheduling of Spark applications in an energy-efficient way. However, the problem is challenging as we also have to consider application performance requirements. In this work, we provide the overview of a novel framework that orchestrates the execution order of Spark applications, exploiting DVFS to tune the computing nodes CPU frequencies in order to minimize the energy consumption and satisfy application's performance requirements. Our early experimental results illustrate the working and benefits of our framework.
[Energy consumption, Time-frequency analysis, distributed processing, Spark application scheduling, Distributed Systems, Radio spectrum management, Distributed processing, power aware computing, energy efficiency, scheduling, application energy consumption, Spark workloads, application performance requirements, Power demand, Green Computing, Big Data, Scheduling, Sparks, distributed processing frameworks, Apache Spark, datacenters, big data applications, Processor scheduling, energy scheduling, US electric usage, energy consumption problem, energy utilization, DVFS]
Towards a Complete Virtual Data Center Embedding Algorithm Using Hybrid Strategy
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
A Virtual Data Center (VDC) is a set of virtual machines (VMs) connected by a Virtual Network (VN) topology. Today's cloud data centers support dynamic requests for VDCs, by using software defined embedding strategies thatallow them to mesh multiple VDCs onto their Physical Data Center(PDC) network and machines. In this paper, we present a solution to this VDC embedding problem that achieves a higher acceptance rate by minimizing fragmentation, compared to existing strategies, while at the same time minimally disrupting the existing VDCs.
[Cloud computing, hybrid strategy, complete virtual data center embedding algorithm, physical data center network, Heuristic algorithms, Virtual Network, Migration, Virtual machining, software defined embedding strategies, computer centres, Sorting, Computer science, virtual network topology, PDC network, Virtual Data Center, Distributed databases, virtual machines, VDC embedding problem, Reliability, cloud computing, cloud data centers, Virtual Machine]
Federating Consistency for Partition-Prone Networks
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Groups of strongly consistent devices can efficiently order events under ideal (data center) conditions, but become less effective in dynamic and heterogeneous environments. Weakly consistent devices efficiently tolerate both faults and dynamic conditions but are slow to converge on a single ordering of system events. We propose "federated consistency\
[Performance evaluation, Out of order, partition-prone networks, Protocols, anti-entropy, open systems, fault tolerance, Conferences, distributed version vectors, interoperability, Topology, federated consistency, federated system, Distributed computing, computer centres, consistency, software fault tolerance, geo-replication, consensus, weakly-consistent devices, data center, totally ordered fault-tolerant event sequence, Delays, cloud computing]
Mitigating Nesting-Agnostic Hypervisor Policies in Derivative Clouds
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
The fixed granularity of virtual machines offered by IaaS providers has prompted the evolution of derivative clouds where resources are repackaged into smaller containers and leased out typically in PaaS mode. In such a setup, containers are provisioned within virtual machines. Such a nested setup results in two control centers for the resources used by those containers-the guest OS and the Hypervisor. The latter's control actions are agnostic of the application executing within a VM. This lack of visibility may result in hypervisor control that has a non-uniform effect on the VM's nested containers which is undesirable. In this work, we propose policy based control of the effect of the hypervisor's control actions amongst the containers nested in the affected VM.
[Cloud computing, PaaS, Containers, IaaS, Virtual machining, Virtual machine monitors, Memory management, nesting-agnostic hypervisor policies mitigation, virtual machines, VM, system monitoring, Resource management, cloud computing, Virtualization, derivative clouds]
A Novel Architecture for Efficient Fog to Cloud Data Management in Smart Cities
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Traditional smart city resources management rely on cloud based solutions to provide a centralized and rich set of open data. The advantages of cloud based frameworks are their ubiquity, (almost) unlimited resources capacity, cost efficiency, as well as elasticity. However, accessing data from the cloud implies large network traffic, high data latencies, and higher security risks. Alternatively, fog computing emerges as a promising technology to absorb these inconveniences. The use of devices at the edge provides closer computing facilities, reduces network traffic and latencies, and improves security. We have defined a new framework for data management in the context of smart city through a global fog to cloud management architecture; in this paper we present the data acquisition block. As a first experiment we estimate the network traffic during data collection, and compare it with a traditional real system. We also show the effectiveness of some basic data aggregation techniques in the model, such as redundant data elimination and data compression.
[data compression, Data Aggregation, redundant data elimination, Smart City, Conferences, cloud management architecture, data acquisition block, Data Management, Data Lifecycle (DLC) model, fog-to-cloud data management, Distributed computing, data collection, cloud based solution, data aggregation techniques, smart cities, cloud based framework, data handling, cloud computing, smart city resources management, Fog-to-Cloud (F2C) computing]
Networklet: Concept and Deployment
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In today's datacenters, resource requests from tenants are increasingly transforming into hybrid requests that may simultaneously demand IaaS, Paas, and SaaS resources. This paper tackles the challenge of modeling and deploying hybrid tenant requests in datacenters, for which we coin "networklet" to represent a set of VMs that collaboratively provide a PaaS or SaaS service. Through extracting networklets from tenant requests and thus sharing them between tenants, we can achieve a win-win situation for datacenter providers and tenants.
[Computational modeling, hybrid tenant requests, resource requests, Software as a service, Paas, IaaS, Servers, computer centres, win-win situation, Hoses, datacenters, Databases, resource allocation, networklet, Bandwidth, virtual machines, VM, SaaS resources, Resource management, cloud computing]
Optimistic Causal Consistency for Geo-Replicated Key-Value Stores
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In this paper we present a new approach to implementing causal consistency in geo-replicated data stores, which we call Optimistic Causal Consistency (OCC). The optimism in our approach lies in that updates from a remote data center are immediately made visible in the local data center, without checking if their causal dependencies have been received. Servers perform the dependency check needed to enforce causal consistency only upon serving a client operation, rather than on the receipt of a replicated data item as in existing systems. OCC explores a novel trade-off in the landscape of causal consistency protocols. The potentially blocking behavior of OCC makes it vulnerable to network partitions. Because network partitions are rare in practice, however, OCC chooses to trade availability to maximize data freshness and reduce the communication overhead. We further propose a recovery mechanism that allows an OCC system to fall back on a pessimistic protocol to continue operating even during network partitions. POCC is an implementation of OCC based on physical clocks. We show that OCC improves data freshness, while offering comparable or better performance than its pessimistic counterpart.
[Protocols, Conferences, dependency check, distributed processing, potentially blocking behavior, Servers, History, system recovery, causal consistency protocol, Semantics, replicated data item, recovery mechanism, Causal Consistency, protocols, causal dependency, physical clocks, Geo-replication, geo-replicated key-value stores, remote data center updates, Optimistic Causal Consistency, data freshness, local data center, data integrity, Synchronization, computer centres, causality, network partition vulnerability, communication overhead reduction, pessimistic protocol, geo-replicated data stores, client operation, optimistic causal consistency, Clocks]
Automated Performance Evaluation for Multi-tier Cloud Service Systems Subject to Mixed Workloads
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In multi-tier cloud service systems, performance evaluation relies on numerous experiments in order to collect key metrics such as resources usage. The approach may result in highly time-consuming in practice. In this paper, we propose an automated framework for performance tracking, data management and analysis to minimize human intervention in multi-tier cloud service systems. The framework support fine-grained analysis of the mixed workloads through the Discrete-time Markov-modulated Poisson process (DMMPP). A general multi-tier application is theoretically formulated as a queueing network to evaluate the performance. The effectiveness of the model has been validated through extensive experiments conducted in the RUBiS benchmark system.
[Performance evaluation, queueing theory, data analysis, data management, DMMPP, fine-grained analysis, Throughput, mixed workloads, Steady-state, Servers, Analytical models, queueing network, discrete-time Markov-modulated Poisson process, RUBiS benchmark system, multi-tier cloud service systems, general multi-tier application, Markov processes, performance tracking, Time factors, cloud computing, automated performance evaluation, Queueing analysis]
Decentralised Runtime Monitoring for Access Control Systems in Cloud Federations
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Cloud federation is an emergent cloud-computing paradigm where partner organisations share data and services hosted on their own cloud platforms. In this context, it is crucial to enforce access control policies that satisfy data protection and privacy requirements of partner organisations. However, due to the distributed nature of cloud federations, the access control system alone does not guarantee that its deployed components cannot be circumvented while processing access requests. In order to promote accountability and reliability of a distributed access control system, we present a decentralised runtime monitoring architecture based on blockchain technology.
[Access control, Cloud computing, system accountability, software reliability, Random access memory, cloud-computing paradigm, Blockchain, system reliability, Runtime Monitoring, cloud federations, Security, access control systems, software architecture, Runtime, Computer architecture, authorisation, data protection, Access Control, cloud computing, blockchain technology, Monitoring, FAA, data flow analysis, Cloud Federation, decentralised runtime monitoring architecture, data privacy, data sharing]
DuoFS: An Attempt at Energy-Saving and Retaining Reliability of Storage Systems
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
As issues of the Energy Wall and the Reliability Wall become unavoidable, it is a demanding and challenging task to reduce energy consumption in large-scale storage systems in modern data centres while retaining acceptable systems reliability. Most energy conservation techniques inevitably have adverse impacts on the parallel disk systems. To address the reliability issues of energy-efficient parallel storage systems, we propose a reliable energy-efficient storage system called DuoFS, which aims at improving both energy efficiency and reliability of parallel storage systems by seamlessly integrating HDDs and SSDs. With the help of the middleware layer, DuoFS can distribute popular data to SSD-based nodes and put HDD-based nodes into the low-power mode under light workload conditions without modification of the parallel systems.
[Energy consumption, reliability, energy wall, HDD, Electronic mail, data centres, parallel processing, hard discs, storage management, power aware computing, File systems, DuoFS, middleware, reliability wall, parallel disk systems, Power demand, SSD, PLFS, Middleware, computer centres, energy saving, hybrid parallel storage system, PVFS, parallel storage systems, Reliability, energy consumption reduction, middleware layer, energy- efficient, Energy storage, disc drives]
A Proposal of an Efficient Traffic Matrix Estimation Under Packet Drops
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Traffic matrix (TM) estimation has been extensively studied for decades. Although conventional estimation techniques assume that traffic volumes are unchanged between origins and destinations, packets are often discarded on a path due to traffic burstiness, silent failures, etc. This paper proposes a novel TM estimation method that works correctly even under packet drops. The method is established on a Boolean fault localization technique; the technique requires fewer counters though it only determines whether each link is healthy. This paper extends the Boolean technique so as to deal with traffic volumes with error bounds just by a small number of counters. Along with submodular optimization for the minimum counter placement, we evaluate our method with real network datasets.
[Radiation detectors, error bounds, Estimation, submodular optimization, Boolean algebra, Optimization, matrix algebra, packet drops, Volume measurement, Measurement uncertainty, minimum counter placement, TM estimation method, Boolean fault localization technique, traffic burstiness, computer network reliability, Mathematical model, traffic volumes, telecommunication traffic, efficient traffic matrix estimation]
Straggler Mitigation for Distributed Behavioral Simulation
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Running large-scale behavioral simulations requires high computational power, which can be acquired by distributing computation workload to multiple computing nodes (i.e., workers) that run in parallel. The implementations of such systems commonly follow the Bulk Synchronous Parallel (BSP) model. However, implementations using BSP usually suffer from the straggler problem, where the delay of any worker slows down the entire simulation. The problem usually occurs due to communication delays or imbalanced workload among workers. To mitigate the straggler problem, we propose a novel parallel computational model, called Priority Synchronous Parallel (PSP) model. PSP exploits data dependencies of parallel processes to determine high priority data to be computed and synchronized while computing the remaining data. PSP is implemented and evaluated using traffic simulations for three large cities. The proposed technique shows significant performance improvements over the BSP model.
[Adaptation models, road traffic, data dependencies, Computational modeling, Urban areas, straggler, distributed computating, priority synchronous parallel model, digital simulation, Partitioning algorithms, traffic engineering computing, city traffic simulations, Synchronization, parallel processing, BSP model, distributed behavioral simulation, straggler mitigation, bulk synchronous parallel model, PSP model, behavioral simulations, Data models, Delays, parallel computational model, BSP]
Supporting Resource Control for Actor Systems in Akka
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Although there are models and prototype implementations for controlling resource use in Actor systems, they are difficult to implement for production implementations of Actors such as Akka. This is because the messaging and scheduling infrastructures of runtime systems are increasingly complex and significantly different from one system to another. This paper presents our efforts in implementing resource control support for Actor systems implemented using the Akka library. Particularly, given the lack of support in Akka for direct scheduling of actors, we compare two different ways of approximating actor-level control support. The first implementation expects messages to actors to provide estimates of resources likely to be consumed for processing them; these estimates are then relied upon to make scheduling decisions. In the second implementation, resource use of scheduled actors is tracked, and compared against allocations to decide when they should be scheduled next. We present experimental results on the performance cost of these resource control mechanisms, as well as their impact on resource utilization.
[Context, production implementations, Akka library, Scala, Switches, actor-level control support, Throughput, resource control, Actors, resource allocation, actor scheduling, Resource Control, messaging infrastructure, Logic gates, scheduling, software tools, actor systems, runtime systems, Resource management, scheduling infrastructure, resource utilization, Message systems, Akka]
A Distributed Operating System Network Stack and Device Driver for Multicores
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
With the advances in network speeds a single processor cannot cope anymore with the growing number of data streams from a single network card. Multicore processors come at a rescue but traditional SMP OSes, which integrate the software network stack, scale only to a certain extent,limiting an application's ability to serve more connections while increasing the number of cores. On the other hand, kernel bypass solutions seem to scale better, but limit resource flexibility and control. We propose attacking these problems with a distributed OS design, using multiple network stacks (one per kernel) and relying on multi-queue hardware and hardware flow steering. This creates a single-socket abstraction among kernels while minimizing inter-core communication. We introduce our design, consisting of a distributed network stack, a distributed device driver, and a load-balancing algorithm. We compare our prototype, NetPopcorn, with Linux, Affinity Accept, FastSocket. NetPopcorn accepts between 5 to 8 times more connections and reduces the tail latency compared to these competitors. We also compare NetPopcorn with mTCP and observe that for high core counts, mTCP accepts only 18% more connections yet with higher tail latency than NetPopcorn.
[multiprocessing systems, Multicore processing, Scalability, OS, Popcorn Linux, software network stack, device drivers, multicore, distributed device driver, network, resource allocation, Linux, single-socket abstraction, single IP, SMP, operating systems (computers), distributed OS design, load-balancing algorithm, Hardware, multicore processors, connections scalability, intercore communication minimization, Kernel, distributed operating system network stack]
Cache Potentiality of MONs: A Prime
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Node buffer size has a big influence on performance of Mobile Opportunistic Networks (MONs). This is mainly because each node should temporarily cache packets to deal with the intermittently connected links. In this paper, we study fundamental bounds on node buffer size below which the network system can not achieve the expected performance. Given the condition that each link has the same probability p to be active, and q to be inactive during each time slot, there exits a critical value p<sub>c</sub> from a percolation perspective. If p &gt; p<sub>c</sub>, the network is in the supercritical case, there is an achievable upper bound on the buffer size of nodes, independent of the inactive probability q. When p &lt;; p<sub>c</sub>, the network is in the subcritical case, and there exists a closed-form solution for buffer occupation, which is independent of the size of the network.
[Closed-form solutions, mobile radio, mobile opportunistic networks, buffer occupation, percolation theory, probability, cache potentiality, Switches, Mobile communication, cache storage, node buffer size, MON, intermittently-connected links, Wireless sensor networks, Upper bound, percolation perspective, Buffer size, Sensors, critical value, fundamental bound, Mobile computing]
Oak: User-Targeted Web Performance
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Web performance has long proved to be one of the most sought after and difficult to achieve components for the web. Since the inception of the modern web infrastructure, the situation has been growing in complexity, adding remote hosts and objects, providing everything from computation infrastructure, content distribution capability, and targeted advertising. While many of these components provide improvements for some users, the complexity of the Internet often leaves other users suffering from poor performance. We propose Oak, a system which addresses client performance on the individual level, hence addressing challenges which may be unique to the user. Oak measures a user's performance for objects loading on a page, and determines which components are under-performing. Oak further provides an automated mechanism by which sites are able to replace resources with those provided by a better performing alternative service for a particular user. In this work, we demonstrate the prevalence of under-performing services on the web, finding that over 60% of the Alexa Top 500 have at least one under-preforming server. We further evaluate Oak on experimental and popular existing webpages, and demonstrate its effectiveness in making decisions in existing environments and with a distributed user base.
[Conferences, distributed user base, distributed processing, Web servers, Complexity theory, user-targeted Web performance, Loading, file servers, user performance, Oak, remote objects, Performance analysis, Internet, Web sites, remote hosts]
A Self-Organizing Distributed and In-Band SDN Control Plane
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Adopting distributed control planes is critical towards ensuring high availability and fault-tolerance of dependable Software-Defined Networks (SDNs). However, designing and bootstrapping a distributed SDN control plane is a challenging task, especially if to be done in-band, without a dedicated control network, and without relying on legacy networking protocols. One of the most appealing and powerful notions of fault-tolerance is self-organization and this paper discusses the possibility of self-organizing algorithms for in-band control planes.
[self-organization, distributed control, telecommunication control, fault-tolerance, software defined networking, self-adjusting systems, self-organizing distributed control planes, Topology, Fault tolerance, bootstrapping, software-defined networks, Network topology, self-organizing algorithms, Fault tolerant systems, Decentralized control, in-band SDN control plane, Computer networks, distributed SDN control plane, fault tolerant control]
Serverless Programming (Function as a Service)
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
In this tutorial, we will present serverless computing, survey existing serverless platforms from industry, academia, and open source projects, identify key characteristics and use cases, and describe technical challenges and open problems. Our tutorial will involve a hands-on experience of using the serverless technologies available from different cloud providers (e.g. IBM, Amazon, Google and Microsoft). We expect our users to have basic knowledge of programming and basic knowledge of cloud computing.
[Cloud computing, serverless computing, Conferences, Tutorials, Quality of service, function as a service, Programming, Mobile communication, Computer architecture, serverless programming, cloud providers, cloud computing, serverless technologies]
Sensor Cloud: A Cloud of Sensor Networks
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Traditional model of computing with wireless sensors imposes restrictions on how efficiently wireless sensors can be used due to resource constraints. Newer models for interacting with wireless sensors such as Internet of Things and Sensor Cloud aim to overcome these restrictions. In this tutorial, I will discuss sensor cloud architectures, which enable different wireless sensor networks, spread in a huge geographical area to connect together and be used by multiple users at the same time on demand basis. I will further discuss how virtual sensors assist in creating a multiuser environment on top of resource constrained physical wireless sensors and can help in supporting multiple applications on-demand basis. I will then present some security issues and provide overview of the solutions to the problems from the literature. In particular, I will discuss energy efficient privacy and data integrity preserving data aggregation algorithm, risk assessment in sensor cloud as well as attribute-based access control for sensor cloud applications. The topics covered will be: 1. Cloud of Sensors - Sensor Cloud Architectures 2. Virtualization in Sensor Cloud 3. Scheduling and QoS in Sensor Cloud 4. Data compression and Secure Aggregation in Sensor Cloud 5. Security, Privacy and Risk Issues in Sensor Cloud Rationale: Sensor cloud is a paradigm of cloud computing within sensor networks for on-demand sensing applications. This tutorial was given in ACM Middleware, 2016.
[sensor cloud virtualization, Cloud computing, energy efficient privacy, risk assessment, wireless sensor networks, data aggregation algorithm, sensor cloud architectures, security issues, sensor networks, virtualisation, Communication system security, Security, data aggregation, Wireless communication, sensor cloud scheduling, resource allocation, QoS, authorisation, cloud computing, data compression, risk management, attribute-based access control, Tutorials, data integrity, wireless sensors, Computer science, Wireless sensor networks, security of data, data privacy, on-demand sensing, virtual sensors]
Publisher's Information
2017 IEEE 37th International Conference on Distributed Computing Systems
None
2017
Provides a listing of current committee members and society officers.
[]
Message from the ICDCS 2016 General Co-Chairs
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from the ICDCS 2016 Program Committee Co-Chairs
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Organizing Committee
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Provides a listing of current committee members and society officers.
[]
Technical Program Committees
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Provides a listing of current committee members and society officers.
[]
Keynotes
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Provides an abstract for each of the keynote presentations and may include a brief professional biography of each
[]
A Look at Basics of Distributed Computing
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
This tutorial presents concepts and basics of distributed computing which are important (at least from the author's point of view!), and should be known and mastered by Master students, researchers, and engineers. Those include: (a) a characterization of distributed computing (which is too much often confused with parallel computing); (b) the notion of a synchronous system and its associated notions of a local algorithm and message adversaries; (c) the notion of an asynchronous shared memory system and its associated notions of universality and progress conditions; and (d) the notion of an asynchronous messagepassing system with its associated broadcast and agreement abstractions, its impossibility results, and approaches to circumvent them. Hence, the tutorial can be seen as a guided tour to key elements that constitute basics of distributed computing.
[asynchronous messagepassing system, Uncertainty, message passing, asynchronous shared memory system, Computational modeling, synchronous system, message adversaries, researchers, distributed processing, master students, engineers, Distributed computing, distributed computing, associated broadcast, Program processors, Education, Parallel processing, shared memory systems, agreement abstractions, Reliability]
Cognitive Technologies for Smarter Cities
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Great computing technology advances during the early years of the new millennium facilitated a smarter cities vision. Smarter cities solutions focusing on specific city problem domains have proven their value in many proofs-of-concept. The insights and experiences from these experiments enable cities to deploy these types of solutions more widely with confidence. New advances in a broad range of technologies, in particular Internet of Things technologies and cognitive technologies, promise to expand the scope and the value of smarter cities solutions to greatly improve the lives of city dwellers. This tutorial gives an overview of smarter cities applications so far, and the technologies powering them. Then it proceeds to review the incipient technologies that will drive the next wave of smarter cities solutions: the Internet of Things driving cognitive computing systems. It concludes by examining two major smarter cities solution categories that promise to greatly improve the lives of city dwellers.
[Cloud computing, Adaptation models, cognitive systems, expert systems, Urban areas, Air quality, Internet of Things, cognitive technologies, town and country planning, Internet of things, smarter cities solutions, smart cities, Robot sensing systems, computing technology advances, Internet of Things technologies, cognitive computing systems, Water pollution]
Reflecting on the Past, Preparing for the Future: From Peer-to-Peer to Edge-Centric Computing
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
In many aspects of human activity, there has been a continuous struggle between the forces of centralization and decentralization. Computing exhibits the same phenomenon; after having abandoned mainframes in favor of PCs, the last decade has witnessed an unparalleled centralization and consolidation of services in data centers and clouds. Yet, trust, privacy, security and autonomy concerns are requiring to shift control again, taking services from the central nodes (the "core") to the other logical extreme (the "edge") of the Internet. This development can help blurring the boundary between man and machine, and embrace social computing in which humans are part of the computation and decision-making loop, resulting in a human-centered system design. In this tutorial we will elaborate on the necessary steps to be taken and challenges to be solved to realize this vision. The tutorial will include an overview of related research topics, including peerto-peer networks, blockchains, hybrid and decentralized cloud architectures.
[Cloud computing, trust concerns, autonomy concerns, privacy concerns, Conferences, decision-making loop, data centers, Electronic mail, human activity, edge-centric computing, human-centered system design, Computer architecture, cloud computing, security concerns, user centred design, decentralized cloud architectures, embrace social computing, peer-to-peer computing, computer centres, Biographies, PC, decision making, data privacy, Peer-to-peer computing, trusted computing]
Software-Based Networks: Leveraging High-Performance NFV Platforms to Meet Future Communication Challenges
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Summary form only given. Communication networks are changing: they are becoming more and more "software-based." The use of network function virtualization (NFV) to run network services in software enables software-defined networks (SDNs) to create a largely software-based network. To truly achieve the vision of a high-performance software-based network that is flexible, lower- cost, and agile, a fast and carefully designed network function virtualization platform along with a comprehensive SDN control plane is needed.Our high-performance NFV platform, OpenNetVM, enables high bandwidth network functions to operate at near line speed, while taking advantage of the flexibility and customization of low cost commodity servers. OpenNetVM exploits Intel's DPDK libraries to minimize the overhead of packet processing, and to provide high throughput, low latency networking in virtualized environments. OpenNetVM allows true zero-copy delivery of data to VMs, both for packet processing and high-speed inter-VM communication through shared huge pages within a trust boundary. We envision a dynamic and flexible network that can support a smarter data plane than just simple switches that forward packets. We build on our OpenNetVM NFV platform by developing our SDNFV network architecture that supports complex stateful routing of flows where processing by network functions (NFs) can dynamically modify the path taken by flows, without unduly burdening the centralized SDN controller. The tutorial will also briefly touch upon the problem of dynamic placement of network functions and routing of flows through a software based network, exploiting a mixture of centralized SDN control and NFV capabilities in the network. The problem can be formulated as a mixed integer linear programming problem, and heuristics can be used to solve the problem incrementally.As a case study, we examine the growing communication needs of 'Internet-of-Things' (IoT). With 'smart' sensing devices becoming ubiquitous, there is a need to support IoT communication at large scale, especially over cellular networks. The use of NFV platforms for the Evolved Packet Core, we see opportunities for supporting IoT communications at large scale in 5G cellular networks. We describe potential solutions in this direction.
[Conferences, integer programming, software defined networks, high-performance software-based network, low cost commodity servers, linear programming, parallel processing, SDN control plane, IoT, Internet of things, high bandwidth network functions, smart sensing devices, mixed integer linear programming problem, Communication networks, designed network function virtualization platform, high-performance NFV platforms, flexible network, SDNFV network architecture, Internet-of-Things, zero-copy delivery, 5G mobile communication, communication networks, software defined networking, 5G cellular networks, Routing, Intel DPDK libraries, Internet of Things, dynamic network, Computer science, high-speed inter-VM communication, trust boundary, OpenNetVM NFV platform, low latency networking, Software, intelligent sensors, centralized SDN controller, cellular radio]
INSPECTOR: Data Provenance Using Intel Processor Trace (PT)
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Data provenance strives for explaining how the computation was performed by recording a trace of the execution. The provenance trace is useful across a wide-range of workflows to improve the dependability, security, and efficiency of software systems. In this paper, we present Inspector, a POSIX-compliant data provenance library for shared-memory multithreaded programs. The Inspector library is completely transparent and easy to use: it can be used as a replacement for the pthreads library by a simple exchange of libraries linked, without even recompiling the application code. To achieve this result, we present a parallel provenance algorithm that records control, data, and schedule dependencies using a Concurrent Provenance Graph (CPG). We implemented our algorithm to operate at the compiled binary code level by leveraging a combination of OS-specific mechanisms, and recently released Intel PT ISA extensions as part of the Broadwell micro-architecture. Our evaluation on a multicore platform using applications from multithreaded benchmarks suites (PARSEC and Phoenix) shows reasonable provenance overheads for a majority of applications. Lastly, we briefly describe three case-studies where the generic interface exported by Inspector is being used to improve the dependability, security, and efficiency of systems. The Inspector library is publicly available for further use in a wide range of other provenance workflows.
[multicore platform, Schedules, schedule dependencies, Instruction sets, software systems, parallel provenance algorithm, compiled binary code level, provenance workflows, shared memory multithreaded programs, Security, Operating systems, Phoenix, Libraries, OS-specific mechanisms, POSIX-compliant data provenance library, multiprocessing systems, concurrent provenance graph, multithreaded benchmarks suites, Debugging multi-threaded programs, microprocessor chips, Inspector library, Intel processor trace, Synchronization, data provenance trace, PARSEC, Parallel programming, Memory management, Data models, Broadwell microarchitecture, CPG, Intel Processor Trace (PT), pthreads library, Intel PT ISA extensions, Data provenance]
PAG: Private and Accountable Gossip
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
A large variety of content sharing applications rely, at least partially, on gossip-based dissemination protocols. However, these protocols are subject to various types of faults, among which selfish behaviours performed by nodes that benefit from the system without contributing their fair share to it. Accountability mechanisms (e.g., PeerReview, AVMs, FullReview), which require that nodes log their interactions with others and periodically inspect each others' logs are effective solutions to deter faults. However, these solutions require that nodes disclose the content of their logs, which may leak sensitive information about them. Building on a monitoring infrastructure and on homomorphic cryptographic procedures, we propose in this paper PAG, the first accountable and partially privacy-preserving gossip protocol. We assess PAG theoretically using the ProVerif cryptographic protocol verifier and evaluate it experimentally using both a real deployment on a cluster of 48 machines and simulations. The performance evaluation of PAG, performed using a video live streaming application, shows that it is compatible with the visualisation of live video content on commodity Internet connections. Furthermore, PAG's bandwidth consumption inherits the desirable scalability properties of gossip when the number of users in the system grows.
[Protocols, cryptographic protocols, homomorphic cryptographic procedures, ProVerif cryptographic protocol verifier, partially privacy-preserving gossip protocol, FullReview, Privacy, Bandwidth, video streaming, Cryptography, Monitoring, bandwidth consumption, peer-to-peer computing, video live streaming application, AVMs, commodity Internet connections, accountability mechanisms, PAG, Streaming media, gossip-based dissemination protocols, data privacy, Peer-to-peer computing, Internet, live video content visualisation, content sharing applications, PeerReview, private and accountable gossip]
Practical Intrusion-Tolerant Networks
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
As the Internet becomes an important part of the infrastructure our society depends on, it is crucial to construct networks that are able to work even when part of the network is compromised. This paper presents the first practical intrusion-tolerant network service, targeting high-value applications such as monitoring and control of global clouds and management of critical infrastructure for the power grid. We use an overlay approach to leverage the existing IP infrastructure while providing the required resiliency and timeliness. Our solution overcomes malicious attacks and compromises in both the underlying network infrastructure and in the overlay itself. We deploy and evaluate the intrusion-tolerant overlay implementation on a global cloud spanning East Asia, North America, and Europe, and make it publicly available.
[network infrastructure, intrusion-tolerant overlay implementation, malicious attacks, critical infrastructure, Europe, Resilient Networks, Routing, East Asia, power grid, global cloud, Intrusion Tolerance, North America, computer network security, Overlay networks, Semantics, overlay networks, Overlay Networks, intrusion-tolerant network service, high-value applications, Internet, IP infrastructure, IP networks, Reliability, Monitoring]
Gremlin: Systematic Resilience Testing of Microservices
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Modern Internet applications are being disaggregated into a microservice-based architecture, with services being updated and deployed hundreds of times a day. The accelerated software life cycle and heterogeneity of language runtimes in a single application necessitates a new approach for testing the resiliency of these applications in production infrastructures. We present Gremlin, a framework for systematically testing the failure-handling capabilities of microservices. Gremlin is based on the observation that microservices are loosely coupled and thus rely on standard message-exchange patterns over the network. Gremlin allows the operator to easily design tests and executes them by manipulating inter-service messages at the network layer. We show how to use Gremlin to express common failure scenarios and how developers of an enterprise application were able to discover previously unknown bugs in their failure-handling code without modifying the application.
[Cloud computing, message-exchange patterns, microservice failure-handling capabilities, program testing, microservice-based architecture, Internet applications, systematic microservice resilience testing, interservice messages, production infrastructures, Power system faults, Standards, software fault tolerance, Microservices, network layer, Systematics, Runtime, Failure-recovery testing, language runtimes, failure-handling code, Power system protection, accelerated software life cycle, Internet, Gremlin, Testing]
CRONets: Cloud-Routed Overlay Networks
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Overlay networking and ISP-assisted tunneling are effective solutions to overcome problematic BGP routes and bypass troublesome autonomous systems. Despite their demonstrated effectiveness, overlay support is not broadly available. In this paper, we propose Cloud-Routed Overlay Networks (CRONets), whereby users can readily build their own overlays using nodes from global and well-provisioned cloud providers like IBM Softlayer or Amazon EC2. While previous studies have demonstrated the benefits of overlay networks with the high-speed experimental Internet2 backbone, we are the first to evaluate the improvements in a realistic -- cloud -- setting. We conduct a large-scale experiment where we observe 6,600 Internet paths. The results show that CRONets improve the throughput for 78% of the default Internet paths with a median and average improvement factors of 1.67 and 3.27 times respectively, at a tenth of the cost of leasing private lines of comparable performance. We also performed a longitudinal measurement, and demonstrate that the performance gains are consistent over time with only a small number of overlay nodes needed to be deployed. However, given the size and dynamic nature of the Internet routing system (e.g., due to congestion and failures), selecting the proper path is still a challenging problem. To address it, we propose a novel solution based on the newly-introduced MPTCP extensions. Our experiments show that MPTCP can achieve the maximum observed throughput across the different overlay paths.
[Cloud computing, Amazon EC2, median improvement factor, overlay network, Performance gain, Throughput, wide-area network, Servers, global well-provisioned cloud providers, Overlay networks, Internet paths, cloud-routed overlay networks, Split-TCP, average improvement factor, cloud computing, MPTCP, overlay nodes, Extraterrestrial measurements, ISP-assisted tunneling, computer network performance evaluation, Internet routing system, performance gains, MPTCP extensions, BGP routes, overlay networks, telecommunication network routing, longitudinal measurement, IBM Softlayer, CRONets, throughput improvement]
The Internet is for Porn: Measurement and Analysis of Online Adult Traffic
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Adult (or pornographic) websites attract a large number of visitors and account for a substantial fraction of the global Internet traffic. However, little is known about the makeup and characteristics of online adult traffic. In this paper, we present the first large-scale measurement study of online adult traffic using HTTP logs collected from a major commercial content delivery network. Our data set contains approximately 323 terabytes worth of traffic from 80 million users, and includes traffic from several dozen major adult websites and their users in four different continents. We analyze several characteristics of online adult traffic including content and traffic composition, device type composition, temporal dynamics, content popularity, content injection, and user engagement. Our analysis reveals several unique characteristics of online adult traffic. We also analyze implications of our findings on adult content delivery. Our findings suggest several content delivery and cache performance optimizations for adult traffic, e.g., modifications to website design, content delivery, cache placement strategies, and cache storage configurations.
[Industries, content injection, content delivery, cache storage, Internet traffic, Servers, temporal dynamics, content popularity, content delivery network, Content delivery, Web site design, social aspects of automation, HTTP logs, device type composition, Social network services, cache storage configurations, online adult traffic, traffic composition, Computer science, Content distribution networks, Aggregates, cache placement strategies, Internet, Web sites, pornographic Web sites, Adult websites, user engagement, Porn, adult Web sites]
Tuning the Aggressive TCP Behavior for Highly Concurrent HTTP Connections in Data Center
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Modern data centers host diverse HTTP-based services, which employ persistent TCP connections to send HTTP requests and responses. However, the ON/OFF pattern of HTTP traffic disturbs the increase of TCP congestion window, potentially triggering packet loss at the beginning of ON period. Furthermore, the transmission performance becomes worse due to severe congestion in the concurrent transfer of HTTP response. In this work, we first reveal that the TCP's aggressive behavior in increasing congestion window causes TCP timeouts and throughput collapse. We further present the design and implementation of TCP-TRIM, which employs probe packets to smooth the aggressive increase of congestion window in persistent TCP connection, and leverages congestion detection and control at end-host to limit the growth of switch queue length under highly concurrent TCP connections. The experimental results of at-scale simulations and real implementations show that TCPTRIM reduces the completion time of HTTP response by up to 80%, while introducing little deployment overhead only at the end hosts.
[HTTP responses, TCP, transport control protocol, Protocols, HTTP requests, hypertext transfer protocol, Packet loss, computer networks, Switches, congestion control, Throughput, Web servers, HTTP, computer centres, HTTP connection, switch queue length, data center, transport protocols, congestion detection, HTTP traffic pattern, congestion window, aggressive TCP behavior]
A Performance Analysis of Incentive Mechanisms for Cooperative Computing
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
As more devices gain Internet connectivity, more information needs to be exchanged between them. For instance, cloud servers might disseminate instructions to clients, or sensors in the Internet of Things might send measurements to each other. In such scenarios, information spreads faster when users have an incentive to contribute data to others. While many works have considered this problem in peer-to-peer scenarios, none have rigorously theorized the performance of different design choices for the incentive mechanisms. In particular, different designs have different ways of "bootstrapping" new users (distributing information to them) and preventing "free-riding" (receiving information without uploading any in return). We classify incentive mechanisms in terms of reciprocity-, altruism-, and reputation-based algorithms, and then analyze the performance of these three basic and three hybrid algorithms. We show that the algorithms lie along a tradeoff between fairness and efficiency, with altruism and reciprocity at the two extremes. The three hybrids all leverage their component algorithms to achieve similar efficiency. The reputation hybrids are the most fair and can nearly match altruism's bootstrapping speed, but only the reciprocity/reputation hybrid can match reciprocity's zero-tolerance for free-riding. It therefore yields better fairness and efficiency when free-riders are present. We validate these comparisons with extensive experimental results.
[Algorithm design and analysis, Measurement, incentive mechanisms, instruction dissemination, Internet connectivity, Classification algorithms, peer-to-peer scenarios, cooperative computing, altruism-based algorithms, free-riding prevention, Bandwidth, Prediction algorithms, cloud servers, cooperative systems, reputation-based algorithms, cloud computing, peer-to-peer computing, Radiation detectors, information dissemination, performance evaluation, incentive schemes, Internet of Things, bootstrapping, computer bootstrapping, reciprocity-based algorithms, Peer-to-peer computing, performance analysis]
GrapH: Heterogeneity-Aware Graph Computation with Adaptive Partitioning
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Vertex-centric graph processing systems such as Pregel, PowerGraph, or GraphX recently gained popularity due to their superior performance of data analytics on graph-structured data. These systems exploit the graph structure to improve data access locality during computation, making use of specialized graph partitioning algorithms. Recent partitioning techniques assume a uniform and constant amount of data exchanged between graph vertices (i.e., uniform vertex traffic) and homogeneous underlying network costs. However, in real-world scenarios vertex traffic and network costs are heterogeneous. This leads to suboptimal partitioning decisions and inefficient graph processing. To this end, we designed GrapH, the first graph processing system using vertex-cut graph partitioning that considers both, diverse vertex traffic and heterogeneous network, to minimize overall communication costs. The main idea is to avoid frequent communication over expensive network links using an adaptive edge migration strategy. Our evaluations show an improvement of 60% in communication costs compared to state-of-the-art partitioning approaches.
[adaptive partitioning, Heuristic algorithms, vertex-cut graph partitioning, heterogeneity-aware graph computation, data access locality, Pregel, GrapH, heterogeneous network, Distributed databases, graph vertices, diverse vertex traffic, Mirrors, cloud computing, adaptive edge migration strategy, Data analysis, data analysis, data analytics, suboptimal partitioning decisions, Computational modeling, Partitioning algorithms, vertex-centric graph processing, PowerGraph, specialized graph partitioning algorithms, graph-structured data, Automata, GraphX, expensive network links]
Minimum-Cost Cloud Storage Service Across Multiple Cloud Providers
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Many Cloud Service Providers (CSPs) provide data storage services with datacenters distributed worldwide. These datacenters provide different Get/Put latencies and unit prices for resource utilization and reservation. Thus, when selecting different CSPs' datacenters, cloud customers of globally distributed applications (e.g., online social networks) face two challenges: (i) how to allocate data to worldwide datacenters to satisfy application SLO (service level objective) requirements including both data retrieval latency and availability, and (ii) how to allocate data and reserve resources in datacenters belonging to different CSPs to minimize the payment cost. To handle these challenges, we first model the cost minimization problem under SLO constraints using integer programming. Due to its NP-hardness, we then introduce our heuristic solution, including a dominant-cost based data allocation algorithm and an optimal resource reservation algorithm. We finally introduce an infrastructure to enable the conduction of the algorithms. Our trace-driven experiments on a supercomputing cluster and on real clouds (i.e., Amazon S3, Windows Azure Storage and Google Cloud Storage) show the effectiveness of our algorithms for SLO guaranteed services and customer cost minimization.
[Cloud computing, Cost minimization, integer programming, Memory, CSP data centers, Amazon S3, customer cost minimization, online social networks, service level objective, dominant-cost based data allocation algorithm, optimal resource reservation algorithm, SLO guaranteed services, resource allocation, cloud customers, Distributed databases, Clustering algorithms, data storage services, globally distributed applications, data availability, SLO, cloud computing, resource utilization, data retrieval latency, heuristic solution, get/put latencies, Minimization, Linear programming, unit prices, computer centres, payment cost minimization, Windows Azure Storage, NP-hardness, supercomputing cluster, SLO constraints, multiple cloud service providers, Cloud storage, social networking (online), Google Cloud Storage, minimum-cost cloud storage service, Data availability, Resource management, minimisation]
A Realistic and Optimized V2V Communication System for Taxicabs
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Due to high mobility and intermittent connections in vehicular networks, reliable and efficient vehicular communication is a challenging task. Previous research on Vehicle-to-Vehicle (V2V) communication mostly focuses on achieving reliable transmissions from a given source to a given destination by mining moving patterns of taxicabs. However, to the best of our knowledge, none of them considered the habit-driven regularities of individual taxicabs as well as the urban-layout-driven time-varying regularities of crowds of taxicabs synthetically. With this insight, we model both individual and holistic driving patterns by Markov Chain models, then devise a new method to predict possible driving routes for every single taxicab. In addition, we design a new method to evaluate the probability that a single taxicab retrieves information of a specific road segment while it drives through another road segment during a given time period, and also to quantify the expected probability that a single taxicab obtains the information of a given road segment in the near future. With such information, our solution enables the selection of the optimal data packet transmission scheme. We evaluate our solution on a real-world taxicab dataset. Experimental results demonstrate that our approach outperforms alternative solutions in terms of diffusion speed and success ratio of data retrieval.
[V2V communication system, data retrieval, taxicab, Roads, Data diffusion, Predictive models, Vehicles, vehicle-to-vehicle communication system, urbanlayout-driven time-varying regularities, vehicular network, Data retrieval, Vehicle-to-Vehicle Communication, Trajectory, Sensors, diffusion speed, intermittent connection, optimal data packet transmission scheme, mobile radio, Urban areas, probability, holistic driving pattern, Vehicular ad hoc networks, habit-driven regularities, Taxicabs, Markov processes, Markov Chain model]
Optimal Marching of Autonomous Networked Robots
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
The recent advances in sensors, actuators, robots, and mobile wireless communication technologies have acceleratedinterest in autonomous networked robots (ANRs), where theindividual robots coordinate among themselves to complete atask, e.g., to explore or monitor a Field of Interest (FoI). Byteamwork, which is especially important in complex tasks, ANRsystem expresses much more capacity than traditional staticsensor networks. Existing work focuses on improving the coverageperformance of a group of ANRs within a single FoI. In thisresearch, we consider a group of ANRs that are instructed toexplore a number of FoIs. After they complete a task at currentFoI, they move to the next one, which may be far away fromcurrent one and the shape can also vary dramatically. Ourresearch focuses on how to efficiently enable such transition. TheANRs must be able to redeploy themselves to desired positionsin the new FoI based on distributed algorithms. Besides, to avoidunexpected event breaks network's integrity, the ANRs shouldpreserve their local connectivities as much as they can andorganize themselves as a whole network without any isolatednodes during the transition. Furthermore, considering energyconsumption, such relocation algorithm should work at the costof reasonable total moving distance. We study this problemand call it optimal marching of autonomous networked robots. The proposed algorithms guarantee global connectivity, andpreserve local connectivities as much as possible at negligiblecost of moving distance. Additionally, ANRs can automaticallyadjust their deployment density in the new FoI based on therequirements of various tasks or regions.
[mobile radio, network integrity, optimal autonomous networked robot marching, multi-robot systems, ANR system, Sensor systems, mobile robots, path planning, Mobile robots, mobile wireless communication, deployment density, static sensor networks, actuators, Robot kinematics, distributed algorithms, sensor placement, local connectivity, Robot sensing systems, Bipartite graph, global connectivity, energy consumption]
RichNote: Adaptive Selection and Delivery of Rich Media Notifications to Mobile Users
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
In recent years, notification services for social networks, mobile apps, messaging systems and other electronic services have become truly ubiquitous. When a new content becomes available, the service sends an instant notification to the user. When the content is produced in massive quantities, and it includes both large-size media and a lot of meta-information, it gives rise to a major challenge of selecting content to notify about and information to include in such notifications. We tackle three important challenges in realizing rich notification delivery: (1) content and presentation utility modeling, (2) notification selection and (3) scheduling of delivery. We consider a number of progressive presentation levels for the content. Since utility is subjective and hard to model, we rely on real data and user surveys. We model the content utility by learning from large-scale real world data collected from Spotify music streaming service. For the utility of the presentation levels we rely on user surveys. Blending these two techniques together, we derive utility of notifications with different presentation levels. We then model the selection and delivery of rich notifications as an optimization problem with a goal to maximize the utility of notifications under resource budget constraints. We validate our system with large-scale simulations driven by the real-world de-identified traces obtained from Spotify. With the help of several baseline approaches we show that our solution is adaptive and resource efficient.
[mobile apps, large-scale simulations, adaptive selection, Metadata, Mobile communication, large-scale real world data, mobile computing, optimisation, Spotify music streaming service, music, resource allocation, RichNote, Bandwidth, scheduling, adaptive delivery, Real-time systems, mobile users, Rich media notifications, publish/subscribe, presentation utility modeling, content utility, messaging systems, resource budget constraints, social networks, large-size media, Media, rich media notifications, Streaming media, notification utility, social networking (online), Feeds, notification services, electronic services]
Decentralized Context Sharing in Vehicular Delay Tolerant Networks with Compressive Sensing
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Vehicles equipped with various types of sensors can act as mobile sensors to monitor the road conditions. To speed up the information collection process, the monitoring data can be shared among vehicles upon their encounters to facilitate drivers to find a good route. The vehicular network experiences intermittent connectivity as a result of the mobility, which makes the inter-vehicle contact duration a scarce resource for data transmissions and the support of monitoring applications over vehicular networks a challenge. We propose a novel compressive sensing (CS)-based scheme to enable efficient decentralized context sharing in vehicular delay tolerant networks, called CS-Sharing. To greatly reduce the data transmission overhead and speed up the monitoring processing, CS-sharing exploits two techniques: sending an aggregate message in each vehicle encounter, and quick collection of information taking advantage of data sharing and the sparsity of events in vehicle networks to significantly reduce the number of measurements needed for global information recovery. We propose a novel data structure, and an aggregation method that can take advantage of the random and opportunistic vehicle encounters to form the measurement matrix. We prove that the measurement matrix satisfies the Restricted Isometry Property (RIP) property required by the CS technique. Our results from extensive simulations demonstrate that CS-Sharing allows vehicles in a large network to quickly obtain the full context data with the successful recovery ratio larger than 90%.
[compressed sensing, mobile sensors, Roads, measurement matrix, CS technique, Vehicular Delay Tolerant Network, Vehicles, restricted isometry property, vehicular network, data transmissions, intervehicle contact duration, Sensors, Monitoring, Context, vehicular ad hoc networks, decentralized context sharing, monitoring processing, Compressive Sensing, Context Sharing, Wireless sensor networks, RIP property, delays, information collection process, vehicular delay tolerant networks, road conditions, Compressed sensing, compressive sensing, novel data structure]
RuleTris: Minimizing Rule Update Latency for TCAM-Based SDN Switches
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Software-dehned network (SDN) is deemed to enable more dynamic management of data center networks that promptly respond to network events with changes in network policies. Although the SDN controller architecture is increasingly optimized for swift policy updates, the data plane, especially the prevailing TCAM-based flow tables on physical SDN switches, remains unoptimized for fast rule updates, and is gradually becoming the primary bottleneck along the policy update pipeline. In this paper, we present RuleTris, the hrst SDN update optimization framework that minimizes rule update latency for TCAM-based switches. RuleTris employs the dependency graph (DAG) as the key abstraction to minimize the update latency. RuleTris efhciently obtains the DAGs with novel dependency preserving algorithms that incrementally build rule dependency along with the compilation process. Then, in the guidance of the DAG, RuleTris optimizes the rule updates in TCAM to avoid unnecessary entry moves, which are the main cause of TCAM update inefhciency. We prove that RuleTris generates TCAM updates with the minimum number of TCAM entry moves. In evaluation, RuleTris achieves a median of &lt;;12ms and 90-percentile of &lt;;15ms the end-to-end per-rule update latency on our hardware prototype, outperforming the state-of-the-art composition compiler CoVisor by ~20 times.
[Schedules, Pipelines, graph theory, Control systems, dependency graph, TCAM update, software-defined network, program compilers, Optimization, CoVisor, policy update, Hardware, data center networks, network policies, Software-defined networks, Process control, composition compiler, software defined networking, DAG, SDN controller architecture, TCAM-based flow tables, compilation process, hardware prototype, RuleTris, SDN update optimization framework, Delays, TCAM-based SDN switches, minimisation, content-addressable storage]
RITM: Revocation in the Middle
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Although TLS is used on a daily basis by many critical applications, the public-key infrastructure that it relies on still lacks an adequate revocation mechanism. An ideal revocation mechanism should be inexpensive, efficient, secure, and privacypreserving. Moreover, rising trends in pervasive encryption pose new scalability challenges that a modern revocation system should address. In this paper, we investigate how network nodes can deliver certificate-validity information to clients. We present RITM, a framework in which middleboxes (as opposed to clients, servers, or certification authorities) store revocation-related data. RITM provides a secure revocation-checking mechanism that preserves user privacy. We also propose to take advantage of content-delivery networks (CDNs) and argue that they would constitute a fast and cost-effective way to disseminate revocations. Additionally, RITM keeps certification authorities accountable for the revocations that they have issued, and it minimizes overhead at clients and servers, as they have to neither store nor download any messages. We also describe feasible deployment models and present an evaluation of RITM to demonstrate its feasibility and benefits in a real-world deployment.
[middleboxes, client-server systems, Scalability, revocation-related data, Middleboxes, Browsers, certificate revocation, Servers, certificate-validity information, content-delivery networks, SSL, network nodes, PKI, Privacy, RITM, certification authorities, Market research, Software, data privacy, revocation dissemination, TLS, user privacy preservation, revocation-checking mechanism, public-key infrastructure]
A Distributed Auctioneer for Resource Allocation in Decentralized Systems
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
In decentralized systems, nodes often need to coordinate to access shared resources in a fair manner. One approach to perform such arbitration is to rely on auction mechanisms. Although there is an extensive literature that studies auctions, most of these works assume the existence of a central, trusted auctioneer. Unfortunately, in fully decentralized systems, where the nodes that need to cooperate operate under separate spheres of control, such central trusted entity may not exist. Notable examples of such decentralized systems include community networks, clouds of clouds, cooperative nano data centres, among others. In this paper, we make theoretical and practical contributions to distribute the role of the auctioneer. From the theoretical perspective, we propose a framework of distributed simulations of the auctioneer that are Nash equilibria resilient to coalitions and asynchrony. From the practical perspective, our protocols leverage the distributed nature of the simulations to parallelise the execution. We have implemented a prototype that instantiates the framework for bandwidth allocation in community networks, and evaluated it in a real distributed setting.
[distributed auctioneer, Cloud computing, auction mechanism, Protocols, bandwidth allocation framework, Computational modeling, decentralized systems, game theory, distributed processing, Nash equilibrium, distributed simulation, Analytical models, resource allocation, Bandwidth, Games, distributed systems, Resource management, auctions]
Datacomp: Locally Independent Adaptive Compression for Real-World Systems
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Non-lossy compression can save time and energy during communication if the cost to compress and send input is less than the cost of sending it uncompressed. Unfortunately, compression can also degrade performance, no single method is always beneficial, and outcomes depend on many factors. As a result, compression choices in real systems are coarsely grained and manually controlled, resulting in suboptimal or even poor performance. Adaptive Compression (AC) systems make compression choices dynamically to optimize utility. Existing AC systems are limited in ways that reduce their suitability for general-purpose computers. Datacomp is an AC system that operates locally and includes no significant hard-coded knowledge. Using real-world data, a broad range of environments and the Comptool "AC Oracle," we show that Datacomp's performance is equivalent or close to the ideal at bandwidths between 1-100Mbit/s, even when static strategies are suboptimal or more costly than no compression. While Datacomp struggles to perform well at 1Gbit/s, understanding why illustrates important challenges for AC systems and suggests solutions.
[Computers, AC systems, data compression, hard-coded knowledge, Adaptive systems, efficiency, Computational modeling, Comptool, real-world systems, AC Oracle, static strategies, general-purpose computers, Throughput, general purpose computers, optimization, locally independent adaptive compression system, Bandwidth, Data models, compression, Datacomp, Monitoring, nonlossy compression, energy]
Hybrid Content-Based Routing Using Network and Application Layer Filtering
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Over the past few decades, content-based publish/subscribe has been primarily implemented as an overlay network of software brokers. Even though such systems provide the possibility of bandwidth efficient expressive filtering in software, they cannot match up to the performance (in terms of end-to-end latency and throughput) of communication protocols implemented on the network layer. To exploit network layer performance benefits, recently, content-based publish/subscribe was realized using the capabilities of Software-defined Networking (SDN). While SDN allows line-rate forwarding of events by content filters directly installed on switches, it suffers from inherent hardware limitations (w.r.t. flow table size, limited availability of bits in header fields) that adversely impact expressiveness of these filters, resulting in unnecessary network traffic. In this paper, we strike a balance between purely application-layer-based and purely network-layer-based publish/subscribe implementations by realizing the first hybrid content-based middleware that enables filtering of events in both layers. Moreover, we provide different selection algorithms with varying degrees of complexity to determine the events to be filtered at each layer such that unnecessary network traffic can be minimized while also considering delay requirements of the middleware. Our hybrid middleware offers full flexibility to configure it according to the performance requirements of the system. We provide a detailed performance evaluation of the proposed selection algorithms to determine their impact on the performance of the designed hybrid middleware which we further compare to state-of-the art solutions.
[software-defined networking, content filters, Throughput, hybrid content-based middleware, Software-defined Networking, Bandwidth, Hardware, Hybrid Routing, SDN, middleware, network layer filtering, network layer performance evaluation, Network Virtualization, communication protocols, software defined networking, Routing, Publish/Subscribe, Middleware, computer network performance evaluation, application layer filtering, hybrid content-based routing, network traffic, software brokers, telecommunication network routing, Content-based Routing, telecommunication traffic, Indexing]
Exploiting Causality to Engineer Elastic Distributed Software
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
This goal of this paper is to anchor elasticity in terms of causality in distributed applications. Assuming a large-scale distributed application architected as a set of interacting components, we motivate the need for (1) analyzing causality between interactions, and (2) estimating casual probability that an increase in the frequency of interaction i<sub>1</sub> can increase the frequency of interaction i<sub>2</sub> caused by i<sub>1</sub>. We present algorithms to estimate causality and causal probability by combining well known sampling, program analysis, path profiling and dynamic slicing algorithms. We apply our algorithms for causal probability to three large-scale distributed applications, to evaluate (a) their effectiveness in the timely provisioning and de-provisioning of compute resources and (b) whether causality and causal probability present a fundamental and widely-applicable way of engineering auto-elasticity.
[Cloud computing, Heuristic algorithms, elasticity, Elasticity, distributed processing, compute resource de-provisioning, auto-elasticity engineering, distributed applications, Runtime, distributed path profiling, resource allocation, program analysis, interaction frequency, software engineering, probability, causal probability, elastic distributed software engineering, compute resource provisioning, casual probability, causality, causality analysis, sampling algorithm, causality estimation, dynamic slicing algorithm, path profiling, Clocks]
RUSH: A RobUst ScHeduler to Manage Uncertain Completion-Times in Shared Clouds
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
We address the problem of scheduling jobs with utilities that depend solely upon their completion-times in a shared cloud that imposes considerable uncertainty on the jobs' runtime. However, it is very hard to estimate the jobs' runtime in a shared cloud where jobs are often delayed due to reasons such as slow I/O performance and variations in memory availability. Unlike prior works, we acknowledge that runtime estimates are often erroneous and instead shift the burden of robustness to the job scheduler. Specifically, we present a scheduling problem that jointly accounts for: (i) job utilities specified as functions of their completion-time, and (ii) uncertainty in the jobs' runtime. Our proposed solution to this problem achieves lexicographic max-min fairness among the job utilities. We implement this as a robust scheduler, named RUSH, for YARN in Hadoop. Our experiments, using real-world data sets, illustrate RUSH's efficacy when compared with other commonly used schedulers.
[Uncertainty, Runtime Estimation, job runtime uncertainty, robust scheduler, Estimation, Hadoop, Containers, minimax techniques, shared clouds, Yarn, YARN, Optimization, job utilities, lexicographic max-min fairness, Robust Scheduling, Runtime, shared cloud, uncertain job completion-time management, scheduling, Robustness, cloud computing, RUSH, job scheduling, real-world data sets]
The Same Speed Timer in Population Protocols
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
A novel concept of the same speed timer is presented, and is applied in the population protocol (PP) model to improve the convergence time of existing loosely-stabilizing leader election protocols. Loosely-stabilizing leader election guarantees that, starting from any configuration, the system reaches a safe configuration within a short time (convergence), and after that, the system keeps the unique leader for a long time (closure). Two loosely-stabilizing leader election protocols for arbitrary graphs exist in the literature; one uses identifiers of nodes and the other uses random numbers to elect a unique leader. Both protocols guarantee that the expected convergence time is polynomial and the expected holding time (the time the leader is kept) is exponential. In this paper, convergence time of these protocols is dramatically improved by the same speed timer without impairing the exponential holding time. Specifically, a fast deterministic loosely-stabilizing leader election protocol that uses identifiers of nodes and a fast randomized looselystabilizing leader election protocol are given. The expected convergence time and expected holding time of the former protocol are O(mN log N) and &#x03A9;(Ne2N), respectively, where m is the number of edges in the graph and N is a given upper bound on the number of nodes n. The expected convergence time and expected holding time of the latter protocol are O(mN2 log n) and &#x03A9;(Ne2N), respectively. A self-stabilizing two-hop coloring protocol that uses only O(log n) memory space of each agent is given as a tool of the latter protocol. A lower bound is also given: any loosely-stabilizing leader election protocol with expected exponential holding time requires &#x03A9;(mN) expected convergence time.
[Protocols, fast deterministic loosely-stabilizing leader election protocol, graph theory, convergence time improvement, Electronic mail, Complexity theory, Convergence, graph edges, Loose stabilization, Sociology, Leader election, PP model, random numbers, protocols, same speed timer, expected exponential holding time, Nominations and elections, upper bound, node identifiers, Statistics, expected polynomial convergence time, self-stabilizing two-hop coloring protocol, population protocol model, Population protocols, computational complexity, number theory]
Interactive Wireless Charging for Energy Balance
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Wireless energy transfer is an emerging technology that is used in networks of battery-powered devices in order to deliver energy and keep the network functional. Existing state-of-the-art studies have mainly focused on applying this technology on networks of relatively strong computational and communicational capabilities (wireless sensor networks, ad-hoc networks), also they assume one-directional energy transfer from special chargers to the network nodes. Different from these works, we here study (for the first time in the state-of-theart) interactive, "peer-to-peer" wireless charging in populations of much more resource-limited, mobile agents that abstract distributed portable devices. In this new model for interactive wireless charging, we assume that the agents are capable of achieving bi-directional wireless energy transfer acting both as energy transmitters and harvesters. We consider the cases of both loss-less and lossy energy transfer and provide an upper bound on the time needed to reach a balanced energy distribution in the population. We investigate the delicate impact of the diversity of energy levels on eventual energy balance achieved and highlight some key elements of the charging procedure. In the light of the above, we design and evaluate three interaction protocols that achieve different tradeoffs between energy balance, time and energy efficiency.
[Protocols, communicational capabilities, Energy exchange, interaction protocols, computational capabilities, energy harvesting, one-directional energy transfer, Ad hoc networks, energy balance, radiofrequency power transmission, energy harvesters, Statistics, interactive wireless charging, lossy energy transfer, Wireless communication, Wireless sensor networks, wireless energy transfer, Sociology, energy efficiency, energy distribution, battery-powered devices, energy transmitters, bidirectional wireless energy transfer]
Nearly Optimal Distributed Algorithm for Computing Betweenness Centrality
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
In this paper, we propose an O(N) time distributed algorithm for computing betweenness centralities of all nodes in the network where N is the number of nodes. Our distributed algorithm is designed under the widely employed CONGEST model in the distributed computing community which limits each message only contains O(log N) bits. To our best knowledge, this is the first linear time deterministic distributed algorithm for computing the betweenness centralities in the published literature. We also give a lower bound for distributively computing the betweenness centrality under the CONGEST model as &#x03A9;(D+N/ log N) where D is the diameter of the network. This implies that our distributed algorithm is nearly optimal.
[Algorithm design and analysis, &#x03A9;(D+N/ log N), Congest Model, Computational modeling, optimal distributed computing community, communication complexity, Distributed computing, deterministic algorithms, Stress, O(N) time distributed algorithm, distributed algorithm, linear time deterministic distributed algorithm, distributed algorithms, O(log N) bits, Approximation algorithms, Distributed algorithms, Time complexity, betweenness centrality, CONGEST model, computational complexity]
An Efficient Lock-Free Logarithmic Search Data Structure Based on Multi-dimensional List
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Logarithmic search data structures, such as search trees and skiplists, are fundamental building blocks of many applications. Although the self-balancing binary search trees are among the most ubiquitous sequential search data structures, designing non-blocking rebalancing algorithms is challenging due to the required structural alternation, which may stall other concurrent operations. Skiplists, which probabilistically create multiple levels of shortcuts in an ordered list, provide practical alternatives to balanced search trees. The use of skiplists eliminates the need of rebalancing and ensures amortized logarithmic sequential search time, but concurrency is limited under write-dominated workload because the linkage between multiple distant nodes must be updated. In this paper, we present a linearizable lock-free dictionary design based on a multi-dimensional list (MDList). A node in an MDList arranges its child nodes by their dimensionality and order them by coordinate prefixes. The search operation works by first generating a one-to-one mapping from the scalar keys to a high-dimensional vectors space, then uniquely locating the target position by using the vector as coordinates. Our algorithm guarantees worst-case search time of O(log N) where N is the size of key space. Moreover, the ordering property of the data structure is readily maintained during mutations without rebalancing nor randomization. In our experimental evaluation using a micro-benchmark, our dictionary outperforms the state of the art approaches by as much as 100% when the key universe is large and an average of 30% across all scenarios.
[Algorithm design and analysis, Dictionary, Dictionaries, structural alternation, MDList, Concurrent Data Structure, Lock-free, nonblocking rebalancing algorithms, Multi-dimensional List, lock-free logarithmic search data structure, data structures, high-dimensional vector space, linearizable lock-free dictionary design, worst-case search time, one-to-one mapping, Binary search trees, Skiplists, Search Tree, tree searching, logarithmic sequential search time, self-balancing binary search trees, search operation, multidimensional list, Layout, Vegetation, Skiplist, Arrays, ubiquitous sequential search data structures, computational complexity]
Characterizing the Computational Power of Anonymous Mobile Robots
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
The distributed setting of computational mobile entities, called robots, thathave to perform tasks without global coordination has been extensively studied in the literature. A well-known scenario is that in which robots operate in Look-Compute-Move (LCM) cycles. During each cycle, a robot acquires asnapshot of the surrounding environment (Look phase), then executes an appropriate algorithm by using the obtained snapshot as input (Computephase), and finally moves toward a desired destination, if any (Movephase). Look-Compute-Move cycles might be subject to different temporal constraints dictated by the considered schedule. The classic models for theactivation and synchronization of mobile robots are the well-known fully-synchronous, semi-synchronous, and asynchronous models. A first comprehensive evaluation of the computational power of robots operating in the LCM model and moving within the Euclidean plane, under different levels of synchronization, has been proposed in [Das et al., Int.'l Conf. on Distributed Computing Systems, 2012]. In detail, the authors provide a series of results that prove relations between classic models and variations of them, which consider the possibility that robots are endowed with a visible light, i.e. they are luminous, or with the capability to store some past snapshots, or combinations of them. In this paper, we are interested in similar settings but for robots moving on graphs. In particular, we propose a characterization of the computational power of mobile robots on graphs as follows. First, we show the relations among the three classic activation and synchronization models. Second, we compare the models where robots are endowed with lights against the models without lights. Third, we highlight the relations among the different models concerning luminous robots. Finally, we provide a detailed comparison of the proposed results with the case of robots moving in the Euclidean plane.
[Movephase, asynchronous models, Computational modeling, Color, Euclidean plane, Computational Power, Computephase, mobile robots, Synchronization, Mobile robots, Look-Compute-Move cycles, Distributed Algorithms, LCM model, Robot kinematics, Luminous Robots, LCM cycles, Mobile Robots, computational mobile entities, Robot sensing systems, computational power, Graphs, luminous robots, anonymous mobile robots, synchronization models]
CryptoLock (and Drop It): Stopping Ransomware Attacks on User Data
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Ransomware is a growing threat that encrypts auser's files and holds the decryption key until a ransom ispaid by the victim. This type of malware is responsible fortens of millions of dollars in extortion annually. Worse still, developing new variants is trivial, facilitating the evasion of manyantivirus and intrusion detection systems. In this work, we presentCryptoDrop, an early-warning detection system that alerts a userduring suspicious file activity. Using a set of behavior indicators, CryptoDrop can halt a process that appears to be tampering witha large amount of the user's data. Furthermore, by combininga set of indicators common to ransomware, the system can beparameterized for rapid detection with low false positives. Ourexperimental analysis of CryptoDrop stops ransomware fromexecuting with a median loss of only 10 files (out of nearly5,100 available files). Our results show that careful analysis ofransomware behavior can produce an effective detection systemthat significantly mitigates the amount of victim data loss.
[CryptoLock, ransomware attacks, invasive software, intrusion detection systems, behavior indicators, user data, file decryption, malware, antivirus, cryptography, behavioral analysis, Entropy, intrusion detection, Encryption, user file encryption, early-warning detection system, ransomware behavior analysis, data protection, Malware, Arrays, Monitoring, ransomware]
Amnesia: A Bilateral Generative Password Manager
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
While numerous flaws have been recognized in using passwords as a method of authentication, passwords still remain the de-facto authentication standard in use today. Though password managers can ameliorate password fatigue, the vast majority of password managers require the user to choose and maintain a strong master password while offering little to no recourse in the event that the master password is compromised. The wide-application of cloud-based password managers congregate passwords in an encrypted database, which becomes an attractive target for attackers and also represents a single point of failure. In this paper, we propose Amnesia, a bilateral generative password manager that requires both the knowledge of the master password and the possession of the user's smartphone to generate website passwords for the user. Our generative password manager is not vulnerable to the password database leakage, since it generates the requested password on demand using both the master password and the secret information on the smartphone. An attacker wishing to steal the user's website passwords has to compromise both the user's smartphone and the master password. Amnesia also has strong recovery capability when either the master password is compromised or the smartphone is lost/stolen. By using an Amnesia server, a user can have the access to the password manager on multiple computers without installing any software on those computers. We implemented an Amnesia system prototype using Android and Cherrypy web framework and evaluated it in terms of security, usability, and overhead. A user study of 31 testers shows that Amnesia increases password security while maintaining reasonable user convenience.
[Computers, generative, mobile computing, bilateral, Web site passwords, cloud-based password managers, user convenience, password manager, encrypted database, cryptography, smart phone, secret information, Web servers, smart phones, Mobile applications, Browsers, de-facto authentication standard, Amnesia server, Android, Amnesia system prototype, Authentication, Cherrypy Web framework, bilateral generative password managers, password security, ameliorate password fatigue, strong master password, password database leakage]
Know Your Phish: Novel Techniques for Detecting Phishing Sites and Their Targets
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Phishing is a major problem on the Web. Despite the significant attention it has received over the years, there has been no definitive solution. While the state-of-the-art solutions have reasonably good performance, they require a large amount of training data and are not adept at detecting phishing attacks against new targets. In this paper, we begin with two core observations: (a) although phishers try to make a phishing webpage look similar to its target, they do not have unlimited freedom in structuring the phishing webpage, and (b) a webpage can be characterized by a small set of key terms, how these key terms are used in different parts of a webpage is different in the case of legitimate and phishing webpages. Based on these observations, we develop a phishing detection system with several notable properties: it requires very little training data, scales well to much larger test data, is language-independent, fast, resilient to adaptive attacks and implemented entirely on client-side. In addition, we developed a target identification component that can identify the target website that a phishing webpage is attempting to mimic. The target detection component is faster than previously reported systems and can help minimize false positives in our phishing detection system.
[phishing Web page, target detection component, HTML, Browsers, Electronic mail, Uniform resource locators, Training, target identification component, phishing sites detection, Training data, computer crime, Feature extraction, phishing detection system, Internet, phishing attacks]
BotMeter: Charting DGA-Botnet Landscapes in Large Networks
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Recent years have witnessed a rampant use of domain generation algorithms (DGAs) in major botnet crimewares, which tremendously strengthens a botnet's capability to evade detection or takedown. Despite a plethora of existing studies on detecting DGA-generated domains in DNS traffic, remediating such threats still relies on vetting the DNS behavior of each individual device. Yet, in large networks featuring complicated DNS infrastructures, we often lack the capability or the resource to exhaustively investigate every part of the networks to identify infected devices in a timely manner. It is therefore of great interest to first assess the population distribution of DGA-bots inside the networks and to prioritize the remediation efforts. In this paper, we present BotMeter, a novel tool that accurately charts the DGA-bot population landscapes in large networks. Specifically, we embrace the prevalent yet challenging setting of hierarchical DNS infrastructures with caching and forwarding mechanisms enabled, whereas DNS traffic is observable only at certain upper-level vantage points. We establish a new taxonomy of DGAs that captures their characteristic DNS dynamics. This allows us to develop a rich library of rigorous analytical models to describe the complex relationships between bot populations and DNS lookups observed at vantage points. We provide results from extensive empirical studies using both synthetic data and real DNS traces to validate the efficacy of BotMeter.
[invasive software, domain name service, hierarchical DNS infrastructure, Botnet, Taxonomy, forwarding mechanism, Population estimation, DGA-bot population landscapes, Registers, Servers, caching mechanism, Statistics, DNS lookups, computer network security, Analytical models, DNS traffic, botnet crimewares, BotMeter tool, domain generation algorithms, Sociology, Libraries, DGA-botnet landscapes, DGA]
Enabling Privacy-Preserving Incentives for Mobile Crowd Sensing Systems
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Recent years have witnessed the proliferation of mobile crowd sensing (MCS) systems that leverage the public crowd equipped with various mobile devices (e.g., smartphones, smartglasses, smartwatches) for large scale sensing tasks. Because of the importance of incentivizing worker participation in such MCS systems, several auction-based incentive mechanisms have been proposed in past literature. However, these mechanisms fail to consider the preservation of workers' bid privacy. Therefore, different from prior work, we propose a differentially private incentive mechanism that preserves the privacy of each worker's bid against the other honest-but-curious workers. The motivation of this design comes from the concern that a worker's bid usually contains her private information that should not be disclosed. We design our incentive mechanism based on the single-minded reverse combinatorial auction. Specifically, we design a differentially private, approximately truthful, individual rational, and computationally efficient mechanism that approximately minimizes the platform's total payment with a guaranteed approximation ratio. The advantageous properties of the proposed mechanism are justified through not only rigorous theoretical analysis but also extensive simulations.
[mobile crowd sensing systems, differentially private incentive mechanism, combinatorial mathematics, incentive mechanism, MCS systems, Mobile communication, Mobile handsets, Sensor systems, Privacy, mobile computing, public crowd, large scale sensing, auction-based incentive mechanisms, mobile crowd sensing, honest-but-curious workers, tendering, privacy-preserving incentives, platform total payment minimization, incentive schemes, privacy-preserving, Computer science, Aggregates, single-minded reverse combinatorial auction, guaranteed approximation ratio, mobile devices, data privacy, minimisation, worker bid privacy]
Quality-Aware and Fine-Grained Incentive Mechanisms for Mobile Crowdsensing
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Limited research efforts have been made for Mobile CrowdSensing (MCS) to address quality of the recruited crowd, i.e., quality of services/data each individual mobile user and the whole crowd are potentially capable of providing, which is the main focus of the paper. Moreover, to improve flexibility and effectiveness, we consider fine-grained MCS, in which each sensing task is divided into multiple subtasks and a mobile user may make contributions to multiple subtasks. In this paper, we first introduce mathematical models for characterizing the quality of a recruited crowd for different sensing applications. Based on these models, we present a novel auction formulation for quality-aware and fine-grained MCS, which minimizes the expected expenditure subject to the quality requirement of each subtask. Then we discuss how to achieve the optimal expected expenditure, and present a practical incentive mechanism to solve the auction problem, which is shown to have the desirable properties of truthfulness, individual rationality and computational efficiency. We conducted trace-driven simulation using the mobility dataset of San Francisco taxies. Extensive simulation results show the proposed incentive mechanism achieves noticeable expenditure savings compared to two well-designed baseline methods, and moreover, it produces close-to-optimal solutions.
[quality-aware incentive mechanisms, mathematical models, Mobile communication, fine-grained MCS, auction formulation, San Francisco taxies, Smartphones, mobile computing, Sociology, Mobile Crowdsensing, mobile crowdsensing, Sensors, Mathematical model, close-to-optimal solutions, Computational modeling, Auction, auction problem, Statistics, Incentive Mechanism, trace-driven simulation, mobility dataset, mobile user, quality-aware MCS, expenditure savings, Quality of Crowd, fine-grained incentive mechanisms, Smart phones, quality of services]
Toward Optimal DoS-Resistant Authentication in Crowdsensing Networks via Evolutionary Game
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
With the increasing demand of Quality of Service(QoS) in Crowdsensing Networks, providing broadcast authentication and preventing Denial of Service (DoS) attacks become not only a fundamental issue but also a challenging security service. The multi-level TESLA is a series of lightweight broadcast authentication protocols, which can effectively mitigate DoS attacks via randomly selected messages. However, the rule of the parameter selection still remains a problem. In this paper, we formulate the attack-defense model as an evolutionary game accordingly, and then present an optimal solution, which achieves security assurance along with minimum resource cost. We then analyze the stability of our evolutionary strategy theoretically. Simulation results are given to evaluate the performance of the proposed algorithm under low QoS channels and severe DoS attacks, which demonstrates that our proposed protocol canworks even in the extreme case.
[optimal DoS-resistant authentication, Protocols, Computer crime, evolutionary game, crowdsensing networks, parameter selection, Timed Efficient Stream Loss-tolerant Authentication (TESLA), DoS attack prevention, Denial of Service, QoS, Sociology, protocols, denial of service, multilevel TESLA protocol, game theory, Lightweight Network, quality of service, Game theory, Statistics, attack-defense model, computer network security, Broadcast Authentication Protocol, Authentication, Games, Internet, Evolutionary Game]
Resource-Aware Photo Crowdsourcing Through Disruption Tolerant Networks
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Photo crowdsourcing with smartphone has attracted considerable attention recently due to the prevalence of smartphones and the rich information provided by photos. In scenarios such as disaster recovery or battlefield, where the cellular network is partly damaged or severely overloaded, Disruption Tolerant Networks (DTNs) become the best way to deliver the crowdsourced photos. Since the bandwidth and storage resources in DTN are very limited and not enough to deliver all the crowdsourced photos, it is important to prioritize more valuable photos to use the limited resources. In this paper, we design a resource-aware photo crowdsourcing framework in DTN, which uses photo metadata including the smartphone's location, orientation, and other built-in camera's parameters, to estimate the value of photos. We propose a photo selection algorithm to maximize the value of photos delivered to the command center considering bandwidth and storage constraints. Both prototype implementation and trace-driven simulations demonstrate the effectiveness of our design.
[Crowdsourcing, Algorithm design and analysis, image processing, photo metadata, storage resources, Metadata, cellular network, smart phones, disaster recovery, crowdsourced photos, storage constraints, trace-driven simulations, Satellites, resource-aware photo crowdsourcing, disruption tolerant networks, battlefield, Bandwidth, photo selection algorithm, Cameras, Sensors, DTN]
Mayflower: Improving Distributed Filesystem Performance Through SDN/Filesystem Co-Design
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
In this paper, we introduce Mayflower, a new distributed filesystem that is co-designed from the ground up to work together with a network control plane. In addition to the standard distributed filesystem components, Mayflower has a flow monitor and manager running alongside a software-defined networking controller. This tight coupling with the network controller enables Mayflower to make intelligent replica selection and flow scheduling decisions based on both filesystem and network information. It further enables Mayflower to perform global optimizations that are unavailable to conventional distributed filesystems and network control planes. Our evaluation results from both simulations and a prototype implementation show that Mayflower reduces average read completion time by more than 25% compared to current state-of-the-art distributed filesystems with an independent network flow scheduler, and more than 75% compared to HDFS with ECMP.
[Replica selection, read completion time, flow scheduling decisions, Network flow scheduling, Servers, Mayflower, software-defined networking controller, Fault tolerance, Network topology, Fault tolerant systems, Distributed databases, Bandwidth, distributed databases, scheduling, Monitoring, ECMP, standard distributed filesystem components, Software-defined networks, Distributed filesystems, software defined networking, HDFS, intelligent replica selection, distributed filesystem performance, network control plane, independent network flow scheduler, SDN-filesystem codesign]
A Parity-Based Data Outsourcing Model for Query Authentication and Correction
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
We propose a Parity-based Data Outsourcing(PDO) model in this paper. This model outsources a set of raw data by associating it with a set of parity data and then distributing both sets of data among a number of cloud servers that are managed independently by different service providers. Users query the servers for the data of their interest and are allowed to perform both authentication and correction. The former refers to the capability of verifying if the query result they receive is correct (i.e., all data items that satisfy the query condition are received, and every data item received is original from the data owner), whereas the latter, the capability of correcting the corrupted data, if any. A data item may be corrupted unintentionally (e.g, because of errors in systems and/or networking) or intentionally (e.g., by malicious service providers or because of systems being compromised by hackers). Existing techniques support only query authentication, but not error correction. Moreover, they all rely on complex cryptographic techniques and require the cloud server to build verification objects. In contrast, our approach achieves both without using any encryption. It does not require to install any additional software on a cloud server and thus can take advantage of the many cloud data management services available on the market today. We address the challenges of PDO implementation, including parity coding, database encoding, data retrieval, and database insertion and deletion, and evaluate the performance potential of PDO through analysis, simulation, and prototyping. Our results indicate its excellent performance in terms of storage, communication, and computation overhead.
[Cloud computing, data retrieval, Query Authentication, query correction, database encoding, Data Outsourcing, cryptographic techniques, Servers, verification objects, query processing, Databases, encryption, query authentication, cloud servers, Cryptography, cloud computing, cloud data management services, database insertion, cryptography, PDO model, Encoding, parity-based data outsourcing model, parity coding, outsourcing, Authentication, Outsourcing, database deletion]
Cure: Strong Semantics Meets High Availability and Low Latency
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Developers of cloud-scale applications face a difficult decision of which kind of storage to use, summarised by the CAP theorem. Currently the choice is between classical CP databases, which provide strong guarantees but are slow, expensive, and unavailable under partition, and NoSQL-style AP databases, which are fast and available, but too hard to program against. We present an alternative: Cure provides the highest level of guarantees that remains compatible with availability. These guarantees include: causal consistency (no ordering anomalies), atomicity (consistent multi-key updates), and support for high-level data types (developer friendly API) with safe resolution of concurrent updates (guaranteeing convergence). These guarantees minimise the anomalies caused by parallelism and distribution, thus facilitating the development of applications. This paper presents the protocols for highly available transactions, and an experimental evaluation showing that Cure is able to achieve scalability similar to eventually-consistent NoSQL databases, while providing stronger guarantees.
[Protocols, classical CP databases, causal consistency, application program interfaces, atomicity, NoSQL databases, Cure, latency, parallelism, Programming, developer friendly API, Servers, NoSQL-style AP databases, Convergence, high-level data types, cloud-scale applications, Databases, Semantics, multikey updates, strong semantics, CAP theorem, cloud computing, Clocks]
Flexible Instance: Meeting Deadlines of Delay Tolerant Jobs in the Cloud with Dynamic Pricing
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
A wide range of cloud computing jobs are delay tolerant up to a predefined deadline. Existing IaaS services offer either high cost and high fulfillment ratio or low cost without fulfillment ratio guarantee, where the fulfillment ratio is the ratio of job execution time to the time between job submission and completion. Neither of the services represents a cost-effective way to exploit job elasticity. This work proposes flexible instance, a cloud service where user-specified service fulfillment ratio, as a new pricing factor, is guaranteed by the provider to meet deadlines. Job elasticity is exploited by the provider to enhance resource utilization, by regulating demand fluctuation through computation arbitrage across the temporal domain. We leverage a two-stage pricing framework to agilely adapt cloud resource price to the demand-supply dynamics. The first stage uses an online strategy to reserve resources for each cloud user to guarantee its specified fulfillment ratio. We set the price of resources dynamically according to resource utilization, with a pricing curve O(ln p)-competitive to the optimal fixed-price offline strategy in provider revenue. The second stage allows cloud users to submit a small budget to compete for extra service fulfillment ratio for execution speedup. A Nash bargaining framework is explored to achieve fairness, resource efficiency, and revenue maximization simultaneously. Extensive simulations driven by real-world traces show that flexible instance can reduce user cost for job execution while increasing provider revenue.
[job elasticity, Cloud computing, provider revenue, pricing factor, pricing curve, cloud computing jobs, resource efficiency, meeting deadline, IaaS services, fulfillment ratio, resource allocation, Batch production systems, Pricing, revenue maximization, Nash bargaining framework, demand-supply dynamics, cloud computing, resource utilization, flexible instance, dynamic pricing, cloud resource price, delay tolerant jobs, game theory, cloud user, optimal fixed-price offline strategy, cloud users, user-specified service fulfillment ratio, cloud service, temporal domain, Streaming media, Delays, Resource management, Acceleration, job submission, two-stage pricing framework, pricing]
SpotLight: An Information Service for the Cloud
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Infrastructure-as-a-Service cloud platforms are incredibly complex: they rent hundreds of different types of servers across multiple geographical regions under a wide range of contract types that offer varying tradeoffs between risk and cost. Unfortunately, the internal dynamics of cloud platforms are opaque along several dimensions. For example, while the risk of servers not being available when requested is critical in optimizing the cloud's risk-cost tradeoffs, it is not typically made visible to users. Thus, inspired by prior work on Internet bandwidth probing, we propose actively probing cloud platforms to explicitly learn such information, where each "probe" is a request for a particular type of server. We model the relationships between different contracts types to develop a market-based probing policy, which leverages the insight that real-time prices in cloud spot markets loosely correlate with the supply (and availability) of fixed-price on-demand servers. That is, the higher the spot price for a server, the more likely the corresponding fixed-price on-demand server is not available. We incorporate market-based probing into SpotLight, an information service that enables cloud applications to query this and other data, and use it to monitor the availability of more than 4500 distinct server types across 9 geographical regions in Amazon's Elastic Compute Cloud over a 3 month period. We analyze this data to reveal interesting observations about the platform's internal dynamics. We then show how SpotLight enables two recently proposed derivative cloud services to select a better mix of servers to host applications, which improves their availability from ~70-90% to near 100% in practice.
[Cloud computing, data analysis, Decision making, fixed-price on-demand server supply, Servers, SpotLight, infrastructure-as-a-Service cloud platforms, cloud risk-cost tradeoffs, Information services, Internet bandwidth probing, market-based probing policy, cloud information service, Real-time systems, cloud spot markets, cloud computing, Probes, Amazon elastic compute cloud, Contracts, data query]
Routing and Scheduling of Social Influence Diffusion in Online Social Networks
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Owing to the rising popularity of online social networking services (OSNs), studies on social influence and its diffusion have received significant attention from the research community. Prior research has mostly studied the impact of single-hop influence diffusion and multi-hop influence broadcast in online social networks. Very little research explores the idea of guiding (routing) multi-hop social influence towards a specific target. In this paper, we motivate the needs of timely routing social influence to formulate a new optimization problem, namely, Routing And Scheduling of Target-Oriented Social Influence Diffusion (RAS-TOSID). Accordingly, we propose the Efficient Routing And Scheduling with Social and Temporal decOmposition (ERASSTO) algorithm, which finds the optimal solution to RAS-TOSID in polynomial time. We carry out a user study by implementing ERASSTO in Facebook and conduct a comprehensive evaluation on ERASSTO and alternative approaches by simulation. The result shows that ERASSTO significantly outperforms other algorithms regarding solution quality and computational efficiency.
[routing and scheduling of target-oriented social influence diffusion, optimization problem, multihop social influence, efficient routing and scheduling with social and temporal decomposition, RAS-TOSID, social networks, Routing, computational efficiency, distributed computation, Scheduling, Optimization, optimisation, online social networking services, scheduling, social networking (online), ERASSTO, polynomial time, Time factors, Facebook, Integrated circuit modeling, social influence, computational complexity]
Social Graph Publishing with Privacy Guarantees
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Online social network graphs provide useful insights on various social phenomena such as information dissemination and epidemiology. Unfortunately, social network providers often refuse to publish their social network graphs due to privacy concerns. Recently, differential privacy has become the widely accepted criteria for privacy preserving data publishing because it provides strongest privacy guarantees for publishing sensitive datasets. Although some work has been done on publishing matrices with differential privacy, they are computationally unpractical as they are not designed to handle large matrices such as the adjacency matrices of OSN graphs. In this paper, we propose a random matrix approach to OSN graph publishing, which achieves storage and computational efficiency by reducing the dimensions of adjacency matrices and achieves differential privacy by adding a small amount of noise. Our key idea is to first project each row of an adjacency matrix into a low dimensional space using random projection, and then perturb the projected matrix with random noise, and finally publish the perturbed and projected matrix. In this paper, we first prove that random projection plus random perturbation preserve differential privacy, and also that the random noise required to achieve differential privacy is small. We then validate the proposed approach and evaluate the utility of the published data for two different applications, namely node clustering and node ranking, using publicly available OSN graphs of Facebook, Live Journal, and Pokec.
[Data privacy, publishing matrices, graph theory, online social network graphs, social network providers, social graph publishing, Privacy, epidemiology, Pokec, Publishing, Databases, random perturbation, low dimensional space, node clustering, Facebook, Symmetric matrices, projected matrix, information dissemination, privacy guarantees, sensitive datasets, Live Journal, random noise, social phenomena, matrix algebra, privacy preserving data publishing, Social Networks, pattern clustering, OSN graph publishing, node ranking, Differential Privacy, social networking (online), data privacy, Random Matrix, differential privacy, random projection]
ChitChat: An Effective Message Delivery Method in Sparse Pocket-Switched Networks
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
The ubiquitous adoption of portable smart devices has enabled a new way of communication via Pocket Switched Networks (PSN), whereby messages are routed by personal devices inside the pockets of ever-moving people. PSNs provide opportunities for various interesting applications such as location based social networking, geolocal advertising, and military missions in active battlefields where the central communication tower is unavailable. One key challenge of the successful roll-out of PSN applications is the difficulty of achieving high message delivery ratio due to the dynamic nature of moving people and spatial-temporal sparsity in such networks. In this paper, we propose a novel message routing approach, called ChitChat, which exploits users' direct and transient social interests via discriminatory gossiping to penetrate messages deeper into the network. Our approach enables message carriers to make opportunistic and distributed routing decisions based on the likelihood a potential message receiver will meet individuals that have a high chance to forward the message to the destination. Our experimental results have demonstrated that our approach achieves higher delivery ratios against the two more recent state-of-the-art algorithms, while maintaining a lower communication overhead against flooding and reducing the amount of time messages remain idlein buffers.
[Opportunistic Networks, electronic messaging, Switches, distributed routing decisions, Metadata, Pocket Switched Networks, message delivery method, communication overhead, spatial-temporal sparsity, ChitChat, routing, mobile computing, Real-time systems, time message reduction, Social Interest, Transient analysis, sparse pocket-switched networks, Delay Tolerant Networks, transient social interests, message carriers, personal devices, Receivers, direct social interests, Routing, message routing, message forwarding, discriminatory gossiping, PSN applications, switching networks, routing protocols, portable smart devices, message receiver, central communication tower, Reliability, opportunistic routing decisions]
On Source Dependency Models for Reliable Social Sensing: Algorithms and Fundamental Error Bounds
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
This paper develops a simplified dependency model for sources on social networks that is shown to improve the quality of fact-finding -- assessing veracity of observations shared on social media. Recent literature developed a mathematical approach for exploiting social networks, such as Twitter, as noisy sensor networks that report observations on the state of the physical world. It was shown that the quality of state estimation from such noisy data, known as fact-finding, was a function of assumptions made regarding the independence of sources or lack thereof. When sources propagate information they hear from others (without verification), correlated errors may arise that degrade fact-finding performance. This work advances the state of the art by developing a simplified model of dependencies between sources and designing an improved dependency-aware estimator to assess veracity of observations, taking into account the observed dependency structure. A fundamental error bound is derived for this estimator to understand the gap in its performance from optimal. It is shown that the new estimator outperforms state of the art fact-finders and, in some cases, yields an accuracy close to the fundamental error bound.
[Maximum likelihood estimation, social networks, Source Dependency, fundamental error bounds, EM, Twitter, source dependency models, Error Bound, Social Sensing, Noise measurement, reliable social sensing, mathematical approach, Heating, social networking (online), state estimation, Silicon, Sensors, social media]
Efficient Top-k Result Diversification for Mobile Sensor Data
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Due to recent developments in sensor technologies, mobile sensor device use has become widespread, and many researchers have been attempting to leverage data collected by these devices. We call such data 'mobile sensor data'. Mobile sensor data are geo-referenced data with environmental attribute values, and they enable us to determine the geographical distribution of hot spots by retrieving data (such as higher air-pollution index values) with comparatively extreme environmental attribute values. Top-k search result diversification in geographical space is valid for applications of this sort. However, the preference scores for data items are different from each user's interest, and must be calculated for each query from scratch. In this case, the computational cost of a naive method is excessively high when the amount of mobile sensor data is very large. Thus, in this paper, we propose an efficient top-k search result diversification method for mobile sensor data. In a naive method, it is necessary to scan all data existing in a given query range when seeking the best data. Our proposed method, however, can reduce the amount of scanned data by exploiting cluster information, and the query result can thereby be returned much more rapidly. Moreover, a number of optimization problems can be solved using only one cluster file set. Experimental results show that our proposed method involves short computation time and reduces the disk IO cost in comparison with a naive method.
[top-k result diversification, mobile sensor device, participatory sensing, Mobile communication, Linear programming, Indexes, leverage data, Optimization, sensor technologies, geographical distribution, extreme environmental attribute values, cluster information, mobile computing, top-k search result diversification, disk IO cost, pattern clustering, geo-referenced data, Distributed databases, mobile sensor data, hot spots, hot spot detection, Computational efficiency, cluster file set, geographical space]
Energy Minimization for Quality-Constrained Video with Multipath TCP over Heterogeneous Wireless Networks
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
The advancements in wireless infrastructures and communication technologies prompt the bandwidth aggregation for mobile video delivery over heterogeneous access networks. Multipath TCP (MPTCP) is the transport protocol recommended by IETF to enable concurrent data transmissions over multiple communication paths. However, it still remains problematic to deliver user-satisfied video services with the existing MPTCP schemes due to the contradiction between mobile device energy and video distortion. To enable the energy-efficient and quality-guaranteed video streaming, this paper presents an Energy-Distortion Aware MPTCP (EDAM) solution. First, we develop an analytical model to capture the energy-distortion tradeoff for multipath video transmission over heterogeneous wireless networks. Second, we propose a video flow rate allocation algorithm to minimize the energy consumption while achieving target video quality based on utility maximization theory. The performance of the proposed EDAM is evaluated through extensive emulations in Exata involving real-time H.264 video streaming. Evaluation results demonstrate that EDAM outperforms the reference MPTCP schemes in reducing energy consumption, as well as in improving video PSNR (Peak Signal-to-Noise Ratio).
[Energy consumption, multiple communication paths, utility maximization theory, Distortion, energy-distortion aware MPTCP solution, concurrent data transmissions, mobile device energy, energy-efficient video streaming, transport protocol, energy-distortion tradeoff, mobile computing, Wireless networks, energy efficiency, video streaming, bandwidth aggregation, quality-guaranteed video streaming, user-satisfied video services, video communication, heterogeneous access networks, energy consumption, video flow rate allocation algorithm, multipath video transmission, Exata, real-time H.264 video streaming, mobile video delivery, multipath channels, communication technologies, EDAM, IETF, video distortion, Video recording, radio access networks, real-time video, MPTCP schemes, energy consumption minimization, quality-constrained video, transport protocols, multipath TCP, Multipath TCP, Streaming media, heterogeneous wireless networks, wireless infrastructures, Quality assessment, energy minimization]
Efficient Resource Allocation and Consolidation with Selfish Agents: An Adaptive Auction Approach
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Through virtualization technologies, modern enterprises can build private clouds to support the daily operations of their subsidiaries and consolidate resources from them. In making these resource allocation and consolidation decisions, they want to maximize the achieved utilities and minimize the incurred costs by their subsidiaries, respectively. However, these subsidiaries very often operate autonomously and have private information about job characteristics and energy costs. Due to this information asymmetry, they might be motivated to behave in their own best interests rather than that of the enterprise. To solve these principal-agent problems, we design a tunable auction under which subsidiaries submit bids and resources are allocated in proportion to their bids. We show that the induced competition game obtains a unique Nash equilibrium, under which hidden characteristics of subsidiaries can be revealed. By using variational inequality techniques, we derive the dynamics of the Nash equilibrium as a function of the auction parameters. We design a feedback control mechanism to dynamically adjust the tunable auction parameters based on observable information such as the bids and the resulting allocations. We prove that our adaptive auction converges to the optimal Nash equilibrium under which the aggregate utility of an enterprise is maximized.
[unique Nash equilibrium, Cloud computing, Theory, Variational Inequality, Nash equilibrium, Feedback control, virtualisation, Optimization, resource allocation, variational inequality techniques, Principal-Agent Problem, tendering, game theory, private clouds, tunable auction parameters, feedback control mechanism, Resource Allocation and Consolidation, adaptive auction approach virtualization technologies, consolidation decisions, Aggregates, information asymmetry, Games, principal-agent problems, Resource management, Virtualization]
HIDE: AP-Assisted Broadcast Traffic Management to Save Smartphone Energy
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
WiFi is a major source of energy consumption on smartphones. Unfortunately, a non-negligible portion of the WiFi energy consumption is spent for frames that are useless to the smartphone. For example, energy is wasted to receive WiFi broadcast frames that are not needed by any smartphone application. What's worse, in order to process the broadcast frames received, a smartphone in suspend mode switches from suspend mode to high power active mode and stays there for a while. As such, additional energy is wasted to do the processing. In this paper, we design a system, namely HIDE, to reduce smartphone energy wasted on useless WiFi broadcast traffic. With our system, smartphones in suspend mode do not receive useless broadcast frames or wake up to process useless broadcast frames. Our trace-driven simulation shows that the HIDE system saves 34%-75% energy for Nexus One when 10% of the broadcast frames are useful to the smartphone. Our overhead analysis demonstrates that our system has negligible impact on network capacity and packet round-trip time.
[Energy consumption, Protocols, Ports (Computers), HIDE system, WiFi broadcast frames, Unicast, network capacity, energy consumption, Power demand, AP-assisted broadcast traffic management, Nexus One, telecommunication power management, suspend mode, smart phones, smartphone energy saving, trace-driven simulation, computer network management, energy conservation, Central Processing Unit, packet round-trip time, wireless LAN, WiFi energy consumption, high power active mode, IEEE 802.11 Standard, telecommunication traffic]
Accurate Spatial Calibration of RFID Antennas via Spinning Tags
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Recent years have witnessed the advance of RFID-based localization techniques that demonstrate high precision. Many efforts have been made locating RFID tags with a mandatory assumption that the RFID reader's position is known in advance. Unfortunately, calibrating reader's location manually is always time-consuming and laborious in practice. In this paper, we present Tagspin, an approach using COTS tags to pinpoint the reader (antenna) quickly and easily with high accuracy. Tagspin enables each tag to emulate a circular antenna array by uniformly spinning on the edge of a rotating disk. We design an SAR-based method for estimating the angle spectrum of the target reader. Compared to previous AoA-based techniques, we employ an enhanced power profile modeling the signal power received from the reader along different spatial directions, which is more accurate and immune to ambient noise as well as measurement errors caused by hardware characteristics. Besides, we find that tag's phase measurements in practice are related to its orientation. To the best of our knowledge, we are the first to point out this fact and quantify the relationship between them. By calibrating the phase shifts caused by orientation, the positioning accuracy can be improved by 3.7&#x00D7;. We have implemented Tagspin withCOTS RFID devices and evaluated it extensively. Experimentalresults show that Tagspin achieves mean accuracy of 7.3cm with standard deviation of 1.8cm in 3D space.
[Phase measurement, reader localization, radiofrequency identification, spatial calibration, radar antennas, RFID antennas, phase measurements, spectral analysis, COTS RFID devices, Tagspin, angle spectrum estimation, hardware characteristics, antenna arrays, noise immunity, reader location calibration, Linear antenna arrays, calibration, enhanced power profile modeling, spinning tags, COTS tags, measurement errors, signal power modeling, RFID, SAR-based method, Calibration, RFID tags, circular antenna array, synthetic aperture radar, rotating disk, phase shift calibration, Spinning, RFID-based localization techniques, positioning accuracy, Radiofrequency identification]
WiLocator: WiFi-Sensing Based Real-Time Bus Tracking and Arrival Time Prediction in Urban Environments
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Offering the services of real-time tracking and arrival time prediction is a common welfare for bus riders and transit agencies, especially in urban environments. On the down side, the traditional GPS-based solutions work poorly in urban areas due to urban canyons, while the location systems based on cellular signal also suffer from inherent limitations. In this paper, we present a powerful tool named Signal Voronoi Diagram (SVD) to partition the radio-frequency (RF) signal space of WiFi Access Points (APs), distributed where a bus travels, into Signal Cells, and then into fine-grained Signal Tiles, tackling the problem of noisy received signal strength (RSS) readings and possible AP dynamics. On top of SVD, we present a novel framework so-called WiLocator, to track and predict the arrival time of an urban bus based on the surrounding WiFi information collected by the commodity off-the-shelf (COTS) smartphones of bus riders, the mobility constraint of a bus and the temporal consistency of travel time of buses on the overlapped road segments. We also show the WiLocator's power of generating an accurate and real-time traffic map with the predicted travel time on each road segment. We implement the prototype of WiLocator and conduct the in-situ experiment to demonstrate its accuracy.
[SVD, WiFi-sensing, location systems, Roads, WiLocator, arrival time prediction, radiofrequency, noisy received signal strength, arrivai time prediction, computational geometry, bus riders, RF signal space, transit agencies, WiFi-sensing based real-time bus tracking, Vehicles, Radio frequency, temporal consistency, traffic information systems, Real-time systems, cellular signal, bus tracking, urban canyons, WiFi information, WiFi access points, GPS-based solutions, Urban areas, signal Voronoi diagram, RSS readings, transportation, urban environments, COTS smartphones, real-time traffic map, overlapped road segments, wireless LAN, fine-grained signal tiles, AP dynamics, IEEE 802.11 Standard, Smart phones, commodity off-the-shelf]
CACE: Exploiting Behavioral Interactions for Improved Activity Recognition in Multi-inhabitant Smart Homes
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
We propose CACE (Constraints And Correlations mining Engine) which investigates the challenges of improving the recognition of complex daily activities in multi-inhabitant smart homes, by better exploiting the spatiotemporal relationships across the activities of different individuals. We first propose and develop a loosely-coupled Hierarchical Dynamic Bayesian Network (HDBN), which both (a) captures the hierarchical inference of complex (macro-activity) contexts from lower-layer microactivity context (postural and improved oral gestural context), and (b) embeds the various types of behavioral correlations and constraints (at both micro-and macro-activity contexts) across the individuals. While this model is rich in terms of accuracy, it is computationally prohibitive, due to the explosive increase in the number of jointly-defined states. To tackle this challenge, we employ data mining to learn behaviorally-driven context correlations in the form of association rules, we then use such rules to prune the state space dramatically. To evaluate our framework, we build a customized smart home system and collected naturalistic multi-inhabitant smart home activities data. The system performance is illustrated with results from real-time system deployment experiences in a smart home environment reveals a radical (max 16 fold) reduction in the computational overhead compared to traditional hybrid classification approaches, as well as an improved activity recognition accuracy of max 95%.
[constraint and correlation mining engine, Conferences, data mining, network theory (graphs), association rules, customized smart home system, Distributed computing, behavioral interactions, macroactivity contexts, behavioral correlations, hybrid classification approaches, multi-modal sensing, CACE, pattern classification, multiple inhabitants, improved activity recognition, HDBN, lower-layer microactivity context, behaviorally-driven context correlations, smart communities, naturalistic multiinhabitant smart home activity data, home automation, spatiotemporal relationships, sensors, Bayes methods, loosely-coupled hierarchical dynamic Bayesian network, scalable activity recognizer]
Overlay Design for Topic-Based Publish/Subscribe under Node Degree Constraints
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
It is important to build overlays for topic-based publish/subscribe (pub/sub) under resource constraints. In a topic-connected overlay (TCO), each topic t induces a connected sub-overlay among all nodes interested in t. Existing work merely consider how to optimize a complete TCO and implicitly commit the unrealistic assumption of unlimited resources. In contrast, we make maximum use of restricted node degree budgets to build a partial TCO. We formalize the notion of TCO support to capture the quality of the pub/sub overlay. Furthermore, we demonstrate that partial TCOs usually exhibit significantly better cost-effectiveness in practice. We propose two problems of maximizing TCO support in a partial TCO: (1) PTCOA with a bounded average node degree and (2) PTCOM under the maximum node degree constraint. We design two greedy algorithms, which achieve the constant approximation ratios of (1-e-1) for PTCOA and (1-e-1/6) for PTCOM, respectively. Empirical evaluation demonstrates the scalability of our algorithms under a variety of pub/sub workloads. Given practical data sets extracted from Facebook and Twitter, our algorithms produce an 80% TCO with fewer than 20% of the node degree budget as a complete TCO. We also show experimentally that it is promising to design decentralized protocols to compute a partial TCO for pub/sub.
[Algorithm design and analysis, Greedy algorithms, TCO, network theory (graphs), overlay design, Twitter, Servers, Internet of things, PTCOM, protocols, Facebook, middleware, PTCOA, cost-effectiveness, bounded average node degree, resource constraints, decentralized protocols, Routing, Partitioning algorithms, restricted node degree budgets, pub/sub, pub-sub overlay, maximum node degree constraint, partial TCO, social networking (online), topic-connected overlay, node degree constraints, Peer-to-peer computing, topic-based publish-subscribe]
Service Placement for Detecting and Localizing Failures Using End-to-End Observations
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
We consider the problem of placing services in a telecommunication network in the presence of failures. In contrast to existing service placement algorithms that focus on optimizing the quality of service (QoS), we consider the performance of monitoring failures from end-to-end connection states between clients and servers, and investigate service placement algorithms that optimize the monitoring performance subject to QoS constraints. Based on novel performance measures capturing the coverage, the identifiability, and the distinguishability in monitoring failures, we formulate the service placement problem as a set of combinatorial optimizations with these measures as objective functions. In particular, we show that maximizing the distinguishability is equivalent to minimizing the uncertainty in failure localization. We prove that all these optimizations are NP-hard. However, we show that the objectives of coverage and distinguishability have a desirable property that allows them to be approximated to a constant factor by a greedy algorithm. We further show that while the identifiability objective does not have this property, it can be approximated by the maximumdistinguishability placement in the high-identifiability regime. Our evaluations based on real network topologies verify the effectiveness of the proposed algorithms in improving the monitoring performance compared with QoS-based service placement.
[Uncertainty, combinatorial mathematics, Quality of service, failure localization, Servers, performance measures, telecommunication network, service placement algorithms, QoS optimization, network coverage, uncertainty minimization, computer network reliability, combinatorial optimizations, greedy algorithm, Monitoring, end-to-end connection states, monitoring performance optimization, monitoring performance improvement, greedy algorithms, Government, maximum-distinguishability placement, telecommunication network topology, quality of service, computer network performance evaluation, Atmospheric measurements, NP-hard problem, network topologies, service placement, Particle measurements, network tomography, high-identifiability regime, minimisation, failure detection, end-to-end observations]
Live Exploration of Dynamic Rings
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Almost all the vast literature on graph exploration assumes that the graph is static: its topology does not change during the exploration, except for occasional faults. To date, very little is known on exploration of dynamic graphs, where the topology is continously changing. The few studies have been limited to the centralized (or post-mortem) case, assuming complete a priori knowledge of the changes and the times of their occurrence, and have only considered fully synchronous systems. In this paper, we start the study of the decentralized (or live) exploration of dynamic graphs, i.e. when the agents operate in the graph unaware of the location and timing of the changes. We consider dynamic rings under the standard 1-interval-connected restriction, and investigate the feasibility of their exploration, in both the fully synchronous and semi-synchronous cases. When exploration is possible we examine at what cost, focusing on the minimum number of agents capable of exploring the ring. We establish several results highlighting the impact that anonymity and structural knowledge have on the feasibility and complexity of the problem.
[Schedules, dynamic graph decentralized exploration, graph theory, fully synchronous systems, Ports (Computers), standard 1-interval-connected restriction, Mobile communication, Topology, Distributed computing, agents, Upper bound, semisynchronous cases, Mobile agents, mobile agents, anonymity, Timing, structural knowledge, Distributed algorithms, dynamic rings live exploration]
Randomized Load Balancing in Finite Regimes
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Randomized load balancing is a cost efficient policy for jobscheduling in parallel server queueing systems whereby, with everyincoming job, a central dispatcher randomly polls some servers andselects the one with the smallest queue. By exactly deriving thejobs' delay distribution in such systems, in explicit and closedform, Mitzenmacher [13] proved the so-called 'power-of-two' result, which states that the random polling of only two serversyields an exponential improvement in delay over randomly selectinga single server. Such a fundamental result, however, was obtainedin an asymptotic regime in the total number of servers, and doesdo not necessarily provide accurate estimates for practical finiteregimes with small or moderate number of servers. In this paper weobtain stochastic lower and upper bounds on the jobs' averagedelay in non-asymptotic/finite regimes, by extending ideas foranalyzing the particular case of the Join-the-Shortest-Queue (JSQ) policy. Numerical illustrations indicate not only that the (lower) bounds are remarkably accurate, but also that the asymptoticapproximation can be misleading in scenarios with a small numberof servers, and especially at very high utilizations.
[join-the-shortest-queue policy, central dispatcher, queueing theory, Computational modeling, nonasymptotic-finite regimes, Servers, parallel processing, asymptotic approximation, parallel server queueing systems, Upper bound, resource allocation, JSQ policy, stochastic bounds, Markov processes, scheduling, job delay distribution, Delays, Numerical models, random polling, randomized load balancing, job scheduling, Manganese, power-of-two]
Spectrum Matching
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Dynamic spectrum access (DSA) redistributes spectrum from service providers with spare channels to those in need for them. Existing works on such spectrum exchange mainly focus on double auctions, where an auctioneer centrally enforces a certain spectrum allocation policy. In this paper, we take a different and new perspective, proposing to use matching as an alternative tool to realize DSA in a distributed way for a free market, which consists of only buyers and sellers, but no trustworthy third-party authority. Compared with conventional many-to-one matching problems, the spectrum matching problem is distinctively challenging due to the interference bound between buyers: the same channel can be reused by an unlimited number of non-interfering buyers, but must be exclusively occupied by only one of interfering buyers. In this paper, we firstly formulate the spectrum matching problem as a many-to-one matching with peer effects, i.e., a buyer's utility is affected by other buyers who are matched to the same seller. We then present a two-stage distributed algorithm that converges to an interference-free and Nash-stable matching result. Simulations show that the proposed distributed matching algorithm can achieve 90% of the social welfare from the optimal matching result.
[Algorithm design and analysis, radio spectrum management, spectrum allocation policy, interference bound, many-to-one matching, Nash-stable matching, dynamic spectrum access, distributed matching algorithm, Interference constraints, trustworthy third-party authority, interference-free matching, matching, Wireless communication, two-stage distributed algorithm, spectrum exchange, social welfare, DSA, peer effect, Dynamic spectrum access, spectrum matching, Resource management, Optimal matching, Distributed algorithms]
CACA: Link-Based Channel Allocation Exploiting Capture Effect for Channel Reuse in Wireless Sensor Networks
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
In this paper, we exploit the capture-effect for channel allocation. We experimentally show the characteristics of capture-effect across different channels, over time, and in different network densities. Then, we introduce CACA, an effective channel assignment protocol for wireless sensor networks. Traditional channel assignment protocols utilize all available channels to minimize interferences between any adjacent links. However, their performances are often not much better than the case of using single channel only. This is mainly due to an assumption that all channels are independent and quality of all channels are similar. However, this is a false assumption. In fact, there are only a few channels that show very good quality at any given time. The CACA avoids this problem by utilizing a few good channels and reuse these channels. When the channels are reused it relies on the capture-effect to ensure at least one of the contending nodes to transmit successfully. Maximizing this capture probability is a main objective of the CACA whenever the channels need to be reused. We evaluate the CACA on a 140-node wireless sensor network testbed and compare its performance with another benchmark protocol. Our results indicate that the CACA can improve the packet reception ratio of every link. As a result of this improvement, end-to-end throughput increases upto 100% in the case of a wireless sensor network with bursty traffic.
[benchmark protocol, packet reception ratio, Protocols, wireless sensor networks, CACA, Packet loss, Interference, Switches, channel diversity, wireless sensor network, Throughput, channel reuse, channel assignment protocol, Wireless sensor networks, radiofrequency interference, Channel allocation, wireless channels, interference, capture-effect, protocols, channel allocation]
An Analysis of Onion-Based Anonymous Routing for Delay Tolerant Networks
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Delay tolerant network (DTN) routing provides a communication primitive in intermittently disconnected networks, such as battlefield communications and human-contact networks. In these applications, the anonymity preserving mechanism, which hides the identities of communicating parties, plays an important role as a defense against cyber and physical attacks. While anonymous routing protocols for DTNs have been proposed in the past, to the best of our knowledge, there is no work that emphasizes the theoretical aspects. In this paper, we first design an abstract of anonymous routing protocols for DTNs and augment the existing solution with multi-copy message forwarding. Then, we construct simplified mathematical models, which can be used to understand the fundamental performance and security guarantees of onion-based anonymous routing in DTNs. The numerical and simulation results using randomly generated contact graphs and the real traces demonstrate that our models provide very close approximations to the performance of the anonymous DTN routing protocol.
[onion routing, message passing, cyber attacks, graph theory, random processes, Routing, cryptography, delay tolerant networks, security guarantees, Encryption, onion-based anonymous routing analysis, physical attacks, Anonymous communications, communicating party identification, Simulation, DTNs, routing protocols, multicopy message forwarding, randomly generated contact graphs, anonymous DTN routing protocol performance, Routing protocols, Mathematical model, anonymity preservation, simplified mathematical models]
Directional Beam Alignment for Millimeter Wave Cellular Systems
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Transmission in millimeter wave (mmW) band has a big potential to provide orders of higher wireless bandwidth. To combat the high channel loss in high frequency band, beamforming is generally taken to transmit along the direction that provides the maximum transmission gain. This requires the MAC protocol to facilitate the finding of the optimal beamforming direction. Exiting protocol suggests the rotational channel measurement which may introduce high measurement cost, and compromise the transmission capacity. This paper presents a comprehensive design for more efficient directional beam alignment in mmW cellular networks. Instead of exhaustively searching all possible beamforming directions at the transmitter (TX) and the receiver (RX), our proposed scheme selects only a fairly small number of TX and RX beam pairs to facilitate effective beam alignment. To avoid long and resource-consuming exhaustive search, our scheme not only takes advantage of the low rank characteristics of the channel to estimate the full channel information with a small number of measurements, but also further exploits the channel estimation from initial measurements to guide the selection of future beam pairs for more effective measurements later. These strategies help to speed up the process of finding satisfactory beam pairs. We perform extensive simulations to evaluate the performance of our proposed schemes, and our results demonstrate our scheme can significantly outperform other schemes in terms of measurement effectiveness and cost efficiency.
[mmW cellular networks, Array signal processing, TX, matrix completion, Transmitting antennas, RX, access protocols, millimeter wave cellular systems, Wireless communication, Channel estimation, directional antenna, wireless channels, optimal beamforming direction, transmitter, Antenna measurements, Base stations, receiver, beam alignment, transmission capacity, radio receivers, directional beam alignment, millimeter wave, mmW band, millimeter wave transmission, MAC protocol, radio transmitters, rotational channel measurement, beamforming, Antenna arrays, cellular radio, wireless bandwidth]
Measurement, Modeling, and Analysis of TCP in High-Speed Mobility Scenarios
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
The rapid growth of high-speed transit systems, such as High Speed Rail (HSR), is putting considerable pressure on TCP-based data transmission. It is well known that TCP is suffering from severe throughput degradation in high-speed mobility scenarios. The root cause at the transport layer however remains unclear and largely undetermined to date. In this paper, we aim to pinpoint the throughput bottlenecks and develop a throughput model to understand TCP in high-speed mobility environments. Based on the analysis of real-world HSR traces, we find that high-speed mobility will introduce significant challenges to the packet retransmission process after timeouts. And ACKs are more likely to trigger spurious retransmission timeouts in TCP flows in high-speed mobile environments. Such problems are not yet considered in the existing TCP models because classic timeouts can easily be recovered by retransmission in stationary scenarios. We therefore propose an enhanced TCP throughput model to integrate the above features. Our model analysis indicates that the optimization of TCP ACK latency is critical to obtain better throughput. Moreover, reliable retransmission mechanisms, e.g., multi-path TCP (MPTCP), can also bring notable benefits in high-speed mobility environments.
[high-speed mobility scenarios, packet retransmission process, high speed rail, modeling, Throughput, Mobile communication, mobility management (mobile radio), real-world HSR traces, measurement, TCP ACK latency, high-speed mobile environments, high-speed mobility environments, Analytical models, mobile computing, transport layer, reliable retransmission mechanisms, Rail transportation, TCP models, MPTCP, throughput degradation, model analysis, TCP flows, Receivers, TCP-based data transmission, TCP throughput model, Telecommunications, stationary scenarios, spurious retransmission timeouts, Rails, high-speed transit systems, modeling TCP throughput, transport protocols, multipath TCP]
Backlog-Aware SRPT Flow Scheduling in Data Center Networks
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
The rapidly developing soft real-time data center applications impose stringent delay requirements on internal data transfers. Therefore many recently emerged network protocols in data center share a common goal of decreasing Flow Completion Time (FCT), in which case the Shortest Remaining Processing Time (SRPT) scheduling discipline has attracted widespread attentions. However, SRPT suffers the instability issue, incurring more and more flows left uncompleted even when traffic load is within network capacity, which implies unnecessary bandwidth waste. To solve the problem, this paper proposes a backlog aware scheduling algorithm (BASRPT) that stabilizes queue length while maintaining relatively low FCT based on Lyapunov optimization. To overcome the huge computational overhead, a fast and practical approximation algorithm called fast BASRPT is also developed. Extensive flow-level simulations show that fast BASRPT indeed stabilizes switch queue and obtains a higher throughput while being able to push FCT arbitrarily close to the optimal value in the condition of feasible traffic load.
[Data Center Network, approximation theory, internal data transfers, Lyapunov optimization, SRPT, Stability analysis, Scheduling, computer centres, Flow Scheduling, backlog-aware SRPT flow scheduling, computer network management, optimisation, Scheduling algorithms, flow completion time, BASRPT approximation algorithm, FCT, flow-level simulation, scheduling, Approximation algorithms, shortest remaining processing time, Fabrics, Delays, data center networks, Backlog-Aware]
An NFV Orchestration Framework for Interference-Free Policy Enforcement
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Network functions virtualization is a new paradigm to offer flexibility of software network function processing on demand. Policy enforcement satisfies network function policies that requires flows to traverse through given sequences of network functions. We summarize three desired properties of virtual network function placement, namely policy enforcement, interference freedom, and resource isolation. However, none of existing solutions can satisfy all of them. In this paper, we present a novel SDN-based NFV orchestration framework, called APPLE, to enforce network function policies while providing the above properties. We present detailed design considerations and prototype implementation. We conduct experiments using representative network topologies, traffic matrices, and policy chains. The results from both prototype experiments and simulations show that APPLE is resource efficient and can quickly react to traffic changes.
[Orchestration, traffic matrices, APPLE, Middleboxes, virtualisation, NFV orchestration framework, Optimization, Engines, SDN-based NFV orchestration framework, Hardware, SDN, Service Chaining, NFV, network function virtualization, interference freedom, Interference, resource isolation, software defined networking, telecommunication network topology, Generators, interference-free policy enforcement, Noise measurement, software network function processing, network topology, matrix algebra, virtual network function placement, policy chains, network function policies]
Bandwidth-Greedy Hashing for Massive-Scale Concurrent Flows
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
The explosion of network bandwidth poses greatchallenges to data-plane flow processing. Due to the variable andpoor worst-case performance, naive hash table is incapable ofwire-speed processing. State-of-the-art schemes rely on multiplehash functions for enhanced load balancing to improve the worst-case performance. These schemes exploit the memory hierarchyand allocate compact on-chip data structures as the off-chiphash table summaries. However, when the flow number inflates, they fail to scale the on-chip memory consumption gracefully. This work is inspired by modern DRAM's burst-transfer feature. Specifically, we propose bandwidth-greedy hashing which resolveshash collisions with just one DRAM burst. Besides, load balancingefforts are made in an "on-demand" fashion. This radicaldesign surmounts the major obstacle of mapping multiple choicehashing schemes to real-world hierarchical memory systems formassive-scale items. Essentially, this solution follows a designpattern of on-demand load balancing and can be regarded asa generalization of closed hashing. To establish its theoreticalbase, we analyze it via Poisson distribution approximation. Theevaluation on DRAMSim2 reports that our scheme requires onlyone DRAM burst access (in 99.999% cases) and minuscule on-chip memory (less than 16MB, or 1% of the previous) to supportlookups for 100M flows at a throughput of 122.82Mpps.
[Performance evaluation, compact on-chip data structure, Random access memory, hash collision, on-demand load balancing, DRAMSim2, resource allocation, hash function, bandwidth-greedy hashing, Bandwidth, DRAM chips, System-on-chip, data-plane flow processing, massive-scale concurrent flow, approximation theory, greedy algorithms, Poisson distribution approximation, Data structures, cryptography, off-chip hash table summaries, Poisson distribution, on-chip memory consumption, Memory management, wire-speed processing, Load management, file organisation, DRAM burst-transfer feature]
Fast Total Ordering for Modern Data Centers
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
The performance profile of local area networks has changed over the last decade, but many practical group communication and ordered messaging tools rely on core ideas invented over a decade ago. We present the Accelerated Ring protocol, a novel ordering protocol that improves on the performance of standard token-based protocols by allowing processes to pass the token before they have finished multicasting. This performance improvement is obtained while maintaining the correctness and other beneficial properties of token-based protocols. On 1-gigabit networks, a single-threaded daemon-based implementation of the protocol reaches network saturation, and can reduce latency by 45% compared to a standard token-based protocol while simultaneously increasing throughput by 30%. On 10-gigabit networks, the implementation reaches throughputs of 6 Gbps, and can reduce latency by 30-35% while simultaneously increasing throughput by 25-40%. A production implementation of the Accelerated Ring protocol has been adopted as the default ordering protocol for data center environments in Spread, a widely-used open-source group communication system.
[Protocols, message passing, Accelerated Ring protocol, token passing, Multicast communication, Throughput, distributed coordination task, computer centres, Standards, data center, Semantics, Prototypes, token-based protocol, message delivery, fast total ordering, Acceleration, protocols]
A De-compositional Approach to Regular Expression Matching for Network Security Applications
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Regular expressions are a very common tool for network security applications because they can match precisely and maintain high matching speed even for many simultaneous patterns. Their core feature is efficient representation as an automaton, where much of the interaction between patterns can be pre-computed and aggregated. Many algorithms have been devised to try and improve this pre-computation to not take exponential space while keeping high performance, but none has met all the requirements of fast, automated construction, small memory image, and high matching speed. We present Match Filtering, a technique for de-composing regular expressions into segments that can be matched independently, while a stateful post-processing engine filters these matches to eliminate those that do not correspond to matches of the original regular expression. Using standard CPU instructions, the post-processing engine can more efficiently represent constructs that would require a multiplicative increase in automaton states. Because the pre-processing is simple, automaton construction can be automated and fast, and because most on-line processing is done by a DFA, its matching speed is close to that of a DFA alone. We demonstrate experimentally 30&#x00D7; smaller, fast (seconds, not minutes) automaton construction and 43% faster matching speeds than state-of-the-art software algorithms.
[distributed finite automata, pattern matching, Filtering, finite automata, network security applications, Security, de-compositional approach, Engines, computer network security, regular expression matching, regex, security, Automata, Hardware, match filtering technique, Communication networks, Pattern matching, DFA]
Privacy-Preserving Data Classification and Similarity Evaluation for Distributed Systems
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Data classification is a widely used data mining technique for big data analysis. By training massive data collected from the real world, data classification helps learners discover hidden data patterns. In addition to data training, given a trained model from collected data, a user can classify whether a new incoming data belongs to an existing class, or, multiple distributed entities may collaborate to test the similarity of their trained results. However, due to data locality and privacy concerns, it is infeasible for large-scale distributed systems to share each individual's datasets with each other for data similarity check. On the one hand, the trained model is an entity's private asset and may leak private information, which should be well protected from all other non-collaborative entities. On the other hand, the new incoming data may contain sensitive information which cannot be disclosed directly for classification. To address the above privacy issues, we propose a privacy-preserving data classification and similarity evaluation scheme for distributed systems. With our scheme, neither new arriving data nor trained models are directly revealed during the classification and similarity evaluation procedures. The proposed scheme can be applied to many fields using data classification and evaluation. Based on extensive real-world experiments, we have also evaluated the privacy preservation, feasibility, and efficiency of the proposed scheme.
[data training, Data privacy, Protocols, Big Data analysis, data mining, Similarity Evaluation, distributed processing, Machine Learning, Training, privacy-preserving data classification, data locality, Distributed databases, Data Classification, distributed systems, pattern classification, noncollaborative entities, Receivers, Big Data, hidden data patterns, large-scale distributed systems, Support vector machines, Privacy Preservation, data similarity evaluation, Data models, data privacy]
Secure Surfing: Privacy-Preserving Speeded-Up Robust Feature Extractor
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Large-scale multimedia data are being exponentially generated, stored and processed continuously nowadays. Along with the data explosion, the data owner is highly motivated to outsource his/her huge amount of data storage and computation-expensive processing jobs to the cloud by leveraging its abundant resources for cost reduction and flexibility. Despite the fascinating advantages, security and privacy concerns are the primary obstacles that prevent the wide adoption of this promising information technology paradigm. In this work, we aim at outsourcing Speeded-up Robust Features (SURF), a widely-used feature extraction algorithm, to the intrinsically untrusted cloud, while protecting data owner's private information on the outsourced image data. We, for the first time, propose a practical privacy-preserving outsoucing scheme for SURF that can preserve its key characteristics in terms of distinctiveness and robustness. By randomly splitting the original image data and distributing the encrypted data shares to two independent cloud servers, we first design two novel efficient protocols for secure multiplication and comparison by leveraging somewhat homomorphic encryption (SHE) and single-instruction multiple-data (SIMD). We then carefully tune every step of the original SURF to adapt it to the ciphertext domain. A thorough theoretical analysis of effectiveness and security shows that our scheme is practically secure and approximates well the performance of the original SURF executed in the plaintext domain. Extensive experiments are also conducted over real-world image datasets to show that our scheme outperforms the existing solution and indeed performs comparably to the original SURF in preserving its various characteristics including image scale invariance, rotation invariance and robust matching across 3D viewpoint change etc.
[Protocols, visual databases, Encryption, intrinsically untrusted cloud, SIMD, Servers, privacy-preserving speeded-up robust feature extractor, SHE, data storage, data owner private information protection, 3D viewpoint change, feature extraction, data protection, cloud servers, Speeded-up robust features, Robustness, single-instruction multiple-data, cloud computing, image scale invariance, cost reduction, somewhat-homomorphic encryption, cryptography, image matching, image datasets, privacy-preserving, ciphertext domain, robust matching, SURF, rotation invariance, privacy-preserving outsoucing scheme, outsourced image data, outsourcing, computation-expensive processing jobs, Feature extraction, data outsourcing, Outsourcing]
D-DEMOS: A Distributed, End-to-End Verifiable, Internet Voting System
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
E-voting systems have emerged as a powerful technology for improving democracy by reducing election cost, increasing voter participation, and even allowing voters to directly verify the entire election procedure. Prior internet voting systems have single points of failure, which may result in the compromise of availability, voter secrecy, or integrity of the election results. In this paper, we present the design, implementation, security analysis, and evaluation of D-DEMOS, a complete e-voting system that is distributed, privacy-preserving and end-to-end verifiable. Our system includes a fully asynchronous vote collection subsystem that provides immediate assurance to the voter her vote was recorded as cast, without requiring cryptographic operations on behalf of the voter. We also include a distributed, replicated and fault-tolerant Bulletin Board component, that stores all necessary election-related information, and allows any party to read and verify the complete election process. Finally, we also incorporate trustees, i.e., individuals who control election result production while guaranteeing privacy and end-to-end-verifiability as long as their strong majority is honest. Our system is the first e-voting system whose voting operation is human verifiable, i.e., a voter can vote over the web, even when her web client stack is potentially unsafe, without sacrificing her privacy, and still be assured her vote was recorded as cast. Additionally, a voter can outsource election auditing to third parties, still without sacrificing privacy. Finally, as the number of auditors increases, the probability of election fraud going undetected is diminished exponentially. We provide a model and security analysis of the system. We implement a prototype of the complete system, we measure its performance experimentally, and we demonstrate its ability to handle large-scale elections.
[Protocols, Voting system, reliability, distributed system, privacy, election-related information, Privacy, security, election result integrity, Prototypes, voter participation, D-DEMOS, privacy-preservation, E-voting systems, fault-tolerant Bulletin Board component, Cryptography, Electronic voting, election auditing, security analysis, cryptographic operations, Nominations and elections, election cost, cryptography, Web client stack, voter secrecy, fraud, election fraud, data privacy, Internet, distributed end-to-end verifiable Internet voting system, government data processing, fully asynchronous vote collection subsystem]
Deadline-Sensitive User Recruitment for Probabilistically Collaborative Mobile Crowdsensing
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Mobile crowdsensing is a new paradigm in which a group of mobile users exploit their carried smart devices to cooperatively perform a large-scale sensing job over urban environments. In this paper, we focus on the Deadline-sensitive User Recruitment (DUR) problem for probabilistically collaborative mobile crowdsensing, in which mobile users perform sensing tasks with certain probabilities, and multiple users might be recruited to cooperatively perform a common task, ensuring that the expected completion time be no larger than a deadline. In order to solve this problem, we propose a greedy approximation algorithm, which can achieve the logarithmic approximation ratio.
[approximation theory, greedy algorithms, Urban areas, large-scale sensing job, logarithmic approximation ratio, Mobile communication, Probabilistic logic, deadline-sensitive user recruitment, collaborative mobile crowdsensing, Recruitment, greedy approximation algorithm, crowdsensing, mobile computing, urban environments, mobile social network, user recruitment, probabilistic collaboration, Approximation algorithms, smart devices, Sensors, mobile users, Mobile computing, DUR problem]
RF-ISee: Identify and Distinguish Multiple RFID Tagged Objects in Augmented Reality Systems
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
In this paper, we leverage RFID technology to label different objects with RFID tags, so as to realize the vision of "show me what I see from the augmented reality system". We deploy additional RFID antennas to the COTS depth camera and propose a continuous scanning-based scheme to scan the objects, i.e., the system continuously rotates and samples the depth of field and RF-signals from these tagged objects. In this way, we can accurately identify and distinguish multiple tagged objects, by pairing the tags with the objects according to the correlations between the depth of field and RF-signals. Our solution achieves an average match ratio of 91% in distinguishing up to dozens of tagged objects with a high deployment density.
[radiofrequency identification, RFID antennas, augmented reality, COTS depth camera, continuous scanning-based scheme, RFID, object detection, Object recognition, RFID tags, RF-ISee, Augmented reality, cameras, augmented reality systems, RFID tagged objects, depth of field, Cameras, Feature extraction, object identification, Augmented Reality System, Antennas]
OmniFlow: Coupling Load Balancing with Flow Control in Datacenter Networks
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
In this paper, we propose OmniFlow, a novel transport protocol which combines load balancing and flow control at the transport layer to optimize datacenter transfers. OmniFlow outweighs previous solutions in two aspects. Firstly, it can simultaneously and precisely measure the queueing latencies on multiple paths between two hosts. Secondly, OmniFlow adaptively integrates the load balancing and flow control modules and shares the same congestion metrics (i.e. queueing latencies) between them. Based on different network conditions, it either dynamically reroutes flows to utilize the bisection bandwidth or proactively adjusts flow rates to bound queueing occupancies. Our results show that OmniFlow can provide both low latency for small flows without sacrificing the throughput of elephant flows.
[bisection bandwidth, Flow Control, Protocols, OmniFlow, congestion metrics, Throughput, queueing latency measurement, datacenter transfer optimization, Load Balancing, Datacenter Networks, coupling load balancing, flow control, queueing occupancies, computer centres, transport protocol, transport layer, resource allocation, transport protocols, Bandwidth, Load management, datacenter networks, elephant flows, Delays, Monitoring]
Approximate Agreement under Mobile Byzantine Faults
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
This paper considers the Approximate Agreement problem in presence of mobile Byzantine agents. We prove lower bounds on the number of correct processes to solve such problem. To do that we prove that the existing solutions tolerant to Byzantine agents still holds in such case and under which conditions.
[mobile Byzantine faults, mobile byzantine failures, fault tolerance, Computational modeling, Conferences, mobile Byzantine agents, Mobile communication, Distributed computing, Fault tolerance, approximate agreement, Fault tolerant systems, mobile agents, Approximation algorithms, fault tolerant computing, approximate agreement problem]
Preserving Incumbent Users' Privacy in Server-Driven Dynamic Spectrum Access Systems
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Dynamic spectrum access (DSA) technique has emerged as a fundamental approach in improving spectrum utilization to mitigate the spectrum scarcity problem. As a key form of DSA, government is proposing to release more federal spectrum for sharing with commercial wireless users. However, the flourish of federal-commercial sharing hinges upon how federal privacy issues are managed. In current DSA proposals, the sensitive operation information of federal incumbent users (IUs) needs to be shared with a dynamic spectrum access system (SAS) to realize spectrum allocation. However, SAS is not necessarily trust-worthy for holding such sensitive IU data, especially considering that FCC allows some industry third parties (e.g., Google) to operate SAS for better efficiency and scalability. Therefore, the current proposals dissatisfy the IUs' privacy requirement. To address the privacy issues, this paper presents an IU-privacy-preserving SAS (IP-SAS) design, which realizes the spectrum allocation process through secure computation over ciphertext based on homomorphic encryption so that none of the IU operation information is exposed to SAS.
[federal privacy issue, radio spectrum management, Google, ciphertext, IU-privacy-preserving SAS, Interference, FCC, incumbent user privacy preservation, federal-commercial spectrum sharing, privacy, Encryption, Servers, Synthetic aperture sonar, secure computation, spectrum scarcity, Privacy, IP-SAS, homomorphic encryption, server-driven dynamic spectrum access system, Dynamic spectrum access, spectrum allocation, Resource management, DSA technique, industry third parties]
On MinMax-Memory Claims for Scientific Workflows in the In-memory Cloud Computing
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
We propose a new concept of minmax memory claim (MMC) to achieve cost-effective workflow computations in in-memory cloud computing environments. The minmax-memory claim is defined as the minimum amount of memory required to finish the workflow without compromising its maximum concurrency. With MMC, the workflow tenants can achieve the best performance via the maximum concurrency while minimizing the cost to use the memory resources. In this paper, we present the algorithms to find the MMC for workflow computation and evaluate its value by applying it to deadlock avoidance algorithms.
[Algorithm design and analysis, Cloud computing, scientific workflow, deadlock avoidance algorithm, workflow scheduling, Conferences, in-memory cloud computing, Lattices, deadlock avoidance, Concurrent computing, storage management, minmax memory claims, Memory management, MMC concept, in-memory caching, memory constraints, minmax memory claim, System recovery, cloud computing, memory resources]
On the Efficiency of Decentralized Search in Expert Networks
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Expert networks are formed by a group of expert-professionals with different specialties to collaboratively resolve specific queries posted to the network. In expert networks, decentralized search, operating purely on each expert's local information without any knowledge of network global structure, represents the most basic and scalable routing mechanism. However, there is still a lack of fundamental understanding of the efficiency of decentralized search. In this regard, we investigate decentralized search by quantifying its performance under a variety of network settings. Our key findings reveal that under certain network conditions, decentralized search can achieve significantly small query routing steps (i.e., between O(log n) and O(log2 n), n: total number of experts in the network). To the best of our knowledge, this is the first work studying fundamental behaviors of decentralized search in expert networks.
[Knowledge engineering, High Efficiency, Government, query routing steps, Decentralized Search, Polylogarithmic Time, scalable routing mechanism, Routing, Search problems, Electronic mail, Expert Networks, query processing, Query processing, expert local information, fundamental behaviors, Problem-solving, expert networks, decentralized search, computational complexity]
Virtual Machine Level Temperature Profiling and Prediction in Cloud Datacenters
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Temperature prediction can enhance datacenter thermal management towards minimizing cooling power draw. Traditional approaches achieve this through analyzing task-temperature profiles or resistor-capacitor circuit models to predict CPU temperature. However, they are unable to capture task resource heterogeneity within multi-tenant environments and make predictions under dynamic scenarios such as virtual machine migration, which is one of the main characteristics of Cloud computing. This paper proposes virtual machine level temperature prediction in Cloud datacenters. Experiments show that the mean squared error of stable CPU temperature prediction is within 1.10, and dynamic CPU temperature prediction can achieve 1.60 in most scenarios.
[Cloud computing, CPU temperature prediction, Temperature, Predictive models, task resource heterogeneity, Calibration, Servers, level temperature profiling, computer centres, Temperature measurement, power aware computing, resource allocation, virtual machine, level temperature prediction, resistor-capacitor circuit models, virtual machines, temperature, cooling power, Mathematical model, cloud computing, task-temperature profiles, thermal management, cloud data centers]
Strategic Security Resource Allocation for Internet of Things
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
In many Internet of Thing (IoT) application domains security is a critical requirement, because malicious parties can undermine the effectiveness of IoT-based systems by compromising single components and/or communication channels. Thus, a security infrastructure is needed to ensure the proper functioning of such systems even under attack. In this paper, we focus on the problem of efficiently and effectively securing IoT networks by carefully allocating security tools.
[Energy consumption, security tools, security infrastructure, Electronic mail, Security, Internet of Things, Optimization, Internet of things, strategic security resource allocation, resource allocation, security of data, IoT application domains security, IoT-based systems, pareto analysis, internet of things, IoT networks, malicious parties, Resource management, Mathematical model]
FSQCN: Fast and Simple Quantized Congestion Notification in Data Center Ethernet
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Currently, Quantized Congestion Notification (QCN) has been accepted as the standard layer 2 congestion control protocol in Data Center Ethernet. Although the good performance of QCN has been validated in many experiments, we find that QCN has two drawbacks. First, the incomplete binary search in QCN fails to find the proper sending rate, leading to complicate supplement mechanisms. Second, in face of unknown network environment, the rate setting of QCN is inconsistent. Consequently, QCN spends much time on acquiring the spare bandwidth. To address theses problems, we propose the Fast and Simple QCN (FSQCN), following the same framework as QCN. FSQCN complements the binary search and removes other complicate supplement mechanisms in QCN. Thus, FSQCN is much simpler than QCN. Moreover, FSQCN resets the sending rate to the link rate explicitly when the switch detects spare bandwidth. Extensive simulations validate that FSQCN controls the queue length well like QCN and responds faster than QCN.
[binary search, Protocols, telecommunication congestion control, standard layer 2 congestion control protocol, Switches, local area networks, Topology, fast and simple quantized congestion notification, computer centres, Standards, FSQCN, link rate, data center Ethernet, Distributed databases, Bandwidth, Face]
Practical Concurrent Wireless Charging Scheduling for Sensor Networks
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
In complex terrain where mobile chargers hardly move around, a feasible solution to charge wireless sensor networks (WSNs) is using multiple fixed chargers to charge WSNs concurrently with relative long distance. Due to the radio interference in the concurrent charging, it is needed to schedule the chargers so as to facilitate each sensor node to harvest sufficient energy quickly. The challenge lies that each charger's charging utility cannot be calculated (or even defined) independently due to the nonlinear superposition charging effect caused by the radio interference. In this paper, we model the concurrent radio charging, and formulate the concurrent charging scheduling problem (CCSP) whose objective is to design a scheduling algorithm for the chargers so as to minimize the time spent on charging each sensor node with at least energy E. We prove that CCSP is NP-hard, and propose a greedy algorithm based on submodular set cover problem. We also propose a genetic algorithm for CCSP. Simulation results show that the performance of the greedy CCSP algorithm is comparable to that of the genetic algorithm.
[wireless sensor networks, Mobile communication, Electronic mail, mobile chargers, scheduling algorithm, wireless charging, Genetic algorithms, genetic algorithm, Wireless Sensor Network (WSN), radiofrequency interference, multiple fixed chargers, Electromagnetic interference, radio interference, scheduling, Silicon, inductive power transmission, CCSP, nonlinear superposition charging effect, concurrent charging scheduling problem, genetic algorithms, Wireless sensor networks, WSN, practical concurrent wireless charging scheduling, submodular set cover, Approximation algorithms]
A Geometric Windowing Algorithm in Network Data-Plane Verification
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Network verification is attracting attention as a key technology to detect configuration errors before deploying the network. In verification, a set of packets to be inspected is usually specified by a window -- a multi-dimensional rectangle defined by packet header fields (e.g., address prefixes and port ranges). Network operators have to know the forwarding behaviors of packets inside the window, this can be regarded as the windowing query problem in computation geometry. This paper proposes a novel windowing algorithm for network verification. Unlike existing windowing algorithms, our algorithm runs on a compressed data structure, because the search space has to be represented in a compressed form due to the space complexity.
[Conferences, Ports (Computers), network operators, computational geometry, packet forwarding behaviors, Complexity theory, network data-plane verification, compressed data structure, data structures, computer network reliability, search space, multidimensional rectangle, packet header fields, computation geometry, configuration error detection, Data structures, windowing query, network verification, Geometry, geometric windowing algorithm, Upper bound, Solids, windowing query problem, space complexity, decision diagram, computational complexity]
Dslash: Managing Data in Overloaded Batch Streaming Systems
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Peak loads are a challenge for streaming systems. Here we present Dslash, a latency-driven controller that keeps and processes as much data as the system resources allow in order to meet a target latency.
[data analysis, data management, Random access memory, Media, Twitter, Dslash, Sparks, Servers, overloaded batch streaming systems, Engines, target latency, batch processing (computers), Real-time systems, latency-driven controller]
Distributed Online Data Aggregation in Dynamic Graphs
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
We consider the problem of aggregating data in a dynamic graph, that is, aggregating the data that originates from all nodes in the graph to a specific node, the sink. In our model, nodes are endowed with unlimited memory and unlimited computational power. Yet, we assume that communications between nodes are carried out with pairwise interactions, where nodes can exchange control information before deciding whether they transmit their data or not, given that each node is allowed to transmit its data at most once. When a node receives a data from a neighbor, the node may aggregate it with its own data. We are interested in giving lower bounds for this problem, under two possible adversaries: the oblivious adversary, and the randomized adversary that chooses the pairwise interactions uniformly at random. For the online adaptive and the oblivious adversary, we give impossibility results when nodes have no knowledge about the graph and are not aware of the future. For the randomized adversary, we propose two optimal algorithms, (i) when nodes have no knowledge at all and (ii) when each node knows its future pairwise interactions with the sink.
[pairwise interactions, Heuristic algorithms, graph theory, Receivers, distributed online data aggregation, distributed processing, optimal algorithms, Topology, oblivious adversary, Indexes, data aggregation, Distributed computing, dynamic graphs, Aggregates, randomized adversary, control information exchange, online adaptive adversary]
Efficient Free-Rider Detection Using Symmetric Overlays
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Edge-computing is one of the most promising techniques to leverage the excess capacity that exists at users' premises. Unfortunately, edge-computing may be vulnerable to free-riding, i.e., to nodes that attempt to benefit from the system without providing any service in return. Traditional approaches model free-riders as rational nodes that strive to maximize a utility and apply Game Theory concepts to devise mechanisms that deny any utility gain to nodes that deviate from the protocol. These mechanisms impose significant overheads. This paper proposes a new approach that avoids these overheads, which applies concepts of evolutionary game theory. We propose to devise lightweight mechanisms targeted for the optimistic setting where nodes adopt one of a small number of behaviours. If a small fraction of nodes follows alternative behaviours, then the lightweight mechanism should limit the utility gain of these nodesto control the increase in the number of these nodes. This allows altruistic nodes to detect their presence in time to switch to more robust mechanisms before the system reaches a state where the lightweight mechanisms can no longer cope with the existing behaviours. We apply this approach in the context of edge-assisted streaming and propose the use of carefully crafted symmetric overlays to support message dissemination in an environment with free-riders and white-washers. The topology maintenance procedures of our overlay encourage nodes to maintain stable symmetric links. Leveraging the topological properties of the resulting symmetric overlay, direct reciprocity mechanisms deny any utility gain to free-riders and white-washers, while limiting the utility gain of small fractions of nodes that adopt more sophisticated behaviours.
[telecommunication links, Protocols, stable symmetric links, Conferences, topology maintenance, white-washers, topological property, direct reciprocity mechanism, message dissemination, evolutionary game theory, edge-assisted streaming, protocol, edge-computing, media streaming, utility gain, Robustness, protocols, utility maximization, altruistic nodes, game theory, Maintenance engineering, telecommunication network topology, efficient free-rider detection, evolutionary computation, symmetric overlay, overlay networks, Games, Peer-to-peer computing]
Distributed Encoding for Multiple-Inherited Locators to Accommodate Billions of Objects in the Internet
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
As the Internet of Things technologies evolve, billions of smart devices will be connected to the Internet. Therefore, the accelerated growth of users, applications and devices pose a great demand on the scalability of the Internet. In this paper, we develop a new Internet architecture -- Multiple-inherited Locators (MiL) -- to meet the future demand on addressing. MiL is based on Locator/ID addressing and hierarchical address allocation. The benefit of this new addressing scheme is the improved scalability of Internet routing by enhancing the prefix aggregation of locators. The number of locators which are needed grows with the Internet scale. In order to represent each locator as a unique binary representation by as few bits as possible, we develop a distributed encoding method to efficiently encode locators. As the Internet topology evolves, the distributed encoding method renumbering locators' codeword for the optimal encoding performance, and meanwhile endeavors to keep their codewords unchanged as much as possible. Because, we want to represent locators by fewer bits as well as keep addressing stable. We adopt an Encoding Table Adjustment algorithm to find a "sweet spot" that balances these two goals. According to our experiment, within a 2% increase on the expected locator length, our algorithm reduces the cost of renumbering locator by 99.9%.
[locator prefix aggregation, binary representation, Scalability, Internet architecture, multiple-inherited locators, Topology, encoding, Internet of Things, encoding table adjustment algorithm, Internet of things, MiL, locator codeword, hierarchical address allocation, telecommunication network routing, smart devices, Internet, Resource management, Huffman coding, Internet scalability, Internet topology]
Sprout: A Functional Caching Approach to Minimize Service Latency in Erasure-Coded Storage
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
The rapid growth of data traffic in storage systems has put a significant burden on the underlying networks of cloud storage systems. Historically, a key solution to relieve this traffic burden is caching [1]. Many companies have adopted erasure-coded storage systems. However, caching for data centers when the files are encoded with an erasure code has not been studied to the best of our knowledge. This paper proposes a new functional caching approach called Sprout that can efficiently capitalize on existing file coding in erasure-coded storage systems. In contrast to exact caching that stores chunks identical to original copies, our functional caching approach forms d new data chunks, which together with the existing n chunks satisfy the property of being an (n + d, k) MDS code. Thus, the file can now be recovered from any out of n + d chunks (rather than k out of n under exact caching), effectively extending coding redundancy, as well system diversity for scheduling file access requests. The proposed functional caching approach saves latency due to more flexibility to obtain k-d chunks from the storage system a very minimal additional computational cost of creating the coded cached chunks. While quantifying service latency erasure-coded storage systems is an open problem, we generalize previous results on probabilistic scheduling policy [2,3] that distributes file requests to cache and storage nodes with optimized probabilities, and derive a closed-form upper bound on mean service latency for the proposed functional caching approach.
[codes, storage system, probabilistic scheduling policy, system diversity, file access request scheduling, cloud storage system, MDS code, telecommunication scheduling, Cache storage, cache storage, functional caching approach, Servers, service latency, Optimization, data traffic, coding redundancy, Silicon, Sprout, cloud computing, erasure-coded storage system, computational cost, Probabilistic logic, Encoding, coded cached chunk, computer centres, data center, file coding, data chunks, closed-form upper bound, telecommunication traffic]
UDS: A Unified Approach to Deterministic Multithreading
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
There are several applications for the deterministic execution of software, e.g. replicated systems, test and febugging scenarios. In all cases the execution should lead zo the same effects in order to be consistent. Multi-threading is one of the major sources of nondeterminism. Several deterministic scheduling algorithms exist that allow concurrent but deterministic executions despite of arbitrary switching decisions of the underlying system schedulers. We present the novel and flexible Unified Deterministic Scheduling algorithm (UDS) for weakly and fully deterministic systems. Compared to existing algorithms, UDS has a broad parameter set that can be (re-)configured even at runtime. Further, we show that many existing algorithms are merely a particular configuration of UDS.
[program debugging, weakly-deterministic systems, Switches, unified deterministic scheduling algorithm, fully-deterministic systems, test scenario, Scheduling algorithms, debugging scenario, scheduling, debugging, deterministic execution, Java, system schedulers, replicated systems, reconfigured parameter set, multi-threading, deterministic software execution, Software algorithms, UDS, state machine replication, Debugging, Multithreading, deterministic multithreading, Software, nondeterminism source]
DMZtore: A dispersed Data Storage System with Decentralized Multi-factor Access Control (Demo)
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
While many commercial systems as well as academic techniques for data outsourcing to and content confidentiality from untrusted data stores have been developed over the last decade, when it comes to multi-factor authentication based layered security, existing approaches typically rely on a logically centralized service. In this demo, we present DMZtore edge storage system that incorporates a decentralized multi-factor access control scheme [1] achieving layered security.
[Access control, Protocols, DMZtore edge storage system, dispersed data storage system, content confidentiality, Servers, storage management, Authentication, Public key, authorisation, data outsourcing, decentralized multifactor access control scheme, multifactor authentication based layered security]
Milk Carton: A Face Recognition-Based FTR System Using Opportunistic Clustered Computing
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Family Tracing and Reunification (FTR) is the process whereby families separated by disasters are reunited. Current FTR systems use either inefficient paper-based forms and notice boards or digital registries that need the Internet, which may be unavailable during disasters. In this demonstration we present Milk Carton: a system that aids in FTR. Milk Carton creates a registry containing evacuee records. To find separated persons, Milk Carton uses Eigenfaces face recognition to match queries with existing records. Milk Carton uses a clustered architecture of Computing Nodes to handle data storage and execute the Eigenfaces algorithm. To operate under challenged-network environments, Milk Carton uses response patrol vehicles as data ferries to deliver data. In this demonstration, we show how Milk Carton uses Eigenfaces to locate separated persons and how data ferries and Computing Nodes function.
[Dairy products, Face recognition, disasters, Eigenface algorithm, computing node function, emergency management, Magnetic heads, image matching, Vehicles, query processing, mobile computing, Clustering algorithms, face recognition-based FTR system, face recognition, data storage handle, patrol vehicles, opportunistic clustered computing, data ferries, Internet, data handling, milk carton, Monitoring]
Memory Efficient One-Sided Communucation Library "ACP" in Globary Memory on Raspberry Pi 2
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Previously, communications in parallel programs for High Performance Computing (HPC) and Distributed Computing (DC) are mostly written with two-sided communication interfaces that are based on a pair of operations, Send and Receive. Since such interface requires explicit synchronization between both sides of the communication, techniques for communication optimization such as overlapping are not efficiently described in many cases. On the other hand, one-sided communication interface is becoming important as a method to describe asynchronous communications to enable highly overlapped communication with computation. As one of such interface, in this demonstration, Advanced Communication Primitives (ACP) is introduced. ACP is a portable interface that supports UDP, IBverbs of InfiniBand and Tofu library of K Computer. In addition to that, it is designed to be memory efficient. For example, with 10 thousand processes, the memory consumption of ACP over UDP is estimated to be less than 1MB. Since the number of computational elements is increasing more rapidly than the amount of available memory, this memory efficiency is becoming one of the keys for parallel programs in HPC and DC. To show this characteristics, we run ACP library on Raspberry Pi 2, and examine its performance and memory consumption.
[UDP, memory efficiency, receive operation, Programming, send operation, Synchronization, parallel processing, distributed computing, ACP, IBverbs, Tofu library, Internet of things, advanced communication primitives, Atomic layer deposition, Memory management, one-sided communucation library, Data transfer, Raspberry Pi 2, Libraries, high performance computing, HPC, DC, two-sided communication interfaces]
An SDN-Based Multipath GridFTP for High-Speed Data Transfer
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
We demonstrate high-speed data transfer GridFTP using a multipath control mechanism with SDN (Software-Defined Networking). GridFTP is a typical tool that has been developed and widely used for bulk data transfer over a wide area network in the field. GridFTP supports a parallel high-speed data transfer scheme using multiple TCP streams. However, one of the shortest paths is used solely for data transfer in the default IP routing while there are multiple network paths (multipath) exist between widely-distributed sites. In this study, we propose a system that distributes the parallel TCP streams of GridFTP into multiple network paths by a traffic engineering technique brought by SDN. Our proposed system has achieved approximately 20% better performance than the conventional method in the best case in a global-scale real enviroment.
[Globus Toolkit, wide area networks, Conferences, Packet loss, grid computing, software defined networking, Throughput, parallel processing, grid computing field, electronic data interchange, Bandwidth, Data transfer, Grid computing, parallel high-speed data transfer, SDN-based multipath GridFTP, multipath control mechanism, wide area network, middleware, bulk data transfer]
An Endpoint Communication Profiling Tool for Distributed Computing Frameworks
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Message exchange is a central activity in distributed computing frameworks. Nevertheless, past research has paid little attention on profiling techniques and tools for endpoint communication. In this paper, we fill this gap by introducing a new fine-grained profiler for endpoints and communication between them in distributed systems. Our tool aids efficient analysis, optimization, and tuning of endpoint communication in distributed frameworks. The paper presents an overview of our profiler, discusses employed profiling techniques and collected metrics, and shows preliminary results in profiling a well-known distributed computing framework.
[Measurement, message passing, message exchange, Instruments, program diagnostics, Electronic mail, Sparks, endpoint communication profiling tool, Distributed computing, distributed computing frameworks, fine-grained profiler, optimization, distributed systems, Informatics, Monitoring]
Cruisers: A Public Automotive Sensing Platform for Smart Cities
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Collecting urban data in a citywide scale plays a fundamental role in the research, development and implementation of smart cities. This demo introduces Cruisers, an automotive sensing platform for smart cities, which is developed based on the following ideas. a) Garbage collecting trucks are used as host automobiles to accommodate sensors, b) 3G cellular communication network is used to wirelessly deliver sensed data directly to servers, and c) Proxy server(s) are adopted to convert the format of sensed data to required ones. This platform has been deployed to 24 garbage collecting trucks at Fujisawa city, i.e., nearly 1/4 of the total number of such trucks in the city. An iOS application is also developed to demonstrate the sensing process and the covered area.
[Cruisers platform, Smart cities, 3G mobile communication, proxy server, Japan, public automotive sensing platform, urban data collection, Servers, Automobiles, town and country planning, Automotive engineering, Fujisawa City, mobile computing, garbage collecting trucks, Publishing, road vehicles, smart cities, 3G cellular communication network, Sensors, iOS application]
DL-Store: A Distributed Hybrid OLTP and OLAP Data Processing Engine
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
There has been a recent push in the database community towards supporting real-time analytical queries (OLAP) while sustaining a large volume of fine-grained updates (OLTP). Supporting these types of workloads require both an efficient data storage layer as well as a distributed architecture. In this demo, we address the latter point with our Distributed Lineage-based Data Store (DL-Store), which is a distributed data processing engine. DL-Store is built on top of L-Store, which is a lineage-based storage architecture designed to handle mixed OLTP and OLAP workloads, and provides scalability and elasticity by supporting multiple L-Store nodes. To maintain the desired consistency semantics, DL-Store employs a distributed transaction handler component which can horizontally scaled by provisioning additional transaction manager nodes. We leverage partitioning in the record space of the transactions to minimize communication across transaction managers while ensuring consistent execution. The demo shows our implementation of DL-Store over Apache Spark using a variety of use cases.
[transaction processing, Distributed Lineage-based Data Store, Computational modeling, data mining, distributed hybrid OLTP-OLAP data processing engine, DL-Store, distributed processing, Transaction handling, real-time analytical queries, Sparks, Apache Spark, Concurrent computing, storage management, mixed OLTP-OLAP workloads, Databases, distributed transaction handler component, OLAP-OLTP, Computer architecture, fine-grained updates, distributed lineage-based storage architecture, consistency semantics, database community, Big data, Spark]
Middleware for Proximity Distributed Real-Time Processing of IoT Data Flows
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
EdgeComputing and Fog Computing are new paradigms where data processing is executed in or on the edge of networks to mitigate cloud server load. However, EdgeComputing and Fog Computing still need powerful servers on the edge of networks which impose additional costs for deployments. We proposed a platform called IFoT (Information Flow of Things) that efficiently performs distributed processing as well as distribution and analysis of data streams near their sources based on "Process On Our Own (PO3)" concept. In IFoT, processing of tasks for cloud servers is delegated to an ad-hoc distributed system consisting of proximity IoT devices for distributed real-time stream processing. In this demonstration, we show a face recognition system for person tracking developed on top of IFoT middleware which locally processes video streams in real-time and in a distributed manner by using computational resources of IoT devices.
[fog computing, proximity distributed real-time processing, data streams, person tracking, Servers, Internet of things, cloud server load, Computer architecture, face recognition, object tracking, cloud servers, Real-time systems, cloud computing, IoT data flows, EdgeComputing, video signal processing, middleware, face recognition system, proximity IoT devices, IFoT middleware, Neurons, data processing, Internet of Things, Middleware, information flow of things, computational resources, ad-hoc distributed system, Streaming media, distributed real-time stream processing, video streams]
Federation of Private IaaS Cloud Providers through the Barter of Resources
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
This paper presents the Fogbow middleware. This middleware addresses cloud federation challenges by providing an additional layer for federation a top each local private IaaS that wants to join the federation. It uses designed-to-federate, internet-friendly technologies like XMPP and it is flexible enough to deal with a wide range of cloud technologies, since it provides a plugin framework for allowing simpler interoperation with a wide range of orchestrators. The plugin framework is also useful to define the behaviour of each member of the federation, providing greater autonomy to these members. A lightweight business model based on barter has been implemented and provides a cheap way to federate small and medium size private IaaS cloud providers.
[Cloud computing, plugin framework, interoperation, cloud technologies, Authorization, resource allocation, local private IaaS, Computer architecture, cloud computing, XMPP, designed-to-federate Internet-friendly technologies, middleware, private IaaS cloud providers, barter, cloud federation, barter based business model, Fogbow middleware, Federation middleware, interpolation, Authentication, small-medium size private IaaS cloud providers, resource sharing, Message service, Resource management, federated cloud]
Publish/Subscribe for Mobile Applications Using Shared Dictionary Compression
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Publish/Subscribe is known as a scalable and efficient data dissemination mechanism. In a mobile environment, there is an added challenge for the pub/sub system to economizemobile bandwidth, which is especially precious in areas not wellcovered by mobile providers. While well-known compressionmethods such as GZip or Deflate are generally useful in suchsituations, we propose using Shared Dictionary Compression(SDC) to achieve a greater level of bandwidth efficiency. SDCrequires a dictionary, generated upfront, to be shared betweentwo communicating peers before it can be used. We proposea design where brokers forming the pub/sub overlay can be incharge of generating and propagating the shared dictionary. Oursolution employs an adaptive algorithm, executed at the brokers, which creates and maintains the dictionaries over time. Withthis approach, it is possible to reduce the required bandwidth byup to 88% including the introduced dictionary overhead. Ourdemo shows this approach applied to a smartphone applicationcommunicating with a publish/subscribe broker using the MQTTprotocol.
[Dictionaries, Protocols, Mobile communication, shared dictionary compression, data dissemination mechanism, MQTT protocol, Jacobian matrices, mobile computing, SDC, Bandwidth, mobile applications, adaptive algorithm, Deflate, protocols, publish/subscribe broker, pubsub, middleware, bandwidth efficiency, dictionary overhead, publish/subscribe, mobile bandwidth, pub/sub overlay, XML, smartphone application, GZip, compression, Smart phones]
Real-Time Client-Side Phishing Prevention Add-On
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Since existing solutions for steering users away from phishing websites are typically server-based, they have several drawbacks: they compromise user privacy, are not robust against adaptive attackers who serve different content at different times, and do not provide any guidance to users after flagging a website as a phish. To address these limitations, we present a new phishing prevention system implementing a fast and effective phishing detection technique we developed recently [1]. It is implemented as a client-side application and a browser add-on. It uses information extracted from website visited by the user to detect if it is a phish and warn the user. It also determines the target of the phish and offers to redirect the user there.
[client-side application, phishing detection technique, Browsers, Electronic mail, Security, Uniform resource locators, real-time client-side phishing prevention add-on, Privacy, user privacy, adaptive attackers, browser add-on, phishing prevention system, Detectors, computer crime, online front-ends, Real-time systems, data privacy, phishing Websites, Web sites]
[Publisher's information]
2016 IEEE 36th International Conference on Distributed Computing Systems
None
2016
Provides a listing of current committee members and society officers.
[]
