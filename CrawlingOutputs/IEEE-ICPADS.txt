2421
A parallel programming environment based on message passing
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
With the development of parallel processing technology, more and more high-performance parallel computer systems have been developed. The convenient and flexible parallel programming environment plays an important role in the spread of parallel computing. How to write efficient parallel codes and how to convert the existing sequential applications into parallel codes have become a very important issue in parallel processing. We introduce a parallel programming environment based on message passing, which is simple to develop parallel applications and has high performance.
[message passing, Application software, parallel processing technology, parallel processing, parallel programming, Concurrent computing, Parallel programming, Message passing, Computer architecture, Parallel processing, efficient parallel codes, Computer networks, Hardware, parallel programming environment, Workstations, Kernel, parallel computing, high-performance parallel computer systems]
Roll-forward error recovery in embedded real-time systems
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
Roll-forward checkpointing schemes are developed in order to avoid rollback in the presence of independent faults and to increase the possibility that a task completes within a tight deadline. However, despite of the adoption of roll-forward recovery, these schemes are not necessarily appropriate for time-critical applications because interactions with the external environment and communications between processes must be deferred during checkpoint validation steps (typically, two checkpoint intervals) until the fault-free processors are identified. The deadlines on providing services may thus be violated. In this paper we present and discuss two alternative roll-forward recovery schemes, especially for time-critical and interaction-intensive applications, that deliver correct, timely results even when checkpoint validation is required.
[Real time systems, Checkpointing, multiprocessing systems, Redundancy, time-critical applications, distributed processing, checkpoint validation steps, fault-free processors, system recovery, Delay, checkpoint validation, Fault diagnosis, Fault detection, interaction-intensive applications, Fault tolerant systems, real-time systems, checkpointing schemes, fault tolerant computing, embedded real-time systems, Timing, Error correction, Time factors, roll-forward error recovery]
Minimizing broadcast costs under edge reductions in tree networks
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
We study the broadcasting of messages in the single-port, blocking communication model under edge reductions. Edge reductions model the decrease or elimination of broadcast costs on edges. Let T be an n-vertex tree and B be a target broadcast cost. We present an O(n) time algorithm for determining which edges to reduce (i.e., assign cost 0) such that the number of reduced edges is a minimum and a broadcast initiated at a vertex in a determined center set costs at most B. We present an O(n log n) time algorithm to minimize the cost of a broadcast initiated at an arbitrary vertex.
[tree networks, Costs, parallel architectures, vertex, Circuits, Project management, multiprocessor interconnection networks, trees (mathematics), blocking communication model, computational geometry, O(n) time algorithm, broadcast costs minimisation, messages broadcasting, Computer science, Intelligent networks, O(n log n) time algorithm, Broadcasting, Telephony, n-vertex tree, edge reductions, Contracts]
An efficient incremental algorithm for identifying consistent checkpoints
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
In a distributed system, identifying consistent checkpoints is essential for error recovery and debugging. We design an efficient incremental algorithm capable of identifying all the consistent and removable checkpoints each time a new checkpoint is reported. By doing so, the required memory space can be minimized by removing those removables. While minimizing the memory space, the algorithm requires only O(p/sup 2/M) time in total, where p is the number of processes and M is the number of checkpoints.
[Algorithm design and analysis, program debugging, Debugging, distributed system, removable checkpoints, memory space, system recovery, incremental algorithm, error recovery, consistent checkpoints, distributed algorithms, debugging, computational complexity]
A new scheme for sharing secret color images in computer network
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
Based on a visual cryptography scheme, an effective and generalized scheme of color image hiding is proposed. By means of little additional computations, it goes through a color index table to hide and recover a secret image. In this scheme, a secret color image hides itself in two arbitrary color images, which can be constructed and then are kept by two participants, separately. Note that the processed images are significant. It is a kind of camouflage. Later, the secret image can be recovered through the same simple algorithms on two camouflage color images. The paper proves that only one of the two camouflage color images cannot reveal the secret image. Thus, the hidden information is secured.
[Military computing, Data security, secret image recovery, Humans, computer networks, Color, color image hiding, cryptography, secret image, visual cryptography scheme, hidden information, Computer science, computer network, Intelligent networks, Information security, generalized scheme, Streaming media, camouflage color images, secret color image sharing, arbitrary color images, Computer networks, image colour analysis, Cryptography, color index table]
A server framework for scheduling multimedia applications in open system environment
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
We propose methods for scheduling multimedia soft real-time tasks in an open system environment. We discuss two issues, guaranteeing the schedulability of every task in the application and meeting the QoS of multimedia soft real-time tasks. First, when a real-time application is composed of a heterogeneous task set, hard real-time tasks and multimedia soft real-time tasks, the operating system must guarantee the schedulability of each task in the application. These guarantees can be achieved by preserving the CPU bandwidth of each task in the application. We have designed a server framework that preserves the CPU bandwidth of each task in the application. Each task can select its own server that can satisfy its time constraints. Second, when an operating system schedules multimedia soft real-time tasks in an open system, it is important for the operating system not only to support sufficient QoS level for the task, but also to utilize the CPU bandwidth of the system efficiently. We invented a new method of utilizing the CPU bandwidth of the system efficiently while supporting the requested QoS level.
[Real time systems, multimedia servers, open systems, Multimedia systems, multimedia applications, operating system, bandwidth, CPU, open system, Telecommunications, quality of service, Application software, Processor scheduling, Operating systems, network operating systems, soft real-time tasks, Open systems, Bandwidth, scheduling, Hardware, Time factors, multimedia server]
Performance evaluation of software RAID vs. hardware RAID for Parallel Virtual File System
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Linux clusters of commodity computer systems and interconnects have become the fastest growing choice for building cost-effective high-performance parallel computing systems. The Parallel Virtual File System (PVFS) could potentially fulfill the requirements of large I/O-intensive parallel applications. It provides a high-performance parallel file system by striping file data across multiple cluster nodes, called I/O nodes. Therefore, the choice of storage devices on I/O nodes is crucial to PVFS. We study the impact of software RAIDs and hardware RAIDs on the performance of PVFS when they are used on I/O nodes. We first establish a baseline performance of both RAIDs in a stand-alone configuration. We then present the performance of PVFS for a workload comprising concurrent reads and writes using ROMIO MPI-IO, and for the BTIO benchmark with a noncontiguous access pattern. We found that software RAIDs have a comparable performance to hardware RAIDs, except for write operations that require file synchronization.
[workstation clusters, Unix, input output nodes, Ethernet networks, file data striping, Software performance, Throughput, cluster computing, Concurrent computing, ROMIO MPI-IO, File systems, TCPIP, Linux clusters, high-performance parallel computing systems, concurrent reads-writes, Hardware, commodity computer systems, Power generation, Parallel Virtual File System, virtual storage, performance evaluation, RAID, synchronisation, Disk drives, Linux, storage devices, PVFS, BTIO benchmark, file synchronization]
PSS: a novel statement scheduling mechanism for a high-performance SoC architecture
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Continuous improvements in semiconductor fabrication density are supporting new classes of system-on-a-chip (SoC) architectures that combine extensive processing logic/processor with high-density memory. Such architectures are generally called processor-in-memory (PIM) or intelligent memory (I-RAM) and can support high-performance computing by reducing the performance gap between the processor and the memory. The PIM architecture combines various processors in a single system. These processors are characterized by their computation and memory-access capabilities. Therefore, a strategy must be developed to identify their capabilities and dispatch the most appropriate jobs to them in order to exploit them fully. Accordingly, this study presents a new automatic source-to-source parallelizing system, called SAGE, to exploit the advantages of PIM architectures. Unlike conventional iteration-based parallelizing systems, SAGE adopts statement-based analyzing approaches. It adopts a new pair-selection scheduling (PSS) mechanism to achieve better utilization and workload balance between the host and memory processors of PIM architectures. This paper also provides performance results and comparison of several benchmarks to demonstrate the capability of this new scheduling algorithm.
[parallel architectures, source-to-source parallelizing system, Random access memory, SAGE, scheduling algorithm, Delay, processor scheduling, Fabrication, resource allocation, memory processing, Computer architecture, semiconductor fabrication, processor-in-memory, System-on-a-chip, Logic, iteration-based parallelizing systems, parallel algorithms, intelligent memory, workload balance, random-access storage, Memory architecture, SoC architecture, pair-selection scheduling, statement scheduling, memory architecture, Processor scheduling, High performance computing, PIM architecture, Continuous improvement, system-on-chip, I-RAM architecture]
More Efficient and Secure Remote User Authentication Scheme using Smart Cards
11th International Conference on Parallel and Distributed Systems
None
2005
In 2004, Lee et al. proposed an improvement to Chien et al's scheme to prevent parallel session attack and in which any legal users could choose and change their passwords freely. This paper, however, demonstrates that Lee et al.'s scheme is vulnerable to masquerading server attack. Additionally, we point out to the system's secret key forward secrecy problem and insecure password change. Furthermore, the current paper presents a more efficient and secure scheme in that it not only resolves such problems but also involves less computations and communications than Lee et al.'s scheme
[Smart cards, insecure password change, Law, smart cards, cryptography, secret key forward secrecy problem, Communication system security, Proposals, Computer crime, parallel session attack, Authentication, message authentication, Resists, authorisation, secure remote user authentication scheme, Cryptography, IP networks, Legal factors]
Fault monitoring and detection of distributed services over local and wide area networks
12th International Conference on Parallel and Distributed Systems -
None
2006
Software development has evolved to incorporate the reusability of software components, enabling developers to focus on the requirements analysis without having to fully develop every component. Existing components that provide a given functionality can be reused by various applications. A parallel development has been the availability, through the World Wide Web, of data, transactions, and communications. These developments have led to the emergence of Web-services, collections of reusable code that use the Web communications paradigm for wider availability and communications between applications. In this context, with services coming and going, as well as possibly crashing, the issue of self-healing is of great relevance. How does an application learn that a remote service has become unavailable? In this paper we consider the issue of service failure detection and replacement, paying special attention to the relationship between the time it takes to find a replacement for a service, and the frequency of failure monitoring by the application
[fault diagnosis, wide area networks, fault detection, Programming, World Wide Web, local area networks, parallel programming, fault monitoring, Software reusability, Monitoring, Wide area networks, Context-aware services, Context, software development, Web-services, service failure detection, Computer crashes, Application software, software component reusability, Fault detection, service replacement, local area network, software reusability, system monitoring, Web sites, wide area network, self-healing, distributed services]
Toward ubiquitous searching
2007 International Conference on Parallel and Distributed Systems
None
2007
In this paper, we propose a novel concept called "ubiquitous searching\
[Pervasive computing, Algorithm design and analysis, Navigation, Ubiquitous Computing, information retrieval, Ubiquitous computing, information browsing, ubiquitous computing, ubiquitous searching, Wireless sensor networks, Wireless Sensor Network, Physics computing, proof-of-concept prototype, Searching, system architecture, Physical World, Dogs, Internet, Virtual prototyping, Lifting equipment]
An Efficient Disjoint Shortest Paths Routing Algorithm for the Hypercube
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
We present a routing algorithm that finds n disjoint shortest paths from the source node to n target nodes in the n-dimensional hypercube in O(n3log n)=O(log3NloglogN) time, where N=2n, provided that such disjoint shortest paths exist which can be checked in O(n5/2) time, improving the previous O(n4) routing algorithm.
[Algorithm design and analysis, Multiprocessor interconnection networks, network routing, target nodes, Routing, hypercube networks, disjoint shortest paths routing algorithm, Hamming weight, Computer science, n-dimensional hypercube, Fault tolerance, Sufficient conditions, Hypercubes, source node, computational complexity]
Evaluation of ConnectX Virtual Protocol Interconnect for Data Centers
2009 15th International Conference on Parallel and Distributed Systems
None
2009
With the emergence of new technologies such as Virtual Protocol Interconnect (VPI) for the modern data center, the separation between commodity networking technology and high-performance interconnects is shrinking. With VPI, a single network adapter on a data center server can easily be configured to use one port to interface with Ethernet traffic and another port to interface with high-bandwidth, low-latency InfiniBand technology. In this paper, we evaluate ConnectX VPI using microbenchmarks as well as real traces from a three-tier data center architecture. We find that with VPI each network segment in the data center can use the most optimal configuration (whether InfiniBand or Ethernet) without having to fall back to the lowest common denominator, as is currently the case. Our results show a maximum 26.7% increase in bandwidth, a 54.5% reduction in latency, and a 5% increase in real data center throughput.
[Virtual Protocol Interconnect, Protocols, Ethernet networks, InfiniBand, data centers, Switches, three-tier data center architecture, converged fabric, virtual private networks, Mathematics, local area networks, computer centres, Delay, Computer science, ConnectX Virtual Protocol Interconnect, High-speed networks, Bandwidth, Ethernet, Ethernet traffic, Fabrics, Computer networks]
GMH: A Message Passing Toolkit for GPU Clusters
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Driven by the market demand for high-definition 3D graphics, commodity graphics processing units (GPUs) have evolved into highly parallel, multi-threaded, many-core processors, which are ideal for data parallel computing. Many applications have been ported to run on a single GPU with tremendous speedups using general C-style programming languages such as CUDA. However, large applications require multiple GPUs and demand explicit message passing. This paper presents a message passing toolkit, called GMH (GPU Message Handler), on NVIDIA GPUs. This toolkit utilizes a data-parallel thread group as a way to map multiple GPUs on a single host to an MPI rank, and introduces a notion of virtual GPUs as a way to bind a thread to a GPU automatically. This toolkit provides high performance MPI style point-to-point and collective communication, but more importantly, facilitates event-driven APIs to allow an application to be managed and executed by the toolkit at runtime.
[Instruction sets, collective communication, MPI, Programming, coprocessors, parallel processing, GPU, Graphics processing unit, many-core processor, MPI style point-to-point communication, Bandwidth, NVIDIA GPU, GPU clusters, data-parallel thread group, Kernel, Message Passing, Message systems, message passing, multi-threading, message passing toolkit, parallel processor, GPU message handler, Cluster, data parallel computing, CUDA, MPI rank, computer graphics, C-style programming languages, Message passing, commodity graphics processing units, 3D graphics, multithreaded processor, virtual GPU]
A Static Task Scheduling Framework for Independent Tasks Accelerated Using a Shared Graphics Processing Unit
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
The High Performance Computing (HPC) field is witnessing the increasing use of Graphics Processing Units (GPUs) as application accelerators, due to their massively data-parallel computing architectures and exceptional floating-point computational capabilities. The performance advantage from GPU-based acceleration is primarily derived for GPU computational kernels that operate on large amount of data, consuming all of the available GPU resources. For applications that consist of several independent computational tasks that do not occupy the entire GPU, sequentially using the GPU one task at a time leads to performance inefficiencies. It is therefore important for the programmer to cluster small tasks together for sharing the GPU, however, the best performance cannot be achieved through an ad-hoc grouping and execution of these tasks. In this paper, we explore the problem of GPU tasks scheduling, to allow multiple tasks to efficiently share and be executed in parallel on the GPU. We analyze factors affecting multi-tasking parallelism and performance, followed by developing the multi-tasking execution model as a performance prediction approach. The model is validated by comparing with actual execution scenarios for GPU sharing. We then present the scheduling technique and algorithm based on the proposed model, followed by experimental verifications of the proposed approach using an NVIDIA Fermi GPU computing node. Our results demonstrate significant performance improvements using the proposed scheduling approach, compared with sequential execution of the tasks under the conventional multi-tasking execution scenario.
[Measurement, GPU computational kernel, performance prediction approach, parallelism, multitasking execution scenario, parallel processing, GPU, Graphics processing unit, resource allocation, GPU task scheduling, shared graphics processing unit, NVIDIA Fermi GPU computing node, Computer architecture, scheduling, GPU-based acceleration, Kernel, floating-point computational capability, Computational modeling, static task scheduling framework, Scheduling, multi-tasking, graphics processing units, application accelerator, Processor scheduling, multitasking execution model, resource sharing, GPU sharing, high performance computing, data-parallel computing architecture, GPU resource, independent task scheduling]
Optimizing Dynamic Programming on Graphics Processing Units Via Data Reuse and Data Prefetch with Inter-Block Barrier Synchronization
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Our previous study focused on accelerating an important category of DP problems, called nonserial polyadic dynamic programming (NPDP), on a graphics processing unit (GPU). In NPDP applications, the degree of parallelism varies significantly in different stages of computation, making it difficult to fully utilize the compute power of hundreds of pro-cessing cores in a GPU. To address this challenge, we proposed a methodology that can adaptively adjust the thread-level parallelism in mapping a NPDP problem onto the GPU, thus providing sufficient and steady degrees of parallelism across different compute stages. This work aims at further improving the performance of NPDP problems. Sub problems and data are tiled to make it possible to fit small data regions into shared memory and reuse the buffered data for each tile of sub problems, thus reducing the amount of global memory access. However, we found invoking the same kernel many times, due to data consistency enforcement across different stages, makes it impossible to reuse the tiled data in shared memory after the kernel is invoked again. Fortunately, the inter-block synchronization technique allows us to invoke the kernel exactly one time with the restriction that the maximum number of blocks is equal to the total number of streaming multiprocessors. In addition to data reuse, invoking the kernel only one time also enables us to prefetch data to shared memory across inter-block synchronization point, which improves the performance more than data reuse. We realize our approach in a real-world NPDP application a&#x0302;" the optimal matrix parenthesization problem. Experimental results demonstrate invoking a kernel only one time cannot guarantee performance improvement unless we also reuse and prefetch data across barrier synchronization points.
[Graphics processing units, inter-block barrier synchronization, tiling, inter-block synchronization point, parallelism degree, thread-level parallelism, GPU processing cores, GPU, streaming multiprocessors, optimization, shared memory systems, data prefetch, Kernel, parallel computing, NPDP applications, buffer storage, NPDP problem, global memory access, Prefetching, kernel, dynamic programming, inter-block synchronization technique, data integrity, Synchronization, graphics processing units, synchronisation, Tiles, Memory management, buffered data reuse, data consistency enforcement, nonserial polyadic dynamic programming, graphics processing unit, optimal matrix parenthesization problem, shared memory]
Evaluating Optimization Strategies for HMMer Acceleration on GPU
2013 International Conference on Parallel and Distributed Systems
None
2013
Comparing a biological sequence to a family of sequences is an important task in Bioinformatics, commonly performed using tools such as HMMer. The Viterbi algorithm is applied as HMMer main step to compute the similarity between the sequence and the family. Due to the exponential growth of biological sequence databases, implementations of the Viterbi algorithm on several high performance platforms have been proposed. Nevertheless, few implementations of the Viterbi algorithm use GPUs as main platform. In this paper, we present the development and optimization of an accelerator for the Viterbi algorithm applied to biological sequence analysis on GPUs. Some of the optimizations analyzed are applied to the sequence comparison problem for the first time in the literature and others are evaluated in more depth than in related works. Our main contributions are: (a) an accelerator that achieves speedups up to 102.90 and 60.46, with respect to HMMer2 and HMMer3 execution on a general purpose computer, respectively, (b) the use of the multi-platform OpenCL programming model for the accelerator, (c) a detailed evaluation of several optimizations such as memory, control flow, execution space, instruction scheduling, and loop optimizations, and (d) a methodology of optimizations and evaluation that can also be applied to other sequence comparison algorithms, such as the HMMer3 MSV.
[execution space, HMMer2, optimization strategies, Graphics processing units, GPU, Sequence-profile alignment, Optimization, instruction scheduling, hidden Markov models, biological sequence databases, optimisation, Viterbi algorithm, Databases, control flow, Parallel processing, scheduling, multiplatform OpenCL programming model, accelerator, loop optimizations, graphics processing units, HMMer acceleration, sequence comparison algorithms, Memory management, Hidden Markov models, bioinformatics, OpenCL, HMMer3 MSV, general purpose computer, Accelerator]
Don't be fat: Towards efficient online flow scheduling in data center networks
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Flow scheduling is one of the primary issues in data centers. The efficiency of flow scheduling affects the user experience significantly, since the service latency is determined by the flow completion time (FCT). Current transport protocols are based on the Processor Sharing (PS) policy by dividing the link bandwidth equally. As short flows may be blocked by long flows under PS policy, these protocols could not meet latency requirements. The Shortest Remaining Processing Time (SRPT) policy provides a near-optimal solution in term of reducing average FCT. However, this flow scheduling policy may cause long flows suffering unfair delays. In this paper, we propose MERP (Minimizing Expansion Ratio Protocol) for flow scheduling, which aims to navigate the tradeoff between fairness and the average FCT. We propose expansion ratio as a new metric to describe the gap between reality and ideal in terms of acquired link resources by a flow. Larger expansion ratio indicates that the flow obtains fewer bottleneck link resources. We strive to make the flow scheduling relatively fair by reducing the largest expansion ratio of all the flows. And we further demonstrate that reducing expansion ratio is more reasonable than reducing average FCT for services with the Partition/Aggregate pattern. And to meet the scalability requirements, we implement distributed MERP through flow preemption and explicit rate control. The experiments indicate that MERP could significantly reduce the completion time for 99.9th percentile flows under high load.
[Measurement, Protocols, distributed MERP, partition/aggregate pattern, flow scheduling, Switches, service latency, flow preemption, Bandwidth, scheduling, data center networks, computer networks, online flow scheduling, SRPT policy, Scheduling, computer centres, explicit rate control, processor sharing policy, shortest remaining processing time policy, minimizing expansion ratio protocol, Aggregates, transport protocols, flow completion time, FCT, PS policy, bottleneck link resources, rate control, data center network]
Adaptive Trust Update Frequency in MANETs
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Most of the existing trust-based security schemes for MANETs compute and update the trustworthiness of the other nodes with a fixed frequency. Although this approach works well in some scenarios, some nodes may not be able to afford the periodical trust update due to the limited resources in energy and computation power. To avoid energy depletion and extend the network lifetime, trust-based security schemes need approaches to update the trust taking into account the network conditions at each node. At the same time, a trade-off in terms of packet loss rate, false positives, detection rate, and energy of nodes is needed for network performance. In this paper, we first investigate the impact of trust update frequency on energy consumption and packet loss rate. We then identify network parameters, such as packet transmission rate, packet loss rate, remaining node energy, and rate of link changes, and leverage these parameters to design an Adaptive Trust Update Frequency scheme that takes into account runtime network conditions. The evaluation of our prototype shows significant improvements in the tradeoff between energy saving and packet loss rate over traditional fixed-frequency approaches.
[telecommunication security, packet loss rate, Energy consumption, Packet loss, Ad hoc networks, Security, energy saving, energy depletion, trust-based security schemes, packet transmission rate, trustworthiness, MANET, runtime network conditions, mobile ad hoc networks, remaining node energy, adaptive trust update frequency, energy consumption, Mobile computing, Monitoring]
NoiseSense: A Crowd Sensing System for Urban Noise Mapping Service
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Noise pollution poses a serious threat to people living in cities today. To alleviate the negative impact of noise pollution, an urban noise mapping can be helpful. In this paper, we present the design of NoiseSense, a crowd sensing system for housing a real-time urban noise mapping service. A major challenge in building such a system is caused by the sparsity problem of the limited noise measurement data from smartphones. To tackle this challenge, we propose a semi-supervised tensor completion algorithm for inferring noise levels for locations without measurements by smartphone users. This algorithm leverages a variety of urban data sources, such as Point of Interests (PoIs), road networks, and check-in data. We implemented the system and developed an APP for smartphone users. We conducted experiments and field study. The experimental results show that the proposed algorithm is superior in inferring noise levels merely with sparse measurements from smartphone users.
[Algorithm design and analysis, urban noise mapping service, APP, noise map, Roads, crowd sensing, noise measurement data, geographic information systems, tensors, road networks, PoI, urban data sources, Noise level, point-of-interests, mobile computing, noise level inference, Noise pollution, Sensors, noise calibration, noise pollution, smart phones, crowd sensing system, Noise measurement, tensor completion, Tensile stress, semisupervised tensor completion algorithm, NoiseSense, sparsity problem, check-in data, Smart phones]
Hierarchical Resource Distribution Network Based on Mobile Edge Computing
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Mobile edge computing (MEC) is an emerging paradigm to support the proliferation of smart phones and recent outstanding mobile data traffic growth. The network edge devices with computation, communication and storage capabilities can be applied to cache popular resource in close proximity to end uses and directly distribute resource to them, which is a promising solution to provide enhanced service. Firstly, in order to respond to the users' access requests at the edge of network, the distributed resource distribution network (DRDN) is constructed to realize the quick query, positioning and distribution of resource. It mainly consists of network elements: NEF (Network Exposure Function) node for global management and MEC nodes for distribution service. Secondly, on the basis of DRDN, two kinds of MEC's service models of distributing resource are defined and classified, the total energy consumption and average service delay for distributing resource to end users are analyzed. Finally, for the heavy access load and high latency of root caching MEC node, a hierarchical caching scheme (HCS) is designed and a caching node selection algorithm based on fuzzy C-means clustering is proposed according to the location characteristics of end users. The simulation results show that the HCS can save a lot of energy and reduce the average service delay. Additionally, for the different total arrival rate of users' access requests, the change of average service delay is stable with good adaptability in the HCS.
[NEF node, Energy consumption, Cloud computing, fuzzy set theory, Network Exposure Function, cache storage, root caching MEC node, caching node selection algorithm, Edge computing, query processing, mobile computing, end users location characteristics, distribution service, average service delay reduction, Clustering algorithms, MEC service models, fuzzy C means clustering, network edge devices, computation capability, mobile data traffic growth, hierarchical caching scheme, smart phones, hierarchical resource distribution Network, communication capability, network elements, fuzzy C-means clustering, storage capability, service delay, pattern clustering, DRDN, mobile edge computing, distributed resource distribution network, distribution energy consumption, Peer-to-peer computing, Delays, Resource management, caching resource, telecommunication traffic]
Parallel block generalized WZ factorization
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
In this paper we first present a block strategy for the generalized WZ factorization, which consists of block factorizing a matrix A in the form A=WZW/sup -1/. This study shows how a block strategy may be used to reduce a large eigenvalue problem into a number of smaller ones. Next, we develop a parallel multi-phase algorithm for this method, which requires processes such as matrices products, Gauss-Jordan elimination, broadcasting, scattering and gathering. To conceive our multi-phase algorithm we have used an informal methodology like the sequential top-down analysis which allows the conception of efficient multiphase parallel algorithms. The experimental tests show a good speed-up and corroborate the theoretical valuations.
[Algorithm design and analysis, parallel algorithms, Scattering, parallel block generalized WZ factorization, arallel multi-phase algorithm, Parallel algorithms, Cost accounting, Gauss-Jordan elimination, broadcasting, Jacobian matrices, Network topology, block strategy, scattering, Gaussian processes, Broadcasting, sequential top-down analysis, Eigenvalues and eigenfunctions, Testing, eigenvalue problem]
An efficient Hough transform algorithm on SIMD hypercube
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
This paper presents a fast algorithm for computing the Hough transform on SIMD hypercube architectures. For an image with N/spl times/N pixels and n quantized /spl theta/ values, our algorithm computes the Hough transform in O(logN) time on a hypercube of size N/spl times/N/spl times/n. An efficient algorithm for large Hough transform problems on a smaller fixed-size hypercube is also given. On a hypercube of size M/spl times/M/spl times/m, the time complexity is O(L/sup 2/l)=O(N/sup 2/n/M/sup 2/m) where L=N/M and l=n/m. Thus, the algorithm for large problems produces linear speed-up and optimal efficiency.
[Computer vision, Hough transform algorithm, Image edge detection, time complexity, SIMD hypercube, Parallel algorithms, Equations, hypercube architectures, Computer science, linear speed-up, Image analysis, Computer architecture, optimal efficiency, Hypercubes, Pattern analysis, Pixel, computational complexity]
Branch prediction for enhancing fine-grained parallelism in Prolog
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Branch instructions create barriers to instruction fetching, thus greatly reducing the fine-grained parallelism of programs. One common method for solving this problem is branch prediction. We first present four lemmas to clarify the relationship between the branch prediction hit rate and system performance, hardware efficiency, and branch prediction overhead. We then propose a new branch prediction method called PAM (Period Adaptive Method). An abstract model and detailed implementation of PAM are described. The prediction hit rate of this method was measured using ten Prolog benchmark programs and found to be 97%. When implemented in a superscalar Prolog system, PAM enhances the degree of system parallelism by 80%.
[Chaos, Costs, branch prediction, instruction fetching, Prediction methods, Performance gain, branch prediction hit rate, system parallelism, History, superscalar Prolog system, abstract model, System performance, branch instructions, hardware efficiency, branch prediction method, Parallel processing, Hardware, PROLOG, prediction hit rate, Prolog, Decoding, Prolog benchmark programs, branch prediction overhead, Computer science, Period Adaptive Method, fine-grained parallelism, system performance]
On the development paradigm of distributed applications
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
We propose an ideal development paradigm facilitating the implementation of distributed applications. In this paradigm a developer focuses his mind only on the application itself and does not need to spend time on application-unrelated activities. The programming style is nearly consistent with that of centralized system. The mechanism of the general support environment to this paradigm is described. An example explaining the implementation for an environment using the Sun RPC facility as the underlying communication component is also presented.
[Availability, development paradigm, Protocols, programming style, Application software, Distributed computing, Sun, distributed applications, Programming profession, parallel programming, Sun RPC facility, Program processors, Computer network reliability, general support environment, Computer networks, Workstations, underlying communication component]
Performance tuning of message passing programs through visual analysis
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Designing a parallel program to fully utilise the processing power of a multiprocessor machine requires a series of performance analysis and tuning. The paper describes a performance tuning tool for message passing parallel programs. The tool combines the advantages of relational databases and spreadsheets to organise the performance data and analyse the program performance through visualisation. Various graphical displays which assist the user to fine tune the performance of message passing programs are discussed.
[Data analysis, message passing, processing power, Instruments, performance tuning, Relational databases, Debugging, message passing programs, Displays, graphical displays, relational databases, parallel programs, Multiprocessing systems, spreadsheets, Message passing, visual analysis, Data visualization, multiprocessor machine, Performance analysis, performance data, parallel program, tuning tool, Monitoring, performance analysis]
Full-color real-time video broadcasting over ATM LAN
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
In this paper our experience of broadcasting full-color, full-size, real-time video over an ATM LAN is reported. The broadcasting system is adaptive in that each receiver can view video at its highest affordable frame rate. Effort has been made to guarantee continuous audio and best-effort audio and video synchronization. The key factors of the major components of the system which limit the performance of real-time ATM video applications are then discussed. Suggestions are given to address these limitations.
[broadcasting system, video synchronization, Switches, full-color real-time video broadcasting, ATM LAN, Multimedia communication, Digital audio broadcasting, synchronisation, continuous audio, Image coding, performance, Video compression, Traffic control, Hardware, best-effort audio, Local area networks, Asynchronous transfer mode, Digital video broadcasting]
Broadcasting on uni-directional hypercubes
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
In this paper, we solve the broadcasting problem for the even dimensional uni-directional hypercube (UHC). In the constant evaluation model, the complexity of one of our all-port broadcasting trees, is n+1, and it is optimal. Whereas the best one of our one-port broadcasting trees needs 4/3(n-n mode 6)+3/2(n mod 6) steps. These algorithms can be extended to solve the odd dimensional case. We also propose an all-port fault-tolerant broadcasting tree whose height is 3/2n+n mode 4/2.
[complexity, Costs, Mathematics, all-port broadcasting trees, fault-tolerant broadcasting tree, broadcasting, unidirectional hypercubes, Fault tolerance, Tree graphs, Broadcasting, Hypercubes, constant evaluation model, computational complexity]
Techniques to tackle state explosion in global predicate detection
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Detecting properties about a distributed program is an important problem in testing and debugging distributed programs. This problem is very hard due to the combinatorial explosion of the global state space. For a given execution, we consider the problem of detecting whether a predicate /spl Phi/ is true at some global state of the system. First, we present a space efficient algorithm for detecting /spl Phi/. Next, we present a parallel algorithm to reduce the time taken to detect /spl Phi/. We then improve the performance of our algorithms, both in space and time, by increasing the granularity of the execution step from an event to a sequence of events in a process.
[System testing, program debugging, Event detection, program testing, parallel algorithm, Debugging, state explosion, Explosions, State-space methods, Parallel algorithms, Computer science, predicate, debugging, Polynomials, Explosives, Safety, distributed program, space efficient algorithm, global predicate detection]
FDDI-M: a scheme to double FDDI's ability of supporting synchronous traffic
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Synchronous messages are usually generated periodically and each of them is required to be transmitted before the generation of the next message. Due to the inherent deficiency in its Medium Access Control (MAC) protocol, an FDDI token ring can use at most one half of its ring bandwidth to transmit such synchronous traffic. This deficiency greatly reduces the FDDI's capability of supporting multimedia applications like real-time voice/video transmissions. In this paper, we show how a few simple modifications to the FDDI's MAC protocol can remove this deficiency and double a ring's ability of supporting synchronous traffic. The modified protocol, called FDDI-M, preserves all other good features of an FDDI network and can also achieve a higher throughput for asynchronous traffic than the standard FDDI and the FDDI-II, thus making it useful even for those networks without heavy synchronous traffic.
[FDDI, token ring, asynchronous traffic, Laboratories, real-time voice/video transmissions, Telecommunication traffic, Access protocols, Medium Access Control protocol, Throughput, synchronous traffic, FDDI-M, synchronous messages, Bandwidth, Media Access Protocol, ANSI standards, Synchronous generators, Token networks]
Scouting: fully adaptive, deadlock-free routing in faulty pipelined networks
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Adaptive routing protocols based on message pipelining using wormhole routing (WR) can provide superior performance. However, the occurrence of faults can lead to situations that may produce deadlock. Variants of adaptive WR have been introduced (P.T. Gaughan and S. Yalamanchili, 1992) that employ backtracking and misrouting to first establish a path, followed by message pipelining (pipelined circuit switching, or PCS). This scheme avoids deadlock due to faults, but is overly conservative leading to reduced performance. The paper introduces a new family of flow control mechanisms ranging from WR to PCS that offers a compromise by only decoupling the routing probe and the data fits the minimal extent required to provide deadlock-free routing in the presence of faults.
[faulty pipelined networks, message pipelining, pipelined circuit switching, PCS, Laboratories, scouting, Communication system control, multiprocessor interconnection networks, deadlock-free routing, routing probe, Circuit faults, adaptive routing protocols, Pipeline processing, Intelligent networks, Fault tolerance, wormhole routing, adaptive WR, fault tolerant routing, System recovery, Routing protocols, Computer networks, flow control mechanisms, minimal extent, Personal communication networks]
Trace-based analysis and tuning for distributed parallel applications
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
We present an integrated approach to deal with timestamp consistency, and trace based performance analysis techniques for distributed parallel applications. Our trace generation facility captures message passing and system events such as process dispatch with minimal trace overhead. Trace driven analysis tools are developed for post execution analysis, reporting information such as the time stolen by other processes in each node, and the observed message passing time and local wait time for each message. We then present our techniques to reduce total elapsed times based on observed message passing times and local wait times.
[trace based performance analysis techniques, message passing, tuning, Computerized monitoring, distributed parallel applications, trace driven analysis tools, Application software, parallel programming, timestamp consistency, post execution analysis, Information analysis, system events, Computer science, Analytical models, Parallel programming, Message passing, integrated approach, process dispatch, local wait time, Performance analysis, Timing, trace-based analysis, trace generation facility, Clocks]
Broadcasting on faulty hypercubes
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
In this paper we propose a method for constructing the maximum number of edge-disjoint spanning trees (in the directed sense) on a hypercube with arbitrary one faulty node. Each spanning tree is of optimal height. By taking the common neighbor of the roots of these edge-disjoint spanning trees as the new root and reversing the direction of the directed link from each root to the new root, a spanning graph, consisting of n-1 edge-disjoint spanning trees of optimal height is formed. Broadcasting based on the spanning graph has an optimal bandwidth utilization and an optimal latency.
[Algorithm design and analysis, Multiprocessor interconnection networks, Routing, hypercube networks, faulty hypercubes, spanning graph, Delay, broadcasting, Fault tolerance, Tree graphs, Network topology, edge-disjoint spanning trees, Bandwidth, Broadcasting, optimal latency, Hypercubes, optimal height, optimal bandwidth utilization]
Sorting networks with built-in error correction
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
A sorting network with built-in error correction is proposed in this paper. A time shared TMR scheme is used to achieve the error correcting capability. A quarter of the original sorting network based on perfect shuffle is triplicated and voted in each stage. The hardware complexity of this time shared TMR error correcting sorting network is a little more than the original sorting network. The price is that the delay time increases by a factor of 4. However, the throughput penalty can be minimized by pipelining. A technology-independent gate level analysis of hardware complexity and delay time is included in this paper. Possible variations of the basic design are also discussed.
[technology-independent gate level analysis, Delay effects, Redundancy, perfect shuffle, Throughput, hypercube networks, Circuit faults, built-in error correction, pipelining, Sorting, Pipeline processing, sorting networks, Computer network reliability, delay time, Computer errors, throughput penalty, Hardware, time shared TMR scheme, Error correction, hardware complexity]
Thread migration on heterogeneous systems via compile-time transformations
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Describes a technique to provide multi-threading an an enhanced C language. In contrast to the traditional design of a thread library, which usually utilizes a few lines of assembly code to effect context-switching between threads, the technique we use is based on compile-time program transformations and a run-time library. Since this approach transforms a thread's physical states into logical forms, thread migration in a heterogeneous distributed environment becomes practically feasible. Performance measurements of the current implementation are reported.
[Measurement, Context, thread migration, Runtime library, multi-threading, Light scattering, thread library, Throughput, physical-logical transformation, heterogeneous distributed environment, C language, Yarn, Programming environments, Degradation, performance measurements, lightweight processes, context-switching, compile-time program transformations, Parallel processing, run-time library, Assembly, enhanced C language]
On parallel transaction processing in a coupled system
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
A performance study is conducted on parallel transaction processing in a coupled system, which is a multi-node system with a shared global buffer. We develop a multiple system simulator and obtain several performance results from it. This simulator has been run against three workloads, and the coupled system behavior with these three different inputs is studied. Several statistics, including those on local and global buffer hits, page writes to the global buffer, cross-invalidations and castouts, are comparatively analysed, and their relationship to the degree of data skew is explored.
[transaction processing, castouts, coupled system, Scalability, Laboratories, Environmental management, Delay, performance study, System performance, Fault tolerant systems, Parallel processing, local buffer hits, data skew, Availability, multi-node system, cross-invalidations, parallel transaction processing, workloads, multiple system simulator, global buffer hits, Transaction databases, shared global buffer, page writes, Load management, statistics]
A discrete-event simulation model for characterizing parallel file transfers
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
In this paper, we characterize the performance of parallel file transfers using the Zebra striped network file system by means of distributed discrete-event simulation model which uses a variation of the conservative timestamping-technique. The simulator is a powerful tool that can be adapted to model related systems such as multiprocessors and distributed shared memories. Preliminary simulation results confirm the results in published literature. Experiences with building the distributed simulator are also discussed.
[simulator, Computational modeling, Computer simulation, parallel architectures, Throughput, Discrete event simulation, Power system modeling, Concurrent computing, discrete-event simulation model, Disk drives, File systems, distributed shared memories, System performance, conservative timestamping-technique, parallel file transfers, Parallel processing, Zebra striped network file system, multiprocessors]
The design and performance considerations for multimedia applications using FDDI synchronous services
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
In this paper, an architectural design and implementation of a multimedia conference system over FDDI networks is presented. In this development process, various critical design issues are considered and approaches are proposed to optimize the architectural design to yield acceptable performance. The issues include 1) multicasting capability, 2) design of high speed transport protocol, and 3) delay and jitters performance of FDDI synchronous service.
[Transport protocols, high speed transport protocol, FDDI, Multimedia systems, Buildings, multimedia applications, Jitter, performance evaluation, Application software, jitters performance, Videoconference, Delay, conference system, FDDI networks, synchronous service, FDDI synchronous services, High-speed networks, delay, Bandwidth, multicasting capability, multimedia communication]
A linear equation model for twisted cube networks
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
The Twisted 3-cube is an interconnection network that twists the edges of the 3-dimensional hypercube to produce a network with diameter 2 and expected distance 11/8. A number of papers have shown that the Twisted 3-cube can be generalized into higher dimensional cube-like networks. We show that many of these networks can be described using a simple model. We place bounds on the diameter and expected distances of networks in this model, and show that the dynamic performance of these networks can match or improve upon the hypercube's performance in most conditions.
[linear equation model, dynamic performance, expected distance, higher dimensional cube-like networks, simple model, Routing, hypercube networks, Vectors, 3-dimensional hypercube, Equations, Delay, Computer science, hypercube performance, Twisted 3-cube, Network topology, twisted cube networks, interconnection network, expected distances, Hypercubes, Computer networks]
Broadcast in all-port wormhole-routed 3D mesh networks using extended dominating sets
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
A new approach to broadcast in wormhole-routed three-dimensional (3D) mesh networks is proposed. The approach extends the concept of dominating sets from graph theory by accounting for the relative distance-insensitivity of the wormhole routing switching strategy and by taking advantage of an all-port communication architecture, which allows each node to simultaneously transmit messages on different outgoing channels. The resulting broadcast operation is based on a tree structure that is composed of multiple levels of extended dominating nodes (EDN). Performance evaluation results, in the form of analysis and simulation, are presented that confirm the advantage of this technique over the recursive doubling approaches to broadcast.
[Tree data structures, broadcast, Computational modeling, graph theory, simulation, extended dominating nodes, performance evaluation, Routing, hypercube networks, Graph theory, all-port communication architecture, all-port wormhole-routed 3D mesh networks, Communication switching, dominating sets, Computer science, Intelligent networks, Mesh networks, recursive doubling approaches, Unicast, extended dominating sets, Broadcasting]
Group communications algorithm for dynamically updating in distributed systems
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
The paper proposes a novel updating technique, dynamically updating to achieve extension or modification of functions in a distributed system. Usual updating techniques require multiple processes to suspend simultaneously in order to avoid an unspecified reception caused by the conflict of different versions of processes. By using the proposed dynamically updating technique, the updating operation can be invoked asynchronously by each process with the assurance of correct execution of the system, i.e., the system can cope with the effect of an unspecified reception caused by a mixture of multiple version processes. This is implemented by using a novel distributed algorithm that consists of group communication, checkpoint setting, and rollback recovery. This algorithm achieves rollback recovery with the lowest overhead, i.e., a set of checkpoints determines the last global state for consistent rollback recovery and a set of processes that need to rollback simultaneously is the smallest.
[Protocols, Heuristic algorithms, Laboratories, Distributed computing, group communications algorithm, updating technique, dynamically updating, distributed systems, Communications technology, Large-scale systems, Workstations, dynamic updating, rollback recovery, Distributed algorithms, unspecified reception, Software algorithms, multiple version processes, last global state, group communication, distributed algorithm, Communication system software, distributed algorithms, consistent rollback recovery, updating operation, checkpoint setting]
Fault tolerance in hyperbus and hypercube multiprocessors using partitioning scheme
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
In this paper, the partitioning scheme is used to achieve fault tolerance in hyperbus and hypercube multiprocessors. Unlike other schemes, processor faults are assumed to be randomly distributed. We propose a novel and practical load redistribution method to tolerate processor faults in a hyperbus structure with insignificant overhead (a slowdown of 2 for computation and a slowdown of 3 for communication in the worst case). Standard routing and broadcasting algorithms were implemented on hypercube computers. To achieve fault tolerance, we present routing and broadcasting algorithms for a faulty hypercube with at most n-1 faults. Compared with other existing algorithms, our methods have better performance in most measures.
[load redistribution method, multiprocessing systems, fault tolerance, Routing, Partitioning algorithms, Topology, Concurrent computing, Fault tolerance, broadcasting algorithms, partitioning scheme, Councils, Broadcasting, Hypercubes, hypercube multiprocessors, Computer networks, hyperbus, Joining processes]
Computing list ranking on a RAP with wider bus networks
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
This paper makes an improvement of computing the list ranking and some related problems on a reconfigurable array of processors (abbreviated to RAP) with wider bus networks by increasing the bus width between processors. Based on such an architecture and a base-n system technique, a constant time basic operation is first introduced for computing the prefix modular n and prefix division n computations of an N-bit binary sequence on a linear RAP using N processors. Then, several fundamental problems can be solved in a constant time on a RAP using N/sup 1+1/c/ processors with N/spl times/N/sup 1+1/c/ bus networks and each bus network with N/sup 1/c/-bit, where c is a constant and e/spl ges/1. These algorithms include the prefix sum of N integers problem, the weight list ranking problem, the Euler tour problem and the tree recursions problem, respectively. Another contribution of this paper is that the execution time of the proposed algorithms is tunable by the bus width.
[RAP, prefix sum of N integers problem, Computational modeling, list ranking, Switches, bus width, tree recursions problem, Data structures, Phase change random access memory, reconfigurable processor array, constant time, Parallel algorithms, wider bus networks, Tree graphs, constant time basic operation, reconfigurable architectures, Euler tour problem, execution time, Computer architecture, weight list ranking problem, Computer networks, System buses, Binary sequences, binary sequence]
Effective load balancing on highly parallel multicomputers based on superconcentrators
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Tree and mesh architectures have been considered as two of the most highly scalable parallel multicomputers due to their scalabilities which are superior to those of hypercubes. However, the load balancing on these two multicomputer systems are not as well as we expected. The worst case of tree architecture requires O(M/spl times/p/spl times/logp) routing time for redistributing the workload over the system and it requires O(M/spl times//spl radic/p) for mesh architecture while pipelined packet routing scheme is used. In this paper, we propose an approach based on superconcentrators to reduce the above bounds to O(Mlogp) for both cases with only additional O(p) cost. Furthermore, by using this scheme, the underlying systems can leave the load balancing problem entirely to the superconcentrator so that there does not arise any additional workload of the systems. In addition, this scheme also adds extra communicating paths to the processors so that it not only increases the communication capacity among the processors but also could tolerate edge faults of the systems.
[Costs, superconcentrators, Navigation, load balancing, Oceans, Scalability, parallel architectures, mesh architectures, tree architectures, Routing, Distributed computing, pipelined packet routing scheme, highly parallel multicomputers, Concurrent computing, communicating paths, communication capacity, Load management, Hypercubes, edge faults, Resource management]
Virtual Permanent Connection: network computing over switch-based high-speed networks
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Recent progress in switch based high speed local area networks (LANs) makes distributed network computing promising. Three evolving switch based high speed networks are the High Performance Parallel Interface (HIPPI), Fiber Channel (FC), and Asynchronous Transfer Mode (ATM) standards. We study how high performance computing can be carried out over such networks. High performance computing can be characterized as follows: it includes multiple modules and each module is executed in a processor; its communication data flow forms a special application topology and usually such application topologies are regular; and it requires frequent communication between adjacent modules in the application topology. In order to reduce the amount of time required for a processor to set up a connection during the execution of an application, we propose a new communication protocol called the Virtual Permanent Connection (VPC). For a given application topology, a set of connections are set up and permanently maintained during the execution of the application. Communication between processors are via this group of connections. We study how a set of VPCs are chosen based on a given application topology (this process is called application topology embedding).
[switch-based high-speed networks, Switches, communication protocol, Optical fiber LAN, local area networks, Distributed computing, Asynchronous Transfer Mode, communication data flow, Network topology, High-speed networks, Fiber Channel, virtual permanent connection, High Performance Parallel Interface, Computer networks, Optical fiber communication, ATM standards, Local area networks, application topology, network computing, distributed network computing, HIPPI, High performance computing, switch based high speed local area networks, LANs, FC, Asynchronous transfer mode, multiple modules]
Achieving dependability in mission-critical operating systems through adaptability and large-scale functional integration
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
As part of the DRAGON SLAYER project, the adaptive and reliable distributed file system MELODY has emerged for supporting hard real-time applications in unpredictable environments. In MELODY, the time criticality of tasks and their sensitivity with respect to the latest file information are explicitly used for novel, flexible task scheduling algorithms and file replication management policies, featuring dynamic file replication and relocation of file copies as well as offering file versions of varying recency. The achieved adaptability far outweighs the additional overhead in comparison to simpler models, and enhances both reliability and real-time responsiveness for file access. Both the development of the model and the experimental analysis at the various stages were done in an incremental manner. This was necessary in order to cope with the complexity of the problems resulting from conflicting requirements and dynamic trade-offs (e.g. real-time responsiveness vs. reliability). As the next incremental model extension, and as the major contribution of this paper, a series of integration policies are developed for task and resource scheduling, after redefining the role and order of task and resource scheduling: the periodic and dynamic models (and combinations thereof) are used to invoke the task scheduler for a fixed interval of time. All of these policies compare very convincingly against the "classical" model where the task scheduler only schedules tasks after their resources have been allocated. The results are discussed.
[Real time systems, unpredictable environments, large-scale functional integration, Mission critical systems, file replication management policies, safety-critical software, real-time responsiveness, periodic model, Delay, file copies relocation, time criticality, File systems, Operating systems, conflicting requirements, Large-scale systems, dynamic model, Job shop scheduling, dependability, MELODY, flexible task scheduling algorithms, overhead, Dynamic scheduling, Large scale integration, adaptability, file access, recency, Scheduling algorithm, reliable distributed file system, hard real-time applications, mission-critical operating systems, DRAGON SLAYER project, dynamic trade-offs, file versions]
Torus with slotted rings architecture for a cache-coherent multiprocessor
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
The slotted ring is a point-to-point unidirectional connection for multiprocessor systems which resolves most of the problems associated with the bus system. However, the cycle time of the ring becomes the bottleneck when the system grows. Torus with slotted rings which is composed of multiple rings is proposed to reduce the cycle time of the resulting system. It is similar to the Wisconsin Multicube built by a grid of buses. The proposed architecture adopts a ring-map directory cache coherence scheme to avoid occupying too many rings during invalidation. Through performance evaluation, it is verified that the torus with slotted rings with ring-map directory scheme is better than the Wisconsin Multicube with the pure snooping scheme.
[Chaos, bus system, cycle time, Protocols, Latches, performance evaluations, torus, Integrated circuit interconnections, Telecommunication traffic, cache-coherent multiprocessor, Routing, multiple rings, point-to-point unidirectional connection, Multiprocessing systems, Pipeline processing, multiprocessor system, slotted rings architecture, shared memory systems, Hardware, Wisconsin Multicube, pure snooping scheme, Clocks, ring-map directory cache coherence scheme]
Efficient fault tolerance: an approach to deal with transient faults in multiprocessor architectures
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Dynamic error processing approaches are an important mechanism to increase the reliability in a multiprocessor system, while making efficient use of the available resources. To this end, dynamic error processing must be integrated with a fault treatment approach aiming at optimising resource utilisation. In this paper we propose a diagnosis approach that, accounting for transient faults, tries to remove units very cautiously and to balance between two conflicting requirements. The first is to avoid the removal of units that have experienced transient faults and can be still useful for the system and the other is to avoid to keep failed units whose usage may lead to a premature failure of the system. The proposed fault treatment approach is integrated with a mechanism for dynamic error processing in a complete fault tolerance strategy. Reliability analyses based on the Markov approach and an efficiency evaluation performed by simulation are carried out.
[Performance evaluation, Costs, multiprocessing systems, fault tolerance, Markov approach, Computational modeling, Redundancy, simulation, reliability, diagnosis approach, transient faults, resource utilisation, Fault diagnosis, multiprocessor architectures, Fault tolerance, Analytical models, dynamic error processing, error processing, Performance analysis, Resource management, Bonding]
Practical distributed garbage collection for networks with asynchronous clocks and message delay
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Distributed garbage collection over a message passage network is discussed in this paper. Traditionally, this can be done by reference counting, which is fast but cannot reclaim cyclic structures or by graph traversal, e.g. mark-and-sweep or time stamping, which is capable of reclaiming cyclic structures but is slow. We propose a combined scheme which is fast in reclaiming acyclic garbage and guaranteed to reclaim cyclic garbage. Our scheme does not rely on synchronized clocks nor zero message delay and is thus practical.
[Computer science, storage management, Stability, message delay, distributed garbage collection, cyclic structures, Synchronization, networks with asynchronous clocks, Delay, Clocks, Pipeline processing, Counting circuits]
Integrated support to improve inter-thread communication and synchronization in a multithreaded processor
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
This paper presents an integrated compiler, runtime control, and hardware solution to improve inter-thread communication and synchronization in a multithreaded processor architecture. Multithreading improves processor utilization by exploiting more parallelism. The improvement in utilization, however, is hindered by interthread communication and synchronization problems, which incur extra communication overhead and thus degrade the performance of the system. In this paper, we propose efficient inter-thread communication and synchronization schemes based on a superscalar DLX processor with multithreading functionality. Compiler, runtime control, and hardware support used in the schemes are discussed. Simulations are presented to show the effectiveness of the proposed schemes.
[Context, Costs, superscalar DLX processor, Communication system control, Switches, multithreaded processor architecture, parallelism, multithreading functionality, Yarn, Communication switching, Delay, integrated compiler, synchronisation, inter-thread communication, Runtime, Multithreading, runtime control, synchronization, Hardware, multithreaded processor]
Optimal information dispersal for reliable communication in computer networks
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
In an (m, n) Information Dispersal Scheme (IDS), the sender node decomposes a message M of length L into n pieces S/sub i/, 1/spl les/i/spl les/n, each of length L/m, such that any m pieces collected by the receiver node over different paths suffice for reconstructing M. Because of variations of network traffic, the number n of available vertex-disjoint paths for the transmission from the sender node to the receiver node may vary in time. It is very difficult to determine the best n and m which gives the highest communication reliability, when given the maximum number of available disjoint paths and an upper bound for the information expansion rate (n/m). In this research, we discovered several interesting features of (m, n) IDSs which can help reduce the complexity for computing the highest communication reliability. From these findings, we propose a method for determining the optimal IDS.
[complexity, computer networks, vertex-disjoint paths, Reliability engineering, Sun, Information analysis, Computer science, optimal information dispersal, network traffic, Intelligent networks, Fault tolerance, Upper bound, Computer network reliability, communication reliability, Intrusion detection, reliable communication, Telecommunication network reliability]
Obtaining nondominated k-coteries for fault-tolerant distributed k-mutual exclusion
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
A k-coterie is a family of sets (called quorums) in which any (k+1) quorums contain at least a pair of quorums intersecting each other. K-coteries can be used to develop distributed k-mutual exclusion algorithms that are resilient to node and/or communication link failures. A k-coterie is said to dominate another k-coterie if and only if every quorum in the latter is a super set of some quorum in the former. Obviously the dominating one has more chance than the dominated one for a quorum to be formed successfully in an error-prone environment. Thus, we should always concentrate on nondominated k-coteries that no k-coterie can dominate. We introduce a theorem for checking the nondomination of k-coteries, define a class of special nondominated k-coteries-strongly nondominated (SND) k-coteries, and propose two operations to generate new SND k-coteries from known SND k-coteries.
[communication link failure, nondominated k-coteries, fault-tolerant distributed k-mutual exclusion, Neodymium, SND k-coteries, quorums, strongly nondominated k-coteries, Computer science, Fault tolerance, Voting, distributed k-mutual exclusion algorithms, Permission, fault tolerant computing, error-prone environment, Resource management, node failure]
Performance analysis of distributed client-server message queuing
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Queuing is an efficient programming paradigm for centralized and distributed systems. It enables multiple programs to share and control the flow of data and work objects, and to establish transparent and asynchronous communication among non-coherent software products, platforms or applications. This paper presents the performance analysis and specifications of a reliable, distributed and high performance queuing facility, which is capable of delivering thousands of enqueue and dequeue operations per second on local or distributed systems.
[Availability, queueing theory, distributed client-server message queuing, Communication system control, asynchronous communication, Application software, Distributed computing, Asynchronous communication, Operating systems, Space technology, Performance analysis, Telecommunication network reliability, Queueing analysis, performance analysis]
A theory of fault-tolerant routing in wormhole networks
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Fault-tolerant systems aim at providing continuous operations in the presence of faults. Multicomputers rely on an interconnection network between processors to support the message-passing mechanism. Therefore, the reliability of the interconnection network is very important for the reliability of the whole system. This paper analyzes the effective redundancy available in a wormhole network by combining connectivity and deadlock freedom. Redundancy is defined at the channel level. We propose a sufficient condition for channel redundancy, also computing the set of redundant channels. The redundancy level of the network is also defined, proposing a theorem that supplies its value. This theory is developed on top of our necessary and sufficient condition for deadlock-free adaptive routing. Finally, a fault-tolerant routing algorithm for n-dimensional meshes is proposed.
[Algorithm design and analysis, n-dimensional meshes, fault-tolerant routing, message-passing, interconnection network reliability, Multiprocessor interconnection networks, deadlock, fault-tolerant systems, channel level, Fault tolerance, Intelligent networks, Computer network reliability, interconnection network, connectivity, Fault tolerant systems, wormhole networks, redundancy, channel redundancy, deadlock-free adaptive routing, Redundancy, Routing, continuous operations, fault-tolerant routing algorithm, multicomputers, System recovery, fault tolerant computing, Telecommunication network reliability]
Varietal hypercube-a new interconnection network topology for large scale multicomputer
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
The paper proposes a new interconnection network topology, called varietal hypercube for large scale multicomputer systems. An n-dimensional varietal hypercube is constructed by two (n-1)-dimensional varietal hypercubes in a way similar to that for the hypercube except for some minor modifications. The resulting network has the same number of nodes and links as the hypercube, and has most of the desirable properties of the hypercube, including recursive structure, partionability, strong connectivity, and the ability to embed other architectures such as ring and mesh. The diameter of the varietal hypercube is about two thirds of the diameter of the hypercube. The average distance of the varietal hypercube is also smaller than that of the hypercube. Optimal routing and broadcasting algorithms which guarantee the shortest path communication are developed. Comparisons with other variations of the hypercube, such as twisted cube, folded hypercube, and crossed cube, are also included.
[recursive structure, Costs, Multiprocessor interconnection networks, crossed cube, Routing, hypercube networks, Electronic mail, shortest path communication, strong connectivity, twisted cube, broadcasting algorithms, Network topology, large scale multicomputer, varietal hypercube, Broadcasting, Hypercubes, Computer networks, Large-scale systems, Communication networks, interconnection network topology, partionability, optimal routing, folded hypercube]
Delayed precise invalidation-a software cache coherence scheme
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Software-based cache coherence scheme is very desirable in scalable multiprocessor as well as massively parallel processor designs. We propose a software-based cache coherence scheme named delayed precise invalidation. The delayed precise invalidation is based on compiler time markings of references and a hardware-based local explicit invalidation of stale data in parallel and selectively. With a small amount of additional hardware and a small set of cache management instructions, the delayed precise invalidation provides more cacheability and allows invalidation of partial elements in an array, overcoming some of the inefficiencies and deficiencies of previous schemes. A correctness proof and a qualitative performance evaluation of the proposed scheme are also presented. Finally, the simulated cache hit ratios of the delayed precise invalidation and the parallel explicit invalidation scheme are given. Simulation results show that the delayed precise invalidation outperforms the parallel explicit invalidation scheme by 1O%.
[Process design, delayed precise invalidation, scalable multiprocessor, Delay effects, software cache coherence scheme, compiler time markings, hardware-based local explicit invalidation, Multiprocessing systems, Information analysis, parallel explicit invalidation scheme, Computer science, Design engineering, Runtime, massively parallel processor designs, Coherence, shared memory systems, Hardware, Large-scale systems, simulated cache hit ratios, cache management instructions]
Communication-efficient implementation of block recursive algorithms on distributed-memory machines
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
This paper presents a design methodology for developing efficient distributed-memory parallel programs for block-recursive algorithms such as the fast Fourier transform and bitonic sort. This design methodology is specifically suited for most modern supercomputers having a distributed-memory architecture with circuit-switched or wormhole routed mesh or hypercube interconnection network. A mathematical framework based on the tenser product and other matrix operations is used for representing algorithms. Communication-efficient implementations with effectively overlapped computation and communication are achieved by manipulating the mathematical representation using the tenser algebra. Performance results for FFT programs on the Intel iPSC/860 and Intel Paragon are presented.
[Algorithm design and analysis, Design methodology, Circuits, matrix operations, mathematical representation, supercomputers, parallel programs, Network topology, Fast Fourier transforms, tenser algebra, Computer architecture, Hypercubes, Hardware, distributed-memory machines, tenser product, design methodology, Intel iPSC/860, Intel Paragon, bitonic sort, hypercube interconnection network, Routing, FFT programs, fast Fourier transform, communication-efficient implementation, Tensile stress, overlapped computation, distributed memory systems, block recursive algorithms]
Causally ordering group communication protocol
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Distributed application systems require group communications among multiple processes. In the group communication, it is important to discuss in what order each process in the group can receive messages. The paper presents a causally ordering group communication (CO) protocol which provides the same causal ordering of messages for all the processes in the group. In the CO protocol, the messages received are causally ordered by using the sequence numbers of the messages. The CO protocol is based on the fully distributed control scheme, i.e. no master controller, and uses high-speed networks where each process may fail to receive messages due to the buffer overrun. Furthermore, the CO protocol provides asynchronous data transmission for multiple processes in the group.
[Transport protocols, asynchronous data transmission, sequence numbers, Laboratories, buffer overrun, Application software, high-speed networks, CO protocol, High-speed networks, causally ordering group communication protocol, multiple processes, fully distributed control scheme, Intersymbol interference, groupware, Distributed control, Data communication, Telecommunication network reliability, distributed application systems, Gas detectors, Clocks]
Implementation of a portable parallelizing compiler with loop partition
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
We have implemented a portable FORTRAN parallelizing compiler with loop partition on our experimental target system, Acer Altos 10000, running OSF/1 operating system. We have defined a minimal set of thread-related functions and data types, called B Threads, that is required to support the execution of this parallelizing compiler. Our compiler is highly modularized so that the porting to other platforms will be very easy, and it can partition parallel loops into multithreaded codes based on several loop partition algorithms. We have also proposed a general model of parallel compilers, which is an extension from previous model and is useful in constructing a parallelizing compiler for a particular language. The experimental results show that the best speedups are 3.75, 3.46, and 3.81 for matrix multiplication, adjoint convolution, and increasing workload sample, respectively, when the number of processors is four. It has been shown that this approach works and the experimental results are satisfied.
[Chaos, loop partition algorithms, B Threads, multithreaded codes, thread-related functions, Partitioning algorithms, Yarn, program compilers, Concurrent computing, matrix multiplication, Program processors, OSF/1 operating system, Convolution, Parallel programming, loop partition, Operating systems, National electric code, portable parallelizing compiler, Parallel processing, FORTRAN, Acer Altos 10000, data types, adjoint convolution]
Plenary Address 3: Heterogeneous Parallel Computing
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
false
[Concurrent computing, Power engineering computing, USA Councils, Parallel processing, Throughput, Application software]
An assertional proof of a lock synchronization algorithm using fetch and store atomic instructions
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
A new lock synchronization algorithm, proposed independently by Craig and the authors, not only eliminates memory contention caused by process spinning but also preserves first in first out property. A previous result, the MCS lock algorithm, requires both compare and swap and fetch and store instructions, or the FIFO property is lost and hence starvation may occur. The new one requires only fetch and store. We provide an assertional proof for the new algorithm. Most of behavioral proofs of concurrent programs are error-prone since it is difficult and tedious to take all possibilities of interleaving among the processes into consideration. An assertional proof replaces a large number of possibilities of interleaving by a small number of invariants. New techniques in this proof are: an assertional characterization of token bit accessibility; the definition of effective assignments that brings about the notion of token creation/destruction; the definition of token count that derives the mutual exclusion theorem; and the constructing procedure of a token-list that faithfully records the arrival time sequence of lock requests so that FIFO ordering can be enforced.
[token creation, first in first out, Stochastic processes, memory contention, concurrent programs, process spinning, processor scheduling, Multiprocessing systems, atomic instructions, token destruction, assertional proof, lock requests, shared memory systems, mutual exclusion theorem, FIFO property, error-prone, arrival time sequence, Data structures, MCS lock algorithm, lock synchronization algorithm, token bit accessibility, Computer science, token count, Interleaved codes, token-list, Spinning, fetch instructions, Queueing analysis, store instructions]
The design and implementation of the Pasda parallel file system
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
This paper presents a parallel file system, called Pasda, that not only addresses the problem of data rate mismatches between the requirements of an application, storage devices, and the inter-connection medium but also supports parallel file accesses in general purpose distributed systems. The Pasda (PArallel Storage Devices Accelerator) parallel file system uses a high-speed interconnection medium to provide high data transfer rates by driving multiple slower storage devices in parallel. A good parallel file accessing interface is also supported by Pasda. The user of Pasda can control the physical layout and partitioning of the file data, and can access files in parallel. An FDDI-based prototype of Pasda is constructed. The prototype provides faster data rates than access to the local SCSI disk.
[FDDI, local SCSI disk, data rate mismatches, Pasda parallel file system, File servers, general purpose distributed systems, high-speed interconnection medium, Application software, Distributed computing, Concurrent computing, Information science, parallel storage devices accelerator, File systems, System performance, Prototypes, high data transfer rates, Parallel processing, file organisation, parallel file accesses, parallel file accessing interface]
An efficient technique to remove transformations [program codes]
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Although the application of code transformations is critical to exploit parallelism in program code, few guidelines or tools are provided to determine what transformations should be applied and where they should be applied. In this paper, we approach this problem by first providing a taxonomy of code transformations to assist the user in parallelizing programs. We then present an efficient technique to remove transformations from the code when it is determined that they are ineffective or prevent more beneficial transformations from being applied. The technique to remove transformations employs inverse primitive actions, making it transformation independent. The technique uses the program dependence graph as the intermediate representation, making it language independent.
[intermediate representation, parallelising compilers, Taxonomy, code transformations, parallelism, inverse primitive actions, Parallel architectures, taxonomy, Application software, program code, Programming profession, Pipeline processing, Guidelines, Computer science, Concurrent computing, Program processors, Parallel processing, language independent, program dependence graph]
Designing general-purpose fault-tolerant distributed systems-a layered approach
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
General-purpose distributed systems comprised of computing nodes with different characteristics and connected by high-speed communication networks are very popular these days. The development of a dependable distributed system, however, necessitates the use of various techniques including fault tolerance to avert occurrences of failures or system malfunction. The ad hoc techniques of adding redundancy to improve reliability are not always suitable in these circumstances because of excessive design cost. Redundancies have to be allocated at various hardware and software levels in order to optimize their utilization in the system. This paper considers the design of general-purpose fault-tolerant distributed systems based on a layered approach. The benefits of the layered approach in the process of allocation of redundancy and fault tolerance at various system levels are presented and analyzed in the paper.
[Costs, Redundancy, distributed processing, Application software, Distributed computing, Computer science, Fault tolerance, Operating systems, layered approach, Fault tolerant systems, general-purpose fault-tolerant distributed systems, high-speed communication networks, system malfunction, Hardware, occurrences of failures, redundancy, Communication networks]
A parallel run-time iterative load balancing algorithm for solution-adaptive finite element meshes on hypercubes
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
To efficiently execute a finite element program on a hypercube, we need to map nodes of the corresponding finite element graph to processors of a hypercube such that each processor has approximately the same amount of computational load and the communication among processors is minimized. If the number of nodes of a finite element graph will not be increased during the execution of a program the mapping only needs to be performed once. However, if a finite element graph is solution-adaptive, that is, the number of nodes will be increased discretely due to the refinement of some finite elements during the execution of a program, a run-time load balancing algorithm has to be performed many times in order to balance the computational load of processors while keeping the communication cost as low as possible. In this paper, we propose a parallel iterative load balancing algorithm (ILB) to deal with the load imbalancing problem of a solution-adaptive finite element program. The proposed algorithm has three properties. First, the algorithm is simple and easy to implement. Second, the execution of the algorithm is fast. Third, it guarantees that the computational load will be balanced after the execution of the algorithm.
[parallel algorithms, Costs, solution-adaptive finite element meshes, run-time load balancing, Electronic mail, hypercubes, Finite element methods, Distributed computing, Computer science, Concurrent computing, Runtime, computational load, load imbalancing problem, finite element graph, Load management, Hypercubes, parallel run-time iterative load balancing algorithm, Iterative algorithms]
Exploiting communication latency hiding for parallel network computing: model and analysis
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Very large problems with high resource requirements of both computation and communication could be tackled with large numbers of workstations. However for LAN-based networks, contention becomes a limiting factor whereas latency appears to limit communication for WAN-based networks, nominally the Internet. We describe a model to analyze the gain of communication latency hiding by overlapping computation and communication. This model illustrates the limitations and opportunities of communication latency hiding for improving speedup of parallel computations that can be structured appropriately. Experiments show that latency hiding techniques increase the feasibility of parallel computing in high-latency networks of workstations across the Internet as well as in multiprocessor systems.
[Protocols, Scientific computing, wide area networks, parallel network computing, multiprocessor systems, parallel computations, Delay, Multiprocessing systems, WAN-based networks, Concurrent computing, workstations, high-latency networks, Parallel processing, communication latency hiding, Computer networks, high resource requirements, Workstations, Internet, IP networks, Local area networks]
A flexible service development support system for communication systems by reuse methodology
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
To flexibly develop services on communication systems, support for specification description is more important than others in development processes. We have developed a specification description language HSC for communication systems. In this paper, we propose a new design method, and its support environment. The method and system effectively utilize the features of HSC and also apply the CBR technique for reusing service specifications being developed. We have experimented with service specifications of communication systems by using this system. Through these experiments, the effectiveness of this method and its support environment is ensured.
[Productivity, expert systems, communication systems, Cable TV, CBR technique, flexible service development support system, reuse methodology, Proposals, Communication system software, Telephony, specification description language HSC, specification description, Expert systems, Software reusability]
Reducing procedure call overhead: optimizing register usage at procedure calls
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Proposes a common global variable reassignment and an integrated approach which takes advantage of the complementary relationship of (1) in-lining and (2) interprocedural register allocation to reduce the procedure call overhead without causing any additional negative effect. Our approach is based on the observation of analyzed program characteristics to identify the heavily called procedure regions, and on register usage information to optimize the placement of resister save/restore code. This method also takes full advantage of free-use registers at each procedure call site. The average performance improvement is 1.233 compared with previous schemes that performed either (1) or (2) independently.
[Automatic programming, in-lining, Registers, register usage optimization, performance improvement, Information analysis, complementary relationship, Computer science, common global variable reassignment, Runtime, Boolean functions, heavily called procedure regions, free-use registers, interprocedural register allocation, Benchmark testing, procedure call overhead, analysed program characteristics, resister save/restore code placement, Libraries, subroutines, Size control]
Optimizing entity join queries by extended semijoins in a wide area multidatabase environment
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
We consider processing entity join queries in a wide area multidatabase environment where the query processing cost is dominated by the cost of data transmission. An entity join operation "integrates" tuples representing the same entities from different relations in which inconsistent data may exist. The semijoin technique has been successfully used in a distributed database system to reduce the cost of data transmission. However, it cannot be directly applied to process the entity join query. An extension of the traditional semijoin, named extended semijoin is proposed to reduce the cost of data transmission for entity join query processing in a wide area multidatabase environment.
[Wide area networks, Costs, entity join query optimization, extended semijoins, entity join queries, query processing cost, Delay, Computer science, query optimization, Query processing, Councils, System performance, wide area multidatabase environment, distributed databases, data transmission, Database systems, entity join operation, Data communication, semijoin technique, Contracts, inconsistent data, distributed database system]
Experiments on high-priority cold requests in the presence of tree saturation
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
In large-scale shared memory multiprocessors, when a multistage interconnection network (MIN) is used for communication between processors and memory modules, hot spot and tree saturation severely delay memory requests and degrade memory bandwidth. We propose the Cold-First scheme, which is based on priority control and virtual channel flow control concepts, to reduce the delay of cold requests in the presence of hot spots. By simulations and results, we show that Cold-First scheme reduces the delay of memory requests, especially the delay of cold requests, and improves the memory bandwidth. In addition, we study the effect caused by the long delay of hot requests on lock and unlock mechanisms generally used for synchronization.
[Multiprocessor interconnection networks, tree saturation, Communication system control, Switches, Telecommunication traffic, hot spot, Degradation, high-priority cold requests, Intelligent networks, priority control, Bandwidth, Traffic control, shared memory systems, Large-scale systems, virtual channel flow control concepts, memory modules, unlock mechanisms, Delay effects, multistage interconnection network, memory requests, lock mechanisms, Cold-First scheme, memory bandwidth, MIN, synchronization, large-scale shared memory multiprocessors]
Storage design and retrieval of continuous multimedia using multi-disks
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
In the domain of multimedia applications, continuous display is an important issue. In this paper, we present a practical method to allocate disk storage for multimedia data so that continuous requirement can be met. This technique explores data-transfer parallelism on a multidisk system. Moreover, in order to ensure that continuous retrieval can be achieved in a multiuser environment, we propose the dynamic scheduling mechanism for real-time object retrieval. It can be seen that, a good scheduling can explore higher access concurrency in display of multimedia applications. Several approaches with different trade-off based upon this mechanism are proposed in this paper, which include delay initiation, read-ahead, migration, segmentation and integration.
[delay initiation, multimedia computing, storage design and retrieval, read-ahead, Delay, Concurrent computing, continuous multimedia, Bandwidth, segmentation, multidisk system, real-time object retrieval, multiuser environment, multi-disks, multimedia data, Multimedia systems, disk storage, Information retrieval, Dynamic scheduling, Application software, Auditory displays, Computer science, Computer displays, dynamic scheduling mechanism, access concurrency, migration, integration, data-transfer parallelism]
Mapping pyramids into 3-D meshes
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Embedding one parallel architecture into another is very important in the area of parallel processing because parallel architectures can vary widely. Given a pyramid architecture of (4/sup N/-1)/3 nodes and height N, this paper presents a mapping method to embed the pyramid architecture into a 2/sup N-1-k//spl times/2/sup N-1-k//spl times/(4/sup k+1/+2)/3 mesh for 0/spl les/k/spl les/N-1. Our method has dilation max{4/sup k/, 2/sup N-2-k/} and expansion 1+2/(4k+1). When setting k=(N-2)/3, the pyramid can be embedded into a 2/sup (2N-1//3)/spl times/2/sup (2N-1//3)/spl times/[4/sup (N+1//3)+2]/3 mesh, and it has dilation and expansion 1+2/[4/sup (N+1//3)]. This result has can optimal expansion when N is sufficiently large and is superior to the previous mapping methods in terms of the same gauges.
[Embedded computing, Image processing, parallel architectures, mapping pyramids, parallel architecture, Parallel architectures, Information management, Parallel algorithms, parallel processing, Concurrent computing, pyramid architecture, Computational geometry, Tree graphs, 3-D meshes, Computer architecture, Parallel processing]
Extracting the parallelism in program with unstructured control statements
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Program parallelization is inhibited by unstructured control statements such as GOTOs, causing interacting and overlapping execution trajectories. In this contribution, a program restructuring method is proposed to convert unstructured control statements into block if statements and while loops. Furthermore, an algorithm is presented to transform a common type of while loops into do loops. The technique works for while loops of which the control variables satisfy a linear recurrence relation. As a result, the loop carried dependencies generated by the control variables are removed. If there are no other loop carried dependencies, the do loop may then be converted into a doall loop. The algorithm has been used to test and convert a significant number of while loops into doall loops for a suite of well-known numerical benchmarks.
[Algorithm design and analysis, program restructuring method, linear recurrence relation, Data mining, Flow graphs, while loops, Logic testing, parallel programming, do loops, unstructured control statements, Ear, program parallelization, Benchmark testing, Electric variables control, GOTOs, parallelism in program]
Evaluation of relaxed memory consistency models for multithreaded multiprocessors
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Stochastic timed Petri nets are developed to evaluate the relative performance of distributed shared memory models for scalable multithreaded multiprocessors. The shared memory models evaluated include the Sequential Consistency (SC), the Weak Consistency (WC), the Processor Consistency (PC) and the Release Consistency (RC) models. Under saturated conditions, we found that multithreading contributes more than 50% of the performance improvement, while the improvement from memory consistency models varies between 20% to 40% of the total performance gain. Our analytical results reveal the lowest performance of the SC model. The PC model requires to use larger write buffers and may perform even lower than the SC model if a small buffer was used. The performance of the WC model depends heavily on the synchronization rate in user code. For a low synchronization rate, the WC model performs as well as the RC model. With sufficient multithreading and network bandwidth, the RC model shows the best performance among the four models.
[memory consistency models, Optical wavelength conversion, Stochastic processes, write buffers, Predictive models, performance evaluation, multithreaded multiprocessors, network bandwidth, Yarn, stochastic timed Petri nets, Delay, Pipeline processing, relaxed memory consistency models, Multithreading, distributed shared memory models, Weak Consistency, Processor Consistency, shared memory systems, Hardware, Frequency synchronization, Release Consistency, Sequential Consistency, Context modeling]
A study of cache hashing functions for symbolic applications in micro-parallel processors
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
This paper presents a study of cache hashing functions for micro-parallel processors (e.g., superpipeline and super-scalar processors). Several novel cache hashing functions are experimented. Our simulation results show that an unconventional cache hashing function applied on a direct-mapped cache results in hit rates as good as a two-way set associative cache with traditional mapping, while the cache hit times are as fast as a direct-mapped cache with traditional mapping.
[Computational modeling, Laboratories, micro-parallel processors, cache storage, Application software, VLIW, direct-mapped cache, two-way set associative cache, superpipeline processors, Runtime, Computer architecture, symbolic applications, cache hashing functions, super-scalar processors]
A distributed memory multiprocessor implementation of C-with-Ease
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
C-with-Ease is a superset of C, with primitives borrowed from Ease for process creation and communication. All communication is via distributed data structures called contexts. We present an efficient implementation of C-with-Ease for the Fujitsu AP1000, a distributed memory, message passing multiprocessor. We use two of the three AP1000 communication networks: the torus net for point to point communication and the broadcast net to maintain a global view of processor allocation.
[Context, broadcast net, Fujitsu AP1000, torus net, Data structures, contexts, Application software, processor allocation, Programming profession, Computer science, Concurrent computing, point to point communication, Message passing, distributed memory multiprocessor implementation, distributed data structures, Broadcasting, message passing multiprocessor, Libraries, data structures, Communication networks, C-with-Ease]
Experiences in session layer conformance testing
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
A protocol standard can be implemented by different manufacturers. Therefore, conformance testing of protocol implementations to standards is important to achieve the main purpose of the open systems interconnection. In this paper, after briefly introducing the session layer protocol and the conformance testing methodology we depict important errors found in the session layer implementations and the shortcomings of the test system.
[System testing, Protocols, ISO, session layer protocol, Laboratories, session layer conformance testing, open systems interconnection, Telecommunication standards, LAN interconnection, Electronic mail, conformance testing, Sun, Information technology, Open systems, protocol standard]
A heuristic algorithm for the reliability-oriented file assignment in a distributed computing system
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
We develop a heuristic algorithm for the reliability-oriented file assignment problem (HROFA), which uses a careful reduction method to reduce the problem space. Based on some numerical results, the HROFA algorithm obtains the exact solution in most cases and the computation time is improved significantly. When it fails to give an exact solution, the deviation from the exact solution is very small.
[Heuristic algorithms, Very large scale integration, distributed processing, Reliability engineering, Throughput, reliability-oriented file assignment, reliability-oriented file assignment problem, Distributed computing, distributed computing system, Computer science, heuristic algorithm, Space technology, Microprocessors, Distributed control, Space exploration, HROFA algorithm]
A fast switching double processing architecture for multi-tasking real-time systems
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
A new fast switching double processing architecture for pipelined cache-based real-time computer systems is proposed to reduce the CPU stalls due to increased cache misses resulting from frequent task switching in multi-tasking real-time applications. In this architecture, two sets of registers are provided so that two tasks can be executed alternatively on a cycle-by-cycle basis. This architecture helps alleviate the problem of unpredictable cache performance due to frequent context switches in multi-tasking systems. The performance of the double processing is evaluated first through trace driven simulation for various cache configurations. An analytical performance model is then derived to further explain the performance advantage.
[Real time systems, cache misses, parallel architectures, Pipelines, trace driven simulation, Switches, analytical performance model, fast switching double processing architecture, Registers, Application software, Analytical models, Reduced instruction set computing, context switches, Computer architecture, pipelined cache-based real-time computer systems, Interleaved codes, registers, Performance analysis, multi-tasking real-time systems]
A tree-based distributed algorithm for the K-entry critical section problem
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
We present a token-based algorithm for solving the K-entry critical section problem. Based on Raymond's (1989) tree-based approach, we regard the nodes as being arranged in a directed tree structure, and all messages used in the algorithm are sent along the directed edges of the tree. There are K tokens in the system; we use a bag structure at each node to record the collection of the neighboring nodes, possibly with multiple occurrences of the same node, through which the K tokens can be located. As a result, there are K paths from each node leading to the K tokens in the system. Our algorithm requires at most 2 KD messages for a node to enter the CS, where D is the diameter of the tree. Therefore, when the diameter D is much smaller than N, the number of nodes, e.g. D=O(1) as in a star or D=O(logN) as in a binary tree, our algorithm's upper bound on the number of messages per CS is smaller than those previously reported.
[Tree data structures, directed edges, tree-based approach, tree-based distributed algorithm, directed tree structure, Data structures, upper bound, K-entry critical section problem, Sliding mode control, multiple occurrence, Computer science, neighboring nodes, Upper bound, distributed algorithms, Binary trees, Permission, tree diameter, bag structure, Distributed algorithms, token-based algorithm, K paths]
Avoiding data link and computational conflicts in mapping nested loop algorithms to lower-dimensional processor arrays
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
This paper describes a unified approach to checking data link and computational conflicts in mapping algorithms to lower-dimensional processor arrays. Based primarily on the notion of Hermite normal form, we propose a range of necessary and sufficient conditions to identify mappings without data link and computational conflicts. These conditions are then used to find optimal time mappings of a transitive closure algorithm to linear processor arrays.
[parallel algorithms, transitive closure algorithm, Terminology, lower-dimensional processor arrays, Mathematics, data link, Statistics, nested loop algorithms, Sufficient conditions, mapping algorithms, Difference equations, computational conflicts, optimal time mappings, Testing]
CCT: a new VLSI architecture for parallel processing
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
We propose a VLSI implementable architecture called Cube Connected Tree having advantages of both trees and hypercubes. This structure has fixed low degree nodes for any size of network, unlike hypercubes, where the node degree is dependent on the size of the hypercube. Complexity of VLSI layout of this structure has been addressed within the grid model of C.D. Thompson (1984). By using spare links and PE's, fault-tolerance capabilities of the system has been enhanced. Easy programmability of this structure has been demonstrated by designing polyalgorithmic algorithms for sorting and discrete Fourier transform.
[Algorithm design and analysis, complexity, VLSI implementable architecture, Very large scale integration, fault-tolerance capabilities, hypercube networks, parallel processing, programmability, polyalgorithmic algorithms, Sorting, Radiofrequency interference, discrete Fourier transform, node degree, grid model, Fault tolerance, Fault tolerant systems, CCT, spare links, Binary trees, Cube Connected Tree, sorting, Parallel processing, Hypercubes, PEs]
A graph model for investigating memory consistency
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
The complexity of a multiprocessor memory system grows with the endeavors people make to improve the performance. The pseudo and real execution graphs introduced here can formally describe the complex event ordering behavior of the multiprocessor memory system and to verify the correctness of a parallel program under a consistency model. A pseudo execution graph represents the programmer's abstraction of an execution in which memory accesses are simple, atomic operations. A loop in the pseudo execution graph indicates an incorrect execution. A real execution graph represents the hardware designer's abstraction of an execution in which each memory access is a causal sequence of events. A loop in the real execution graph indicates that this execution is impossible to occur. A program is correct if all loops in the pseudo execution graphs cause loops in the corresponding real execution graphs.
[Computers, pseudo execution graph, Costs, Scalability, memory consistency, complex event ordering behavior, real execution graph, multiprocessor memory system complexity, Delay, atomic operation, graph model, Parallel processing, Hardware, fault tolerant computing, consistency model, parallel program correctness]
A new and efficient FFT algorithm for distributed memory systems
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
This paper presents a new and optimal parallel implementation of multidimensional fast Fourier transform algorithm on distributed memory multiprocessors. Its optimality is obtained by minimizing the number of message passings necessary, at the cost of increase in message length. This distinctive feature of the new algorithm effectively utilizes the important architectural property of most of today's distributed memory multiprocessors-wormhole routing for interprocessor communications. By using the algebra of stride permutations and tenser products as a mathematical tool, we are able to derive and formulate an efficient data partition and communication scheme that reduces communication cost from O(N/sup 2/) required for the best known FFT to O(N) on an N/sup 2/-processor machine. Our data partition scheme is natural and efficient for solving discretized boundary value problems such as partial differential equations and finite element analysis. To evaluate the actual performance of our new algorithm in comparison with other existing parallel FFT algorithms, we have carried out implementation experiments on the Intel's Touchstone Delta machine.
[data partition scheme, Partial differential equations, Finite element methods, permutations, discretized boundary value problems, Fast Fourier transforms, Algebra, message passings, interprocessor communication, Cost function, mathematical tool, Multidimensional systems, Boundary value problems, performance evaluation, Routing, optimal parallel implementation, Partitioning algorithms, finite element analysis, tenser products, wormhole routing, FFT algorithm, Message passing, Intel's Touchstone Delta machine, distributed memory systems, partial differential equations]
Simulation and performance evaluation of a modularly configurable attached processor
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
A new architecture for high-performance parallel attached processors is studied in this paper. The unique features are that the attached processor can be configured to match a set of algorithms and its memory controllers can be programmed to fit the access patterns required by the algorithms. As a result, high utilization of the processing logic for given sets of algorithms can be obtained. A simulator with interactive graphic interface is designed to study the performance of the proposed architecture. An example based on matrix multiplication is used for illustration. The simulation results show that a sustained execution rate as high as 95% of the peak speed for matrices with a size of 128/spl times/128 can be achieved in the proposed attached processor architecture. If CMOS technology is chosen to implement the MCAP architecture, a sustained speed of 190 MFLOPS can be obtained for matrix multiplication with four multipliers and four adders.
[Algorithm design and analysis, Computational modeling, modularly configurable attached processor, simulation, performance evaluation, CMOS process, access patterns, memory controllers, Graphics, processing logic, Operating systems, Computer architecture, CMOS technology, parallel attached processors architecture, Logic, interactive graphic interface, Pattern matching, Arithmetic]
A basis approach to loop parallelization and synchronization
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Loop transformation is a crucial step in parallelizing compilers. We introduce the concept of positive coordinate basis for deriving loop transformations. The basis serves to find proper loop transformations to change the dependence vectors into the desired forms. We demonstrate how this approach can, systematically extract maximal outer loop parallelism. Based on the concept, we can also construct a minimal set of synchronization vectors, which are deadlock free, to transform the inner serial loops into doacross loops.
[dependence vectors, positive coordinate basis, parallelising compilers, Law, loop parallelization, parallelizing compilers, maximal outer loop parallelism, loop transformation, synchronization vectors, deadlock free, Concurrent computing, Computer science, doacross loops, Councils, Message passing, Parallel processing, System recovery, Signal processing, synchronization, inner serial loops, Contracts, Legal factors]
Parallel design of Q-coders for bilevel image compression
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
A parallel algorithm is presented in this paper to implement the adaptive binary arithmetic coding for lossless bilevel image compression. Based on the sequential Q-coder, software analysis in C is carried out to establish a tree array to process 4 bits in parallel. This development of parallel Q-coder substantially improves the encoding speed of bilevel images. As a matter of fact, the parallel algorithm can also be extended theoretically to any number of bits to be processed in parallel. The implication involved will be the design of internal structure for each PE, especially the buffer size where each bit renormalized locally is to be updated by the PE at the next level before it is sent out at the top of the tree array.
[data compression, tree array, Data compression, parallel algorithm, Predictive models, encoding speed, C language, Parallel algorithms, bilevel image compression, bilevel images, Image coding, software analysis, Signal processing algorithms, arithmetic coding, Parallel processing, Q-coders, Image storage, Logic, buffer size w, Arithmetic, Indexing]
Plenary Address 2: Computing in the '90s, Microsoft, and Supercomputers
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
false
[Computer industry, Supercomputers]
Toward semantic-based parallelism in production systems
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
We propose a new approach for the parallel execution of production system programs. This approach embodies methods of decomposition abstraction using declarative mechanisms. Application semantics can then be exploited to achieve a much higher degree of concurrency. We present the underlying object-based framework of production systems and discuss the ensuing semantic-based dependency analysis technique. In particular, we define a new notion of functional dependency to characterize associative relationships among data objects, which can be used to determine concurrently executable rules.
[Production systems, concurrently executable rules, decomposition abstraction, production systems, semantic-based parallelism, Interference, parallel execution, semantic-based dependency analysis, declarative mechanisms, functional dependency, Application software, Engines, parallel programming, concurrency, Concurrent computing, Runtime, Databases, application semantics, Failure analysis, Parallel processing, Large-scale systems, object-based framework, associative relationships]
A new approach for finding loop transformation matrices
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Traditional approach for generating loop transformation matrix, which is based upon the computation of distance vectors or direction vectors, does not work for those nested loops whose distance vectors are uncomputable and direction vectors contain no useful information. In this paper, we present a new technique for generating transformation matrix that is based upon identifying certain types of linear equations or inequalities of distance vectors. Two issues related to this technique are discussed in this paper: 1) Given a nested loop how to identify these linear equations or inequalities; 2) Given such a linear equation or inequality how to generate a legal and unimodular transformation matrix for the purpose of loop parallelization.
[programming theory, inequalities, Law, loop parallelization, Geophysics, Parallel machines, Vectors, Linear matrix inequalities, direction vectors, Equations, unimodular transformation matrix, nested loops, loop transformation matrices, distance vectors, Legal factors, linear equations]
Using the imprecise-computation technique for congestion control on a real-time traffic switching element
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
The broadband integrated services digital network provides communication services with different requirements, including real-time services such as voice and video. Real-time services are affected by the probabilistic behavior of such a network. In particular, when the network becomes congested, the end-to-end packet delay may exceed the maximum allowed. Fortunately, many real-time services are willing to trade service quality for information timeliness. The imprecise-computation technique, in combination with layered coding schemes, makes this tradeoff possible.
[voice, Fluctuations, service quality, B-ISDN, imprecise-computation technique, Telecommunication traffic, information timeliness, Jitter, congestion control, end-to-end packet delay, layered coding schemes, video, Delay, Communication switching, Computer science, broadband integrated services digital network, real-time traffic switching element, Communication system traffic control, Intserv networks, probabilistic behavior, Asynchronous transfer mode]
Distributed routing schemes for strictly nonblocking networks
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Strictly nonblocking networks may hold the key for high performance multiprocessor systems. While a number of strictly nonblocking networks have been reported in the literature, their use in multiprocessors is hampered by a lack of efficient distributed routing at algorithms to set paths in these networks. As a step in this direction, the paper presents two distributed routing algorithms for D.G. Cantor's (1971) strictly nonblocking network. For N inputs, the first routing algorithm takes O(t)+O(logt log N) steps/sup 1/ to routing t requests in parallel. While this algorithm performs quite well for i=O(log/sup 2/ N), for larger valves of t, we present a randomized version of the same algorithm with an expected time complexity of O(log/sup 2/ N) for any number of requests. These results, when combined with the crosspoints and depth complexities of a Cantor network, give a strictly nonblocking network with O(N log/sup 2/ N) crosspoints, O(logN) depth and O(log/sup 2/N) routing time.
[Multiprocessor interconnection networks, distributed routing schemes, multiprocessor interconnection networks, Switches, Fasteners, time complexity, Routing, Educational institutions, Data structures, Cantor network, Multiprocessing systems, crosspoints, Tree graphs, High performance computing, Computer networks, strictly nonblocking networks, depth complexities, high performance multiprocessor systems, randomized version]
An object-oriented model for distributed system management
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
The design and the architecture of the management service in the Guide object-oriented distributed system is presented. First, we discuss general problems of distributed system management. Then we propose the use of persistent objects to specify system, configuration and the different interfaces these objects need to bind in order to allow the control and the monitoring of managed entities. After that we discuss the need for several levels of representation of management information. And finally we discuss the use of database services and data replication to ensure the availability, the reliability, and the consistency of these objects.
[Availability, Guide, Object oriented modeling, Computerized monitoring, configuration, object-oriented model, reliability, Control systems, distributed system management, consistency, Distributed processing, Computer network reliability, data replication, Information security, distributed databases, management information, Hardware, Resource management, Computer network management, persistent objects]
A kernel-level DSVM controller for the diskless cluster system
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Investigates the distributed shared virtual memory (DSVM) technique in a distributed diskless PC cluster system environment. By integrating distributed shared memory (DSM) and virtual memory functions, the DSVM model not only supports the sharing of process space but also provides the sharing of system-supported space (e.g. swapping area). DSVM controllers are handlers in a diskless PC cluster system whose responsibility is to control the operations of DSM and to handle the sharing when the local physical memory needs to do page/process swap-in and out. This paper presents the model, describes the requirements and how the model supports the diskless PC in a diskless PC cluster system, discusses several design issues, and narrates prototype implementation details and considerations. The developed primitives of this prototype are the base of a complete DSVM mechanism and process migration. Integrating the two mechanisms will promote the system to be a powerful group computing platform.
[distributed shared memory, process space sharing, Control systems, File servers, kernel-level DSVM controller, page swapping, Distributed computing, process migration, Space technology, Physics computing, Operating systems, Prototypes, process swapping, system-supported space sharing, Power system modeling, distributed diskless PC cluster system environment, swapping area, prototype implementation, Memory management, design issues, group computing platform, distributed memory systems, distributed shared virtual memory technique, local physical memory, Personal communication networks]
Load balancing in pipelined processing of multi-join queries
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Looks at how to effectively exploit pipelining for multi-join queries in shared-nothing systems. A multi-join query can be processed using an iterative approach. In each iteration, several relations are selected and are joined in a pipelined fashion. However, algorithms that are based on this approach have traditionally assumed that the relations are uniformly distributed or only slightly skewed. When this assumption is relaxed, i.e. when the data is skewed, some nodes may be assigned a larger amount of data than can fit into their memories. As such, pipelining cannot be effectively exploited, and performance may degenerate drastically. We propose four skew handling techniques to deal with data skew for multi-join queries. The results of a performance study show that a hybrid technique is superior in most cases.
[multi-join queries, load balancing, pipelined processing, shared-nothing systems, performance degeneration, Pipeline processing, Information systems, Computer science, relation joining, System performance, distributed databases, iterative approach, Load management, Iterative algorithms, Database systems, Iterative methods, Communication networks, hybrid technique, data skew]
Multicast communication in 2-D mesh networks
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Multicast refers to the message delivery from a source node to an arbitrary number of destination nodes in a communication network. The 2D mesh topology has become increasingly popular as interconnection network for multicomputers and distributed systems. Two multicast algorithms are proposed for 2D mesh networks. The computational complexity of the algorithms is analyzed. The performance of the proposed algorithms is evaluated by intensive simulations. A comparison between the proposed algorithms and two existing algorithms is given.
[Algorithm design and analysis, communication network, Integrated circuit interconnections, multiprocessor interconnection networks, Telecommunication traffic, Multicast communication, destination nodes, Delay, Switching circuits, Intelligent networks, Mesh networks, Multicast algorithms, Network topology, interconnection network, multicast algorithms, multicomputers, multicast communication, message delivery, distributed systems, 2D mesh topology, 2-D mesh networks, source node, computational complexity]
An optimal fault-tolerant design approach for array processors
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
A systematic approach for designing fault tolerant systolic array using space/time redundancy is proposed. The approach is based upon a fault tolerant mapping theory which relates space-time mapping and concurrent error detection techniques. By this design approach, the resulting systolic array is fault tolerant and optimal. Besides, it has the capability to compute more problem instances simultaneously without extra cost.
[Costs, Redundancy, concurrent error detection, optimal fault-tolerant design approach, fault tolerant mapping theory, systolic arrays, Log periodic antennas, space/time redundancy, Computer science, Fault tolerance, Information science, Runtime, array processors, Fault detection, systolic array, Systolic arrays, Error correction]
Generalising the unimodular approach [program code transformation]
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Most of the available parallelism in source code is contained in loops and is exploited by applying a sequence of loop transformations. Different methods of representing and ordering sequences of transformations have been developed, including the use of unimodular transformations, which unify loop permutation, loop reversal, and loop skewing of perfectly nested loops. This paper presents three extensions to the unimodular approach that make it applicable to a wider range of source code structures. First, the unimodular transformations are extended to represent additional loop transformation techniques, namely loop fission, loop fusion, loop blocking (tiling), strip mining, cycle shrinking, loop coalescing, and loop collapsing. Second, the application of unimodular transformations is generalized to handle both perfectly and imperfectly nested loops. Third, attractive properties of the original unimodular transformations are preserved by the generalized model.
[cycle shrinking, Strips, strip mining, tiling, loop reversal, Scheduling, program code transformation, program compilers, loop skewing, perfectly nested loops, Computer science, unimodular approach, loop permutation, loop collapsing, source code parallelism, loop fission, Linear algebra, Parallel processing, Traffic control, loop blocking, loop transformations, loop coalescing, Kernel, loop fusion]
On the embedding of a class of regular graphs in a faulty hypercube
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
A wide range of graphs with regular structures are shown to be embeddable in an injured hypercube with faulty links. These include rings, linear paths, binomial trees, binary trees, meshes, tori, and many others. Unlike many existing algorithms which are capable of embedding only one type of graphs, our algorithm embeds the above graphs in a unified way, all centered around a notion called edge matrix. In many cases, the degree of fault tolerance offered by the algorithm is optimal or near-optimal.
[binary trees, Embedded computing, fault tolerance, binomial trees, tori, injured hypercube, faulty links, regular graphs, linear paths, Computer science, Concurrent computing, Fault tolerance, Tree graphs, rings, Fault tolerant systems, Binary trees, Computer architecture, faulty hypercube, Hypercubes, Computer networks, fault tolerant computing, embedding, meshes]
Parallel implementation of the trie structure
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
We present two simple algorithms for implementing the trie structure based on the MIMD (Multiple Instruction Streams Multiple Data Streams) model of computation. Unlike the implementation, our schemes allow operations on the trie structure by multiple processes. No hash function will be employed and the problem due to collision is thus, eliminated. By making use of a simple segmentation scheme. We are able to garbage collect and recycle unused memory space in an efficient manner. Results from simulation show that our proposed algorithms attain satisfactory speedup.
[Tree data structures, Dictionaries, Computational modeling, parallel implementation, unused memory space, Data structures, Information retrieval, garbage collection, segmentation scheme, Information systems, Computer science, Computer aided instruction, trie structure, MIMD, Natural language processing, Multiple Instruction Streams Multiple Data Streams, Recycling, tree data structures]
Extending Vienna Fortran with task parallelism
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Vienna Fortran supports a wide range of data-parallel numerical problems. However, a significant number of scientific and engineering applications are of a multi-disciplinary and heterogeneous nature and thus do not fit well into the data parallel paradigm. In this paper we present new language extensions to fill this gap. Tasks can be spawned as asynchronous activities in a homogeneous or heterogeneous computing environment; they interact by sharing access to Shared Data Abstractions (SDAs). SDAs are an extension of Fortran 90 modules, representing a pool of common data, together with a set of methods for controlled access to these data and a mechanism for providing persistent storage. These extensions support the integration of data and task parallelism and can be used to express task parallel applications in a natural and efficient way.
[Parallel languages, persistent storage, task parallel applications, parallel architectures, NASA, Software performance, Data engineering, Parallel architectures, Application software, Radio access networks, task parallelism, Space technology, Vienna Fortran, Shared Data Abstractions, Fortran 90 modules, asynchronous activities, Parallel processing, data-parallel numerical problems, Contracts]
Extracting multi-thread with data localities for vector computers
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
In this paper, we propose a source-to-source compilation strategy to partition vectorized loop programs into multithread execution form. Each partitioned thread consists of instances of statements with localities in vector registers. The multi-threading scheme gives a novel combination of loop unrolling, statement instances reordering, index shifting, vector register reuse exploiting, and multi-threading. Experimental results show that our multithreading scheme assists vector compiler of Convex C38 series to reduce the number of memory accesses and the number of synchronizations among CPUs and usually obtains a better performance.
[index shifting, Convex C38 series, Optimizing compilers, vectorized loop programs, multithread execution, Data engineering, Registers, Data mining, Yarn, synchronisation, Concurrent computing, Computer science, loop unrolling, Program processors, data localities, performance, Memory management, statement instances reordering, source-to-source compilation strategy, Parallel processing, vector computers, vector register reuse exploiting]
A new approach of constructing information mutual exclusion in distributed systems
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
We present a decentralized, symmetric mutual exclusion algorithm in a computer network. The proposed algorithm has the same message complexity as that of the Maekawa's O(/spl radic/(N)) mutual exclusion algorithm ( Maekawa, 1985) and can be applied to arbitrary sizes of distributed systems. It is more suitable than Maekawa's when the symmetry of a system is a criticism. In addition, our algorithm has smaller request set size than that of Gupta, Bruell and Ghosh's (1987) mutual exclusion algorithm on a system of size 2/sup n/ for some integer n.
[information mutual exclusion, decentralized symmetric mutual exclusion algorithm, integer, communication complexity, Distributed computing, smaller request set size, computer network, Intelligent networks, Information science, message complexity, arbitrary size, Computer architecture, Permission, Hypercubes, distributed systems, Computer networks, Clocks]
A mesh partitioning tool and its applications to parallel processing
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
This paper describes the features and implementation of a mesh partitioning tool called PSAINT and its applications to parallel processing research and education. PSAINT is an interactive graphics program with a friendly interface for user-program interaction. It offers several automatic mesh partitioning algorithms as well as a set of graphics tools for manual partitioning and for evaluation and modification of the results of automatic partitioning. The program automatically generates various statistics results and allows for visualization of the mesh partitions. PSAINT was originally developed as a key component in an integrated parallel finite element analysis system. Recently, it has been extended to facilitate evaluation and comparative studies of automatic mesh partitioning algorithms. It has also been used to aid the teaching in a graduate-level structural engineering course for parallel processing.
[Educational programs, Visualization, PSAINT, visualization, user-program interaction, automatic mesh partitioning algorithms, mesh partitioning tool, Partitioning algorithms, Structural engineering, graphics tools, integrated parallel finite element analysis system, Finite element methods, Statistics, parallel processing, interactive graphics program, Graphics, statistics result, Education, Parallel processing, Mesh generation, graduate-level structural engineering course]
Implementation of a tree-structured vector quantizer for image compression on the MasPar MP-1 parallel machine
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
The transmission of digitized images over limited bandwidth channels motivates the use of data compression techniques. Many data compression techniques are not suitable for such applications because compression ratios of more than 20:1 are often required. One technique that can provide this level of compression is vector quantization. The processes of codebook generation and, especially, encoding and decoding are tasks well suited for execution on a massively parallel machine. For codebook generation, an SIMD algorithm is developed whose control flow is based on sequencing through the training data, rather than the tree structure, to achieve improved performance. Results from execution on the 16384 processor MasPar MP-1 SIMD machine are presented. The approaches taken could be adapted for other SIMD as well as MIMD machines.
[Laboratories, Data compression, image compression, Image coding, control flow, Training data, compression ratios, Bandwidth, limited bandwidth channels, Parallel processing, tree-structured vector quantizer, data compression, Vector quantization, Parallel machines, Encoding, Decoding, SIMD machines, encoding, decoding, vector quantisation, massively parallel machine, performance, MasPar MP-1 parallel machine, codebook generation, vector quantization, MIMD machines]
Invited Mini-review Session 1: Object Technology And Distributed Operating Systems
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
false
[]
Probabilistic timed protocol verification for the extended state transition model
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
We propose a Timed Communicating State Machine (TCSM), which belongs to the extended state transition model, to formally specify protocols that incorporate timed properties as part of their specifications. Based on the TCSM model we also propose (1) a timed global state reachability analysis that takes time bounds and predicates into consideration, and (2) a probabilistic timed verification scheme that is based on the occurrence rates of communicating entities' transitions and the occurrence probabilities of channel entities' transitions. In this way, probability-based partial timed verification can be achieved for extended-state-transition-specified timed protocols.
[Protocols, extended state transition model, Probability, occurrence rates, Explosions, State-space methods, probabilistic timed protocol verification, Reachability analysis, formal specification, timed global state reachability analysis, Councils, Automata, Timed Communicating State Machine, protocols, occurrence probabilities, Context modeling]
Load balancing and query optimization in dataflow parallel evaluation of Datalog programs
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
A dataflow model to obtain parallelism in the evaluation of Datalog is presented. This model performs query evaluation as a dataflow through a network of communicating concurrent processes capable of solving the query. This process network is based on the intensional database definition, plus the concrete query to be evaluated. A cost model to cope with the load balancing problem is described. A load balancing algorithm is presented and discussed. An algorithm to optimize the evaluation is described which is based on process network rewriting. This utilizes information in the query bindings to be evaluated in order to optimize the dataflow graph.
[Performance evaluation, load balancing, intensional database definition, Datalog programs, Relational databases, Database languages, cost model, Concurrent computing, deductive databases, Parallel processing, concrete query, Logic, communicating concurrent processes, process network rewriting, query bindings, set oriented declarative query language, dataflow parallel evaluation, query evaluation, DATALOG, dataflow graph, query optimization, Query processing, Load management, Concrete, Deductive databases, process network]
An efficient emulation for tree-connected networks
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Efficient emulations provide general methods to convert algorithms designed on a network into algorithms on smaller networks (with the same interconnection structure). In this paper, an optimal emulation for trees is proposed. With slight modification, our emulation can be applied to X-trees without loss of any efficiency. By the strategy of our emulation, optimal emulations for m-ary trees and pyramids can be obtained. An extended problem of the emulation problem on trees is to emulate a weighted tree, in which every node is associated with a weight by a smaller tree. In this paper, we also consider the extended problem and show that the problem is NP-hard.
[Algorithm design and analysis, NP-hard, Computational modeling, Multiprocessor interconnection networks, multiprocessor interconnection networks, X-trees, optimal emulation, tree-connected networks, pyramids, Parallel algorithms, Sorting, Computer science, Concurrent computing, Emulation, Parallel processing, Hypercubes, m-ary trees]
Stochastic modeling of scaled parallel programs
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Testing the performance scalability of parallel programs can be a time consuming task, involving many performance runs for different computer configurations, processor numbers, and problem sizes. Ideally, scalability issues would be addressed during parallel program design, but tools are not presently available that allow program developers to study the impact of algorithmic choices under different problem and system scenarios. Hence, scalability analysis is often reserved to existing (and available) parallel machines as well as implemented algorithms. In this paper we propose techniques for analyzing scaled parallel programs using stochastic modeling approaches. Although allowing more generality and flexibility in analysis, stochastic modeling of large parallel
[Algorithm design and analysis, System testing, complexity, Scalability, Instruments, Stochastic processes, parallel program design, Predictive models, stochastic modeling, performance scalability, parallel programming, scalability, parallel programs, Concurrent computing, performance runs, computer configurations, Runtime, Stochastic systems, Performance analysis, scaled parallel programs, processor numbers]
Performance modelling and evaluation for the XMP shared-bus multiprocessor architecture
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
This paper presents the performance modelling and evaluation of a shared bus multiprocessor, XMP. A key characteristic of XMP is that it employs a special shared bus scheme featuring separate address bus and data bus with split transaction, pipelined cycle (called SSTP scheme). To assist evaluating the architectural alternatives of XMP, the features of the SSTP bus scheme as well as two important performance impacting factors: (1) cache, bus, and memory interferences and (2) DMA transfer, are modelled. We employ a Subsystem Access Time (SAT) modelling methodology. It is based on a Subsystem Access Time Per Instruction (SATPI) concept, in which we treat major components other than processors (e.g. off-chip cache, bus, memory, I/O) as subsystems and model for each of them the mean access time per instruction from each processor. Validated by statistical simulations, the performance model is fed with a given set of representative workload parameters, and then used to conduct performance evaluation for some initial system design issues. Furthermore, the SATPIs of the subsystems are directly utilized to identify the bottleneck subsystems and to help analyze the cause of the bottleneck.
[Process design, data bus, bus, Industrial control, address bus, split transaction, performance model, Control systems, Multiprocessing systems, Subsystem Access Time Per Instruction, memory interferences, Prototypes, Computer architecture, SSTP scheme, Electrical equipment industry, Neck, performance modelling, cache, XMP shared-bus multiprocessor architecture, Process control, pipelined cycle, Interference, performance evaluation, DMA transfer, Subsystem Access Time modelling methodology]
Some optimal parallel algorithms on weighted cographs
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
The class of cographs, or complement-reducible graphs, arises naturally in many different areas of applied mathematics and computer science. In this paper we present an O(n) time sequential algorithm and a parallel algorithm of O(log n) time and O(n/log n) processors on the EREW PRAM model to solve the maximum weight independent set problem on weighted cographs. Using such algorithms we can easily solve the minimum weight vertex cover, maximum weight clique, minimum weight independent dominating set, minimum weight dominating set, and minimum weight maximal irredundant set problems on weighted cographs with the same bounds of time and processors.
[sequential algorithm, parallel algorithms, Terminology, Phase change random access memory, minimum weight maximal irredundant set problems, Mathematics, minimum weight vertex cover, minimum weight dominating set, Parallel algorithms, Computer science, Tree graphs, minimum weight independent dominating set, optimal parallel algorithms, weighted cographs, maximum weight independent set problem, Intrusion detection, EREW PRAM model, maximum weight clique, High definition video, complement-reducible graphs]
Efficient algorithms for data distribution on distributed memory multicomputers
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Data distribution has been one of the most important research topics in parallelizing compilers for distributed memory parallel computers. In this paper, we show that data re-distribution is necessary for executing a sequence of Do-loops if the communication cost due to perform this sequence of Do-loops is larger than a threshold value. Based on this observation, we propose efficient algorithms which can determine effective data distribution schema for executing a sequence of Do-loops with a general structure. Our result contributes towards automatic compilation of sequential programs to message-passing version programs running on distributed memory parallel computers.
[Algorithm design and analysis, Costs, Automatic programming, parallelising compilers, message-passing, Heuristic algorithms, data distribution, Distributed computing, component alignment, Concurrent computing, Shortest path problem, Information science, Program processors, dynamic programming algorithm, distributed memory multicomputers, distributed memory parallel computers, automatic compilation, Dynamic programming, Do-loops]
Implementation of fast Hartley transform on multiple bus cache coherent multiprocessors
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
The use of multiple bus as interconnection network for multiprocessors has shown attractive features as compared to the existing ones. The addition of cache memory makes the architecture still a high performance one. In this paper we consider the implementation of Hou's FHT on multiple bus cache coherent multiprocessors. The analytical formulas are developed and performances are analysed in terms of speedup using these formulas. We also study the limitations of the inter processor communication overhead and propose a modification to the signal flow graph in order to minimise the multiprocessor execution time and hence to improve the speedup performance of the system.
[Algorithm design and analysis, Fourier transforms, Multiprocessor interconnection networks, Discrete Fourier transforms, multiprocessor interconnection networks, inter processor communication overhead, Flow graphs, fast Hartley transform, cache memory, Discrete transforms, multiprocessor execution time, Signal processing algorithms, multiple bus cache coherent multiprocessors, Parallel processing, speedup performance, Performance analysis, Kernel, performances, signal flow graph]
Algorithms for node disjoint paths in incomplete star networks
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
We give efficient algorithms for node disjoint path problems in incomplete star graphs which are defined in this paper to reduce the large gaps in the size of systems based on star graph topologies. Four disjoint path paradigms in incomplete star graphs are discussed: (1) disjoint paths between a pair of nodes s and t, (2) disjoint paths from a node s to a set T of nodes, (3) disjoint paths from a set S of nodes to a set T of nodes, and (4) disjoint paths between node pairs (s/sub i/,t/sub i/). We give algorithms which can find the maximum number of disjoint paths for these paradigms in optimal time. For an n-dimensional incomplete star graph G/sub n,m/, the length of the disjoint paths constructed by our algorithms is at most d(G/sub n,m/)+c, where d(G/sub n,m/) is the diameter of G and c is a small constant. This paper also shows that the k-wide-diameter d/sub n-2//sup W/(G/sub m,n/), k-Rabin-diameter d/sub n-2//sup R/(G/sub m,n/), k-set-diameter d/sub n-2//sup S/(G/sub m,n/), and k-pair-diameter d/sub n-2//sup P/(G/sub m,n/) of G/sub n,m/ are at d(G/sub n,m/)+c.
[incomplete star networks, n-dimensional incomplete star graph, Multiprocessor interconnection networks, Software algorithms, multiprocessor interconnection networks, Routing, Distributed computing, Multiprocessing systems, Concurrent computing, Intelligent networks, Network topology, Fault tolerant systems, Hypercubes, node disjoint paths algorithms]
On evaluating parallel sparse Cholesky factorizations
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Though many parallel implementations of sparse Cholesky factorization with the experimental results accompanied have been proposed, it seems hard to evaluate the performance of these factorization methods theoretically because of the irregular structure of sparse matrices. This paper is an attempt to such research. On the basis of the criteria of parallel computation and communication time, we successfully evaluate four widely adopted Cholesky factorization methods, including column-Cholesky, row-Cholesky, submatrix-Cholesky and multifrontal. The results show that the multifrontal method is superior to the others.
[Linear systems, parallel computation, column-Cholesky, Symmetric matrices, row-Cholesky, performance evaluation, Sparse matrices, Distributed computing, multifrontal, Concurrent computing, Computer science, communication time, Computer architecture, submatrix-Cholesky, parallel sparse Cholesky factorizations, sparse matrices]
Semigroup computation and its applications on mesh-connected computers with hyperbus broadcasting
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Let /spl oplus/ be an associative operation on a domain D. The semigroup problem is to compute a/sub 0//spl oplus/a/sub 1//spl oplus/...a/sub N-1/, where a/sub i/ /spl isin/D, for 0/spl les/i<N. The algorithm described here runs on SIMD mesh-connected computers with hyperbus broadcasting using p processors in time O(N/p+logp), where p/spl les/N. It as shown optimal when p=N and optimal speedup when p log p=N. Based on the proposed semigroup algorithm, other applications such as matrix multiplication, all-pair shortest path, shortest path spanning tree, topological sorting and connected component problems can be also solved in the order of logarithmic time using N/sup 3/ processors.
[topological sorting, SIMD mesh-connected computers, Broadcast technology, parallel algorithm, Parallel algorithms, logarithmic time, Concurrent computing, connected component problems, all-pair shortest path, semigroup computation, Broadcasting, semigroup algorithm, Grid computing, Computer networks, shortest path spanning tree, hyperbus broadcasting, associative operation, mesh-connected computers, Parallel architectures, Application software, optimal speedup, Sorting, matrix multiplication, Computer applications, semigroup problem]
Lock-free concurrent tree structures for multiprocessor systems
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
This paper presents a window-based approach to design lock-free concurrent implementations for a class of top-down tree structures that supports operations whose executions can be modeled as a process of moving a window along a rooted simple path. Our approach can be implemented on multiprocessor systems that, support load-linked, store-conditional and check-valid synchronization primitives that are supported in MIPS-II and DEC Alpha architectures. Our approach achieves high degree of concurrency, requires low coordination overhead, is wait-free and fault tolerant. Simulation shows that our approach is efficient.
[Tree data structures, load-linked synchronization primitives, lock-free concurrent tree structures, multiprocessor systems, coordination overhead, Data structures, check-valid synchronization primitives, Application software, Delay, top-down tree structures, Multiprocessing systems, concurrency, Concurrent computing, Computer science, Fault tolerance, MIPS-II, lock-free concurrent implementations, Operating systems, DEC Alpha architecture, System recovery, tree data structures, store-conditional synchronization primitives, window-based approach]
Grouping array layouts to reduce communication and improve locality of parallel programs
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
A data layout method, array grouping, is proposed to improve communication efficiency and cache utilization of parallel programs containing indirect array references or nonunit stride indexing. Conditions on where to apply this technique are specified in a series of theorems. The technique is then applied to a real finite element application. The experimental results show that communication is reduced by 15%, and data subcache misses by 40% on 56 processors of the KSR1 parallel computer.
[indirect array references, Scalability, data subcache misses, locality, Data structures, Finite element methods, Application software, parallel programming, parallel programs, Multiprocessing systems, Degradation, Concurrent computing, data layout method, Pollution, array grouping, finite element application, KSR1 parallel computer, communication efficiency, cache utilization, nonunit stride indexing, Indexing, Context modeling]
Invited Mini-review Session 2: Software Specification Of Real-time Systems
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
false
[]
Pyramided hypercube: an extendable cubic network with fixed degree
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
The proposed network is a pyramid-like network of hypercubes, called pyramided hypercube. A pyramided hypercube containing m levels and having n-cube as basic configuration is denoted by PH(m,n). A PH(m,n) with fixed degree n+2, possesses logarithmic diameter, optimal fault tolerance, and capability of simple routing. The PH(m,n) can be extended smoothly with a constant predefined building block based on its recursive structure such that there is no change in the basic node configuration as we increase the network size. Moreover, links once connected are left undisturbed when the network grows in size. The PH(m,n) is also Hamiltonian.
[recursive structure, extendable cubic network, Multiprocessor interconnection networks, optimal fault tolerance, Routing, hypercube networks, pyramided hypercube, n-cube, Electronic mail, Parallel architectures, Partitioning algorithms, logarithmic diameter, network size, basic node configuration, Computer science, Fault tolerance, simple routing, Network topology, constant predefined building block, Hypercubes, Hardware, Hamiltonian]
An improved characterization of 1-step recoverable embeddings: rings in hypercubes
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
An embedding is 1-step recoverable if any single fault occurs, the embedding can be reconfigured in one reconfiguration step to maintain the structure of the embedded graph. In this paper we present an efficient scheme to construct this type of 1-step recoverable ring embeddings in the hypercube. Our scheme will guarantee finding a 1-step recoverable embedding of a length-k (even) ring in a d-cube where 6/spl les/k/spl les/(3/4)2/sup d/ and d/spl ges/3, provided such an embedding exists. Unlike previously proposed schemes, we solve the general problem of embedding rings of different lengths and the resulting embeddings are of smaller expansion than in previous proposals. A sufficient condition for the non-existence of 1-step recoverable embeddings of rings of length >(3/4)2/sup d/ in d-cubes is also given.
[Embedded computing, Costs, Force measurement, fault tolerance, Computational modeling, hypercube networks, Topology, hypercubes, Proposals, Concurrent computing, Degradation, Fault tolerance, d-cube, rings, Hypercubes, 1-step recoverable embeddings]
Use of sequencing constraints for specifying, testing, and debugging concurrent programs
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
This paper introduces the use of sequencing constraints for specifying, testing, and debugging concurrent programs. An execution of a concurrent program P nondeterministically exercises a sequence of synchronization events, called a synchronization sequence (or SYN-sequence). Sequencing constraints (or constraints) specify restrictions on the allowed SYN-sequences of P. Constraints for P are derived from a formal or informal specification of P and do not have to be complete. The SYN-sequences collected during nondeterministic testing of P can be used to measure coverage and detect violations of P's constraints. Also, SYN-sequences can be generated according to P's constraints and used for deterministic testing of P. This paper shows in detail how to accomplish coverage and detect violations of constraints written in CSPE (Constraints on Succeeding and Preceding Events) by nondeterministic and deterministic testing.
[Software testing, System testing, Costs, Event detection, deterministic testing, Debugging, testing, synchronization events, concurrent programs, Distributed computing, parallel programming, Concurrent computing, Computer science, Sequential analysis, specifying, sequencing constraints, Writing, debugging, nondeterministic testing, synchronization sequence]
Parallel algorithms for verification and sensitivity analysis of minimum spanning trees
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
To verify whether a spanning tree T(V,E) of graph G(V,E') is a minimum spanning tree, two parallel algorithms are presented. The first algorithm requires O(log n) time and O(max{m/log n, n/sup 3/2//log n}) processors, where |E'|=m and |V|=n. The second algorithm requires O(log n) time and O(m) processors or O(log nloglog n) time and O(max{m/log n, n}) processors. The first algorithm is optimal when G is dense, compared with its O(m) time sequential version. The second algorithm has better performance when G is sparse. By using above results, we also present an efficient algorithm for sensitivity analysis of minimum spanning trees which requires O(log n) time and O(max{m, n/sup 2//log n}) processors. All proposed algorithms in this paper are based on the parallel computational model called CREW PRAM.
[parallel algorithms, Costs, Sensitivity analysis, Computational modeling, minimum spanning trees, sensitivity analysis, Phase change random access memory, Parallel algorithms, Computational complexity, Computer science, Concurrent computing, Tree graphs, performance, Cities and towns, CREW PRAM, verification, parallel computational model]
Efficient implementation of sorting algorithms on asynchronous distributed-memory machines
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
The problem of merging two sequences of elements which are stored separately in two processing elements (PEs) occurs in the implementation of many existing sorting algorithms. We describe efficient algorithms for the merging problem on asynchronous distributed-memory machines. The algorithms reduce the cost of the merge operation and of communication, as well as partly solving the problem of load balancing. Experimental results on a Fujitsu AP1000 are reported.
[Costs, merging, load balancing, Fujitsu AP1000, Merging, Laboratories, Topology, sorting algorithms, Distributed computing, Delay, Sorting, sorting, processing elements, Load management, Australia, asynchronous distributed-memory machines, merge operation]
Convexity problems on reconfigurable meshes
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
The reconfigurable mesh, a parallel system with bus autonomy, can support various interconnection schemes during execution of an algorithm. It offers very efficient computation power in many application domains. In digital image processing and computer vision, convexity is a natural shape descriptor and a classifier for objects in the image space. We first present that the problem of identifying extreme points of convex hulls can be solved in O(1) time on the reconfigurable mesh proposed. Furthermore, we present constant time algorithms for a number of convexity-related problems on reconfigurable meshes. These problems include point inclusion, interior detection, area, and width of convex hulls.
[digital image processing, Shape, Image processing, parallel algorithm, Very large scale integration, interconnection schemes, convexity, convex hull area, natural shape descriptor, image space, reconfigurable architectures, constant time algorithms, point inclusion, Power system interconnection, convexity problems, Computer architecture, Broadcasting, convex hull width, Computer vision, parallel system, bus autonomy, computation power, Digital images, Computational modeling, interior detection, Application software, object classifier, convexity-related problems, computer vision, reconfigurable meshes]
Specification of stochastic properties with CSP
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
The research in formal specification and verification of complex systems has often ignored the specification of stochastic properties of the system. We are exploring new methodologies and tools to permit stochastic analysis of CSP-based systems specifications. In doing so, we have investigated the relationship between specification models and stochastic models by translating the specification into another form that is amenable to such analyses (e.g., from CSP to stochastic Petri Nets). This process can give insight for further refinements of the original specification (i.e., identify potential failure processes and recovery actions). It does this by relating the parameters needed for reliability analysis to user level specifications which is essential for realizing systems that meet the users needs in terms of cost, functionality, performance and reliability.
[cost, Computational modeling, communicating sequential processes, Petri nets, Stochastic processes, CSP-based systems specifications, Discrete event simulation, functionality, Formal specifications, formal specification, Distributed processing, formal verification, performance, Stochastic systems, reliability analysis, Failure analysis, Cost function, Performance analysis, user level specifications, stochastic Petri Nets, stochastic properties specification, stochastic analysis]
CGIN: a modified Gamma interconnection network with multiple disjoint paths
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
To ensure high terminal reliability for the Gamma interconnection network (GIN), we propose a new modified GIN, referred to as CGIN (cyclic Gamma interconnection network) as its connecting patterns between stages exhibit a cyclic feature. The fact that there exist multiple disjoint paths between any communication pair for all types of CGINs makes it possible to tolerate any arbitrary single fault and to accomplish enhanced terminal reliability accordingly. The performance of the CGIN is also evaluated through simulation.
[Costs, arbitrary single fault, Multiprocessor interconnection networks, communication pair, simulation, Switches, Very large scale integration, Routing, multistage interconnection networks, high terminal reliability, Gamma interconnection network, connecting patterns, Fault tolerance, cyclic Gamma interconnection network, performance, Production, multiple disjoint paths, Hardware, Joining processes, Contracts]
Parallel execution of nested loops in band parallelism
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
How to execute a nested loop in band parallelism on a multiprocessor system is addressed. The mathematical models of the waveband method, the hyperplane method, the modified hyperplane method and the linear band method are derived and compared. Since the structures of the real multiprocessor systems are at most 3-dimensional, in order to map the loop into these systems, an efficient algorithm for finding the optimal linear band in 2-dimensional index space, instead of a high dimensional index space, is proposed.
[parallel architectures, linear band method, parallel execution, mathematical models, Vectors, hyperplane method, Multiprocessing systems, Pipeline processing, modified hyperplane method, Concurrent computing, Computer science, nested loops, Control engineering, 2-dimensional index space, band parallelism, multiprocessor system, Parallel processing, Systolic arrays, Mathematical model, waveband method, Testing]
Two expansible multistage interconnection networks
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
Two new construction methods for multistage interconnection networks (MINs) are proposed. These methods provide low overhead in enlarging the size of MIN scheme and they have the same features as those of MIN schemes by typically designed method. The first proposed method proves that, in enlarging the size of MIN scheme, the full access interconnection property and the self-routing ability are also available in the enlarged scheme and the hardware and reconstruction overhead is low. However, the requests to be accepted by the MIN scheme depend not only on the configuration of MIN but also on another mechanism in the first proposed method. It results in low success probability for any request. The second proposed method releases this disadvantage. The following are the features of our two proposed schemes: (1) The full-access interconnection property; (2) Simple and distributed self-routing ability; (3) The least hardware cost; (4) Low reconstruction overhead.
[Costs, Multiprocessor interconnection networks, Design methodology, full-access interconnection property, Integrated circuit interconnections, Switches, Routing, Educational institutions, self-routing ability, multistage interconnection networks, Distributed computing, full access interconnection property, Concurrent computing, construction methods, least hardware cost, distributed self-routing ability, lw reconstruction overhead, Hardware, low overhead]
Dynamic load balancing in parallel simulation using time warp mechanism
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
This paper presents a loan balancing algorithm for a time warp discrete event simulation running on non-dedicated heterogeneous processors. This algorithm dynamically balances the load on processors in order to reduce the number of rollbacks, and thus increases the total simulation speed. Simulation processes are allowed to migrate according to the load on processors. An emulated multiprocessor environment was developed in order to evaluate the algorithm. The simulation results indicate that the running time of the time warp simulation can be substantially reduced.
[Protocols, Computational modeling, Pipelines, Time warp simulation, time warp mechanism, time warp simulation, emulated multiprocessor environment, Discrete event simulation, Computer science, heterogeneous processors, Manufacturing processes, rollbacks, parallel simulation, Load management, Virtual manufacturing, discrete event simulation, dynamic load balancing, Clocks]
Keynote Address: The Prospects For Architecture-independent Parallel Programming
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
false
[]
Plenary Address 1: Towards Teraflop Computing
Proceedings of 1994 International Conference on Parallel and Distributed Systems
None
1994
false
[Technology management]
A distributable algorithm for optimizing mesh partitions
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
Much effort has been directed towards developing mesh partitioning strategies to distribute a finite element mesh among the processors of a distributed memory parallel computer. The distribution of the mesh requires a partitioning strategy. This partitioning strategy should be designed such that the computational load is balanced and the communication is minimized. Unfortunately, many of the existing approaches for the mesh partitioning are sequential and are performed as a sequential pre-processing step on a serial machine. These approaches may not be appropriate if the mesh is very large, that is to say, this pre-processing on a serial machine may not be feasible due to memory or time constraints, or on a parallel machine if the mesh is already distributed. In this paper we propose an algorithm with a fundamentally distributed design for the optimization of mesh partitions. To assess the quality of the partitioning, the size of edge cuts is the taken as the chosen metric.
[Algorithm design and analysis, mesh partitioning, partitioning strategy, Parallel machines, Partitioning algorithms, Finite element methods, Application software, finite element analysis, Distributed computing, balanced, Design optimization, Concurrent computing, finite element mesh, distributed memory parallel computer, Memory management, distributed algorithms, optimization, edge cuts, distributed memory systems, distributable algorithm, Time factors]
Distributed fault detection in communication protocols using extended finite state machines
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
Run-time fault detection in communication protocols is essential because of faults that occur in the form of coding defects, memory problems, and external disturbances. Finite State Machine models have been used in the past to detect and diagnose protocol faults. However, the fault coverage of these models is limited to vocabulary faults and sequencing faults. We present an Extended Finite State Machine Model (EFSM) to augment the fault coverage of the FSM model. We extend the parallel decomposition method to EFSMs in order to reduce the size of the observer used to detect faults. The decomposition of the EFSM into several independent EFSMs results in multiple observers. The distributed fault detection mechanism increases the reliability of the fault detection and the EFSM model improves the fault coverage.
[Vocabulary, System testing, Protocols, distributed fault detection, finite state machines, Fault diagnosis, Degradation, Runtime, distributed fault detection mechanism, memory problems, extended finite state machines, parallel decomposition method, communication protocols, vocabulary faults, fault coverage, encoding, multiple observers, Fault detection, transport protocols, coding defects, Automata, protocol faults, fault tolerant computing, run-time fault detection, sequencing faults]
Benchmarking IBM SP1 system for SPMD programming
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
The IBM SP1 is the first member of the IBM Scalable POWERparallel series, a distributed memory parallel computer based on RISC System/6000 processing element. In this paper, the benchmarking exercise of two message passing libraries, MPL and PVM, on the IBM SP1 for SPMD programming is described. We will discuss the benchmarks used in our experiment, and present the results we obtained. Our results indicate that to achieve performance improvement and to reduce the communication overhead, the use of the high performance switch is essential on the IBM SP1 machine.
[PVM, Switches, MPL, Distributed computing, parallel processing, communication overhead, Concurrent computing, distributed memory parallel computer, Reduced instruction set computing, Benchmark testing, IBM SP1 system benchmarking, Kernel, high performance switch, message passing libraries, message passing, reduced instruction set computing, performance evaluation, RISC System/6000 processing element, Communication switching, Computer aided instruction, Software libraries, Message passing, SPMD programming, distributed memory systems, IBM Scalable POWERparallel series]
Fault-tolerant routing strategy using routing capability in hypercube multicomputers
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
This paper addresses fault-tolerant routing which is concerned with finding feasible minimum paths in a faulty hypercube. The concept of routing capability, which is defined with respect to the entire spectrum of distance, is proposed to assist routing function. The amount of information that is useful for message routing is increased with our scheme. The proposed algorithm routes a message in an attempt to minimize derouting. In particular, it makes use of the information embedded in routing capabilities to establish a path for a message for which an upper bound on its length may be determined at the source. We then propose the notion of directed routing capability which captures more useful information for shortest path routing in comparison with undirected counterpart. Routing in hypercubes with link failures is also addressed.
[feasible minimum paths, routing capability, message passing, Delay effects, network routing, hypercube multicomputers, Routing, hypercube networks, upper bound, message routing, fault-tolerant routing strategy, Fault tolerance, Upper bound, shortest path routing, faulty hypercube, Parallel processing, Broadcasting, Hypercubes, fault tolerant computing, Safety, link failures]
Fault-tolerant causal delivery in group communication
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
In distributed systems, a group of processes are cooperated to execute an application program. A group is established among multiple processes and only processes in the group communicate with each other. This type of group communication is named intra-group communication. The communication system has to support the reliable intra-group communication in the presence of the process fault. In order to tolerate the process fault, each process in the group is replicated into a collection of multiple replicas named a cluster. In this paper, we would like to propose a new intra-group communication protocol which supports the causally ordered delivery of messages for the processes within the group. In addition, the protocol supports the reliable delivery of messages in the presence of the Byzantine faults of the processes.
[Protocols, Multimedia systems, intra-group communication, distributed processing, communication protocol, Byzantine faults, Electronic mail, Multimedia communication, Distributed computing, group communication, multiple replicas, Fault tolerance, Teleconferencing, application program, Telemedicine, Fault tolerant systems, groupware, Collaborative work, distributed systems, fault tolerant computing, fault-tolerant causal delivery, protocols, multimedia communication]
State reduction in the exact analysis of fork/join queueing systems with homogeneous exponential servers
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
A state reduction technique for the exact analysis of fork/join queueing systems is presented in this paper. The technique is based on the standard Markov model and can be applied to systems having K homogeneous exponential servers. For a closed system with M jobs, the technique reduces the size of the state space from (M+1)/sup K/-M/sup K/ states to (M+K-1/K-1) states. This amounts to more than five orders of magnitude of state reduction for a typical value of K=M=10. The state reduction technique can also be applied to the analysis of an open fork/join queueing system. It reduces the size of the state space from (B+1)/sup K/ states to (B+K/K) states where B is the maximum number of jobs allowed in the open queueing system. The state reduction amounts to more than six orders of magnitude for a typical value of K=10 and B=500.
[queueing theory, Computational modeling, performance evaluation, state reduction, State-space methods, exact analysis, parallel processing, homogeneous exponential servers, fork/join queueing systems, Multiprocessing systems, synchronisation, Analytical models, Parallel processing, standard Markov model, Performance analysis, Queueing analysis]
Allowing cycle-stealing direct memory access I/O concurrent with hard-real-time programs
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
Hard-real-time schedulability analysis is carried out based on the assumption that the worst-case execution time (WCET) of each task is known. Cycle-stealing Direct Memory Access (DMA) I/O steals bus cycles from an executing program and prolongs the execution time of the program. Because of the difficulty in bounding the interference on the executing program, cycle-stealing DMA I/O is often disabled in hard-real-time systems. This paper presents an analytical method for bounding the WCET of a program executing concurrently with cycle-stealing DMA I/O. This is an extension of our previous work which bounded the WCET of a straight-line sequence of instructions when cycle-stealing operations are allowed. We demonstrate the effectiveness of our method with experiments on several programs.
[straight-line sequence, cycle-stealing direct memory access, Pipelines, hard-real-time programs, Programming profession, Computer science, worst-case execution time, Reduced instruction set computing, Processor scheduling, cycle-stealing operations, schedulability analysis, real-time systems, bus cycles, Frequency, file organisation, fault tolerant computing, I/O concurrent, Timing, Performance analysis, Time factors, Assembly]
A distributed algorithm for implementation of first-order multiparty interactions
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
First-order multiparty interactions, a generation of multiparty synchronization, are a powerful communication mechanism that allows a set of processes to enroll into different roles of an interaction and to execute the interaction in a synchronous way, and guarantees conflict interactions to be executed exclusively. It is not a trivial problem to implement first-order multiparty interactions in a network environment. The implementation has to maintain the synchronous and exclusive properties mentioned above, be fair and make progress. In this paper, we propose a new distributed algorithm for implementing first-order multiparty interactions based on a new implementation model, in which a role manager is devised for each role. Our algorithm solves the problem with O(|R|/sup 2/.|P|) messages per interaction as opposed to |P|/sup 2/ messages required in Joung and Smolka (1994), where |R| is the number of roles of the interaction and |P| is the number of processes. Our algorithm is more efficient than that in Joung and Smolka, when |R|/sup 2/<|P|. Furthermore our algorithm is closer to the fully distributed one in the sense that (1) every process (role manager) only knows its adjacent role managers (processes), and (2) the time complexity of local computation in each process and role manager is equal.
[multiparty synchronization, multiparty interactions, Communication system control, communication complexity, role manager, Distributed computing, Information systems, Centralized control, distributed algorithm, Communication system software, distributed algorithms, first-order multiparty interactions, Communications technology, Synchronous generators, Distributed algorithms, Energy management, Power generation, computational complexity]
Extended group communication algorithm for updating distributed programs
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
Large-scale distributed systems are not always stable because the environments and the user requirements are changeable. The author has discussed the receptive platform to make distributed systems flexible and reliable. In order to achieve the receptive platform, it is essential to discuss how to update the distributed programs. Since conventional updating methods require that multiple processes be suspended simultaneously, the availability of the system becomes lower. A new method called the dynamic updating has been proposed. The key idea is that multiple versions of processes are allowed to co-exist temporarily while the old-version processes are not used anymore. If such protocol errors as the unspecified receptions and the dead-locks caused by the co-existence occurs, the suspended old-version processes are restarted from the consistent state. An algorithm, has been designed for treating the unspecified receptions. This paper proposes an extended group communication algorithm that can detect and resolve the deadlocks. By using the algorithm, the dynamic updating can be applied to more general distributed application programs.
[Availability, Algorithm design and analysis, distributed programs updating, Heuristic algorithms, distributed application programs, distributed processing, Application software, Distributed computing, Information systems, extended group communication algorithm, user requirements, concurrency control, System recovery, Computer networks, protocol errors, Large-scale systems, dynamic updating, Communication networks, protocols, deadlocks]
A linear-time algorithm for computing the diameters of the incomplete WK-recursive networks
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
The WK-recursive networks, which were originally proposed by Vecchia and Sanges, have suffered from the rigorous restriction on the number of nodes. Like the other incomplete networks, the incomplete WK-recursive networks have been proposed to relieve this restriction. In this paper, it is first shown that the structures of the incomplete WK-recursive networks are conveniently represented with multistage graphs. This representation can provide a uniform look at the incomplete WK-recursive networks. With its help, a linear-time algorithm using the prune-and-search technique is presented for computing the diameters of the incomplete WK-recursive networks.
[Algorithm design and analysis, Costs, Multiprocessor interconnection networks, Scalability, Educational institutions, multistage interconnection networks, multistage graphs, linear-time algorithm, Multiprocessing systems, Computer science, WK-recursive networks, prune-and-search technique, Network topology, interconnection network, multiprocessor system, Computer networks, Manufacturing]
An alternative addressing scheme for conventional CDMA fiber-optic networks allows interesting parallel processing capabilities
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
A different approach to addressing on a CDMA (code division multiple access) fiber based LAN (local area network) is investigated. This change serves to enable a network with unusual properties: specifically this implementation modification enables a network that provides: immediate and simultaneous access to all users (real-time); arbitrary length transmissions (including continuous); all data available to all nodes; support of sensor type nodes (no receive or monitor capability). The technique is discussed and a theoretical use of this technique to allow a fully dynamically reconfigurable parallel multiprocessor is presented.
[Bit error rate, dynamically reconfigurable parallel multiprocessor, Optical fiber networks, optical fibre LAN, Optical receivers, Decoding, code division multiple access, Multiaccess communication, parallel processing, sensor type nodes, Computer science, CDMA fiber-optic networks, reconfigurable architectures, real-time systems, local area network, Spread spectrum communication, Parallel processing, Bones, alternative addressing scheme, fiber based LAN, Local area networks]
Fast parallel chessboard distance transform algorithms
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
In this paper, based on the diagonal propagation approach, we first provide an O(N/sup 2/) time sequential algorithm to compute the chessboard distance transform (CDT) of an N/spl times/N image, which is a DT using the chessboard distance metrics. Based on the proposed sequential algorithm, the CDT of a 2-D binary image array of size N/spl times/N can be computed in O (log N) time on the EREW PRAM model using O(N/sup 2//log N) processors, O(log N/log log N) time on the CRCW PRAM model using O(N/sup 2/log log N/log N) processors and O(log N) time on the hypercube computer using O(N/sup 2/) processors. Following the mapping as proposed by Y.H. Lee and S.J. Horng (1995), the algorithm for the MAT is also efficiently derived. The medial axis transform of a 2-D binary image array of size N/spl times/N can be computed in O(log N) time on the EREW PRAM model using O(N/sup 2//log N) processors, O(log N/log log N) time on the CRCW PRAM model using O(N/sup 2/log log N/log N) processors, and O(log N) time on the hypercube computer using O(N/sup 2/) processors. Our algorithms are faster than the best previous results as proposed by J.F. Jenq and S. Sahni (1992).
[image processing, Computer vision, parallel algorithms, diagonal propagation approach, O(N/sup 2/) time sequential algorithm, Shape, Image processing, Digital images, CRCW PRAM model, computational geometry, Phase change random access memory, hypercube networks, medial axis transform, Data mining, Application software, Parallel algorithms, 2-D binary image array, EREW PRAM model, hypercube computer, Hypercubes, fast parallel chessboard distance transform algorithms, Pixel, computational complexity]
Expressing concurrency in Griffin
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
Griffin is a statically typed language designed specifically for the rapid prototyping of Ada software. In this paper we describe the Griffin language constructs for expressing concurrency. There are two salient innovations: (1) an extended select statement that provides greater flexibility in managing non-deterministic behavior and (2) the capability of a receiving thread to concurrently rendezvous with multiple sending threads. Thus by increasing concurrency, we alleviate the chief drawback to synchronous communication. We then apply the Griffin constructs in implementing solutions to the readers-writers problem, group lock mechanism, and scheduling groups of concurrent operations. The Griffin constructs facilitate the expression of high-level algorithms that maximize concurrency.
[program debugging, software prototyping, receiving thread, Yarn, parallel programming, Information systems, Concurrent computing, group lock mechanism, Contracts, synchronous communication, Software prototyping, rapid prototyping, Debugging, readers-writers problem, extended select statement, Data structures, high-level algorithms, Programming profession, synchronisation, Writing, Griffin, statically typed language, Error correction, Griffin language constructs, scheduling groups, Ada software, Ada]
Efficiency of the domain decomposition method for the parallelization of implicit finite element code
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
An analytical model is presented for estimating parallel efficiency of the domain decomposition method which is used for the parallelization of implicit finite element code. Serial and parallel finite element codes with domain decomposition and direct LDU solution of equation systems are developed. Dependencies of parallel efficiency on problem size are obtained for IBM SP2 with 4,6 and 8 processor nodes. It is shown that interprocessor load balancing during assembly-decomposition phase which can be achieved by partitioning into unequal subdomains increase parallel efficiency considerably. Predicted and measured values of parallel efficiency are in reasonable agreement.
[domain decomposition method, parallel algorithms, interprocessor load balancing, Chemical analysis, Laboratories, mathematics computing, Distributed decision making, Finite element methods, finite element analysis, parallelization, analytical model, Equations, Concurrent computing, Fabrication, Analytical models, Load management, direct LDU solution, IBM SP2, assembly-decomposition phase, Assembly, implicit finite element code]
Soft resource reservation: A flexible guarantee of QoS
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
Much research efforts are presently involved in developing network protocols suitable for multimedia services through LAN and even Internet. Due to large size and time sensitiveness of data, it is usually a hard task to deliver the services up to the expectations of the users. But the basic intention of the user, for the multimedia services, usually have a wide scope for tolerance and flexibility. If that flexibility is explored and exploited, it is possible to basically satisfy the user even with marginal resources. It is important to intelligently interpret user's request as well as distribute the system and network resources optimally. In this paper we are proposing an Intelligent Soft Resource Reservation Protocol (ISORRP) for realizing acceptable running quality of application by intelligent interpretation of users' requests and optimum distribution of available resources among them. In this architecture QoS to a session is only flexibly guaranteed but always to a level acceptable to the user.
[Protocols, soft resource reservation, Quality of service, Jitter, multimedia systems, local area networks, Multimedia communication, Computer crime, Delay, network protocols, user's request, Degradation, multimedia services, flexibility, Operating systems, transport protocols, QoS, Bandwidth, Streaming media, LAN, fault tolerant computing, Intelligent Soft Resource Reservation Protocol, Internet, tolerance]
A distributed connection manager interface for web services on IBM SP systems
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
In essence, the World Wide Web is a worldwide string of computer databases using a common information retrieval architecture. With the increasing popularity of the World Wide Web, more and more functions have been added to retrieve not only documents written in HTML (Hypertext Markup Language), but also those in other forms through the Common Gateway Interface (CGI), by constructing HTML documents dynamically. Dynamic construction of HTML documents for handling information such as digital libraries is slow and requires much more computer power. A significant performance bottleneck is the initialization and setup phase for a CGI process to gain access to the system containing the data. In this paper we describe the design and implementation of a Connection Manager Interface on IBM SP systems. The Connection Manager provides cliette processes to serve CGI requests and eliminates such bottlenecks. An IBM SP system is used for this emerging area to show that our design and implementation is flexible enough to take advantage of the High-Performance Switch in an IBM SP system. We trace and monitor this scalable Web services using UTE (Unified Trace Environment) tools, and present its performance analysis and visualization.
[cliette processes, web services, Common Gateway Interface, Switches, hypermedia, World Wide Web, HTML, Unified Trace Environment, network interfaces, Databases, data visualisation, Computer architecture, distributed databases, High-Performance Switch, data visualization, performance bottleneck, distributed connection manager interface, Power system management, Hypertext Markup Language, Service oriented architecture, information retrieval, Information retrieval, digital libraries, computer databases, Web services, information retrieval architecture, Markup languages, IBM SP systems, Internet, Web sites]
What features really make distributed minimum spanning tree algorithms efficient?
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
Since Gallager, Humblet and Spira first introduced the distributed Minimum Spanning Tree problem, many authors have suggested ways to enhance their basic algorithm to improve its performance. Most of these improved algorithms have even been shown to be very efficient in terms of reducing their worst-case communications and/or time complexity measures. In this paper, however, we take a different approach, basing our comparisons on measurements of the actual running times, numbers of messages sent, etc., when various algorithms are run on large numbers of test networks. We also propose the Distributed Information idea, that yields several new techniques for performance improvement. Simulation results show that contrary to the theoretical analysis, some of the techniques in the literature degrade the performance. Moreover, a simple technique that we propose seems to achieve the best time and message complexity among several algorithms tested.
[Greedy algorithms, graph theory, running times, Nominations and elections, multiprocessor interconnection networks, Distributed Information, Time measurement, communication complexity, performance improvement, Computer science, Degradation, Analytical models, message complexity, Tree graphs, Network topology, distributed algorithms, complexity measures, Performance analysis, distributed minimum spanning tree, Testing, computational complexity]
An Optical Bus Computer Cluster with a deferred cache coherence protocol
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
In this paper, we first propose a class of workstation cluster which utilizes optical wavelength-division multiplexing (WDM) technology to connect nodes (work-stations) of the cluster. The Optical Bus Computer Cluster (OBCC) falls in the class of cache coherent non-uniform memory access (CC-NUMA) multiprocessors. The basic topology of the cluster is star-shaped with an optical star-coupler in the center to enable one-hop simultaneous broadcasting of information packets from one node to all other cluster nodes. WDM technology not only multiplies by N times the network bandwidth using a single optical fiber, where N is the degree of wavelength multiplexing, but also provides independent communication paths between pairs of cluster nodes by properly assigning wavelengths to inter-node communication. Then we identify the cache subsystem requirements for the OBCC and propose or deferred cache coherence protocol suitable for the OBCC. The basic coherence maintenance scheme is to lazy-evaluate the cache coherence transactions among cluster nodes, utilizing the weak consistency memory model. By deferring the transactions, it is possible to combine multiple transaction issues into one transaction by accumulating modified status bits in the enhanced cache status fields. Since not only the remote memory access but also coherence transaction are costly operations in CC-NUMA systems, the deferred invocation of coherence transactions is particularly useful in CC-NUMA systems such as OBCC. We then give a performance evaluation by simulation that the coherence protocol effectively reduces coherence transactions, particularly in situations where false sharing of longer cache lines becomes noticeable.
[Optical fibers, Optical Bus Computer Cluster, Protocols, deferred cache coherence, Optical computing, wavelength-division multiplexing, WDM networks, Wavelength division multiplexing, cache storage, coherence maintenance, one-hop simultaneous broadcasting, Network topology, optical interconnections, Coherence, Bandwidth, Broadcasting, deferred cache coherence protocol, Workstations, protocols, optical star-coupler, wavelength multiplexing]
Connection rerouting strategy applicable to various connection-oriented mobile communication networks
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
To date, various connection rerouting methods for connection-oriented mobile network services have been proposed. The previous methods, however, are limited to specific topologies or environments. In this paper, we propose CI(Connection Information)-based rerouting widely applicable to various connection-oriented mobile networks. This method requires neither a specific topology nor a complex connection, guarantees fast rerouting, and takes a good trade-off between rerouting simplicity and route optimality. These features make the method adequate for general application to various connection-oriented mobile networks.
[CI, Mobile communication, Routing, mobile networks, Application software, connection-oriented mobile communication networks, Network topology, mobile communication, connection rerouting, Computer networks, rerouting strategy, Internet, Personal communication networks, wireless LAN, Mobile computing]
Fault-tolerant distributed match-making with weights
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
Protocols to solve several distributed issues, such as name service, mutual exclusion, and creation of an atomic shared register, require two types of subsets with intersection property. Distributed match-making provides a method of creating the subsets, and the lower bound of the number of messages to solve the issues. This paper discusses the fault-tolerant and weighted case, in which a protocol is fault-tolerant regarding node failures, and in which weights of subsets are different. The paper first provides the lower bound of the number of messages required for a protocol in a general form. Then, it concentrates a symmetric case and shows the lower bound in a simpler form. The paper also provides a method of constructing the two types of subsets, which realize the lower bound. It first shows a method for a fully symmetric case, and extends it for other cases. The extended method is practical. It creates a cyclic communication structure, and is valid for any degree of fault-tolerance and weights.
[Protocols, cyclic communication structure, fault-tolerance, Laboratories, Communication system control, distributed match-making, mutual exclusion, distributed processing, Electronic mail, name service, weights, Centralized control, Fault tolerance, Broadcasting, Frequency, fault tolerant computing, protocols, atomic shared register]
A high performance reliable atomic group protocol
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
A novel and efficient group communication (multicast) protocol, based on a single logical-clock token ring approach, is described. The protocol is highly efficient and it guarantees total ordering and atomicity of multicast messages for asynchronous distributed systems. Unlike other logical token-ring algorithms, the protocol does not have a problem of token loss. The optimized fault-tolerant algorithms of the protocol can handle process failures and network partitioning. The experiment results of the implemented protocol in a local area network of workstations have demonstrated that its performance is better than any existing solutions in the same environment, especially, for achieving message total ordering and atomic (safe) delivery.
[local area networks, optimized fault-tolerant algorithms, high performance reliable atomic group protocol, Clustering algorithms, Broadcasting, message total ordering, multicast messages, Token networks, Workstations, Local area networks, total ordering, message passing, atomicity, atomic delivery, Nominations and elections, single logical-clock token ring approach, Multicast protocols, Partitioning algorithms, Computer science, Multicast algorithms, transport protocols, local area network, network partitioning, fault tolerant computing, token networks, group communication multicast protocol, process failures]
Performance evaluation of a WDMA OIDSM multiprocessors
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
Optically interconnected distributed shared memory (OIDSM) systems offer significant performance advantages due to the fast interconnection network. The photonic network of the proposed approach is based on a wavelength-division multiplexed (WDMA) passive star-coupled configuration. Optical self-routing is achievable which partitions the traffic, relaxing the design constraints on the receiver subsystem since a node now only receives and processes a fraction of the network traffic. A major concern with the multi-access approach is that a media access control and a cache coherence protocol are required to provide access to a distributed arbitration of the WDMA photonic network. In particular, one class of media access protocol (TDMAC) requires a control channel to broadcast reservation requests, and the broadcast capability is also able to support coherence level control signals such as invalidations which enable a snooping based coherence protocol. This paper evaluates how OIDSM can ease the traffic in large-scale snooping-based shared memory multiprocessors.
[wavelength division multiplexing, photonic network, distributed shared memory, Multiprocessor interconnection networks, multiprocessor interconnection networks, Optical fiber networks, WDMA, access protocols, reservation requests, Optical design, interconnection network, Broadcasting, shared memory systems, Communication system traffic control, Optical interconnections, coherence level control signals, performance evaluation, Wavelength division multiplexing, shared memory multiprocessors, Optical receivers, media access protocol, WDMA OIDSM, optically interconnected, optical interconnections, distributed memory systems, Media Access Protocol, Level control]
Mapping asynchronous parallel simulation on a network of workstations
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
This paper examines the performance of mapping schemes used in parallel simulation on a network of workstations. A conservative parallel simulation of multistage interconnection networks is used as an example in our analytical model. Performance metrics such as elapsed time, speedup and simulation bandwidth associated with different schemes for partitioning/mapping parallel simulation onto physical processors are evaluated. Our studies show that a perfectly balanced workload distribution may not necessary translate into better performance. We also show in both analytical and implementation results that inter-processors communication overheads incurred in a balanced mapping may occasionally cause dominant aggravation to the program elapsed time.
[elapsed time, Packet switching, Multiprocessor interconnection networks, Computational modeling, asynchronous parallel simulation mapping, Switches, performance evaluation, digital simulation, inter-processors communication, multistage interconnection networks, balanced mapping, Information systems, Computer science, physical processors, Analytical models, Runtime, performance metrics, simulation bandwidth, network of workstations, partitioning, Workstations, Performance analysis]
Agent-based approach for information gathering on highly distributed and heterogeneous environment
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
The core of the problem of Information Gathering is how to generate a concise, high-quality response to the information needs of a user. However, this task is becoming difficult due to the explosion in the amount of electronic information. Agent-based solutions have become an popular approach for locating information in an growing distributed heterogeneous environment like Internet. We first survey the existing non-agent and partially agent-based solutions; then to overcome their drawbacks we propose a completely agent-based solutions, when the different types of agents introduced-named user agent, machine agent, manager-cooperate to decrease user load and gain efficiency in the retrieval process.
[Information resources, manager, knowledge acquisition, machine agent, Information retrieval, Control systems, distributed, distributed heterogeneous information network, Explosions, Distributed power generation, Application software, Environmental management, software agents, heterogeneous, named user agent, agent-based solutions, Internet, IP networks, Problem-solving, information gathering, society of agents]
Modeling and analysis of channel transferability in mobile computing environments
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
Mobile computers use wireless channels to communicate with other computers. The finite number of channels should be efficiently allocated to maximize throughput and to avoid co-channel interference. Temporal variations in, channel demand (load) require channel allocation to adapt dynamically to the changing demand. The presence of load imbalance does not automatically imply the usefulness of channel transfer. In this paper we present a probabilistic analysis of the temporal imbalance in channel demand. The effectiveness of channel transfer decisions depends on the probability distribution of the duration for which the imbalance persists. The results of our analysis provide guidelines for the design of dynamic distributed channel allocation strategies.
[frequency allocation, load sharing, transfer window, co-channel interference, transfer potential, Mobile communication, channel transferability, Guidelines, Wireless communication, Information science, distributed channel allocation, mobile computing, performance modeling, Bandwidth, Channel allocation, probability distribution, Frequency, bulk channel transfer, Computer networks, wireless channels, wireless LAN, channel allocation, Interchannel interference, Mobile computing]
An efficient asynchronous data transmission mechanism for data parallel languages
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
Reducing the overhead of data transmissions is crucial to harnessing the potential of distributed memory multiprocessors. Some optimization techniques such as overlapping communication and computation have been proposed. However, the most overlapping techniques are still limited by the following factors. (1) There is no linguistic support for overlapping communication and computation in data parallel languages; (2) There is no global analysis performed for array references over different nested loops, which will decide how to overlap communication and computation; (3). There is no formalized asynchronous data transmission mechanism in data parallel language compilers. This paper proposes an asynchronous data transmission mechanism for data parallel languages. It describes a new linguistic support, called an N-level message queue that designed to overlap communication and computation in a program. Based on an exact data-flow analysis on individual array element accesses, the compiler inserts data transmission requests to message queue into SPMD code. Experiments show that introducing an asynchronous transmission mechanism into data parallel language compilers is effective.
[Parallel languages, Optimizing compilers, Distributed computing, program compilers, compilers, parallel programming, Concurrent computing, linguistic support, SPMD code, Program processors, array references, Performance analysis, Data communication, parallel languages, data parallel languages, Data analysis, distributed memory multiprocessors, data transmission requests, Programming profession, distributed memory systems, optimization techniques, exact data-flow analysis, asynchronous data transmission mechanism, N-level message queue, Queueing analysis]
Distribution transparent MIB based on MSA (Management System Agent) model
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
OSI Management model defines only peer to peer communication process between a managing application (AP) and a managed AP. Thus, when more than one managed AP stores portions of Management Information Base (MIB) distributively, the managing AP needs a sophisticated distributed processing function. This paper proposes an agent, called Management System Agent (MSA) which provides location transparency and schema transparency of MIB for managing APs, to ease the realization of managing APs. Location transparency hides the distribution of MIB from managing APs. Schema transparency also enables managing APs to invoke management operation on unified schema. This paper demonstrates the effectiveness of the MSA model through applying it to implementation of Customer Network Management (CNM) system focusing X.25 public data networks.
[Terminology, open systems, Laboratories, distributed processing, location transparency, Information management, distribution transparent MIB, Research and development, Distributed processing, management system agent model, Management Information Base, Aging, MSA, managing application, customer network management system, managed AP, Peer to peer computing, management information systems, peer to peer communication process, management operation, X.25, computer network management, OSI management model, Open systems, Research and development management, Joining processes, schema transparency]
Program dependence analysis of concurrent logic programs and its applications
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
In this paper a formal model for program dependence analysis of concurrent logic programs is proposed with the following contributions. First, two language-independent program representations are presented for explicitly representing control flows and/or data flows in a concurrent logic program. Then based on these representations, program dependences between literals in concurrent logic programs are defined formally, and a dependence-based program representation named the Literal Dependence Net (LDN) is presented for explicitly representing primary program dependences in a concurrent logic program. Finally, as applications of the LDNs, some important software engineering activities including program slicing, debugging, testing, complexity measurement, and maintenance are discussed in a programming environment for concurrent logic programs.
[Software testing, program debugging, data flows, Optimizing compilers, Communication system control, concurrent logic programs, program dependence analysis, Logic testing, language-independent program representations, parallel programming, programming environment, dependence-based program representation, Literal Dependence Net, logic programming, debugging, software engineering, complexity measurement, Software measurement, program slicing, Logic programming, control flows, testing, Application software, software maintenance, Programming environments, Computer science, formal model, programming environments, maintenance, Software engineering, software metrics]
An optimal deadlock resolution algorithm in multidatabase systems
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
In this paper, we propose a novel deadlock resolution algorithm. In the presence of global deadlocks in a multidatabase system, this algorithm always selects an optimal set of victims for removing deadlocks. It makes use of network flow techniques, and runs in time O(n/sup 3/), where n is the number of the deadlocked global transactions. Furthermore, the proposed deadlock resolution algorithm does not have livelock and transaction processing starvation problems.
[transaction processing, global deadlocks, Costs, Programming, Concurrency control, Transaction databases, Software development management, deadlock resolution, Computer science, Concurrent computing, concurrency control, distributed databases, global transactions, System recovery, multidatabase systems, Database systems, Communication networks, multidatabase system]
Efficient fault protection of block gradient-based adaptive filters
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
Adaptive FIR filters are important in modern digital signal processing applications, and many situations require significant computational power attainable only through multi-processor configurations. Various block processing techniques have been proposed as viable alternatives to the sequential implementations to achieve the high throughput rate and reduce the effect of finite word length. While there are algorithm-based fault-tolerant techniques for protecting general digital signal processing systems against temporary/permanent failures, the specific features and requirements of the adaptive filtering systems have been largely ignored in the fault tolerance literature. By identifying common operations among different block adaptive filtering algorithms, however most operations of a particular algorithm can be protected with reasonably low-cost. We present a fine-grained approach, a highly efficient implementation of the well-known checksum encoding scheme, for protecting block adaptive filters. Through properly partitioning the input data, checksums for a parallel adaptive FIR filter can be calculated with reduced complexity. Computational and hardware overhead of the proposed scheme are analyzed, and shown to be proportional to the total number of partitioned submatrices.
[fault-tolerant techniques, checksums, complexity, finite word length, signal processing, Adaptive filters, Throughput, block gradient-based adaptive filters, FIR filters, Fault tolerant systems, Filtering algorithms, fault protection, adaptive filters, digital signal processing applications, sequential implementations, Encoding, Partitioning algorithms, encoding, fine-grained approach, Power system protection, Signal processing algorithms, Digital signal processing, Finite impulse response filter, computational power, fault tolerant computing, encoding scheme, computational complexity, block processing techniques, block adaptive filtering algorithms]
Bubblesort star graphs: a new interconnection network
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
In this paper, we propose and analyze a new interconnection network called bubblesort star graph, which is the merger of the bubblesort graph and the star graph. We present the deadlock-free wormhole routing algorithm for the proposed network. We also develop the method to embed a mesh into a bubblesort star graph with dilation two and expansion one. Besides, we use the recursive scheme to embed the multiple disjoint copies of the hypercube into a bubblesort star graph with all faults recovery capacity as well as constant expansion and dilation one or two. This reflects the fact that the embeddability of the bubblesort star graph is much better than that of the star graph.
[dilation two, hypercube, Corporate acquisitions, Multiprocessor interconnection networks, network routing, graph theory, multiprocessor interconnection networks, Routing, Information management, Electronic mail, deadlock-free wormhole routing algorithm, Computer science, Network topology, bubblesort star graphs, interconnection network, Binary trees, System recovery, Hypercubes, multiple disjoint copies, expansion one, mesh]
A study on call modelling for AIN/B-ISDN integration
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
ITU-T has considered the Intelligent Network(IN) Capability Set 2(CS 2) phase, which includes the FPLMTS capabilities for providing mobile services in Advanced IN(AIN) based network. Simultaneously, AIN and B-ISDN integrated architecture would be recommended upto IN CS-3 phase. This paper describes the call model of new Service Switching Point(SSP) based on the integration technology of AIN and B-ISDN. It may be the switching functions which will be much affected by AIN/B-ISDN integration. We modify functional elements for integrated network focusing on Broadband SSP(B-SSP) as well as a call model supporting the separation of call control and connection control. We propose a three layered call model, which consists of session control state model, resource control model, and bearer control model, that supports various types of call (multi connection, multi party, etc.). Centralized originating and terminating parties. AIN/B-ISDN integrated call modeling should fully include B-ISDN signaling capabilities and AIN benefits. In this paper, the proposed call model is evaluated for specific multimedia service, Video On Demand(VOD) service.
[connection control, Communication system control, session control state model, multimedia systems, AIN/B-ISDN integration, call control, Centralized control, Intelligent networks, ISDN, Service Switching Point, Bandwidth, Telephony, Video On Demand, resource control model, three layered call model, B-ISDN, Multimedia systems, call modelling, Telecommunication switching, Topology, Intelligent Network, intelligent networks, multimedia service, bearer control model, Resource management, ITU-T]
A flexible protocol synthesis method for adopting requirement changes
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
Communicating entities in a protocol specification communicate with each other and provide services to their users. Once the behaviours of the entities (specification of the protocol) are specified they are not changed. The entities provide a fixed set of services to their users. However, different users have different requirements and their requirements change often. The requirement changes in general are small in terms of the size of the behaviour expressions of the entities. Traditional protocol synthesis techniques have given considerable attention in construction of new protocols for fixed set of services but less attention to the attractive maintenance issue of protocol to adopt new protocol requirement changes. What is desirable is a protocol synthesis method which adopts new protocol requirement changes into the behaviours of entities or protocol specifications. In this paper, we propose a protocol synthesis method which adopts the new protocol requirement changes into the protocol specification. In this way, we can use existing protocol specifications and maintain them to adopt requirement changes. We use the formal specification language LOTOS to specify requirement changes and the behaviours of entities.
[Protocols, Educational technology, Educational institutions, Specification languages, Formal specifications, software maintenance, formal specification, requirement changes adoption, Asynchronous communication, Information processing, specification languages, maintenance issue, Computer errors, formal specification language LOTOS, Network synthesis, Computer networks, protocol synthesis method, protocols, communicating entities, flexible protocol synthesis method, protocol specification]
Distributed checkpointing based on influential messages
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
In distributed applications, a group of multiple objects are cooperated to achieve some objectives. The computation on the objects are based on the massage passing, i.e. remote procedure call. The objects may suffer from different kinds of faults. In the presence of the object faults, the states of the objects in the system have to be kept consistent. If some object o is faulty, o is rolled back to the checkpoint and objects which have received messages from o are also required to be rolled back. In this paper, we define influential messages whose receivers are required to be rolled back from the application point of view if the senders are rolled back on the basis of the message semantics. By using the influential messages, we would like to define a significant checkpoint which denotes a consistent global state of the system but might be inconsistent from the traditional definition. We would like to present protocols for taking the significant checkpoint and for rolling back the objects by using the influential messages.
[Checkpointing, message passing, consistent global state, distributed processing, Data structures, distributed checkpointing, Application software, Distributed computing, object faults, Teleconferencing, influential messages, Systems engineering and theory, remote procedure calls, remote procedure call, protocols, massage passing]
Adaptive routing in k-ary n-cube multicomputers
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
Wormhole routing makes the communication latency insensitive of the network diameter and reduces the buffer size of each router. It has been widely adopted in many new generation distributed memory multiprocessor systems (DMMPs). The concept of virtual system is invented for deadlock-free routing design. Using a great deal of virtual channels is quite expensive to be implemented in practice and is not limitless. In this paper experimental results are given to evaluate the tradeoff between network congestion delay and throughput. Some results show that the congestion delay does not reduce visibly when the number of virtual channels is greater than that of internal channels by 2. A reasonable number of virtual channels can be derived from the results. A virtual networks system using only four virtual networks, two virtual channels in each direction, is presented. It shows that the minimum number of virtual networks to support a partially adaptive, minimal and deadlock free routing in k-ary n-cubes is just four. It uses only four virtual networks but can get a high degree of adaptiveness and high traffic capacity. Simulation results are presented to verify the performance.
[Costs, virtual reality, Throughput, hypercube networks, digital simulation, Delay, buffer size, Multiprocessing systems, Intelligent networks, k-ary n-cube multicomputers, deadlock free routing, throughput, Computational modeling, network routing, adaptive routing, performance evaluation, Routing, deadlock-free routing, distributed memory multiprocessor systems, Parallel architectures, communication latency, Communication switching, wormhole routing, virtual system, simulation results, distributed memory systems, System recovery, network congestion delay]
Transactional programming for distributed agent systems
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
A new multiagent programming paradigm based on the transactional logic model is developed. This paradigm enables us to construct a distributed agent transactional program (DATRAP). Such a construction is carried out in two stages: first expressing a program into a production rule system, and then converting the rule applications into a set of transactions on a database of active objects represented using high-level data structures. The formal specification and refinement calculus are key features in the development of a DATRAP. We also indicate how to specify granularity of parallelism and also achieve several types of parallelism. One can associate with a DATRAP two different types of execution semantics called set-based and instance-based semantics. We also show how to prove correctness of DATRAP, achieve maximal concurrence and reduce the complexity of a distributed program.
[complexity, high-level data structures, refinement calculus, execution semantics, formal specification, database, production rule system, Production, Parallel processing, transactional programming, Mathematical model, distributed program, Mathematical programming, Logic programming, multiagent programming paradigm, Transaction databases, Information technology, DATRAP, active objects, transactional logic model, distributed algorithms, granularity, distributed agent systems, Problem-solving, Australia, Artificial intelligence, computational complexity]
An efficient algorithm for set-to-set node-disjoint paths problem in hypercubes
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
Set-to-set node-disjoint paths problem is that given two sets S={s/sub 1/,...,s/sub k/} and T={t/sub 1/,...,t/sub k/} of nodes in a graph, find k node disjoint paths s/sub i//spl rarr/t/sub ji/, where (j/sub 1/,...,jk) is a permutation of (1,...,k). For general undirect graphs G(V,E), this problem is usually solved by applying flow techniques which take Poly(|V|) time. In this paper, we give an algorithm which, given S={s/sub 1/,...,s/sub k/} and T={t/sub 1/,...,t/sub k/}, 1/spl les/k/spl les/n, in an n-dimensional hypercube H/sub n/ which has 2/sup n/ nodes, finds the k disjoint paths s/sub i//spl rarr/t/sub ji/ of length at most n+log k+2 in O(kn log* k) time. This improves the previous results of n+k and O(kn log k), respectively.
[Optical fibers, set-to-set node-disjoint paths, Multiprocessor interconnection networks, graph theory, Very large scale integration, hypercube networks, Graph theory, Topology, hypercubes, interconnection networks, undirect graphs, Distributed processing, node-disjoint paths, Hypercubes, Computer networks, Graph algorithms, Books]
Deadlock-free routing in an optical interconnect for high-speed wormhole routing networks
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
The Supercomputer SuperNet (SSN) is a two-level hierarchical high-speed network. The lower level is a high speed electronic mesh fabric; the higher level is a WDM optical backbone network interconnecting the high-speed fabrics distributed across a campus or metropolitan area. The salient characteristics of this architecture are the use of wormhole routing and backpressure hop-by-hop flow control mechanism. Because of these features, deadlocks are possible in SSN. In this paper, we address the issue of deadlock-free routing which is an essential prerequisite for the proper operation of SSN. To this end, we first present a deadlock free routing scheme for the WDM backbone which is implemented with a shufflenet multihop virtual topology. We use the notion of virtual channels to obtain mappings of virtual channels to physical channels such that deadlock-free routing is achieved for any (p,k) shufflenet (uni and bidirectional). Then, we compare the virtual channels scheme with the more conventional up/down deadlock free routing scheme for the bidirectional shufflenet and show that the former yields much better performance. Finally, we address the problem of deadlock prevention across the entire network (i.e., lower level fabric as well as the optical backbone) and develop an integrated solution combining different schemes best suited for the different levels.
[wavelength division multiplexing, deadlock prevention, optical backbone, Spine, multiprocessor interconnection networks, electronic mesh fabric, Optical fiber networks, physical channels, Supercomputer SuperNet, High-speed networks, two-level hierarchical high-speed network, shufflenet multihop virtual topology, Fabrics, high-speed wormhole routing networks, Optical interconnections, network routing, virtual channels, up/down deadlock free routing scheme, optical interconnect, Routing, Wavelength division multiplexing, deadlock-free routing, bidirectional shufflenet, Supercomputers, backpressure hop-by-hop flow control mechanism, optical interconnections, System recovery, metropolitan area, WDM optical backbone network, campus area, High speed optical techniques]
On parallelism of hyper-linking theorem proving: a preliminary report
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
This paper exploits the parallelism of a hyper-linking based theorem prover. We analyze the unique properties of the the hyper-linking proof procedure and present the preliminary results. With respect to these properties four parallel strategies, phase-level, clause-level, literal-level, search level parallelism are designed for different implementation schemes of the prover. Results and analysis of the experiments on these parallel strategies are presented.
[search level parallelism, Production systems, parallel algorithms, parallel strategies, parallel architectures, Knowledge based systems, parallelism, phase-level, hyper-linking theorem proving, Parallel architectures, Sun, artificial intelligence, Concurrent computing, literal-level, clause-level, Parallel processing, hyper-linking proof procedure, theorem proving, Artificial intelligence, Joining processes, Testing, Electrons]
Register renaming for x86 superscalar design
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
Register renaming eliminates storage conflicts for registers to allow more instruction level parallelism. This idea requires nontrivial implementation, however, especially when registers are accessible with different fields and data lengths. As a result, not all bits in a register are to be updated upon a register write, and a register read may be data-dependent on multiple register writes. We propose two hardware renaming schemes to solve these difficulties: One for its ultimate performance, and the other for its desirable cost/performance ratio. We evaluate these two schemes on an aggressive superscalar machine model for Intel 80/spl times/86 architecture. Simulation results show that the second scheme can effectively reduce the hardware cost while retaining about 99% of the performance of the first.
[Out of order, Costs, Computational modeling, parallel architectures, microprocessor chips, Registers, instruction level parallelism, hardware renaming schemes, Electrostatic precipitators, Computer science, storage conflicts, Microprocessors, Councils, register write, simulation results, data lengths, Parallel processing, aggressive superscalar machine model, Hardware, register renaming, Intel x86 superscalar design, register read]
Info-Plaza: A social information filtering system for the World-Wide Web
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
We have prototyped a social information filtering system based on personal interests. The system is called Info-Plaza and is accessed via the World-Wide Web. Info-Plaza helps people to get new URLs which likely point to interesting web pages. Info-Plaza is a client-server system, which consists of XPlaza and Plaza-Server. XPlaza is a GUI-editor to create and arrange URL-Maps. The URL-Maps are used to register interesting web pages with the server. Plaza-Server has a database of URL-Maps collected from many users. It receives a URL-Map from a user, makes a new URL-Map based on similarities between the URL-Map, and the database entries, and then returns the new URL-Map, to the user. Thus users can exchange interesting URLs through Plaza-Server. Users are expected to edit the returned URL-Map, leaving useful URLs and removing unnecessary URLs. After repeating this process several times, the user will have a URL-Map that contains many links to interesting sites.
[client-server systems, URL-Maps, Laboratories, Info-Plaza, Plaza-Server, Information filtering, database management systems, GUI-editor, Uniform resource locators, XPlaza, Information science, database, Databases, World-Wide Web, Web pages, Prototypes, Virtual groups, social information filtering system, Information filters, database entries, Internet, client-server system]
Implementation of MAP: A system for mobile assistant programming
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
We have define a network programming model called Mobile Assistant Programming (MAP) for development and execution of communication applications in large scale networks of heterogeneous computers. MAP assistants are high-level interpreted programs that can move between nodes, create clones and report results. Their execution is asynchronous and persistent to take into account client disconnections and node failures. This paper presents the implementation of the MAP model using the World-Wide Web framework and the Scheme programming language. Our measures and the first experience with MAP applications show that significant performance improvement can be achieved by moving computation closer to data: both the elapsed time and network traffic are reduced.
[high-level interpreted programs, Cloning, computer networks, Telecommunication traffic, distributed processing, Mobile communication, Time measurement, mobile assistant programming, Application software, performance improvement, network traffic, Computer languages, World-Wide Web, heterogeneous computers, Traffic control, Computer networks, Large-scale systems, MAP, Mobile computing]
Fast sorting algorithms on reconfigurable array of processors with optical buses
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
The Reconfigurable Array with Spanning Optical Buses (RASOB) has recently received a lot of attention from the research community. By taking advantage of the unique properties of optical transmission, the RASOB provides flexible reconfiguration and strong connectivities with low hardware and control complexities. In this paper, we use this architecture for the design of efficient sorting algorithms on the 1-D RASOB and the 2-D RASOB. Our parallel sorting algorithm on the 1-D RASOB, which sorts N data items using N processors in O(k) time where k is the size of the data items to be in bits, is based on a novel divide-and-conquer scheme. On the other hand, our parallel sorting algorithm on the 2-D RASOB is based on the sorting algorithm on the 1-D RASOB in conjunction with the well known Rotatesort algorithm. This algorithm sorts N data items on a 2-D RASOB of size N in O(k) time. These sorting algorithms outperform state-of-the-art sorting algorithms on reconfigurable arrays of processors with electronic buses.
[parallel algorithms, Optical interconnections, Optical switches, parallel architectures, parallel sorting algorithm, Communication system control, system buses, reconfigurable array, sorting algorithms, Optical arrays, Sorting, Optical control, optical buses, Computer science, reconfigurable architectures, Optical buffering, optical interconnections, sorting, reconfigurable array of processors, Hardware, Reconfigurable architectures, reconfigurable arrays]
A new scheduling strategy for NUMA multiprocessor systems
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
This study examined the impact of multiprocessor scheduling strategies on the performances of non-uniform memory access (NUMA) shared-memory multiprocessor systems. A new scheduling algorithm named the Longest Extended Critical Path First (LECPF) algorithm is proposed here. The proposed algorithm schedules parallel tasks by considering complex communication options and the contentions of shared communication resources. It also ensures performance within a factor of two of the optimum for general directed acyclic task graphs (DATGs). Experimental results show the superiority of the LECPF algorithm over that of scheduling algorithms in the literature.
[Thyristors, Costs, Heuristic algorithms, multiprocessor systems, Optimal scheduling, Longest Extended Critical Path First, shared-memory multiprocessor, scheduling strategies, Scheduling algorithm, Delay, processor scheduling, Multiprocessing systems, Computer science, Processor scheduling, non-uniform memory access, Clustering algorithms, shared memory systems, NUMA]
Double parity sparing for performance improvement in disk arrays
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
RAID level 5 disk arrays provide highly reliable, cost effective secondary storage with high performance for reads and large writes. Small writes, however, cause the degradation of their performance, since small writes need additional disk accesses to update parity data. There have been many studies to overcome this problem. In this paper, we propose a new technique, double parity sparing, which is a variation of disk array with a spare. To improve the parity update process, the proposed scheme uses a spare block and a parity block as two available parity blocks. We present results from simulations to show that the proposed scheme offers better performance while there is no failure in disk arrays.
[magnetic disc storage, Costs, Delay effects, disk arrays, performance evaluation, Throughput, double parity sparing, degradation, parity data, performance improvement, Computer science, Degradation, storage management, Microprocessors, Protection]
Two-level-hierarchies of objects: a unit of reference for distributed object-oriented databases
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
We introduce and formalize the concept of two-level-hierarchies (t-l-hs) of objects. The t-l-hs are special partial directed acyclic graphs having as vertices objects. An equivalence relation is also defined on their edges. They are used for modelling complex objects by supporting, in a uniform way, the well known abstractions like association, aggregation, grouping as well as specialization and generalization. Considering the t-l-h as the design unit, we develop a new design process (Uni-Design) for the distributed object-oriented databases (DOODBs) by combining the main design approaches (Top-Down and Bottom-Up) in a consistent and complementary way leading to capture and represent explicitly most of the semantics of a system. We also choose the t-l-h as the elementary unit of distribution in order to adopt a distribution design strategy (a DOODB is now a set of possibly interrelated t-l-hs) and reconsider the notions of fragmentation and replication by integrating the distribution process at the last phase of the Uni-Design process.
[Process design, Encapsulation, Object oriented databases, unit of reference, vertices objects, Object oriented modeling, distribution design strategy, object-oriented databases, two-level-hierarchies of objects, Relational databases, Africa, association, aggregation, Distributed computing, grouping, Computer science, directed graphs, Distributed databases, distributed databases, partial directed acyclic graphs, Uni-Design, Computer networks, equivalence relation, distributed object-oriented databases]
A wireless multiple access control protocol for voice-data integration
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
In this paper, a new wireless medium access control protocol, called Mini-Packets Reservation Multiple Access (MPRMA), is proposed for the microcellular wireless environments to support voice and data traffic. Instead of using information packets to contend for reservation, the protocol uses smaller reservation packets for contention. Thus, regular slots can be further divided into smaller slots for sending reservation packets. Furthermore, in order to facilitate subsequent packet transmission, a continuation flag is used in each information packet for requesting further reservation. Comparing to the Packet Reservation Multiple Access (PRMA), it is shown by simulation that MPRMA can support more voice and data users, respectively, in a single traffic type environment. And more importantly, MPRMA is also a better alternative to PRMA for supporting voice-data integration. In order to counter adverse channel conditions, a strategy is also proposed. In addition, MPRMA requires no deadline scheduling at the base stations and minimizes overhead bandwidth on the downlink for sending acknowledgment messages.
[Access control, Base stations, MPRMA, overhead bandwidth, Access protocols, continuation flag, wireless multiple access control protocol, voice-data integration, Time division multiple access, PRMA, Bandwidth, microcellular wireless environments, Media Access Protocol, data communication, Communication system traffic control, Mini-Packets Reservation Multiple Access, voice communication, Personal communication networks, Data communication, wireless LAN, Propagation delay, packet reservation multiple access]
Relaxed consistency requirements for replicated objects
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
Shared objects are usually replicated in a distributed environment for higher availability and fault tolerance. Coherent or strongly consistent implementation of replicated objects have been studied extensively in the literature. This paper concentrates on a general method for specifying and implementing replicated objects that only require weaker consistency conditions. The weakness of the objects is left open and users are allowed to specify their objects by defining how concurrent sets of operations should be performed. The implementation of a replicated object consists of two modules: one module is an algorithm to identify concurrent sets from a distributed computation, and the other module is a user defined procedure to process the concurrent sets. We show examples of weaker objects that can be defined and implemented efficiently because of the less stringent requirements imposed on these objects. In particular, the ordered set dictionary problem is used to demonstrate our scheme and we show that non-blocking implementation of the dictionary is possible.
[ordered set dictionary problem, Dictionaries, multiprocessing systems, object-oriented programming, fault tolerance, Read-write memory, distributed environment, availability, weaker consistency conditions, History, Distributed computing, Delay, Computer science, Concurrent computing, replicated objects, Distributed databases, concurrency control, Communication channels, Broadcasting, concurrent sets]
Communication protocol for group of distributed objects
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
In distributed applications, a group of multiple objects are cooperated. On receipt of request messages, the objects send back the responses. The kinds of group communication protocols discussed so far, support the reliable and ordered delivery of messages at the network level. Only messages to be ordered at the application level, not necessarily all messages, are required to be causally delivered in the required order. The state of the object depends on in what order the requests are computed and the responses and requests are transmitted. In this paper, we would like to define the significant precedence order of messages based on the conflicting relation among the requests. We would like to discuss a protocol which supports the significantly ordered delivery of request and response messages.
[Protocols, Client-server systems, distributed processing, communication protocol, Application software, Distributed computing, Teleconferencing, protocol, transport protocols, ordered delivery, multiple objects, distributed objects, Systems engineering and theory, Telecommunication network reliability, conflicting relation]
Distributed arithmetic-based architectures for high speed IIR filter design
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
Dedicated VLSI has been considered as an effective method to realize the DSP algorithms which require massive amount of computations. To speed up the computing, parallel processing and pipelining techniques are often employed. However, for those recursive algorithms where the available computing concurrency is very limited, these design tactics do not help. Previous results suggest a Look-ahead transform method applied to the algorithm first to create the parallelism at the cost of drastically increased hardware complexity. In this paper, we present a Distributed Arithmetic based scheme to solve the problem without resorting to the expensive look-ahead methods. In contrast to the conventional "bit-parallel word-serial" computing paradigm, the new scheme features a "bit-serial word-parallel" approach. In this scheme, instead of waiting the entire data word available from the previous recursion, current recursion's computation can start as soon as the LSB from the previous recursion is obtained. This means the initiation interval between two successive input data is reduced from the delay of computing one word to the delay of computing one bit. To illustrate the merits of this new scheme, we present a DA based VLSI design for an IIR filter. The design is implemented using a 0.8 /spl mu/m SPDM technology and can achieve high throughput rate operation for real time applications.
[Algorithm design and analysis, recursive filters, Costs, parallel architectures, IIR filters, Very large scale integration, SPDM technology, Distributed Arithmetic, DSP applications, parallel processing, Delay, Pipeline processing, Concurrent computing, pipelining techniques, high speed IIR filter, digital arithmetic, Computer architecture, Digital signal processing, Parallel processing, Hardware, recursion]
Performance of scheduling strategies for client-server systems
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
Scheduling on client-server systems has not received much attention from researchers. Based on simulation this research presents a number of insights into system behavior and scheduling. Two phenomena, CPU monopolization by large service requests and software bottlenecking are observed to have a strong influence on system performance. Software bottlenecking is a new phenomenon, observed on distributed client server systems with multiple levels of servers and occurs when a higher level server is blocked waiting for a response to a service request from a lower level server. Policies based on request characteristics such as service times and path lengths are found to effectively control these effects and improve system performance.
[client-server systems, path lengths, Client-server systems, request characteristics, Computational modeling, simulation, CPU monopolization, Software performance, performance evaluation, digital simulation, Transaction databases, Distributed computing, Network servers, Processor scheduling, System performance, service times, scheduling, system performance, Systems engineering and theory, Resource management, higher level server, scheduling strategies performance, software bottlenecking]
Cost-balanced cooperation protocol in multi-agent robotic systems
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
This paper proposes a cooperation protocol based on the cost-balanced strategy for the Object-Sorting Task in multi-agent robotic systems. The protocol coordinates agents for carrying objects to destinations efficiently and effectively. Each agent autonomously makes subjective optimal decision, then the coordination algorithm resolves their conflicts by balancing the load, which is measured in terms of cost. Since coordination can be performed simultaneously with agent movement, it incurs very little overhead. The protocol is efficient because every agent runs the same algorithm to obtain the common results without further communication. Implementation of the protocol is realized on a distributed modular agent architecture for design simplicity, flexibility, and reactivity. Experimental results have shown that (1) the protocol has better performance than a previously proposed help-based cooperation protocol, (2) the protocol is flexible, and (3) the protocol can effectively utilize the agent power to achieve linear and superlinear speedup in most cases.
[help-based cooperation protocol, Protocols, cost-balanced cooperation protocol, performance evaluation, Mobile communication, Cleaning, mobile robots, multi-agent robotic systems, artificial intelligence, Painting, cost-balanced strategy, Object-Sorting Task, Computer science, Parallel robots, Robot kinematics, Computer architecture, Cost function, Power system reliability, cooperative systems, protocols, subjective optimal decision, distributed modular agent architecture, computational complexity]
Multimedia task reliability analysis based on token ring network
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
In this paper, we attempt to develop reliability models for the reliability analyses in distributed multimedia system. We propose two polynomial-time algorithms to compute the multimedia task reliability (MTR) and time-constraint multimedia task reliability (TCMTR) for distributed multimedia system based on token ring networks. Two main algorithms, multimedia task reliability for ring (MTRR) and multimedia task reliability for path (MTRP), are used to compute the MTR. For time constraint media access, we extend our proposed algorithms to compute TCMTR.
[Algorithm design and analysis, reliability models, time-constraint multimedia task reliability, Multimedia systems, multimedia systems, multimedia task reliability, Reliability engineering, Distributed computing, distributed multimedia system, Computer science, multimedia task reliability analysis, reliability analyses, Computer network reliability, polynomial-time algorithms, Polynomials, Computer networks, token ring network, Token networks, Time factors, token networks]
Scalable routing schemes for massively parallel processing using reconfigurable optical interconnect
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
We consider the message routing/broadcasting problem in an optically interconnected massively parallel processing system, where each node in the system sends/broadcasts randomly generated packets to others. The network model considered is the reconfigurable optical interconnect (ROI). It is based on the new device capabilities enabled by recent advances in optical technology. A ROI node can use light beams to transmit messages to any other nodes in the network, provided that no others transmit to the same destination concurrently. We present communication schemes that can achieve near optimal throughput with significantly lower delay. The difference in performance for routing is on the order of /spl Omega/(n/sup 2/3/ log log n/log n) when the number of nodes n in the network is large.
[Optical interconnections, Routing, Throughput, Optical receivers, message routing, Laser beams, parallel processing, device capabilities, Delay, message broadcasting, massively parallel processing, reconfigurable architectures, reconfigurable optical interconnect, optical interconnections, scalable routing schemes, Surface emitting lasers, randomly generated packets, Parallel processing, Broadcasting, massively parallel processing system, Vertical cavity surface emitting lasers]
Scheduling of conditional branches using SSA form for superscalar/VLIW processors
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
Global scheduling and optimization techniques are proposed to get more enough speedup for superscalar and VLIW (Very Long Instruction Word) processors. When we consider global scheduling and optimization, one of the most important issue is how to schedule conditional branches. Control dependences are caused by conditional branches and limit the scope of scheduling. Most of previous scheduling schemes are based on speculative or predicated execution techniques to overcome conditional branches. However, speculative execution requires computation for code motion and insertion of compensation code to preserve semantics. The complexity of scheduler is largely due to computation for code motion. In addition, performance is dependent on branch outcomes. Predicated execution makes it possible to schedule more simpler. But the difficulties in the design of the instruction set are a serious problem. This paper proposes scheduling method using SSA form. The scheduling algorithm can be more simpler by utilizing /spl phi/-functions aggressively because computations for code motion are not required. We don't need complex hardware support. Our scheme also makes the performance independent on the result of branch outcomes.
[complexity, parallel architectures, Optimal scheduling, VLIW, Delay, processor scheduling, Runtime, instruction set, optimization, VLIW processors, global scheduling, Parallel processing, Hardware, superscalar processors, instruction sets, SSA, very long instruction word processors, conditional branches, compensation code, Dynamic scheduling, conditional branches scheduling, Scheduling algorithm, Computer science, Processor scheduling, code motion, computational complexity]
A unified media presentation method for multimedia information networks
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
Hypermedia systems are very important as platforms in the future multimedia information network services. In order to provide more flexible and sophisticated multimedia presentation, it is desirable to simply and efficiently transmit various media data from the databases distributed over network and present them on the user station depending on scenario so as to semantically integrate into multimedia information objects. In this paper, a new hypermedia system which is based on object-oriented structure to perform two different temporal synchronization methods, "lip synchronization" and "scene synchronization" is introduced. Lip synchronization method performs, fine-grained temporal synchronization between continuous media, such as audio and video. On the other hand in scene synchronization, coarse-grained temporal relation between continuous media and discrete media, such as image, graphics or text is maintained depending on the presentation scenario. Furthermore, several media which are semantically related to each other are unified into a Media Group (MG) to simplify its media treatment. The MGs and their involved processing functions are encapsulated into a Media Object (MO) and easily controlled by message passing procedure. In this paper, implementation of the hypermedia model which performs scene synchronization function between discrete and continuous media in the MOs depending on the presentation scenario, and context switching function between each media presentation is precisely described. Finally "Electric Museum" as a prototype application of hypermedia systems is introduced to evaluate the suggested synchronization mechanism.
[scene synchronization, multimedia presentation, hypermedia, Multimedia databases, multimedia systems, unified media presentation method, database management systems, fine-grained temporal synchronization, Hypermedia systems, lip synchronization, Media Object, Electric Museum, Distributed databases, Prototypes, context switching function, message passing, Object oriented databases, Multimedia systems, Object oriented modeling, user station, synchronisation, Graphics, multimedia information objects, multimedia information networks, Message passing, Layout, information retrieval systems, object-oriented structure, synchronization methods, Context modeling]
Synchronization model for multimedia communication and presentation in distributed systems
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
The integration of time dependent media in applications requires the synchronization of media streams. The problem of synchronization is more important if we consider multimedia data presented in distributed systems. The specification of synchronization is supported by the synchronization editor, the creation and the transport of synchronization objects are managed by the synchronization coordinator, and the execution of multimedia presentations is performed by the synchronizer. We use a Petri nets model for the specification of synchronization, and propose a method using by the Normalized Relative Time Stamps (NRTS) to synchronize multimedia objects. We also use two policies, aggressive and conservative, for execution of synchronization presentations. We believe that a best synchronization scheme must be adaptive to various applications, i.e., the performance of synchronization among various media streams is dependent on the applications. On the view of resources utilization, we try to propose a scheme that can tune the accuracy of detecting the asynchrony. Thus, we can get the balance on the tradeoff between the performance of presentation and utilization of resources.
[Multimedia systems, Petri nets, Multimedia databases, Jitter, distributed processing, media streams, Multimedia communication, Synchronization, multimedia computing, multimedia presentations, synchronisation, Computer science, Network servers, Feedback, time dependent media, Streaming media, distributed systems, synchronization, multimedia communication, Clocks]
Programming concurrency and synchronisation in Actel
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
This paper introduces mechanisms for exploiting concurrency and synchronisation in Actel: a concurrent object based language. It focuses on issues of combining parallelism with object orientation, performance, and synchronisation. Actel offers several mechanisms such as a new mode of message passing called 'semi-reference' to achieve an efficient inter-object concurrency. Parallel functions and parallel compound statements are provided to exploit concurrency inside an object at several levels without recourse to explicit synchronisation. Implicit synchronisation is obtained through future variables.
[Costs, message passing, concurrent object based language, object orientation, Yarn, Centralized control, concurrency, synchronisation, Concurrent computing, performance, Message passing, Distributed control, Lead, System recovery, object-oriented languages, Actel, Protection, parallel languages]
Swapped networks: unifying the architectures and algorithms of a wide class of hierarchical parallel processors
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
In this paper, we propose a new class of interconnection networks, called swapped networks, for general-purpose parallel processing. Swapped networks not only generate a wide class of high-performance interconnection networks, but also generalize, and serve to unify, many proposed parallel architectures as well as their algorithms. We show that swapped networks can efficiently emulate hypercubes, high-dimensional meshes, or generalized hypercubes, while having node degrees significantly smaller than the emulated network in each case. We also show that some subclasses of swapped networks can achieve asymptotically optimal diameters. Swapped networks are highly modularized, make the use of fixed-degree building blocks possible for any practically realizable system, and lead to the construction of high-performance scalable networks with reasonable cost.
[parallel algorithms, Costs, Symmetric matrices, hierarchical parallel processors, fixed-degree building blocks, Multiprocessor interconnection networks, Scalability, parallel architectures, multiprocessor interconnection networks, generalized hypercubes, Parallel architectures, hypercubes, interconnection networks, Tree graphs, swapped networks, Computer architecture, Parallel processing, Hypercubes, Large-scale systems, high-dimensional meshes]
A management information repository for distributed applications management
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
The management of distributed applications and systems (MANDAS) project addresses problems arising in the management applications. The MANDAS information repository (MIR) provides database support for the management applications and supports their integration into a single management environment. We examine the problem of distributed applications management to extract the requirements for an MIR. Based on the requirements, we present an information model for distributed applications management and outline a prototype MIR developed for the MANDAS project.
[MANDAS information repository, Project management, management information systems, Information management, database support, Application software, Distributed computing, Environmental management, distributed applications management, MANDAS project, Operating systems, Prototypes, Distributed databases, information retrieval systems, distributed databases, management information repository, prototype MIR, Communication networks, Monitoring, information model]
PPD: A practical parallel loop detector for parallelizing compilers
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
It is well known that extracting parallel loops plays a significant role in designing parallelizing compilers. The execution efficiency of a loop is enhanced when the loop can be executed in parallel or partial parallel, like a DOALL or DOACROSS loop. This paper reports on the practical parallelism detector (PPD) that is implemented in PFPC (a portable FORTRAN parallelizing compiler running on OSF/1) at NCTU to concentrate on finding the parallelism available in loops. The PPD can extract the potential DOALL and DOACROSS loops in a program by invoking a combination of the ZIV test and the I test for verifying array subscripts. Furthermore, if DOACROSS loops are available, an optimization of synchronization statement is made. Experimental results show that PPD is more reliable and accurate than previous approaches.
[Chaos, Knowledge engineering, System testing, ZIV test, parallelising compilers, I test, OSF/1, PPD, parallelizing compilers, parallel programming, Concurrent computing, Information science, Program processors, practical parallel loop detector, array subscripts, synchronization statement, Detectors, Parallel processing, portable FORTRAN parallelizing compiler, practical parallelism detector, Equations, synchronisation, Computer languages, DOALL, DOACROSS loop, FORTRAN]
RDB structure chart display using fuzzy rules
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
With rapid spread of EUC (E_nd U_ser C_omputing), RDB (R_elational D~ataB_ase) construction and retrieval using GUI (G_raphical U_ser I_nterface) have become a major part of information network users. Since main activity in the RDB construction is its structure design, it must be done efficiently in order to shorten the construction term. Although display function of RDB entire structure (table structures and relations between tables) defined by users is provided as a "structure chart" display function in most RDBMS (R_elational D~ataB_ase M_anagement S_ystem) to support the activity, we can say that its quality is not high enough at present. For example, the chart contains many backward or intersecting arrows (relations between tables) and tables in the chart are not well-balanced. Therefore, it is not easy to grasp the entire structure by the present chart. This has been the cause of inefficiency in the RDB construction. In this paper, we propose a new method of making and displaying a high quality structure chart using BBM (B_ranch and B_ound M_ethod) and fuzzy inference as a solution to the above problems. Furthermore, we verify the effect of the proposed method in experiment.
[Computer interfaces, graphical user interfaces, fuzzy inference, Laboratories, graphical user interface, Grasping, fuzzy logic, fuzzy rules, RDB structure chart display, Displays, Information retrieval, information network users, end user computing, relational database, relational databases, inference mechanisms, table structures, Database languages, high quality structure chart, Computer networks, Graphical user interfaces, Quality management, Cascading style sheets]
Exploiting the locality of data structures in multithreaded architecture
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
Multithreaded architectures taking a hybrid approach of von Neumann computers and dataflow computers are recently in active research. Multithreaded architectures can improve the performance by the locality exploitation within threads and asynchronous parallel execution among threads. However, it has been overlooked how to exploit effectively the locality of large shared data structures among threads in multithreaded architectures. In this paper, we propose a new data structure, called IB-structure, which is an extension of I-structure to facilitate the distribution of data structure and exploit the locality of data structure, and implement it on multithreaded architecture DAVRID (DAtaflow Von Neumann RISC hybrID). The simulation results show that IB-structure is superior to I-structure over several benchmarks.
[asynchronous parallel execution, Roads, parallel architectures, von Neumann computers, Data structures, digital simulation, Yarn, Electrostatic precipitators, Petroleum, Programming profession, Computer science, Concurrent computing, locality exploitation, DAVRID, I-structure, performance, Microprocessors, dataflow computers, simulation results, Computer architecture, data structures, multithreaded architecture, IB-structure]
Support of cooperating and distributed business processes
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
Workflow management systems/business process management systems (BPMS) provide for an integral support of computer-based information processing, personal activities, business procedures and their relationships to organizational structures. They support the modeling and analysis of so-called business processes and offer means for the application-near design and implementation of computer-based business process assistance. Mainly, the BPMSs concentrate on the support of enterprise-internal processes. Our approach extends the scope of business process management. Enterprise-internal processes are viewed as sub-processes of global inter-enterprise processes. Additional global process assistance is based on the definition of global activity models and global information models. Features of dynamic naming and binding can be provided by business process brokers, which extend the concepts of object trading to the trading of opportunities to participate in global processes.
[distributed business processes, Companies, business procedures, binding, management information systems, Transaction databases, computer-based business process assistance, object trading, Environmental management, cooperating business processes, workflow management systems, personal activities, business process management systems, Distributed databases, dynamic naming, distributed databases, computer-based information processing]
Automatic modification of a protocol specification based on changes of a service specification
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
The problem of modifying an existing protocol specification of a communicating system in Labelled Transition System (LTS) based on changes of it's service specification in LTS is discussed. Two types of changes of a service specification are treated: addition of a behaviour expression to the service specification and deletion of some parts of the behaviour expression from the service specification. For each change, a modification method is proposed and a property of the method is shown. From the point of view of verification and validation of a specification, it is desirable to separate a changed part of the specification from the existing specification. Thus, especially for the former type of changes that is addition of a behaviour expression to the service specification, the method to express changes of the service specification by using a concurrent LTS is proposed. The method proposed in this paper is efficient and desirable when the size of the changed part of the service specification is small in terms of behaviour expressions.
[Protocols, Labelled Transition System, Educational technology, Communications technology, Formal specifications, protocols, formal specification, service specification, verification, validation, protocol specification]
All-fault-tolerant embedding of a complete binary tree in a group of Cayley graphs
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
This paper proposes an approach for embedding a complete binary tree with height k/spl times/(n-2/sup k/+1)+(k-2)/spl times/2/sup k/+1, where k=[log n], into an n-dimensional complete transposition graph (CT/sub n/), star graph (ST/sub n/), and bubblesort graph (BS/sub n/) with dilations 1,3, and 2n-3 respectively. Furthermore, a fault-tolerant scheme is developed to recover multiple faults, and the dilations after recovery become at most 3,5, and 2n-1 for the CT/sub n/, ST/sub n/, and BS/sub n/ respectively.
[n-dimensional complete transposition graph, bubblesort graph, Costs, Multiprocessor interconnection networks, multiprocessor interconnection networks, Information management, Parallel algorithms, Sorting, Concurrent computing, Fault tolerance, complete binary tree, fault-tolerant scheme, star graph, all-fault-tolerant embedding, Binary trees, Hypercubes, fault tolerant computing, tree data structures, Cayley graphs]
Some sparse time-relaxed broadcast communication networks
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
Broadcasting is the process of information dissemination in communication networks (modelled as graphs) whereby a message originated at one vertex becomes known to all members under the constraints that each call requires one unit of time and at every step any member can call at most one of its neighbours. A broadcast graph on n vertices is a network in which message can be broadcast in minimum possible (=[log/sub 2/n]) time regardless of originator. Broadcast graphs having the smallest number of edges are called Minimum Broadcast Graphs, and are subject of intensive study. On the other hand, in Shastri (1995) we have considered how quickly broadcasting can be done in trees. In this paper, we construct more and more sparse time-relaxed broadcast graphs for some small values of n, as we allow additional time over [log/sub 2/n], until we get a tree.
[Minimum Broadcast Graphs, Protocols, information dissemination, graph theory, multiprocessor interconnection networks, time-relaxed, broadcast communication networks, broadcast graphs, Computer science, Strontium, Tree graphs, telecommunication networks, sparse, Bidirectional control, Broadcasting, information theory, Communication networks, Contracts]
Studies for realizing soft TCCS residing on top of Ethernet LAN
Proceedings of 1996 International Conference on Parallel and Distributed Systems
None
1996
In this paper, the theoretical studies and solutions regarding problems for realizing Soft TCCS (Time Critical Communication System), residing on top of Ethernet LAN are presented. We analyze Ethernet LAN including its precise back off sequence to obtain throughput and delay characteristics in a light loaded circumstances. We also analyze the same performance characteristics effected by multi-modal condition which is the mode of transmitting both short packets for control messages and long packets for bulk data concurrently in Factory Automation system. These theoretical studies are validated through computer simulations. We end by presenting the definition of the bandwidth in Ethernet LAN and also formulae regarding its estimation needed for admission control to maintain the total load within the predetermined value.
[Ethernet networks, Communication systems, Computer simulation, Ethernet LAN, Throughput, Control systems, admission control, local area networks, Factory Automation system, Delay, computer communications software, Automatic control, Time Critical Communication System, Performance analysis, performance characteristics, Local area networks, Manufacturing automation, Soft TCCS]
Scalable parallel and cluster computing
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Summary form only given, as follows. In this talk, Dr. Hwang will present two approaches to scalable parallel computing: The first approach is based on scaling STAP benchmarks on Cray T3D, IBM SP2, and Intel Paragon. STAP was originally developed by the MIT Lincoln Laboratory for real-time signal processing. The benchmark suite was recently parallelized and tested on several available MPPs (massively parallel processors). The second approach is to develop an SSI (Single-System-Image) cluster of UNIX workstations and SMP servers for distributed multimedia and Intranet applications. The talk will evaluate scalable multiprocessors and network-based cluster architectures. Specific R/D topics to be covered include the following: - STAP benchmark evaluation of T3D, SP2, Paragon, and Power Challenge/XL - Architectural - Hardware and software support for SSI in a cluster of workstations and servers. - Distributed cluster computing in multimedia and Intranet applications. assessment of recent advances in CC-NUMA and Clusters. Scalable parallel computer platforms provide not only an incremental growth path, but also performance scaling and enhanced availability. These features are very much desired in today's business and scientific applications. Dr. Hwang will share the research findings and benchmarking experiences from his research groups at USC and HKU. He will also comment on the research, development, and application trends in parallel and cluster computing.
[Concurrent computing, Network servers, Computer architecture, Benchmark testing, Parallel processing, Signal processing, Hardware, Workstations, Application software]
Post modern distributed systems [keynote address]
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Summary form only given, as follows. With the increasing speed of computers and communication linlts, and the successful convergence of both fields, computers connected by high speed links now represent an enormously large distributed computing system. However, current information distributed system architectures are too rigid to cope with heterogeneity, inaccuracy and inflexibility of the real world requirements. In order to overcome such limitations, in this talk, we present Post Modem Distributed System based on Flexible Network using the concept of Flexible Computing proposed by the authors. We first give the outline of the Flexible Computing and the basic architecture of Flexible Network based on multi-agent systems. Then the characteristics of Modem Distributed Systems based on Flexible Network will be discussed. As application examples, we will show a virtual office and flexible video conference system.
[Multiagent systems, Teleworking, Computer architecture, Modems, Computer networks, Distributed computing, Videoconference, Convergence]
An equivalence algorithm to point out errors for basic LOTOS in a distributed system environment and its prototype
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
LOTOS formal description technique (FDT) is applied to the formal description of distributed systems and the specification of OSI protocol layers. LOTOS is difficult to understand and to learn because it is based on a mathematical model. In this paper, for supporting the knowledge acquisition of learners studying basic LOTOS, we suggest a new algorithm that can verify an equivalence relation, find an error location, and correct an error when the error occurs at another process. Finally, we give an example of a prototype program for an educational support system for basic LOTOS.
[LOTOS, Protocols, open systems, equivalence algorithm, distributed processing, distributed system, educational support system, Electronic mail, error detection, formal specification, Communication standards, Prototypes, specification languages, Computer science education, Mathematical model, protocols, error correction, Educational programs, computer science education, specification, formal description technique, prototype, OSI protocol layers, equivalence relation verification, Open systems, Computer errors, mathematical model, Error correction, errors]
Detection of summative global predicates
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
In distributed programs, we usually keep some global predicates from being satisfied to make it easy to run the programs correctly. A common type of global predicates are: the total number of certain tokens in the whole distributed system is always the same or in specific ranges. In this paper, we call this summative predicates, classified into the following four: (1) at some global state of the system, N/spl ne/K, (2) N<K (or N/spl les/K), (3) N>K (or N/spl ges/K), and (4) N=K, where N is the total number of tokens and K is a constant. This paper investigates the methods of detecting various summative global predicates. The first class of summative predicates are trivial to detect by simply checking each message. For the second class of summative predicates, B. Groselj (1993) and V.K. Garg (1995) solved the problem by reducing the problem to a maximum network flow problem. In this paper, we propose an elegant technique, called normalization, to allow the second and third classes of summative predicates to be solved by also reducing the problem to a maximum network flow problem. For the fourth class of summative predicates, we prove that it is an NP-complete problem.
[program debugging, Debugging, summative global predicates detection, NP-complete problem, error detection, Programming profession, parallel programming, distributed programs, Computer science, summative predicates, normalization, Load management, computational complexity]
Client location tracking in ubiquitous information service network
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
The objective of this paper is to present a feasible and efficient methodology that improves client mobility management under ubiquitous information service network. We base our system on the Open Integrated Service Network Architecture and propose a hybrid infrastructure with facilities of both a centralized Network Management Center (NMC) and a distributed Home Base Node (HEN). We also have criteria for choosing a HEN with minimum cost by computing the expected registration cost and location tracking cost according to users characteristics and called frequencies. Running a simulation, results show that our methodology is able to save up to 40% of the cost comparing to that of IS-41 when the value of Call-Mobility Ratio (CMR) is in the range of (1, 100), which means either clients are called with high frequencies or are moving with low mobilities. As the value of CMR goes down to the range of (0.01, 1), which means either clients are called with low-frequencies or are moving with high mobilities we may save up to 50% cost.
[Pervasive computing, open integrated service network architecture, Costs, Transportation, simulation, ubiquitous information service network, digital simulation, Mobile radio mobility management, Computer science, Intelligent networks, client location tracking, computer network management, Cellular phones, network management center, ISDN, distributed home base node, client mobility management, call mobility ratio, Frequency, Internet, IP networks, Computer network management]
A concurrent program debugging environment using real-time replay
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
A common debugging strategy involves re-execution of a program (on a given input) over and over, each time gaining more information about bugs. Such techniques can fail in concurrent programs. Because of variations in message latencies and process scheduling, different runs on the given input may produce different results. This non-repeatability rules out the reproduction of errors which is the cornerstone of conventional debugging techniques. So, guaranteeing reproducibility is major issue in the concurrent program debugging. This paper discusses the design and implementation of a concurrent program debugging environment which replays the error-occurred execution and debugs the errors using a recorded event history file and input data.
[program debugging, Debugging, error-occurred execution, Reproducibility of results, History, Programming profession, Delay, parallel programming, message latencies, concurrent program debugging environment, Runtime, event history file, Computer bugs, real-time replay, process scheduling, Probes, Joining processes, programming environments, Monitoring]
Performances of the PS/sup 2/ parallel storage and processing system for tomographic image visualization
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
We propose a new approach for developing parallel I/O- and compute-intensive applications. At a high level of abstraction, a macro data flow description describes how processing and disk access operations are combined. This high-level description (CAP) is precompiled into compilable and executable C++ source language. Parallel file system components specified by CAP are offered as reusable CAP operations. Low-level parallel file system components can, thanks to the CAP formalism, be combined with processing operations in order to yield efficient pipelined parallel I/O and compute intensive programs. The underlying parallel system is based on commodity components (PentiumPro processors, Fast Ethernet) and runs on top of WindowsNT. The CAP-based parallel program development approach is applied to the development of an I/O and processing intensive tomographic 3D image visualization application. Configurations range from a single PentiumPro I-disk system to a four PentiumPro 27-disk system. We show that performances scale well when increasing the number of processors and disks. With the largest configuration, the system is able to extract in parallel and project into the display space between three and four 512/spl times/512 images per second. The images may have any orientation and are extracted from a 100 MByte 3D tomographic image striped over the available set of disks.
[C++ source language, Ethernet networks, image visualization, parallel storage and processing, parallel file system, performance evaluation, Yarn, parallel processing, Concurrent computing, storage management, PS/sup 2/, Computer displays, computerised tomography, File systems, tomographic image visualization, Data visualization, data visualisation, tomography, Computer applications, Tomography, Parallel processing, Image storage, macro data flow]
Algorithmic influences on I/O access patterns and parallel file system performance
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
For many scalable parallel applications, the input/output (I/O) barrier rivals or exceeds that of computation and interprocessor communication. Consequently, scalable parallel secondary and tertiary storage systems are necessary to satisfy the resource demands of many national challenge problems. At present, one major challenge facing the designers of such storage systems is the wide range of I/O access patterns and the lack of general purpose file system policies that achieve high performance for variable I/O requirements. We analyze the I/O behavior of two scientific applications on the Intel Paragon XP/S. Although the two applications solve the same scientific problem and their I/O access patterns are qualitatively similar, their interactions with the file system are decidedly different. Our results show that appropriate tuning of file system policy parameters to I/O demands can significantly increase I/O throughput.
[application program interfaces, Educational institutions, Throughput, Application software, Intel Paragon XP/S, Computer science, Concurrent computing, storage management, Chemistry, file system policies, File systems, System performance, I/O access patterns, parallel file system performance, interprocessor communication, resource demands, storage systems, input/output barrier, algorithmic influences, Pattern analysis, high performance, Contracts, software performance evaluation]
A dynamic reconfiguration manager for graph-oriented distributed programs
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Dynamic reconfiguration allows the system configuration to be changed while the system is in execution. It is definitely a desirable property for any large distributed system where dynamic modification and extension to the system and its applications are required. We have developed a software platform based on a graph-oriented model, which provides support for programming reconfigurable distributed applications. In this paper, we describe a dynamic reconfiguration manager for the graph-oriented distributed programming environment. The services and requirements of dynamic reconfiguration are identified, the architecture design of a dynamic reconfiguration manager is presented, and a central server-based prototypical implementation of the manager on a local area network of workstations is described.
[software platform, Logic programming, system configuration, server-based prototypical implementation, local area networks, Application software, dynamic reconfiguration manager, Distributed computing, Environmental management, parallel programming, Programming environments, graph-oriented distributed programs, Prototypes, distributed programming environment, local area network, Computer architecture, Dynamic programming, Workstations, programming environments, Local area networks]
Efficient execution of parallel programs using partial strict triggering of program graph nodes
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Almost all coarse grained program graph nodes don't need all of their input operands at the beginning of their execution. Thereafter they can be scheduled a bit earlier. This type of program graph nodes triggering is called partial strict triggering. The missing operands will be requested later during the execution. Coarse grained program graph nodes send their output operand to all successors, as soon as they produce them. Successors of coarse grained program graph nodes will be scheduled earlier too, because they will receive their input operands sooner. An evaluation of improved CPM, VL and DSH scheduling algorithms is done in this paper. We have improved them with partial strict triggering of coarse grained program graph nodes.
[CPM, program graph nodes, data flow graphs, Dynamic scheduling, Partitioning algorithms, Topology, DSH scheduling algorithms, partial strict triggering, Computational complexity, Scheduling algorithm, parallel programming, processor scheduling, parallel programs, Multiprocessing systems, coarse grained program graph nodes, Concurrent computing, Computer science, Processor scheduling, Clustering algorithms, VL]
A data placement strategy on MZR for VOD servers
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
The number of concurrent users in VOD servers mainly depend on the disk placement strategies of video blocks. Especially, the storage server should make the most of MZR (multi-zone recording) disks' characteristics. The variable transfer rates and capacity in zones feature the MZR disk, so the outward zones have more transfer rates and capacity than inward zones. Also, Users' access patterns concentrate on the popular videos. So, if the popular video blocks are placed at the outward zones, the number of concurrent users can be increased. In this paper we present the popularity-based variable way placement strategy for disk groups (PVW) based on the LP and the SHP. The PVW aims to effectively support popular streams of high access rates without additional storage capacity. The more popular video blocks are to be placed in all disk groups to utilize the total bandwidth of a storage subsystem, and the less popular video blocks are to be placed in the small disk groups. By the simulation study, we have found that more users could be supported by this strategy.
[Video on demand, MZR, concurrent users, access patterns, Videoconference, Video recording, VOD servers, Computer science, PVW, storage management, variable way placement strategy, Space technology, Tiles, Bandwidth, interactive television, Hard disks, Systems engineering and theory, file organisation, multi-zone recording, storage capacity, data placement strategy, Disk recording]
Hybrid checkpointing protocol based on selective-sender-based message logging
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
This paper presents a hybrid checkpointing protocol-an asynchronous checkpointing protocol using a message sending/receiving state change for reducing the overhead of failure-free operation combined with a selective sender-based message logging protocol for reducing the cascade rollback of asynchronous checkpointing protocol. The selective sender-based message logging protocol records only potential orphan messages when taking a checkpoint. And this paper presents a message dependency tree recording the inter-process message sending/receiving information on a volatile storage for reducing the search time of inter-process information during the failure recovery.
[Checkpointing, hybrid checkpointing protocol, asynchronous checkpointing protocol, search time, Protocols, failure recovery, message dependency tree, failure-free operation, system recovery, Counting circuits, Message passing, Fault tolerant systems, selective-sender-based message logging, fault tolerant computing, cascade rollback, protocols]
Performance predictor for HPF compilers
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
In HPF programming environment, program behavior needs to be reported for the development of well-tuned application programs. Performance predictor has been regarded as a proper approach for such requirement. In this paper, we propose a performance predictor which includes profiler, static performance predictor and dynamic performance predictor. Profiler generates execution count of each statement by profiling run of the program. While the static performance predictor generates general performance data, dynamic performance predictor reports some advanced performance data such as computation/communication overlapping and performance degradation by cache mis hit. Program slicing technique is applied to profiler and skeleton program is extracted from source program in dynamic performance predictor to speed up prediction time.
[application program interfaces, dynamic performance predictor, program compilers, Concurrent computing, Degradation, HPF programming environment, static performance predictor, application programs, Skeleton, Dynamic programming, HPF compilers, skeleton program, program behavior, Instruments, performance evaluation, performance predictor, profiler, performance degradation, Application software, cache mis hit, Programming profession, Programming environments, Computer languages, program slicing technique, Writing, programming environments]
Reordering the statements with dependence cycles to improve the performance of parallel loops
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
In this paper, we study the exploitation of statement-level parallelism in dependence cycles of Do-loops executed in a random (general) synchronization mode, with emphasis on the effect of statement ordering on parallelism level. For a dependence cycle of a single parallel loop, the parallelism exposed, in general, varies with the alignment of statements. Statement reordering (without modifying the semantics) relies on the compile-time prediction of execution-time of the loop. An improved timing formula and the derived algorithm of statement reordering in single Do loop with dependence cycles to improve the performance of parallel loops are proposed and discussed.
[parallelism level, parallelising compilers, performance evaluation, parallel loops performance, dependence cycles, parallel programming, synchronisation, Computer science, statement-level parallelism, synchronization mode, compile-time prediction, Parallel processing, statements reordering, timing formula, statement ordering, Timing, State estimation, Do-loops]
Transaction-based causally ordered protocol for distributed replicated objects
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
In group communications, larger computation and communication overhead are considered to causally order all the messages transmitted in the network. Transactions in clients manipulate objects in servers by sending read and write requests to the servers. In this paper, we define significant messages by using the relation among the transactions. We newly propose an object vector to causally order only the significant messages. The scheme of the object vector is invariant in the change of the group membership. We also show a TBCO (transaction-based causally ordered) protocol which adopts the object vector, by which the number of messages to be causally ordered are reduced.
[transaction-based causally ordered protocol, Protocols, replicated databases, distributed replicated objects, Distributed computing, communication overhead, Delay, Network servers, group communications, Systems engineering and theory, object vector, Computer networks, Communication networks, Telecommunication network reliability, protocols, Clocks]
A multimedia programming toolkit/environment
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
This paper provides details and implementation experiences of a multimedia programming language and associated toolkits. The language, a data-flow paradigm for multimedia streams, consists of blocks of code that can be connected through their data ports. Continuous media flows through these ports into and out of blocks. The blocks are responsible for the processing of continuous media data. Examples of such processing include capturing, displaying, storing, retrieving and analyzing their contents. The blocks also have parameter ports that specify other pertinent parameters, such as location, and display characteristics such as geometry, etc. The connection topology of blocks is specified using a graphical editor called the Program Development Tool (PDT) and the geometric parameters are specified by using another graphical editor called the User Interface Development Tool (UIDT). Experience with modeling multimedia presentations in our environment and the enhancements provided by the two graphical editors are discussed in detail.
[user interface management systems, connection topology, Content based retrieval, multimedia systems, graphical editors, Displays, Electronic mail, Databases, multimedia programming environment, multimedia programming language, software tools, parallel languages, Multimedia systems, multimedia programming toolkit, geometric parameters, multimedia presentations, Programming profession, Computer science, Computer languages, program development tool, Streaming media, parameter ports, data-flow paradigm, graphical editor, user interface development tool, Artificial intelligence, programming environments]
Potentials and limitations of parallel computing on a cluster of workstations
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Parallel computing on clusters of workstations is receiving much attention from the research community. Unfortunately, many aspects of parallel computing over this parallel computing engine is not very well understood. Some of these issues include the workstation architectures, the network protocols, the communication-to-computation ratio, the load balancing strategies, and the data partitioning schemes. The aim of this paper is to assess the strengths and limitations of a cluster of workstations by capturing the effects of the above issues. This has been achieved by evaluating the performance of this computing environment in the execution of a parallel ray tracing application through analytical modeling and extensive experimentation.
[Protocols, parallel ray tracing application, performance evaluation, cluster of workstations, parallel processing, data partitioning schemes, Engines, network protocols, Concurrent computing, Analytical models, resource allocation, performance, analytical modeling, Computer architecture, Parallel processing, Ray tracing, Load management, Workstations, Performance analysis, protocols, parallel computing, load balancing strategies, workstation architectures, communication-to-computation ratio]
A data allocation considering data availability in distributed database systems
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
In designing distributed databases, data allocation is one of the key design issues. Some advantages of optimal data allocation are reducing cost, increasing performance and availability. However, because of that most of current data allocation techniques have considered only cost and performance, those techniques cannot maintain the availability; and are not proper to the system requiring high-availability. In this paper, we suggested the TCOP (Transaction COmmit Probability) model, which is a new availability model considering multiple path failures occurred during the execution time for distributed transactions. In addition, we proposed a data allocation technique using the genetic algorithms, which can maintain the availability and minimize the processing cost. According to the experimental results, the proposed TCOP model and data allocation technique have shown improvement of the data availability.
[Availability, Real time systems, performance evaluation, Throughput, Topology, genetic algorithms, Delay, distributed database systems, Computer science, data allocation, performance, Distributed databases, availability model, distributed databases, data availability, Cost function, Database systems, Computer networks, transaction commit probability model]
CrownFS: a clustered continuous media server
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
CrownFS is a file system for continuous media server in the CROWN (Clustering Resources On Workstations' Network) system. Because of the high I/O bandwidth rate and strict real-time constraints, the design issues of CrownFS are that of efficiently balancing user load against limited disk: bandwidth, network and I/O channel bandwidth. CrownFS accomplishes this balancing by striping continuous data file across several workstations and computing at each workstation in the CROWN system. This paper describes the CROWN system architecture and the CrownFS design. CrownFS runs on a cluster of workstations consisted of Kawaiis, AlphaStations and PCs.
[continuous media server, PCs, Costs, clustered, real-time constraints, Kawaiis, multimedia systems, File servers, cluster of workstations, AlphaStations, file system, Network servers, File systems, balancing user load, file servers, Bandwidth, Streaming media, Systems engineering and theory, file organisation, CrownFS, Hardware, Workstations, continuous data file, Personal communication networks]
Configurable hardware for symbolic search operations
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Most intermediate and high-level vision tasks manipulate symbolic data. A kernel operation in these vision tasks is to search symbolic data satisfying certain geometric constraints. Such operations are data-dependent and their memory access patterns are irregular. In this paper, we propose a fast parallel design for symbolic search operations using configurable hardware. Using a pointer array and a bit-level index array, we manipulate the symbolic data and show high performance can be achieved. Depending on the input data, a corresponding search window is calculated and symbolic search operations are performed in parallel. Performance estimates using 16 Xilinx XC6216s and memory modules are very promising. Given 3519 line segments (extracted from an 1024/spl times/1024 pixel image), the operation can be performed in 1.11 milliseconds on our FPGA-based platform. On a Sun UltraSPARC Model 140, the same operation implemented using C takes 690 milliseconds. Although we illustrate our design for a specific search operation, our design technique can be applied to related search operations with minor modifications. Also, it can be ported to other FPGA devices.
[Real time systems, Computer vision, parallel architectures, Data engineering, pointer array, Data mining, vision tasks, Concurrent computing, geometric constraints, bit-level index array, symbolic data, High performance computing, Machine vision, computer vision, symbolic search operations, Hardware, Kernel, configurable hardware, Field programmable gate arrays, search problems]
Performance analysis and experiments of sorts on a parallel computer with parallel computation models
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
This paper investigates the execution behaviors of parallel sorting algorithms on an experimental multiprocessor (KuPP) and compares with predicted performance under LogP and BSP (Bulk Synchronous Parallel) models. Since the communication overhead is considered a primary candidate for improvement, a few schemes are devised and experimented on KuPP to reduce the time spent in communication, thus to enhance the overall performance. The authors believe the ideas can be adopted in other high-performance parallel computers.
[parallel algorithms, Computational modeling, Computer simulation, parallel architectures, Predictive models, performance evaluation, LogP, KuPP, parallel machines, communication overhead, Sorting, Concurrent computing, Design engineering, performance, Message passing, execution behaviors, sorting, parallel sorting, Parallel processing, parallel computation models, Performance analysis, performance analysis, BSP, Context modeling]
Extended flexible processor allocation strategy for mesh-connected systems using shape manipulations
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Current processor allocation techniques for mesh-connected parallel systems are restricted to rectangular submesh allocation strategies causing significant fragmentation problems. In this paper, we propose an EFPA (Extended Flexible Processor Allocation) strategy to reduce external fragmentation and job response time, simultaneously. EFPA manipulates the shape of the required submesh to a more generalized L-shaped submesh. When an incoming job requests a rectangular submesh, EFPA first tries to allocate the conventional rectangular submeshes as other strategies. If it fails, EFPA further tries to allocate more flexible and robust L-shaped submeshes instead of signaling the allocation failure. Thus, EFPA accommodates incoming job earlier than other strategies. All the shape manipulations to the L-shaped submeshes are transparent to the application programmers. Our simulations show that EFPA performs more efficiently than other strategies in terms of the external fragmentation and the job response time.
[mesh-connected systems, Shape, multiprocessor interconnection networks, submesh, extended flexible processor allocation, Very large scale integration, performance evaluation, digital simulation, Topology, Delay, Programming profession, processor scheduling, L-shaped submesh, shape manipulations, simulations, job response time, Computer science, resource allocation, Prototypes, fragmentation problems, Hypercubes, rectangular submesh allocation, Robustness, application programmers]
Implementation of an object-oriented functional language on the multithreaded architecture
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
We suggest a new object-oriented functional language, OOId, which is based on a functional language Id and extended with object-oriented language features. We also discuss how to map OOId program onto DAVRID, which is a kind of multithreaded architecture based on the dataflow model. The comparison of execution time of two programs written in Id and OOId for the matrix multiplication problem, shows we can effectively run OOId program with just a little slow-down of execution time. In addition, the rate of slow-down is decreased, as the size of the matrix is larger. That means we can use good features of object-oriented language by paying just a little slow-down of execution time.
[Object oriented modeling, parallel architectures, object-oriented language, object-oriented functional language, dataflow model, matrix multiplication problem, Parallel machines, Programming profession, parallel programming, Concurrent computing, Computer science, matrix multiplication, DAVRID, Parallel programming, Computer architecture, data flow computing, Parallel processing, Writing, object-oriented languages, multithreaded architecture, Object oriented programming, OOId, functional languages, functional language Id]
Design and implementation of software simulation platform for ATM switching systems
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Since the target system, where the software for an ATM switching system will be loaded, has limited resources and various functional constraints, it is difficult to develop and test ATM software on the target system. Therefore, its development and testing are accomplished usually by a general UNIX system. However, testing environment developed until now has emphasized concurrent processing simulation of CHILL language, which means that there have been a great deal of restrictions in testing software of switching systems. In order to resolve this problem, we develop software simulation platform system (SSPS), which is a testing environment to observe and analyze actions of software of ATM switching systems by simulating target environment including the operating system of switching systems at a general UNIX system. SSPS is characterized in that it supports distributed concurrent environment, provides a communication function with an operation and management system, add facilitates extension to testing environment for other systems. The test coverage over all ATM S/W blocks has increased by SSPS.
[Software testing, Real time systems, ATM software, System testing, Switching systems, program testing, Electronic equipment testing, asynchronous transfer mode, Logic testing, telecommunication computing, target environment, concurrent processing simulation, Analytical models, software simulation platform, general UNIX system, Operating systems, CHILL language, ATM switching systems, functional constraints, real-time systems, specification languages, software simulation platform system, Software systems, Asynchronous transfer mode]
An parallel diagnosis method for an optimal fault-tolerant network
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Recently, researchers have begun studying adaptive approaches to the diagnosable system. This paper overviews a class of optimally fault tolerant multiprocessor network architecture based on the network proposed by Pradhan and Sengupta. Because of optimal fault tolerance, the number of connections per node is precisely related to the degree of fault tolerance the network is designed to provide. Hakimi et al. assume that each unit is capable of testing every other unit using the test being conducted one at a time in such a way that the choice of the next test to be performed depends on the results of the previous tests rather than on a preselected pattern of test studied earlier. Rhee investigated the application of adaptive diagnosis approach to a D(n, t, X) system which belongs to the PMC model. In this paper, we investigate an application of the adaptive diagnosis approach to an optimally fault-tolerant multiprocessor network. The diagnosis algorithm developed here can be applied to a hypercube or a binary cube since the optimally fault-tolerant network is similar to a hypercube or a binary cube.
[Performance evaluation, optimal fault-tolerant network, System testing, hypercube, multiprocessor interconnection networks, parallel diagnosis method, binary cube, adaptive diagnosis approach, Fault diagnosis, Computer science, multiprocessor network architecture, Fault tolerance, Sufficient conditions, diagnosable system, Fault detection, Computer architecture, Hypercubes, Computer networks]
Design of an augmented generalized cube network
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
This paper demonstrates how the demultiplexor is used for making a new tree embedded in a MIN and how the Augmented Generalized Cube Network is designed using this tree. Evaluation of its performance and examination of its characteristics are also included. The demultiplexor was originally used for providing tolerance for destination link failures. The above shows that the AGCN gets better performance than other MINs. Not only does the AGCN have all of the Generalized Cube Network's features but it also offers multiple paths using a new embedded tree and intrastage links. Its self-adaptive routing in the last stage can alleviate tree saturation and contribute greatly to performance improvement.
[demultiplexor, Multiprocessor interconnection networks, multiple paths, tree saturation, Switches, performance evaluation, Routing, multistage interconnection networks, embedded tree, demultiplexing equipment, performance improvement, Computer science, Parallel programming, Network topology, MIN, performance, telecommunication network routing, augmented generalized cube network, destination link failures, self-adaptive routing, intrastage links]
Techniques to provide run-time support for solving irregular problems
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
We present a runtime library design based on the two-phase collective I/O technique for irregular applications. The design is motivated by the requirements of a large number of ASCI (Accelerated Strategic Computing Initiative) applications, although the design and interface is general enough to be used from any irregular applications. We present two designs, namely, Collective I/O and Pipelined Collective I/O. In the first scheme, all processors participate in the I/O at the same time, making scheduling of I/O requests simpler, but creating a possibility of contention at the I/O nodes. In the second approach, processors are grouped into several groups, so that only one group performs I/O simultaneously, while the next group performs communication to rearrange data, and this entire process is pipelined. This reduces the contention at the I/O nodes but requires more complicated scheduling and a possible degradation in communication performance. We obtained up to 40 MBytes/sec application level performance on the Caltech's Intel Paragon (with 16 IO nodes, each containing one disk) which includes on-the-fly reordering costs. We observed up to 60 MBytes/sec on the ASCI/Red machine with only three I/O nodes (with RAIDS).
[run-time support, 60 MByte/s, ASCI, input-output programs, two-phase collective input output technique, parallel machines, software libraries, processor scheduling, parallel programming, Concurrent computing, 40 MByte/s, Runtime, Red machine, scheduling, irregular problem solving, Accelerated Strategic Computing Initiative, Collective I/O, Mesh generation, runtime library design, Intel Paragon, on-the-fly reordering costs, Computational fluid dynamics, Computational modeling, performance evaluation, Pipelined Collective I/O, RAIDS, Caltech, Application software, communication performance, Computational geometry, Data visualization, Writing, pipeline processing, Acceleration]
Are 2*-trees interesting topologies for the interconnection of a network?
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
In this paper, we explore in detail some properties of 2*-trees. A 2*-tree is a connected graph with N nodes, diameter log/sub 2/N, and degree of nodes 10. We propose routing algorithms for the unicasting, the broadcasting and the gossiping on a 2*-tree network. The routing algorithms we present have their time and communication complexities lower than the corresponding complexities for the routing on H/sub n/, i.e., the hypercube of dimension n=log/sub 2/N. Consequently, we think that a 2*-tree is an interesting topology to be considered for a network of processors.
[Disruption tolerant networking, 2*-trees, gossiping, multiprocessor interconnection networks, time complexity, Routing, Complexity theory, communication complexity, Distributed computing, Delay, routing algorithms, Network topology, Tree graphs, Distributed databases, telecommunication network routing, Broadcasting, Hypercubes, connected graph, unicasting, computational complexity]
A loop parallelization method for nested loops with non-uniform dependences
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
This paper proposes an efficient method of partitioning nested loops with non-uniform dependences for maximizing parallelism. Our approach is based on convex hull theory, and it will divide the iteration space of the loop into three regions as two parallel regions where the iterations can be fully executed in parallel and one parallelizable region where the iterations are inherently serial, but possible parallelism can be exploited. And in order to maximize parallelism from the parallelizable region, an algorithm using integer programming which partitions a loop into variable size partitions is also proposed. In comparison with some works on partitioning, the proposed method is a simple and exact partitioning method, and it gives much better speedup and extracts more parallelism than them.
[convex hull theory, integer programming, parallelism, computational geometry, Linear programming, Partitioning algorithms, parallelizable region, Equations, parallel programming, Computer science, nested loops, exact partitioning method, Upper bound, Parallel processing, nonuniform dependences, loop parallelization method, Testing]
A high-performance ATM switch with completely and fairly shared buffers
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Sharing buffer space between switch ports greatly improves the performance of the switching systems. However sharing buffers in a fair manner is not an easy task. In this paper we propose a high-performance ATM switching system with fairly and completely shared buffers. The core of the proposed switching system is a novel buffer management mechanism with which buffer space in the switching system can be completely shared by all the switch ports in a fair manner. The proposed buffer management mechanism works based on a simple algorithm and can be easily implemented by hardware. The performance of the proposed switching system is evaluated by both analytical model and simulation. The results show that high buffer utilization and low packet lost ratio can be achieved under both uniform and nonuniform traffic loads.
[Packet switching, buffer storage, Switching systems, Buffer storage, Switches, performance evaluation, Throughput, asynchronous transfer mode, shared buffers, Computer science, Analytical models, switch ports, performance, multiplexing, nonuniform traffic loads, buffer management mechanism, Hardware, Workstations, low packet lost, Asynchronous transfer mode, high-performance ATM switch]
Optimal embeddings of multiple graphs into a hypermesh
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
A hypermesh, a versatile parallel architecture, is obtained from a 2-dimensional mesh by replacing each linear connection with a hyper-edge. We optimally embed multiple graphs into a hypermesh by a labeling strategy. This optimal embedding provides an optimal expansion, dilation and congestion at the same time. First, we label on an N-node graph G, possibly disconnected, such that this labeling makes it possible to optimally embed multiple copies of G into an N'/spl times/N' hypermesh when N' is divisible by N. Second, we show that many important classes of graphs have this labeling: for example, tree, cycle, mesh of trees and product graphs including mesh, torus, and hypercube. Third, we generalize these results to optimally embed multiple graphs into a multidimensional and possibly non-square hypermesh. This labeling strategy is applicable to the embeddings of other classes of graphs into a hypermesh.
[parallel architectures, multiple graphs, hyper-edge, hypercube networks, Parallel algorithms, optimal expansion, Concurrent computing, hypermesh, Tree graphs, Computer architecture, optimal embeddings, 2-dimensional mesh, linear connection, Hypercubes, N-node graph, Labeling, parallel algorithms, Multidimensional systems, hypercube, torus, labeling strategy, Parallel architectures, Computer science, dilation, versatile parallel architecture, Ear, congestion]
A deadlock-free routing scheme for interconnection networks with irregular topologies
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Interconnection networks with irregular topologies (or irregular networks) are ideal communication subsystems for workstation clusters owing to their incremental scalability. While many deadlock-free routing schemes have been proposed for regular networks such as mesh, torus, and hypercube, they cannot be applied in irregular networks. This paper presents a cost-effective routing architecture, called TRAIN, to solve the routing problem with irregular networks. We show that TRAIN is a deadlock-free scheme. Furthermore, unlike many other routing schemes proposed previously for irregular networks, TRAIN does not require a routing table in the switch. Due to this feature, a TRAIN switch is small and the routing decision can be made rapidly. In order to evaluate the effectiveness of our routing scheme, analysis and event-driven simulation have been performed for various irregular networks. Our results show that TRAIN outperforms other schemes with a higher maximum throughput and lower average latency consistently.
[workstation clusters, communication subsystems, Multiprocessor interconnection networks, Scalability, parallel architectures, average latency, multiprocessor interconnection networks, Switches, event-driven simulation, incremental scalability, local area networks, interconnection networks, Analytical models, Network topology, deadlock-free routing scheme, Hypercubes, Workstations, Performance analysis, mesh, hypercube, torus, network routing, irregular topology networks, performance evaluation, cost-effective routing architecture, Routing, TRAIN, performance, concurrency control, irregular networks, virtual machines, System recovery, routing table, maximum throughput]
Permutation mapping for MIN using high level net models
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
This paper proposes to structurise the analysis of the performance of Multistage Interconnection Networks (MINs). A new type of High level net, named Modified Petri net (MP-net) which has subsequently been refined to S-net, has been defined in the process. Essentially, a variation of Coloured and Stochastic nets, this S-net has been shown to be a very effective tool for dynamic modelling of MINs. The results reported so far in the context of performance estimation of MINs, are mainly based on direct combinatorial analysis of the concerned networks with very little consideration towards the modelling aspects. The outcome of the present research work may be utilized for modelling of a wide range of MINs consisting of 2/spl times/2 crossbar switches. Multistage redundant path networks like Omega and non-redundant path networks such as Baseline network have been considered and modelled with S-net as a case study. It has also been established that using S-net, a MIN of N/spl times/N size can be modelled with 3N/2 number of places as against O(Nlog/sub 2/N) no. of switching elements for the equivalent MIN. Using the S-net model connectivity status of constituent cross-bar switches of a MIN, has been represented in a matrix, proposed to be termed as Control matrix. An algorithm has been developed to study and modify entries of the control matrix to analyse performance of MIN using S-net model.
[Multiprocessor interconnection networks, S-net, Petri nets, Stochastic processes, Switches, coloured nets, graph colouring, Concurrent computing, crossbar switches, combinatorial analysis, Fires, permutation mapping, Cities and towns, Computer networks, Performance analysis, control matrix, multistage redundant path networks, stochastic nets, performance evaluation, multistage interconnection networks, high level net models, MIN, modified Petri net, performance analysis, Context modeling]
Distributed Real-time Systems : Issues, Solutions, And Applications
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
false
[Real time systems, Computerized monitoring, Distributed databases, Quality of service, Computer architecture, Application software, Distributed computing, Environmental management, Intelligent robots, Intelligent transportation systems]
Design, implementation and performance of a mutex-token based fault-tolerant tuple space machine
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
LiPS is a system for distributed computing using idle-cycles in networks of workstations. The system provides its user with the tuple space based generative communication paradigm of parallel computing as known from the coordination language LINDA. The core of the LiPS system is the fault-tolerant tuple space machine replicating the tuple space. The implementation, based on the protocols given by Y. Amir et al. (1993), is set up on a mutex token based membership protocol for handling crashing and joining tuple space servers, and a closely related total order protocol which establishes a linear order among all tuples. The linear order on tuples is used to speed up replication of reading tuple space accesses and decreases the memory requirements for message logs. As it is possible to add or remove tuple space servers at runtime, efficiency/reliability can be adjusted to the application needs.
[Lips, LiPS, membership protocol, memory requirements, Access protocols, performance evaluation, mutex-token based fault-tolerant tuple space machine, Distributed computing, Programming profession, distributed computing, Fault tolerance, Runtime, performance, Operating systems, idle-cycles, Fault tolerant systems, total order protocol, Computer architecture, fault tolerant computing, coordination language LINDA, Workstations, protocols, parallel computing, tuple space based generative communication paradigm]
Scalable Scheduling For Distributed Memory Machines: A Reality Abstract
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
false
[Processor scheduling, NP-hard problem, Optimal scheduling, Polynomials, Application software, Scheduling algorithm]
New architectures and I/O scheduling methods for scalable storage products
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
The paper presents solutions for an element of the cluster that will be primarily used as a scalable storage product for a collection of mainframes. Existing storage solutions either employ server attached disks: where the problem is the number of I/O slots, or are specially designed products where the problem is the lack of standards. The essence of both proposed solutions is the introduction of the file usage locality concept, together with methods and techniques to exploit it. The first solution is based on the standard SMP architecture, while the second one employs the DSM architecture as the basic building block of a cluster product.
[parallel architectures, scalable storage products, file usage locality concept, Throughput, processor scheduling, Uniform resource locators, Concurrent computing, DSM architecture, storage management, Fault tolerant systems, Bandwidth, Computer architecture, I/O slots, Parallel processing, standard SMP architecture, Modems, shared memory systems, server attached disks, mainframes, cluster product, I/O scheduling methods, standards, Processor scheduling, specially designed products, distributed memory systems]
An approach for mobile agent security and fault tolerance using distributed transactions
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Mobile agents are no longer a theoretical issue since different architectures for their realization have been proposed. With the increasing market of electronic commerce it becomes an interesting aspect to use autonomous mobile agents for electronic business transactions. Being involved in money transactions, supplementary security features for mobile agent systems have to be ensured. In this paper we present an architecture for a mobile agent system which guarantees security for the host as well as security for the agent. This architecture additionally offers fault tolerance for the whole agent system at a high level. To handle these issues for mobile agents we use various encryption mechanisms and we apply a novel method for mobile agent systems by using distributed transactions processing based on the OMG Object Transaction Service in our architecture. With this security architecture an agent will be enabled to do money transactions.
[transaction processing, Protocols, fault tolerance, money transactions, electronic business transactions, OMG Object Transaction Service, distributed processing, encryption mechanisms, Electronic commerce, Information technology, Fault tolerance, Mobile agents, Fault tolerant systems, Information security, Prototypes, distributed transactions, fault tolerant computing, mobile agent security, Cryptography, Consumer electronics, electronic commerce]
Load balancing strategies for parallel forward search algorithm with conflict based backjumping
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Constraints satisfaction problems (CSP) is a well studied example of the NP-complete family which is usually solved by backtracking search algorithms. Performance comparisons of different algorithms led us to consider FC or FC-CBJ as one of the best sequential algorithms to solve a great family of CSP's. In this paper we give an explicit formulation for a generic parallel search algorithm based on typical parallel behaviors, such as load balancing policy. From this paradigm, we exhibit parallel FC-CBJ. The experimental study is up to now restricted to the performance of this within the scope of phase transition phenomenon for randomly generated binary CSPs and more particularly in the mushy region where the hardest problems are encountered. We discuss the performances of our algorithm relatively to a server initiated load balancing strategy with four techniques of tree-search splitting and four partner selection policies.
[Algorithm design and analysis, NP-complete family, explicit formulation, load balancing, Heuristic algorithms, conflict based backjumping, parallel FC-CBJ, Tree graphs, resource allocation, parallel forward search algorithm, Computational efficiency, constraint handling, randomly generated binary CSPs, Intelligent systems, load balancing strategies, search problems, constraints satisfaction problems, parallel algorithms, performance evaluation, generic parallel search algorithm, tree-search splitting, Load management, backtracking search algorithms, performance comparisons, phase transition phenomenon, computational complexity]
Parallel clustering algorithms on a reconfigurable array of processors with wider bus networks
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Clustering techniques are usually used in pattern recognition, image segmentation and object detection. For N patterns and k centers each with M features, in this paper, we first design an O(kM) time optimal parallel algorithm for one pass process of clustering with the k-means method on a linear array of processors with a wider bus network using N/sup 1+1/c/ processors with one bus network, where c is any constant and c/spl ges/1. Then, based on the proposed algorithm, two O(k) and O(1) time optimal parallel clustering algorithms are also derived using MN/sup 1+1/c/ and kMN/sup 1+1/c/ processors with M row and MN row bus networks, respectively. These results improve the best known bounds and achieve cost optimal in their time and processor complexities.
[Algorithm design and analysis, parallel algorithms, Very large scale integration, time complexity, object detection, Pattern recognition, Parallel algorithms, wider bus networks, Image segmentation, processor complexity, reconfigurable architectures, parallel clustering algorithms, image segmentation, Clustering algorithms, O(kM) time optimal parallel algorithm, Computer architecture, reconfigurable array of processors, Hypercubes, Pattern analysis, Business, pattern recognition]
Attribute partitioning algorithm in DOODB
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
In distributed object-oriented databases (DOODB), objects are distributed in different sites on communication networks. In DOODB, class fragmentation to divide a class into fragments is needed for improving performance and for reducing data transfer. Class fragmentation is different from conventional relational databases. We have proposed the vertical class fragmentation to reflect the characteristics of object-oriented databases. In this paper, we define the attribute partitioning algorithm and describe the results of implementation and comparison.
[Object oriented databases, object-oriented databases, communication networks, Relational databases, performance evaluation, attribute partitioning algorithm, Data engineering, Partitioning algorithms, Electronic mail, Distributed computing, Intelligent networks, performance, class fragmentation, Distributed databases, distributed databases, data transfer, Computer networks, Communication networks, distributed object-oriented databases]
Distributed network computing over wireless links
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
With the advances of wireless communication technology, using the wireless LAN as a platform to perform distributed network computing becomes feasible. We study the characteristics of end-to-end communication over wireless links. With the advantage of reduced bandwidth competition in each LAN segment separated by the wireless bridges, and with the overlap of wireless and wired communications, an analytical comparison showed that the group communications over wireless links can be more efficient than over a single segment wired LAN. We conducted experiments of running distributed applications and the results showed that with the support of threads, wireless network computing can achieve the same performance as the wired networks. Furthermore, the statistical results from our survey showed that the users cannot tell the difference between wireless and wired settings in terms of the data access speed.
[Wireless LAN, experiments, distributed processing, Distributed computing, Yarn, wireless links, distributed applications, Wireless communication, wireless communication technology, reduced bandwidth, wired communications, network performance, Wireless networks, data access speed, Bandwidth, Computer networks, Communications technology, Local area networks, end-to-end communication, performance evaluation, threads, distributed network computing, Bridges, group communications, survey, statistical results, wireless LAN]
Design and evaluation of a distributed multimedia synchronization algorithm using media scalings and variable service rates
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
This paper presents a distributed multimedia synchronization algorithm that supports both intramedia and intermedia synchronizations. The intramedia synchronization is achieved by media scaling techniques, and the intermedia synchronization is achieved by variable service rates. We compute the check period of media buffer and the scaling duration for intramedia synchronization, and compute the comparison period between master media's and slave media's relative time stamps for intermedia synchronization. We also evaluate our algorithm through simulations. Simulation results show that our algorithm performs well in both intramedia and intermedia synchronizations.
[Algorithm design and analysis, intermedia synchronization, media scalings, Master-slave, multimedia systems, distributed processing, Multimedia communication, time stamps, intramedia, check period, intermedia, Feedback, variable service rates, distributed multimedia, Multimedia systems, Computational modeling, media buffer, Educational institutions, Computational Intelligence Society, intramedia synchronization, synchronisation, Streaming media, synchronization, scaling duration, Frequency synchronization]
Distributed shared memory on IBM SP2
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Distributed Shared Memory (DSM) approach provides the illusion of a global shared address space by implementing a layer of shared memory abstraction on a physically distributed memory system. In this paper, we present DSM-SP2, a software distributed shared memory system built on IBM SP2, a distributed memory machine. DSM-SP2 is implemented completely in software as a set of user-level library routines on top of the AIX operating system without requiring any modifications to the operating system or any additional compiler support. The salient features of DSM-SP2 are: (i) it implements lazy release consistency model with hybrid coherence protocol to reduce the communication overheads; (ii) it allows multiple concurrent writers to minimize the effects of false-sharing; (iii) to reduce the DSM overheads and the idling time of processes, the DSM-SP2 implementation allows multiple processes per node; and (iv) it implements a new synchronization primitive called conditional lock acquire/release for effective simple producer-consumer type of synchronization. Detailed performance measurements for three benchmark programs namely, Water, Jacobi and Tomcatv are reported.
[synchronization primitive, distributed shared memory, Scalability, Multiprocessor interconnection networks, AIX operating system, Random access memory, Delay, Operating systems, communication overheads, shared memory systems, DSM-SP2, user-level library routines, multiple concurrent writers, lazy release consistency model, operating system, memory protocols, physically distributed memory system, Supercomputers, Magnetic heads, Programming profession, synchronisation, software distributed shared memory system, hybrid coherence protocol, false-sharing, Software libraries, shared memory abstraction, global shared address space, benchmark programs, Coherence, distributed memory systems, IBM SP2, compiler support]
Ultrafast compact CMOS dividers
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
This paper introduces a new class of ultrafast and compact divider circuits. They are obtained by a systematic parallelization of a simple restoring division algorithm. Emphasis is put onto the stepwise parallelization methodology. Furthermore, a detailed discussion of implementational aspects is presented, including timing and layout. When, compared to SRT-schemes, these divider circuits allow a speed-up by a factor 2 to 3, while the cost is nearly identical. In particular, it is expected that double-precision IEEE Std. 756 floating-point division is performed within ca. 13 ns in 0.3 /spl mu/m, CMOS technology.
[Costs, floating-point division, Image processing, parallel architectures, compact, Circuits, timing, restoring division, Image restoration, CMOS dividers, parallelization, Delay, Research and development, ultrafast, layout, 0.3 mum, CMOS logic circuits, digital arithmetic, Linear algebra, CMOS technology, divider circuits, Hardware, dividing circuits, Adders]
A provably fastest parallel algorithm for the recognition of the consecutive ones property with selected applications
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Presented here is a parallel algorithm that decides if an m/spl times/n (0, 1)-matrix has the consecutive 1's property for rows, and if so, turns the matrix into one with consecutive 1's in each row by column permutation. The algorithm runs in optimal O(log(mn)) time with O(M(m)n log m/m+M(n)m/sup 2/ log n/n/sup 2/) work on CREW PRAM where M(n) denotes the processor bound for multiplying two n/spl times/n matrices in O(log n) time and is o(n/sup 2.376/). We then show that this procedure can recognize doubly convex bipartite graphs in O(log n) time with O(M(n)) processors.
[parallel algorithms, consecutive ones property, doubly convex bipartite graphs, graph theory, processor bound, Phase change random access memory, rows, matrix, Parallel algorithms, provably fastest parallel algorithm, CREW PRAM, Bipartite graph, computational complexity]
Hardware support for release consistency with queue-based synchronization
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Shared-memory multiprocessors are known to have intrinsic overheads imposed by synchronization and data coherency. Release consistency is a memory consistency model, which alleviates these overheads by relaxing the execution order of a parallel program. This paper proposes techniques to improve the performance of release consistency by combining and QOLB synchronization with write caches. In our scheme, writes in a critical section are deferred with a write cache until a lock variable protecting the critical section is released. On a release, the deferred writes are performed not globally but only to a processor that will execute the critical section exclusively at the next time. To determine the next processor at the execution time, we adopt QOLB primitives as synchronization primitives. We evaluate the performance of our scheme by program driven simulation. Experimental results show that it can improve the performance of shared-memory multiprocessors by reducing read stall time and synchronization stall time.
[Out of order, Protocols, parallel architectures, Cache memory, Telecommunication traffic, Data engineering, synchronization stall time, System performance, Traffic control, shared memory systems, read stall time, Hardware, release consistency, Protection, QOLB primitives, data coherency, synchronization primitives, memory consistency models, shared-memory multiprocessors, performance evaluation, queue-based synchronization, Programming profession, synchronisation, write caches, performance, intrinsic overheads]
Augmenting work-greedy assignment schemes with task duplication
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
This paper analyses the effect of task duplication on the assignment of task dependency graphs onto concurrent processor systems. It presents a scheme to augment work-greedy assignment schemes with task duplication. Such augmentation results in a time-complexity increase which is well below that of comparable assignment schemes with task duplication. The paper shows empirical results comparing the augmented assignment schemes.
[parallel algorithms, Costs, task duplication, Optimal scheduling, task dependency graphs, concurrent processor systems, time-complexity, augmented assignment schemes, Computer science, work-greedy assignment schemes, Processor scheduling, resource allocation, Polynomials, Clocks, computational complexity]
A study of train communication systems for future railways using multimeter-wave radio
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
This paper presents a concept of multimedia railway information and communication system for train operation and passengers, which is intra-network system for railway information connected to public network. And this paper proposes a new train radio communication system between trains and ground facilities which adopts subcarrier multiplexed (SCM) lightwave system in feeder links and 45 GHz band millimeter wave radio communication system in radio links, for aiming high-speed and large-capacity communication such as multi-channel quasi-videos. Also this paper describes performance estimation of this proposed system obtained by computer simulation. Results of estimation show a high degree of feasibility in all cases.
[Transmitting antennas, Communication system control, multimedia systems, multi channel quasi-videos, digital simulation, Proposals, telecommunication computing, Radio spectrum management, train communication systems, microwave links, computer simulation, Rail transportation, Optical fiber communication, Radio control, multimedia railway information, Base stations, large-capacity communication, future railways, communication system, Optical fiber cables, ground facilities, subcarrier multiplexed lightwave system, performance estimation, Millimeter wave communication, railways, public network, feeder links, intra-network system, multimeter-wave radio]
Query expansion for intelligent information retrieval on Internet
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Most systems that manage distributed information on Internet have difficulties in retrieving relevant information for they are not able to include the exact semantics of retrieval queries that users request. In this paper, we propose an automatic query expansion method based on term distribution, which naturally reflects semantics of retrieval terms in order to enhance the performance of information retrieval. The SVD technique in the LSI is utilized in the proposed method to measure the term distribution which appears similar to a query. Terms appearing most similar to the query in consideration of the distribution are appended to the query. Thereby, the query can hit documents without having common terms but with common concepts. An automatic term reduction technique is also proposed which does not choose to append all the terms in the same distribution area. The experimental results show our method maintains comparable retrieval effectiveness as the other LSI methods without having to append many unnecessary terms.
[LSI, information retrieval, Information retrieval, Large scale integration, Explosions, term distribution, Electronic mail, Information management, semantics, intelligent information retrieval, Computer science, query processing, Engineering management, automatic term reduction technique, Systems engineering and theory, Internet, query expansion, distributed information, SVD technique, Software engineering]
Instruction cache prefetching with extended BTB
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Instruction cache prefetching is a technique to reduce the penalty caused by instruction cache misses. The prefetching methods generally determine the target line to be prefetched based on the current fetched line address. However, as the cache line becomes wider, there may be multiple branches in a cache line which hurdles the decision made by these methods. This paper develops a new instruction cache prefetching method in which the prefetch is directed by the prediction on branches. We call it the branch instruction based (BIB) prefetching. In BIB prefetching, the prefetch information is recorded in an extended BTB. Simulation results show that, the BIB prefetching outperforms the traditional sequential prefetching by 7% and other prediction table based prefetching methods by 17% on average. As the BTB designs become more sophisticated and achieve higher hit and accuracy ratio, the BIB prefetching can achieve higher performance.
[instruction sets, Prefetching, Predictive models, Very large scale integration, performance evaluation, cache storage, digital simulation, instruction cache misses, fetched line address, Degradation, Computer science, Computer aided instruction, instruction cache prefetching, branch instruction based prefetching, System performance, Microprocessors, simulation results, instruction cache prefetching method, Contracts, Clocks]
Optimal computation of shortest paths on doubly convex bipartite graphs
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
An optimal parallel algorithm for computing all-pair shortest paths on doubly convex bipartite graphs is presented here. Our parallel algorithm runs in O(log n) time with O(n/sup 2//log n) processors on an EREW PRAM and is time-and-work-optimal. As a by-product, we show that the problem can be solved by a sequential algorithm in O(n/sup 2/) time optimally on any adjacency list or matrix representing a doubly convex bipartite graph. The result in this paper improves a recent work on the problem for bipartite permutation graphs, which are properly contained in doubly convex bipartite graphs.
[Algorithm design and analysis, sequential algorithm, parallel algorithms, doubly convex bipartite graphs, Genetic mutations, optimal parallel algorithm, computational geometry, Phase change random access memory, optimal computation of shortest paths, all-pair shortest paths, Parallel algorithms, bipartite permutation graphs, Concurrent computing, EREW PRAM, adjacency list, time-and-work-optimal, Bipartite graph, computational complexity]
Effective computer technology for data processing
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
In this paper a non-traditional computer technology for data representation and processing is presented. It is developed on the basis of classic number theory and some results of fundamental research of Russian academicians Chebyshev and Vinogradov. A peculiarity of the algebraic constructions discussed is determined by using special polynomial conversions and mixed number systems. Some moments concerning this specific had been described previously. Special methods of representing the polynomials allow to obtain the product of two complex numbers in parallel mode using only two real multiplications instead of four as well as to obtain the product of two polynomials of a degree 'n' using n multiplications instead of n. This technology is based on applying both a polynomial ring mapping (PRM) and an extended Galois Fields techniques which allows large dynamic range computations to be performed using massively parallel small finite ring computations. Such computations can offer distinct advantages over computations using usual binary number system.
[dynamic range computations, data representation, polynomials, mathematics computing, polynomial conversions, data processing, Data processing, computer technology, algebraic constructions, Galois fields, binary number system, Zinc, Concurrent computing, polynomial ring mapping, Sufficient conditions, Chebyshev approximation, Polynomials, Hardware, data structures, extended Galois fields techniques, number theory, massively parallel small finite ring computations]
Fault-tolerant ring embedding in faulty arrangement graphs
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
The arrangement graph A/sub n,k/, which is a generalization of the star graph (n-t=1), presents more flexibility than the star graph in adjusting the major design parameters: number of nodes, degree, and diameter. Previously the arrangement graph has proven hamiltonian. In this paper we further show that the arrangement graph remains hamiltonian even if it is faulty. Let |F/sub e/| and |F/sub v/| denote the numbers of edge faults and vertex faults, respectively. We show that A/sub n,k/ is hamiltonian when (1) (k=2 and n-k/spl ges/4, or k/spl ges/3 and n-k/spl ges/4+[k/2]), and |F/sub e/|/spl les/k(n-k-2)-1, or (2) k/spl ges/2, n-k/spl ges/2+[k/2], and |F/sub e/|/spl les/k(n-k-3)-1, or (3) k/spl ges/2, n-k/spl ges/3, and |F/sub 3/|/spl les/k.
[Computer science, Fault tolerance, fault-tolerant ring embedding, star graph, design parameters, vertex faults, performance evaluation, faulty arrangement graphs, Hypercubes, hypercube networks, edge faults, fault tolerant computing]
Synchronous load balancing in hypercube multicomputers with faulty nodes
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
This paper presents a new dynamic load balancing algorithm for hypercube multicomputers with faulty nodes. The emphasis in our method is on obtaining global load information and performing task migration using "short paths" in a synchronous manner so that a minimal amount of communication overhead is required. To accomplish this, we present an algorithm for constructing a new logical topology from a hypercube topology with faulty nodes. This new topology is used to obtain the global load information and to perform task migration. Simulation results are used to evaluate the performance of our dynamic load balancing method. The proposed strategy shows good performance in the case of a small number of faulty nodes when compared with previous methods.
[synchronous load balancing, Heuristic algorithms, Multiprocessor interconnection networks, hypercube multicomputers, performance evaluation, hypercube networks, digital simulation, Topology, Application software, Distributed computing, communication overhead, Concurrent computing, logical topology, resource allocation, simulation results, Parallel processing, Load management, Hypercubes, Computer networks, fault tolerant computing, faulty nodes, task migration]
A formal versioning approach for distributed objectbases
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
We initially provide, in summary, a formal object-oriented model for objectbases considering (i) simple objects (classes and their instances), (ii) 2-objects (2-classes and their 2-instances) where each 2-object encapsulates a two-level-hierarchy of objects, and (iii) complex objects defined inductively from the 2-objects through a composition operation. The inheritance concept is replaced by that of a "link" between two objects. Based on the above model, we introduce and develop a formal versioning approach by defining the concepts of "core data" and "core behaviour" that together are expressed through the term "compatibility" among objects. We extend the "versioning" notion to encompass not only the instances but also the classes, and 2-classes of our formal object-oriented model and include the different "views" (modelled as objects) of an entity within the same versioning framework. We further refer to the appropriate versioning structures. We then develop a distribution strategy encompassing 2-objects and complex objects, as well as their versions, and stress the advantages (both at the design and distribution level) of this approach through an example.
[core data, Distribution strategy, Object oriented databases, Object oriented modeling, distribution strategy, object-oriented databases, Africa, Displays, inheritance, History, formal versioning approach, Stress, Computer science, configuration management, formal object-oriented model, Distributed databases, distributed databases, distributed objectbases, core behaviour, Usability, composition operation]
Parallel discrete event processing of sequential computations
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Previous proposals for the application of discrete event methods to parallelization of sequential software have been based on the optimistic execution strategy. We present a new method which avoids optimistic execution. This is motivated by the observation that the control structure of a sequential program constitutes a temporal coordinate system which is exogenous to the program execution. The method employs a logical time mechanism and provides adaptive synchronisation for the distributed execution. Hence data dependent and/or conditional parallelism is released without the risk of coherency violation. The paper begins with a brief introduction to the parallel discrete event simulation paradigm. The efficient coarse grain mapping of conventional programs onto this paradigm is then discussed.
[parallelising compilers, parallel discrete event simulation, Optimization methods, Discrete event simulation, parallel programming, coherency violation, Concurrent computing, Runtime, discrete event simulation, program control structures, discrete event methods, optimistic execution strategy, program control structure, data dependent parallelism, conditional parallelism, Synchronization, Application software, synchronisation, parallel discrete event processing, Partial response channels, sequential software parallelization, logical time mechanism, adaptive synchronisation, Communication channels, sequential computations, Timing, temporal coordinate system, coarse grain mapping, Clocks]
Protocol synthesis from time Petri net based service specifications
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Some methods for deriving protocol specifications from given service specifications with time constraints have been proposed. However, existing methods cannot treat the class of service specifications with both parallel synchronization and data values. They also assume that all clocks in the distributed system are synchronized. We propose an algorithm to derive a correct protocol specification automatically from a given service specification described in an extended model of time Petri nets where the above restrictions are eliminated. Using our method, we will be free from considering the details of communication delays on the design of real-time distributed systems.
[Real time systems, time constraints, Petri nets, distributed processing, distributed system, formal specification, protocol synthesis, time Petri net, protocols, real-time distributed systems, Delay effects, Access protocols, Synchronization, synchronisation, clocks, data values, Automata, delays, real-time systems, protocol specifications, parallel synchronization, Time factors, communication delays, Clocks, service specifications]
The parallel approximability of a subclass of quadratic programming
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
In this paper we deal with the parallel approximability of a special class of Quadratic Programming (QP), called Smooth Positive Quadratic Programming. This subclass of QP is obtained by imposing restrictions on the coefficients of the QP instance. The Smoothness condition restricts the magnitudes of the coefficients while the positiveness requires that all the coefficients be non-negative. Interestingly, even with these restrictions several combinatorial problems can be modeled by Smooth QP. We show NC Approximation Schemes for the instances of Smooth Positive QP. This is done by reducing the instance of QP to an instance of Positive Linear Programming, finding in NC an approximate fractional solution to the obtained program, and then rounding the fractional solution to an integer approximate solution for the original problem. Then we show how to extend the result for positive instances of bounded degree to Smooth Integer Programming problems. Finally, we formulate several important combinatorial problems as Positive Quadratic Programs (or Positive Integer Programs) in packing/covering form and show that the techniques presented can be used to obtain NC Approximation Schemes for "dense" instances of such problems.
[smoothness condition, Statistical analysis, smooth positive quadratic programming, integer programming, Linear programming, Phase change random access memory, Large scale integration, linear programming, Regression analysis, Quadratic programming, quadratic programming, parallel programming, positive integer programs, positive linear programming, Integer linear programming, parallel approximability, Marketing and sales, Polynomials, NC approximation schemes, Functional programming, positive quadratic programs]
Bottleneck-free interconnect and IO subsystem in SPAX
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
The performance of parallel systems under commercial applications strongly depend on the speeds of IO subsystems and interconnects. Any single bottleneck in there hinder one from fully taking advantages of today's faster CPUs and better instruction architectures. Therefore, configuring bottleneck-free IO subsystems and interconnects is essential. In this paper we considered IO subsystem and interconnect in SPAX system. When the SPAX's IO subsystem is lightly equipped, the first bottleneck appears at disk drives. Although the disk bottleneck can be removed by putting more disks, another bottleneck was detected at the data buffer which is the first component in communication path. That is to say, all components in IO subsystems and interconnects must be sufficiently equipped, simultaneously, to thoroughly eliminate the bottleneck. We could reach a cost-effective bottleneck-free configuration of SPAX by making use of its flexible hardware design. The simulation results of SPAX then show significant performance gain from faster CPUs with better instruction architectures.
[bottleneck-free interconnect, Scientific computing, parallel architectures, interconnects, Electronic equipment testing, multiprocessor interconnection networks, performance evaluation, input-output programs, Parallel architectures, Concurrent computing, IO subsystems, Computer aided instruction, Disk drives, SPAX, instruction architectures, Benchmark testing, Hardware, IO subsystem, Neck, Computer buffers]
Control of parallel task granularity by throttling decomposition
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
This paper introduces a new mechanism which performs lazy task creation in a novel way for the exposure of large grain parallelism. In lazy task creation, all new-born tasks are provisionally inlined and parallelism is extracted from the inlined information later on demand. Large grain parallelism is achieved by executing the inlined tasks in the current thread unless a new task is demanded. However, the new scheme leaves a self-divisible task called seed instead inlining the task and let further task demands be satisfied by sprouting new tasks from the seed rather than retrospectively reversing the inlining decision of the task. In the scheme, decomposition itself is throttled rather than just the extraction of a task. The scheme makes the serial section clearly separated from the parallel section in an evaluation tree, and this allows the serial section to adopt a sequential algorithm. The performance improvement is significant in divide-and-conquer applications by adoption of sequential algorithms.
[Availability, sequential algorithm, parallel algorithms, Costs, divide and conquer methods, divide-and-conquer applications, Heuristic algorithms, Parallel machines, lazy task creation, decomposition, Data mining, Yarn, performance improvement, Programming profession, self-divisible task, Computer science, Program processors, resource allocation, Parallel processing, parallel task granularity, software performance evaluation, large grain parallelism]
Run-time parallelization for partially parallel loops
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
In this paper, a run-time technique based on inspector-executor scheme is proposed to find available parallelism on loops in this paper. Our inspector can determine the wavefronts by building a DEF-USE table. Additionally, the process of inspector for finding the wavefronts, can be parallelized fully without any synchronization. Our executor can perform the loop iterations concurrently. For each wavefront in a loop, the auto-adapted function is used to get a tailored thread number rather than using fixed thread number for execution. Experimental results show that our new parallel inspector can handle complex data dependency patterns and reduce itself execution time obviously. Besides, the new partitioning strategy for executor can also improve the performance of run-time parallelization obviously.
[Chaos, loop iterations, fixed thread number, tailored thread number, run-time technique, parallelism, Sparse matrices, Data mining, Yarn, program compilers, parallel programming, Concurrent computing, Runtime, partially parallel loops, Parallel processing, wavefronts, Data analysis, complex data dependency patterns, Helium, auto-adapted function, synchronisation, Processor scheduling, run-time parallelization, inspector-executor scheme, DEF-USE table]
Performance of multiple links over single link in STC104 networks
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
There is a great demand for an efficient and reliable router in high performance parallel processing systems or data management networks. The fast routing chip, Inmos STC104 has been designed and now available in the market. The performance and characteristics of each different network topology, multistage networks, meshes, tori, and N-cubes are vital information to make decision on an appropriate network. This paper reviews the technology utilized in a fast packet switch STC104 and studies reliable routing algorithms for various network configurations. The performances of several configurations under a single link and multiple links environment are also simulated and compared.
[tori, Communication system control, multiprocessor interconnection networks, packet switching, N-cubes, Switches, Very large scale integration, Data engineering, Inmos STC104, STC104 networks, parallel processing, fast routing chip, Intelligent networks, high performance parallel processing systems, Network topology, Computer network reliability, Computer networks, data management networks, single link, meshes, fast packet switch STC104, Packet switching, performance evaluation, Routing, reliable router, multiple links performance, network configurations, multistage networks]
A distribution format for communication-free reshape in distributed memory computers
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Data parallel languages designed for distributed memory computing environments provide a single global address space to the programmer. The mapping from this global address space to the distributed local address space is performed by a compiler, which does this mapping based on the array distribution format. Thus, each array in a data parallel language program has its own distribution format. The Reshape function changes the array distribution format as well as the array shape. However, the changed distribution format cannot be represented by any distribution format supported in current languages. Because there is no suitable distribution format, it is necessary to change the reshaped distribution format to an existing distribution format, with heavy overhead due to the redistribution function. To eliminate the redistribution step in Reshape function, we have proposed a new distribution format, HIER-CYCLIC, which can represent the reshaped distribution format. We have also proposed a language syntax to use HIER-CYCLIC and a compiling mechanism. Finally, we performed an experiment on an IBM-SP2 machine using a shift function.
[Parallel languages, Shape, Data engineering, compiling mechanism, shift function, Distributed computing, program compilers, Concurrent computing, HIER-CYCLIC, language syntax, Reshape function, IBM-SP2 machine, distributed memory computing environments, distributed local address space, data parallel language program, compiler, Marine vehicles, parallel languages, Contracts, data parallel languages, array distribution format, single global address space, Programming profession, Computer science, distributed memory computers, distribution format, distributed memory systems, Systems engineering and theory, global address space, programming environments, communication-free reshape]
Performance of the r-truncated Benes networks under randomized routing algorithms
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Benes networks have the potential for balanced traffic, fewer conflicts, and can route any permutation in one pass due to their multiplicity of paths. Omega networks, on the other hand, have fast set-up and low hardware cost, but could take more than one pass to route a permutation. This paper introduces a new class of networks referred to as r-truncated Benes, which is a Benes network with r randomization stages eliminated. Using randomized routing, we will show that r-truncated Benes networks is an excellent trade-off between Omega and Benes networks. In particular, it will be shown that the I-truncated Benes network out performs Omega and is also superior to Benes and other truncated Benes networks in cost and performance.
[Costs, cost, Multiprocessor interconnection networks, network routing, multiprocessor interconnection networks, Telecommunication traffic, performance evaluation, Benes networks, Routing, Ambient intelligence, Parallel algorithms, randomised algorithms, randomized routing, Concurrent computing, performance, balanced traffic, Parallel processing, Hardware, Computer networks, truncated Benes networks, r-truncated Benes networks]
Design and implementation of PVM-based portable distributed shared memory system on the workstation cluster environment
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Cluster of workstations or personal computers connected with high speed network has become one of major architectures of distributed memory parallel computers. However, software on the cluster environment is still not improved in performance. In particular, there has not been proposed a standard distributed shared memory mechanism on the cluster environment. The distributed shared memory can be a solution of programming style on distributed memory parallel system including clusters because we know from experiences that shared memory model makes programming easy. It must be implemented with care for the performance problem because we cannot assume any hardware support for shared memory system in the cluster environment. Another, but serious problem is the portability. In this paper, we discuss the design and implementation of portable distributed shared memory system. Our shared memory system is based on PVM in consideration of portability. Our contribution in this paper is the design and implementation of portable shared memory system on the cluster environment using faithful implementation of active messages fully in software, together with an enhancement of PVM to support active messages.
[Memory architecture, Software performance, Microcomputers, performance evaluation, PVM-based portable distributed shared memory system, cluster of workstations, Distributed computing, Concurrent computing, workstations, portability, Parallel programming, High-speed networks, performance problem, Computer architecture, distributed memory systems, virtual machines, workstation cluster environment, shared memory systems, Computer networks, Workstations]
Performance analysis of a parallel distributive join algorithm on the Intel paragon
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
In this paper, we analyze the performance of the parallel distributive join algorithm that we proposed previously (1996). We implemented the algorithm on an Intel Paragon machine and analyzed the effect of the number of processors and the join selectivity on the performance of the algorithm. We also compared the performance of the distributed join (DJ) algorithm with that of the hybrid-hash (HH) join algorithm. Our results show that the DJ performs comparably with the HH over the entire range of number of processors used and different join selectivities. A big advantage of the parallel DJ algorithm over the RH join algorithm is that it can easily support non-equijoin operations. The results can also be used to estimate the performance of file I/O intensive applications to be implemented on the Intel Paragon machine.
[Algorithm design and analysis, parallel algorithms, hybrid-hash join algorithm, Poles and towers, Partitioning algorithms, relational databases, Delay, Computer science, file I/O intensive applications, file organisation, Intel paragon, Hardware, Performance analysis, distributed join algorithm, parallel distributive join algorithm, software performance evaluation, performance analysis]
SliM-II: a linear array SIMD processor for real-time image processing
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
This paper describes architectures and design of a linear array processor chip called a SliM-II Image Processor. The chip has a linear array of 64 processing elements (PEs). In contrast to existing array processors, each PE has a multiplier that is quite effective for convolution, template matching, etc. The instruction set can execute an ALU, a data I/O, and an inter-PE communication operations simultaneously in an instruction cycle. In addition, during the ALU/multiplier operation, SliM-II provides parallel data load/store between the register file and on-chip memory as in DSP chips. The SliM-II contains about 1.5 million transistors in a 13.2/spl times/13.0 mm/sup 2/ core size and the package type is 208 pin PQ2. The performance estimation shows a significant improvement for algorithms requiring multiplications compared with existing array processors.
[Real time systems, image processing, Image processing, parallel architectures, real-time image processing, Inspection, template matching, performance estimation, Registers, SliM-II, linear array SIMD processor, ALU/multiplier operation, Digital signal processing chips, Convolution, Surveillance, real-time systems, linear array processor, Streaming media, Packaging, convolution, Clocks]
An algorithmic framework for parallelizing vision computations on distributed-memory machines
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
With advances in processor and networking technologies, current distributed-memory machines can achieve hundreds of Giga Floating-Point Operations Per Second (GFLOPS) of performance. By using such machines, many application problems having regularly structured computations have been successfully parallelized using the explicit message passing paradigm, However, it is difficult to parallelize vision problems having irregularly structured computations. Parallel solutions to these problems are characterized by uneven distribution of symbolic features among the processors, unbalanced workload, and irregular interprocessor data dependency caused by the input image. It is therefore necessary to develop efficient algorithmic techniques to achieve large speed-ups. In this paper, we propose an algorithmic framework to design efficient and portable parallel algorithms for irregular vision problems on distributed-memory machines. Based on this algorithmic framework, we develop techniques for task scheduling, load balancing, and overlapping communication with computation.
[Computer vision, parallel algorithms, message passing, GFLOPS, load balancing, vision problems, Distributed computing, Parallel algorithms, Scheduling algorithm, Concurrent computing, Reduced instruction set computing, Parallel programming, Message passing, irregular vision problems, distributed memory systems, computer vision, Streaming media, Computer networks, distributed-memory machines, task scheduling, vision computations]
Fault tolerant cluster computing through replication
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Long-lived parallel applications running on work station clusters are vulnerable to single-node or multiple-node failures. Fault recovery is therefore required to prevent immature program termination. However, much of the runtime overhead imposed by fault tolerance schemes is generally due to the cost of transferring the checkpoint states of applications by disk I/O operations. In this paper, we propose a fault tolerant model in which checkpoint states are transferred between replicated parallel applications. We also describe how the resource consumption of the replicated applications can be minimized. The fault tolerant model has been implemented and tested on a workstation cluster and a Fujitsu AP3000 multi-processor machine. The measurements of our experiments have showed that efficient fault tolerance can be achieved by replicating parallel applications on clusters of computers.
[Checkpointing, workstation clusters, runtime overhead, program verification, multiprocessor interconnection networks, checkpoint states, Discrete event simulation, fault tolerance schemes, system recovery, Information systems, Concurrent computing, Fault tolerance, Runtime, program termination, fault tolerant model, fault tolerant cluster computing, Workstations, discrete event simulation, replication, Fujitsu AP3000 multi-processor machine, performance evaluation, Application software, workstation cluster, Computer science, fault recovery, parallel programming5795293, Writing, fault tolerant computing, resource consumption]
Embeddings of cycles, meshes and tori in faulty k-ary n-cubes
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
We investigate the existence of cycles, meshes and tori in a k-ary n-cube Q/sub n//sup k/ in which a limited number of nodes and links are faulty. Our main result is that in a k-ary n-cube Q/sub n//sup k/ in which there are v faulty nodes /spl lambda/faulty links where v+/spl lambda//spl les/n, there is a cycle of length at least k/sup n/-vw, where w=1 if k is odd and w=2 if k is even (throughout k/spl ges/3 and n/spl ges/2). We extend this result so as to prove the existence of large meshes and tori in such a faulty k-ary n-cube.
[Multiprocessor interconnection networks, tori, Scattering, performance evaluation, Routing, hypercube networks, parallel processing, Fault tolerance, cycles embedding, Broadcasting, Hypercubes, fault tolerant computing, faulty k-ary n-cubes, meshes]
An efficient adaptive routing algorithm for the faulty star graph
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
This paper introduces an adaptive distributed routing algorithm for the faulty star graph. By giving two routing rules based on the properties of nodes, an optimal routing function for the fault-free star graph is presented. For a given destination in the n-star graph, n-1 node-disjoint and edge-disjoint subgraphs, which are derived from n-1 adjacent edges of the destination, can be constructed by applying this routing function and the concept of breadth first search. When faults are encountered, the algorithm can route messages to the destination by finding a fault-free subgraph based on the local failure information. As long as the number f of faults (node faults and/or edge faults) is less than the degree n-1 of the n-star graph, the algorithm can adaptively find a path of length at most d+4f to route messages successfully from a source to a destination, where d is the distance between two nodes.
[Algorithm design and analysis, Costs, Adaptive systems, optimal routing function, node-disjoint subgraphs, parallel architectures, multiprocessor interconnection networks, Very large scale integration, Concurrent computing, Fault tolerance, Hypercubes, multiprocessor interconnection network, message passing, network routing, routing rules, fault-free star graph, faulty star graph, Routing, message routing, Topology, tree searching, edge-disjoint subgraphs, local failure information, distributed algorithms, directed graphs, adaptive distributed routing algorithm, System recovery, fault tolerant computing, breadth first search]
A new interconnection network for parallel computer with low diameter
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
In this paper, we propose and analyze the new interconnection network for parallel computer, called graycube. The graycube has the same number of nodes and edges as hypercube, but it's diameter is about one half of the equivalent hypercube. It has simple recursive structure, routing and broadcasting algorithms. Since hypercube can be embedded into graycube with dialation 2, algorithms developed based on hypercube are easily simulated in graycube. The basic properties, routing and broadcasting algorithms, and hypercube embedding are presented.
[recursive structure, hypercube embedding, Multiprocessor interconnection networks, graycube, Routing, hypercube networks, parallel processing, parallel computer, Concurrent computing, Computer science, routing, broadcasting algorithms, Network topology, interconnection network, Broadcasting, Hypercubes, Computer networks, Reflective binary codes, dialation 2, Joining processes, low diameter]
A delivery scheduling system by the distributed cooperative multiple agents
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
In this paper, we propose a practical method for solving the delivery scheduling problem with multiple agents and discuss its implementation. The method is based on the distributed cooperative problem solving. In the delivery scheduling system, the covered region is partitioned into multiple subregions and each subregion is assigned a subproblem solving agent which solves the problem of scheduling of drivers and trucks in the subregion. Integrating those subproblem solving agents, an integration-and-evaluation agent solves the total problem. By employing the distributed cooperative problem solving framework for the delivery scheduling problem, we achieved an easy incorporation of various evaluation parameters in the process of scheduling, efficient use and management of scheduling knowledge of various levels, and reduction of computer processing by division of the problem into subproblems.
[Costs, evaluation parameters, Knowledge management, problem solving, distributed cooperative multiple agents, Personnel, Application software, Environmental management, Distributed computing, Vehicles, transportation, logistics data processing, delivery scheduling system, subproblem solving agent, distributed cooperative problem solving, Processor scheduling, knowledge based systems, Large-scale systems, Problem-solving]
Parallel Systems And Parallel I/O Technology
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
false
[File systems, Computer architecture, Very large scale integration, User interfaces, Modems, Hardware, Application software, Programming profession, Research and development, Multiprocessing systems]
A distributed memory MIMD multi-computer with reconfigurable custom computing capabilities
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Armstrong III is a multi node multi-computer designed and built at the Laboratory for Engineering Man/Machine System (LEMS) of Brown University. Each node contains a RISC processor and reconfigurable resources implemented with FPGAs. The primary benefit in using FPGAs is that the resulting hardware is neither rigid nor permanent but is in-circuit reprogrammable. This allows each node to be tailored to the computational requirements of an application. This paper describes the Armstrong III architecture and concludes with a substantive example application that performs HMM Training for speech recognition with the reconfigurable platform.
[parallel architectures, Application software, MIMD multi-computer, Distributed computing, Sun, RISC processor, reconfigurable resources, Reduced instruction set computing, speech recognition, reconfigurable architectures, distributed memory, Hidden Markov models, Armstrong III, Computer architecture, Computer applications, distributed memory systems, reconfigurable platform, Computer performance, Communication cables, reconfigurable custom computing, LEMS, Field programmable gate arrays]
A hierarchical BSP model supporting processor locality
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
The paper presents a parallel computing model, called H-BSP, which adds a hierarchical concept to the BSP (Bulk Synchronous Parallel) computing model. A H-BSP program consists of a number of BSP groups which are dynamically created at run time and executed in a hierarchical fashion. H-BSP allows the algorithm designer to develop a more efficient algorithm by utilizing processor locality in the program. The paper describes the structure of the H-BSP model, complexity analysis and an example of the H-BSP algorithm. Also presented are the performance characteristics of the H-BSP algorithm based on simulation analysis. Simulation results show that H-BSP model takes advantages of processor locality and performs well in low bandwidth networks or in a constant valence architecture such as a 2 dimensional mesh. It is also proved that H-BSP model can predict algorithm performance better than the BSP model due to its locality preserving nature.
[Algorithm design and analysis, H-BSP algorithm, locality preserving nature, processor locality, complexity analysis, low bandwidth networks, Predictive models, parallel computing model, parallel programming, Concurrent computing, Analytical models, hierarchical BSP model, Parallel processing, Hardware, hierarchical fashion, Manufacturing, performance characteristics, software performance evaluation, parallel algorithms, Computational modeling, 2 dimensional mesh, constant valence architecture, Phase change random access memory, Computer science, hierarchical concept, BSP groups, Bulk Synchronous Parallel computing model, algorithm designer, simulation analysis, computational complexity]
Multi-agent based decision mechanism for distributed meeting scheduling system
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
In existing meeting scheduling systems, finding a common free time slot of all (prospective) participants is the goal. When such a free-time slot is not found, the host has to ask the busy participants to change their schedules. Changing a schedule is a complex process requiring a lot of negotiations and, there is no guarantee that the goal will be achieved. In the worst case the host has to ignore the time slot (which was probably the first choice) and try to schedule the meeting in another time slot (probably a lesser choice). The process will continue till the scheduling succeeds. We argue that it is not always necessary for all the participants to attend a meeting. There are some participants whose presence is mandatory for the meeting. The presence of other participants are non-mandatory. The meeting can proceed in the absence of some of the non-mandatory participants. This allows greater flexibility and more efficient scheduling. This paper introduces the concept of a Quorum in the meeting scheduling process and proposes a two-round negotiating protocol to find the time-slot which meets the quorum requirements. To establish the robustness of our scheduler, we simulated scheduling in our lab, and showed its flexibility in scheduling a meeting.
[Protocols, Quorum, multi-agent based decision mechanism, Scheduling, Intelligent agent, processor scheduling, two-round negotiating protocol, Privacy, free-time slot, scheduling, Robustness, protocols, distributed meeting scheduling system, Artificial intelligence, Protection, Testing]
Competitive execution: a method to exploit idle workstations
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
This paper presents competition protocols to speed up distributed programs on a network of shared workstations in the background by exploiting their wasted computing capacity, without interfering with foreground processes. Competition protocols are transparent operating system facilities that involve creating multiple instances (called clones) p/sub 1/, p/sub 2/, etc. of a process P on different processors, and making clones "compete\
[Protocols, Costs, clones, Snow, idle workstations, network of shared workstations, Cloning, competitive execution, sequential programs, Distributed computing, parallel programming, distributed programs, workstations, competition protocols, Processor scheduling, Operating systems, transparent operating system, multiple instances, moment-by-moment basis, Computer networks, Workstations, protocols, Capacity planning]
An efficient and authenticated group-oriented cryptoscheme based on a geometric method in Internet environments
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Based on a geometric method, this paper presents an efficient and authenticated cryptoscheme for establishing secure group-oriented data communications in Internet environments. We assume that the Internet environments consist of many hosts, and each host has many users attached to it. The secure group-oriented communication scheme proposed in this paper incorporates the public-key distribution and the trigonometry concepts as the basic theory. Since this scheme does not need any trusted key distribution center to distribute the common secret session key between two parties and can reduce the computation time needed for securely sending messages to a group of receivers by using multiplication operations instead of modular exponentiation, it is quite suitable to be used in Internet environments so that the key distribution is convenient, time-saving and reparable. Furthermore, an authentication protocol is also proposed. Such a protocol can not only identify both the sender and the receiver of a group correctly, but can also make sure the transmitted message has reached its destination safely.
[Protocols, Data security, computer networks, cryptography, secure group-oriented data communications, Distributed computing, Internet environments, authenticated group-oriented cryptoscheme, geometric method, Public key, Authentication, Public key cryptography, common secret session key, Computer networks, Internet, trigonometry concepts, authentication protocol, Data communication, IP networks, protocols, group-oriented communication scheme, public-key distribution]
/spl Delta/-causality in wide-area group communications
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
In distributed applications, a group of multiple processes cooperate by exchanging multimedia messages. It is critical to support the group of application processes with enough quality of service (QoS) in addition to the ordered delivery of messages. The delay time and the message loss ratio are significant QoS parameters. In the Internet application, the delay time and the loss ratio are significantly different in different communication channels. We define a novel causality named /spl Delta/*-causality among the messages to hold in the world-wide environment. We discuss how to transmit messages to the destination processes and how to resolve the message loss and delay supporting the /spl Delta/*-causality given the requirements of delay time and message loss ratio.
[Protocols, telecommunication channels, wide area networks, telecommunication services, Quality of service, distributed applications, delay time, multiple process cooperation, Propagation losses, multimedia message exchange, protocols, multimedia communication, communication channels, message passing, Delay effects, Europe, /spl Delta/-causality, quality of service, Application software, wide-area group communications, Teleconferencing, ordered message delivery, delays, message loss ratio, Communication channels, Internet, destination processes, Telecommunication network reliability, Internet application]
Evaluation of a memory hierarchy for the MTS multithreaded processor
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Executing multiple threads simultaneously on superscalar processor can improve hardware resource utilization and instruction throughput. The Multi-Threaded Superscalar (MTS) processor efficiently achieves the concurrent execution of multiple instruction streams using a VLIW, multiple functional unit architecture. However, the limitations of the memory system may impede the potential performance of the MTS processor. An interactive, parameter-driven simulator of the MTS architecture was developed using SES/workbench. A set of numerical benchmarks was run on it with varying memory system configurations. Assuming a single instruction cache, a single data cache, and one instruction queue per thread, varied parameters included the size of the instruction queues, the number of ports to the instruction cache, main memory latency, and cache hit rates. Based on simulation results, optimal values were chosen for certain parameters. To reasonably utilize the MTS processor, the memory system must provide at least 64 instruction bytes per cycle, though the demands for data access are far less severe. For reasonable memory speeds, this requires a roughly IMB three-ported instruction cache capable of providing 128 bits per port per cycle, as well as an IMB single-ported 32-bit wide data cache. A more realistic multilevel cache hierarchy is proposed.
[program testing, numerical benchmarks, realistic multilevel cache hierarchy, cache hit rates, multiple functional unit architecture, Throughput, cache storage, Registers, VLIW, Yarn, parallel processing, Delay, SES/workbench, main memory latency, Computer architecture, Bandwidth, Hardware, multithreaded superscalar processor, discrete event simulation, memory system configurations, instruction sets, multiple instruction streams, superscalar processor, parameter-driven simulator, instruction throughput, memory hierarchy evaluation, instruction cache, MTS multithreaded processor, Resource management, Impedance, hardware resource utilization]
Parallel maximum matching algorithms in interval graphs
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
We develop new parallel maximum matching algorithms in interval graphs by exploiting the characteristics of interval graphs. For general interval graphs, our algorithm requires O(log/sup 2/ v+(n log n)/v) time and O(nv/sup 2/+n/sup 2/) operations on the CREW PRAM, where n is the number of intervals and v/spl les/n is a parameter. By choosing v=/spl radic/n, we obtain an O(/spl radic/n log n)-time algorithm in O(n/sup 2/) operations. For v=n/log n, we have an O(log/sup 2/ n)-time algorithm with n/sup 3//log/sup 4/ n processors. The previously best known solution takes O(log/sup 2/ n) time with n/sup 3/ processors. For proper interval graphs, our algorithm runs in O(log n) time using n/log n processors if input intervals are sorted and using n processors otherwise on the EREW PRAM. Our algorithms are much simpler than the previous ones.
[parallel algorithms, Computational modeling, Phase change random access memory, Application software, Cultural differences, Scheduling algorithm, Concurrent computing, parallel maximum matching algorithms, Processor scheduling, Computer applications, CREW PRAM, Polynomials, interval graphs, Electronic circuits, computational complexity]
Improve HTTP/TCP performance over ATM networks: new schemes and performance comparisons
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
This paper proposes a "selective packet retransmission" scheme for improving HTTP/TCP performance when transmitting through ATM networks. In selective packet retransmission, we utilize the property of human's perception tolerance for errors to determine whether to retransmit a corrupted TCP segment or not. For lossable data, such as image, when error occurs because of cell lost, it will not be retransmitted. The simulations show that, for the same buffer size and traffics load, selective packet retransmission results in higher throughput than PPD, EPD, and plain TCP over ATM.
[Protocols, human's perception tolerance, Telecommunication traffic, performance evaluation, Throughput, asynchronous transfer mode, digital simulation, Information management, buffer size, simulations, Graphics, traffics load, Image segmentation, Space technology, transport protocols, HTTP/TCP performance, Computer errors, selective packet retransmission, performance comparisons, Web server, telecommunication traffic, Asynchronous transfer mode, ATM networks]
Dynamic load distribution on a mesh with a single bus
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
In this paper, we consider the mesh with a single bus as a multi-computer topology that enhances the communication capability of the mesh and show that the mesh with a single bus has more salient properties than the mesh, the hypercube, and other mesh variants. These properties are small diameter, relatively small degree, small average distance, suitable for broadcasting, small initial data distribution time, etc. We propose a dynamic load distribution algorithm to utilize the enhanced communication capability of the mesh with a single bus. Also, asynchronous bus control and arbitration logic is designed to support the proposed algorithm efficiently. It is shown through simulation that the proposed dynamic load distribution is superior to the previous receiver-initiated diffusion method known as the best to-date. The proposed algorithm shows better total execution time of tasks and better processor utilization with a smaller number of task migrations.
[Algorithm design and analysis, Heuristic algorithms, Communication system control, Very large scale integration, hypercube networks, digital simulation, asynchronous bus control, salient properties, multicomputer topology, dynamic load distribution, resource allocation, Broadcasting, task migrations, Hypercubes, mesh, hypercube, initial data distribution time, performance evaluation, arbitration logic, Logic design, Topology, communication capability, Computer science, single bus, total execution time, processor utilization, Load management]
A parallel processing method for receding horizon tracking control
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
A recursive square root algorithm for Receding Horizon Tracking Control (RHTC) based on state-space model which the original sequential algorithm exhibits O(Nn/sup 2/) complexity is derived and its systolic implementation method is also shown. The parallel processing algorithm needs O(Nn) computations for an n-th order plant and N-step prediction horizon on O(n/sup 2/+nN) processor array.
[parallel algorithms, recursive square root algorithm, Design methodology, Very large scale integration, O(Nn/sup 2/) complexity, Control systems, adaptive control, state-space model, parallel processing method, Application software, Concurrent computing, multivariable control systems, Signal processing algorithms, Parallel processing, systolic implementation, MIMO, Systolic arrays, receding horizon tracking control, Predictive control, computational complexity]
A unified approach to global concurrency control and global deadlocks in a multidatabase environment
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Our objective is to provide a theoretical foundation for multidatabase transaction management that deals with global concurrency control and global deadlocks in a uniform manner. We first propose rigid conflict serializability as a sufficient condition for the global transaction management to ensure global serializability in multidatabase environment. Subsequently, it is shown that the enforcement of rigid conflict serializability through a rigid method at the time each global subtransaction begins its execution avoids global deadlocks. The deadlock-free policy in the paper seems to be attractive due to the simple and uniform approach it takes. The basic advantage of the approach is that the global transaction manager can allow any interleavings among normal database operations belonging to global transactions without any mechanism at global level.
[global deadlocks, Protocols, Terminology, multidatabase transaction management, Concurrency control, interleavings, Transaction databases, global serializability, system recovery, multidatabase environment, unified approach, Milling machines, Concurrent computing, database operations, rigid conflict serializability, concurrency control, distributed databases, System recovery, operating systems (computers), Database systems, Computer science education, global concurrency control, sufficient condition]
Information flow in a purpose-oriented access control model
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
In distributed applications, a group of multiple objects are cooperating to achieve some objectives. An object is modeled as a pair of data structures and operations. Each object is manipulated through an operation supported by the object and then the operation may further invoke operations of other objects, i.e., nested operations. The purpose-oriented access rules indicate which operation on each object can invoke operations of other objects. The information flow among the objects occur if the requests and responses of the operations carry some data. Only the purpose-oriented access rules which imply the legal information flow are allowed. We discuss how to test the access rules if the information flow occurring in the nested invocation of the operations is legal.
[Access control, legal information flow, System testing, Law, multiple object cooperation, distributed processing, purpose-oriented access control model, Distributed computing, distributed applications, information flow, object oriented method, authorisation, flow graph, data structures, object-oriented methods, client-server systems, Client-server systems, flow graphs, Data structures, Application software, nested invocation, security of data, nested operations, Information security, client server systems, Systems engineering and theory, Legal factors]
Media allocation methods to improve multimedia query and access reliability
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Distributed Multimedia Database Systems (DMDB) have gained in popularity for today's database system design due to their fault-tolerance, resource-sharing capacity, and reliable performance. Reliability performance is an important issue in DMDB design. The reliability of a DMDB depends on the reliability of its communication links and nodes as well as the distribution of its resources, such as various media. The distribution of media is an important factor that affects the multimedia query and access reliability (MQAR). The reliability-oriented media assignment problem is to find a media distribution such that the MQAR is maximal. In this paper, we attempt to develop a reliability model and algorithms to achieve reliability-oriented media allocation.
[distributed multimedia database systems, telecommunication links, fault-tolerance, software reliability, reliable performance, Multimedia databases, multimedia systems, communication links, Reliability engineering, Electronic mail, media allocation methods, multimedia access reliability, Graphics, Computer science, query processing, Fault tolerant systems, multimedia query, resource-sharing capacity, distributed databases, Database systems, fault tolerant computing, Telecommunication network reliability, Communication networks, Time factors]
Optimal broadcast in /spl alpha/-port wormhole-routed mesh networks
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
We consider the problem of broadcasting in 2- and 3-dimensional mesh networks with the wormhole-routing capability. Our model assumes that each node is capable of communicating with /spl alpha/ nodes simultaneously where /spl alpha/ is a given integer with 1/spl les//spl alpha//spl les/4 for 2-dimensional meshes and 1/spl les//spl alpha//spl les/6 for 3-dimensional meshes. We give broadcasting algorithms that use optimal number of phases (if each side of the mesh is (/spl alpha/+1)/sup +/) or at most optimal number plus two phases (otherwise). Our algorithms require the predetermined location of the source node and this assumption can be relaxed by allowing one additional initial phase.
[Algorithm design and analysis, optimal broadcast, multiprocessor interconnection networks, Routing, wormhole-routed mesh networks, broadcasting, Computer science, Intelligent networks, Mesh networks, Casting, broadcasting algorithms, Processor scheduling, wormhole-routing, Broadcasting, Computer networks]
Efficient distributed deadlock detection and resolution using probes, tokens, and barriers
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Probes and tokens are used in many deadlock detection and resolution algorithms. A deadlock is detected by propagating probes along dependency edges. When the initiator p/sub i/ of a probe receives its probe back, it knows of the existence of a deadlock. p/sub i/ then sends out a token to clean up those probes in the deadlock; cycle which, if not removed, may later lead to phantom deadlock detections. Only after the token returns to p/sub i/ is the deadlock resolved by aborting a 'victim' (usually p/sub i/). As a result, all involved transactions remain waiting and all involved resources locked until the token returns to p/sub i/, although the deadlock was already detected when the probe returned to p/sub i/. This paper proposes the idea of barriers to allow the deadlock to be resolved without waiting for the token to return to p/sub i/, thereby reducing the average deadlock persistence time considerably.
[Acoustic propagation, system recovery, Delay, Computer science, dependency edges, deadlock persistence time, barriers, distributed deadlock detection, distributed deadlock resolution, concurrency control, System recovery, Computer errors, Imaging phantoms, operating systems (computers), Database systems, probes, tokens, Probes]
Universally fault-tolerant broadcasting in trees
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
We consider broadcasting a message from one node of a tree to all other nodes. In the presence of up to k link failures the tree becomes disconnected, and only nodes in the connected component C containing the source can be informed. The maximum ratio between the time used by a broadcasting scheme B to inform C and the optimal time to inform C, taken over all components C yielded by configurations of at most k faults, is the k-vulnerability of B. This is the maximum slowdown incurred by B due to the lack of a priori knowledge of fault location, for at most k faults. Since the upper bound k on the number of faults is not always known, it is important to design broadcasting schemes that behave well under any possible number of faults. It turns out that achieving the lowest possible k-vulnerability for all k simultaneously is impossible for some trees. Hence a natural goal is to seek, for any tree T, a broadcasting scheme that simultaneously approximates the lowest possible k-vulnerability for every k, up to a given constant factor c (independent of L). We describe a polynomial algorithm which decides if such a "universally fault-tolerant" broadcasting scheme exists for given T and c, and constructs such a scheme if it exists.
[Fault location, fault trees, polynomial algorithm, Distributed computing, trees, message broadcasting, Concurrent computing, Fault tolerance, Casting, Upper bound, k-vulnerability, distributed algorithms, Distributed databases, Broadcasting, universally fault-tolerant broadcasting, Polynomials, fault tolerant computing, Communication networks, link failures]
Instruction folding in Java processor
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Traditionally, the performance of a stack machine was limited by the true data dependency. A performance enhancement mechanism-Stack Operations Folding-was used in Sun Microelectronics' picoJava design and it can reduce up to 60% of all stack operations. In this paper, we use the Java bytecode language as the target machine language, and study its instruction folding on a proposed machine model. Three folding strategies: 2-foldable, 3-foldable and 4-foldable, were simulated and evaluated. Statistical data show that our third folding strategy eliminates 73% of all stack operations, and each strategy has an overall program speedup of 1.19, 1.25 and 1.26, respectively, as compared to a traditional stack machine. Moreover, a Java machine model suitable for instruction folding, together with its pipeline stages, are presented. It seems to have the best cost/performance effectiveness of a Java stack machine if six bytes decoder width and the second folding strategy-the three-foldable strategy-are adopted.
[instruction folding, Costs, parallel architectures, Pipelines, parallel machines, Java processor, Analytical models, Computer networks, Performance analysis, 3-foldable, Java, Java bytecode, performance enhancement, performance evaluation, Java stack machine, Decoding, Application software, 4-foldable, Sun, Computer science, Stack Operations Folding, picoJava design, data dependency, 2-foldable, stack machine, pipeline processing]
Minimizing communication in bitonic sorting software
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Two parallel sorting algorithms, GENERAL-BS and MINIMIZING-BS, which are implemented on shared-memory parallel computers, are presented in this paper. A parity strategy which gives an idea for the efficient usage of the local memory associated with each processor is introduced. The number of network accesses(or communications) of the algorithm MINIMIZING-BS is reduced by approximately one half compared with the algorithm GENERAL-BS. On the basis of decreasing the communication, the algorithm MINIMIZING-BS results in a significant improvement of performance.
[parallel algorithms, Computational modeling, Multiprocessor interconnection networks, bitonic sorting software, GENERAL-BS, MINIMIZING-BS, parallel sorting algorithms, Sorting, local memory, Concurrent computing, Phased arrays, Computer science, communication minimisation, High performance computing, parity strategy, sorting, Parallel processing, Hypercubes, shared memory systems, Computer networks, shared-memory parallel computers]
The design and evaluation of policy-controllable buffer cache
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Traditional software using BBA (Black Box Abstraction) paradigm may show performance degradations under unpredicted situations. OI (Open Implementation) is proposed to solve the problem of BBA. While conserving the advantages of the BBA, the OI provides clients with interfaces to control the strategy of the black box, and thus makes the black box to be flexible and reusable in various situations and requests. In this paper we attempt to solve the problem of the buffer cache in the operating systems using the OI. We propose a buffer cache design which can support multiple buffer replacement policy. The buffer replacement policy can be controlled by client using provided meta-interface. We modify the kernel of Linux to get performance data. According to our experimental results on Linux, the performance is improved up to 36%. We conclude that the OI can be an alternative to BBA.
[open implementation, Costs, Buffer storage, Multimedia systems, black box abstraction paradigm, operating systems, cache storage, Delay, Computer science, Degradation, Linux, Operating systems, Database systems, policy-controllable buffer Cache, performance data, Kernel, programming environments, multiple buffer replacement policy, software performance evaluation]
The design and performance evaluation of the RAID 5 controller using the load-balanced destage algorithm
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Write requests are written from the disk cache to the disks by the destage algorithm, and the response time of host read request dominates the performance of the disk. Since RAID is composed of multiple disks, although the performance at the disk level is important to service requests, effectively the performance at the disk array level is more important. In RAID, a host request cannot be completed until all the striped requests are completed and the response time of the host request is dependent on the response time of the disk with the heaviest load. However, existing destage algorithms do not take into consideration the overall performance of all disks in the RAID but destage write requests for optimal performance at each individual disk, and it may eventually lead to overload of a few disks. It may delay the service of some striped requests, and therefore, the response time of host request increases. The paper suggests a novel Load-Balanced Destage (LBD) algorithm adopted at the disk array level, and shows that the LBD algorithm has a higher performance than existing destage algorithms by evaluating their performance using a simulator.
[Algorithm design and analysis, destage algorithms, cache storage, load balanced destage algorithm, LBD algorithm, Fault tolerance, storage management, resource allocation, Operating systems, destage write requests, Bandwidth, RAID 5 controller, host request, Redundant Arrays of Inexpensive Disks, disk cache, magnetic disc storage, write requests, simulator, Delay effects, Computational modeling, Redundancy, optimal performance, Read-write memory, performance evaluation, service requests, host read request, disk array level, response time, Hard disks, striped requests, disk performance]
Performance and cost analysis of mobile Internet multicast protocols
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
This paper presents performance analysis of two general approaches to IP multicasting in mobile environments. In the first approach, copies of each multicast datagram were forwarded to Mobility Agents in all wireless LANs on a campus. The other approach designated the Mobility Agent as a proxy (fixed host) for its local mobile hosts to participate in communication group, thus allowing current multicast protocols on the wired backbone to be easily applied. We develop an analytical model for quantifying their performance metrics. Performance is measured in terms of the latency required for group members, fixed and mobile hosts, to receive a datagram addressed to them, network bandwidth required to accomplish a single multicast. Numerical results show that each approach can outperform the other in certain circumstances.
[Measurement, wireless LANs, Wireless LAN, Costs, Spine, latency, performance evaluation, Mobile communication, Multicast protocols, multicast datagram, network bandwidth, Delay, communication group, Analytical models, performance metrics, Performance analysis, Internet, wireless LAN, protocols, cost analysis, performance analysis, IP multicasting, mobile Internet multicast protocols]
Multiprocessor scheduling algorithm utilizing linear clustering of directed acyclic graphs
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
The multiprocessor scheduling is the method to allocate modules(or tasks) with precedence relationship onto the processors in such a way that the parallel time (completion time of a program in multiprocessor system) is minimized. In this paper, we propose a scheduling heuristic which is based on linear clustering scheme and uses only local information of a DAG (Directed Acyclic Graph). Proposed algorithm gives an optimal scheduling result for a coarse grain DAG with only fork or join nodes and a greatly reduced parallel time for general DAGs. Simulation results showed that proposed algorithm has better scheduling result and less algorithm execution time than any of compared algorithms.
[optimal scheduling, Heuristic algorithms, scheduling heuristic, Optimal scheduling, Very large scale integration, digital simulation, completion time, linear clustering, Scheduling algorithm, processor scheduling, Multiprocessing systems, Processor scheduling, linear clustering scheme, directed graphs, Clustering algorithms, multiprocessor scheduling algorithm, simulation results, multiprocessor system, Cost function, Polynomials, Computational efficiency, directed acyclic graphs, precedence relationship]
ICOMA: an open infrastructure for agent-based intelligent electronic commerce on the Internet
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
With the increasing importance of EC (Electronic Commerce) across the Internet, the need for agents to support both customers and suppliers is growing rapidly. But the lack of standard on product ontology, message and negotiation protocol between agents and brokering makes full automation of EC infeasible. In this paper, we describe an open infrastructure for agent-based EC and design a virtual market server. As an open infrastructure, we propose a complete architecture and message protocol for inter-agent negotiation. We designed and partially implemented a virtual marker server, named ICOMA (Intelligent electronic COmmerce system based on Multi-Agent) based on the advanced agent technologies. The goal of ICOMA is to construct the decentralized, dynamic, and diverse EC environment.
[EFTS, Protocols, inter-agent negotiation, product ontology, Ontologies, agent-based intelligent electronic commerce, Information filtering, Electronic commerce, ICOMA, Intelligent agent, virtual market server, electronic data interchange, virtual marker server, Information filters, Software agents, Internet, protocols, Intelligent systems, Consumer electronics, open infrastructure, message protocol, negotiation protocol]
On the effectiveness of sectored caches in reducing false sharing misses
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
In this paper we study the effectiveness of sectored caches in reducing false sharing misses on bus-based multiprocessors. In a sectored cache, each cache line is divided into several subblocks. A subblock is a basic coherence unit. When false sharing occurs, the involved cache line needs not be invalidated or transferred, as long as the corresponding subblocks are kept coherent. To facilitate the study, we extend the conventional MESI protocol to sectored caches and define a performance metric called the degree of false sharing reduction to quantify the false sharing reduction on such caches. We simulated the execution of typical benchmarks, FFT, LU, Radix, SORBYR and SORBYC, on sectored caches. Evaluation results show that our scheme can effectively reduce about 30% to 80% false sharing misses and avoid useless coherence operations.
[Measurement, Protocols, Optimizing compilers, SORBYR, sectored caches, performance evaluation, LU, cache storage, false sharing misses, Delay, Computer science, Degradation, MESI protocol, performance metric, bus-based multiprocessors, Councils, FFT, Radix, Prototypes, SORBYC, shared memory systems, benchmarks, protocols, coherence unit]
Solution of path problems using associative parallel processors
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
In this paper we study a question of representing a group of associative algorithms on an abstract model of the SIMD type with vertical data processing (the STAR-machine). This group includes the Warshall transitive closure algorithm, the Floyd shortest path algorithm and the Maggs-Plotkin minimal spanning tree algorithm. These algorithms are represented as the corresponding STAR procedures whose correctness is verified and time complexity is evaluated.
[computational geometry, parallel processing, STAR procedures, Associative memory, associative parallel processors, abstract model, Dynamic programming, Performance analysis, vertical data processing, Digital audio players, correctness, SIMD type, Warshall transitive closure algorithm, time complexity, Data processing, Supercomputers, Equations, associative processing, Maggs-Plotkin minimal spanning tree algorithm, Automata, STAR-machine, Gaussian processes, path problems, Floyd shortest path algorithm, Arithmetic, computational complexity, associative algorithms]
An overview of IP switching and Tag switching
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Both IP switching and Tag switching were recently proposed to improve the performance of IP routers. They are all based on a multi-layer label-swapping mechanism, but their implementations are quite different. In this paper, we present an overview of both switching mechanisms, compare their key features, identify their constraints, and analyze the effect of these constraints on performance. Our study shows that both IP switching and Tag switching are better than the conventional IP routing, but neither of them is universally superior to the other.
[multi-layer label-swapping mechanism, IP routers, Packet switching, Protocols, Technical Activities Guide -TAG, Tag switching, Switches, performance evaluation, Routing, IP switching, Explosions, Proposals, Multiprotocol label switching, Computer science, switching mechanisms, performance, transport protocols, telecommunication network routing, Performance analysis, Internet, telecommunication traffic]
A fault tolerant protocol as an extension to a distributed mutual exclusion algorithm
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
In this paper, we present an extension to Kerry Raymond's tree based distributed mutual exclusion algorithm to make it fault tolerant. The algorithm uses a spanning tree of a computer network. With N nodes in the network the number of messages exchanged per critical section entry is typically O(log N). The novelty of the algorithm lies in each node holding information only about its immediate neighbors in the spanning tree. We basically present a protocol to make the algorithm tolerant to single node/link failure and associated network partition. This protocol assumes that the graph of the underlying network is biconnected. Basically we attempt reconfiguration of the tree after isolating the failed components. With this enhancement a significant number of cases in which the algorithm would have halted due to lack of communication paths, can be effectively handled by utilizing alternative communication paths now available, thus continuing to provide the mutual exclusion service.
[network partition, Protocols, Software algorithms, Drives, performance evaluation, Partitioning algorithms, Distributed computing, computer network, Fault tolerance, Tree graphs, distributed algorithms, fault tolerant protocol, spanning tree, communication paths, Computer networks, fault tolerant computing, protocols, Distributed algorithms, Protection, distributed mutual exclusion algorithm]
Constant time algorithms for computing the contour of maximal elements on the reconfigurable mesh
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
There has recently been an interest in the introduction of reconfigurable buses to existing parallel architectures. Among them Reconfigurable Mesh (RM) draws much attention because of its simplicity. This paper presents two O(1) time algorithms to compute the contour of the maximal elements of N planar points on the RM. The first algorithm employs an RM of size N/spl times/N while the second one uses a 3-D RM of size /spl radic/N/spl times//spl radic/N/spl times//spl radic/N.
[parallel architectures, reconfigurable buses, O(1) time algorithms, Switches, reconfigurable mesh, Parallel machines, computational geometry, Parallel architectures, Concurrent computing, Connectors, Computational geometry, maximal elements contours, reconfigurable architectures, constant time algorithms, Automata, Grid computing, Australia]
Mapping nested loops onto distributed memory multiprocessors
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
The paper presents Chain grouping; a new low complexity method for the problem of partitioning the index space into groups with little intercommunication requirements, for mapping onto distributed mesh connected architectures. First the loop iterations are scheduled in time, according to the hyperplane method, taking into consideration the minimum time displacement. Then, the index space is divided into discrete groups of related computations, which are assigned to different processors, while preserving the optimal makespan. The Chain grouping method is based on grouping along a uniform chain of computations, formed by a particular dependence vector. This vector will be proved as the best to reduce the total communication requirements. Inside every group, the optimal hyperplane scheduling is preserved, and the references to intragroup computations are considerably increased. The partitioned groups are afterwards assigned to meshes of processors. The resulting space mapping maximises processor utilisation and cuts down overall communication delays while preserving the optimal hyperplane time schedule.
[loop iterations, space mapping, low complexity method, dependence vector, parallel architectures, intragroup computations, processor utilisation, Chain grouping, processor scheduling, hyperplane method, Concurrent computing, optimal makespan, optimal hyperplane time schedule, Computer architecture, Parallel processing, Systolic arrays, minimum time displacement, intercommunication requirements, Delay effects, Vectors, distributed memory multiprocessors, uniform chain, Parallel architectures, index space partitioning, Scheduling algorithm, Computer science, nested loops, discrete groups, Processor scheduling, optimal hyperplane scheduling, distributed mesh connected architectures, distributed memory systems, partitioned groups, communication delays]
A multiprocessor scheduling heuristic for functional parallelism and its performance measure
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
This paper addresses the following scheduling problem: given a precedence graph with communication costs and a machine architecture with different types of processors, construct a schedule that runs on the given architecture at the minimum possible execution time. The main contributions are: Firstly, we present a static scheduling algorithm that keeps processors idle for future important tasks and fills idle time slots incurred due to interprocessor communication. Secondly, to evaluate the effectiveness of the algorithm, we develop a lower bound on the length of a optimal schedule as a yardstick. Experiments show that this new approach produces better schedules and takes much less compile time.
[machine architecture, Costs, Optimal scheduling, static scheduling algorithm, multiprocessor scheduling heuristic, idle time slots, lower bound, Scheduling algorithm, Machine intelligence, processor scheduling, parallel programming, Computer science, Concurrent computing, Processor scheduling, functional parallelism, performance measure, Information processing, Computer architecture, communication costs, interprocessor communication, Parallel processing, precedence graph, software performance evaluation]
A simulation study of interoperative methods for congestion control over TCP/ATM
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
ATM rate-based flow controls can effectively reduce cell losses and buffer requirement in ATM switches, and thus enhance overall performance in ATM networks. However, many simulation studies show that the performance of TCP can be reduced over ATM networks in some situation like network congestion. This comes from the absence of direct channel between TCP and ATM protocol stack. That is, the ATM layer in a host does not notify TCP of the congestion information from ATM networks through resource management (RM) cells. So, TCP may generate vast packets regardless of present network status. In this paper, we introduce the direct mechanism, interoperative method between TCP and ATM protocol stack and perform a simulation in order to justify our mechanism. The simulation results, by SIMNET from NIST, show that the TCP adopting mechanism has better performance over that only with ATM congestion control mechanism.
[resource management, ATM rate-based flow controls, Protocols, interoperative method, telecommunication congestion control, Switches, Throughput, asynchronous transfer mode, digital simulation, simulation study, buffer requirement, Packet switching, cell losses, Computational modeling, interoperative methods, performance evaluation, congestion control, SIMNET, Computer science, TCP/ATM, performance, transport protocols, NIST, Performance loss, Resource management, ATM switches, telecommunication traffic, Asynchronous transfer mode]
A fault-tolerant distributed algorithm for termination detection using roughly synchronized clocks
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
A fully symmetric and distributed solution to the termination detection problem is presented along with a proof of its correctness. Each of the processors in an asynchronous (or synchronous) network has a physical clock which is roughly synchronized with all other processors' physical clocks. The control messages that are timestamped using roughly synchronized clocks are circulated around an unidirectional (logical or physical) ring of all processors to detect termination of the underlying computation. The algorithm detects termination along with the correctness of the result of the underlying computation even in the situations when the processors fail and recover. The algorithm also works correctly even after the failure of clock synchronization. This algorithm involves considerably less number of messages to detect global termination in any network (synchronous or asynchronous) compared to other similar algorithms in the literature.
[Automation, program verification, global termination, physical clock, fault-tolerant distributed algorithm, Programming, termination detection, Synchronization, Distributed computing, correctness proof, synchronisation, Concurrent computing, Fault tolerance, Fault detection, Physics computing, control messages, distributed algorithms, roughly synchronized clocks, fault tolerant computing, Distributed algorithms, Clocks]
Efficient algorithms for prefix and general prefix computations on distributed shared memory systems with applications
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
The paper presents efficient scalable algorithms for performing prefix (PC) and general prefix (GPC) computations on a distributed shared memory, (DSM) system with applications. PC and GPC are generic techniques that can be used to design sequential and parallel algorithms for a number of problems from diverse areas (K. Arvind et al., 1995; V. Kamakoti and C. Pandurangan, 1992).
[Algorithm design and analysis, parallel algorithms, general prefix computations, Scalability, Phase change random access memory, Supercomputers, Distributed computing, Parallel algorithms, Programming profession, Concurrent computing, distributed memory systems, Parallel processing, distributed shared memory systems, generic techniques, Software systems, shared memory systems, prefix computations, GPC, DSM, PC computations]
A performance modeling technique for mesh-connected multicomputers
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
Modeling the perfomance of space-shared multicomputers is a non-trivial task mainly due to difficulty in modeling the effect of external fragmentation on system performance. Mesh-connected multicomputers are hard to model in particular because of great variance in job sizes. Therefore, researchers have relied on simulation method to evaluate the mesh performance. We propose a novel modeling technique called hybrid method in this paper. The proposed technique utilizes simulation method to estimate the capacity of a system. Then, a queueing model with multiple servers is constructed using the system capacity as the number of servers in the queueing system. The technique is validated through simulation experiments. The results reveal that the hybrid method provides very close estimation of the mesh performance with very little overhead. The proposed technique can also be used for performance modeling of other multicomputers with different topologies.
[hybrid method, Very large scale integration, system capacity, Electronic mail, processor scheduling, mesh-connected multicomputers, Technology management, Operating systems, performance modeling, shared memory systems, queueing model, performance modeling technique, external fragmentation, queueing theory, multiple servers, simulation method, performance evaluation, Dynamic scheduling, Job design, Topology, simulation experiments, Computer science, Processor scheduling, system performance, space-shared multicomputers, Resource management, mesh performance]
A parallel Cholesky factorization routine with a new version of PB-BLAS
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
PB-BLAS (Parallel Block Basic linear Algebra Subprograms) is the first parallel implementation of BLAS, whose data are decomposed based on a block cyclic data distribution. It is functionally an extended subset of the Level 2 and 3 BLAS for distributed-memory systems. The authors present a new modified version of PB-BLAS, which eliminated the positional restrictions of the data matrices from the old version. And it is more efficient in memory space management as well as in performance for PB-BLAS routines which handle a triangular, symmetric, or Hermitian matrix. The paper outlines PB-BLAS and ScaLAPACK, which is a library of high performance linear algebra routines for distributed-memory computers. And it describes a parallel Cholesky factorization routine, and shows the performance with the old and the new version of PB-BLAS on the Intel Paragon computer.
[Software maintenance, data decomposition, Intel Paragon computer, memory space management, Hermitian matrix, high performance linear algebra routine library, Hermitian matrices, Distributed computing, software libraries, parallel programming, Parallel Block Basic linear Algebra Subprograms, Concurrent computing, storage management, symmetric matrix, distributed-memory systems, ScaLAPACK, Libraries, PB-BLAS, software performance evaluation, Symmetric matrices, Vectors, triangular matrix, High performance computing, Message passing, Memory management, block cyclic data distribution, Linear algebra, distributed memory systems, PB-BLAS routine performance, parallel Cholesky factorization routine]
I/O and memory-efficient matrix multiplication with user-controllable parallel I/O
Proceedings 1997 International Conference on Parallel and Distributed Systems
None
1997
The UPIO (user-controllable parallel I/O) proposed by the authors in 1996 allows users to determine a file's structure by considering the access patterns of particular applications and the distribution of data for parallel access, and them do I/O collectively. This enables users to produce high-performance external computation codes by planning I/O, computations, communication, and the reuse of data effectively in the codes. They show how well UPIO produces high performance external computation codes by designing I/O and memory-efficient external matrix multiplication algorithms and exploring the effects of UPIO with the codes.
[codes, algorithms, data distribution, memory-efficient matrix multiplication, parallel access, high-performance external computation codes, high performance external computation codes, input-output programs, user-controllable parallel I/O, data reuse, Distributed computing, parallel processing, Concurrent computing, Degradation, File systems, Control system synthesis, file structure, I/O planning, computation planning, UPIO, access patterns, communication planning, Programming profession, Computer languages, matrix multiplication, Shape control, file organisation]
Congestion-free embedding of multiple spanning trees in an arrangement graph
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
The arrangement graph A/sub n,k/ is a generalization of star graph (n-k=1) and more flexible than the star graph. In this paper we consider the embedding of multiple spanning trees in an arrangement graph with the objective of being congestion-free. This is first result to exploit multiple spanning trees in the arrangement graphs. We develop a congestion-free embedding of n-k spanning trees with height 2k-1 in an (n, k)-dimensional arrangement graph.
[Algorithm design and analysis, multiprocessing systems, multiprocessor interconnection networks, Routing, Topology, Parallel algorithms, Computer science, Fault tolerance, Tree graphs, multiple spanning trees, star graph, Broadcasting, Hypercubes, Argon, congestion-free embedding, arrangement graph]
On the stability of a distributed dynamic load balancing algorithm
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
We present a new fully distributed dynamic load balancing algorithm called DASUD (Diffusion Algorithm Searching Unbalanced Domains). Since DASUD is iterative and runs in an asynchronous way, a mathematical model that describes DASUD behaviour has been proposed and has been used to prove DASUD's convergence. DASUD has been evaluated by comparison with another well known strategy from the literature, namely, the SID (Sender Initiated Diffusion) algorithm. The comparison was carried out by considering a large set of load distributions which were applied to ring, torus and hypercube topologies, and the number of processors ranged from 8 to 128. From these experiments we have observed that DASUD outperforms the SID strategy as it provides the best trade-off between the global balance degree obtained at the final state and the number of iterations required to reach such a state.
[Heuristic algorithms, parallel architectures, graph theory, multiprocessor interconnection networks, Sender Initiated Diffusion algorithm, Mathematics, SID strategy, Postal services, Convergence, load distributions, resource allocation, Diffusion Algorithm Searching Unbalanced Domains, Mathematical model, global balance degree, Identity-based encryption, message passing, Stability, torus, DASUD behaviour, hypercube topologies, Computer science, distributed dynamic load balancing algorithm, distributed algorithms, mathematical model, Load management]
Flat indexing: a compilation technique to enhance parallelism of logic programs
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
The paper presents a systematic approach to the compilation of logic programs for efficient clause indexing. As the kernel of the approach, we propose the indexing tree which provides a simple, but precise representation of average parallelism per node (i.e., choice point) as well as the amount of clause trials. It also provides the way to evaluate the number of the cases that the control is passed to the failure code by the indexing instruction such as switch on term, switch on constant, or switch on structure. By analyzing the indexing tree created when using the indexing scheme implemented in the WAM, we show the drawback of the WAM indexing scheme in terms of parallelism exposition and scheduling. Subsequently we propose a new indexing scheme, which we call Flat indexing. Experimental results show that over one half of the benchmarks benefit from the Flat indexing, such that compared with the WAM indexing scheme, the number of choice points is reduced by 15%. Moreover, the amount of failures which occur during the execution of indexing instructions is reduced by 35%.
[logic programs, Terminology, WAM indexing scheme, program compilers, parallel programming, flat indexing, Concurrent computing, Runtime, indexing tree, indexing instructions, indexing instruction, Parallel processing, logic programming, clause indexing, scheduling, tree data structures, Logic, trees (mathematics), compilation technique, Warren Abstract Machine, failure code, average parallelism, clause trials, parallelism exposition, Indexing]
Path-based multicast communication in wormhole-routed star graph multicomputers
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
In this paper, we propose four efficient multicast routing schemes in wormhole-routed star networks with multidestination routing capability. All of the four proposed schemes are path-based and deadlock-free. The first scheme, dual-path routing, sends the message in parallel through two independent paths. The second one, shortcut-node-based datapath routing, is similar to dual-path routing except that the routing tries to find a shortcut node to route the message as soon as possible to reduce the length of transmission path. The third one, multipath routing, is a multiple dual-path routing strategy that includes source-to-relay and relay-to-destination phases. The last scheme, proximity grouping routing, is similar to multipath routing except that in the partitioning step of source and destination nodes the relation of spatial locality of nodes is also taken into account to reduce the length of transmission paths. Finally, the experimental results are given to show that the performance based on unicast-based and traditional Hamiltonian-path routing schemes can be improved significantly by the four proposed routing schemes respectively.
[spatial node locality, Hamiltonian path routing schemes, Multiprocessor interconnection networks, multiprocessor interconnection networks, Multicast communication, deadlock-free scheme, Information management, Relays, source-to-relay phases, dual-path routing, multicast communication, multicast routing schemes, Hypercubes, transmission path, parallel message sending, network routing, proximity grouping routing, destination nodes, Routing, shortcut-node-based datapath routing, source nodes, partitioning step, Communication switching, Computer science, multidestination routing capability, Multicast algorithms, path-based scheme, relay-to-destination phases, unicast-based routing schemes, distributed memory systems, multiple dual-path routing strategy, System recovery, multipath routing, wormhole-routed star graph multicomputers, transmission path length, path-based multicast communication]
A certification protocol with low space overhead
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
The performance of optimistic concurrency control schemes is sensitive to the transaction abort rate. The abort probability can be reduced by reordering in order to reduce read-write conflicts. However, a potential drawback of reordering schemes is the space overhead in maintaining timestamps, multiple versions or a serialization graph. Furthermore, some transactions in optimistic concurrency control schemes may perform unnecessary operations even after the transactions have accessed write-write conflicting data items, because aborts happen only in the validation phase. In this paper, we propose a new broadcast scheme called BOCC-RS (Broadcast Optimistic Concurrency Control with Re-ordering Scheme) that can reduce the abort probability with low space overhead. In our scheme, the server maintains only one timestamp per data item for reordering. Moreover, our scheme reduces unnecessary operations. This paper presents a simulation study which shows that our scheme outperforms BOCC.
[transaction processing, space overhead, Protocols, Costs, broadcast optimistic concurrency control scheme, Optimization methods, simulation, read-write conflicts, abort probability, validation phase, Concurrent computing, Information science, transaction abort rate, Broadcasting, BOCC-RS, write-write conflicting data items, serialization graph maintenance, protocols, certification protocol, probability, unnecessary operations, Concurrency control, multiple version maintenance, Certification, broadcasting, certification, database theory, Computer science, configuration management, timestamp maintenance, performance, concurrency control, reordering schemes]
Agent-based forwarding strategies for reducing location management cost in mobile networks
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
A major issue with location management in mobile wireless networks is the high cost associated with location updates and searches. A search occurs when a caller needs to connect to a mobile user whose location is unknown to the caller and the network must locate the callee. An update occurs when a mobile user moves into a new registration area and the network is informed of the location change. We propose and analyze a class of new agent-based forwarding strategies with the objective to reduce the location management cost in mobile wireless networks. We develop analytical Markov models to compare the performance of the proposed strategies with existing location management schemes to demonstrate their feasibility and also to reveal conditions under which the proposed schemes are superior to existing ones.
[Costs, Wireless communication, location updates, Intelligent networks, search, Databases, Wireless networks, agent-based forwarding strategies, Performance analysis, personal communication networks, performance evaluation, wireless networks, mobile networks, software agents, Computer science, mobile communication, mobile user, performance, Markov processes, Markov models, Computer network management, Personal communication networks, wireless LAN, location management cost, Mobile computing]
Symbolic partitioning and scheduling of parameterized task graphs
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
The DAG based task graph model has been found effective in scheduling for performance prediction and optimization of parallel applications. However the scheduling complexity and solution normally depend on the problem size. We propose a symbolic scheduling scheme for a parameterized task graph which models coarse grain DAG parallelism, independent of the problem size. The algorithm first derives symbolic clusters to a group of tasks in order to minimize communication while preserving parallelism, and then it evenly assigns task clusters to processors. The run time system executes clusters on each processor in a multithreaded fashion. The paper also presents preliminary experimental results to demonstrate the effectiveness of our techniques.
[Algorithm design and analysis, problem size, World Wide Web, symbolic partitioning, parameterized task graphs, Electrical capacitance tomography, multithreaded fashion, Delay, processor scheduling, Concurrent computing, Runtime, directed acyclic dependence graphs, Clustering algorithms, Parallel processing, symbolic clusters, coarse grain DAG parallelism, symbolic scheduling scheme, parallel algorithms, multi-threading, task clusters, parallel applications, Scheduling algorithm, performance prediction, communication minimisation, Processor scheduling, directed graphs, scheduling complexity, computational complexity, DAG based task graph model, run time system]
A model of mobile agent services enhanced for resource restrictions and security
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
Mobile agent technologies are becoming popular as an efficient way to access network resources. Because an application using mobile agents has some unique problems caused by frequent creation, migration and disappearance of mobile agents, a mobile agent platform has to provide not only agents and their execution engines but some functions and mechanisms which are specialized to mobile agent systems. We discuss the practical problems occurring in mobile agent environments, that is, agent controllability, resource restrictions and security. To solve these problems, we propose a mobile agent platform, called SFM (Secured Floating Market) model. This model fulfils the agent controllability by agent control parameters and has the measures against the resource restrictions such as location of resources and agents, processing capability and condition of load. Besides, this model guarantees some suitable security strength levels for flexible execution of various services and user requirements. We implement the prototype of this model using Aglets.
[telecommunication security, computer networks, Security, Application software, Electronic commerce, software agents, Engines, Network servers, mobile agent services, security, SFM model, resource allocation, security of data, user requirements, Mobile agents, Prototypes, Aglets, network resources, Controllability, Load management, Computer networks, resource restrictions, agent controllability, Secured Floating Market model]
Efficient address generation for affine subscripts in data-parallel programs
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
This paper presents an efficient compilation technique to generate the local memory access sequences for block-cyclically distributed array references with affine subscripts in data-parallel programs. For the memory accesses of an array reference with affine subscript within a two-nested loop, there exist repetitive patterns both at the outer and inner loops. We use tables to record the memory accesses of repetitive patterns. According to these tables, a new start-computation algorithm is proposed to compute the starting elements on a processor for each outer loop iteration. The complexities of the table constructions are O(k+s/sub 2/), where k is the distribution block size and s/sub 2/ is the access stride for the inner loop. After tables are constructed, generating each starting element for each outer loop iteration can run in O(1) time. Moreover, we also show that the repetitive iterations for outer loop are Pk/gcd(Pk,s/sub 1/), where P is the number of processors and s/sub 1/ is the access stride for the outer loop. Therefore, the total complexity to generate the local memory access sequences for a block-cyclically distributed array with affine subscript in a two-nested loop is O(Pk/gcd/(Pk,s/sub 1/)+k+s/sub 2/).
[complexity, repetitive patterns, Data engineering, loop iteration, Electronic mail, Distributed computing, program compilers, affine subscripts, memory access, parallel programming, address generation, Concurrent computing, access stride, Program processors, tables, data structures, starting element, program control structures, local memory access sequences, Induction generators, data-parallel programs, Educational institutions, compilation technique, Programming profession, two-nested loop, Computer science, Parallel programming, array reference, distributed memory systems, start-computation algorithm, computational complexity, block-cyclically distributed array references]
Traffic management for wireless ATM networks
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
To support multimedia traffic consisting of diverse traffic classes for mobile devices, a traffic management scheme should be developed to provide a seamless wireless ATM based communication system. We propose a novel traffic management scheme based on the Packet Reservation Multiple Access (PRMA) protocol to support QoS guarantee for downlink traffic. The proposed traffic management scheme involves cell scheduling, buffer management, traffic shaping, traffic control, and flow control. The simulation results reveal that the proposed scheme can guarantee QoS (cell delay and cell loss ratio) for both real time (CBR, VBR) and non real time (ABR) traffic in the wireless ATM networks, reduce the buffer size in the Base Station (BS), and enhance the utilization of wireless bandwidth.
[seamless wireless ATM based communication system, PRMA protocol, telecommunication congestion control, Telecommunication traffic, Downlink, asynchronous transfer mode, traffic management, downlink traffic, buffer size, Wireless communication, multimedia traffic, traffic shaping, QoS, traffic control, cell delay, Traffic control, Base Station, Communication system traffic control, Packet Reservation Multiple Access, Base stations, Multimedia systems, Delay effects, diverse traffic classes, Access protocols, cell scheduling, quality of service, wireless ATM networks, QoS guarantee, flow control, Shape control, computer network management, buffer management, mobile devices, cell loss ratio, traffic management scheme, wireless LAN, packet reservation multiple access, wireless bandwidth]
On reconfiguring query execution plans in distributed object-relational DBMS
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
Massive database sizes and growing demands for decision support and data mining result in long-running queries in extensible object-relational DBMSs, particularly in decision support and data warehousing analysis applications. Parallelization of query evaluation is often required for acceptable performance, yet queries are frequently processed suboptimally due to (1) only coarse or inaccurate estimates of the query characteristics and database statistics being available prior to query evaluation; (2) changes in system configuration and resource availability during query evaluation. In a distributed environment, dynamically reconfiguring query execution plans (QEPs), which adapts QEPs to the environment as well as to the query characteristics, is a promising means to significantly improve query evaluation performance. Based on an operator classification, we propose an algorithm to coordinate the steps in a reconfiguration and introduce alternatives for execution context checkpointing and restoring. A syntactic extension of SQL to expose the relevant characteristics of user-defined functions in support of dynamic reconfiguration is proposed. An example from the experimental system is presented.
[user-defined functions, distributed object-relational DBMS, Laboratories, Optimization methods, data mining, parallel query evaluation, operator classification, execution context restoration, data warehousing, resource availability, decision support, Data mining, parallel processing, query processing, Databases, execution context checkpointing, Statistical distributions, distributed databases, database statistics, Parallel processing, dynamic reconfiguration, Availability, Data analysis, query evaluation performance, object-oriented databases, system configuration, long-running queries, query execution plan reconfiguration, query characteristics, relational databases, SQL, Computer science, database size, SQL syntactic extension, Query processing, data warehouses]
Reusing MS-Windows software applications under CORBA environment
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
CORBA is becoming the most important middleware that supports object oriented and client/server paradigm in distributed computing systems. However the application systems based on CORBA are still scarce to date. One main reason is that only few CORBA object services have been developed. Without added help mechanisms, to have a new CORBA application, a programmer should make efforts to design a program with a CORBA interface from scratch. In our previous work (K. Liang et al., 1997), a reengineering approach was proposed to convert RPC based programs to CORBA objects, which successfully speeded up the development of CORBA applications. However the source code is required in this approach. In many cases, software designers may not be able to get hold of the source code, so it is necessary to adapt existing PC software applications, particularly for Windows applications, in binary code mode to the object services under CORBA. Our study addresses this problem. A graphic factory temperature monitor system, which integrates MS-Excel under MS-Windows has been implemented to demonstrate the feasibility of our approach.
[reengineering approach, CORBA object services, Windows applications, Production facilities, Distributed computing, Temperature sensors, graphic factory temperature monitor system, CORBA environment, Software design, binary code mode, Binary codes, client/server paradigm, distributed object management, distributed programming, CORBA application, MS-Excel, middleware, client-server systems, MS-Windows software application reuse, object-oriented programming, RPC based programs, source code, CORBA interface, Application software, Middleware, Programming profession, Graphics, Temperature measurement, distributed computing systems, PC software applications, software reusability]
Modeling the static and dynamic guard channel schemes for mobile transactions
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
There are more and more information services provided on the wireless network. Due to long network delay of wireless link, the transaction will be a long lived transaction. In such a situation, the occurrence of handoff is inevitable, and thus a wireless link held by the mobile unit crossing the cell boundaries might be forced to terminate. It is undesirable that an active transaction is forced to terminate. We propose two guard channel schemes (GCS), static and dynamic, to reduce the probability of forced termination of transactions. In dynamic GCS, the number of reserved channels of a base station is dynamically assigned according to the number of transaction calls which may handoff to this cell while the number of guard channels is fixed in the static GCS. An analytic model based on Markov chain is derived to evaluate the system performance. The correctness of this model is verified by a simulation program. The experimental results show that a significant improvement is achieved by using the dynamic GCS.
[transaction processing, mobile transactions, Portable computers, guard channel schemes, Mobile communication, Electronic switching systems, Wire, Markov chain, Network servers, mobile computing, reserved channels, active transaction, Computer architecture, wireless link, wireless network, base station, forced transaction termination, Base stations, mobile radio, long lived transaction, simulation program, dynamic GCS, dynamic guard channel schemes, handoff, information services, network delay, Land mobile radio cellular systems, transaction call, mobile unit, system performance, Personal communication networks, cell boundaries, Mobile computing]
A cost and performance comparison for wormhole routers based on HDL designs
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
Our research investigates cost and performance characteristics for wormhole routers based on HDL designs. Comparison for dimension order routers and turn model based adaptive routers leads to the following conclusions: (1) static and additional routing information which we propose, such as prior dimension specification and in-order delivery, improves the communication performance; (2) an adaptive routing algorithm must be implemented to satisfy the objective speed of the design (the operation speed of the routers significantly affects the network performance); (3) the virtual channels cancel the improvement not only for the dimension order router but also for the naive implementation of the adaptive routers when they degrade the operation speed.
[Costs, Adaptive systems, Virtual colonoscopy, multiprocessor interconnection networks, in-order delivery, hardware description languages, objective speed, Concurrent computing, Degradation, Network topology, network performance, turn model based adaptive routers, performance characteristics, operation speed, virtual channels, naive implementation, performance evaluation, Routing, Application software, communication performance, adaptive routing algorithm, dimension order routers, prior dimension specification, wormhole routers, performance comparison, HDL designs, Hardware design languages, Clocks]
User-controllable parallel I/O for scientific applications
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
We propose user controllable I/O operations for scientific applications in parallel computing environments and explore the effects of them with some synthetic access patterns. The operations allow users to present numerous access patterns collectively with a minimum number of I/O requests. This enables systems to aggregate bandwidth of parallel disks, optimize disk head movements, and then increase I/O performance.
[disk head movements, synthetic access patterns, Control systems, Data engineering, I/O performance, input-output programs, user interfaces, scientific applications, Application software, parallel disks, Distributed computing, Sun, parallel programming, Concurrent computing, Shape control, parallel computing environments, Aggregates, user controllable parallel I/O, Bandwidth, I/O requests, Parallel processing, bandwidth aggregation, natural sciences computing, user controllable I/O operations]
Cluster queue structure for shared-memory multiprocessor systems
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
Three basic organizations have been proposed to construct the task queues for a shared memory multiprocessor system: centralized, distributed, and hierarchical structures. The centralized structure is not suitable for massively parallel systems since the centralized queue is a bottleneck. The distributed structure, on the other hand, will end up with load imbalancing due to the random execution time of tasks. The hierarchical structure tends to combine the advantages of the previous two structures and reduces the impact of bottleneck and load imbalancing. However, we found out that the load imbalancing still exists in the hierarchical structure. It also affects system performance, in particular, when the workload becomes heavier. We further identified the cause of this problem. We propose the use of a forest structure to provide load balancing and contention minimization.
[forest structure, Scalability, distributed structure, cluster queue structure, massively parallel systems, processor scheduling, Multiprocessing systems, Analytical models, resource allocation, System performance, Prototypes, Parallel processing, shared memory systems, Load modeling, queueing theory, hierarchical structure, contention minimization, random execution time, workload, shared memory multiprocessor system, load imbalancing, Computer science, centralized structure, task queues, system performance, Load management]
High-performance external stencil computations using user-controllable I/O
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
The UPIO (User-controllable Parallel I/O) we proposed in (Lee et al., 1996) extends the abstraction of a linear file model into an n-dimensional file model and removes the limitations of the linear file model, allowing users to control the layout of data blocks across disks and aggregating disk bandwidth through UPIO's interfaces. This enables users to plan I/Os, computations, communications, and data reuse effectively in many scientific and engineering applications. These applications can be characterized by the corresponding stencil and implemented using stencil algorithms. We show how well UPIO produces high-performance external stencil codes by designing I/O and communication-efficient external Laplace equation solver algorithms and exploring the effects of UPIO with the codes.
[Algorithm design and analysis, mathematics computing, Communication system control, Data engineering, Control systems, input-output programs, data reuse, parallel processing, stencil algorithms, Concurrent computing, communications, external stencil computations, Bandwidth, n-dimensional file model, data blocks, Laplace equation solver algorithms, software performance evaluation, Laplace equations, computations, UPIO, disk bandwidth, high-performance computations, Application software, Sun, linear file model, Computer applications, user-controllable parallel input output]
Contention-free communication scheduling for array redistribution
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
Array redistribution is required often in programs on distributed memory parallel computers. It is essential to use efficient algorithms for redistribution, otherwise the performance of the programs may degrade considerably. The redistribution overheads consist of two parts: index computation and interprocessor communication. If there is no communication scheduling in a redistribution algorithm, the communication contention may occur, which increases the communication waiting time. In order to solve this problem, we propose a technique to schedule the communication so that it becomes contention-free. Our approach initially generates a communication table to represent the communication relations among sending nodes and receiving nodes. According to the communication table, we then generate another table named communication scheduling table. Each column of the communication scheduling table is a permutation of receiving node numbers in each communication step. Thus the communications in our redistribution algorithm are contention-free. Our approach can deal with multi-dimensional shape changing redistribution.
[parallelising compilers, communication scheduling table, Shape, parallelizing compiler, Distributed computing, parallel machines, communication waiting time, Scheduling algorithm, parallel programming, index computation, redistribution overheads, Concurrent computing, Degradation, communication table, Processor scheduling, performance, concurrency control, distributed memory systems, interprocessor communication, scheduling, distributed memory parallel computers, array redistribution, contention-free communication scheduling, software performance evaluation]
Decoding unit with high issue rate for x86 superscalar microprocessors
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
In the new generation of x86 microprocessors, superscalar techniques are used to achieve higher performance by executing multiple instructions in parallel. For compatibility and higher execution parallelism, the decoding units of these microprocessors translate the x86 instructions into primitive operations. These microprocessors translate x86 instructions by the similar way of merging address generating into load/store operations. We develop a new translation strategy of translating isolated address generation operations. Simulation results show that, in high issue rate decoding units, translating isolated address generation operations improves the performance for 20% to 25%. Besides, we find that enhancing the store buffer with the ability of snooping result buses is important for high issue rate decoding units. Furthermore, considering the tradeoff of the hardware cost and performance, we examine the decoding rules to design a decoding unit. According to the simulation results, we suggest a good decoding rule suitable for current commercial programs.
[Costs, Buffer storage, Merging, Pipelines, x86 superscalar microprocessors, simulation, isolated address generation operations, Electronic switching systems, parallel programming, Postal services, microprogramming, Microwave integrated circuits, multiple instructions, Microprocessors, instruction set, decoding unit, load store operations, instruction sets, buffer storage, merging, high issue rate, store buffer, performance evaluation, microprocessor chips, Decoding, AC generators, decoding, performance, compatibility]
An intelligent policing-routing mechanism based on fuzzy logic and genetic algorithms
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
The Asynchronous Transfer Mode (ATM) has been standardized and widely accepted as a technique to support future B-ISDN networks. In ATM networks, traffic control design becomes an important challenge, because of the diverse services support and the need for efficient network resource engineering. Two important functions for traffic control in ATM networks are policing and routing. The goal of these two functions is to guarantee the required quality of service. So far, all studies have treated policing and routing in a separate way. The combination of policing and routing can guarantee a better quality of service and increase the network utilization. Traditional network control strategies are not well suited for high speed networks. Use of intelligent algorithms based on fuzzy logic, neural networks and genetic algorithms can prove to be efficient for traffic control in ATM networks. We propose an intelligent policing-routing mechanism based on fuzzy logic and genetic algorithms. Performance evaluation via simulations shows that the proposed mechanism performs better than conventional policing mechanisms and routing algorithms.
[network utilization, telecommunication congestion control, Quality of service, neural networks, asynchronous transfer mode, traffic control design, telecommunication computing, network resource engineering, Genetic algorithms, Asynchronous Transfer Mode, fuzzy control, routing algorithms, Design engineering, Intelligent networks, High-speed networks, intelligent algorithms, Traffic control, network control strategies, high speed networks, future B-ISDN networks, B-ISDN, fuzzy logic, performance evaluation, Routing, quality of service, genetic algorithms, intelligent policing-routing mechanism, Fuzzy logic, telecommunication network routing, neural nets, Asynchronous transfer mode, ATM networks]
An optimal routing scheme for multiple broadcast
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
The dynamic broadcast problem is the communication problem where source packets to be broadcast to all the other nodes are generated at each node of a parallel computer according to a certain random process, such as a Poisson process. The lower bounds on the average reception delay required by any oblivious dynamic broadcast algorithm in a d-dimensional hypercube are /spl Omega/(d+1/1-/spl rho/) when packets are generated according to a Poisson process, where p is the load factor. The best previous algorithms for hypercubes only achieve /spl Omega/(d/1-/spl rho/) average reception delay. In this paper, we propose dynamic broadcast algorithms that require optimal O(d+1/1-/spl rho/) average reception delay in d-dimensional hypercubes and n/sub 1//spl times/n/sub 2//spl times//spl middot//spl middot//spl middot/n/sub d/ tori with n/sub i/=O(1). We apply the proposed broadcast scheme to a variety of other network topologies for efficient dynamic broadcast and present several methods for assigning priority classes to packets.
[dynamic broadcast algorithms, dynamic broadcast problem, priority class assignment, hypercube networks, Random processes, communication complexity, source packets, parallel machines, Poisson process, parallel computer, Concurrent computing, random process, average reception delay lower bounds, Network topology, efficient dynamic broadcast, Broadcasting, Parallel processing, Hypercubes, Computer networks, load factor, Mesh generation, oblivious dynamic broadcast algorithm, optimal routing scheme, parallel algorithms, Delay effects, network routing, random processes, communication problem, Routing, multiple broadcast, broadcasting, delays, network topologies, d-dimensional hypercube, average reception delay, optimal average reception delay]
Data duplication and consistency in a mobile, multidatabase environment
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
As technological advances are made in software and hardware, the feasibility of accessing information "anytime, anywhere" is becoming a reality. There are similarities involved in accessing information in a multidatabase environment and a mobile computing environment. We have discussed the characteristics of a wireless-mobile computing environment superimposed on a multidatabase system to realize a system capable of effectively accessing a large amount of data over a wireless medium. In order to limit the effects of the restrictions imposed by a mobile environment, data duplication is used at the mobile unit. The duplicated data at the mobile node provide additional availability in case of a weak connection or disconnection. In such an environment, it is important to retain the consistency between the data residing on fixed nodes and mobile nodes. Current research has concentrated on page- and file-based caching or replication schemes to address the availability and consistency issues in a mobile environment. In a mobile multidatabase environment, local autonomy restrictions prevent the use of a page- or file-based data duplication scheme. We propose a new data duplication scheme to address the limited bandwidth and local autonomy restrictions in such an environment. Queries and their associated data are cached at the mobile unit as a complete object. Consistency is maintained by using a parity-based invalidation scheme. Additionally, a simple prefetching scheme is used in conjunction with caching to further improve effectiveness of the proposed scheme.
[data duplication, Military computing, limited bandwidth, Mobile communication, Electronic switching systems, cache storage, availability, query processing, mobile computing, local autonomy restrictions, parity-based invalidation scheme, information access, Database systems, Hardware, mobile nodes, disconnection, replicated databases, file-based caching, Concurrency control, data integrity, queries, prefetching scheme, data consistency, wireless-mobile computing environment, Computer science, fixed nodes, mobile multidatabase environment, replication schemes, User interfaces, weak connection, page-based caching]
Toward supporting data parallel programming on clusters of symmetric multiprocessors
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
The paper reports the design of a runtime library for data-parallel programming on clusters of symmetric multiprocessors (SMP clusters). Our design algorithms exploit a hybrid methodology which maps directly to the underlying hierarchical memory system in SMP clusters, by combining two styles of programming methodologies-threads (shared memory programming) within a SMP node and message passing between SMP nodes. This hybrid approach has been used in the implementation of a library for collective communications. The prototype library is implemented based on standard interfaces for threads (pthread) and message passing (MPI). Experimental results on a cluster of Sun UltraSparc-II workstations are reported.
[shared memory programming, programming methodologies, prototype library, collective communication, MPI, hierarchical memory system, runtime library, Concurrent computing, Clustering algorithms, SMP node, shared memory systems, Hardware, Workstations, hybrid approach, pthread, multiprocessing systems, message passing, Sun UltraSparc-II workstations, multi-threading, standard interfaces, SMP clusters, Application software, symmetric multiprocessor clusters, Sun, hybrid methodology, Computer science, data parallel programming, Parallel programming, Message passing, High performance computing, design algorithms]
Interval algebra for spatio-temporal composition of distributed multimedia objects
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
Relations among temporal intervals can be used to model time dependent objects. The extension of temporal relation results in many researches related to the spatio-temporal modeling of distributed objects. We propose a fast mechanism for temporal relation compositions. A temporal transitive closure table is derived, and an interval-based temporal relation algebraic system is constructed. Thus, we propagate the time constraints of arbitrary two objects across long distances n by linear time. A set of algorithms is proposed to derive reasonable relations between intervals. Possible conflicts in the user specification are detected and eliminated. The algorithms are extended for time-based media in an arbitrary n-dimensional space. The contributions of these algorithms and the interval algebra system can be used to detect and eliminate time conflicts, to generate the schedule and layout of multimedia presentations, to model interval-based synchronization specifications, to analyze the distributed real-time playout, to denote the semantics of natural language sentence tense, and to capture the temporal semantics among distributed multimedia objects.
[interval algebra, algorithms, multimedia presentation schedule, time constraints, arbitrary n-dimensional space, computational linguistics, time dependent object modelling, interval-based temporal relation algebraic system, Electronic mail, semantics, multimedia computing, multimedia presentation layout, processor scheduling, user specification, spatio-temporal composition, distributed multimedia objects, time conflicts, Application specific integrated circuits, Algebra, time-based media, natural language sentence tense, distributed real-time playout, Robots, distributed object management, temporal intervals, Multimedia systems, temporal transitive closure table, Scheduling algorithm, synchronisation, Computer science, Integrated circuit layout, process algebra, temporal semantics, spatio-temporal modeling, Streaming media, Integrated circuit modeling, interval-based synchronization specifications, temporal relation compositions]
A scheme for high-performance data delivery in the Web environment
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
The paper describes a scheme for high performance and dependable data storage and delivery in a large scale distributed computing and communication environment such as the Web environment. The proposed scheme utilizes the parallelism of several distributed data servers storing striped data blocks to achieve high throughput. It employs coding techniques to protect the system against data unavailability and hence achieve dependable service. The performance results show that the proposed method has several advantages over traditional ones, such as data service through mirror sites. The error probability of the proposed method is orders of magnitude smaller than that of the mirroring with the same redundancy rate. The data rates for file downloading could be improved significantly by the proposed scheme.
[mirror sites, dependable service, Memory, dependable data storage, high performance data delivery, Delay, Network servers, resource allocation, data unavailability, file downloading, coding techniques, Hardware, Mirrors, Protection, Availability, communication environment, replicated databases, error probability, Maintenance, distributed data servers, encoding, data rates, Computer science, striped data blocks, Web environment, redundancy rate, fault tolerant computing, Internet, large scale distributed computing]
Proposal and verification of a workflow coordination model for core business
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
As corporations continue to introduce workflow systems, different systems have appeared for processing of core-business tasks for information processing and clerical work. A number of different models for coordination of different workflow systems have been proposed; but workflows for core-business tasks constitute the central business processes of the company, and certain aspects of coordination of such workflows differ from other kinds of coordination. This paper clarifies these differences, and proposes a model for coordination, which takes these differences into consideration. In addition the application of this model to an actual system and confirmation of its effectiveness are also reported.
[information processing, clerical work, Collaborative software, Companies, Proposals, workflow coordination model proposal, corporations, Business communication, Databases, formal verification, workflow coordination model verification, Management information systems, groupware, core business, workflow systems, Systems engineering and theory, Collaborative work, Marketing and sales, Manufacturing, workflow management software, business data processing]
Adaptive and fault-tolerant routing with 100% node utilization for mesh multicomputer
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
We propose an adaptive and deadlock-free routing algorithm to tolerate irregular faulty patterns using two virtual channels per physical link. It can improve the node utilization up to 100%. When a node becomes faulty or recovered, the central control unit constructs a directed path graph which is used for generating the intermediate nodes of the message path. Thus a message can be transmitted from sources or to destinations within faulty blocks via a set of "intermediate nodes". Our method requires the global failure information if the central control unit is not available.
[message path, Multiprocessor interconnection networks, Heuristic algorithms, Glass, parallel machines, Read only memory, Concurrent computing, Fault tolerance, deadlock-free routing algorithm, global failure information, Large-scale systems, directed path graph, message passing, node utilization, virtual channels, irregular faulty patterns, Routing, Communication switching, faulty blocks, intermediate nodes, directed graphs, fault tolerant routing, System recovery, fault tolerant computing, mesh multicomputer, central control unit]
A parallel parsing VLSI architecture for arbitrary context free grammars
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
We propose a fixed size one dimensional VLSI architecture for the parallel parsing of arbitrary context free (CF) grammars, based on Earley's algorithm. The algorithm is transformed into an equivalent double nested loop with loop carried dependencies. We first map the algorithm into a 1D array with unbounded number of cells. The time complexity of this architecture is O(n), which is optimal. We next propose the partitioning into a fixed number of off the shelf processing elements. Two alternative partitioning strategies are presented considering restrictions, not only in the number of the cells, but also in the inner structure of each cell. In the most restricted case, the proposed architecture has time complexity O(n/sup 3//p*k), where p is the number of available cells and the elements inside each cell are at most k.
[equivalent double nested loop, parallel architectures, loop carried dependencies, 1D array, Laboratories, Very large scale integration, parallel parsing VLSI architecture, Delay, parallel programming, Concurrent computing, Computer architecture, context-free grammars, Systolic arrays, Dynamic programming, CF grammars, program control structures, VLSI, Natural languages, fixed size one dimensional VLSI architecture, time complexity, Partitioning algorithms, Pattern recognition, arbitrary context free grammars, partitioning strategies, Earley algorithm, off the shelf processing elements, computational complexity]
Significant message precedence in object-based systems
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
Distributed applications are realized by cooperation of objects. A state of the object depends on in what order the object exchanges request and response messages and computes requests. In this paper, we newly define a significantly precedent order of messages based on a conflicting relation among requests. The objects can be mutually consistent if the objects take messages in the significantly precedent order. We discuss a protocol which supports the significantly ordered delivery of request and response messages. Here, an object vector is newly proposed to significantly order messages.
[message passing, response message exchange, Delay effects, request message exchange, object-based systems, Yarn, distributed applications, significant message ordering, Intelligent networks, protocol, request computation, object vector, significant message precedence, mutually consistent objects, protocols, conflicting relation, cooperation of objects, distributed object management]
Design of reservation mechanism based on the slotted ring network
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
The traditional slotted ring network is one of the high speed networks, however its transmission delay in heavy network load is too long. We propose a reservation mechanism to improve this drawback. The mechanism is particularly useful in the heavy network load to get a lower delay for high priority packet transmission. We compare it with a traditional slotted ring network by using simulations, and the results show that this mechanism provides the lower delay when the network traffic load is heavy.
[Real time systems, Protocols, telecommunication congestion control, packet switching, local area networks, reservation mechanism, slotted ring network, network traffic load, Delay, heavy network load, High-speed networks, high priority packet transmission, Data communication, transmission delay, high speed networks]
Automatically partitioning threads based on remote paths
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
In order to program multithreaded architectures effectively, compiler support to automatically partition programs into threads is essential. This paper proposes a remote-path-based thread partitioning framework, which can generate low-level threads from procedural programs automatically. The framework has been implemented in the EARTH-C compiler, which uses a data dependence graph (DDG) as an intermediate representation for thread partitioning. To make the compiler work fast, a practical O(n/sup 2/) algorithm is designed to build a non-redundant DDG. To generate correct and efficient threaded code, the remote path heuristic is employed to satisfy thread partitioning constraints and to schedule threads to run quickly. Experimental results show that the DDG building algorithm is fast and the remote-path-based heuristic is very effective in partitioning programs into "optimized" threads.
[Algorithm design and analysis, intermediate representation, parallel architectures, graph theory, Communication system control, remote paths, Yarn, Delay, processor scheduling, Program processors, Bandwidth, Parallel processing, EARTH-C compiler, automatic program partitioning, optimising compilers, multi-threading, Buildings, multithreaded architectures, Partitioning algorithms, optimized threads, Communication switching, nonredundant data dependence graph, thread scheduling, procedural programs, low-level threads, thread partitioning constraints, computational complexity]
Supporting small accesses for the parallel file subsystem on distributed shared memory systems
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
The main goal of parallel file subsystem on Distributed Shared Memory (DSM) systems is to reduce the network traffic in page based software DSM systems, thereby improving system performance. Our laboratory has built a prototype of the parallel file subsystem on two DSM systems, namely Cohesion and TreadMarks. But these two prototypes have several limitations: users must read/write the whole parallel file in a single access; users cannot modify an existing parallel file; the parallel file request must be issued from the root node. In our new parallel file subsystem on Teamster, a new DSM system developed by our laboratory, we eliminate the limitations revealed in the two previous parallel file subsystems. In addition, we have developed two new mechanisms, the software cache mechanism and the asynchronous file offset mechanism, to lessen the performance degradation caused by the frequent small accesses.
[page based software DSM systems, Cohesion, small accesses, Laboratories, Telecommunication traffic, Software performance, TreadMarks, asynchronous file offset mechanism, parallel programming, Degradation, Prototypes, parallel file subsystem, root node, Software prototyping, performance degradation, Application software, Programming profession, network traffic, Tiles, software cache mechanism, distributed shared memory systems, system performance, Software systems, file organisation, parallel file request, Teamster]
Design and implementation of multi-threaded object request broker
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
The distributed object oriented computing model is the next logical step to develop distributed applications. In recent years, several object models have been proposed, such as COM/DCOM, CORBA, and JAVA Bean etc. In CORBA, which was announced by OMG, object request broker is a software bus to connect applications and object components. In addition, multi threaded programming is a well known technique to improve the performance of applications. In a CORBA environment, clients can invoke the remote objects that are shared. If those objects are single threaded it will affect system performance in large distributed applications. We describe in detail the design and implementation of multi threaded object request broker based on CORBA. Our ORB was implemented atop Windows NT and underlying TCP protocol. Finally, we compare our system's performance with IONA's Orbix, which is a well known commercial product, in both one-way and two-way request.
[Windows NT, commercial product, Protocols, multi threaded programming, Electrical capacitance tomography, software bus, Environmental management, Distributed computing, distributed applications, Information science, CORBA environment, JAVA Bean, COM/DCOM, distributed object management, Orbix, Java, client-server systems, multi-threading, large distributed applications, Object oriented modeling, object models, TCP protocol, Application software, Software development management, Councils, multi threaded object request broker, remote objects, system performance, remote procedure calls, object components, distributed object oriented computing model, ORB]
Alignment and distribution is NOT (always) NP-hard
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
An efficient algorithm to simultaneously implement array alignment and data/computation distribution is introduced and evaluated. We re-visit previous work of Li and Chen (J. Li and M. Chen, 1990; 1991), and we show that their alignment step should not be conducted without preserving the potential parallelism. In other words, the optimal alignment may well sequentialize computations, whatever the distribution afterwards. We provide an efficient algorithm that handles alignment and data/computation distribution simultaneously. The good news is that several important instances of the whole alignment/distribution problem have polynomial complexity, while alignment itself is NP-complete (J. Li and M. Chen, 1990).
[program control structures, NP-hard, data/computation distribution, array alignment, NP-complete, Distributed computing, polynomial complexity, alignment/distribution problem, distributed algorithms, Parallel processing, Grid computing, alignment step, optimal alignment, distributed programming, computational complexity]
Secure and scalable inter-domain group key management for N-to-N multicast
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
The paper contributes a new architecture for secure and scalable inter-domain group key management for N-to-N (conference) IP multicast. The architecture views the multicast routing infrastructure from the key management plane, and logically divides it into two general types of regions for key management to achieve scalability. The work extends the centralized solution of (Wong et al., 1998) into a distributed key management scheme suitable for inter-domain multicast group key management. Methods for initiating new multicast groups, as well as for members joining and leaving, are presented. The paper also considers two general types of IP-multicast that need to be made secure if multicast is to be one of the vehicles for future wide-scale delivery of voice, video and text over the Internet.
[telecommunication security, voice, Scalability, Data security, Conference management, Multicast protocols, Routing, cryptography, video, Proposals, N-to-N multicast, scalability, Vehicles, Postal services, inter-domain group key management, telecommunication network routing, multicast routing infrastructure, multicast communication, Internet, IP multicast, text, Cryptography, distributed key management scheme]
An effective cache-index forwarding over wireless network for the World-Wide Web
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
Caching and prefetching are well-known solutions to the problem of long latency in both wired and wireless networks. Caching is prevalently used to reduce round-trips and network traffic. The prefetching and uploading-prefetching data from fixed hosts on a wired network and uploading to mobile hosts-in the base station for the WWW has been proposed to solve those problems and has shown high performance improvement. The performance of prefetching in the base station can be improved when the base station has the caching information of mobile hosts. Cache-index forwarding can be used to let the base station see information of caching on mobile hosts. Here we propose a method named "CINDEX: Cache-Index Forwarding per Document for the WWW\
[document, World Wide Web, Etching, cache storage, Electronic mail, long latency, caching, performance improvement, Delay, Wireless communication, cache-index forwarding, mobile computing, mobile host, Wireless networks, network traffic reduction, Computer networks, uploading, Data communication, wireless network, information resources, client-server systems, Prefetching, prefetching, Computer science, CINDEX, World-Wide Web, telecommunication traffic, round-trip reduction]
Real-time gang schedulings with workload models for parallel computers
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
Gang scheduling has been shown to be an effective job scheduling policy for parallel computers that combines elements of space sharing and time sharing. We propose new policies to enable gang scheduling to adapt to environments with real-time constraints. Our work, to our best knowledge, is the first work to attempt to address the real-time aspects of gang scheduling. Our system guided by a metric, called "task utilization workload\
[Real time systems, Parallel languages, Job shop scheduling, Optimizing compilers, simulation, real-time constraints, Application software, parallel machines, time sharing, task utilization workload, Concurrent computing, Software libraries, Processor scheduling, metric, real-time systems, scheduling, workload models, parallel computers, job scheduling policy, real-time gang scheduling, System software, Large-scale systems, space sharing, throughput]
A program-driven parallel machine simulation environment
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
In recent years, it has been very popular to employ discrete-event simulation as a hardware architecture analytical tool to study distributed-memory multicomputers and shared-memory multiprocessors. After the hardware architecture prototype has been completed, a complete and detailed machine simulation environment can be utilized to evaluate the architecture's efficiency under real operating systems and application software. In this article, we discuss all the development and implementation of a program-executable Transputer network multicomputer as well as 80x86 series multiprocessors, and how they can be operated. On another level, owing to the extreme complexity of the simulated computer systems, parallel discrete-event simulation has also been used to shorten the time of running the simulation. In practice, this simulator can solve problems through a network connection with many workstations. Some of the workstations may be in charge of computing, while others can be responsible for the management of memory, thus making it simpler to establish a parallel machine simulation environment. In addition to providing an environment for programs to execute on it, such a simulator also calculates the time spent in running these programs, so as to evaluate the feasibility for these application programs to run on a hardware system.
[workstation clusters, application software, parallel architectures, transputer systems, Discrete event simulation, parallel machines, Concurrent computing, memory management, distributed-memory multicomputers, Computer architecture, parallel discrete-event simulation, Hardware, Workstations, discrete event simulation, Software prototyping, multiprocessing systems, efficiency, Computational modeling, Computer simulation, program-executable Transputer network multicomputer, shared-memory multiprocessors, Parallel machines, operating systems, program execution time, workstations, 80x86 series multiprocessors, Memory management, hardware architecture analytical tool, virtual machines, network connection, program-driven parallel machine simulation environment]
Dynamic management of URL based on object-oriented paradigm
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
As the World Wide Web is popular, it is in high demand for retrieving the right information. In addition, the amount of information that is available has been increased. A search engine is used to locate information, due to the fact that searching for information from an increased amount of information is difficult. The search engine has started to use robot agents to facilitate easier access to the information. The paper proposes a dynamic URL management technique by using the object oriented paradigm for efficient performance of the robot agent. While URLs are saved in a file or database to decide whether the site is visited or not, this technique creates a URL object and pushes in a hash table. Therefore we can easily compare a new URL to others in a hash table which allows us to save, and insert a new URL if the URL is not in a hash table. This technique has the advantage of saving URL in a unique way. Other useful information about URL also can be stored. If the robot agent is suspended, we can save into files by using object serialization to save the main memory.
[search engines, World Wide Web, HTML, Electrical capacitance tomography, Uniform resource locators, dynamic URL management, Databases, Search engines, Robots, URL object, information resources, object-oriented programming, hash table, search engine, information retrieval, Information retrieval, Robotics and automation, software agents, dynamic URL management technique, object serialization, Computer science, object oriented paradigm, file organisation, Internet, robot agents]
Broadcasting on wormhole-routed 2D tori with arbitrary size
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
For distributed memory parallel computers, broadcast operations are widely used in a variety of applications. In this paper we propose an efficient algorithm for broadcasting on an all-port wormhole-routed 2D torus with arbitrary size. The underlying network is assumed to support only the dimension-ordered unicast. By taking the advantage of the all-port model and the distance insensitivity of the wormhole routing, the proposed algorithm can greatly reduce the number of message-passing steps. In addition, it can be proved to be depth contention-free. The performance study in this paper clearly shows the advantage of the proposed algorithm.
[distance insensitivity, Computer worms, message passing, efficient algorithm, dimension-ordered unicast, Multiprocessor interconnection networks, network routing, multiprocessor interconnection networks, Switches, Routing, message-passing steps, broadcasting, Concurrent computing, Information science, Reactive power, Network topology, depth contention-free algorithm, Councils, distributed algorithms, algorithm performance, distributed memory systems, Broadcasting, distributed memory parallel computers, arbitrary size all-port wormhole-routed 2D torus]
Design, implementation, and evaluation of a parallel index server for shape image database
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
Describes the design, implementation and evaluation of a parallel indexer called PAMIS (PArallel Multimedia Index Server) for a polygonal 2D shape image database. PAMIS is based on a shape representation scheme called the turning function, which exhibits the desirable properties of position, scale and rotation invariance, and has a similarity metric function that satisfies the triangular inequality, which is required for efficient database indexing. Because the goal of the PAMIS project is to support "like-this" image queries, the indexing scheme we chose, the vantage-point tree (VPT), uses relative rather than absolute distance values to organize the database elements for efficient nearest-neighbor searching. We have successfully implemented PAMIS on a network of workstations to exploit the I/O and computation parallelism inherent in the VPT algorithm. We found that it is preferable to make the VPT node size as small as possible in order to have a lean and deep VPT structure, and the best-case scheduling strategy performs the best among the scheduling strategies considered. Overall, the performance of the VPT algorithm scales very well with the number of processors, and the indexing efficiency (defined as the percentage of database elements touched by the search) of PAMIS is 6% and 39% for "good" queries that ask for 1 and 50 nearest neighbors, respectively.
[workstation clusters, Shape, scale invariance, shape representation scheme, Multimedia databases, visual databases, parallel index server, relative distance values, workstation network, indexing efficiency, Turning, I/O parallelism, computation parallelism, scalability, Concurrent computing, database indexing, node size, file servers, similarity metric function, scheduling, Computer networks, invariance, Workstations, tree data structures, triangular inequality, PAMIS, nearest-neighbor searching, multimedia servers, multimedia databases, parallel databases, image queries, Indexes, tree searching, vantage-point tree, polygonal 2D shape image database, turning function, rotation invariance, multimedia, Image databases, Processor scheduling, position invariance, algorithm performance, ne, image retrieval, best-case scheduling strategy, Indexing]
Design of a system to support security communication between a Web proxy and a CGI program based on PKI
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
This paper discusses the design of a system to support security communications between a Web browser and a CGI program using PKI (public key infrastructure). TLS (transport layer security) supports security communications between a Web browser and a Web server, but this system supports security communications between a Web server and a CGI program as well as between a Web browser and a Web server. This system uses GSS (generic security service) API to communicate with PKI, and offers a Web user a Web proxy, and offers three library functions for CGI applications related to security. This makes it easier for a CGI developer to write a CGI program.
[telecommunication security, Protocols, application program interfaces, public key infrastructure, Communication system security, transport layer security, library functions, public key cryptography, Cryptography, Web server, Protection, Computer security, Context, information resources, security communication, Data security, CGI, PKI, Web browser, generic security service, Information security, Authentication, Web proxy, API, Internet]
Distributed Inter-AS route monitor-Distributed Internet Route Eye (DIRE)
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
Distributed Internet Route Eye (DIRE) is a distributed agent architecture for inter-autonomous system (AS) route monitoring. In this architecture, monitor agents (MAs) record detailed BGP route updates from border gateway protocol (BGP) routers, and operator agents (OAs) query multiple MAs for specific information and infer the source and cause of the problems. Currently, MA implementation is near completion and actual route transitions can be monitored. The potential for flap source inference by OAs is examined using actual route transition data monitored by MAs. This system is capable of providing faster, easier access to more detailed information for coping with routing instabilities.
[border gateway protocol routers, Protocols, Laboratories, Optical fiber networks, operator agents, Electrical capacitance tomography, distributed inter-autonomous system route monitoring, Read only memory, Web and internet services, monitor agents, information access, IP networks, Monitoring, route transition data, monitor agent querying, distributed agent architecture, routing instabilities, Routing, Topology, route updates, monitoring, software agents, flap source inference, telecommunication network routing, Distributed Internet Route Eye, Internet]
The distributed program reliability analysis on star topologies
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
We show that computing distributed program reliability on the star distributed computing system is NP-hard. We develop a polynomially solvable case to compute distributed program reliability when some additional file distribution is restricted on the star topology. We also propose a polynomial time algorithm for computing distributed program reliability with an approximate solution when the star topology is not satisfied with the additional file distribution.
[programming theory, NP-hard, software reliability, Reliability engineering, Educational institutions, Topology, Information management, polynomial time algorithm, distributed computing system, Information analysis, Computer science, distributed program reliability analysis, star topologies, polynomially solvable case, Computer network reliability, Physics computing, file distribution, Distributed control, Polynomials, distributed programming, computational complexity]
The Adaptive Arena: language constructs and architectural abstractions for concurrent object-oriented systems
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
In this paper we present a concurrent object-oriented model in which a concurrent object, which represents a shared resource abstraction in our model, is decomposed into a hierarchy of architectural abstractions: a shared data abstraction, a synchronization abstraction, and a scheduling abstraction. It will be shown that the separation of concerns among the three major components of the concurrent objects avoids many of the conceptual difficulties that arise when integrating concurrency into the object-oriented paradigm. The proposed model presents a formal methodology for the architectural design and specification of the concurrent object-oriented software systems. The notion of scheduling policy inheritance in our model facilitates the process of engineering adaptability in the development of the intelligent reactive/adaptive systems.
[shared data abstraction, Adaptive systems, scheduling abstraction, inheritance, shared resource abstraction, concurrent object, formal specification, processor scheduling, intelligent adaptive systems, Concurrent computing, software architecture, parallel languages, architectural abstraction hierarchy, architectural design, Object oriented modeling, Adaptive Arena, specification, concurrent object-oriented model, synchronisation, synchronization abstraction, intelligent reactive systems, object-oriented languages, formal methodology, Software systems, scheduling policy inheritance, language constructs, engineering adaptability]
A general broadcasting scheme for recursive networks with complete connection
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
A recursive network can be constructed with basic building blocks in a recursive manner. For example the hypercube is a recursive network because an n-dimensional hypercube consists of two (n-1)-dimensional hypercubes. A recursive network is said to be with complete connection if the subnetwork that constitute it are connected as a complete graph. A general broadcasting scheme is proposed for recursive networks with complete connection. The scheme is simple, efficient, and easy to be implemented. Moreover, no redundant message will be generated. Four networks: WK-recursive networks, hypercomplete networks, hypernet networks, and star networks, are taken as examples to show the effectiveness of the scheme.
[message passing, hypercube, parallel architectures, graph theory, hypercube networks, hypernet networks, broadcasting, complete connection, n-dimensional hypercube, WK-recursive networks, hypercomplete networks, recursive networks, subnetwork, Broadcasting, complete graph, general broadcasting scheme, star networks]
Detecting the first races in parallel programs with ordered synchronization
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
Detecting races is important for debugging shared memory parallel programs, because the races result in unintended nondeterministic executions of the programs. Previous on-the-fly techniques to detect races in programs with inter thread coordination such as ordered synchronization cannot guarantee that the race detected first is not preceded by events that also participate in a race. The paper presents a novel two pass on-the-fly algorithm to detect the first races in such parallel programs. Detecting the first races is important in debugging, because the removal of such races may make other races disappear including those detected first by the previous techniques. Therefore, this technique makes on-the-fly race detection more effective and practical in debugging parallel programs.
[program debugging, parallel program debugging, Event detection, Debugging, Access protocols, on-the-fly techniques, ordered synchronization, Yarn, Information technology, Research and development, parallel programming, synchronisation, Computer science, shared memory parallel programs, Parallel programming, first race detection, Computer errors, shared memory systems, Monitoring, unintended nondeterministic executions, inter thread coordination, two pass on-the-fly algorithm, on-the-fly race detection]
Routing in hypercubes with large number of faulty nodes
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
One of the fundamental routing problems is to find a path from a source node s to a target node t in computer/communication networks. In an n-connected network, a nonfaulty path from s to t exists if there are at most n-1 faulty nodes. However, the network can be disconnected by n faulty nodes. Since the connectivity is usually a worst-case measure which is unlikely to happen in practice, it is important to develop routing algorithms for the case that more than n-1 faulty nodes present. We propose algorithms for finding the routing path from s to t in a hypercube with a large number of faulty nodes. Let H/sub n/ be the n-dimensional hypercube and H/sub n//F be the reduced graph obtained by removing the nodes of F from H/sub n/. The reduced graph H/sub n/F is called k-safe if each node of H/sub n//F has degree at least k. Our first algorithm, given a set F of faulty nodes in H/sub n/ such that |F|/spl les/2/sup k/(n-k)-1 and H/sub n//F is k-safe for 0/spl les/k/spl les/n/2, and s,t /spl isin/H/sub n//F, finds a nonfaulty free path s/spl rarr/t of length d(s,t)+O(k/sup 2/) in O(|F|+n) optimal time, where d(s,t) is the distance between s and t. We show that a lower bound on the length of the nonfaulty path s/spl rarr/t is d(s,t)+2(k+1) for 0/spl les/k/spl les/n/2. Furthermore, for k=1 and 2, we give O(n) time algorithms which find a nonfaulty path s/spl rarr/t of length at most d(s,t)+4 and d(s,t)+6, respectively, which is tight to the lower bound.
[Multiprocessor interconnection networks, graph theory, hypercube networks, hypercubes, communication complexity, Read only memory, nonfaulty path, routing algorithms, Fault tolerance, routing, nonfaulty free path, Network topology, connectivity, computation time, Cities and towns, Hypercubes, Computer networks, Communication networks, source node, reduced graph, communication networks, computer networks, Routing, telecommunication network routing, faulty nodes]
Design, and implementation of a Java execution environment
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
Discusses how a Java execution environment, named Gabi, is designed and implemented. It includes the design and implementation of the interpreter of a Java Virtual Machine (JVM), the class loader, the frame and Java stack, the hash table (class table, native function table and Java string table), the handling of classes.zip, the interface to the native function, the interface to the just-in-time (JIT) compiler and its compiled code for Java methods and the multithreading support, and approaches to boost the performance of the bytecode interpreter by programming skills. Benchmarks are made to compare Gabi with Sun's JRE (Java Runtime Environment).
[application program interfaces, Laboratories, Licenses, Electrical capacitance tomography, programming skills, program compilers, multithreading support, bytecode interpreter, Runtime, native function interface, Operating systems, Java execution environment, design, zip code handling, native function table, Computer networks, Safety, benchmarks, software performance evaluation, Gabi, Sun JRE, Java, hash table, multi-threading, implementation, just-in-time compiler, Virtual machining, interpreter, Java Runtime Environment, Sun, Java Virtual Machine, program interpreters, Java string table, compiled code, performance, virtual machines, Java methods, class loader, file organisation, programming environments, frame, Java stack, class table]
Effective mechanisms to reduce the overhead of migratory sharing for linked-based cache coherence protocols in clustering multiprocessor architecture
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
Migratory-sharing data references will incur many cache misses that can be reduced by merging the invalidation/update requests and the cache misses. This paper presents effective software and hardware techniques to reduce the overhead of migratory-sharing references for the linked-based cache coherence protocols. The software scheme combines both compiler labeling and run time detection techniques. The hardware scheme uses the special access patterns in the linked-based protocoIs to detect migratory data objects. We have evaluated the performance on the linked-based program-driven simulation environment by using a set of SPLASH benchmarks. According to the simulation results, our software and hardware methods effectively enhanced the system performance up to 29% and 23% respectively by reducing the overhead of the migratory-sharing references.
[run time detection, parallel architectures, Merging, simulation, Software performance, compiler labeling, cache storage, program compilers, invalidation update requests, System performance, linked-based cache coherence protocols, Hardware, Labeling, migratory sharing, SPLASH benchmarks, multiprocessing systems, merging, cache misses, Access protocols, memory protocols, performance evaluation, data integrity, data references, performance, Object detection, clustering multiprocessor architecture]
IPLS: an intelligent parallel loop scheduling for multiprocessor systems
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
We propose a knowledge based approach for solving loop scheduling problems. A rule based system, called the IPLS, is developed by repertory grid and attribute ordering table to construct the knowledge base. The IPLS chooses an appropriate scheduling algorithm by inferring some features of loops and assigns parallel loops on multiprocessors for achieving high speedup. In addition, the refined system of IPLS can automatically adjust the attributes in a knowledge base according to profile information; therefore IPLS has feedback learning ability.
[Chaos, knowledge base, Costs, multiprocessor systems, knowledge based approach, scheduling algorithm, IPLS, Yarn, parallel programming, processor scheduling, Multiprocessing systems, Runtime, feedback learning ability, knowledge based systems, program control structures, rule based system, multiprocessing systems, Knowledge based systems, intelligent parallel loop scheduling, profile information, Dynamic scheduling, loop scheduling problems, Helium, Scheduling algorithm, repertory grid, attribute ordering table, Processor scheduling]
Hierarchical commitment algorithm for permanent time stamp ordering
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
Permanent time stamp ordering is one of the concurrency control mechanism for a distributed database system. It can guarantee that transaction can be processed in the order of arrival. According to this method, the transaction is temporally committed when it has been processed, and it is truly committed when its timestamp becomes less than the least timestamp of the processing transactions. In this paper, we propose a hierarchical commitment algorithm for permanent timestamp ordering and we also show that our new algorithm is superior to traditional centralized or distributed commitment algorithms with respect to the response time of the transaction.
[transaction processing, transaction response time, Concurrency control, transaction processing order, temporal commitment, Delay, database theory, concurrency control, concurrency control mechanism, distributed databases, hierarchical commitment algorithm, Database systems, permanent timestamp ordering, distributed database system]
The XBW model for dependable real-time systems
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
This paper presents a new conceptual model, the XBW-model. Distributed computing is becoming a cost effective way to implement safety critical control systems. To support the development of such systems the XBW conceptual model was developed. The model describes the time behavior and distribution properties of a system in such a way that static scheduling and systematic fault tolerance can be applied. The conceptual model also enables the definition of an appropriate fault model. This fault model along with the XBW-model allow efficient and systematic use of well known software based error detection methods. A distributed steer-by-wire control system is described, which is developed according to the model. The XBW-model is developed within in the European Brite-EuRam III project X-By-Wire.
[Real time systems, Costs, software reliability, safety-critical software, distributed processing, Control systems, Distributed computing, distributed computing, Fault tolerant systems, XBW model, error detection methods, distribution properties, Control system synthesis, scheduling, Safety, static scheduling, dependable real-time systems, computerised control, fault tolerance, safety critical control systems, Brite-EuRam III project, steer-by-wire control system, Processor scheduling, cost effective, Fault detection, real-time systems, time behavior, Distributed control, conceptual model]
Comparative evaluation of adaptive stochastic and ERICA switch algorithms for ABR traffic management in ATM networks
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
Performances of two explicit rate based congestion control schemes for ABR service in ATM networks: Explicit Rate Indication for Congestion Avoidance (ERICA) and proposed Adaptive Stochastic (AS), are compared under heavy load conditions. Neither of the schemes dictates a particular switch architecture. The requirements tested include: utilization, queuing delay, queuing delay variance and queue size. A simple network topology involves an ATM switching node modeled as a single server queuing system, fed by a number of highly bursty traffic sources. Small and large bursts are used in simulations. The controllable traffic sources' allowed cell rates (ACR) are dynamically shaped by the explicit feedback messages from the switch. The results of a simulation study suggest that AS scheme can provide for higher priority traffic in ATM LANs, shorter queuing delay, queuing delay variance, maximum and mean queue size, while retaining ERICA scheme's utilization.
[switch architecture, explicit rate based congestion control schemes, single server queuing system, telecommunication congestion control, telecommunication network management, Stochastic processes, Switches, simple network topology, explicit feedback messages, ATM switching node, asynchronous transfer mode, highly bursty traffic source, controllable traffic sources, Adaptive control, telecommunication computing, Delay, queuing delay variance, Network topology, ERICA switch algorithms, available bit rate, Traffic control, AS scheme, Communication system traffic control, stochastic processes, Testing, queueing theory, queue size, ABR traffic management, performance evaluation, heavy load conditions, Explicit Rate Indication for Congestion Avoidance, Programmable control, ABR service, allowed cell rates, Adaptive Stochastic, comparative evaluation, Asynchronous transfer mode, ATM networks]
Object-oriented Ease-based parallel primitives in C++
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
The paper presents our language C++-with-Ease, a superset of C++ with primitives for process creation and communication. The work adopts the semantics of the Ease parallel primitives, as defined by S.E. Zenith (1990), within the object oriented paradigm. The result is a general purpose, high level, imperative parallel programming language that allows the simple expression of parallel algorithms within a type-safe implementation. Our language implementation is best suited to homogeneous parallel MIMD machines, independent of architecture, but also runs under threads packages. C++-with-Ease addresses efficiency for message copying and provides extensions to allow the passing of arbitrary messages in a natural fashion within the paradigm. The implementation and associated message protocols are discussed.
[homogeneous parallel MIMD machines, Protocols, arbitrary messages, Communication system control, imperative parallel programming language, language implementation, process creation, semantics, Yarn, Parallel algorithms, parallel machines, parallel programming, C++-with-Ease, type-safe implementation, parallel languages, Context, parallel algorithms, Ease parallel primitives, message passing, message protocols, threads packages, Data structures, object oriented Ease based parallel primitives, C++ language, Packaging machines, Computer science, object oriented paradigm, Computer languages, message copying, Parallel programming, C++ superset]
Distributed object relocation protocol for wide area networks
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
When distributed object technology is used in a wide area network, an object may not be accessed efficiently, due to a large communication delay. For this problem, we have proposed a Replicated Object Relocation Protocol (RORP), where the replicated object is relocated to an intermediate site, considering the communication delay, the processing ability of the servers, the reference frequency of the objects, the time of object transmission, the domain and the limited number of replications. This paper overviews the protocol and discusses its performance through the implementation of a RORP prototype. We evaluate the client-oriented replication scheme by the amount of processing distributed to the clients, the intermediate sites and the servers. Then, the paper shows how RORP enables a reduction in the response time and the total network traffic, including control messages and object retrieval messages.
[Software maintenance, protocol performance, wide area networks, object transmission time, replication number, Electrical capacitance tomography, communication delay, Delay, control messages, Prototypes, Computer architecture, distributed object technology, protocols, RORP, distributed object management, Magnetooptic recording, Wide area networks, object retrieval messages, client-server systems, replicated databases, Object oriented modeling, object-oriented databases, client-oriented replication scheme, Access protocols, Application software, object reference frequency, server processing ability, network traffic, response time, domain, Replicated Object Relocation Protocol, intermediate site, wide area network]
An efficient multi-sampling strategy for interactive display in a video-on-demand server
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
One of the most important challenges in a video-on-demand system is to support interactive browsing functions such as fast forward and fast backward. There are several possible approaches to implementing these functions which require additional resources. Although the segment-sampling strategy proposed by Chen et al. (1995) can support browsing at any desired speed without any additional resource, this strategy only considers the display of a single continuous object at a time. In this paper, we propose a new strategy, called the multi-sampling strategy, which generalizes the approach of the segment-sampling strategy so that it can support continuous display of multiple objects at different display speed rates, simultaneously, without any additional resource.
[interactive browsing functions, Buffer storage, Jitter, multimedia systems, Mathematics, video-on-demand server, video servers, Auditory displays, efficient multi-sampling strategy, display speed rates, Information science, Disk drives, Computer displays, video on demand, interactive display, Bandwidth, fast backward, continuous multiple object display, Sampling methods, fast forward, segment-sampling strategy, Computer science education]
Web-enabling legacy applications
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
A recent research trend in Web applications is the integration of legacy applications on the World Wide Web. The motivations behind this research are the goals of producing a hybrid system where the Web can provide greater accessibility and distribution for legacy applications, and some standards to increase the interoperability and ease of use. For user interaction driven legacy applications, we propose a 3-tier conceptual architecture to support applications for the WWW, and present a approach to building sophisticated inactive Web systems. We benefit from this modelling approach in terms of universal accessibility, platform independency, modularity and migration efficiency. The interaction scenario between the client and server in the prototype implementation is an example to demonstrate the procedures for migrating a legacy application to the Web. Some basic technologies are reviewed and deployed for implementation of our design, including Java applet, servlet, JDBC and CORBA. Such an approach can also be extended to the newly developed Web based applications.
[inactive Web systems, Java applet, Protocols, open systems, interaction scenario, servlet, World Wide Web, File servers, user interfaces, Web enabling legacy applications, hybrid system, Network servers, user interaction driven legacy applications, modularity, Prototypes, JDBC, Web server, distributed object management, information resources, Java, client-server systems, platform independency, Web based applications, interoperability, migration efficiency, Application software, software maintenance, Computer science, standards, CORBA, 3-tier conceptual architecture, universal accessibility, User interfaces]
Design and implementation of agent-based flexible asynchronous messaging system
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
Asynchronous messaging systems like e-mail systems need some advanced characteristics, such as intelligence, controllability and scalability, to accomplish more effective and sophisticated message handling. We propose a framework of next generation messaging systems, Flexible Asynchronous Messaging System (FAMES), which consists of autonomous and collaborative software agents. In FAMES, various messaging functions composing of agents can be utilized to integrate heterogeneous user environment. Moreover, FAMES operates message flow in an intelligent manner, considering the receiver's own convenience and that flexible message delivery can be achieved. We designed and implemented the proposed system based on multi agent technology, and we show its effectiveness through experimental studies using a prototype system.
[Scalability, Flexible Asynchronous Messaging System, electronic messaging, electronic mail, distributed processing, e-mail systems, Electronic mail, sophisticated message handling, messaging functions, scalability, collaborative software agents, controllability, Prototypes, Controllability, FAMES, Computer networks, agent based flexible asynchronous messaging system, heterogeneous user environment, Collaborative software, Microcomputers, software agents, Intelligent agent, Computer science, prototype system, multi agent technology, next generation messaging systems, Internet, message flow, flexible message delivery]
Fast mutual exclusion algorithms using read-modify-write and atomic read/write registers
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
Three fast mutual exclusion algorithms using read-modify-write and atomic read/write registers are presented in a sequence, with an improvement from one to the next. The last algorithm is shown to be optimal in minimizing the number of remote memory accesses required in a resource busy period. Remote memory access is the key factor of memory access bottleneck in large shared-memory multiprocessors. The algorithm is particularly suitable in such systems for applications with small critical sections and frequent resource requests.
[Algorithm design and analysis, small critical sections, memory access bottleneck, Access protocols, read-modify-write, fast mutual exclusion algorithms, Registers, resource busy period, Multiprocessing systems, Computer science, Flowcharts, sequence, Councils, distributed algorithms, atomic read/write registers, System recovery, large shared-memory multiprocessors, shared memory systems, remote memory access minimization, frequent resource requests]
A generalized basic cycle calculation method for efficient array redistribution
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
In many scientific applications, dynamic array redistribution is usually required to enhance the performance of an algorithm. We present a generalized basic cycle calculation (GBCC) method to efficiently perform a BLOCK-CYCLIC(s) over P processors to BLOCK-CYCLIC(t) over Q processors array redistribution. In the GBCC method, a processor first computes the source/destination processor/data sets of array elements in the first generalized basic cycle of the local array it owns. A generalized basic cycle is defined as lcm(sP,tQ)/(gcd(s,t)/spl times/P) in the source distribution and lcm(sP,tQ)/(gcd(s,t)/spl times/Q) in the destination distribution. From the source/destination processor/data sets of array elements in the first generalized basic cycle, we can construct packing/unpacking pattern tables. Based on the packing/unpacking pattern tables, a processor can pack/unpack array elements efficiently. To evaluate the performance of the GBCC method, we have implemented this method on an IBM SP2 parallel machine, along with the PITFALLS method and the ScaLAPACK method. The cost models for these three methods are also presented. The experimental results show that the GBCC method outperforms the PITFALLS method and the ScaLAPACK method for all test samples. A brief description of the extension of the GBCC method to multi dimensional array redistributions is also presented.
[local array, Costs, destination distribution, GBCC method, Parallel machines, scientific applications, source distribution, parallel machines, parallel programming, array elements, distributed algorithms, BLOCK-CYCLIC, ScaLAPACK method, IBM SP2 parallel machine, PITFALLS method, multi dimensional array redistributions, test samples, generalized basic cycle calculation method, source/destination processor/data sets, packing/unpacking pattern tables, Testing, dynamic array redistribution, cost models]
An efficient thread architecture for a distributed shared memory on symmetric multiprocessor clusters
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
The purpose of the paper is to demonstrate an efficient thread architecture for a distributed shared memory (DSM) system on symmetric multiprocessor (SMP) clusters. For DSM systems on SMP, how to utilize the processors efficiently without wasting available computational power is a major issue. We discuss three approaches that use the process, the kernel level thread, and the user level thread to map application threads onto execution entities respectively. Considering the advantages and disadvantages of each method, we construct our thread package by combining both the user level thread and the kernel level thread. User level threads correspond to application threads and kernel level threads schedule these user level threads across multiple processors. Threads are light weighted and can be migrated in our thread package. With this thread architecture, our DSM system performs well in elementary experiments.
[workstation clusters, application threads, distributed shared memory, Yarn, processor scheduling, Operating systems, Computer architecture, user level thread, Parallel processing, thread package, Workstations, Kernel, execution entities, DSM system, multi-threading, Application software, symmetric multiprocessor clusters, kernel level thread, multiple processors, Processor scheduling, thread architecture, Packaging, distributed shared memory systems, computational power, Central Processing Unit]
Flexible distributed systems for multimedia applications
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
This paper discusses how to make a distributed object system flexible so as to satisfy applications' requirements when changing the system environment. The system change is modeled to be the change of not only types of service but also quality of service (QoS) supported by the objects. There are two types of methods changing the objects, one for manipulating the states of the objects and another for changing QoS of the objects. We discuss new relations among methods with respect to QoS. By using the QoS-based relations, we newly discuss a QoS-based compensating way to recover the object from the less qualified state.
[Encapsulation, Checkpointing, Multimedia systems, multimedia applications, Quality of service, multimedia systems, Displays, flexible distributed systems, distributed object system, Image restoration, Electrical capacitance tomography, quality of service, Application software, Distributed computing, application requirements, object recovery, Systems engineering and theory, fault tolerant computing, system environment change, service type, distributed object management, object state manipulation]
Adaptive fault-tolerant wormhole routing for torus networks
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
In this paper, we present an adaptive fault-tolerant deadlock-free routing algorithm for torus networks by using four virtual channels. Messages are routed via shortest paths when there are no faults. When a message is blocked by a faulty block, the message will find a detour path to route around the faulty block. Based on the concept of unsafe nodes, we design a routing algorithm for tori that can tolerate block faults.
[Algorithm design and analysis, Multiprocessor interconnection networks, Pipelines, multiprocessor interconnection networks, message blocking, block faults, Fault tolerance, torus networks, Hypercubes, Large-scale systems, detour path, message passing, network routing, adaptive fault-tolerant deadlock-free routing algorithm, adaptive fault-tolerant wormhole routing, virtual channels, faulty block, Routing, Educational institutions, message routing, distributed algorithms, System recovery, fault tolerant computing, Reliability, unsafe nodes]
A new replication strategy for unforeseeable disconnection under agent-based mobile computing system
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
The goals of this paper are to revise previous replication strategies and propose a new hierarchy structure, client-agent-server architecture, to fit a mobile computing system that has more resource constraints than a conventional distributed database system has. This three-tier agent-based mobile computing system frees the limitations on time and whereabouts of computing. Moreover, a new replication strategy, three-copy replication that is based on an optimistic replication strategy, is proposed to deal with the problems of mobility management and disconnection management in a mobile computing system. It allows mobile clients to read and update the database while they are disconnected from the network, if the mobile copy is available. In addition, the implementation issues of this new replication strategy are also provided.
[Costs, Memory, optimistic replication, replication strategy, Mobile communication, mobility management, Delay, mobile computing, Databases, Marketing and sales, Contracts, agent-based mobile computing, distributed database, client-server systems, client-agent-server architecture, replicated databases, three-copy replication, fault tolerance, resource constraints, unforeseeable disconnection, software agents, software fault tolerance, Computer science, Councils, disconnection management, Mobile computing]
Allocation time-based processor allocation scheme for 2D mesh architecture
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
The mesh is a widely used architecture in parallel computing systems. Research on efficient allocation of processors to incoming tasks on a mesh architecture is very important in achieving the desired high performance. The processor allocation strategy proposed in this paper is based on a well-known boundary search approach by considering allocation time similarity as another primary allocation decision-making factor. In this proposed technique, an additional novel heuristic is employed to consider allocating tasks with similar allocation times with submeshes adjacent to each other, whenever feasible. The external fragmentation problem is expected to be alleviated, which leads to improvement in, better utilization and shorter task waiting time. Our simulation results demonstrate a substantial improvement.
[Costs, parallel architectures, simulation, processor allocation scheme, 2D mesh architecture, parallel machines, resource allocation, heuristic programming, Search methods, parallel computing systems, Computer architecture, external fragmentation problem, Parallel processing, search problems, Computational modeling, Delay effects, Decision making, performance evaluation, heuristic, parallel architecture, Topology, boundary search approach, allocation time, performance, High performance computing, decision making, Resource management]
A distributed multicast routing algorithm for delay-sensitive applications
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
We present a distributed multicast algorithm for constructing minimum cost multicast trees with delay constraints. The proposed algorithm, which provides multicasting and guaranteed end-to-end delay bound at the network layer, is also designed to find a reduced cost routing tree. The proposed algorithm requires limited network state information and the routing tree is computed through a single round of message exchanges between network nodes. We prove the algorithm's correctness by showing that it is always capable of constructing a delay constrained multicast tree, if one exists. The algorithm is verified by simulation, and it is shown that it exhibits superior performance compared to existing ones for the tree cost measure.
[Costs, tree cost measure, Communication system control, delay constraints, minimum cost multicast trees, Delay, network nodes, network layer, distributed multicast routing algorithm, Web and internet services, multicast communication, Computer networks, tree data structures, delay constrained multicast tree, delay-sensitive applications, message passing, routing tree, message exchanges, trees (mathematics), Routing, Multicast protocols, Application software, reduced cost routing tree, Multicast algorithms, guaranteed end-to-end delay bound, distributed algorithms, limited network state information, algorithm correctness, Resource management]
One-phase commit: does it make sense?
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
Although widely used in distributed transactional systems, the so-called Two-Phase Commit (2PC) protocol introduces a substantial delay in transaction processing, even in the absence of failures. This has led several researchers to look for alternative commit protocols that minimize the time cost associated with coordination messages and forced log writes in 2PC. In particular, variations of a One-Phase Commit (1PC) protocol have recently been proposed. Although efficient, 1PC is however rarely considered in practice because of the strong assumptions it requires from the distributed transactional system. The aim of the paper is to better identify and understand those assumptions. Through a careful look into the intrinsic characteristics of 1PC, we dissect the assumptions underlying it and we present simple techniques that minimize them. We believe that these techniques constitute a first step towards a serious reconsideration of 1PC in the transactional world.
[Process design, transaction processing, time cost minimization, Costs, Delay effects, 2PC, Access protocols, Read only memory, Two-Phase Commit protocol, Partial response channels, distributed transactional systems, coordination messages, alternative commit protocols, forced log writes, Voting, Constraint theory, Desktop publishing, Database systems, fault tolerant computing, protocols, one-phase commit, 1PC protocol]
A share assignment method to maximize the probability of secret sharing reconstruction under the Internet
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
The use of the Internet for various business applications and resource sharing has grown tremendously over the last few years. In some applications, an important document may be required to be divided into pieces and be allocated in different locations over the Internet for security access concerns. For example, an important map that can be used to access a military base, an important key that can be used to give a military order or command. To access such an important document, one must reconstruct the divided pieces from different locations. In this paper, a probability model for reconstructing the secret sharing under the Internet is proposed. Also, how to assign the divided shares into different locations is studied. Particularly, algorithms to perform share assignment and to reconstruct the divided pieces into the original secret are proposed.
[Algorithm design and analysis, telecommunication security, military base, probability, Access protocols, document, security access, File servers, cryptography, Security, Computer science, secret sharing reconstruction, Lungs, share assignment method, resource sharing, Internet, Cryptography, IP networks, Resource management, business applications, map, business data processing]
Hybrid prefetching for WWW proxy servers
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
As the application of the World Wide Web grows, technologies for reducing Web latency are becoming important. Proxy caching is one of the most effective and widely implemented mechanisms. By prefetching documents on the proxy server, a better cache hit rate can be achieved and the clients experience a faster response time. In this thesis, we propose a hybrid prefetching technique, combined with different constraints, which could increase cache hit rate. Under this new prefetching scheme, the additional traffic is controllable. We also show that, when prefetching techniques are used in the proxy server, separated cache management is better if the whole cache size is small. All of the mechanisms are experimented by trace driven simulation.
[search engines, telecommunication congestion control, clients, trace driven simulation, World Wide Web, cache storage, cache hit rate, controllable traffic, Delay, Network servers, document prefetching, File systems, Bandwidth, Traffic control, Web latency reduction, Web server, separated cache management, client-server systems, Prefetching, WWW proxy servers, Internet, proxy caching, Web sites, hybrid prefetching, telecommunication traffic]
Parallel neural learning by iteratively adjusting error thresholds
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
We first propose a modified backpropagation learning algorithm that incrementally decreases the error threshold by half in order to process training instances with large weight changes as quickly as possible. This modified backpropagation learning algorithm is then parallelized using the single-channel broadcast communication model to n processors, where n is the number of training instances. Finally, the parallel backpropagation learning algorithm is modified for execution on a bounded number of processors to cope with real-world conditions.
[parallel algorithms, parallel neural learning, Communication system control, single-channel broadcast communication, training instances, modified backpropagation learning algorithm, Partitioning algorithms, Information management, error threshold adjustment, large weight changes, Neural networks, Backpropagation algorithms, Machine learning, backpropagation, Broadcasting, Systolic arrays, Iterative algorithms, Error correction, neural nets, errors]
Decompositions of de Bruijn networks
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
This paper deals with problems of decomposition of de Bruijn graph into isomorphic building blocks based on cover sets. The aim is to find so called lowest-cost cover sets which provide decompositions into building blocks with minimal number of external edges. We present formulae for the costs of basic covers. We give new results on the lowest-cost cover set design. We also give new results on the topology of graphs of building blocks based on basic cover sets. We also discuss several open problems.
[graph topology, de Bruijn network decomposition, Multiprocessor interconnection networks, isomorphic building blocks, Buildings, graph theory, basic cover cost formulae, multiprocessor interconnection networks, Very large scale integration, set theory, lowest-cost cover sets, Computer science, lowest-cost cover set design, Network topology, Chromium, minimal external edges, Cost function]
A dualthreaded Java processor for Java multithreading
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
The Java-Web computing paradigm has changed the Internet into a computing environment. For Java-Web computing and many Java applications, a new Java processor called simultaneous multithreaded (SMT) JavaChip, is proposed to enhance the performance of previous Java processors by hardware support of Java multithreading. SMT JavaChip is a modified architecture with the enhanced mechanism of stack cache, instruction cache, functional units, etc. It executes dual independent threads simultaneously and enhances instruction level parallelism. The performance of SMT JavaChip is evaluated through the simulation using JavaSim, a Java processor simulator. This research is focused to enhance the performance of the Java processor by considering the characteristics of the Java language and computation environment. Performance results show that SMT JavaChip can provide an execution speedup of between 1.28 and 2.00 compared with the single threaded Java processors.
[Computer interfaces, Process design, execution speedup, Surface-mount technology, hardware support, cache storage, functional units, dual independent threads, stack cache, Computer architecture, Parallel processing, Java-Web computing paradigm, Hardware, dual-threaded Java processor, Java, object-oriented programming, multi-threading, microprocessor chips, instruction level parallelism, Programming profession, Multithreading, performance, virtual machines, Java multithreading, simultaneous multithreaded JavaChip, instruction cache, Internet, Java processor simulator]
On termination detection protocols in a mobile distributed computing environment
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
Incorporating mobile components into a distributed system has posed new challenges to the design of distributed computation. This paper studies a fundamental problem in distributed computing, the termination detection problem, in a mobile environment. Two types of termination detection protocols already exist: the diffusion-based schemes and the weight-throwing schemes, that are designed for traditional static distributed systems. We propose a hybrid scheme by combining these two protocols together. The scheme can better exploit the communication hierarchy (in terms of wired and wireless bandwidths) and can pave the gaps of computation and communication capability between static and mobile hosts, thus more scalable to larger distributed systems. Simulation results are presented, which show the advantage of the hybrid scheme over existing schemes.
[Checkpointing, Protocols, message passing, Computational modeling, simulation, distributed system, Mobile communication, communication hierarchy, mobile distributed computing, hybrid scheme, Distributed computing, Concurrent computing, mobile computing, Handheld computers, weight-throwing schemes, termination detection protocols, System recovery, static distributed systems, diffusion-based scheme, protocols, wireless LAN, Personal digital assistants, Mobile computing]
A programmable digital neuro-processor design with dynamically reconfigurable pipeline/parallel architecture
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
Previous neural network processors were configured either into a SIMD or into an instruction systolic array (ISA) ring architecture using the canonical mapping methodology. The disadvantages of these processors are the lack of generality, scalability, programmability and reconfigurability. So, we propose a programmable neuroprocessor whose architecture is dynamically reconfigurable into either SIMD or an ISA ring according to the data dependencies of any neural network model. To improve the computing time, the computation of an activation function, which typically needed tens of cycles in previous processors, can be done in a single cycle by using piecewise linear (PWL) function approximation. Using a simple bus architecture and instruction set, the proposed processor allows the implementation of neural networks larger than the physical processor element array and allows the user to solve any neural network model. We verify these properties with the error backpropagation (EBP) model and estimate the computation time of the proposed processor.
[Scalability, Pipelines, Communication system control, systolic arrays, pipeline architecture, Function approximation, neural chips, programmability, scalability, Concurrent computing, neural network model, reconfigurable architectures, computation time, instruction set, function approximation, canonical mapping methodology, Computer architecture, Parallel processing, Computer networks, instruction systolic array ring architecture, bus architecture, activation function, data dependencies, processor element array, Computational modeling, piecewise linear techniques, SIMD architecture, parallel architecture, generality, error backpropagation, Neural networks, backpropagation, piecewise linear function approximation, neural net architecture, reconfigurability, pipeline processing, programmable digital neuroprocessor, dynamically reconfigurable architecture]
Performance evaluation of cache depot on CC-NUMA multiprocessors
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
Cache depot is a performance enhancement technique on cache-coherent non-uniform memory access (CC-NUMA) multiprocessors, in which nodes in the system store extra memory blocks on behalf of other nodes. In this way memory requests from a node can be satisfied by nearby depot nodes without going all the way to the home node. This not only reduces memory access latency and network traffic, but also spreads the network load more evenly. We study the design strategy for cache depot that: enhances the network interface of each node to include a depot cache, which stores those extra memory blocks for other nodes; and employs a new multicast routing scheme, which is called the multi-hop worms and works cooperatively with depot caches, to transmit coherence messages. By considering message routing and depot caches together the design concept can be applied even to those CC-NUMA systems that have a non-hierarchical, scalable interconnection network. We have developed an execution-driven simulator to evaluate the effectiveness of the design strategy. Performance results from using four SPLASH-2 benchmarks show that the design strategy improves the performance of the CC-NUMA multiprocessor by 11% to 21%. We have also studied in depth various factors which affect the performance of cache depot.
[cache depot, Computer worms, cache-coherent non-uniform memory access, network load, Multiprocessor interconnection networks, memory access latency, Telecommunication traffic, SPLASH-2 benchmarks, cache storage, Electrical capacitance tomography, Read only memory, Delay, storage management, multicast communication, performance enhancement technique, multi-hop worms, message passing, coherence messages, CC-NUMA multiprocessors, scalable interconnection network, performance evaluation, Routing, data integrity, message routing, memory blocks, Computer science, network traffic, network interface, distributed shared memory systems, execution-driven simulator, multicast routing scheme]
Implementation and evaluation of the parallel Mesa library
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
Describes the implementation and performance evaluation of a 3D graphics library that can be readily linked with parallel applications to provide run-time visualization on large-scale message-passing parallel machines, such as the Intel Paragon. The prototype implementation is currently fully operational, and is based on Mesa, a public-domain OpenGL implementation, and on a sort-last parallelization strategy. Through a detailed performance analysis, we show that the scalability of the current prototype is close to the theoretical limit for the given hardware architecture. We have also developed a unified framework to describe parallel compositing algorithms and show that two popular parallel compositing algorithms, binary swapping and parallel pipeline compositing, are just two extreme instances of this framework. Such a framework is important because it allows users to tailor the compositing algorithm according to the computation/communication characteristics of specific parallel machines by tuning the parameters appropriately. The current parallel Mesa library prototype implements such a parameterizable family of compositing algorithms.
[sort-last parallelization strategy, Visualization, parallel pipeline compositing, Runtime library, open systems, Scalability, binary swapping, Pipelines, computation characteristics, visual databases, communication characteristics, parallel machines, scalability, Prototypes, data visualisation, run-time visualization, Hardware, Large-scale systems, Performance analysis, software performance evaluation, Intel Paragon, message passing, parallel Mesa library, parallel databases, parallel compositing algorithms, Parallel machines, performance evaluation, parallel applications, parameter tuning, 3D graphics library, prototype, parameterizable algorithms, Graphics, large-scale message-passing parallel machines, OpenGL implementation, hardware architecture]
A causal logging scheme for lazy release consistent distributed shared memory systems
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
This paper presents a causal logging scheme for the lazy release consistent distributed shared memory systems. Causal logging is a very attractive approach to provide fault tolerance for distributed systems, since it eliminates the need for stable logging. However since interprocess dependency must causally be transferred with the normal messages, the excessive message overhead has been a drawback of this approach. In order to achieve an efficient implementation of causal logging for distributed shared memory systems, the data structures and the operations supported by the lazy release consistency memory model are utilized. For example, to implement the sender-based logging of the message contents, the diff structure is utilized, and for the causal logging of the dependency information, the operations to support the causal propagation of the write notices are utilized. As a result, the causal logging for the lazy release consistent distributed shared memory system can be implemented with a very low overhead. The simulation results using parallel applications show only 1%-4.4% increases in the execution time.
[Checkpointing, Costs, fault tolerance, causal logging scheme, lazy release consistent systems, simulation, message contents, diff structure, system recovery, interprocess dependency, dependency information, Computer science, write notices, sender-based logging, Fault tolerant systems, execution time, distributed shared memory systems, Computer networks, fault tolerant computing, data structures, message overhead, Workstations, parallel application, stable logging]
Christmas tree: a 1-fault-tolerant network for token rings
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
The token ring topology is required in the token passing approach used in distributed operating systems. Fault tolerance is also required in the design of distributed systems. We consider the 1-fault-tolerant design for token rings, which can tolerate 1-processor fault- or 1-link fault. Note that the 1-fault-tolerant design for token rings is equivalent to the design of 1-Hamiltonian graphs. The paper introduces a new family of interconnection networks called Christmas tree. The under graph of the Christmas tree, denoted by CT(s), is a 3-regular, planar, 1-Hamiltonian, and Hamiltonian-connected graph. The number of nodes and the diameter of CT(s) are 3/spl times/2/sup s/-2 and 2s, respectively. In other words, the diameter of CT(s) is 2 log/sub 2/ n-O(1), where n is the number of nodes.
[Multiprocessor interconnection networks, 1-fault-tolerant design, multiprocessor interconnection networks, communication complexity, interconnection networks, Information science, Fault tolerance, 1-Hamiltonian graphs, Network topology, Tree graphs, Operating systems, Fault tolerant systems, network operating systems, token ring topology, Computer networks, Token networks, protocols, Christmas tree, distributed systems design, fault tolerance, trees (mathematics), under graph, Councils, Hamiltonian-connected graph, 1-fault-tolerant network, token passing approach, fault tolerant computing, token networks, distributed operating systems]
Incrementally extensible folded hypercube graphs
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
In this paper we propose the incrementally extensible folded hypercube (IEFH) graph as a new class of interconnection networks for an arbitrary number of nodes. We show that this system is optimal fault tolerant and almost regular (i.e., the difference between the maximum and the minimum degree of nodes is at most one.). The diameter of this topology is half that of the incomplete hypercube (IH), the supercube, or the IEH graph. We also devise a simple routing algorithm for the IEFH graph. Further we embed cycles and complete binary trees into this graph optimally.
[almost regular system, network routing, graph theory, arbitrary node number, hypercube networks, cycles, interconnection networks, incrementally extensible folded hypercube graphs, topology diameter, routing algorithm, complete binary trees, Hypercubes, fault tolerant computing, optimal fault tolerant system]
Design and implementation of a copy update mechanism on a mobile information announcement system
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
The bandwidth problem between a mobile computer and an access point is pointed out, and a copy update mechanism, which can make the best use of the limited bandwidth, is proposed. If the bandwidth of a network is not sufficient, a mobile computer cannot announce all resources, and cannot update some copies on fixed computers. To satisfy client's requests with given bandwidth as much as possible, the proposed mechanism limits traffic of storage type resources in order to announce non-storage type resources prior to storage type resources. Traffic is controlled by limiting the number of concurrent update operations for announcing storage type resources. We implement the prototype and evaluate it. The results show that it can stably announce non-storage type resources.
[IEEE news, fixed computers, limited bandwidth, concurrent update operations, Information science, client requests, mobile computing, mobile information announcement system, resource allocation, traffic control, Prototypes, Bandwidth, Computer networks, non-storage type resources, access point, client-server systems, storage type resources, copy update mechanism, bandwidth problem, Application software, Home computing, Teleconferencing, mobile computer, Packaging, Mobile computing, portable computers]
Design and performance evaluation of an adaptive cache coherence protocol
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
In shared-memory multiprocessor systems, the local caches which are used to tolerate the performance gap between processor and memory cause additional bus transactions to maintain the coherency of shared data. Especially, coherency misses and data traffic due to spatial locality and false sharing have a significant effect on the system performance. In this approach, an adaptive cache coherence protocol based on the sectored cache is introduced. It determines the size of a block to be migrated or invalidated dynamically, depending on the transfer mode, so that it can exploit the spatial locality and reduce useless data traffic due to false sharing at the same time. This protocol is evaluated via event-driven simulation, and its results show a 58% decrease in the data traffic and a 45% decrease in the cache miss ratio. Thus, the adaptive cache coherence protocol provides about a 56% improvement in the execution time.
[block size, transfer mode, coherence, processor-memory performance gap, event-driven simulation, cache storage, Electrical capacitance tomography, false sharing, Delay, data traffic, adaptive cache coherence protocol, Degradation, System performance, Parallel processing, shared data coherency, shared memory systems, discrete event simulation, shared-memory multiprocessor systems, sectored cache, coherency misses, Access protocols, memory protocols, performance evaluation, block invalidation, local caches, adaptive systems, bus transactions, Computer science, block migration, execution time, cache miss ratio, spatial locality, protocol design, telecommunication traffic]
Self-synchronized vector transfer for high speed parallel systems
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
Communications between processing elements (PEs) in high speed parallel systems become a bottleneck as the function and speed of the PEs improve continuously. Clocked I/O ports in PEs may malfunction if data read failure occurs due to clock skew. To reduce the clock skew, global clock distribution is utilized, however it seems to be more difficult to use this for high speed parallel systems in the future. This paper addresses a self-tested self-synchronization (STSS) method for vector transfer between PEs. A test signal is added to remove the data read failure. This method has these features: high data throughput; low power consumption; no constraints on clock skew and system scale; flexibility in design; less latency. A failure zone concept is used to characterize the behavior of storage elements. Using a jitter injected test signal, robust vector transfer between PEs with arbitrary clock phases is achieved without global synchronization.
[self-synchronized vector transfer, Energy consumption, parallel architectures, Circuits, low power consumption, latency, Jitter, Throughput, Electronic switching systems, data read failure, parallel machines, Delay, vector processor systems, clocked input output ports, Microprocessors, bottleneck, jitter injected test signal, high speed parallel systems, clock skew, Testing, global synchronization, high data throughput, performance evaluation, global clock distribution, storage elements, synchronisation, jitter, processing elements, design flexibility, Frequency synchronization, Clocks]
Electronic exchange check system on the Internet
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
This paper deals with a new payment scheme on the Internet, the electronic exchange check system, based on Nguyen's (1997) scheme and Hwang's (1997) scheme. The electronic exchange check system is a kind of electronic cash system which has pre-denoted receiver and effective time. The scheme satisfies all the basic requirements for an electronic payment scheme in the aspect of security and privacy. In particular, it provides the prior-restraint of double-spending as well as the detection of the identity of a double-spender after the fact.
[data security, payment scheme, Credit cards, Telecommunication computing, Electronic commerce, Communication system security, Distributed computing, electronic money, electronic cash, Privacy, security of data, Information security, electronic exchange check system, double-spending, Communications technology, data privacy, Internet, IP networks]
Shrinking timestamp sizes of event ordering protocols
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
Almost all published work on causal ordering mechanisms assumes theoretically unbounded counters for timestamps, thus ignoring the real world problem that arises if one is actually interested in an operable implementation, since unbounded counters simply cannot be realized. An argument for its justification often encountered states, that the counter size can be chosen such that counters practically do not overflow or wrap around. For example, using matrix timestamps in a distributed computation involving not more than 50 processes and 32 bits per integer, results in a timestamp size of almost 10 K byte. We present a solution, called Factorized Timestamp Approach (FTA) that substantially reduces the amount of piggybacked control information. It is based on introducing the notion of phases in which much smaller timestamps are used. Simulation results given in the paper show the suitability of this approach.
[timestamp size, Protocols, matrix timestamps, operable implementation, distributed processing, distributed computation, Synchronization, event ordering protocols, Delay, Counting circuits, synchronisation, clocks, theoretically unbounded counters, Factorized Timestamp Approach, real world problem, counter size, US Department of Transportation, timestamp sizes, piggybacked control information, protocols, causal ordering mechanisms, Clocks]
A new representation of graphs and its applications to parallel processing
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
In this paper, we propose a new presentation of graphs, called the index-permutation (IP) graph model, and apply it to the systematic development of communication-efficient interconnection networks. We derive several new classes of interconnection networks based on IP graphs to achieve the desired properties. We show that the diameters of some classes of IP graphs are optimal within a factor of 1+0(1) given their node degrees. Designs based on IP graphs can balance system resources by appropriately selecting network parameters, thus providing adaptability to future technology and application requirements.
[network parameters, system resource balancing, Multiprocessor interconnection networks, Scalability, Genetic mutations, graph theory, multiprocessor interconnection networks, adaptability, Application software, Appropriate technology, parallel processing, application requirements, Concurrent computing, communication-efficient interconnection networks, Network topology, Computer network reliability, Parallel processing, Hypercubes, index-permutation graph model, systematic development]
A news on demand service system based on robot agent
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
The World Wide Web has a lot of distributed digital information. When the size of the Web is increased beyond a few sites and a small number of documents, finding information requires time-consuming searching and is too tedious when a user retrieves specific information every day. We propose a news on demand (NOD) service system that gathers daily news information using a robot agent and delivers integrated news to users. Once a user registers his information and preferences, he can get the news information that he is most interested in via multimedia e-mail.
[information resources, Java, Protocols, search engines, electronic mail, information retrieval, daily news information, user preferences, World Wide Web, multimedia computing, software agents, Postal services, news on demand service system, Computer science, robot agent, Internet, integrated news delivery, IP networks, Web sites, information needs, Web server, distributed digital information, Robots, multimedia e-mail]
Probability based replacement algorithm for WWW server arrays
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
This paper describes a scalable Web server array architecture which uses a caching policy called the probability based replacement (PBR) algorithm. The server array consists of a central server and several Web servers. The central server stores the whole document set, and sends the user requested documents to the Web servers by a technique called the selective broadcast technique. Web documents are cached in the Web servers and are replaced based on the PBR algorithm. Performance comparison using NASA and ClarkNet access logs between PBR server arrays and purely mirrored Web servers is performed. The results show that with 10% document caching, the maximum throughput of the former one is nearly the same as that of mirrored Web servers. The PBR server arrays, however, require much smaller disk storage in the Web servers than the mirrored Web servers. The PBR server arrays are also much more scalable than the mirrored ones.
[search engines, WWW server arrays, document set, probability based replacement algorithm, World Wide Web, cache storage, selective broadcast, access logs, Network servers, central server, Workstations, IP networks, Communication networks, Web server, software performance evaluation, information resources, client-server systems, cache, ClarkNet, NASA, Service oriented architecture, probability, disk storage, caching policy, PBR algorithm, Councils, Bioreactors, performance comparison, Internet, World Wide Web server arrays, maximum throughput]
Computing multidimensional aggregates in parallel
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
Computing multiple related group-by aggregates is one of the core operations of online analytical processing (OLAP) applications. This kind of computation involves a huge volume of data operations (megabytes or treabytes). The response time for such applications is crucial, so, using parallel processing techniques to handle such computation is inevitable. We present several parallel algorithms for computing a collection of group-by aggregates based on a multiprocessor system with shared disks. We focus on a special case of the aggregation problem-"Cube" operator which computes group-by aggregates over all possible combinations of a list of attributes. The proposed algorithms introduce a novel processor scheduling policy and a non-trivial decomposition approach for the problem in the parallel environment. Particularly, the hybrid algorithm has the best performance potential among the four proposed algorithms. All the proposed algorithms are scalable.
[multidimensional aggregates, parallel algorithms, data operations, Multidimensional systems, hybrid algorithm, data mining, parallel databases, parallel processing, processor scheduling, database theory, online analytical processing, Concurrent computing, performance, Aggregates, response time, Cube, multiple related group-by aggregates, multiprocessor system, decomposition approach, shared memory systems, software performance evaluation]
User-defined telecooperation services
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
A user-defined telecooperation service (UTS) provides service elements for application-specific communication and cooperation processes as well as integrated means for the service definition, adaptation, and management. It supports user groups with particular communication, cooperation, and coordination needs which may change over time and which may be that special, that the service maintenance can be carried out only by the users themselves in an economic and satisfactory way. The users may be organized in various open and closed groups. They dispose of personal computing equipment connected via wide-area telecommunication networks. The users participate only from time to time. Therefore, there is a partial and varying accessibility of users and user sites. Interactions are mainly based on asynchronous communication operations. The cooperation and coordination functions have to consider unreachable users. The paper introduces the notion of UTS. Fields of application are addressed. Moreover, we describe the principles of the service element definitions and outline the architecture of a supporting system.
[user groups, Electronic switching systems, Electrical capacitance tomography, Tellurium, Read only memory, application-specific communication processes, Concurrent computing, Reactive power, service adaptation, communication needs, application-specific cooperation processes, groupware, asynchronous communication operations, Identity-based encryption, service definition, Collaborative software, service management, cooperation needs, Automobiles, user accessibility, Collaborative work, user-defined telecooperation services, service maintenance, user sites, coordination needs, teleworking]
A comparison of two torus-based k-coteries
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
We extend a torus-based coterie structure for distributed mutual exclusion to allow k multiple entries in a critical section. In the original coterie, the system nodes are logically arranged in a rectangle, called a torus, in which the last row (column) is followed by the first row (column) using end wraparound. A torus quorum consists of a head and a tail, where the head contains one entire row and the tail contains one node from each of the s succeeding rows, s/spl ges/1 is a system parameter. It has been shown that by setting s=[h/2], where h=the number of rows, the collection of torus quorums form an equal-sized, equal-responsibility coterie. In this paper we propose two extensions to k-coteries: the Div-Torus method divides the system nodes into k clusters and runs a separate instance of a torus coterie in each cluster; the k-Torus method uses quorums of tail s=[h/(k+1)]. We compare the quorum size and quorum availability of the two proposed methods, and against the DIV method which is based on the majority quorums in each of the k divided clusters, assuming the node reliability is a constant. Numerical data demonstrate that DIV and Div-Torus have similar system availability, better than that of the k-Torus, although all 3 methods' availability becomes comparable when the node reliability is higher than 0.9. However, Div-Torus has the smallest quorum size and k-Torus the second smallest, which has the potential of causing less network traffic when requesting permissions from a quorum.
[multiple entries, majority quorums, quorum size, Communication system control, distributed mutual exclusion, distributed processing, Control systems, Electronic switching systems, node reliability, Div-Torus method, end wraparound, Read only memory, Delay, head, equal-sized equal-responsibility coterie, Tail, Permission, quorum availability, clusters, torus quorum, permission request, tail, torus-based k-coteries, critical section, system node arrangement, network traffic, numerical data, System recovery, system availability, fault tolerant computing, Clocks]
Two problems on butterfly graphs
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
The cycle partition problem and the pancycle problem on butterfly graphs are studied in this paper. Suppose G=(V,E) is a graph and {V/sub 1/,V/sub 2/,...,V/sub s/} is a partition of V. We say that {V/sub 1/,V/sub 2/,...,V/sub s/} forms a cycle partition of G if each subgraph of G induced by V/sub 1/ contains a cycle of length |V/sub i/|, where 1/spl les/i/spl les/s. A cycle partition {V/sub 1/,V/sub 2/,...,V/sub s/} is /spl lambda/-uniform if |V/sub 1/|=|V/sub 2/|=...=|V/sub s/|=/spl lambda/. G has /spl lambda/-complete uniform cycle partitions if G has m/spl lambda/-uniform cycle partitions for all 1/spl les/m/spl les/(r+n)/2 and m dividing |V|//spl lambda/. Let BF(k,r) denote the r-dimensional k-ary butterfly graph. For the cycle partition problem, we construct a lot of uniform cycle partitions for BF(k,r). Besides, we construct r-complete uniform cycle partitions for BF(2,r), and kr-complete uniform cycle partitions for BF(k,r). For the pancycle problem, given any pair of n and r we can determine if there exists a cycle of length n in BF(2,r), and construct it if it exists. The results of this paper reveal that the butterfly graphs are superior in embedding rings. They can embed rings of almost all possible lengths. Besides, there are many situations in which they can embed the most rings of the same length.
[r-dimensional k-ary butterfly graph, cycle partition problem, ring embedding, graph theory, subgraph, Hypercubes, hypercube networks, pancycle problem, uniform cycle partitions, r-complete uniform cycle partitions, butterfly graphs, kr-complete uniform cycle partitions]
Fault tolerant all-to-all broadcast in general interconnection networks
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
With respect to scalability and arbitrary topologies of the underlying networks in multiprogramming and multithread environments, fault tolerance in acknowledged ATAB and concurrent communications become a challenge to reliable general wormhole routing multicomputers with arbitrary topologies. In this paper, the virtual ring tree (VRT) is proposed to deal with the challenge. A single startup is needed in the two proposed algorithms by a simple virtual node space, which also reduces the complexity of routing at intermediate steps of ATAB algorithms and re-beginning an ATAB, by cacheable virtual channels. The proposed algorithm can automatically handle static faults in networks.
[wormhole routing multicomputers, Multiprocessor interconnection networks, parallel architectures, multiprocessor interconnection networks, ATAB, interconnection networks, scalability, routing complexity, static faults, Fault tolerance, Intelligent networks, Network topology, fault tolerant all-to-all broadcast, virtual ring tree, Broadcasting, Computer networks, cacheable virtual channels, Workstations, IP networks, virtual node space, cache, multi-threading, network routing, multithread, trees (mathematics), Routing, Sun, arbitrary topologies, multiprogramming, fault tolerant computing, concurrent communications]
Synchronous flow control in wormhole routed optical networks
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
In this paper, we propose a synchronous flow control mechanism in wormhole routed optical networks. It is expected that the benefit of shorter routing delay and smaller buffer size requirement in wormhole routing will be significant in optical networks. Different from the traditional bi-directional asynchronous back-pressure flow control, the flow control is modified to be unidirectional and synchronous. The size of synchronized control slot does not depend on the routing path length and the number of bits is a constant which is equal to the total number of virtual channels and nodes. The proposed flow control takes advantage of the restricted order of accessing channels in deadlock-free routing to broadcast their control information in a corresponding restricted order. Furthermore, in order to reduce the buffer size to only one unit, the virtual channels which share the same physical channel must be able to simultaneously transmit data. The low channel utilization induced by such mechanism is overcome by our modified source routing. In summary, this paper introduces a flow control mechanism which easily incorporates the benefit of wormhole routing into the limited-resource optical networks.
[low channel utilization, telecommunication congestion control, routing delay, Optical fiber networks, optical fibre LAN, wormhole routed optical networks, synchronous flow control mechanism, Intelligent networks, Size control, source routing, routing path length, virtual channels, synchronized control slot, bits, Routing, deadlock-free routing, virtual nodes, Optical transmitters, Optical control, Image motion analysis, channel access, telecommunication network routing, limited-resource optical networks, Bidirectional control, data transmission, System recovery, unidirectional flow control, control information broadcast, buffer size requirement, High speed optical techniques]
A multicast communication system over Internet and its application
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
Many networking applications become feasible and required with the popular use of Internet. We present an Interactive Digital Radio Station, which is called IDRS, over Internet. Several architecture and synchronization methods are proposed to solve the communication problems that only occur during the digital radio multicasting. In order to perform the communication between DJ and audience, two operation modes, which are called the DJ mode and the Call-in mode, are defined. Related technique issues and the corresponding system development are presented in detail.
[digital radio, operation modes, Call-in mode, Laboratories, Spine, networking applications, Multicast communication, Application software, Interactive Digital Radio Station, Computer science, communication problems, multicast communication system, digital radio multicasting, Bandwidth, multicast communication, system development, Speech, Digital communication, synchronization methods, DJ mode, Internet, IP networks, Internet telephony, IDRS]
Object replication using version vector
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
In object-based systems, objects supporting abstract methods are replicated to increase the performance, reliability and availability. We discuss a novel object-based locking (OBL) protocol to lock replicas of objects by extending the quorum-based protocol for read and write to abstract methods. Unless two methods conflict, subsets of the replicas locked by the methods do not intersect even if the methods change the replicas. Methods not computed on a replica A but computed on another replica are computed on A when a method conflicting with the methods are issued to A in the OBL protocol. We newly propose a version vector to identify what methods are computed on a replica.
[Availability, Protocols, read, object-based locking protocol, software reliability, reliability, object replication, Reliability engineering, object-based systems, availability, quorum-based protocol, OBL protocol, performance, Systems engineering and theory, version vector, protocols, write, distributed object management, software performance evaluation, abstract methods]
An x86 load/store unit with aggressive scheduling of load/store operations
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
Because of register-memory instruction set architecture and limited register set, there are significant amounts of memory access instructions in x86 microprocessors. As the higher issue degree of superscalar microprocessor is provided, an aggressive scheduling policy of load/store operations becomes crucial. We examine the scheduling policies of loads/stores on x86 superscalar microprocessors and propose a new aggressive scheduling policy called load speculation, which allows loads to precede the previous unsolved pending stores. Simulation results show that the load speculation achieves the higher performance in comparison with the traditional scheduling policies such as load bypassing and load forwarding. Furthermore, by reducing the pipeline stages, the load speculation can achieve even higher performance.
[register memory, memory access instructions, simulation, Registers, Electrical capacitance tomography, parallel programming, Postal services, microprogramming, Analytical models, superscalar microprocessor, Microprocessors, instruction set, scheduling, load store operations, x86 load store unit, Contracts, software performance evaluation, load bypassing, instruction sets, Job shop scheduling, Memory architecture, load forwarding, microprocessor chips, x86 microprocessors, Processor scheduling, performance, load speculation, aggressive scheduling, register set]
PANDA: ring-based multiprocessor system using new snooping protocol
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
The PANDA is a ring-based Cache Coherent Non-Uniform Memory Access (CC-NUMA) multiprocessor system under implementation at the Seoul National University. Its main goal is to ameliorate the data miss latency by using the unidirectional point-to-point interconnection network. We introduce the PANDA architecture and present a new snooping protocol for this system. We evaluate the performance of the PANDA for a small to medium scale multiprocessor system using analytical models and a program-driven simulator. We compare the proposed system to other alternatives of point-to-point connected machines, such as the Express Ring and full map directory based system. The simulation results show up to 29% performance improvement against the Express Ring. They also show that the PANDA performs no worse than the full map directory based system, which has the additional hardware costs for the directory management.
[Access control, Costs, Multiprocessor interconnection networks, parallel architectures, multiprocessor interconnection networks, snooping protocol, Registers, analytical models, Delay, Multiprocessing systems, data miss latency, program-driven simulator, full map directory, computer architecture, Bandwidth, protocols, Cache Coherent Non-Uniform Memory Access, CC-NUMA, Access protocols, directory management, performance evaluation, Seoul National University, unidirectional point-to-point interconnection network, hardware costs, Pipeline processing, performance, distributed shared memory systems, ring-based multiprocessor system, Express Ring, PANDA]
Look-ahead memory consistency model
Proceedings 1998 International Conference on Parallel and Distributed Systems
None
1998
We propose a hardware-centric look-ahead memory consistency model that makes the data consistent according to the special ordering requirement of memory accesses for critical sections. The novel model imposes fewer restrictions on event ordering than previously proposed models thus offering the potential of higher performance. The architecture has the following features: blocking and waking up processes by hardware; allowing instructions to be executed out-of-order; until having acquired the lock can the processor allow the requests for accessing the protected data to be evicted to the memory subsystem. The advantages of the look-ahead model include: more program segments are allowed parallel execution; locks can be released earlier, resulting in reduced waiting times for acquiring locks; and less network traffic because more write requests are merged by using two write caches.
[Out of order, instructions, Telecommunication traffic, cache storage, memory access, parallel programming, storage management, program segments, ordering requirement, blocking, Traffic control, Hardware, Protection, software performance evaluation, merging, write requests, parallel execution, look-ahead memory consistency model, waking up processes, data integrity, data consistency, network traffic, write caches, performance, event ordering]
Environment for performing collaborative distributed virtual environments with QoS
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
The spread of virtual environments is creating a new set of challenges in the management of system resources for guaranteeing quality of service (QoS). This paper describes an implementation of end-user QoS control to a distributed virtual environment platform called DVECOM. In particular, we describe how to guarantee mandatory properties, such as synchronization and consistency, how to reduce the impact on the application in case of a sudden overload in the system, and how we master this degradation to guarantee a smooth degradation according to the end-user's requirements. This representation degradation is driven by the user's choices and profile-selected through the offered rendering strategy API. We also present our evaluation of the QoS implementation, describing some interesting results we achieved in a simulation of collaborative work in the system.
[virtual reality, profile selection, application program interfaces, simulation, Quality of service, distributed processing, Displays, Environmental management, consistency, Degradation, groupware, representation degradation, sudden system overload, collaborative work, Testing, Quality management, Virtual environment, Collaborative software, quality of service, DVECOM, smooth degradation, system resource mangement, synchronisation, collaborative distributed virtual environments, end-user requirements, mandatory property guarantees, rendering strategy API, Collaboration, end-user service quality control, Collaborative work, synchronization]
Pilgrim performance over a new CAliF communication layer
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
Previously, we defined CAliF (Cooperative Application Framework): a platform for cooperative work applications. First, the lower CAliF layers have been developed separately: the interprocess communication layer CAliFCom and the shared memory layer where the Pilgrim, a new consistency protocol, is implemented. The authors show the qualities of CaliFCom, and study Pilgrim performance over this interprocess communication layer. Results are promising and thus, the system allows transparent handling of data sharing.
[Quality of service, Communication system security, Pilgrim performance, interprocess communication layer, groupware, Broadcasting, shared memory systems, Cooperative Application Framework, Libraries, shared memory layer, transparent handling, Context, CAliFCom, message passing, performance evaluation, Multicast protocols, lower CAliF layers, Multicast algorithms, CAliF communication layer, Message passing, Memory management, cooperative work applications, Collaborative work, consistency protocol, data sharing]
Software agent-mediated confidential information gathering system
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
We propose a confidential information gathering system which employs a software agent traversing a list of information resource servers on the Internet to gather confidential information. Both software agent technology and cryptographic technology are applied to automate and secure the confidential information gathering process.
[Information resources, Humans, cryptography, cryptographic technology, software agents, information resource servers, Certification, security, software agent-mediated confidential information gathering system, automation, Public key, Information security, information retrieval systems, Public key cryptography, Software agents, data privacy, Internet, Digital signatures]
Optimal all-to-all personalized exchange in multistage networks
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
All-to-all personalized exchange is one of the most dense collective communication patterns and occurs in many important applications in parallel computing. Previous all-to-all personalized exchange algorithms were mainly developed for hypercube and mesh/torus networks. Although the algorithms for a hypercube may achieve optimal time complexity, the network suffers from unbounded node degrees and thus has poor scalability. On the other hand, a mesh/torus has a constant node degree and better scalability, but the all-to-all personalized exchange algorithms have higher time complexity. The authors propose an alternative approach to efficient all-to-all personalized exchange by considering another important type of network, multistage networks for parallel computing systems. We present a new all-to-all personalized exchange algorithm for a class of unique-path multistage networks. We first develop a generic method for decomposing all-to-all personalized exchange patterns into some permutations which are realizable in these networks and then present a new all-to-all personalized exchange algorithm based on this method. The newly proposed algorithm has O(n) time complexity for an n/spl times/n network, which is optimal for all-to-all personalized exchange.
[Multiprocessor interconnection networks, Scalability, Switches, mesh/torus networks, permutations, dense collective communication patterns, scalability, Intelligent networks, all-to-all personalized exchange patterns, optimisation, Network topology, Fast Fourier transforms, unique-path multistage networks, parallel computing systems, optimal time complexity, Broadcasting, Parallel processing, Hypercubes, parallel computing, parallel algorithms, hypercube, optimal all-to-all personalized exchange, unbounded node degrees, Routing, multistage interconnection networks, generic method, all-to-all personalized exchange algorithms, computational complexity]
A mobile agent-based active network architecture
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
Active networks enable the customization of network functionality without the lengthy standard-mediated committee processes. Most of the works in the literature utilize capsules or active packets as the means to transfer code information across active networks. In this paper, we propose an active network infrastructure based on mobile agent technologies. In our prototype implementation, mobile agents are the building blocks carrying functional customizations, and the active nodes offer software application layers, caled agent servers, to process mobile agent-specific customizations to facilitate network functionality. Both integrated and discrete operational models of network customizations are supported. In addition, for the application-specific protocol development and deployment, an abstract protocol structure and a protocol loading mechanism are presented. Furthermore, we provide an agent management/control mechanism and devise a protocol management/control mechanism. As a result, improved network functionality can be achieved.
[active networks, active nodes, Protocols, network servers, wide area networks, discrete operational models, abstract protocol structure, agent control mechanism, protocol management mechanism, Electronic mail, customized network functionality, code information transfer, agent management mechanism, protocol loading mechanism, Network servers, Mobile agents, Prototypes, Computer architecture, TCPIP, network customization, mobile agent-specific customizations, IP networks, protocols, mobile agent-based active network architecture, distributed programming, protocol control mechanism, integrated operational model, agent servers, active network infrastructure, software agents, Computer science, functional customizations, prototype implementation, software application layers, application-specific protocol development, Impedance]
eBroker: an agent-based query routing system for distributed E-commerce databases
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
With the rapid increase in the number of E-commerce Web sites, Internet users are now able to purchase items from the Web. Nevertheless, the large number of E-commerce Web sites also brings about the problem of identifying sites that sell the desired products wanted by a potential buyer. To avoid broadcasting the same queries on products to large set of databases maintained at different E-commerce Web sites, buyers can rely on a query routing system that matches the queries with the E-commerce database content. Much of the previous work in query routing has focused on databases that are document collections and queries that involve text attributes only. We propose a new database selection technique designed for distributed E-commerce databases. The technique deals with queries that involve attributes of different data types. We have incorporated the proposed technique into an agent-based query routing system known as eBroker. The performance of e-Broker for the E-commerce databases of 5 online CD stores is reported.
[information resources, online CD stores, Merging, World Wide Web, Information systems, database selection techniques, query processing, eBroker, agent-based query routing system, Query processing, distributed E-commerce databases, Distributed databases, Web pages, distributed databases, Search engines, Broadcasting, data types, Internet, Web search, electronic commerce, retail data processing, E-commerce Web sites]
Dynamic window-based traffic-smoothing for optimal delivery of online VBR media streams
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
Traffic-smoothing for delivery of online VBR media is one of the most important problems when designing streaming multimedia applications. Given the available client buffer b and playback delay D, Rexford et al. (1997) introduced a window-based approach called SLWIN(k) to smooth online generated traffic for the pre-specified window size W (W<D) and sliding distance k (k/spl les/W). The required time complexity is O(n*W/k) where n is the number of media frames. Note that, by using the maximum sliding distance W, SLWIN(W) has the minimum computation cost O(n). However, the required peak bandwidth is large. Although SLWIN(I) with the minimum sliding distance 1 can achieve small peak bandwidth, it requires O(n*W) computation cost. As there is a tradeoff between the required peak bandwidth and computation cost, it is hard to decide the best sliding distance k for the conventional static window-sliding SLWLN(R) method. In this paper, a novel dynamic window-sliding scheme is proposed. Our approach can dynamically adjust the sliding distance of window to minimize both the computation cost and the peak bandwidth for online traffic-smoothing. Given the same client buffer, playback delay and window size, our allocated peak bandwidth can be the same as that achieved by SLWIN(I). Besides, our computation cost is the same as that required for SLWIN(W).
[client buffer, playback delay, Multimedia systems, Telecommunication traffic, time complexity, Encoding, sliding distance, computation cost, Delay, streaming multimedia applications, Network servers, optimal online VBR media stream delivery, peak bandwidth, pre-specified window size, Layout, Bandwidth, dynamic window-based traffic-smoothing, Streaming media, Computational efficiency, Resource management, telecommunication traffic, multimedia communication, computational complexity]
Improved and extended scalable image coding by spline approximation for a binary, gray-scale and color image
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
The image coding method for a gray-scale image, which can reconstruct the high quality image against several transformations such as scaling by expressing the image signal as the shape of the mathematical function, are proposed. This method is also applicable to a binary image, but it has not been applicable to a color image; there are some problems of quality with this method. The paper attempts to improve the quality and to extend the preview method to a color image. For the quality improvements, the processes involved in region segmentation and approximation for low frequency components are reformed. For the extension to a color image, the process of region segmentation is particularly reformed again. The performance of the proposed method is verified by some experiments. Although the quality problems remain to a certain extent, it is much better than previous results. The extension to a color image proves successful. Moreover, this improvement and extension can parallelize the process more effectively.
[splines (mathematics), Shape, preview method, binary image, Gray-scale, quality improvements, gray-scale image, low frequency components, Spline, Image reconstruction, parallel programming, mathematical function, Information science, Image coding, high quality image, image segmentation, image colour analysis, image coding method, image signal, quality control, Color, extended scalable image coding, spline approximation, image reconstruction, region segmentation, parallelization, Image segmentation, Software quality, color image, Frequency, image coding, quality problems]
Does multicast communication make sense in write invalidation traffic?
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
In distributed shared memory (DSM) multiprocessors, a write operation requires multiple messages to invalidate the nodes which share and cache the memory block to be written. The resulting write stall time is a performance hurdle to such systems. One approach to efficient invalidation is to use multicast messages to reach the sharing nodes. We use application driven simulation to evaluate two multicast based invalidation schemes: dual path (X. Lin and L.M. Ni, 1993) and pruning (M.P. Malumbres et al., 1996). Based on our experimental settings, we found that multicast improves invalidation traffic for four of the six evaluated real applications. The remaining two programs are computation intensive, and multicast based validation is less effective. But since they induce bursty communication, we found that multicasts help to relieve the network congestion during those periods of time. Dual path performs a little better than pruning, because it is less sensitive to routing delay in the routers. We also found that cache size is an important design parameter for multicast based invalidation. It is more effective for DSM multiprocessors with large caches.
[write invalidation traffic, pruning, Scalability, routing delay, real applications, Multicast communication, cache storage, Distributed computing, Delay, write stall time, design parameter, multicast based invalidation schemes, Power system interconnection, multicast communication, sharing nodes, Computer networks, multicast messages, cache size, application driven simulation, multicast based invalidation, large caches, multiple messages, performance hurdle, message passing, Computational modeling, write operation, bursty communication, network congestion, Routing, dual path, invalidation traffic, Computer science, DSM multiprocessors, multicast based validation, Multicast algorithms, memory block, distributed shared memory multiprocessors, distributed shared memory systems]
Some mechanisms to improve TCP/IP performance over wireless and mobile computing environment
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
The Internet and wireless networks are two of the most important technical developments that have had a direct effect on the lives of people over the past few years. The next step is to combine these two technologies to provide wide-area wireless Internet access. Currently, TCP/IP (Transport Control Protocol/Internet Protocol) is the most important protocol that integrates a wide range of different physical networks into the global Internet. The environment of wireless networks violates many assumptions made by traditional TCP/IP. Compared with fixed and wired networks, wireless networks have a high bit-error rate (BER) and offer less available bandwidth. Furthermore, the host mobility also results in packet losses or delays during handoff procedures. Hence, the performance of TCP/IP over such wireless networks without any modification suffers significant degradation of throughput and high interactive delay. In this paper, we propose some schemes to improve the performance of TCP/IP over wireless networks according to their characteristics. We analyze the performance of TCP/IP under different network conditions by using ns-2 (Network Simulator version 2) to facilitate comparisons with previous studies. The simulation results show that our proposed mechanisms achieve better performance than other protocols.
[Transport protocols, network conditions, Bit error rate, wide-area wireless Internet access, digital simulation, telecommunication computing, Delay, Degradation, mobile computing, Wireless networks, TCPIP, Bandwidth, host mobility, handoff procedures, packet losses, IP networks, TCP/IP performance, bit-error rate, interactive delay, Network Simulator version 2, Access protocols, performance evaluation, wireless networks, LAN interconnection, available bandwidth, wireless computing environment, ns-2 simulator, transport protocols, Transport Control Protocol, mobile computing environment, Internet, Internet Protocol, wireless LAN]
The design and implementation of a WWW traffic generator
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
With the growing importance of the World Wide Web, Web managers are facing the problem of how to accurately measure the performance of their Web servers. This paper addresses this problem by introducing a traffic generator which is developed by using the Java language. The generator generates Web workload by using an analytic model which models the arrivals at the user level. It is both self-scaling and self-configuring, meaning that it can scale the traffic to any intensities and configure the arrival characteristics to conform to those of any specific Web site. The latter is done by analyzing a site's access log for adjusting the model parameters being used during traffic generation. In contrast to other traffic generator being used in common Web benchmark software, the real content of the target Web server is used as the testing file set instead of just using a reduced standard file set. Experimental results on using the generator to test two Web servers are also presented. The results show that the generator is both functional and useful to measure the performance of Web servers under real situations.
[Software testing, information resources, Web workload, Java, search engines, arrival modelling, Web benchmark software, World Wide Web, HTML, Web servers, performance measurement, Engineering management, Web site access log, Traffic control, Benchmark testing, Java language, testing file set, analytic model, Web sites, Web server, Monitoring, telecommunication traffic, WWW traffic generator]
Flexible multimedia system architecture with adaptive QoS guarantee functions
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
Proposes a flexible multimedia system (FMS) which is based on an agent-oriented architecture. The system is able to organize the required functions itself, and to provide both real-time and stored multimedia information services simultaneously to users, even though various users' multimedia environments and the resource utility of the computers or networks have been dynamically changed. This distributed multimedia system not only integrates various multimedia information distributed over computer networks but it also provides that information to the users in accordance with their requirements during real-time multimedia communication which must guarantee the user-requested quality of service (QoS), even though the computer and network resources may change statically or dynamically. This paper describes the FMS architecture for a multimedia teleconferencing service with adaptive QoS-guarantee functions, and designs the FMS so as to implement a prototype system. In this teleconferencing service, the FMS is able to realize one of the multimedia communication services flexibly for service requests and QoS requests from users.
[Real time systems, Adaptive systems, Quality of service, Multimedia communication, Distributed computing, teleconferencing, distributed multimedia system, flexible multimedia system architecture, software architecture, user requirements, Computer architecture, distributed databases, Computer networks, Flexible manufacturing systems, multimedia communication, real-time information services, agent-oriented architecture, Multimedia systems, multimedia teleconferencing service, multimedia databases, computer networks, service requests, quality of service, resource utility, information services, software agents, adaptive systems, dynamic changes, multimedia environments, Teleconferencing, real-time multimedia communication, user-requested service quality, multimedia communication services, prototype system, adaptive QoS guarantee functions, stored multimedia information services, self-organizing system]
Multi-node broadcasting in an arrangement graph using multiple spanning trees
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
The arrangement graph A/sub n,k/ is not only a generalization of the star graph (n-k=1), but is also more flexible. Designing an efficient routing algorithm on a regular interconnection network is a fundamental issue for parallel processing techniques. We elucidate the multi-node broadcasting problem in an all-port communication model on the arrangement graph. Our routing strategy is proposed by construction of 2(n-k) spanning trees, where the height of each spanning tree is 2k-1. We also extend the routing strategy to a one-to-all broadcasting algorithm. Using 2(n-k) spanning trees allows us to present efficient (one/multi)- node broadcasting algorithms in the arrangement graph. The arrangement graph is assumed to use one-port and all-port models and packet-switching (or store-and forward) technique. Moreover we show that our (one/multi)-node broadcasting algorithms outperform previous results in the literature. This is justified by our performance analysis.
[Algorithm design and analysis, Multiprocessor interconnection networks, multiprocessor interconnection networks, packet switching, parallel processing techniques, parallel processing, Engines, Fault tolerance, routing strategy, Tree graphs, star graph, regular interconnection network, Broadcasting, tree data structures, multi-node broadcasting problem, arrangement graph, packet-switching, network routing, store-and forward, trees (mathematics), Routing, one/multi node broadcasting algorithms, multi-node broadcasting, Statistics, broadcasting, Computer science, Casting, multiple spanning trees, routing algorithm, spanning tree, one-to-all broadcasting algorithm, all-port communication model, performance analysis]
Fault-free Hamiltonian cycles in faulty butterfly graphs
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
Butterfly graphs were originally defined as the underlying graphs of fast Fourier transform (FFT) networks, which can perform the FFT very efficiently. Since butterfly graphs are regular, of degree four, they can tolerate at most two edge faults in the worst case in order to establish a Hamiltonian cycle. In this paper, we show that a butterfly graph contains a fault-free Hamiltonian cycle even if it has two random edge faults.
[fast Fourier transforms, Embedded computing, faulty butterfly graphs, fault tolerance, graph theory, regular graphs, switching theory, fault-free Hamiltonian cycle, fast Fourier transform, Computer science, Intelligent networks, Fast Fourier transforms, Network topology, Computer networks, random edge faults, FFT networks, Indexing, Binary sequences, Arithmetic]
Performance evaluation of location information distribution strategies for mobility tracking
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
A user location information distribution strategy is proposed for mobile systems. Local anchors (LAs), replicated databases and forwarding pointers are applied in an integrated way that greatly reduces the location update rate and the call delivery cost incurred by location tracking. In particular, the visitor location registers (VLRs) associated with a user's home and workplace are selected as the user's LAs, which are connected by a forwarding pointer chain. When the user changes their registration area (RA) within a local signal transfer point (LSTP) region, the new location is reported to the home location register (HLR) and to the replicas. In the call delivery procedure, if a call orginates from one of the LAs or replicas, the HLR does not need to be queried. An analytic model is developed and numerical examples are given to compare the performance of the proposed scheme, the standard scheme, the replication scheme and the local anchoring scheme. The results show that the proposed strategy outperforms the other three strategies over a wide range of system parameters.
[mobility tracking, home location register, user workplace, Costs, local anchors, mobile systems, Distributed computing, Relays, tracking, local signal transfer point region, mobile computing, Databases, Employment, forwarding pointer chain, visitor location registers, registration area, Performance analysis, analytic model, Standards development, Informatics, GSM, Distribution strategy, replicated databases, location update rate, performance evaluation, system parameters, user home, user location information distribution strategy, call delivery cost, user location tracking]
A communication-induced checkpointing algorithm using virtual checkpoint on distributed systems
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
Checkpointing is a fault-tolerant technique for restoring faults and restarting jobs quickly. The algorithms for checkpointing on distributed systems have been under study for years. These algorithms can be classified into three types: coordinated, uncoordinated and communication-induced algorithms. In this paper we propose a new communication-induced checkpointing algorithm that has a minimum checkpointing count equivalent to the periodic checkpointing algorithm, and relatively short rollback distance at fault situations. The proposed algorithm is compared with the previously proposed communication-induced checkpointing algorithms with simulation results. In the simulation, the proposed algorithm produces better performance than other algorithms in terms of task completion time in both fault-free and fault situations.
[Checkpointing, Terminology, virtual checkpoint, Communication system control, simulation, distributed processing, system recovery, Degradation, rollback distance, Fault tolerant systems, task completion time, virtual machines, distributed systems, Hardware, fault tolerant computing, communication-induced checkpointing algorithm, Force control]
Web operation recorder and player
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
This paper describes mechanisms for recording and playing back Web browser operations. A recorder detects a user's operations on a Web browser and saves them as an event sequence called a scenario. A player plays back the scenario by controlling an actual Web browser. In addition, the recorder and player allow a user to add explanations to existing HTML contents by making "ink" annotations and attaching text, images and hyperlinks on a Web page. The recorder and player not only enable a Web site owner to gather users' behavior at his or her Web site, but also make it easy to create a Web-based automatic presentation scenario, which can be played back later. The recorder and player run in a Java-enable Web browser. Users do not have to prepare the Web contents to be recorded; they can work with ordinary Web pages.
[Computer interfaces, information resources, Java, event sequence, images, Event detection, Laboratories, Web-based automatic presentation scenario, HTML, Web browser operation playback, Information analysis, Uniform resource locators, Java-enable Web browser, scenario, Web pages, online front-ends, HTML contents, User interfaces, Web browser operation recording, text, Web site, Joining processes, hyperlinks, hypermedia markup languages]
Frequency-based selective caching strategy in a continuous media server
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
We propose a frequency-based selective caching scheme where the buffer allocation unit varies with the access frequency of the requested object. With this scheme, buffer allocation works either at the distance level or object level. Once a request is admitted, it is sent to the disk(s) directly if the average arrival rate for the requested object does not exceed the marginal arrival rate. A distance between the requested stream and the preceding one accessing the same object is cached as a function of the length of the distance and the size of the buffer cache. An object is cached in its entirety, i.e., its caching mode can be changed into object mode if the average distance among the current streams for that object exceeds the average distance of the total number of current cached streams in the buffer cache. Our conclusion derived from the simulation is that our proposed caching scheme is more effective than other caching policies such as LRU and interval when it is applied to continuous media data.
[continuous media server, access frequency, buffer allocation, Video on demand, multimedia servers, Buffer storage, marginal arrival rate, Fitting, simulation, buffer allocation unit, Cache storage, cache storage, Radio spectrum management, Computer science, Network servers, virtual machines, Streaming media, Frequency, frequency-based selective caching strategy, average arrival rate]
Awareness modelling and its application in cooperative network management
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
Although communication among the collaborating workers in business processes has always been an important issue to business organisations, there is no measure for the effectiveness of collaboration. This paper presents a model based on "awareness levels" to measure the level of cooperation amongst different human roles in a business process. The hypothesis is that collaboration among various roles within an enterprise environment is improved by maintaining the awareness levels of individuals at its desired level. This paper illustrates the awareness model through its application in a large telecommunication service provider organisation. According to this study, the awareness model seems to be able to explain some problems of cooperation in this business environment. Since the model is abstract in nature, it can be applied in different organisations and businesses.
[awareness levels, large telecommunication service provider organisation, Humans, collaborating worker communication, business processes, DP management, Information management, Environmental management, Business communication, Intelligent networks, Technology management, computer network management, Collaboration, Management information systems, groupware, Collaborative work, cooperative network management, enterprise environment, human roles, Computer network management, business data processing, awareness modelling]
Dynamic multicast routing in advance resource reservation environment
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
Since network resources are usually limited, the ability to carry out resource reservation in advance, as well as efficient dynamic multicasting when destination nodes can join and leave the group during the communication period, are essential in all distributed multi-party applications. Total or partial re-routing of a multicast tree for dynamic multicast groups can optimize the tree cost to some extent but they are complex. Furthermore, optimal multicast routing has been proved to be an NP-complete problem. In this paper, we propose an efficient dynamic multicast routing algorithm, which optimizes the total cost of the Steiner tree over the whole session period in an advance resource reservation environment, where information about the resource reservation (i.e. leaving times of participants at the time of joining) are available. The efficiency of our algorithm compared to other existing algorithms is shown by various simulation results.
[Protocols, total multicast tree rerouting, tree cost optimization, Heuristic algorithms, trees (mathematics), dynamic multicast routing, simulation, Quality of service, destination nodes, Routing, Electronic mail, NP-complete problem, Information technology, Information science, Multicast algorithms, resource allocation, advance resource reservation environment, Steiner tree, telecommunication network routing, multicast communication, distributed multi-party applications, network resources, Cost function, partial multicast tree rerouting]
A practical nonblocking queue algorithm using compare-and-swap
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
Many nonblocking algorithms have been proposed for shared queues. Previous studies indicate that link-based algorithms perform best. However, these algorithms have a memory management problem: a dequeued node cannot be freed or reused without proper handling. The problem is usually overlooked; one just assumes the existence of a lower level mechanism, which takes care of all the details of a lower level mechanism, which takes care of all the details of handling the problem. Employing such a mechanism incurs significant overheads, and consequently the link-based queues may not perform as well as claimed. A new non-blocking queue algorithm based on a finite array is proposed. Compared with the link-based algorithms, the new algorithm provides the same degree of concurrency without being subject to the memory problem, hence suggests a good performance.
[parallel algorithms, Costs, queueing theory, Interference, Data structures, concurrency theory, Delay, shared queues, concurrency, Concurrent computing, Computer science, Degradation, finite array, Councils, nonblocking queue algorithm, Memory management, System recovery, compare-and-swap]
Group protocol for quorum-based replication
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
Distributed applications are realized by cooperation of multiple objects. Objects in the systems are replicated to make the systems fault-tolerant. Read and write request messages are issued to the replicas in a quorum based scheme. A quorum based ordered relation among request messages is defined to make replicas consistent. We discuss a group protocol which supports a group of replicas with the quorum based ordered delivery of request messages.
[quorum based replication, Protocols, group protocol, multiple object cooperation, quorum based scheme, Distributed computing, distributed applications, fault-tolerant systems, write request messages, Network servers, quorum based ordered delivery, Computer network reliability, Fault tolerant systems, request messages, request message delivery, Computer networks, protocols, Availability, client-server systems, message passing, object replication, concurrency theory, Maintenance, Application software, quorum based ordered relation, Systems engineering and theory, fault tolerant computing, consistent replicas]
Object-oriented real-time distributed programming and support middleware
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
The object oriented (OO) distributed real time (RT) programming movement started in the 1990s and is growing rapidly at this turn of this century (year 2000). The motivations are reviewed and then a brief overview is given of the particular programming scheme which the author and his collaborators have been establishing. The scheme is called the time triggered message triggered object (TMO) programming scheme and it is used to make specific illustrations of the issues and potentials of OO RT programming. The desirable features of middleware providing execution support for OO RT distributed programs are then discussed. The issue of fault tolerant execution of distributed RT objects and that of RT distributed/parallel simulation are also discussed.
[object oriented real time distributed programming, execution support, digital simulation, fault tolerant execution, Distributed computing, Fault tolerance, distributed RT objects, Quantum computing, OO RT programming, OO RT distributed programs, Object oriented programming, distributed programming, distributed object management, client-server systems, object-oriented programming, TMO programming scheme, RT distributed/parallel simulation, support middleware, Computational modeling, Object oriented modeling, Application software, Middleware, bibliographies, software fault tolerance, real-time systems, time triggered message triggered object programming scheme, Computer applications, programming scheme, Software engineering]
The specification and implementation of a virtual university software system
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
Distance learning is one of the interesting research topics of distributed computing. This paper discusses a joint research project between the University of Aizu in Japan and Tamkang University in Taiwan. A software system supporting virtual university operations is proposed. The software architecture is designed based on three criteria of virtual university operations: administration, awareness and assessment. Specifications of each tools in the system are proposed, with some implementation details and solutions being discussed. We also point out some interesting research directions in the realization of a virtual university.
[educational computing, administration, multimedia systems, distributed processing, software system implementation, World Wide Web, multimedia education, software system specification, Videoconference, teleconferencing, formal specification, distributed computing, Postal services, software architecture, assessment, awareness, Tail, virtual university, joint research project, Multimedia systems, Educational technology, Information retrieval, group communication, distance learning, Computer aided instruction, Software systems, Internet, video conferencing]
An algorithm for distributed hierarchical diagnosis of dynamic fault and repair events
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
The components of a fault-tolerant distributed system must be capable to accurately determine which components of the system are faulty and which are fault-free. In this paper, we present a new distributed algorithm for event diagnosis in fully-connected networks. An event is defined as a faulty node becoming fault-free, or vice versa. Previous hierarchical algorithms considered a static fault situation, in which an event can only occur after a previous event has been fully diagnosed. The new algorithm is capable of achieving the diagnosis of dynamic events as long as the nodes stay in a given state for a period of time long enough for all testers to detect that state. Each node running the algorithm keeps a timestamp for the state of each other node in the system. This timestamp is implemented as a counter, which is incremented every time a node changes its state. In this way, each tester may obtain information about a given node in the system from more than one tested node without causing any inconsistencies, i.e. without taking an older state for a newer one. Nodes run a hierarchical testing strategy, which is a hypercube when all nodes are fault-free. When a fault-free node is tested, the tester gets diagnostic information about N/2 nodes for a system of N nodes. In spite of the overhead of keeping and transferring timestamps, the new algorithm significantly reduces the average latency when compared to other similar approaches, presenting a new option for practical diagnosis implementation.
[System testing, faulty component determination, Adaptive systems, fault diagnosis, Event detection, dynamic repair events, latency, Delay, Counting circuits, Fault diagnosis, event diagnosis, Fault tolerant systems, fault-free nodes, node state, dynamic fault events, Informatics, Distributed algorithms, dynamic events, Local area networks, timestamp, hierarchical algorithms, hypercube, distributed hierarchical diagnosis, overhead, hierarchical testing strategy, counter, distributed algorithm, distributed algorithms, fault-tolerant distributed system, fault tolerant computing, fully-connected networks]
An efficient algorithm for processing distributed queries using partition dependency
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
Presents an efficient algorithm for processing distributed queries with the existence of partition dependencies. For a given query, the algorithm first partitions the referenced relations into a number of non-exclusive subsets such that the join operation(s) associated with the relations in the subset can be locally processed without data transfer. Each subset is associated with a set of processing sites and can be used to generate an execution plan for the given query. Then, the algorithm determines a set of referenced fragmented relations that are not in the subset, such that only the fragments (instead of the whole relation) need to be replicated at the processing sites. The other referenced relations are duplicated at each of the processing sites. Among the alternatives, the algorithm picks the plan that gives the minimum response time for the query. Experimental results show that our algorithm improves the performance of distributed query processing significantly.
[referenced relations, Random access memory, distributed query processing algorithm, Delay, fragment replication, Information systems, join operations, query processing, minimum response time, Databases, Computer architecture, distributed databases, duplicated relations, local processing, referenced fragmented relations, execution plan generation, Partitioning algorithms, database theory, partition dependency, Computer science, nonexclusive subsets, Disk drives, performance, Query processing, distributed algorithms, data transfer, processing sites, Qualifications]
A competence-based scheduling method for Web computing
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
We propose a heuristic scheduling algorithm, called Competence Based Method (CBM), to dynamically distribute workload for the task allocation problem in a Web/Java based computing environment. CBM is designed to meet four properties of Web computing: heterogeneity, scalability, centralization, and non-dedication host. CBM can be applied to both independent and dependent links scheduling problems. Compared with the competing algorithms, the experimental results show that the proposed CBM performs faster in parallel time and is close to the optimal.
[parallel time, Heuristic algorithms, Scalability, task allocation problem, Security, Distributed computing, parallel programming, CBM, Concurrent computing, dynamic distribution, heuristic programming, resource allocation, heuristic scheduling algorithm, scheduling, dependent links scheduling problems, information resources, Java, Web/Java based computing environment, workload, non-dedication host, competence based scheduling method, Computer science, Processor scheduling, Web computing, competing algorithms, Time sharing computer systems, Load management, Competence Based Method]
System requirements and formal specifications of hierarchical reactive systems
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
A methodology for the description of system requirements and formal specifications of reactive systems and the synthesis of formal specifications is presented. Based on a hierarchical structure of system properties, a hierarchical assertion language is used as a requirement language and hierarchical state transition systems are used as formal specifications. Sound and complete formal specifications are synthesized from system requirements automatically. Modularity and reusability are supported by the introduction of requirement and specification modules and a partial order relation over these modules. The methodology has a practical significance because desired specifications of reactive systems can be derived or synthesized from user requirements on system functions in a systematic and stepwise way.
[hierarchical assertion language, reusability, completeness, formal specification, modularity, user requirements, system functions, Prototypes, Broadcasting, system requirements, specification modules, Software prototyping, hierarchical reactive systems, soundness, Reflection, Formal specifications, formal specifications, hierarchical state transition systems, requirement modules, Computer science, requirement language, partial order relation, real-time systems, software reusability, subroutines]
Projecting the performance of decision support workloads on systems with smart storage (SmartSTOR)
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
Recent developments in both hardware and software have made it worthwhile to consider embedding intelligence in storage to handle general-purpose processing that can be off-loaded from the hosts. In particular, low-cost processing power is now widely available and software can be made robust, secure and mobile. In this paper, we propose a general smart storage (SmartSTOR) architecture in which a processing unit that is coupled to one or more disks can be used to perform such off-loaded processing. A major part of the paper is devoted to understanding the performance potential of the SmartSTOR architecture for decision support workloads. Our analysis suggests that there is a definite performance advantage in using fewer but more powerful processors, a result that bolsters the case for sharing a powerful processor among multiple disks. As for software architecture, we find that the off-loading of database operations that involve only a single relation is not very promising. In order to achieve significant speed-up, we have to consider the off-loading of multiple-relation operations. In general, if embedding intelligence in storage is an inevitable architectural trend, we have to focus on developing parallel software systems that can effectively take advantage of the large number of processing units that will be in the system.
[SmartSTOR architecture, offloaded processing, parallel software systems development, artificial intelligence, robust secure mobile software, Embedded software, software architecture, Databases, embedded systems, embedded intelligence, Computer architecture, decision support workload peformance, Hardware, Robustness, Performance analysis, software performance evaluation, storage units, powerful processors, processor sharing, disk-couped processing unit, speedup, Microelectronics, Application software, Sun, decision support systems, Computer science, memory architecture, multiple-relation operations, smart storage systems]
Recover-x: an adaptive router with limited escape channels
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
In order to improve network performance, a variety of adaptive routing algorithms have been proposed. Recent research focuses on their implementation costs, as well as the performance to enhance their practical applications. The paper proposes the Recover-x adaptive router, which limits escape message candidate in a blocked or deadlocked configuration. This limitation simplifies the routing logic and offers a chance to balance the usage between adaptive and non-adaptive channels. The cost and performance of four wormhole routers based on Verilog-HDL designs were compared. Synthesis results for the chosen gate array technology show that the Recover-x router attains a fast operating speed and low-latency, with high-bandwidth communication performance.
[limited escape channels, Costs, Virtual colonoscopy, multiprocessor interconnection networks, hardware description languages, Recover-x, Delay, network performance, Space technology, adaptive router, gate array technology, Logic, deadlocked configuration, Verilog-HDL designs, network routing, routing logic, adaptive routing algorithms, operating speed, Routing, adaptive systems, non-adaptive channels, high-bandwidth communication performance, escape message candidate, wormhole routers, System recovery, Network synthesis, Resource management, Hardware design languages]
Parallel spatial joins using grid files
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
The most costly spatial operation in spatial databases is spatial join which combines objects from two data sets based on spatial predicates. Even if the execution time of sequential processing of a spatial join has been considerably improved, the response time is far from meeting the requirements of interactive users. In this paper, we have developed two kinds of parallel spatial join algorithms based on grid files: a parallel spatial join using a multi-assignment grid file and a parallel spatial join using a single-assignment grid file. We also present the cost of the two join algorithms in terms of the number of MBR comparisons. The experimental tests on the MIMD parallel machine with shared disks show that the first join algorithm based on disjoint decomposition of a data space outperforms the second based on non-disjoint decomposition.
[Algorithm design and analysis, parallel algorithms, Costs, parallel spatial join algorithms, spatial predicates, MIMD parallel machine, parallel databases, Parallel machines, visual databases, nondisjoint decomposition, Educational institutions, Spatial databases, single-assignment grid file, multi-assignment grid file, Spatial indexes, shared disks, Delay, spatial databases, interactive users, Analytical models, Parallel processing, data sets, disjoint decomposition, Performance analysis]
Java 2 distributed object models performance analysis, comparison and optimization
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
The paper focuses on the performance analysis, comparison and optimization of the most important distributed object models for Java: RMI (Remote Method Invocation) and RMI-IIOP (Remote Method Invocation over Internet Inter-ORB Protocol). The paper presents the following contributions to the research on distributed object performance. First, a detailed performance analysis of both models is provided with the in-depth comparison. These results help to understand how the models perform. Second an overhead analysis and bottleneck identification is presented with an explanation of why there are differences in performance. Third optimization and the results for performance improved post-beta RMI-IIOP versions are presented. These show considerably better performance in all areas compared to the original beta release, with RMI-IIOP having equivalent or better performance to RMI in almost all cases.
[beta release, Protocols, post-beta RMI-IIOP versions, application program interfaces, Scalability, Optimization methods, Delay, Java 2 distributed object models, RMI-IIOP, Remote Method Invocation over Internet Inter-ORB Protocol, Network servers, overhead analysis, Performance analysis, distributed object management, Java, Object oriented modeling, performance evaluation, RMI, distributed object performance, Remote Method Invocation, Sockets, remote procedure calls, Internet, bottleneck identification, performance analysis]
An associative parallel algorithm for finding a critical cycle in directed graphs
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
In this paper, we propose a novel associative parallel algorithm for selecting a critical cycle in directed weighted graphs by means of an abstract model of the SIMD type with vertical data processing (the STAR-machine). This problem arises when performing J. Edmonds' (1967) algorithm for finding optimum branchings. This algorithm is represented as the corresponding STAR procedure whose correctness is verified and whose time complexity is evaluated.
[optimum branchings, parallel algorithms, critical cycle selection, time complexity, Data processing, Mathematics, Graph theory, Electronic mail, Application software, STAR procedure, Parallel algorithms, Geophysics computing, SIMD-type abstract model, Concurrent computing, associative processing, Tree graphs, directed weighted graphs, directed graphs, STAR-machine, Polynomials, correctness verification, vertical data processing, associative parallel algorithm, computational complexity]
Checkpointing protocol for object-based systems
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
Object-based checkpoints are consistent in an object-based system but may be inconsistent according to the traditional message-based definition. We present a protocol for taking object-based checkpoints among objects. An object to take a checkpoint in the traditional message-based protocol does not take one if the current checkpoint is object-based consistent with the other objects. The number of checkpoints can be reduced by the object-based protocol.
[Checkpointing, Protocols, object-oriented programming, object-based checkpoints, Systems engineering and theory, message-based protocol, protocols, checkpointing protocol, Yarn, system recovery, distributed programming]
A synchronization and flow control scheme for interactive multimedia-on demand (MOD) systems
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
Synchronization control with VCR-like user interactions in the multi-stream environment is very complicated and difficult. The main reasons are that: (i) those media units stored in the client buffers and flowing in the networks may be either useful or useless when the user interaction is issued, and (ii) the servers' media transmission should be adjusted according to the presentation status of the client. Additionally, different VCR-like user interactions have their own processing considerations, which result in different control schemes for different VCR-like user interactions. We propose some control schemes to handle interactive multimedia communication in the distributed multi-stream environment and develop an MOD system on SUN SPARC workstations accordingly.
[interactive multimedia-on-demand systems, interactive multimedia communication, distributed multi-stream environment, Buffer storage, MOD system, multimedia systems, Control systems, user interaction, user interfaces, processing considerations, SUN SPARC workstations, Network servers, synchronization control, multi-stream environment, interactive systems, flow control scheme, Workstations, media transmission, media units, control schemes, presentation status, Multimedia systems, Sun, client buffers, synchronisation, Graphics, Communication channels, Streaming media, VCR-like user interactions, Internet]
The net disk architecture for dynamic load balancing among disk arrays
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
A disk array is a proposed approach for a high-performance I/O subsystem. It requires dynamic load balancing against varying accesses to prevent some disks from becoming bottlenecks. Thus, we connected the disks by a bus and developed a local migration strategy to migrate data items with a high access frequency within the disk array. However, the bus may become a bottleneck when too many disks are connected to it. To solve this problem, we propose a net disk architecture, which contains multiple disk arrays connected by a crossbar network. Based on the load balancing within each disk array, a global migration strategy is developed to maintain the load balancing among the disk arrays. Therefore, load balancing is established for the entire architecture. Compared to conventional data reallocation techniques that are performed as a background process, dynamic load balancing is effectively maintained even though the disk arrays are burdened with a heavy load by the proposed architecture.
[storage allocation, Temperature, Communication system control, disk arrays, local migration strategy, bottlenecks, Data mining, RAID, data reallocation techniques, bus connection, net disk architecture, high-performance I/O subsystem, Information science, resource allocation, crossbar network, Computer architecture, Broadcasting, Load management, Frequency, global migration strategy, Neck, Joining processes, dynamic load balancing, data access frequency]
Formal modeling and analysis of atomic commitment protocols
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
The formal specification and mechanical verification of an atomic commitment protocol (ACP) for distributed real-time and fault-tolerant databases is presented. As an example, the non-blocking ACP of Babaoglu and Toueg (1993) is analyzed. An error in their termination protocol for recovered participants has been detected. We propose a new termination protocol which has been proved correct formally. To stay close to the original formulation of the protocol, timed state machines are used to specify the processes, whereas the communication mechanism between processes is defined using assertions. Formal verification has been performed incrementally: adding recovery from crashes only after having proved the basic protocol. The verification system PVS was used to deal with the complexity of this fault-tolerant protocol.
[Real time systems, complexity, Protocols, distributed fault-tolerant databases, atomic commitment protocols, Electronic mail, crashes, recovery, Distributed computing, formal specification, system recovery, Fault tolerance, formal verification, Fault tolerant systems, Distributed databases, distributed databases, mechanical verification, formal analysis, assertions, protocols, Computer crashes, Time measurement, software fault tolerance, formal modeling, timed state machines, real-time systems, communication mechanism, distributed real-time databases, termination protocol, Formal verification]
Fast and cost effective cache invalidation in DSM
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
Most distributed shared memory systems use point-to-point networks in conjunction with directory-based cache coherence protocols. A cache invalidation transaction generates a number of unicast invalidation messages and as many acknowledgment messages. This results in heavy network traffic, high latency, and high occupancy at home nodes. This paper introduces a fast cache invalidation method, called collective cache invalidation (CCI), and its simple and cost effect implementation method, called virtual bus based collective cache invalidation (VCCI). The simulation results show that we can reduce the total network traffic up to 45% and the overall execution time up to 11% by VCCI. The proposed method keeps the system scalable-the growth rate of the implementation cost if O(N/spl radic/N), and, VCCI can reduce the complexity of coherence protocol and make directory controllers simple since it does not require acknowledgment messages.
[virtual bus based collective cache invalidation, Costs, Protocols, Scalability, simulation, Telecommunication traffic, unicast invalidation messages, acknowledgment messages, latency, cache storage, directory controllers, Delay, Unicast, Bandwidth, Broadcasting, Traffic control, Hardware, point-to-point networks, protocols, collective cache invalidation, directory-based cache coherence protocols, home node occupancy, cache invalidation transaction, network traffic, execution time, virtual machines, distributed shared memory systems]
An optimized dependence convex hull partitioning technique to maximize parallelism of nested loops with non-uniform dependences
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
There are many methods existing for nested loop partitioning; however, most of them perform poorly when partitioning loops with non-uniform dependences. This paper proposes a generalized and optimized loop partitioning mechanism to exploit parallelism from nested loops with non-uniform dependences. Our approach, based on dependence convex theory, divides a loop into variable-size partitions. Furthermore, the proposed algorithm partitions a nested loop by using the copy-renaming and optimized partitioning techniques in order to minimize the number of parallel regions of the iteration space, outperforming other previous mechanisms for partitioning nested loops with non-uniform dependences.
[Performance evaluation, optimized partitioning technique, program control structures, generalized loop partitioning mechanism, Data analysis, optimising compilers, parallelising compilers, copy-renaming technique, dependence convex theory, Partitioning algorithms, variable-size partitions, parallel programming, Computer science, Program processors, Upper bound, nested loop parallelism maximization, performance, Parallel processing, nonuniform dependences, convex hull partitioning technique, iteration space, Pattern analysis, parallel region minimization]
Management of environments in 2K
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
Computer users are increasingly multi-device equipped and no longer sedentary. It is desirable that the execution environment in any of these devices be customized to the user preferences and to the device characteristics. This paper describes a framework for managing execution environments in 2K, an adaptable, distributed, network-centric, user- and application-oriented operating system aimed at accommodating change. A 2K environment is a container of components, devices and configuration parameters and provides an execution context for the user within the 2K distributed system. A user has a distributed execution environment that consists of several subenvironments running on different platforms or temporarily suspended to be resumed later. The management of the execution environments is designed to provide a user-centric view of the system; it facilitates user mobility, by liberating users from the restriction of being explicitly attached to specific platforms, and by seamlessly recruiting resources where they are available.
[components, Containers, Environmental management, Distributed computing, Recruitment, execution environment customization, mobile computing, execution environment management, Operating systems, network operating systems, distributed operating system, user preference, 2K, Workstations, Personal digital assistants, application-oriented operating system, adaptable operating system, configuration parameters, Computer science, network-centric operating system, user-oriented operating system, user mobility, management of change, Resource management, Computer network management]
Implementation techniques and an object group service for CORBA-based applications in the field of parallel processing
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
We examine the suitability of CORBA based solutions for meeting application requirements in the field of distributed parallel programming. We outline concepts defined within CORBA which are helpful for the development of parallel applications, and we describe which programming techniques are at hand for this purpose. Subsequently, we present an object group service which facilitates the development of CORBA based distributed and parallel software applications. Moreover, we introduce some basic ideas of how the Unified Modeling Language (UML) can be used for modeling parallel applications.
[Unified modeling language, CORBA based applications, parallel application modeling, parallel processing, parallel programming, application requirements, Computer architecture, specification languages, programming techniques, Parallel processing, Web server, distributed object management, parallel software applications, Java, implementation techniques, Unified Modeling Language, Object oriented modeling, Application software, Middleware, Computer languages, CORBA based solutions, Parallel programming, object group service, UML, distributed parallel programming]
A stateless QoS signaling protocol for the Internet
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
We describe a simple protocol that enhances the communication between end nodes and "the network". Other than the majority of QoS signaling systems, it achieves scalability by avoiding per-flow state. We also show how it can be used to decrease an adaptive multimedia application's packet loss ratio.
[Protocols, Welding, packet switching, Quality of service, Electronic mail, losses, scalability, Web and internet services, Bandwidth, Traffic control, stateless signalling protocol, IP networks, protocols, multimedia communication, network-node communication enhancement, service quality, adaptive multimedia application, telecommunication signalling, quality of service, adaptive systems, packet loss ratio reduction, Communication system signaling, per-flow state avoidance, Performance loss, Internet]
Software interoperability of telemedicine systems: a CSCW perspective
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
Computer supported cooperative work (CSCW) provides a fusion of the understanding of organisational processes with communication technologies. Telemedicine involves an integration of networking technologies with health-care processes. Since different modalities of patient care require applications running on heterogeneous computing environments, interoperability is a major issue in telemedicine. Software interoperability provides two distinctly classified benefits-benefits for the users of the system and benefits to the development and maintenance of the system. Software interoperability between different applications can be modeled at different levels of abstractions can be modeled at different levels of abstractions such as physical interoperability and semantic interoperability. Various mechanisms exist to resolve the problem at different levels. This paper presents the design issues of an interoperable. CSCW system in a distributed health-care environment through an illustrative study in the area of telecardiology.
[Heart, distributed health care environment, semantic interoperability, open systems, Medical services, Communication standards, networking technologies, heterogeneous computing environment, Telemedicine, Physics computing, Management information systems, groupware, Electrocardiography, Communications technology, computer supported cooperative work, telemedicine, software interoperability, biomedical communication, Application software, patient care, software maintenance, cardiology, physical interoperability, telecardiology, Collaborative work, CSCW, medical computing, maintenance, health care processes]
ORION: an adaptive home-based software distributed shared memory system
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
The authors describe a multithreaded software distributed shared memory, (DSM) system named Orion. It has been developed to provide a POSIX-thread (pthread) like interface. We believe this will avoid creating another unique set of application programming interfaces and ease the porting of pthread programs to a distributed environment. Orion implements a home based consistency model. We also present 2 adaptive schemes for home based DSM systems: home migration and dynamic adaptation between write-invalidation and require-update protocols. The two fully automatic schemes aim to involve minimal user intervention and yet deliver good performances with some speedups ranging from 2% to 79% observed in some 8 benchmarks tested.
[Performance evaluation, Protocols, application program interfaces, application programming interfaces, Drives, distributed environment, write-invalidation, home based consistency model, home based DSM systems, require-update protocols, pthread programs, Benchmark testing, Software standards, ORION, automatic schemes, DSM system, POSIX-thread like interface, home migration, adaptive home based software distributed shared memory system, multi-threading, data integrity, Application software, multithreaded software distributed shared memory, adaptive schemes, adaptive systems, Computer science, Multithreading, dynamic adaptation, Automatic testing, minimal user intervention, distributed shared memory systems, Software systems]
Load balancing with multiple token policy
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
In distributed systems, uneven arrivals of tasks may overload a few hosts, whereas some of the hosts are lightly loaded. This load imbalance prevents a distributed system from delivering performance to its capacity. Load balancing has been advocated as a means of improving the performance and reliability of distributed systems. A new load balancing approach has been proposed by the authors (1998) to deal with this problem. In this paper, we extend this model with multiple tokens. With some parameters in the algorithm set to intelligent values, the algorithm promises better load balancing results.
[Real time systems, load balancing, Decision making, reliability, distributed processing, performance evaluation, Partitioning algorithms, parameter values, Counting circuits, Computer science, overloaded hosts, load imbalance, resource allocation, performance, System performance, uneven task arrivals, Load management, Frequency, distributed systems, fault tolerant computing, multiple token policy, Resource management, Load modeling]
Circumventing/identifying faults effects
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
The literature includes a variety of techniques to address the Byzantine Generals Problem or the Byzantine approach. While the main goal within the Byzantine framework is to circumvent and mask the effect of unreliable units (e.g., traitors), much research has been done on identifying (i.e., demasking) unreliable units. This is often called fault identification in system diagnosis. The paper focuses on the identification of unreliable units within the Byzantine framework. Beyond its theoretical interest, an identification of faulty units contributes to accelerating the agreement process itself and drastically reducing the number of messages exchanged between units. The main features of this work are twofold. It does not impose additional assumptions or constraints on the agreement process. It limits the overhead for identifying unreliable units: the identification process is in O(n/sup 3/).
[Context, message passing, message exchange, Redundancy, identification process, concurrency theory, fault effect circumvention, Byzantine approach, fault identification, agreement process, Fault diagnosis, Byzantine framework, traitors, unreliable units, Message passing, Voting, distributed algorithms, fault effect identification, Byzantine Generals Problem, system diagnosis, Acceleration, faulty units, computational complexity]
Performance evaluation of nested-loop join processing on networks of workstations
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
Networks of workstations (NOWs) are attractive for parallel processing due to their cost advantage. This paper investigates the performance issues in processing join operations and the inherent tradeoff in the networked workstation environment. Specifically, we look at the performance of the nested-loop join algorithm. Since NOWs are heterogeneous in nature, load sharing is important for their performance. We evaluated the performance of three load sharing methods: static equal, static proportional and dynamic scheduling with fixed-chunk size. The three scheduling methods are evaluated on an experimental heterogeneous network of workstations with non-query background loads. Our experimental result suggest that, when there is no background load, dynamic scheduling outperforms static equal scheduling (up to 40%) and marginally better (about 10% better speedup) than the static proportional scheduling. When there is dynamic background load on nodes, dynamic scheduling provides substantial performance improvement over the static proportional scheduling (up to 50%) and static equal scheduling (up to about 100%). In all cases, selection of an appropriate chunk size is important in dynamic scheduling.
[workstation clusters, nested-loop join algorithm, static equal scheduling, load sharing, parallel databases, Master-slave, performance evaluation, workstation network, Dynamic scheduling, Throughput, Application software, dynamic scheduling, Distributed computing, parallel processing, processor scheduling, Concurrent computing, query processing, Databases, Processor scheduling, Parallel processing, static proportional scheduling, nonquery background loads, Workstations, nested-loop join processing]
Performance analysis of k-ary n-cubes with fully adaptive routing
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
Several analytical models of deterministic routing in wormhole-routed k-ary n-cubes have already been reported in the literature. The performance characteristics of most fully-adaptive routing algorithms have often been analyzed by means of simulation and there is hardly any analytical model proposed for calculating message latency in wormhole-routed k-ary n-cubes using adaptive routing. The paper proposes an accurate analytical model to predict the message latency in wormhole-routed k-ary n-cubes with fully adaptive routing. The proposed model is general in that it exhibits a good degree of accuracy for various network configurations and under different operating conditions.
[Algorithm design and analysis, operating conditions, multiprocessor interconnection networks, simulation, Telecommunication traffic, analytical model, analytical models, message latency, Delay, Analytical models, Hypercubes, Performance analysis, performance characteristics, k-ary n-cubes, deterministic routing, message passing, Computational modeling, network routing, adaptive routing, performance evaluation, Routing, wormhole-routed k-ary n-cubes, adaptive systems, Computer science, network configurations, fully adaptive routing, Distributed control, performance analysis]
Electoral voting protocol-a quorum-based approach for replica control
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
Data replication is an important technique in distributed systems. Most replication techniques are quorum-based. These approaches employ logical structures or mathematical methods to solve the consistent problem of data replication. They have some improvements in getting smaller quorum size, higher availability, and better load balancing. However, most of them may have the following disadvantages: (1) the load distribution is unbalanced, and (2) these methods do not apply to any arbitrary number of sites. This paper presents a new approach called electoral voting protocol, an approach that imitates the American presidential election. It is based on a circular numbering system. This protocol is symmetric in that every site in the system bears the same responsibility. In addition, it is applicable to any arbitrary number of sites.
[Availability, Protocols, replicated databases, load balancing, American presidential election, Nominations and elections, Scattering, computer networks, distributed system, Control systems, Information retrieval, logical structures, Distributed computing, resource allocation, Voting, Computer network reliability, Fault tolerant systems, data replication, mathematical methods, quorum-based replica control, electoral voting protocol, distributed systems, circular numbering system, protocols]
An efficient strategy to support continuous retrieval with dynamic bandwidths
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
To efficiently support continuous retrieval for continuous media, many approaches based on the striping strategy that is implemented on a multi-disk drive have been proposed. However, the striping strategy only considers the fixed bandwidth of an object. Moreover, the aggregate bandwidth of the multi-disk drive is determined when the object is striped. Furthermore, for some applications, such as self-learning systems, users may quickly browse some familiar objects while they will slowly scan some unfamiliar objects. Therefore, we propose an efficient approach to supporting continuous retrieval with dynamic required bandwidths without reorganizing the whole striped object. From the performance study, we observe that the number of subobjects that must be moved is proportional to the initial required bandwidth and the current required bandwidth.
[striped object, multi-disk drive, continuous retrieval, multimedia systems, familiar objects, unfamiliar objects, Information science, performance study, Bandwidth, Computer science education, continuous retrieval support, HDTV, continuous media, information retrieval, Information retrieval, Educational institutions, Magnetic heads, aggregate bandwidth, Computer science, bandwidth allocation, Disk drives, dynamic bandwidths, Aggregates, self-learning systems, striping strategy, fixed bandwidth, dynamic required bandwidths, disc drives]
Multimedia real-time disk scheduling by hybrid local/global seek-optimizing approaches
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
Real-time disk scheduling is one of the most important problems in designing a multimedia system. It has been proved to be NP-complete. Recently, various approaches have been proposed to improve disk throughput under guaranteed real-time requirements. SCAN-EDF, which scans the disk surface to retrieve the task data block under the disk head in order to re-schedule tasks in a real-time EDF (earliest deadline first) schedule, is one of the best-known real-time disk scheduling methods. Since tasks rescheduled in SCAN-EDF should have the same deadline, its efficiency depends on the number of tasks with the same deadline. If all tasks have different deadlines, the scheduling results of SCAN-EDF would be the same as EDF. In this paper, we improve SCAN-EDF by applying different hybrid local-merging and global-inserting schemes. As opposed to SCAN-EDF, in our method tasks rescheduled by SCAN may have different deadlines. Its efficiency is not limited by the number of tasks that have the same deadlines. Experiments show that the proposed method is significantly better than SCAN-EDF. In terms of disk throughput, the improvement obtained is 24% greater than the best-known SCAN-EDF method.
[Real time systems, Optimization methods, multimedia system design, task rescheduling, Throughput, global inserting scheme, SCAN-EDF method, multimedia computing, disc storage, optimisation, Operating systems, scheduling, earliest deadline first scheduling, disk throughput, Minimization methods, efficiency, Multimedia systems, performance evaluation, multimedia real-time disk scheduling, Information retrieval, local merging scheme, NP-complete problem, Scheduling algorithm, guaranteed real-time requirements, Processor scheduling, real-time systems, operating systems (computers), Time factors, hybrid local/global seek-optimizing approaches, computational complexity]
Efficient query result retrieval over the Web
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
Consider a geographic information system (GIS) which is set up as a Web server that allows users to query the database with a Web browser. As the query result may be huge and the network delay could be significant, we investigate the fundamental problem of how to deliver the query result efficiently over the network . In a conventional client-server database system, the commonly used application programming interface (API) is the so-called iterator-based interface in which a client queries the server with an ISQL statement and the result, which is called a result or active set, is generated. To retrieve the query result, multiple calls are made to the server and objects in the result set are retrieved sequentially. To enhance system performance, objects in a result set can also be retrieved in bulk by storing them in an array. In the Web environment, a database server is commonly implemented with a distributed object technology such as Java or CORBA. As network delay could be significant and the client memory spaces are limited and varying, neither multiple calls nor bulk-retrieval is a viable solution to this problem. We propose a technique by caching and piping the result set through a socket connection without forfeiting the iterator-based interface. We show that the proposed method is superior in delivering a query result in a LAN and in the Web environment. We then investigate how to retrieve and display geometric data in a map efficiently in a network environment.
[result set caching, geometric data display, Geographic Information Systems, search engines, application program interfaces, application programming interface, geographic information systems, socket connection, query processing, Network servers, Space technology, System performance, Distributed databases, client memory spaces, Database systems, geographic information system, distributed object technology, Web server, Local area networks, distributed object management, information resources, Java, client-server systems, client-server database system, database server, geometric data retrieval, iterator-based interface, ISQL statement, result set piping, CORBA, network delay, efficient query result retrieval, Sockets, Web browser, LAN]
Network computing technology and information society toward 21st century
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
The process of development of network computing towards the 21st century is explained. In order to make clear the key technologies required for the development of network computing, the logical model of the system is represented by a site-layer model and the key technologies corresponding to each layer are introduced. For the future development of the information society, the importance and the technologies concerning thin computers and the new platform constructed by the unification of the Internet and broadcasting are also discussed. Finally, the future trends of EC (electronic commerce) is envisioned.
[information society, thin computers, Broadcast technology, technological forecasting, site-layer model, 21st century, network computing technology, Operating systems, Broadcasting, Computer networks, EC, Business, electronic commerce, social aspects of automation, Packet switching, network computers, future trends, computer networks, Microcomputers, Large scale integration, broadcasting, future development, Internet, US Department of Defense, logical model]
Some space considerations of VLSI systolic array mappings
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
The space-time mapping of the dependency matrix of an algorithm is used to study spatial properties of a systolic array implementation of a 3-nested loop structure. Elementary expressions are developed for both the number of processing elements and the area of the array. These expressions involve only the space-time transformation and the lengths of the loops. As well, characterizations have been found for the form of the space-time transformation which produces a systolic array with the minimum number of processing elements, and one which has both the minimum number of processing elements and the smallest area. Moreover, the theorems can also be applied to more general algorithms, such as those with variable lengths of loops.
[processing element number, parallel algorithms, program control structures, Costs, VLSI, Area measurement, space-time mapping, Very large scale integration, systolic arrays, Extraterrestrial measurements, Mathematics, dependency matrix, Statistics, systolic array area, 3-nested loop structure, systolic algorithm, Computer science, Fabrication, spatial properties, Current measurement, VLSI systolic array mappings, Systolic arrays, loop length, space-time transformation]
A distributed solution for resources allocation to overlapping groups
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
The distributed resource allocation problem is a well known fundamental problem in distributed systems. Many solutions which avoid the deadlock and starvation have been developed. With the progress of computer networks, however, distributed cooperative group activities in a network environment have been increasing, so that several groups may compete for some resources in the network environment and deadlock among groups and starvation of a group may happen. Since previous allocation models are mainly for representation of competition for resources among processes, they cannot reflect clearly the competition for resources among groups of processes. Moreover, though the previous solutions to the distributed resource allocation problem can avoid the deadlock and starvation, they cannot deal with the deadlock among groups and starvation of a group. The authors present a new model which explicitly describes the competition for resources among process groups which may share common processes, and a definition of "Distributed Allocation of Resources to process Group" (DARG) under the model. A solution to DARG is also proposed by extending an acyclic graph approach to the dining philosopher problem. Our solution allocates resources to groups of processes with deadlock among groups and starvation of a group never happening. In addition, our solution guarantees that more than one group works mutually exclusively if a common process belongs to these groups.
[graph theory, mutual exclusion, distributed cooperative group activities, deadlock, Distributed computing, High-speed networks, resource allocation, common processes, Cities and towns, distributed systems, process groups, Computer networks, distributed resource allocation problem, Distributed algorithms, dining philosopher problem, Codecs, computer networks, distributed solution, overlapping groups, network environment, DARG, Computer science, common process, distributed algorithms, concurrency control, allocation models, System recovery, Software, Resource management, acyclic graph approach]
A new paradigm of user intention preservation in realtime collaborative editing systems
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
The concept of time and the ordering of events are correlated key issues in distributed computing as well as in computer-supported collaborative work (CSCW) systems. The paper revisits some of the assumptions that have generally been made regarding time and event ordering in distributed systems and argues that they are no longer appropriate if the goal is to faithfully preserve user intentions in CSCW systems. In particular, the following contributions are made in the context of collaborative editing systems. First, we discuss how the user intentions might be impacted when the finite duration of drawing operations are considered. Secondly, we propose that the total ordering of events should give the users the right of participation instead of being solely determined mechanically by the system. Thirdly, a new concept of an active whiteboard is proposed which supports various integrity constraints on objects and object groups to maintain user intentions in a more sophisticated way. Additionally, for the sake of completeness, the problem of maintaining consistency in the face of unreliable and high-latency communication channels is also covered.
[telecommunication channels, user participation rights, Distributed computing, user modelling, Delay, distributed computing, integrity constraints, object groups, groupware, Computer networks, unreliable communication channels, IP networks, CSCW systems, user intention preservation, computer-supported collaborative work, text editing, data integrity, consistency maintenance, Computer science, drawing operations duration, Collaboration, real-time systems, Communication channels, Collaborative work, Internet, active whiteboard, high-latency communication channels, real-time collaborative editing systems, event ordering, Clocks]
Reducing cache conflicts by multi-level cache partitioning and array elements mapping
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
The paper presents an algorithm to reduce cache conflicts and improve cache localities. The proposed algorithm analyzes unique locality reference space for each reference pattern, partitions the multi-level cache into several parts with different size, and then maps array data onto the scheduled cache positions such that cache conflicts can be eliminated. To reduce the memory overhead for mapping array variables onto partitioned cache, a greedy method for rearranging array variables in declared statement is also developed. In addition, we combine loop tiling and the proposed schemes for exploiting both temporal and spatial reuse opportunities. To demonstrate that our approach is effective at reducing the number of cache conflicts and exploiting cache localities, we use Atom as a tool to develop a simulator for simulation of the behavior of direct-mapping cache. Experimental results show that applying our cache partitioning scheme can largely reduce the cache conflicts and thus save program execution time in both one-level cache and multi-level cache hierarchies.
[Algorithm design and analysis, Costs, Cache memory, cache storage, reference pattern, scheduled cache positions, Degradation, loop tiling, Information science, multi-level cache partitioning, cache partitioning scheme, scheduling, cache conflicts, memory overhead, array variables, Pattern analysis, Round robin, search problems, greedy method, program control structures, simulator, array element mapping, cache localities, declared statement, direct-mapping cache simulation, program execution time, array data, Partitioning algorithms, spatial reuse opportunities, Scheduling algorithm, Atom, Computer science, one-level cache, cache conflict reduction, distributed algorithms, unique locality reference space, partitioned cache]
Reaching fault diagnosis agreement on dual link failure mode
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
Previously, most Byzantine Agreement protocols could reach an agreement by way of fault masking. Few of them can detect and locate the faulty components. On the other hand, most fault diagnosis algorithms can detect and locate faulty components but few of them can make all fault free processors reach an agreement. The study analyses the messages received at the period of reaching agreement, and then detects and locates the faulty components of the network. Finally, the proposed protocol can further make all fault free processors agree on the common failure report of the synchronous connected network. The symptoms of the faults include the malicious fault and the dormant fault.
[Chaos, Algorithm design and analysis, fault diagnosis algorithms, Protocols, faulty component detection, Information management, fault free processors, Fault diagnosis, common failure report, Broadcasting, dormant fault, protocols, dual link failure mode, concurrency theory, malicious fault, synchronous connected network, faulty components, Fault detection, Councils, distributed algorithms, messages received, fault diagnosis agreement, fault masking, fault tolerant computing, Byzantine Agreement protocols, Reliability]
A fault-tolerant adaptive and minimal routing approach in 3-D meshes
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
We propose a sufficient condition for minimal routing in 3-dimensional (3D) meshes with fault nodes. Unlike many traditional models that assume all the modes know global fault distribution or just adjacent fault information, our approach is based on the concept of limited global fault information. First, we propose a fault model called faulted cube in which all faulty nodes in the system are contained in a set of faulty cubes. Fault information is then distributed to limited number of nodes while it is still sufficient to support minimal routing. The limited fault information collected at each node is represented by a vector called extended safety level. The extended safety level associated with a node can be used to determine the existence of a minimal path from this node to a given destination. Our results show that any minimal routing that is partially adaptive can be applied in our model as long as the destination node meets a certain condition. We also propose a dynamic planar adaptive routing scheme that offers better fault tolerance and adaptivity than the planar adaptive routing scheme in 3D meshes. Our approach is the first attempt to address adaptive and minimal routing in 3D meshes with faulty nodes using limited fault information.
[Costs, partially adaptive routing, multiprocessor interconnection networks, minimal routing approach, destination node, limited global fault information, limited fault information, fault tolerant adaptive routing, Fault tolerance, Network topology, 3D meshes, 3-dimensional meshes, Hypercubes, Safety, Labeling, sufficient condition, dynamic planar adaptive routing scheme, Hamming distance, fault tolerance, network routing, faulted cube, Routing, global fault distribution, adaptive systems, extended safety level, Computer science, fault model, minimal path, adjacent fault information, System recovery, fault tolerant computing, planar adaptive routing scheme, faulty nodes]
Improving performance of parallel transaction processing systems by balancing data load on line
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
The performance of parallel transaction processing systems can be degraded significantly due to data skew, a phenomenon of unbalanced data distribution over the nodes of a system. Rebalancing the data load of a system with data skew by redistributing its data is known to be an effective approach to cope with data skew. Unfortunately, for most of the existing approaches, the data being redistributed is unavailable (off-line). Numerous applications, such as those for reservations, finance, process control, hospitals, police and the armed forces, however, cannot afford off-line data for any significant amount of time. These applications call for the ability of balancing data load online. In this paper, a new online data redistribution approach is proposed. A prototype of the approach has been implemented, and experiments have been conducted. Experimental results confirm the substantial performance gains of the approach.
[transaction processing, Data analysis, online data redistribution, Process control, Software performance, Performance gain, performance evaluation, online data load balancing, parallel processing, performance improvement, prototype, Degradation, Information science, Databases, resource allocation, online operation, unbalanced data distribution, parallel transaction processing systems, Prototypes, Cities and towns, Performance analysis, data handling, data skew]
Reducing cognitive overheads in a Web warehouse using reverse-osmosis
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
This paper provides a quantitative analysis of reducing cognitive overheads in a Web warehouse using an important class of operation called reverse osmosis. The analysis is used to examine two different cognitive overheads of locating relevant nodes or information and display time of a Web table. A reverse-osmosis operation enables us to eliminate in relevant information from a collection of Web documents stored in the form of a Web table. We call such an operation reverse-osmosis because it is analogous to the reverse osmosis process in the field of water purification. We discuss a formal algorithm of the reverse-osmosis operation.
[Algorithm design and analysis, information resources, Data analysis, cognitive overhead reduction, Web table, formal algorithm, information retrieval, reverse osmosis, Displays, Information analysis, Web warehouse, Warehousing, Web documents, Reverse osmosis, Biomembranes, Water pollution, Web sites, quantitative analysis, data warehouses, Water resources]
An Estelle-based probabilistic partial timed protocol verification system
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
Complete verification of communication protocols is usually very hard to achieve due to the combinatorial state space explosion problem. Probability based partial verification provides an alternative solution to solve this problem. We adopt a Timed Communicating State Machine (TCSM), which belongs to the Extended Communicating Finite State Machine (ECFSM) model, to formally specify protocols that incorporate timed properties as part of their specifications. Based on the TCSM model, we propose a probabilistic timed verification scheme that is based on the occurrence rates of communicating entities' transitions and occurrence probabilities of channel entities' transitions. Using our probabilistic partial timed protocol verification scheme, an Estelle based Probabilistic Partial Timed Protocol verification system, which is called PTPVS, is developed on SUN SPARC workstations. In this way, protocol designers can use PTPVS to design and partially verify Estelle based protocol specifications with time properties.
[TCSM model, time properties, Protocols, probability based partial verification, PTPVS, occurrence rates, Information management, finite state machines, formal specification, SUN SPARC workstations, formal verification, protocol designers, channel entity transitions, specification languages, probabilistic timed verification scheme, Workstations, Timed Communicating State Machine, protocols, communicating entity transitions, probability, communication protocols, Probability, Explosions, State-space methods, Sun, Reachability analysis, combinatorial state space explosion problem, Extended Communicating Finite State Machine, Computer science, Automata, Estelle based probabilistic partial timed protocol verification system, timed properties, occurrence probabilities]
Cooperative workflows to coordinate asynchronous cooperative applications in a simple way
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
Current workflow models are mainly concerned with the automation of administrative and production business processes. These processes coordinate well-defined activities which execute in isolation, i.e. synchronize only at their start/terminate states. If these models can be applied efficiently for a class of applications, they show their limits when one wants to model the subtlety of cooperative interactions as they occur in more creative processes, typically co-design and co-engineering processes. In this paper, we introduce the concept of cooperative workflow, i.e. a workflow model which extends classical workflow models with capabilities to synchronize activities interacting not only when they start and when they terminate, but also at any point of their execution. In the spirit of the workflow approach, the modelling and enactment of cooperative workflows must remain simple.
[transaction processing, Automation, start state, terminate state, Buildings, Pulp manufacturing, asynchronous cooperative applications coordination, creative processes, virtual enterprise, transactions, synchronisation, codesign processes, cooperative interactions, cooperative workflows, production business processes, coengineering processes, Production, groupware, Isolation technology, synchronization, cooperative systems, administrative processes, workflow management software, Assembly]
Optimal fault-tolerant routing in hypercubes using extended safety vectors
Proceedings Seventh International Conference on Parallel and Distributed Systems
None
2000
Reliable communication in cube-based multicomputers using the extended safety vector concept is studied. Each node in a cube-based multicomputer of dimension n is assorted with an extended safety vector of n bits, which is an approximated measure of the number and distribution of faults in the neighborhood. In the extended safety vector model, each node knows fault information within distance-2 and fault information outside distance-2 is coded in a special way based on the coded information of its neighbors. The extended safety vector of each node can be easily calculated through n-1 rounds of information exchanges among neighboring nodes. Optimal unicasting between two nodes is guaranteed if the kth bit of the safety vector of the source node is one, where k is the Hamming distance between the source and destination nodes. In addition, the extended safety vector can be used as a navigation tool to direct a message to its destination through a minimal path. Simulation results show a significant improvement in terms of optimal routing capability in a hypercube with faulty links using the proposed model, compared with the one using the original safety vector model.
[optimal unicasting, safety vector, faulty links, Reliability engineering, hypercube networks, hypercubes, Fault tolerance, original safety vector model, Prototypes, safety, fault information, Hypercubes, Safety, information exchanges, multiprocessing systems, hypercube, Hamming distance, Navigation, optimal routing capability, approximated measure, Routing, Topology, cube-based multicomputers, optimal fault-tolerant routing, navigation tool, Computer science, extended safety vectors, neighboring nodes, minimal path, fault tolerant computing, reliable communication, coded information]
A scalable yet transparent infrastructure for distributed applications: core design of Jasmine ii framework
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
At the beginning of the new millennium we continue to witness in the modern information technology industry two fundamental trends: ongoing Internet revolution and global consolidation. IT departments of all kinds have their resources free to deal with the consequences of these phenomena. Integration of data and services is rapidly becoming their number one task. Information stored in different types of providers distributed throughout the enterprise network must be integrated, and used to build new kinds of e-commerce applications, information portals, and other types of applications leveraging the power of distributed computing. We recognized these trends and developed Jasmine ii, a new information management infrastructure to build, deploy, and manage such distributed applications capable of bringing together data from distributed and heterogeneous sources of information. The infrastructure is designed to be scalable and transparent with regards to concurrency, replication, access, and location. We describe how we achieve such design objective by illustrating the key components of the infrastructure: object cache manager, event manager, and transaction manager. We also explain some novel features of each component.
[transaction processing, Relational databases, information portals, cache storage, Information management, Distributed computing, distributed applications, distributed computing, Concurrent computing, enterprise network, distributed databases, e-commerce, distributed object management, information technology industry, transaction manager, Business, replication, Information resources, Object oriented databases, object-oriented databases, event manager, object oriented data model, Application software, concurrency, Jasmine ii framework, Internet, information management infrastructure, Portals, object cache manager]
Dynamic load-balancing in a data parallel object-oriented system
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
In this paper, a parallel object collection (POC) model is introduced to support data parallelism in a parallel object-oriented system. This model is based on the idea of data partitioning and method replication. To achieve load-balancing, partition objects are dynamically migrated at runtime according to the system load situation. A threshold-based strategy is used in the dynamic load-balancing. To avoid over-convergence of load during partition object migration, a new destination node selection algorithm is proposed. The threshold values used in the algorithm are also adaptively adjusted to better reflect the fluctuation of the load during execution. To evaluate the performance of the dynamic load balancing algorithm, simulation experiments are conducted. The simulation results are reported and discussed in the paper.
[Fluctuations, object-oriented programming, data parallel object-oriented system, Object oriented modeling, parallel object collection model, threshold-based strategy, destination node selection algorithm, Data engineering, digital simulation, dynamic load-balancing, Partitioning algorithms, Yarn, parallel programming, Concurrent computing, Computer aided instruction, Runtime, resource allocation, partition objects, data parallelism, simulation results, Parallel processing, Load management, data partitioning, method replication]
Ring embedding in faulty (n,k)-star graphs
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
In this paper we consider the ring embedding problem in faulty (n,k)-star graphs. An (n,k)-star graph is recently proposed as an attractive interconnection network topology and also known as the generalized version of an n-star graph with scalability such that the number of nodes in the graph can be suitably adjustable by two dimensioning parameters n and k. Our scheme is proceeded in top-down style in such a manner that the resulting sub-stars maintain evenly distributed faults. We show that a ring of length n!/(n-k)!-f can be found in an (n,k)-star graph having n!/(n-k)! nodes when the number of faulty nodes f is at most n-3 and n-k/spl ges/2.
[Embedded computing, n-star graph, Multiprocessor interconnection networks, Scalability, network routing, generalized version, evenly distributed faults, multiprocessor interconnection networks, network topology, Distributed computing, scalability, Multiprocessing systems, Computer science, Fault tolerance, Circuit topology, Network topology, ring embedding, Bidirectional control, fault tolerant computing, faulty (n, k)-star graphs, interconnection network topology, top-down style]
Parallel and distributed mining with ensemble self-generating neural networks
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
In this paper, we present the improving capability of accuracy and the parallel efficiency of ensemble self-generating neural networks (ESGNNs) for classification on a MIMD parallel computer. Self-generating neural networks (SGNNs) are originally proposed for classification or clustering by automatically constructing self-generating neural tree (SGNT) from given training data. ESGNNs are composed of plural SGNTs each of which is independently generated by shuffling the order of the given training data, and the output of ESGNNs are averaged outputs of the SGNTs. We allocate each of SGNTs to each of processors in the MIMD parallel computer. Experimental results show that the more the number of processors, the more the misclassification rate decreases for all problems.
[parallel mining, Neurons, data mining, ensemble self-generating neural networks, Data engineering, self-generating neural tree, parallel processing, distributed mining, Convergence, Concurrent computing, Neural networks, self-organising feature maps, Training data, Backpropagation algorithms, training data, Computer networks, MIMD parallel computer, Bagging, Classification tree analysis]
Design and implementation of a fibre channel network driver for SAN-attached RAID controllers
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Fibre channel SAN (storage area network) is considered to be a promising solution to address storage problems caused by the sheer volume of data and their management. To adopt this new storage environment, we design and implement a high performance fibre channel network driver for SAN-attached RAID controllers in a real-time operating system. We describe the architecture of the fibre channel driver which consists of two modes; a target mode and an initiator mode. An exception handling mechanism for enduring a disk failure is also given. Lastly, we measured the performance of the fibre channel driver. Testing results reveal a moderately successful performance of the fibre channel driver.
[Real time systems, data management, Communication system control, exception handling, performance evaluation, Control systems, local area networks, RAID, SAN-attached RAID controllers, disk failure, Optical fiber testing, Storage area networks, storage area network, Operating systems, network operating systems, real-time systems, Computer architecture, Optical fiber communication, System software, real-time operating system, Microprogramming, fibre channel network driver, fibre channel SAN]
Optimal parallel algorithms for cut vertices, bridges, and Hamiltonian path in bounded interval tolerance graphs
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
We present parallel algorithms to find cut vertices, bridges, and Hamiltonian Path in bounded interval tolerance graphs. For a graph with n vertices, the algorithms require O(log n) time and use O(n) processors to run on Concurrent Read Exclusive Write Parallel RAM (CREW PRAM) model of computation. Our approach transforms the original graph problem to a problem in computational geometry. The total work done by the parallel algorithms is comparable to the work done by the best known sequential algorithms for the more restricted class of graphs, namely, interval graphs and permutation graphs. In this sense our algorithms have optimal complexity.
[Algorithm design and analysis, parallel algorithms, Computational modeling, graph theory, Hamiltonian path, Read-write memory, Very large scale integration, computational geometry, Phase change random access memory, bounded interval tolerance graphs, bridges, Parallel algorithms, permutation graphs, Computer science, Concurrent computing, Computational geometry, optimal parallel algorithms, Bridge circuits, CREW PRAM, optimal complexity, cut vertices, interval graphs, sequential algorithms]
An active scheduler: autonomous concurrency control of parallel programs in a distributed environment
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
We propose a new scheduling method that can simultaneously achieve two main goals of task scheduling in distributed parallel systems; i.e., to minimize the execution time of a parallel job without disturbing the execution of other jobs. We challenge to achieve those goals by introducing a new scheduler, called active scheduler, that controls the priority of parallel programs dynamically and balances the workload of computers, depending on the current status of the runtime environment. The priority of parallel programs is controlled by controlling the concurrency of the programs. We implemented a prototype system to evaluate the effectiveness of active scheduler. The results of experiments imply that the overhead of introducing the active scheduler is bounded by 15% of the original execution time, and it is in fact effective to adjust the execution of parallel programs to an actual distributed parallel processing environment in which many users execute their jobs at the same time.
[autonomous concurrency control, load balancing, experiments, Dynamic scheduling, Concurrency control, Job design, distributed environment, Distributed computing, Yarn, parallel programming, parallel programs, Concurrent computing, Runtime, Processor scheduling, resource allocation, active scheduler, Physics computing, prototype system, concurrency control, execution time, Parallel processing, scheduling, task scheduling, runtime environment]
Communication-efficient bitonic sort on a distributed memory parallel computer
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Sort can be speeded up on parallel computers by dividing and computing data individually in parallel. Bitonic sorting can be parallelized, however, a great portion of execution time is consumed due to O(log/sup 2/P) time of data exchange of N/P keys where P, N are the number of processors and keys, respectively. This paper presents an efficient way of data communication in bitonic sort to minimize the interprocessor communication and comparison time. Before actual data movement, each pair processor exchanges the minimum and maximum in its list of keys to determine which keys are to be sent to its partner. Very often no keys need to exchange, or only a fraction of them are exchanged. At least 20% or greater of execution time could be reduced on the T3E computer in our experiments. We believe the scheme is a good way to shorten the communication time in similar applications.
[parallel algorithms, Costs, data exchange, Merging, experiments, T3E computer, Partitioning algorithms, Application software, Distributed computing, parallel machines, Sorting, Concurrent computing, distributed memory parallel computer, Information science, communication-efficient bitonic sort, execution time, sorting, distributed memory systems, interprocessor communication, Load management, data communication, parallel computers, Data communication]
Jato: a compact binary file format for Java class
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Java has been a very important programming language, especially with its cross-platform characteristics, but the CLASS file format defined in the Java Virtual Machine (JVM) specification contains many redundancies and replications of information. These redundancies most come from the "constant pool" of a CLASS file. We propose a compact binary file format, called Jato, and its associated archive format, called Jatar, for the Java system. Using these two formats, many of the redundancies can be removed. We didn't utilize any text compression technique in the proposed formats, so they do not sacrifice the loading speed and are thus very suitable for use in embedded environments. We've also implemented a class loader that is capable of loading the Jato files into a regular JVM. Using this approach, we show that the Jato file format is effective and promising, while still keeping the cross-platform features of Java.
[Real time systems, Code standards, software libraries, Jatar archive format, Operating systems, Embedded system, data replications, Java classes, Libraries, Robustness, redundancy, loading speed, file loading, Java, Jato file format, embedded environments, Image converters, Virtual machining, cross-platform characteristics, Java Virtual Machine, Computer languages, software portability, virtual machines, constant pool, class loader, file organisation, redundancies, compact binary file format, CLASS file format]
Efficient implementation of Edmonds' algorithm for finding optimum branchings on associative parallel processors
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
We propose an efficient parallel implementation of Edmonds' (1967) algorithm for finding optimum branchings on a model of the SIMD type with vertical data processing (the STAR-machine). To this end for a directed graph given as a list of triples (edge vertices and the weight), we construct a new associative version of Edmonds' algorithm. This version is represented as the corresponding STAR procedure whose correctness is proved. We show that on vertical processing systems Edmonds' algorithm takes O(n log n) time, where n is the number of graph vertices.
[optimum branchings, parallel algorithms, parallel algorithm, Data processing, Data structures, Mathematics, Graph theory, Electronic mail, SIMD, Geophysics computing, parallel machines, Concurrent computing, associative parallel processors, associative processing, Tree graphs, directed graphs, STAR-machine, directed graph, graph vertices, Polynomials, Associative processing, vertical data processing]
Information flow control in role-based model for distributed objects
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Various kinds of distributed applications are realized in object-based frameworks. Object-based technologies are used to design applications and enhance the interoperability among applications. In addition to realizing the interoperability, the system is required to be secure. The secure system is required to not only protect objects from illegal manipulation but also illegal information flow among objects. We discuss a role-based access control model in the object-based systems and how to resolve illegal information flow in the roles. We define a safe set of roles where no illegal information flow occurs. We discuss an algorithm to check if illegal information flow occurs. In addition, we discuss how to safely perform transactions belonging to unsafe roles.
[Access control, Law, open systems, Object oriented modeling, unsafe roles, information flow control, interoperability, Information filtering, Application software, transactions, Distributed computing, distributed applications, role-based access control, authorisation, distributed objects, Information filters, Systems engineering and theory, Protection, Legal factors, distributed object management, secure system]
An efficient adaptive broadcast algorithm for the mesh network
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Most existing broadcast algorithms proposed for the mesh do not scale well with the network size. Furthermore, they have been mainly based on deterministic routing, which cannot exploit the alternative paths provided by mesh topology to reduce communication latency. Motivated by these observations, this paper introduces a new adaptive broadcast algorithm for the mesh. The unique feature of our algorithm is its ability to handle broadcast operations with only two message-passing steps irrespective of the network size. Results from extensive comparative analysis reveal that the proposed algorithm exhibits superior performance characteristics over those of the well-known recursive doubling and extending dominating node algorithms.
[Algorithm design and analysis, adaptive broadcast algorithm, network routing, multiprocessor interconnection networks, Glass, Routing, mesh topology, communication latency, broadcast operations, deterministic algorithms, Delay, Mesh networks, comparative analysis, Network topology, mesh network, Broadcasting, System recovery, Hardware, dominating node algorithms, Performance analysis, performance characteristics, recursive doubling, deterministic routing]
Evaluation of emerging high speed networks on Brazos software DSM system
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
One of the factors that can influence the performance of a DSM system is the efficiency of multipoint access on the interconnection network. This work presents an evaluation of the Brazos system, a software implemented distributed shared memory (DSM) system designed for x86 SMP nodes running Windows NT and that takes advantage of multicast communication, with regard to different configurations of the communication network: broadcast fast Ethernet, switched fast Ethernet and LANE-ATM. In this evaluation, although the bandwidth of the ATM is larger than the fast Ethernet, low performance using LANE-ATM was observed, compared to the Brazos system performance using broadcast or switched Fast Ethernet. It shows the importance of the communication subsystem and the necessity of improvements on aspects like multipoint access, which has been not much explored on networks like ATM.
[Windows NT, multipoint access, Ethernet networks, Multiprocessor interconnection networks, Multicast communication, asynchronous transfer mode, local area networks, High-speed networks, LANE-ATM, interconnection network, network operating systems, multicast communication, Broadcasting, distributed shared memory system, broadcast fast Ethernet, Communication networks, high speed networks, Brazos system, DSM system, performance evaluation, Communication switching, switched fast Ethernet, x86 SMP nodes, Communication system software, performance, distributed shared memory systems, Software systems, Asynchronous transfer mode]
Performance of parallel I/O scheduling strategies on a network of workstations
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Techniques for scheduling parallel I/O for both uniprogrammed systems that run single jobs in isolation and multiprogrammed environments that execute multiple parallel jobs simultaneously are presented. The performance of the scheduling algorithms is evaluated on a network of workstations. A new scheduling algorithm proposed in this paper is observed to perform very well for systems running single jobs in isolation. The algorithms that use knowledge of job characteristics are observed to produce a superior performance in multiprogrammed parallel environments.
[workstation clusters, Buildings, workstation network, performance evaluation, input-output programs, Scheduling algorithm, Concurrent computing, Degradation, uniprogrammed systems, parallel input output scheduling, File systems, Processor scheduling, parallel I/O scheduling, System performance, multiprogramming, Parallel processing, scheduling, Systems engineering and theory, Workstations, software performance evaluation, multiprogrammed environments]
Alias analysis for Java with reference-set representation
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Proposes a flow-sensitive, context-insensitive alias analysis in Java that is more efficient and precise than previous analyses in C++. For this, we propose a reference-set alias representation and we present the propagation rules for this representation. For the type determination, the type table is built with reference variables and with all possible types of those variables. We propose an algorithm in a popular iterative loop method with a structural traversal of a context-free grammar. Finally, we show that our reference-set representation has better performance for the alias analysis algorithm than the existing object-pair representation does.
[Algorithm design and analysis, iterative methods, Switches, type table, iterative loop method, object-pair representation, Information analysis, type determination, context-free grammars, propagation rules, Safety, Performance analysis, Iterative methods, context-free grammar structural traversal, software performance evaluation, flow-sensitive context-insensitive alias analysis algorithm, Java, program diagnostics, reference-set alias representation, abstract data types, Computer science, performance, High performance computing, Iterative algorithms, reference variables]
An adaptive and dynamic timer design to maintain soft state in RSVP
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
An enhanced RSVP refresh timer is introduced to support adaptive and dynamic message delivery for fastness and reliability of seamless multimedia data such as voice. To increase adaptiveness of a network, we suggest a feedback mechanism to improve robustness and a dynamic timer mechanism to reduce signaling overhead. The suggested timer adjusts the refreshing time interval in RSVP dynamically using round trip time. With the new mechanism, we could significantly reduce the signaling overhead to maintain soft state in RSVP.
[voice, RSVP refresh timer, multimedia data, adaptive message delivery, reliability, Maintenance, Application software, Multimedia communication, Videoconference, dynamic message delivery, Computer science, feedback, dynamic timer, signaling overhead, Computer network reliability, transport protocols, Feedback, Robustness, Computer networks, Internet, computer network reliability, multimedia communication, Biomedical imaging, round trip time]
A fuzzy multicast method providing linguistic guarantee of disruption free service in mobile wireless networks
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
The goal of seamless communication is to provide disruption free service to a mobile user. A disruption in mobile service could occur due to active handoffs. There are not many user applications that require either total guarantee for disruption free service or tolerance for very frequent disruptions. In this paper, a fuzzy multicast method that provides a linguistic guarantee for disruption free service is proposed. The performance of our scheme is evaluated through a simulation with various mobile directions and mobile velocities, and we know that the fuzzy multicast method significantly reduces the static network bandwidth usage and also provides linguistic guarantee for disruption fee service.
[Computational modeling, fuzzy multicast method, Quality of service, fuzzy logic, Mobile communication, mobile wireless networks, Application software, radio access networks, Intelligent networks, Information science, mobile user, Wireless networks, static network bandwidth usage, Bandwidth, multicast communication, seamless communication, Computer networks, linguistic guarantee, active handoffs, disruption free service, Mobile computing]
Development and implementation of an operation efficiency management system using the UPH according to operation start time
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
An automation system which depends solely on production improvements has been developed recently focusing on the optimization system, maximization of the efficiency, stability of the system, convenience of operation, and compatibility. Various studies on enhancement of the automation system efficiency are being considered by many researchers and software developers in order to achieve maximum production improvement and optimization of operations in the effort to maximize the effectiveness of the automation system. The production measurement UPH (unit per hour) has been used to determine the automation system efficiency but the efficiency of each operation is calculated manually on the basis of the accumulated UPH in the conventional system. Therefore, more accurate efficiency measurements are required. The operational efficiency of the automation system has been measured using UPH to provide users with a more quantified and objective evaluation of the system. The user should provide the initiating time to operate the process so that the operation efficiency is obtained as results sorted by date of the operations.
[Production systems, efficiency measurements, Automation, Educational products, production engineering computing, efficiency maximization, Control systems, factory automation, unit per hour, operation efficiency management system, automation system, optimisation, optimization system, operation start time, Automatic control, User interfaces, Hardware, Computer science education, Systems engineering education, maximum production improvement, stability, Graphical user interfaces]
Causal precedent relations among messages in object-based systems
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Distributed applications are realized by cooperation of a group of multiple objects. In the group cooperation, a message is multicast and multiple kinds of messages are in parallel sent to multiple destinations. The object waits for multiple messages in conjunctive and disjunctive ways from multiple objects. We newly define a novel precedent relation of messages exchanged among objects in presence of multicast and parallel-cast and conjunctive-receipt and disjunctive-receipt of messages.
[Encapsulation, message conjunctive-receipt, Protocols, message passing, causal precedent relations, multiple object cooperation, multicast message, object-based systems, Electronic mail, distributed applications, message disjunctive-receipt, parallel-cast, Teleconferencing, Unicast, multicast communication, Systems engineering and theory, distributed object management, Clocks]
A dynamic grouping scheduling for heterogeneous Internet-centric metacomputing system
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
In this paper we propose a dynamic scheduling heuristic for allocating task graphs onto Internet-centric metacomputing systems. This proposed algorithm, which is called the Dynamic Grouping Scheduling (DGS), differs from conventional algorithms in three respects. First, DGS employs a dynamic task grouping strategy to determine task computational cast. Second, this algorithm estimates the competence of processors for unscheduled tasks at each scheduling step. Third, the proposed scheme takes the fluctuations of power weight for processors into consideration. Experimental results show that the proposed DGS performs better than the competing scheduling schemes under the effects of varying power weights of processors.
[Fluctuations, task computational cast, Heuristic algorithms, dynamic scheduling heuristic, distributed processing, Dynamic scheduling, Graph theory, Scheduling algorithm, processor scheduling, task graphs, Computer science, power weight, Processor scheduling, Metacomputing, heterogeneous Internet-centric metacomputing system, dynamic grouping scheduling, Internet, Computational efficiency]
Path selection algorithms for real-time communication
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Real-time messages, which have user defined end-to-end delay requirements, can be delivered using a real-time communication method. This paper proposes two path selection algorithms for real time communication and conducts a simulation study to evaluate these two algorithms. On the basis of this simulation study, it can be seen that, for effective real-time communication, it is important to consider the number of hops utilized by each path and the link utilization of all effected links. In particular, the number of hops required for each path was found to be the most important factor. The proposed path selection algorithms are found to perform as well or better than previously proposed algorithms while using fewer network resources.
[Real time systems, Statistical analysis, end-to-end delay requirements, Circuit simulation, Delay effects, simulation, distributed processing, link utilization, digital simulation, real-time communication, Processor scheduling, real-time systems, real-time messages, Bandwidth, Computer networks, Hardware, hops, path selection algorithms, Power generation, Testing]
A cost-based admission control algorithm for handling mixed workloads in multimedia server systems
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
We propose and analyze a cost-based, resource-reservation admission control algorithm for handling mixed workloads in modern multimedia systems such as a digital library multimedia system that must provide access services to heterogeneous objects stored in the library. The cost-based scheme considered is based on the concept of "rewards" and "penalties" associated with requests of various object types. Instead of admitting object requests until resources are exhausted as a condition for admission control, resources are reserved to requests of different types dynamically based on the cost-based scheme so that the system is capable of maximizing the total reward received by the system in response to workload changes in the environment.
[heterogeneous objects, penalties, multimedia system, Quality of service, Throughput, cost-based admission control, digital library, resource allocation, Bandwidth, Video compression, scheduling, disk scheduling, multimedia servers, Multimedia systems, Video sharing, digital libraries, quality of service, workload change, Computer science, Software libraries, Admission control, mixed workload handling, Streaming media, resource-reservation admission control, rewards, multimedia server]
FRD-FPRED: a novel re-decode based error compensation method using fast re-decoding and fast prediction algorithm
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
The study aims to find an efficient compensation method of transmission errors in low bit-rate mobile communications using the H.263 video codec. We suggest a novel error compensation method in the encoder (or transmitter), that can remove visual quality degradation due to spatio-temporal error propagation by utilizing a feedback channel. Our proposed method can reduce the computational complexity of the re-decoding method and FRD (fast re-decoding) method by using the characteristic for non-error-propagated macroblocks. Experimental results show that the proposed error compensation method can give higher coding efficiency compared with the other two non-re-decoding based methods: the reference picture selection (RPS) method and the error tracking (ET) method. Finally, our proposed method requires only 30% of the operations compared to the fast re-decoding (FRD) procedure.
[Video coding, transmission errors, Error compensation, Mobile communication, Degradation, feedback channel, Image coding, Transmitters, fast re-decoding, Feedback, low bit rate mobile communications, Video compression, FRD-FPRED, Discrete cosine transforms, spatio-temporal error propagation, error compensation, fast prediction algorithm, video codecs, Decoding, video coding, decoding, re-decode based error compensation method, mobile communication, H.263 video codec, computational complexity]
Parallel loop transformation technique for efficient race detection
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Races might result in unintended nondeterministic execution of parallel programs and thus race detection is one of the critical issues to be resolved in debugging of shared-memory parallel programs. On-the-fly race detection techniques have been developed as one of approaches for the problem. However on-the-fly race detection techniques suffer from the huge run-time overhead because the whole execution behavior of the program being debugged must be monitored at run-time. In this paper we present a practical loop transform technique which can significantly reduce the monitoring overhead required for detecting races on-the-fly in parallel programs. Our technique achieves the improvement by minimizing the number of iteration counts to be monitored of each parallel loop by transforming the original loop with the technique. An experimental performance measurement of our technique shows dramatic improvement on the monitoring overhead and it detects more races than those detected by traditional on-the-fly techniques.
[program debugging, Event detection, Instruments, iteration counts, Debugging, parallel loop transformation, Telecommunication computing, parallel programming, parallel programs, Concurrent computing, monitoring overhead, Runtime, debugging, Software, Libraries, race detection, Timing, shared-memory parallel programs, Monitoring]
Parallel and distributed multi-agent reinforcement learning
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
The application of parallel and distributed systems to multi-agent environments has attracted recent attention. Multi-agent systems are a particular type of distributed artificial intelligence system. This paper presents an approach to learning in parallel and distributed systems. A variant of the job assignment problem is chosen as an evaluation task. This is an NP-hard problem, which is relevant to many industrial application domains. Experimental results show the effectiveness of the proposed approach.
[Multiagent systems, multi-agent systems, distributed processing, production control, Control systems, Application software, Distributed computing, Intelligent robots, distributed artificial intelligence, Learning, Concurrent computing, parallel systems, multi-agent reinforcement learning, NP-hard problem, Parallel processing, scheduling, distributed systems, job assignment problem, industrial applications, Robustness, learning (artificial intelligence), Artificial intelligence, computational complexity]
Improving the performance of TCP Vegas in a heterogeneous environment
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Many results indicate that TCP Vegas exhibits better throughput and higher stability than TCP Reno in homogeneous cases where a single version exists, but it performs poorly in heterogeneous cases where two versions coexist. Hence users are delaying, even protesting, the adoption of TCP Vegas. The difference in performance is due to the fact that Reno vibrates in the high throughput level and Vegas almost stabilizes in the low level. We propose two simple approaches to address this problem, the RED (random early discard) approach and parameter adjustments to Vegas. These approaches enable Vegas to achieve its fair share of bandwidth, even gaining an advantage over Reno, encouraging users to switch their TCP from Reno to Vegas.
[Protocols, TCP Reno, bandwidth, Switches, Throughput, parameter adjustments, Computer science, transport protocols, Bandwidth, TCP Vegas, heterogeneous environment, Internet, random early discard approach, Propagation delay]
Seamless mobile transaction processing: models, protocols and software tools
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
This paper describes the operational models, protocols and software tools needed for seamless mobile transaction processing in a wireline (wired) and wireless environment. In particular, we consider the role of various interactive distributed computing models, different logical modes of programming (Imperative, declarative, subjunctive and abductive), transaction and workflow models (that relax atomicity, consistency, isolation, durability and serializability properties), new protocols and software tools that are needed.
[transaction processing, Protocols, atomicity, durability, Mobile communication, isolation, seamless mobile transaction processing, Application software, workflow models, Distributed computing, consistency, serializability properties, interactive distributed computing models, mobile computing, wireless environment, Computer networks, Power system reliability, software tools, Telecommunication network reliability, protocols, Software tools, Personal digital assistants, Mobile computing]
A multicast delivery scheme for VCR operations in a large VOD system
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Multicast transmission is one of the solutions to reduce the cost in large VOD systems. However, it is difficult to implement the interactive operations for an individual user in a multicast transmission system. We propose a frame based multicast transmission scheme to serve a large number of users and reduce additional channels while supporting VCR operations. This scheme supports both normal playback and VCR playback via the identical channels. The proposed FBMT is to deliver frames via appropriate channels based on frame type (I, P, B) of MPEG video. FBMT has superior performance over the multicast systems in terms of both initial blocking probabilities and VCR blocking probability. The initial blocking probability of the FBMT is decreased as the arrival rate is increased further, VCR blocking probability is always 0% to admitted users.
[Video on demand, FBMT, Video sharing, Buffer storage, VCR, probability, VOD, Video recording, Delay, MPEG video, Network servers, frame based multicast transmission, Unicast, performance, video on demand, Bandwidth, multicast communication, Streaming media, playback, VCR blocking probability, Web server, initial blocking probability]
Specializing the Java object serialization using partial evaluation for a faster RMI [remote method invocation]
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
The Java object serialization is designed generic method to handle all possible objects, and it performs a considerable amount of interpretation in determining the properties of objects before taking an appropriate action. In this paper, we present a mechanism by which the Java serialization is specialized using partial evaluation techniques to reduce the amount of interpretation that happens at run-time. Our approach specializes the serialization code using information about variables and arguments whose values or types are statically bound. To generate optimized code in a context-sensitive manner, it performs an interprocedural binding-time analysis. Performance measurements show an average 13% improvement in serialization, with some cases showing as high as an 18% improvement. Our approach is not restricted to serialization; rather we believe it gives a basis for a more general solution that is applicable to other aspects of Java applications.
[Performance evaluation, Computer interfaces, serialization code specialization, partial evaluation techniques, Java applications, Electrical capacitance tomography, Distributed computing, partial evaluation (compilers), Design engineering, Runtime, statically bound values, statically bound types, partial evaluation, object-oriented methods, run-time object properties interpretation, software performance evaluation, interprocedural binding-time analysis, Pervasive computing, Java, object-oriented programming, variables, Computer languages, performance measurements, Sockets, remote method invocation, context-sensitive optimized code generation, Java object serialization, remote procedure calls, arguments]
Implementing Web access control system for the multiple Web servers in the same domain using RBAC concept
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
As the Web server based system is being used more and more, having separate Web servers for each task to distribute the Web server's load are gaining much more popularity over having one main Web server to process all the tasks. When the user tries to access each Web server that contains a number of Web documents that are linked to each other via hyper-links within the domain, each Web server asks the user to follow the verification process even though the user is identical, and this prohibits the user from using the system efficiently. The role based access control method, which is the most suitable access control concept available now for the distributed Web server based system within the domain, is used in this paper. Additionally the method for controlling the level of Web document contents available to the user based on the user's access permission rights is introduced to reduce the granularity of the document content access.
[Access control, information resources, access permission rights, hyper-links, role based access control, document content access, Web access control system, Sparks, Environmental management, Computer science, Network servers, RBAC, Authentication, multiple Web servers, authorisation, Permission, Error correction, Internet, Resource management, Web server]
A proxy server management scheme for continuous media objects based on object partitioning
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
We propose a caching policy to service streams for continuous media objects over wide area networks. With the proposed policy, a group of proxy servers representing a tree-like structure are set per each local area network, providing as much transfer bandwidth as to service the expected maximum numbers of concurrent streams. Subsequently, two physical caches, the parent and each child cache, are combined and then treated as one logical cache. With this proxy server configuration, to reduce the initial latency taken to access the requested object from the central server and replacement overhead in the proxy server, each object is also logically partitioned into two parts (the front-end and rear-end partition) so as to cache as many front end partitions as possible; each partition of the objects is cached step by step in two phases. Finally, a new replacement policy is applied, where the recency, frequency and size of the requested objects are taken into consideration.
[multimedia servers, wide area networks, proxy server management scheme, Spine, bandwidth, replacement policy, caching policy, cache storage, Delay, object partitioning, Computer science, Network servers, Multicast algorithms, multimedia, local area network, Bandwidth, Streaming media, Frequency, continuous media objects, tree-like structure, Time factors, Local area networks, multimedia communication]
An efficient implementation of Virtual Interface Architecture using adaptive transfer mechanism on Myrinet
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
User-level communication is investigated by many researchers, in order to resolve the performance degradation of cluster systems due to inefficient communication protocols. It removes the kernel intervention from the critical communication path. Intel, Microsoft and Compaq introduced the Virtual Interface Architecture (VIA), a standard for user-level communication. However, the existing VIA implementation shows low performance in transferring small messages, because it uses a single mechanism to transfer messages without regard to their message size. We implement a high performance VIA, KVIA (Kaist VIA). KVIA, based on descriptor and message size, dynamically selects a proper transfer mechanism. This implementation effectively handles not only large messages but also small messages. Thus, it can be better applied to the systems that frequently use small messages (e.g., lock protocols for software distributed shared memory). The performance of KVIA is reported using round-trip latency and one-way bandwidth. Our results show the round-trip latency of 40 micro-seconds and the maximum one-way bandwidth of 950 Mbits per second, which is about 74% of Myrinet link's peak bandwidth.
[workstation clusters, high performance VIA, Protocols, software distributed shared memory, Communication system control, Myrinet, Delay, Intel, Communication standards, 950 Mbit/s, user-level communication, Virtual Interface Architecture, Bandwidth, Computer architecture, Microsoft, adaptive transfer mechanism, Size control, protocols, Web server, Context, KVIA, message passing, round-trip latency, communication protocols, performance evaluation, performance degradation, one-way bandwidth, Communication switching, VIA, cluster systems, Compaq]
Multiphase minimal fault-tolerant wormhole routing in 2D meshes
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
A fault-tolerant wormhole routing algorithm using multiphase minimal routing paths for mesh networks is proposed in this paper. When routing messages come in contact with a fault region, they always select a local shortest path around the fault-region in clockwise or counter clockwise direction. The proposed algorithm can tolerate convex fault-connected regions with four virtual channels per physical channel regardless of how processors of different f-polygons overlap. The fault regions divide each routing path into multiple minimal routing paths-a multiphase minimal routing path. The performance of multiphase minimal routing vs. minimal routing is compared by simulation.
[convex fault-connected regions, Computational modeling, network routing, multiphase minimal routing path, multiprocessor interconnection networks, simulation, performance evaluation, Routing, Circuit faults, Information systems, Counting circuits, Computer science, multiphase minimal fault-tolerant wormhole routing, local shortest path, Fault tolerance, Intelligent networks, Mesh networks, multiphase minimal routing paths, 2D meshes, performance, fault tolerant computing, Clocks]
Network Membership: a partition model for reliable mobile communication
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
We give a formal specification of a new model called Network Membership for reliable mobile communication in asynchronous distributed systems. Our approach is new in the sense that the Network Membership service does not have any join or leave procedures. We let the system flow, views are not forced and are installed with stability. The model is less restrictive than others since no consensus is required. The Network Membership allows multiple partitions to operate simultaneously and provides connectivity feedback. We have built on top of this Network Membership service an efficient reliable broadcast service that is resistant to network partitions. The protocol ensures that all recipients eventually receive the message even if a receiver has been partitioned away at any time. We show how we use an unreliable channel detector in conjunction with data forwarding and stability to achieve this goal.
[Protocols, Stability, connectivity feedback, Mobile communication, data forwarding, asynchronous distributed systems, formal specification, reliable mobile communication, mobile computing, mobile communication, Feedback, Computer applications, partition model, Broadcasting, computer network reliability, Telecommunication network reliability, Computer network management, protocols, Marine vehicles, Mobile computing, unreliable channel detector, stability, Network Membership]
Cost-based transaction coordinator algorithm implemented at persistent distributed shared virtual memory
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
In order to support processing of highly concurrent transactions at low price on distributed database server systems, a network of workstations (NOW) is used as the hardware environment. In order to share resources that are distributed among different sites of NOW and support the database functionalities, persistent distributed shared virtual memory (PDSVM) is implemented on ShusseUo, an object database system developed by Kyushu University, Japan. In ShusseUo, all workstations cooperate to perform jobs submitted by database applications. Each job consists of several transactions. These transactions are executed on PDSVM and the cost of each transaction varies according to the workstation on which the transaction runs. We present the cost-based transaction coordinator (CTC) algorithm. In CTC, the load information of a transaction is collected automatically while the transaction is running, and it is fed back when the transaction is committed. In CTC, each transaction is coordinated to a certain workstation based on its cost as calculated using the fed back information and the distribution information of the database. The algorithm is evaluated in terms of the TPC-C benchmark. The benchmark result is presented and analyzed.
[transaction processing, workstation clusters, Costs, workstation network, persistent distributed shared virtual memory, Network servers, Intelligent networks, PDSVM, Distributed databases, distributed databases, Computer networks, Hardware, Workstations, Intelligent systems, software performance evaluation, object-oriented databases, ShusseUo, virtual storage, object oriented database, workstation, Transaction databases, cost-based transaction coordinator, TPC-C benchmark, distributed shared memory systems, distributed database server systems, Deductive databases, software performance, persistent objects, concurrent transactions]
A distributed consistent global checkpoint algorithm for distributed mobile systems
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
A distributed coordinated checkpointing algorithm for distributed mobile systems is presented. A consistent global checkpoint is a set of states in which no message is recorded as received in one process and as not yet sent in another process. It is used for rollback when process failure occurs. A consistent global checkpoint must be obtained for any checkpoint initiation by any process. This paper shows a checkpoint algorithm in which the amount of information piggybacked on program messages does not depend on the number of mobile processes. The number of checkpoints is minimized under two assumptions: (1) one consistent global checkpoint is taken for concurrent checkpoint initiations and (2) a checkpoint is initiated at each handoff by mobile processes. This algorithm is thus optimal among the generalizations of Chandy and Lamport's distributed snapshot algorithm under the latter assumption.
[Checkpointing, rollback, Wireless LAN, Protocols, mobile processes, process failure, Laboratories, distributed snapshot algorithm, distributed mobile systems, Mobile communication, system recovery, checkpoint initiation, distributed algorithms, concurrent checkpoint initiations, concurrency control, coordinated checkpointing algorithm, Bandwidth, information piggybacked, Local area networks, Mobile computing, distributed consistent global checkpoint algorithm]
Volume management in SAN environment
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Logical volume managers have long been key components of a storage system. Their key features are creation of logical or virtual views of physical storage devices and support for various software RAID levels. These make it possible to overcome the limits to capacity, availability and performance of a physical storage device. Most logical volume managers are operated in a single system environment. They are not adequate for SAN (storage area network) environments where several hosts share and access a logical volume at the same time. Some recent logical volume managers are run in a multi-host environment. However, they cannot support the enterprise computing environments in which the system must support 24/spl times/7/spl times/365 uptime operations such as online resizing and online backup. We propose a logical volume manager called 'SANtopia Volume Manager' that supports multihost environments and provides various volume management features to support enterprise computing. Also it is a cluster enabled logical volume manager that maximizes the parallelism for high performance, and provides high scalability and high availability.
[SAN, Scalability, SANtopia Volume Manager, local area networks, enterprise computing, availability, Environmental management, scalability, storage management, File systems, Web and internet services, Bandwidth, Parallel processing, Optical fiber devices, storage device, Availability, online resizing, performance evaluation, online backup, RAID, multi-host environment, Storage area networks, storage area network, Disk drives, logical volume managers, multihost environments]
A comprehensive investigation of distribution in the context of workflow management
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Distribution is often discussed under different issues of interest in the context of workflow management. This paper contributes a general framework for this discussion, i.e. it presents a general framework for workflow management in distributed environments. Technical and organizational issues are analyzed that have an impact on the design of a distributed workflow management system. Within the core part of the paper we present a taxonomy enabling to compare implementation concepts of different workflow management systems especially with respect to distribution.
[Taxonomy, distributed processing, distributed environments, distribution, Distributed computing, Environmental management, Computer science, Computer architecture, Hardware, Computer networks, workflow management, Large-scale systems, Computer network management, workflow management software, distributed workflow management system, Workflow management software]
AMRB: toward location and migration transparency of services
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
In this paper, we present a new mobility-support model for applications, Application Module Request Broker (AMRB). We focus on two types of mobility: host mobility and application code mobility. These two types of mobility dynamically change the binding between applications name and location in the network. AMRB conceals these changes of banding to reduce a complexity in development of applications. In AMRB, we deal with mobile application codes that communicate with each other as Application Modules (AMs). AMRB provides AM's service transparent communication for applications by using a specifier which does not need to include any network location information. Furthermore, applications can use AM's service transparently of migration by exploiting location management mechanism. In this paper, we describe the design and implementation of AMRB and some evaluations. Also, we demonstrate a sample application that AMRB is effective for developing mobile sensor type applications.
[complexity, Home automation, object-oriented programming, AMRB, location and migration transparency, Mobile communication, Production facilities, application module request broker, Application software, mobile application codes, parallel programming, Home computing, Home appliances, mobile computing, application code mobility, host mobility, Computer networks, Hardware, mobility-support model, Mobile computing, computational complexity]
Fault diameter and fault tolerance of HCN(n,n)
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
We provide the way to make an n+1 node disjoint parallel path between any two nodes of HCN(n,n) which has a better network cost than the hypercube, and prove that the fault diameter of HCN(n,n) is dia(HCN(n,n))+4 by result. These parallel paths can reduce the time of transmitting messages between nodes, and they mean that if some nodes of HCN(n,n) would fail, there is still no communication delay time. Also, by analyzing the fault tolerance of the interconnection network HCN(n,n), we prove that there is maximal fault tolerance.
[Costs, message passing, hypercube, fault tolerance, Multiprocessor interconnection networks, Delay effects, parallel architectures, multiprocessor interconnection networks, disjoint parallel path, Electronic mail, Application software, communication delay, HCN(n, Concurrent computing, Fault tolerance, fault diameter, Parallel processing, Hypercubes, Computer networks, fault tolerant computing, n), multiprocessor interconnection network]
Improving the performance of broadcasting in ad hoc wireless networks
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Broadcasting is an often seen operation in the wireless network environment. In an ad hoc wireless network, because of the mobility of each host, it is not an easy job for every mobile host to receive the broadcast message. Applications such as current time synchronisation, paging, finding the route to a particular destination, etc., all need to broadcast frequently. A simple broadcasting strategy is flooding. However, flooding leads to many redundant broadcasts and makes poor utilization of bandwidth. We try to improve the performance of broadcasting in an ad hoc wireless network. We propose several methods to reduce the redundancy of rebroadcasting the same message and increase the reachability.
[Redundancy, bandwidth, performance evaluation, Routing, broadcast message, paging, ad hoc wireless networks, Floods, Radio broadcasting, synchronisation, broadcasting, Intelligent networks, Time division multiple access, Network topology, Wireless networks, Earthquakes, current time synchronisation, Bandwidth, redundant broadcasts, host mobility, redundancy, reachability, wireless LAN]
Avoiding faulty privileges in self-stabilizing depth-first token passing
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
This paper presents a new algorithm and its experimental results for self-stabilizing depth-first token circulation in arbitrary networks. The algorithm has an additional property of avoiding faulty privileges. Using an auxiliary token and a state with a large state space, almost all faulty privileges can be prevented. Simulation experiments show its performance compared with the previously proposed algorithm. Detailed results on the relation between network topologies and stabilization time, the effects of number of faults are revealed.
[experimental results, auxiliary token, performance evaluation, self-stabilizing depth-first token passing, State-space methods, simulation experiments, Convergence, Network topology, Fault detection, faulty privileges, Distributed control, network topologies, computer network reliability, protocols, arbitrary networks, stabilization time, Business]
Unfairness of measurement-based admission controls in a heterogeneous environment
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Admission controls are required to determine whether new connections should be admitted to networks. These controls ensure the quality of service (QoS) for data transmission. This paper introduces three measurement-based admission control algorithms called Measured Sum, Hoeffding bound and Adaptive Weight Factor. The unfairness of these algorithms in a heterogeneous environment is investigated. Simulation results indicate the fairness of the Measured Sum exceeds that of the other methods. Connections with large peak rates or traveling many hops are difficulty admitted.
[fairness, Costs, telecommunication congestion control, Measured Sum, computer networks, Adaptive Weight Factor, simulation, Quality of service, Hoeffding bound, quality of service, Adaptive control, Delay, measurement-based admission controls, Computer science, Intelligent networks, Programmable control, Admission control, Bandwidth, data transmission, heterogeneous environment, Communication system traffic control, multimedia communication]
Embedding among HCN(n,n), HFN(n,n) and hypercube
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
We analyze the embedding among HCN(n,n), HFN(n,n) and 2n-dimensional hypercube Q/sub 2n/. Further we prove that it is possible to embed HCN(n,n) to HFN(n,n) with dilation 3 and the cost of embedding HFN(n,n) to HCN(n,n) is O(n). We prove that it is possible to embed 2n-dimensional hypercube Q/sub 2n/ to HCN(n,n) and HFN(n,n) with dilation 3 and the cost of embedding HCN(n,n) and HFN(n,n) to 2n-dimensional hypercube Q/sub 2n/ is O(n).
[Embedded computing, HFN(n, Costs, Multiprocessor interconnection networks, parallel architectures, hypercube networks, Helium, HCN(n, Computer science, 2n-dimensional hypercube, Network topology, Broadcasting, Hypercubes, n), multiprocessor interconnection network]
BClassifier: a personal agent for bookmark classification
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
For the study of software systems helping users reduce personal tasks, we implemented an agent, called BClassifier, which classifies bookmarks on behalf of the user. The BClassifier is an agent based on a naive Bayesian learning algorithm which acquires knowledge through training with examples. We present the findings of experimentation where the BClassifier consistently exhibits success rates over 70%. Two other types of learning methods are compared with the naive Bayesian technique.
[information resources, BClassifier, personal agent, Software algorithms, bookmark classification, Microcomputers, World Wide Web, classification, software agents, Uniform resource locators, Computer science, Learning systems, Bayesian methods, experiment, naive Bayesian learning algorithm, Software systems, Bayes methods, Internet, Communication networks, Acceleration, IP networks, learning by example]
Dynamic voltage scaling on MPEG decoding
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
A number of research efforts have been devoted to reduce energy consumption of a processor without impacting the performance through the use of dynamic voltage scaling (DVS). The paper presents two DVS algorithms on MPEG decoding. One is DVS with delay and drop rate minimizing algorithm (DVS-DM) where voltage is determined based on previous workload only. Another algorithm scales the supply voltage according to the predicted MPEG decoding time and previous workload (DVS with predicted decoding time or DVS-PD). Simulation results show that DVS-PD improves energy efficiency as much as 56% compared to the conventional shutdown algorithm. We also found that the amount of energy saving with DVS-PD is not affected by the fluctuation of the movie stream, but is related to the error rate of the predictor, which implies that if decoding time is predicted more accurately, the DVS algorithm can be more efficient.
[Energy consumption, simulation, Mobile communication, Dynamic voltage scaling, Voltage control, Delay, DVS with predicted decoding time, predictor error rate, computer power supplies, mobile computing, energy efficiency, Motion pictures, movie stream, DVS with delay and drop rate minimizing algorithm, MPEG decoding, dynamic voltage scaling, processor energy consumption, Decoding, video coding, decoding time, decoding, energy conservation, CMOS technology, Energy efficiency, Mobile computing, portable computers]
Geometric scheduling of 2-D uniform dependence loops
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
One of the primary tasks in the area of uniform dependence loops, is predicting the execution propagation, as well as finding an optimal time schedule. In this work, the problem of scheduling using wavefront prediction is presented. The geometric concepts of time instance subspaces and execution pattern are introduced. A quite simple and low complexity scheduling algorithm is presented. The index space is split into geometric subspaces and any point can be located in them. Each point is then scheduled according to the subspace where it belongs.
[Costs, Shape, Terminology, Delay effects, Laboratories, time instance subspaces, index space, computational geometry, geometric scheduling, low complexity scheduling algorithm, 2D uniform dependence loops, processor scheduling, execution pattern, Computer science, Processor scheduling, Polynomials, optimal time schedule, wavefront prediction, computational complexity]
Optional instant locking in distributed collaborative graphics editing systems
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Real-time collaborative editing systems are distributed groupware systems that allow multiple users to edit the same document at the same time from multiple sites. A specific type of collaborative editing system is the object-based collaborative graphics editing system. Traditionally, locking has been used as the major concurrency control techniques in this type of system. This paper examines locking in a supporting role to the concurrency control technique of multi-versioning. Two types of locks are examined: object and region. Two optional and responsive locking schemes, instant locking and instant exclusive locking, are presented. Their advantages and disadvantages are discussed.
[Real time systems, Computer interfaces, Collaborative software, Control systems, Concurrency control, multi versioning system, instant locking, Sun, distributed collaborative graphics editing systems, Delay, optional instant locking, Graphics, configuration management, object-based collaborative graphics editing system, instant exclusive locking, computer graphics, distributed groupware systems, Collaboration, concurrency control, groupware, Collaborative work, real-time collaborative editing systems]
Development of artificial life based optimization system
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Describes the computer middleware called the Hyper Artificial Life (HAL) optimization system, which is based on artificial life theories and which is effective for almost all kinds of combinatorial optimization problems in our actual world. This middleware aids the efficient development of parallel processing of an application program for combinatorial optimization problems by adopting a conventional evolution procedure. The application based on this middleware has high autonomy and high robustness, and improves its performance on a parallel computer. In this case, a supply-chain management (SCM) scheduling program, which is actually used by many users, has been applied to this middleware in parallel in order to verify and evaluate HAL. In its evaluation, we found we could obtain a remarkable improvement in the performance. This model has the characteristics of reproduction, mutation and genetics, and we found a rare phenomenon, considered as emergence in the actual result. This obviously transcends the concept of many conventional algorithms and their ability for optimization. Moreover, the model has a hyper-structure, which is why we named it the Hyper Artificial Life system.
[Supply chain management, combinatorial mathematics, Supply chains, mathematics computing, emergent phenomenon, parallel processing, parallel programming, parallel computer, Concurrent computing, logistics data processing, autonomy, application program, artificial life, genetics, model hyper-structure, Parallel processing, scheduling, supply-chain management scheduling program, Robustness, software performance evaluation, middleware, client-server systems, parallel algorithms, Job shop scheduling, Hyper Artificial Life system, robustness, performance evaluation, genetic algorithms, Application software, Manufacturing industries, evolution procedure, combinatorial optimization, mutation, HAL, Processor scheduling, High performance computing, artificial life-based optimization system, reproduction]
On the switch architecture for fibre channel storage area networks
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
The fast growth of data intensive applications has caused a change in the traditional storage model. The server-to-disk approach is being replaced by storage area networks (SANs), which enable storage to be externalized from servers, thus allowing storage devices to be shared among multiple servers. Nowadays, the majority of SANs use fibre channel. The standard for fibre channel defines several issues related to the switch interface, but does not make any suggestion about the internal switch architecture to be implemented by manufacturers. We analyze the key architectural switch characteristics for building fibre channel storage area networks. To do so, our starting point is the performance analysis of two different switch architectures, identifying their strongest and weakest points, and thus taking advantage of the best features from both of them. After this first analysis, we introduce several other features in the switch, concluding with a proposed architecture that doubles network throughput while reducing response delay.
[switch architecture, Switches, fibre channel storage area networks, performance evaluation, File servers, network throughput, LAN interconnection, local area networks, telecommunication switching, storage device sharing, Storage area networks, Network servers, memory architecture, local area network, Computer architecture, Computer networks, Optical fiber devices, Manufacturing, response delay, Local area networks, performance analysis, data intensive applications]
Accelerating the continuous data in a SCI-based multimedia system
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
The SCI (Scalable Coherent Interface) is widely used in high performance clustered systems these days. However, there has not been any consideration for scheduling policies of improving transmission performance of continuous data in a SCI-based multimedia server. The scheduling policy that does not take into account the property of continuous data may hurt the overall performance. We propose two scheduling policies. The first policy is to maintain separate queues in a node in order to provide differentiated transmission services to continuous data and discrete data. The second one is to let continuous data even preempt discrete data that is being transmitted, which may bring more timely service to continuous data. We measured how the continuous data latency is reduced by simulations, which show that 8-70% reduction is expected, on an average, with scheduling policy and observed even better reduction with the second scheduling policy.
[workstation clusters, multimedia system, simulation, system buses, Throughput, Delay, Network servers, high performance clustered systems, scheduling, Scalable Coherent Interface, Workstations, Web server, multimedia servers, FDDI, Multimedia systems, continuous data, data latency, scheduling policies, discrete data, Computer science, Processor scheduling, SCI, Acceleration, differentiated transmission services, multimedia server]
An optimal scheduling algorithm based on task duplication
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
The task scheduling problem in distributed memory machines is to allocate the tasks of an application into processors in order to minimize the total execution time. This is known as an NP-complete problem. Under the condition where the communication time is relatively shorter than the computation time for every task, the task duplication based scheduling (TDS) algorithm proposed by Darbha and Agrawal (1998) generates an optimal schedule. We propose an extended TDS algorithm whose optimality condition is less restricted than the TDS algorithm. Given a DAG where the condition is met, our algorithm has the time complexity of O(|V|/sup 2/d/sup 2/) where |V| represents the number of tasks, and d represents the maximum degree of tasks.
[task duplication, Optimal scheduling, optimal scheduling algorithm, task scheduling problem, time complexity, directed acyclic graph, Application software, NP-complete problem, Scheduling algorithm, Equations, Computer science, Processor scheduling, resource allocation, communication time, computation time, directed graphs, distributed algorithms, Clustering algorithms, execution time, distributed memory systems, scheduling, distributed memory machines, optimality condition, computational complexity]
CORBA wrapping of legacy scientific applications using remote variable scheme
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
A distributed object is a reusable, self-contained piece of software that cooperates with other objects on the same machine or across a network in a plug-and-play fashion via a well-defined interface. The Numerical Propulsion System Simulation (NPSS) attempts to provide a collaborative aircraft engine design and simulation environment based on this concept. Many scientific applications in aerodynamics and solid mechanics are written in Fortran. Refitting these legacy Fortran codes with distributed objects can increase the code reusability. The remote variable scheme provided in NPSS helps programmers easily migrate the Fortran codes towards a client-server platform. This scheme gives the client the capability of accessing the variables at the server site. Through the operator overloading features in C++, remote variables can be used in much the same way as traditional variables. The remote variable scheme adopts the lazy update approach and the prefetch method. Preliminary performance evaluation shows that communication overhead can be greatly reduced.
[Numerical Propulsion System Simulation, NPSS, digital simulation, prefetch method, aerospace simulation, legacy scientific applications, lazy update, collaborative aircraft engine design, legacy Fortran codes, Software reusability, distributed object management, client-server system, client-server systems, remote variable scheme, software reuse, Prefetching, CAD, performance evaluation, Aerodynamics, C++ language, Application software, Wrapping, Aircraft propulsion, Programming profession, CORBA, Collaboration, distributed objects, software reusability, plug-and-play, Numerical simulation, Solids, FORTRAN]
Ethernet Wrapper: extension of the TCP Wrapper
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
One of the popular network security programs supporting host access control is the 'TCP Wrapper' (Venema, 1992). TCP Wrapper is a software-only system and many computers connected to the Internet are using it. However, TCP Wrapper does 'IP address-based' access control. The IP address is not such a reliable source when authenticating a host. We point out two possible attacks against the TCP Wrapper, propose a new way to prevent them, and describe the prototype implementation, Ethernet Wrapper. By adding an Ethernet address check, we augmented the TCP Wrapper. The test results showed that Ethernet Wrapper can prevent such attacks effectively.
[Access control, telecommunication security, Ethernet networks, network security, host authentication, IP address-based access control, TCP Wrapper, local area networks, Computer science, host access control, transport protocols, Authentication, Prototypes, Ethernet Wrapper, TCPIP, authorisation, address check, System software, Internet, Computer security, National security]
Least Popularity-Per-Byte Replacement algorithm for a proxy cache
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
With the recent explosion in usage of the World Wide Web, the problem of caching Web objects has gained considerable importance. The performance of these Web caches is highly affected by the replacement algorithm. Today, many replacement algorithms have been proposed for Web caching and these algorithms use the on-line fashion parameters. Recent studies suggest that the correlation between the on-line fashion parameters and the object popularity in the proxy cache are weakening due to the efficient client caches. We suggest a new algorithm, called Least Popularity Per Byte Replacement (LPPB-R). We use the popularity value as the long-term measurements of request frequency to make up for the weak point of the previous algorithms in the proxy cache and vary the popularity value by changing the impact factor easily to adjust the peformance to needs of the proxy cache. We examine the performance of this and other replacement algorithms via trace driven simulation.
[information resources, Web cache, Content based retrieval, request frequency, trace driven simulation, Telecommunication traffic, performance evaluation, World Wide Web, Explosions, cache storage, online fashion parameters, Computer science, client cache, Network servers, Least Popularity-Per-Byte Replacement, Traffic control, Frequency, proxy cache, Internet, Web sites, Web server, software performance evaluation]
Mobile IP network supporting private IP addresses utilizing regional registration and NAT function
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
It is widely recognized that IP-based mobile network will be a dominant trend and that the address starvation problem and scalable mobility management for mobile nodes are important issues for mobile IP networks. In order to cope with these issues, we propose an approach to realize mobile IP network supporting private addresses for mobile nodes. Our approach introduces regional registration of mobile nodes (Hierarchical Mobile IPv4) and coordinates NAT and DNS functions with the mobile IP protocol. It enables a mobile node to be assigned a global address temporally in a visited network and to accept a call initiated by a correspondent node connected to the global IP network. This paper describes the detailed design of our approach.
[Home automation, Protocols, mobile IP network, Laboratories, address starvation problem, mobile IP protocol, Mobile communication, private addresses, Mobile radio mobility management, Research and development, Next generation networking, radio access networks, scalable mobility management, Collaboration, private IP addresses, IP-based mobile network, IP networks, protocols, regional registration, Network address translation, mobile nodes, NAT function]
A secure transaction environment for workflows in distributed systems
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
The paper describes the design of a model as well as an architecture to provide support for distributed advanced workflow transactions. We discuss the application of transaction concepts to activities that involve integrated execution of multiple tasks over different processes. This kind of application is described as transactional workflow. The classical commit protocol, used in many commercial systems, is not suitable for use in multilevel secure distributed workflow database systems that use a locking protocol for concurrency control. The reason is that it is not possible for a locking protocol to guarantee that read locks won't be released by a subtransaction during its window of uncertainty-the period after a participant has voted yes to commit, but before it receives the commit or abort decision from the coordinator, possibly resulting in nonserializable executions. A distinguishing feature of the proposed workflow transaction support system is the ability to manage the arbitrary distribution of business processes over multiple workflow management systems.
[transaction processing, Protocols, Uncertainty, secure transaction environment, Distributed computing, multiple tasks, arbitrary business process distribution, Computer architecture, distributed databases, multilevel secure distributed workflow database systems, Database systems, Gratings, integrated execution, workflow management software, Workflow management software, Business, architecture, read locks, Concurrency control, distributed advanced workflow transactions, security of data, concurrency control, Systems engineering and theory, locking protocol, subtransaction, business data processing]
Schemes for utilizing efficiently disk bandwidth and buffer in video server
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
A video server which stores video streams should serve more users with the requested streams rapidly, satisfying their time constraints. For this purpose, the storage system of the video server needs a large disk bandwidth and buffer capacity. However, because the given disk bandwidth and buffer capacity are limited, technologies that utilize effectively the given disk bandwidth and buffer capacity without waste might be needed. We propose the schemes that not only support more users but also maintain the short startup latency by utilizing the disk bandwidth and buffer efficiently which also considers time constraint characteristics of the video stream. When multiple streams are served in the storage system of the multiple disk environment, the short startup latency might be induced by asynchronizing the disk head's moves. If the disk head also moves towards only one side, being based on double buffering, the limited disk bandwidth is saved owing to the reduced disk seek latency so that more streams might be served with satisfying the time constraint characteristics of the streams. The correctness of the proposed schemes is not only analyzed theoretically but also evaluated through simulations.
[buffer storage, video server, Buffer storage, Computational modeling, Computer simulation, video discs, time constraints, Educational institutions, disk bandwidth, startup latency, video servers, Delay, disk head, simulations, buffer capacity, Network servers, Analytical models, time constraint, Bandwidth, Streaming media, video streams, Time factors]
Selecting an audio redundancy codec combination for error control in Internet telephony
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
In the current Internet, the quality of service of audio application is not guaranteed due to variable loss and delay. Forward error correction (FEC)-based error control mechanisms which send the redundant information with the main information can be used to minimize the effects of packet loss. However, increasing the amount of redundancy may waste bandwidth and increase the end-to-end delay. The existing FEC-based error control mechanisms select the codec combination only based on the network loss rate. These mechanisms do not consider the end-to-end delay by increasing the amount of redundancy. This may result in exceeding the acceptable maximum delay. We propose a selection mechanism of the audio codec combination for error control. In simulations, we confirm that the proposed algorithm reduces the end-to-end delay and the packet loss together.
[simulation, Jitter, packet loss, Delay, audio coding, end-to-end delay, Web and internet services, Bandwidth, redundancy, audio redundancy codec, speech codecs, Codecs, Packet switching, Redundancy, bandwidth, network loss rate, forward error correction, quality of service, delay, delays, Forward error correction, variable loss, redundant information, Error correction, Internet telephony]
Improving workload balance and code optimization in processor-in-memory systems
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
PIM (Processor-In-Memory) architectures have been proposed in recent years. One major objective of PIM is to reduce the performance gap between the CPU and memory. To exploit the potential benefits of PIM, we designed a statement base parallelizing system-SAGE. In this paper, we extend this system to achieve better performance by devising several comprehensive optimizing techniques, which include IMOP (Intelligent Memory Operation) recognition, tiling for PIM, and a precise mechanism to get load-balanced execution schedule. The experimental results are also presented and discussed.
[Costs, workload balance, processor-in-memory systems, Memory architecture, Random access memory, statement base parallelizing system, performance evaluation, performance gap, SAGE, parallel processing, Delay, intelligent memory operation, load-balanced execution schedule, Program processors, Processor scheduling, resource allocation, Computer architecture, Complex networks, Hardware, Coprocessors, code optimization]
A distributed implementation of Structured Gamma
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Presents a distributed implementation of the Structured Gamma programming language, a language based on the Gamma multi-set rewriting paradigm. Structured Gamma offers, in addition to the advantages introduced by Gamma, implicit concurrent behavior and a type system where not only types themselves are defined but also the automatic verification of user-defined types at compilation time. The problems and mechanisms involved in an MPI-based implementation of Structured Gamma using a type-checking engine based on the most general unifier (MGU) are investigated.
[user-defined types, Structured Gamma programming language, program verification, application program interfaces, structured programming, Gamma multi-set rewriting paradigm, most general unifier, Engines, distributed implementation, type system, Robustness, Functional programming, parallel languages, rewriting systems, message passing, abstract data types, data integrity, MPI-based implementation, Computer languages, type-checking engine, Parallel programming, automatic compile-time verification, implicit concurrent behavior, Data models, MGU, Chemical elements, Arithmetic]
Tightening of cell discarding method in RED for improving TCP over ATM network
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
TCP suffers from low performance over Asynchronous Transfer Mode networks. This is mainly because during the phase of congestion, ATM drops cells without taking into account the effect this has on the upper layer protocols. We propose an enhancement of the cell discarding method in the RED algorithm supporting TCP over ATM traffic with an UBR service. We call this method TCD-RED (tightening of cell discarding). The proposed method uses the advantage of RED to keep the average queue size as low as possible. The TCD method is optimal for discarding cells in a TCP over ATM network to overcome shortcomings of RED. We propose a TCD-RED that improves TCP performance dramatically on congested networks and guarantees fairness among multiple connections.
[Transport protocols, TCP, telecommunication congestion control, queue size, Switches, Telecommunication traffic, asynchronous transfer mode, UBR service, TCD-RED, Delay, ATM network, Postal services, Degradation, ATM traffic, Intelligent networks, transport protocols, RED algorithm, Traffic control, Internet, protocols, congestion, telecommunication traffic, Asynchronous transfer mode, tightening of cell discarding method]
An efficient cache invalidation scheme for mobile wireless environments
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
The stateless-based cache invalidation schemes for wireless environments can be categorized into either asynchronous or synchronous cache invalidation according to the broadcasting way of invalidation report. However, if the asynchronous cache invalidation scheme attempts to support local processing of read-only transaction, a critical problem may occur; the asynchronous invalidation reports provide no guarantee of waiting time for mobile transactions requesting commit. To solve this problem, the server in our approaches broadcasts two kind of messages, asynchronous invalidation report to reduce transaction latency and periodic guide message to avoid the uncertainty of waiting time for the next invalidation report. This paper presents a simulation-based analysis on the performance of the suggesting algorithms. The simulation experiments show that the local processing algorithms of read-only transaction based on asynchronous cache invalidation scheme get better response time than the algorithm based on synchronous cache invalidation scheme.
[Algorithm design and analysis, mobile transactions, Uncertainty, asynchronous cache invalidation, Mobile communication, cache storage, digital simulation, stateless-based cache invalidation schemes, Delay, radio access networks, Computer science, simulation-based analysis, Analytical models, mobile computing, cache invalidation scheme, mobile wireless environments, Bandwidth, Broadcasting, transaction latency, Performance analysis, Mobile computing]
An improved packet pair method for filtering estimation noise and fast convergence in measuring bottleneck bandwidth
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
A new scheme is proposed to speed up a known bandwidth measuring method which employs potential bandwidth for filtering out noises (in estimation) from time compression caused by a packet queuing ahead of two probe packets. Instead of incrementing the potential bandwidth by a fixed amount as in the original method we increase the potential bandwidth exponentially for faster convergence. To retain its filtering capability as well as its agility to adapt to new bottleneck bandwidth, each trial potential bandwidth (PB) is adjusted using MAX and MIN as upper bound and lower bound. An experiment using known bandwidths shows 45-89% improvement in convergence time.
[Filtering, convergence, packet pair method, performance evaluation, Time measurement, Noise measurement, estimation noise, Convergence, packet queuing, Network servers, Filters, experiment, time compression, bandwidth measuring method, Bandwidth, noise filtering, bottleneck bandwidth, Internet, Velocity measurement, Web server]
Multicast-based Distributed LVS (MD-LVS) for improving scalability and availability
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Clustering with a single-system image view is the most commonly used approach to increase throughput of a Web site. There are two types of clustering architecture: centralized IP cluster and distributed IP cluster. In the centralized IP cluster the load balancer distributes incoming requests of clients to an appropriate real server based on load characteristics. However the availability and scalability of the cluster system is low, since this kind of solution creates a single-point-of-failure and the total throughput of the cluster is limited by the performance of the load balancer. We propose the Multicast-based Distributed LVS (MD-LVS), which combines the centralized IP cluster and the distributed IP cluster. There are multiple load balancers, and each of the load balancers has an individual and independent cluster group, which consists of several real servers. This mechanism efficiently can improve scalability and availability of the cluster system, since it is easy to expand the cluster system and may avoid the entire system failure.
[centralized IP cluster, load balancing, Scalability, Throughput, availability, Distributed computing, scalability, Multicast-based Distributed LVS, resource allocation, Computer architecture, multicast communication, distributed IP cluster, Web server, client-server systems, system failure, client server system, MD-LVS, clustering architecture, Computer science, transport protocols, Load management, Explosives, Internet, Web sites, Web site]
An extended fault-tolerant link-state routing protocol on the Internet
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Link-state routing protocols, such as OSPF and IS-IS, are widely used on the Internet. In link-state routing protocols, global network topology is first collected at each node. A shortest path tree (SPT) is then constructed by applying Dijkstra's shortest path algorithm at each node. Link-state protocols normally require the flooding of new information to the entire (sub)network after changes in any link state (including link faults). Narvaez et al. (2000) proposed a fault-tolerant link-state routing protocol without flooding. The idea is to construct a shortest restoration path for each uni-directional link fault. Faulty link information is distributed only to the nodes in the restoration path and only one restoration path is constructed. It is shown that this approach is loop-free. However, the approach of Narvaez et al. is inefficient when a link failure is bi-directional, because a restoration path is uni-directional and routing tables of nodes in the path are partially updated. In addition, two restoration paths may be generated for each bi-directional link fault. We extend the Narvaez protocol to efficiently handle a bi-directional link fault by making the restoration path bi-directional. Several desirable properties of the proposed extended routing protocol are also explored.
[shortest path algorithm, Buildings, Floods, Delay, Computer science, extended fault-tolerant link-state routing protocol, shortest path tree, Fault tolerance, Intelligent networks, restoration path, Network topology, telecommunication network routing, Bidirectional control, global network topology, bi-directional link fault, Routing protocols, fault tolerant computing, Internet, protocols]
An efficient recovery scheme for mobile computing environments
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
This paper presents an efficient recovery scheme based on checkpointing and message logging for mobile computing systems. For the efficient management of checkpoints and message logs, a movement-based scheme is proposed. Mobile hosts carrying their recovery information to the nearby mobile support station can recover instantly in case of a failure, however, the cost to transfer the recovery information must be high. On the other hand, the recovery information remaining dispersed over a number of support stations visited by mobile hosts must incur very high recovery cost. To balance the failure-free operation cost and the recovery cost, in the proposed scheme, the recovery information of a mobile host remains at the visited support stations while the host moves within a certain range. Only when the host moves out of the range, the recovery information is transferred to a nearby mobile support station. As a result, the proposed scheme can control the information transfer cost as well as the recovery cost.
[Checkpointing, checkpointing, Energy consumption, Costs, fault tolerance, information transfer cost, Batteries, Distributed computing, system recovery, Computer science, message logging, mobile computing, Wireless networks, Fault tolerant systems, mobile hosts, Bandwidth, failure-free operation cost, fault tolerant computing, computer network reliability, Mobile computing, system recovery scheme]
Mapping strategies for switch-based cluster systems of irregular topology
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Mapping virtual process topology to physical processor topology is one of the most important issues in parallel computing. The mapping problem for switch-based cluster systems of irregular topology is very complicated due to the connection irregularity and routing complexity. This paper proposes two mapping schemes for irregular cluster systems, which try to map the nearest neighbors in the process topology to physically adjacent processors. In addition, an application-oriented performance metric, weighted cardinality, is introduced to represent the quality of mapping. A simulation study shows that, for a virtual topology of a 16/spl times/16 mesh, the proposed mapping schemes result in better mapping quality and about 15/spl sim/20% shorter communication latency compared to random mapping. The proposed algorithms should also be beneficial when they are applied to metacomputing and cluster of cluster systems, where the communication costs are an order of magnitude different depending on the relative position of the processor nodes.
[Measurement, graph theory, multiprocessor interconnection networks, simulation, Switches, MPI, virtual process topology, switch-based cluster systems, Telecommunication computing, Delay, routing complexity, weighted cardinality, Network topology, Physics computing, communication costs, parallel computing, irregular topology, message passing, network routing, Telecommunication switching, performance evaluation, Routing, physical processor topology, communication latency, Communication switching, Nearest neighbor searches, metacomputing, connection irregularity, application-oriented performance metric]
Analysis of deterministic routing in k-ary n-cubes with virtual channels
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Adding virtual channels to wormhole-routed networks greatly improves performance because they reduce blocking by acting as "bypass" lanes for non-blocked messages. Although several analytical models have been proposed in the literature for k-ary n-cubes with deterministic routing, most of them have not included the effects of virtual channel multiplexing on network performance. This paper proposes a new and simple analytical model to compute message latency in k-ary n-cubes with an arbitrary number of virtual channels. Results from simulation experiments confirm that the proposed model exhibits a good degree of accuracy for various network sizes and under different operating conditions. The proposed model is then used to investigate the relative performance merits of two different organisations of virtual channels.
[multiprocessor interconnection networks, simulation, Telecommunication traffic, experiments, message latency, Delay, Analytical models, network performance, blocking, Bandwidth, Hypercubes, Communication system traffic control, Logic, k-ary n-cubes, deterministic routing, message passing, Computational modeling, network routing, multiprocessor interconnection, virtual channels, performance evaluation, Routing, wormhole-routed networks, multiplexing, System recovery]
Fault-tolerant routing and disjoint paths in dual-cube: a new interconnection network
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
In this paper, we introduce a new interconnection network, the dual-cube, its topological properties, and the routing/broadcasting algorithms in the dual-cube. The advanced subjects such as fault-tolerant routing and constructing multiple disjoint paths in dual-cubes are also included in this paper. The binary hypercube, or r-cube, can connect 2/sup r/ nodes. In contrast, a dual-cube with r links for each node, F/sub r/, can connect 2/sup 2r-1/ nodes while keeps most of topological properties of hypercubes. Fault-tolerant routing and constructing multiple disjoint paths in dual-cubes can be solved elegantly using a new structure, called extended cube. We show that for any two nonfaulty nodes s and t in F/sub r/ which contains up to r-1 faulty nodes, we can find a fault-free path s to t, of length at most 3d(s,t) in O(r) optimal time, where d(s,t) is the distance between s and t. We also show that, in a fault-free F/sub r/, r disjoint paths s to t, of length at most d(s,t)+6 can be constructed in O(r/sup 2/) optimal time.
[nonfaulty nodes, fault-tolerant routing, r-cube, Multiprocessor interconnection networks, Routing, hypercube networks, Electronic mail, binary hypercube, Concurrent computing, Fault tolerance, Intelligent networks, Casting, broadcasting algorithms, dual-cube, interconnection network, Fault tolerant systems, multiple disjoint paths, Hypercubes, Computer networks, fault tolerant computing, disjoint paths]
Replication for a distributed multimedia system
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Replicating data and services at multiple networked computers increases the service availability of distributed systems. This paper presents the design and implementation architecture of a replication mechanism for a distributed multimedia system medianode which is developed as an infrastructure to share multimedia-enhanced teaching materials among lecture groups. With the replication mechanism, medianode provides enhanced access to presentation materials in both connected and disconnected operation modes. The main contribution of this paper is the identification of new replication requirements in distributed media systems and a multicast-based update propagation mechanism by which not only the update events are signaled, but also the updated data are exchanged between replication managers.
[Availability, multimedia-enhanced teaching materials, multicast-based update propagation, replicated databases, data exchange, Multimedia systems, Delay effects, multimedia databases, medianode, Quality of service, service availability, lecture groups, disconnected operation mode, Information technology, distributed multimedia system, networked computers, Fault tolerant systems, data replication, Prototypes, multicast communication, Signal processing, connected operation mode, computer aided instruction, Resource management]
An efficient buffer control and packet scheduling for the access point of IP networks and ATM network
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Various multimedia services such as network games and other audio/video applications cause traffic increases in the Internet, necessitating the interconnection of the IP networks and the ATM network. To interconnect IP networks and the ATM network effectively, efficient traffic control and scheduling schemes are required. We propose a novel buffer control and packet scheduling algorithm for the access point, which can cope effectively with the changing status of the network through the service point concept and the OAM cells.
[Buffer storage, network games, asynchronous transfer mode, ATM network, Counting circuits, Web and internet services, network traffic control, TCPIP, Traffic control, scheduling, Communication system traffic control, packet scheduling, IP networks, multimedia communication, access point, OAM cells, Multimedia computing, audio applications, Scheduling algorithm, multimedia services, video applications, Internet, buffer control, telecommunication traffic, Asynchronous transfer mode]
The parallel processing of spatial selection for very large geo-spatial databases
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Earth science (ES) applications handle very large geo-spatial data sets and interactive response time is required by its query processing. Spatial selection is one of the very important basic operations for geo-spatial databases. It retrieves all the objects that intersect with a given point or rectangle. We present a novel approach for the parallel processing of spatial selection of very large geo-spatial databases using partitioned parallelism. To evaluate this approach, we use the Extended Sequoia 2000 benchmark, which has real world data and real queries. In addition, we use an actual object database management system, ShusseUo, which we developed previously. The experimental results of parallel processing of spatial selection show good speed-up.
[spatial selection, very large geospatial databases, partitioned parallelism, Relational databases, Geoscience, visual databases, geographic information systems, parallel processing, Delay, query processing, Information science, Filters, very large databases, geospatial data sets, Parallel processing, Database systems, software performance evaluation, interactive response time, Earth science applications, experimental results, object-oriented databases, ShusseUo, object oriented database, Spatial databases, Rivers, Query processing, Extended Sequoia 2000 benchmark]
Optimization of nested invocation on replicas in object-based systems
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
An object-based system is composed of multiple objects which are encapsulation of data and methods. Objects are replicated in order to increase performance and throughput. If a method t is invoked on multiple replicas and each instance of t invokes another update method u, u is performed multiple times on replicas and then the replicas get inconsistent, i.e. redundant invocations. In addition, since each instance of a method issues a request to its own quorum, more number of the replicas are manipulated than the quorum number, i.e. quorum explosion. We discuss a protocol named QB (quorum-based) one to resolve the redundant invocations and quorum explosion. We show the number of replicas manipulated and requests issued are reduced by the QB protocol.
[Encapsulation, Availability, Protocols, object-oriented programming, replicas, Throughput, Explosions, object-based systems, nested invocation, optimisation, protocol, performance, optimization, multiple objects, Systems engineering and theory, throughput, protocols, data encapsulation]
Metadata management of the SANtopia file system
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Recently, LINUX cluster file systems using the storage area network (SAN) have been developed. The Global File System (GFS) implemented at the University of Minnesota is such an example. These systems do not use a central file server, and instead, multiple clients sharing the whole disk storage through Fibre Channel can freely access disk storage and act as file servers. Accordingly, they offer advantages such as availability, load balancing, and scalability.
[Unix, load balancing, Scalability, metadata management, File servers, LINUX cluster file systems, availability, disc storage, Environmental management, scalability, Fibre Channel, storage management, multiple clients, File systems, resource allocation, network operating systems, file servers, Computer networks, meta data, Multimedia systems, disk storage, SANtopia file system, Storage area networks, storage area network, Linux, Load management, Power capacitors, Global File System]
Performance and scalability analysis on client-server workflow architecture
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
We excogitate a performance analytic model and describe its analysis results conceived to be helpful in the understanding of the spectrum of possibilities for large-scale workflow architecture. The analytic model is extended to represent several types of client-server workflow architectures. Especially, we focus on performance estimates of the conventional workflow management systems that are characterized by the client-server workflow architectures. The development of a workflow management system is typically a large and complex task. Decisions need to be made about the hardware and software platforms, the data structures, the algorithms, and the interconnection of various modules utilized by various users and administrators. These design decisions are further complicated by the requirements, such as scalability, flexibility, robustness, speed, and usability. We are particularly concerned about issues of scalability to see how well the client-server workflow architecture is dealing with the large amount of workcases. Finally, we graphically show the comparisons of performance evaluation results for several types of client-server workflow architectures on behalf of the single-server, and the multiple-server workflow systems on the distribution environment.
[client-server systems, Scalability, Software algorithms, performance evaluation, performance estimates, Data structures, scalability, workflow management systems, usability, workflow architecture, scalability analysis, Computer architecture, Hardware, Robustness, data structures, Performance analysis, Large-scale systems, Usability, workflow management software, Workflow management software, performance analysis, client-server system]
Adaptive parallel distributive join algorithm for skewed data
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
We present an adaptive version of the parallel distributive join (DJ) algorithm that we proposed in (Chung and Yang, 1996). The adaptive parallel DJ algorithm can handle the data skew in operand relations efficiently. We implemented the original and adaptive parallel DJ algorithms on a network of Alpha workstations using the Parallel Virtual Machine (PVM). We analyzed the performance of the algorithms, and compared it with that of the parallel Hybrid-Hash (HH) join algorithms. Our results show that the parallel DJ algorithms perform comparably with the parallel HH join algorithms over the entire range of the number of processors used and for different join selectivities. A significant advantage of the parallel DJ algorithms is that they can easily support non-equijoin operations.
[Algorithm design and analysis, workstation clusters, parallel algorithms, relational algebra, PVM, nonequijoin operations, Virtual machining, parallel Hybrid-Hash join algorithms, Partitioning algorithms, Parallel Virtual Machine, relational databases, Parallel algorithms, Delay, Computer science, adaptive parallel distributive join algorithm, Alpha workstation network, algorithm performance, Clustering algorithms, operand relations, Hardware, Workstations, Performance analysis, software performance evaluation, data skew]
Optimisation of the performance of neural network based pattern recognition classifiers with distributed systems
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
In this study, a new algorithm with distributed systems is proposed in order to optimise the structure of the classifiers, which have great importance in pattern recognition. The algorithm is applied to a multilayer neural network classifier, which uses the rule of backpropagation learning. The long process period is shortened and expected high operation speed is achieved in pattern recognition by minimizing the hardware realization of the classifier.
[multilayer perceptrons, pattern classification, multilayer neural network classifier, performance evaluation, Pattern recognition, backpropagation learning, Distributed computing, Multi-layer neural network, Concurrent computing, Power engineering computing, High performance computing, Physics computing, Neural networks, distributed algorithms, backpropagation, distributed systems, Computer networks, Power system reliability, pattern recognition classifiers]
Enhancement of the CBT multicast routing protocol
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
We propose a simple practical scheme for many-to-many IP multicasting. The proposed scheme is based on the core based tree (CBT) protocol, and designed to enhance the CBT in the viewpoints of tree cost and traffic concentration. In the scheme, each group user is simply connected to the nearest core router in the network. Each core router forwards the source traffic to networks via the pre-configured backbone core trees spanning all the core routers in the network. To ensure that the backbone core tree keeps only the core routers with active group users, the core routers that have no downstream users are removed from backbone core tree. By experiments, we see that the proposed scheme significantly improves the existing CBT scheme in terms of tree cost and traffic concentration.
[Costs, Scalability, core routers, Spine, Quality of service, Telecommunication traffic, CBT multicast routing protocol, pre-configured backbone core trees, Delay, Postal services, core router, multicast communication, Routing protocols, protocols, Classification tree analysis, source traffic, core based tree protocol, trees (mathematics), traffic concentration, tree cost, Multicast protocols, backbone core tree, many-to-many IP multicasting, active group users, telecommunication network routing, telecommunication traffic]
Dynamic update of aggregated routing information for hierarchical QoS routing in ATM networks
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
For achieving scalable and QoS-ware in large ATM networks, the ATM Private Network-to-Network Interface (PNNI) adopts hierarchical routing which reduces nodal and link information. In order to perform hierarchical routing efficiently, the network will consist of subnetworks, called peer groups, and peer groups will advertise the aggregated information periodically which is based on the time-based update interval or on an event driven basis. The time-based update policy is not adequate to cope with dynamic network traffic. Therefore, we propose a dynamic update policy, referred to as the cost-based update (DCU) policy, to enhance the accuracy of aggregated information and the performance of hierarchical routing, while decreasing the frequency of re-aggregation and information distribution and the overhead of communication. In our simulations, we compare the DCU policy with PNNI time-based update (DCU) policy, full update (FU) policy, and logarithm of residual bandwidth update (LRBU) policy. Our simulation results indicate that the proposed DCU policy yields better performance while significantly reduces the frequency of re-aggregation and the amount of distributed aggregation information.
[peer groups, wide area networks, dynamic network traffic, Scalability, time-based update interval, simulation, asynchronous transfer mode, dynamic update policy, ATM Private Network-to-Network Interface, Intelligent networks, Network topology, logarithm of residual bandwidth update policy, Bandwidth, time-based update policy, Hysteresis, aggregated routing information update, Peer to peer computing, hierarchical routing, hierarchical QoS routing, Routing, quality of service, Computer science, cost-based update policy, full update policy, performance, Information security, telecommunication network routing, Frequency, telecommunication traffic, ATM networks]
A fast table update scheme for high-performance IP forwarding
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
The construction of routing tables has been studied extensively. Although existing work has certain advantages, it either uses complicated data structures which result in large storage requirements and high complexity for updating/building the forwarding table, or it is not scalable to fit in IPv6. Lampson et al. (1999) proposed an IP lookup algorithm which performs binary search on prefixes (BSP). The algorithm is attractive, even for IPv6, because of its bounded worst-case memory requirement. For achieving fast forwarding, the cost is the slowing down of insertion. Although this can be justified, the performance of routing-table reconstruction in BGP is too time-consuming to handle frequent route updates. We propose a fast forwarding table construction algorithm, which can handle more than 4,000 route updates per second. Moreover, it is simple enough to fulfil the need of fast packet forwarding. By using the modified multiway search tree, we can further reduce the depth of the tree and eliminate storage for pointers. This reduces the forwarding table size and shortens the lookup time.
[Packet switching, Costs, high-performance IP forwarding, fast table update scheme, Laboratories, Random access memory, Switches, Routing, Data structures, Telecommunications, route updates, table lookup, fast packet forwarding, modified multiway search tree, fast forwarding table construction algorithm, telecommunication network routing, Hardware, Internet, protocols, routing tables, lookup time, pointers]
A distributed location management scheme for mobile hosts
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
With the increasing growth in mobile computing devices and wireless networks, users are able to access information from anywhere and at anytime. In such situations, the issues of location management for mobile hosts are becoming increasingly significant. Different location management schemes such as Columbia University's mobile IP scheme and IETF mobile IP have been proposed. In this paper, we propose a new distributed location management scheme and discuss the advantages of the proposed scheme over the others. The paper then considers the issues of multicasting in the proposed architecture.
[mobile IP scheme, Mobile communication, Routing, wireless networks, Application software, mobile computing devices, distributed location management scheme, mobile computing, Unicast, Wireless networks, mobile hosts, IETF mobile IP, multicasting, Computer architecture, multicast communication, Computer networks, Communications technology, Impedance, wireless LAN, Mobile computing]
Parallel routing in hypercube networks with faulty nodes
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
The concept of strong fault-tolerance was introduced to characterize the property of parallel routing. A network G of degree d is said to be strongly fault-tolerant if with at most d-2 faulty nodes, any two nodes u and v in G are connected by min{deg/sub f/(u), deg/sub f/(v)} node-disjoint paths, where deg/sub f/ (u) and deg/sub f/ (v) are the numbers of non-faulty neighbors of the nodes u and v in G, respectively. We show that the hypercube networks are strongly fault-tolerant and develop an algorithm that constructs the maximum number of node-disjoint paths in a hypercube network with faults. Our algorithm is optimal in terms of time and length of node-disjoint paths.
[Multiprocessor interconnection networks, network routing, strong fault tolerance, Routing, hypercube networks, parallel processing, parallel routing, Computer science, Intelligent networks, Fault tolerance, Network topology, node-disjoint paths, Broadcasting, Hypercubes, Computer networks, fault tolerant computing, faulty nodes]
Optimistic transaction processing algorithms in pure-push and adaptive broadcast environments
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Considering the properties of mobile computing environments, push-based data dissemination systems have lately attracted considerable interest. However, the skewed access pattern of mobile clients makes the average wait time worse and they may want to request the data object to the server explicitly through the backchannel. We call the broadcast model supporting backchannels adaptive broadcast. We devise new algorithms for adaptive broadcast based on our previous works; that is, we divide data objects which the server maintains into push-data and pull-data. Clients have to explicitly request data objects in pull-data. Maintaining transactional consistency in both pure-push and adaptive broadcast environment is our main concern. We also evaluate the performance behavior through a simulation study.
[transaction processing, client-server systems, Costs, simulation, performance evaluation, Information retrieval, Data engineering, push-based data dissemination, transactional consistency, Environmental management, Delay, Computer science, adaptive broadcast environment, mobile computing, mobile clients, pure-push environment, Broadcasting, optimistic transaction processing algorithms, backchannels, push-data, Computer science education, Systems engineering education, pull-data, Mobile computing]
On some properties of k-ary n-cubes
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
The k-ary n-cube has been used as the underlying topology for most practical multicomputers, and has been extensively studied in the past. We investigate some properties of this network. In particular, we study the problem of finding the number of nodes located i hops away from a given node (surface area) and the number of nodes located within i hops away from a given node (volume) in both the unidirectional and bidirectional k-ary n-cube, and have derived exact expressions calculating these numbers. These results are very useful when studying, for example, the spanning tree structure of the k-ary n-cube and the problem of resource placement in this network.
[Tree data structures, unidirectional k-ary n-cube, Hamming distance, Multiprocessor interconnection networks, parallel architectures, multiprocessor interconnection, multiprocessor interconnection networks, trees (mathematics), topology, resource placement, Routing, Delay, spanning tree structure, bidirectional k-ary n-cube, Network topology, resource allocation, nodes, multicomputers, Broadcasting, Hypercubes, Load management, hops]
An agent-based distributed service model for nomadic users
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Nomadic information access provides considerable and hence desirable convenience for mobile users, but it comes with new challenges introduced by wireless communication, user mobility and limited resources in portable devices. The traditional approaches to client-server architectures have to be reevaluated in mobile wireless environments. This paper presents an innovative agent-based distributed service model which focuses on providing a generalized infrastructure to enable efficient nomadic service access. The model supports asynchronous service lookup, dynamic service invocation and flexible adaptation to environments, thus improves the service availability and service quality as well as reduces network bandwidth consumption. Furthermore, mobility specific location dependent services are integrated in this model.
[Error analysis, agent-based distributed service model, asynchronous service lookup, Mobile communication, dynamic service invocation, Distributed computing, client-server architectures, flexible adaptation, Wireless communication, mobile computing, portable devices, mobile wireless environments, Mobile agents, Web and internet services, information access, Bandwidth, nomadic users, Availability, quality of service, software agents, network bandwidth consumption, user mobility, Telematics, wireless LAN, Consumer electronics]
Parallel skeletons for tabu search method
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
We present two generic parallel skeletons for the tabu search method-a well known meta-heuristic for approximately solving combinatorial optimization problems. The first skeleton is based on independent runs while the second in the classical master-slave model. Our starting point is the design and implementation of a sequential skeleton that is used later as basis for the two parallel skeletons. Both skeletons provide the user with the following: a permit to obtain parallel implementations of the tabu search method for concrete combinatorial optimization problems from existing sequential implementations; there is no need for the user to know either parallel programming or communication libraries; and the parallel implementation of tabu search for a concrete problem is obtained automatically from a sequential implementation of tabu search for the problem. The skeletons, however, require from the user a sequential instantiation of the tabu search method for the problem at hand. The skeletons are implemented in C++ using MPI as the communication library and offer genericity, flexibility, component reuse, robustness and time savings. We have instantiated the two skeletons for the 0-1 multidimensional knapsack problem, among others, for which we report computational results.
[Optimization methods, MPI, Master-slave, parallel skeletons, software libraries, multidimensional knapsack problem, meta-heuristic, optimisation, Search methods, Parallel processing, Skeleton, Libraries, Robustness, Informatics, search problems, master-slave model, parallel algorithms, message passing, tabu search method, C++, C++ language, component reuse, Parallel programming, Concrete, combinatorial optimization problems, communication library]
RSA-based partially blind signature with low computation
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
With a partially blind signature scheme, the signer inoculates non-removable common information into his blind signature. This common information may represent the date or the amount of cash. Due to its untraceability and partial blindness property, the partially blind signature plays an important role in many e-commerce applications. Based on the RSA scheme, we propose a partially blind signature with low computation which makes it attractive for mobile client and smart-card implementation.
[Smart cards, Protocols, RSA-based partially blind signature, mobile client, smart cards, Educational institutions, Mathematics, Information management, Information systems, Databases, unremovable common information, Voting, public key cryptography, message authentication, Blindness, e-commerce, smart card, low computation, distributed programming, Business]
A push-based key distribution and rekeying protocol for secure multicasting
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
We present a novel solution to the problem of scalable group key management. For the rekeying of the group key all members of the group should agree the rekeying of the key. However synchronization with all members for agreement of the rekeying is very costly. We have proposed an asynchronous rekeying framework based on the reliable and totally ordered multicast protocols (RTOMP). In our framework, a group consists of some domains which have some group members and one trusted key distribution server (KDS). The secure RTOMP channel is assumed for the communication channel among KDSs. When the membership of a domain is changed, the KDS of the domain creates the group key and distributes to the all other KDSs via the secure multicast channel on its own responsibility. Each member has an individual key between the KDS in the domain. The member can take a new key from the KDS in its own domain. Although this approach can disperse the cost of key creation and distribution to all KDSs, each KDS is responsible to key distribution to all members in the domain. Therefore, the scalability is limited by the performance of the KDS. We introduce the push-based key distribution and propose the combined approach of pull-based and push-based key distribution. From our simulation, the proposed approach can reduce the cost of the KDSs and improve the scalability.
[telecommunication security, key creation, Costs, telecommunication channels, Scalability, asynchronous rekeying framework, simulation, communication channel, Electronic mail, scalable group key management, Delay, scalability, Unicast, secure multicasting, Bandwidth, multicast communication, Computational efficiency, Cryptography, protocols, pull-based key distribution, push-based key distribution, Multicast protocols, cryptography, trusted key distribution server, synchronisation, Communication channels, synchronization, rekeying protocol, reliable totally ordered multicast protocols]
QoS streaming based on a media filtering system
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
A media filter can ensure full quality media playout at the high-performance computing and communicating environment while lowering to an appropriate media quality for the smooth playout in a limited bandwidth network. We present a new streaming architecture for the establishment of continuous media data flow over heterogeneous computer networks. Based on the new streaming architecture, we have developed a flexible and efficient audio/video transmission system. In order to eliminate heterogeneity problems found in networks, end-systems and applications, we adopt the concept of QoS media filter. The individual or receiver-driven quality of service requirement can be achieved through the application of a QoS media filtering mechanism.
[data compression, limited bandwidth network, Filtering, Laboratories, media filtering system, computer networks, Quality of service, QoS streaming, quality of service, video coding, Computer science, full quality media playout, high-performance, Filters, video compression, Computer architecture, Bandwidth, Streaming media, Video compression, Computer networks, heterogeneous computer networks, multimedia communication, continuous media data flow, audio video transmission]
Multicasts on WDM all-optical multistage interconnection networks
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Wavelength-division multiplexing (WDM) optical networks provide huge bandwidth by allowing multiple data streams to be transmitted simultaneously along the same optical fiber, with each stream assigned a distinct wavelength. A key issue of WDM optical networks is the minimization of the number of wavelengths for realizing a routing request. Let W be the number of wavelengths supported by a WDM optical network. For a routing request R which needs l wavelengths, if l/spl les/W then R can be realized in one round of routing. However, when l>W, multiple rounds of routing for R are required. In this case, it is important to minimize the number of routing rounds. Multicast transmits a data stream from one input to multiple outputs (one-to-many), a fundamental communication pattern in many applications. We study the problem of minimizing the number of wavelengths and the number of routing rounds for realizing a set R={(u, /spl nu/)} of multicasts, where each output /spl nu/ receives a data stream from exactly one input u, on an n-dimensional WDM all-optical multistage interconnection networks (MINs). For a network with wavelength converters, we show that any set of multicasts can be realized by 2/sup [(n-1)/(k+1)]/ wavelengths in k rounds of routing. For one round of routing, the upper bound 2/sup [(n-1)/2]/ is tight to the lower bound. We also give algorithms for multicasts on a network without wavelength converters. Computer simulation results show that any set of multicasts can be realized in at most two rounds of routing on a network of practical size.
[wavelength division multiplexing, Multiprocessor interconnection networks, WDM networks, Optical fiber networks, digital simulation, WDM all-optical multistage interconnection networks, wavelength-division multiplexing optical networks, Wavelength routing, wavelength converters, Bandwidth, multicast communication, optical fiber, computer simulation, Optical fibers, Optical wavelength conversion, Wavelength division multiplexing, multistage interconnection networks, routing request, multicast, Upper bound, Multicast algorithms, multiple data streams, telecommunication network routing, optical fibre networks, wavelength minimization]
Combinatorial properties of hierarchical cubic networks
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
An n-dimensional hierarchical cubic network (denoted by HCN(n)) contains 2/sup n/ n-dimensional hypercubes. The diameter of an HCN(n), which is equal to n+[(n+1)/3]+1, is about two-thirds the diameter of a comparable hypercube, although it uses about half as many links per node. In this paper, a maximal number of node-disjoint paths are constructed between every two distinct nodes of an HCN(n). Their maximal length has an upper bound of n+[n/3]+4, which is nearly optimal. The (n+1)-wide diameter and n-fault diameter of an HCN(n) are shown to be n+[n/3]+3 or n+[n/3]+4, which are about two-thirds those of a comparable hypercube. Our results reveal that an HCN(n) has shorter node-disjoint paths, wide diameter, and fault diameter than a comparable hypercube.
[Multiprocessor interconnection networks, parallel architectures, graph theory, multiprocessor interconnection, multiprocessor interconnection networks, Containers, Routing, Educational institutions, upper bound, Electronic mail, hypercubes, combinatorial properties, Delay, Computer science, Upper bound, links per node, node-disjoint paths, fault diameter, hierarchical cubic networks, Broadcasting, Hypercubes, fault tolerant computing]
Collaborative and secure resource management with distributed agents
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
As an increasing number of people communicate and collaborate computationally over the Internet, the need for a collaborative environment that facilitates secure and reliable access to large quantities of distributed resources has become manifest. However the breadth of resource distribution makes it difficult for Internet users to acquire the necessary resources for collaborative computing. We have developed a framework for an efficient infrastructure that provides a reasonable mapping between users and resources distributed over the Internet. The proposed framework contributes: an authority mechanism that can be applied to independent and widely distributed resource-providers; a resource hierarchy for partitioning global resource spaces into manageable subspaces; and systematic management of dynamic groups of users and resources running on different sites. This paper describes the schema and processes for resource partition, mapping, negotiation, and forming a resource domain in our framework.
[collaborative environment, distributed agents, Buildings, dynamic group management, secure resource management, resource distribution, Telecommunication computing, Distributed computing, software agents, Computer science, NP-hard problem, resource allocation, security of data, Collaboration, groupware, Cities and towns, authority mechanism, Computer networks, Internet, Resource management, resource partitioning]
SIMMT-II: implementation of network simulator for IP multicast using multiple MCSs on the ATM networks
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
We implemented a network simulator for performance evaluation of various IP multicast schemes over ATM networks. We also described design details of the network simulator, in which necessary software modules can be added easily. This network simulator can generate various random networks, assigns multiple multicasting servers and many group members with various distribution patterns over random networks. In addition, we can apply various routing-related algorithms to the network simulator to acquire results of performance evaluation. We show the correct behavior of the implemented simulator by evaluating the performance of several recovery schemes for an impaired multicast tree in the case where some multicast servers fail.
[Virtual colonoscopy, network servers, performance evaluations, impaired multicast tree, asynchronous transfer mode, digital simulation, group members, Electronic mail, recovery schemes, system recovery, Delay, Network servers, Mars, multicast communication, Robustness, Computer networks, multiple multicasting servers, IP multicast, SIMMT-II, distribution patterns, Computational modeling, Computer simulation, software modules, network simulator, random networks, telecommunication network routing, Asynchronous transfer mode, ATM networks]
An alternate dimension-order collective communication scheme on packet-switched 2D-mesh network
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Cluster computing, which employs many cheap node machines, will replace expensive supercomputers. However, there exist only a few enhanced communication schemes for cheap packet switches, especially in the case of collective communication. We devised a new collective communication scheme from original dimension-order routing. The proposed scheme is mainly aimed at a non-uniform traffic situation by communication locality that causes longer communication delays than those in a uniform-traffic situation. By adding a 'flow bit' in each packet, packets can traverse, alternating their directions on hop by hop. The new scheme is devised for a 2D mesh and enhanced the original X-Y routing.
[workstation clusters, nonuniform traffic situation, communication locality, Multiprocessor interconnection networks, flow bit, packet switching, multiprocessor interconnection networks, Telecommunication traffic, Routing, Supercomputers, cluster computing, 2D mesh, packet-switched 2D mesh network, Delay, alternate dimension order collective communication scheme, Concurrent computing, telecommunication network routing, Bandwidth, Traffic control, Hardware, Computer networks, communication delays, telecommunication traffic, dimension-order routing]
I-DOCS: distributed agent-assisted knowledge fusion for disease gene discovery
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
New methods of electronic collaboration are needed to manage and reconcile the vast scientific knowledge made available through the experience of diverse experts. Fundamental research in capturing, managing, analyzing, and explaining information and making it available for use is seen as a high priority for software research worldwide. Therefore, we have developed a method and software for intelligent-agent assisted knowledge fusion systems to address these needs. This system utilizes intelligent agents to facilitate the construction and fusion of knowledge from distributed human experts working on a common knowledge-sharing task. We describe our multi-agent system prototype, the Internet-Based Distributed Ophthalmology Consensus System (I-DOCS) and how we are employing it for finding disease genes. We also describe some results of using I-DOCS with clinicians attempting to develop a common clinical nomenclature and classification system for disease subtypes.
[multi-agent systems, Humans, Information analysis, intelligent agents, nomenclature, I-DOCS, Prototypes, groupware, knowledge-sharing task, computer supported cooperative work, Software prototyping, Multiagent systems, Internet-Based Distributed Ophthalmology Consensus System, multi-agent system, diseases, Knowledge management, knowledge fusion, Intelligent agent, Diseases, disease gene discovery, eye, classification system, distributed agent, Collaboration, Internet, medical computing]
An efficient multicast routing protocol for mobile hosts
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
We present an alternative design, Eff-MM (efficient mobile multicast). For efficiently supporting multicast for mobile hosts on the Internet. Providing multicast services to hosts has become popular and many multicast protocols have been proposed for mobile hosts. In the delivery tree problems occur whenever a member moves. Such problems are non-optimal delivery route and overheads caused by the frequent reconstruction of a multicast routing tree. To solve problems like this, the paper used the tunneling range and search method to routing paths around migration places with a simple message. The result provides the shortest routes for delivery of multicast datagrams to mobile hosts and reduces frequent reconstruction of the multicast tree reconfiguration.
[multicast routing tree reconstruction, overheads, Subscriptions, multicast datagram delivery, mobile computing, Unicast, efficient multicast routing protocol, mobile hosts, multicast communication, tunneling range and search method, Tunneling, Routing protocols, protocols, nonoptimal delivery route, multicast services, Multicast protocols, message, Computer science, delivery tree, mobile communication, telecommunication network routing, Bidirectional control, migration, Frequency, efficient mobile multicast, Internet, Mobile computing]
Recursive construction of hierarchical Fibonacci cubes and hierarchical extended Fibonacci cubes
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Hierarchical Fibonacci cubes and hierarchical extended Fibonacci cubes are recursively constructed. This property comes from the point of these derived networks. These nets are basically derived from Fibonacci series, which are recursive series.
[Costs, Network topology, Multiprocessor interconnection networks, Fibonacci series, series (mathematics), Hypercubes, hypercube networks, hierarchical extended Fibonacci cubes, recursive series, hierarchical Fibonacci cubes, recursive construction]
A migration strategy of mobile agent
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
We present an efficient migration strategy of a mobile agent for data mining applications. The purpose of the proposed algorithm is to set up the best migration plan of the mobile agent with regard to minimizing network execution time. In order to verify the effectiveness of the proposed algorithm, we designed a performance evaluation model for three distributed paradigms from data mining, i.e. RPC (remote procedure call), mobile agent and locker pattern, and we then evaluated the algorithm by simulation.
[Algorithm design and analysis, Information resources, network execution time, Computational modeling, mobile agent migration strategy, data mining, simulation, Telecommunication traffic, Mobile communication, Data mining, software agents, Computer science, performance evaluation model, locker pattern, Mobile agents, Traffic control, remote procedure calls, remote procedure call, Global communication, distributed programming, software performance evaluation]
A scheduling mechanism for lock-free operation of a lightweight process library for SMP computers
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
We have designed and implemented a lightweight process (thread) library called "Lesser Bear" for SMP computers. Lesser Bear has high portability and thread-level parallelism. Lesser Bear executes threads in parallel by creating UNIX processes as virtual processors and a memory mapped file as a huge shared-memory space. To schedule threads in parallel, the shared-memory space has been divided into working spaces for each virtual processor and a ready queue has been distributed. But the preview version of Lesser Bear sometimes requires a lock operation for dequeueing. We therefore proposed a scheduling mechanism that does not require a lock operation. To achieve this, each divided space forms a rotatory topology through the queue, and we use a lock-free algorithm for the queue operation. This mechanism is applied to Lesser Bear and evaluated by experimental results.
[Unix, ready queue, UNIX processes, rotatory topology, Lesser Bear, thread-level parallelism, Yarn, processor scheduling, parallel programming, software libraries, Multiprocessing systems, portability, Operating systems, network operating systems, virtual processors, shared memory systems, Libraries, Protection, queueing theory, shared-memory space, lightweight process library, Topology, working spaces, Application software, Computer science, scheduling mechanism, memory mapped file, Processor scheduling, dequeueing, lock-free operation, SMP computers, Frequency, parallel thread execution]
Algebraic expression for extra degree added to hypercube interconnection network in case of multiple edge-faults
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Researchers have used more than one method for implementing algorithms on faulty hypercube interconnection networks. Some modified the structures of the hypercube whilst others didn't, implementing their operations on a faulty hypercube. Modifying the structure of a hypercube entails adding extra degrees to each node, called reconfiguration of hypercubes. I improved an algebraic expression for added extra degrees to a hypercube to make it non-faulty.
[Computer aided software engineering, Multiprocessor interconnection networks, multiple edge faults, extra degree addition, Routing, hypercube networks, reconfiguration, matrix algebra, Intelligent networks, Fault tolerance, Upper bound, Algebra, reconfigurable architectures, algebraic expression, Broadcasting, Hypercubes, faulty hypercube interconnection network, Computer networks, fault tolerant computing]
A reliable processor-allocation strategy for mesh-connected parallel systems
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Efficient utilization of processing resources in a large, multi-user parallel computer system depends on the reliable processor allocation algorithms. The paper presents and LSSA (L-shaped submesh allocation) strategy to reduce external fragmentation and job response time, simultaneously. LSSA manipulates the shape of the required submesh to fit into the fragmented mesh system and accommodates incoming jobs faster than other strategies. LSSA can be applied to mesh-connected parallel systems with faulty processors. The basic idea is to reconfigure a faulty mesh system into a maximum convex system using the fault-free upper or lower boundary nodes to compensate for the non-boundary faulty nodes. To utilize the non-rectangular shaped system parts, LSSA tries to allocate L-shaped submeshes instead of signaling the allocation failure. Extensive simulations show that LSSA performs more efficiently than other strategies in terms of the external fragmentation, the job response time and the system utilization.
[Shape, maximum convex system, parallel architectures, multiprocessor interconnection networks, Very large scale integration, reconfiguration, fault-free lower boundary nodes, Delay, mesh-connected parallel systems, resource allocation, System performance, L-shaped submesh allocation strategy, Robustness, external fragmentation, efficient processing resource utilization, faulty processors, faulty mesh system, Topology, Maintenance, fault-free upper boundary nodes, job response time, incoming jobs, simulations, Computer science, virtual machines, large multi-user parallel computer system, reliable processor allocation algorithms, LSSA strategy, fault tolerant computing, nonboundary faulty nodes, Resource management, allocation failure]
QoS-based method for compensating multimedia objects
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Distributed applications are manipulating multiple multimedia objects. QoS of an object is manipulated in addition to the state of the object. After objects are manipulated, they may need to be rolled back in order to undo the manipulation. It is not easy to take a checkpoint of the object because the state is larger and the data structure is more complex than traditional data. We discuss an approach in which the methods performed are logged instead of the states and compensating methods of the methods in the log are performed to undo the methods. In addition, it is sufficient for applications to restore a state which supports enough QoS even if the state is different from the previous one.
[checkpointing, multimedia object compensation, QoS-based method, Multimedia systems, Quality of service, data structure, Data structures, multiple multimedia object manipulation, Electronic mail, quality of service, Application software, multimedia computing, Distributed computing, system recovery, distributed applications, Systems engineering and theory, Artificial intelligence, distributed object management]
Optimal data reduction on reconfigurable tori
Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001
None
2001
Data reduction is a fundamental operation of parallel computing. We derive lower bounds on communication latency for global data reduction and multiple global data reduction on reconfigurable tori. We present optimal global data reduction algorithms and multiple global data reduction algorithms on reconfigurable tori of any dimension. The formal reduction algorithms we give can make reduction and broadcast operations easy to implement.
[parallel architectures, multiprocessor interconnection networks, optimal data reduction, communication latency, broadcast operations, reconfigurable tori, Delay, lower bounds, Information systems, Computer science, Concurrent computing, global data reduction, data reduction, reconfigurable architectures, Broadcasting, multiple global data reduction, formal reduction algorithms, parallel computing, Assembly]
The impact of RTS threshold on IEEE 802.11 MAC protocol
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Wireless technologies and applications received great attention in recent years. The medium access control (MAC) protocol is the main element that determines the efficiency in sharing the limited communication bandwidth of the wireless channel in wireless local area networks (WLANs). The request-to-send/clear-to-send (RTSICTS) mechanism is an optional handshaking procedure used by the IEEE 802.11 wireless network to reduce the possibility of collision. The RTS-Threshold (RT) value which determines when the RTS/CTS handshaking mechanism should be used is an important parameter to investigate; since different RT values will produce different performance characteristics in data transmission. This paper presents an analysis of the influence of the RT parameter on the IEEE 802.11 wireless network, and gives a guideline to dynamically adjust the RT value. Simulation results of this paper show that the RTSICTS mechanism should be always turned on (RT = 0) to achieve an excellent performance while saving complex work designing a dynamic RT mechanism which will not have notable effect.
[Wireless LAN, IEEE 802.11 wireless network, Wireless application protocol, medium access control protocol, RTS threshold value, Access protocols, wireless local area networks, Throughput, IEEE 802.11 MAC protocol, access protocols, wireless applications, handshaking procedure, Application software, Computer science, Wireless networks, request-to-send/clear-to-send mechanism, Bandwidth, data transmission, communication bandwidth sharing, Media Access Protocol, wireless channel, performance characteristics, Data communication, wireless LAN]
Self-stabilizing wormhole routing on ring networks
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Wormhole routing is the most common in parallel architecture in which messages are sent in small fragments called flits. It is a lightweight and efficient method of routing messages between parallel processors. Self-stabilization is a technique that guarantees tolerance to transient faults (e.g. memory corruption or communication hazard) for a given protocol. Self-stabilization guarantees that the network recovers to a correct behavior infinite time, without the need for human intervention. Self-stabilization also guarantees the safety property, meaning that once the network is in a legitimate state, it will remain there until another fault occurs. This paper presents the first self-stabilizing network algorithm in the wormhole routing model, using the unidirectional ring topology. Our solution benefits from wormhole routing by providing high throughput and low latency, and front self-stabilization by ensuring automatic resilience to all possible transient failures.
[parallel algorithms, Protocols, message passing, fault tolerance, network routing, self-stabilization, fault-tolerance, parallel architectures, Humans, multiprocessor interconnection networks, Routing, Throughput, Hazards, Parallel architectures, network topology, Delay, Resilience, wormhole routing, Network topology, distributed algorithms, data structures, Safety, unidirectional ring topology]
A data broadcast scheme based on prediction for the wireless environment
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Broadcasting is an essential paradigm to transport data and support scalability in the wireless environment. Traditional index-based broadcast schemes take advantage of tree structures for saving energy but they record too much index information to make the broadcast cycle short. On the other hand, the hash scheme may suffer from longer broadcast cycles and data collision problems, resulting in longer access and tuning time. In this paper we propose a novel broadcast scheme, named predictive indexing, which allows mobile clients to access data by predicting data locations in the broadcast cycle. The new scheme not only has very compact structure, but also is flexible enough to allow the server to adjust the index structure for saving energy or reducing access time. Experimental studies show that it outperforms the tree-based schemes 2.5 times in energy saving, while maintaining roughly the same access time. It also outperforms the flexible indexing 70% and the hash scheme 80% in energy consumption.
[Tree data structures, broadcast, Energy consumption, client-server systems, wireless broadcast, Scalability, mobile client, Energy measurement, Battery charge measurement, Time measurement, Computer science, mobile computing, wireless environment, Broadcasting, Frequency, wireless LAN, index-based broadcast, Indexing]
An efficient optimization technique for task matching and scheduling in heterogeneous computing systems
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
A new optimization technique, the genetic annealing algorithm (GAA), is proposed to solve the task matching and scheduling problem in a heterogeneous computing system. The GAA is simple in design; it employs only the stir operation, a novel idea with the annealing concept, to locate optimal solutions. Experimental evaluation shows that compared with the genetic algorithm, simulated annealing and guided evolutionary simulated annealing approaches, the GAA yields constantly favorable performance in terms of speedup, running time, cost and complexity.
[simulated annealing, task matching, heterogeneous computing system, distributed processing, genetic algorithms, Distributed computing, processor scheduling, Concurrent computing, genetic algorithm, Processor scheduling, guided evolutionary simulated annealing, optimization, task scheduling, genetic annealing algorithm]
Scheduling video stream transmissions for distributed playback over mobile cellular networks
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
In this paper, we present the Buffer Sensitive Rate-Based (BSRB) algorithm, which provides a fair scheduling scheme to serve video playback requests, while maximizing the performance of individual playback. In BSRB, the amount of bandwidth allocated to serve a video request depends on the buffer level of the requesting client, and the expected and minimum bandwidth requirements of the video. By maintaining a high video buffer level at a client, the quality of the playbacks can be maintained and its performance will be more adaptive to the changing playback conditions. By employing the rate-based policy in BSRB, the performance of the requests is less affected by the poor performance of a single client and hence fair services can be provided to the clients.
[fair scheduling, playback status, distributed mobile video player, video server, mobile client, mobile cellular networks, video servers, Scheduling algorithm, Computer science, mobile computing, Buffer Sensitive RateBased algorithm, Processor scheduling, Land mobile radio cellular systems, Bandwidth, Streaming media, Communication system traffic control, Resource management, Personal digital assistants, Mobile computing, video playback]
Adaptive live broadcasting for highly-demanded videos
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
With the growth of broadband networks, the video-on-demand (VoD) becomes realistic. Many significant broadcasting schemes are proposed to reduce the bandwidth requirements for stored popular videos, but they cannot be used to support live video broadcast perfectly. Herein, we propose a new broadcasting scheme, called adaptive live broadcasting (ALB) scheme, which supports live video broadcasting and performs well over a wide range of request arrival rates. From our analysis and comparison, we find that our ALB scheme is suitable to broadcast live video. It has several significant advantages: (1) it has the shortest maximum waiting time with fixed channels; (2) it has the least maximum I/O transfer requirements with fixed maximum waiting time at client end. Finally, a simulation is employed to evaluate several live broadcasting schemes, such as UD, ST AFB and ALB. The results reveal our ALB scheme consumes the least server bandwidth.
[maximum input output transfer, waiting time, Delay effects, broadband networks, Broadband communication, Multimedia communication, adaptive systems, Videos, broadcasting, Computer science, Network servers, video on demand, Bandwidth, Broadcasting, US Department of Transportation, Streaming media, video-on-demand, popular video service, multimedia communication, adaptive live broadcasting, network bandwidth scheduling]
Distributed real-time system design using CBS-based end-to-end scheduling
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Distributed real-time applications share a group of processors connected by some local area network. A rigorous and sound methodology to design real-time systems from independently designed distributed real-time applications is needed. In this paper, we study a distributed real-time system design scheme using CBS-based end-to-end scheduling. The scheduling scheme utilizes CBS to allocate both CPU shares and network bandwidth to a distributed real-time application when it arrives at the system. Our proposed solution uses the same scheduling paradigm for both resources. In this way, we believe the system can have a more consistent scheduling objective and may achieve a tighter schedulability condition.
[Real time systems, Control systems, local area networks, Application software, Distributed computing, end-to-end scheduling, processor scheduling, distributed real-time system, Processor scheduling, CBS, real-time systems, local area network, Bandwidth, Computer architecture, distributed systems, constant bandwidth server, Computer networks, real-time system, Local area networks, Monitoring, controller area networks]
Distributed heterogeneous inspecting system and its middleware-based solution
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
There are many cases where an organization needs to monitor the data and operations of its supervised departments, especially un-owned departments, which are managed by its own information systems. A distributed heterogeneous inspecting system (DHIS) is a system the organization uses to monitor its supervised departments by inspecting their information systems. In DHIS, the inspected systems are generally distributed, heterogeneous, and constructed by different companies. DHIS has three key processes-to abstract core data sets and core operation sets, to collect these sets, and to inspect these collected sets. We present the concept of DHIS, mathematical definition of DHIS, a metadata method to solve the interoperability, security strategy for data transfer, and a middleware-based solution of DHIS. We also describe an example of the inspecting system at Wenzhou custom.
[meta data, metadata, data analysis, distributed heterogeneous inspecting system, Computerized monitoring, Geology, Data security, data security, Companies, Data engineering, Enterprise resource planning, interoperability, Computer science, companies, Databases, security of data, Engineering management, Management information systems, organization, data sets, information systems, business data processing, middleware]
Performance evaluation of distributed computing paradigms in mobile ad hoc sensor networks
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
The emergence of mobile ad hoc sensor networks has brought new challenges to traditional network design. This paper compares the performance of two distributed computing paradigms, the client/server-based paradigm and the mobile-agent-based paradigm, through mathematical modeling and simulation. Previous works have shown that the mobile-agent-based paradigm is more appropriate to handle computations in ad hoc sensor networks. However, no simulation work has been done to quantitatively measure the performance. This paper first describes how computing is accomplished in the mobile-agent-based paradigm. It then presents a modified mathematical model and uses the execution time as a metric to measure the performance. Eight experiments are designed to show the effect of different parameters to the performance of the paradigms. Experimental results show that in the context of mobile ad hoc sensor networks with hundreds or even thousands of nodes, unreliable communication links and reduced bandwidth, the mobile-agent-based computing provides solutions to low network latency and reliable data processing.
[Context, client-server systems, mobile-agent-based paradigm, Computational modeling, client server system, mobile ad hoc sensor networks, Mobile communication, Time measurement, Distributed computing, Delay, distributed computing, mobile computing, mobile agents, Bandwidth, agent migration, mathematical model, Computer networks, Mathematical model, ad hoc networks, Mobile computing]
Achieving high efficient Byzantine agreement with dual components failure mode on a multicasting network
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Under many circumstances, reaching a common agreement in the presence of faulty components is the central issue of fault tolerant distributed computing. So the Byzantine agreement (BA) problem has become more and more important in distributed systems. Traditionally, the BA problem was visited in a fully connected network (FCN), broadcasting network (BCN) and generalized connected network (GCN). Subsequently, malicious fault assumption with processors or communication media was extended to a hybrid fault model on both processors and communication media. However, the network structures (topologies) of FCN, BCN and GCN are not practical. In this study, we lighten restrictions of the network structure to revisit the BA problem with multicasting network (MCN). The proposed protocol uses the minimum number of rounds of message exchange and can tolerate the maximum number of allowable faulty components to make each fault-free processor reach a common agreement in an MCN.
[Protocols, dual component failure mode, message exchange, Byzantine agreement, hybrid fault model, communication media, Information management, processors, Distributed computing, Business communication, multicasting network, Fault tolerance, Network topology, Chaotic communication, Fault tolerant systems, multicast protocols, malicious fault assumption, Broadcasting, fault tolerant computing, fault tolerant distributed computing, Clocks]
Cooperative proxy scheme for large-scale VoD systems
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Video-on-Demand (VoD) systems are attractive multimedia services over the Internet. However, conventional VoD systems can not serve many viewers simultaneously because such large-scale services require considerable server load and network bandwidth. This study proposes a cooperative proxy scheme, consisting of a streaming cache and a query protocol revised from Internet Cache Protocol (ICP), to improve performance of large-scale VoD systems. The streaming cache we proposed attempts to serve the most requests with slightly computation overhead In addition, our query protocol further extends service scalability by satisfying requests "as far as possible\
[Protocols, multimedia servers, Internet Cache Protocol, Multimedia systems, Scalability, Cache memory, cache storage, cooperative proxy scheme, Network servers, Video-on-Demand, multimedia services, query protocol, Web and internet services, video on demand, Bandwidth, Streaming media, streaming cache, Large-scale systems, Internet, protocols, Web server, multimedia communication, VoD systems]
A mobile cache consistency protocol using shareable read/write time locks
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Object caching is often used to improve the performance of mobile applications, but the gain is often lessened by the additional load of maintaining consistency between an original object and its cached copy. This paper aims at reducing the consistency maintenance work and proposes a protocol that distinguishes between two classes of consistency (i.e. weak and strong) and treats them differently. Strong consistency is used for data that needs to be consistent all the time, whereas weak consistency is for cases when stale data can be tolerated or only specific updates are relevant to the application. Consistency is maintained by using strict and permissive read/write time locks that enable data sharing for a fixed time period and support concurrency control. A notification protocol for propagating updates to clients is also proposed. Performance tests have shown that switching from strong to weak consistency reduces the number of aborts due to conflicting operations by almost half even with high read/write sharing.
[Protocols, object caching, weak consistency, Performance gain, cache storage, notification protocol, Distributed computing, mobile computing, Bandwidth, mobile applications, mobile cache consistency protocol, memory protocols, strong consistency, Concurrency control, data integrity, Application software, Information technology, Computer science, shareable read write time locks, performance, concurrency control, Australia, Mobile computing, data sharing, performance tests]
Hardware-based IP routing lookup with incremental update
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Nowadays, the commonly used table lookup scheme for IP routing is based on the so-called classless interdomain routing (CIDR). With CIDR, routers must find out the best matching prefix (BMP) for IP packet forwarding, which complicates the IP lookup. Since the IP lookup performance is a major design issue for the new generation routers, we investigate the properties of the routing table and present a new IP lookup scheme. By adopting the extreme compression, the size of the forwarding table can be compressed to 360 Kbytes for a large routing table with 58,000 routing entries. It can accomplish one IPv4 route lookup within maximum nine memory accesses. Consequently, the data structure for the incremental update is introduced. With the new data structure, the proposed scheme can accomplish one route update within 500 ns. Even under the circumstance with 4000 route updates per second, the performance degradation is as low as 0.2 %.
[Spine, Laboratories, data structure, World Wide Web, Table lookup, best matching prefix, memory access, Degradation, table lookup, Technology management, hardware-based IP routing lookup, data structures, incremental update, IP packet forwarding, classless interdomain routing, data compression, IPv4 route lookup, performance evaluation, Routing, Data structures, Telecommunications, performance degradation, IP lookup performance, transport protocols, telecommunication network routing, Internet]
A general resource management framework for real-time operating systems
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
In RED-Linux kernel, a General Scheduling Framework (GSF) has been implemented. Under GSF, different scheduling algorithms can be easily implemented. However GSF only addresses the scheduling of CPU. Other system resources are not considered. In this paper we propose a general resource management framework for various OS resources. When user applications request and consume system resources, they may use uniform APIs although each type of OS resource has its own properties and means to be controlled. In the resource management framework, we also allow the inter-relationship between different types of resources to be defined and managed.
[Real time systems, Unix, resource management, real-time operating systems, Project management, kernel, Control systems, Videoconference, processor scheduling, scheduling algorithms, Processor scheduling, resource allocation, Operating systems, RED-Linux, Bandwidth, Streaming media, operating systems (computers), general scheduling framework, Resource management, Kernel]
Reach reliable decision by using secret agreement method
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Traditionally, it is very important to deliver data secretly and at low cost in a distributed environment. Using a reliable method to assist in communicating with each other needs to be discussed. Thus, we propose the two-phase agreement (TPA) protocol to solve those problems by verifying the received messages and achieve the agreement. The TPA protocol will assist the organization in achieving agreement and generate a common key to verify it. Similarly, there are many faults and attacks in the network, such as interruption, interception, modification and fabrication. The messages from a source might be changed. However we can use the TPA protocol to remove these faulty influences. The proposed TPA protocol doesn't only decrease the complexity of messages but also increases the capability of fault tolerance. Similarly, it is more reliable via the secret key in a real distributed system. This protocol can solve the agreement problem in net-meeting between arbitrary processors, like sub-company communication with the main-company or makes a decision in the company conference.
[Protocols, Costs, reliable decision making, distributed environment, Information management, Communication system security, faults, Privacy, Voting, attacks, protocols, message passing, fault tolerance, Collaborative software, Data security, secret agreement method, fabrication, group decision support systems, modification, two-phase agreement protocol, company conference, Chaotic communication, security of data, decision making, interception, Collaborative work, messages, interruption, sub-company communication]
A strategy for semantic integrity checking in distributed databases
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Integrity constraints represent knowledge about data with which a database must be consistent. The process of checking constraints to ensure that the update operations or transactions which alter the database will preserve its consistency has proved to be extremely difficult to implement efficiently, particularly in a distributed environment. In the literature, most of the approaches/methods proposed for finding/deriving a good set of integrity constraints concentrate on deriving simplified forms of the constraints by analyzing both the syntax of the constraints and their appropriate update operations. These methods are based on syntactic criteria and are limited to simple types of integrity constraints. Also, these methods are only able to produce one integrity test for each integrity constraint. In Ibrahim, Gray, and Fiddian (1997), we introduced an integrity constraint subsystem for a relational distributed database. The subsystem consists of several techniques necessary for efficient constraint checking, particularly in a distributed environment where data distribution is transparent to application domain. However, the technique proposed for generating integrity tests is limited to several types of integrity constraints, namely: domain, key, referential and simple general semantic constraint and only produced two integrity tests (global and local) for a given integrity constraint. In this paper, we present a technique for deriving several integrity tests for a given integrity constraint where the following types of integrity constraints are considered: static and transition constraints.
[Algorithm design and analysis, semantic integrity checking, Costs, Relational databases, checking constraints, relational distributed database, data integrity, Transaction databases, Information technology, Distributed computing, Computer science, Distributed databases, integrity constraint, distributed databases, Robustness, Testing, integrity tests]
An energy-efficient diagonal-based directed diffusion for wireless sensor networks
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
We present a new energy-efficient directed diffusion protocol by using the proposed diagonal-based hexagonal-mesh scheme for a wireless sensor network. The wireless sensor network is more reasonable to build a fixed-topological wireless network environment than the conventional MANET due to the low mobility. Therefore, all sensor nodes are arranged into a fixed-topological wireless network structure, namely the hexagonal-mesh, while the MAC protocol is adopted using the periodic active-and-sleep model. The wireless sensor networks use battery-operated computing and sensing devices. The directed diffusion is mainly operated on the diagonal-paths of the hexagonal-mesh under the energy-efficient consideration. To achieve the energy-efficient purpose, our diagonal-based directed diffusion scheme has the following main contributions: (1) a periodic active-and-sleep MAC protocol on TDMA channel model is designed; (2) a periodic backbone-path-exchange scheme is periodically performed on the diagonal-mesh to consider the per-node fairness problem; and (3) a directed diffusion communication application is developed based on the diagonal-based scheme. Finally, the performance analysis result is demonstrated to illustrate the energy-efficient achievement of our proposed scheme.
[directed diffusion protocol, Military computing, Wireless application protocol, wireless sensor network, periodic backbone-path exchange scheme, network topology, Computer science, Wireless sensor networks, sensor nodes, Power engineering computing, time division multiple access, TDMA channel model, MAC protocol, Media Access Protocol, Energy efficiency, Computer networks, data acquisition, diagonal-based hexagonal-mesh, wireless LAN, protocols, Remote monitoring, Power engineering and energy]
A parallel divide-and-conquer scheme for Delaunay triangulation
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
This paper describes a parallel divide-and-conquer-algorithm for Delaunay triangulation. This algorithm finds the affected zone that cover the triangulations that may be modified during the merge of two sub-block triangulations. With the aid of the affected zone, communications between processors are reduced, the time complexity of divide-and-conquer remains O(n log n), and the affected zone can be found in O(n) time steps, where n is the number of points. The code was implemented with C, FORTRAN and MPI, so it was easy to port this program to other machines. Experimental results on IBM SP2 show that a parallel efficiency of 34%-96% for general distributions can be achieved on an 16-node distributed memory system.
[Algorithm design and analysis, divide and conquer methods, C, parallel efficiency, MPI, Magnetic analysis, Finite element methods, Parallel algorithms, Concurrent computing, communications, sub-block triangulations, Delauney triangulation, Computer graphics, Management information systems, parallel divide-and-conquer algorithm, Mesh generation, parallel algorithms, Computational fluid dynamics, time complexity, mesh generation, distributed memory system, Solids, FORTRAN, IBM SP2, computational complexity]
A speed-adaptive location management scheme
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Location management is one of the fundamental issues in cellular networks. It includes two operations: location update and paging. The goal of location management is to minimize the combined cost of these two operations. Recently a distance-based scheme using a predefined look-up table for location updates has been proposed. The table describes the relationship between the distance and the time: the distance decreases while the time increases. In this scheme, the paging area for a mobile station will be automatically reduced if the mobile station does not update its location over a certain time period. Therefore this scheme performs quite well when a mobile station travels at a low speed. However, it does not perform well when the speed of a mobile station is high or when the incoming call arrival rate is high. To overcome these drawbacks, a speed-adaptive location management scheme is proposed in this paper. The proposed scheme uses an enhanced look-up table. The table consists of two parts: the distance in the first part increases while the time increases; in the second part, the distance decreases with the increasing time. To reduce the paging cost of high-speed mobile stations further, the concepts of speed ranges and paging polar angles are introduced in the scheme. Numerical simulations using activity-based, random walk and fluid flow mobility models have shown that the proposed scheme performs well for mobile stations traveling at a high speed as well as for those traveling at a low speed.
[Base stations, numerical simulations, Costs, activity-based mobility models, Shape, Fluid flow, fluid flow mobility models, speed-adaptive location management scheme, paging, Table lookup, speed ranges, predefined look-up table, mobile station, random walk mobility models, Computer science, Land mobile radio cellular systems, paging polar angles, Telephony, Numerical simulation, Computer network management, cellular networks, distance-based scheme, cellular radio, location update]
The MPEG-4 streaming player using adaptive Decoding Time Stamp synchronization
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
This paper introduces the MPEG-4 streaming player offering the streaming service of the MPEG-4 visual standard by applying the adaptive Decoding Time Stamp (DTS) synchronization. The player uses the general development interfaces; it is also interoperable in different systems because it streams the international standard, MPEG-4 visual. Likewise, it minimizes the dependence of the systems resources as it utilizes the adaptive synchronization mechanism. We propose the new synchronization method that it guarantees the Quality of Service (QoS) of the user interface by synchronizing the control of the decoding level without using the Presentation Time Stamp (PTS). This method obtains the best efficiency in the client system even with the resource restriction like a mobile device or a home appliance.
[ISO standards, Quality of service, visual standard, Decoding Time Stamp, Decoding, user interfaces, audiovisual media, MPEG 4 Standard, Home appliances, MPEG-4 streaming player, IEC standards, Streaming media, User interfaces, synchronization, Standards development, client system, Personal digital assistants, multimedia communication]
A fast branch-and-bound algorithm with an improved lower bound for solving the multiprocessor scheduling problem
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
This paper proposes a fast branch-and-bound algorithm for solving the multiprocessor scheduling problem. The key point of our method is the proposal of an efficient quadratic algorithm for calculating the Fernandez and Bussell's (1973) lower bound. In the following, we discuss about the trade-off between the cost for calculating a lower bound and the size of pruned subtrees. Several experiments are conducted to evaluate the goodness of the proposed method.
[Costs, Partitioning algorithms, lower bound, Proposals, tree searching, multiprocessor scheduling problem, Scheduling algorithm, processor scheduling, Program processors, Processor scheduling, NP-hard problem, High performance computing, fast branch-and-bound algorithm, Signal processing algorithms, Hafnium, pruned subtrees]
A new scheme for improving performance of TCP over network including wireless links
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
With the proliferation of mobile and wireless computing devices, the demand for continuous network connectivity exists for various wired-and-wireless-integrated networks. Since TCP is the standard network protocol stack for communication on the Internet, its use over the networks is a certainty because it allows seamless integration with the fixed infrastructure. Transmission Control Protocol (TCP) assumes a relatively reliable underlying where most packet losses are due to congestion. In a wireless network, however, packet losses will occur more often due to unreliable wireless links than due to congestion. When using TCP over wireless links, each packet loss on the wireless link results in congestion control measures being invoked at the source. This causes severe performance degradation. We propose a new TCP scheme for the various wireless-integrated network topology. The scheme distinguishes wireless losses from packet losses regardless of the location of wireless links on the networks. This solves the severe performance degradation because of the miss notification of transmission loss. Experiments are performed using the Network Simulator (NS-II) from Lawrence Berkeley Labs. The simulator has been extended to incorporate wireless link characteristics.
[TCP, Protocols, wireless-integrated network topology, Communication system control, experiments, Mobile communication, wireless links, Network Simulator, Communication standards, Degradation, network protocol stack, mobile computing, Propagation losses, Computer networks, packet losses, IP networks, performance evaluation, Transmission Control Protocol, congestion control measures, performance, transport protocols, continuous network connectivity, Internet, Telecommunication network reliability, wireless LAN, Mobile computing]
Concurrency control in XML document databases: XPath locking protocol
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
XML has become the most important technique to exchange data in WWW. Providing efficient access to XML document databases is thus crucial. Concurrency control protocols allow transactions to be executed concurrently to improve performance. However no proposed research aims at the concurrency control protocol for XML databases. In this paper we propose a lock-based concurrency control protocol, called XLP for XML documents. XLP takes into consideration the access behavior of the XPath model, which is an important language for addressing data in XML documents suggested by W3C XLP has the features of richer lock modes, refined lock granularity, lower lock conflict and lock conversion. We prove that XLP always generates serializable schedules. Our simulation results show that, for XML documents of different sizes, XLP outperforms 2PL by 84.4% on average and outperforms the tree locking protocol by 95.6% on average. The performance comparison of XLP with both 2PL and the tree locking protocol is also made in this study for various transactions with different XPath lengths, read/write ratios and the existence of predicates.
[data exchange, read/write ratios, World Wide Web, lock conflict, Tree graphs, Laser mode locking, protocols, XLP, hypermedia markup languages, XPath locking protocol, XPath lengths, Access protocols, refined lock granularity, Concurrency control, Spatial databases, Transaction databases, transactions, lock conversion, Computer science, electronic data interchange, XML, concurrency control, WWW, serializable schedules, lock-based concurrency control protocol, Internet, lock modes, XML document databases]
Mobile agents for distributed logic programming
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
This paper describes the use of mobile agent technologies in building a framework for supporting distributed logic programming. The distinctive idea is to replace the distributed unification mechanism in most distributed logic programming languages with the mobility and execution locality of mobile agents. Mobile agents, migrating among logic server hosts, accomplish distributed deductions by asserting program clauses and queries into the server triggering inferences, and retrieving results. The mobile agent framework is designed to integrate a mobile agent system and necessary logic servers. One of the distinguishing features of this framework is that each logic server retains its own autonomy. Another notable characteristic is the clauses exchange ability among distributed logic servers that may make many operations required by distributed knowledge processing easier. In a prototypical implementation, a Prolog system on a host will serve as a logic server and, in the mean time, as a standalone logic programming system in the host.
[Logic programming, distributed logic programming, distributed knowledge processing, Computational modeling, inference, Telecommunication traffic, Prolog, program clauses, distributed unification mechanism, Application software, Distributed computing, queries, Network servers, execution locality, Mobile agents, Prototypes, mobile agents, Computer applications, Traffic control, logic programming, distributed logic programming languages, PROLOG, distributed programming]
Power-aware resource allocation for independent tasks in heterogeneous real-time systems
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
In recent years, power management and power reduction has become a critical issue in portable systems that are designed for real-time use. In this paper, we study the problem of static allocation of a set of independent tasks onto a real-time system consisting of heterogeneous processing elements, each enabled with discrete Dynamic Voltage Scaling. The allocation problem is first formulated as an extended Generalized Assignment Problem. A linearization heuristic (LR-heuristic) is then extended for solving the problem. An analysis of the upper bound on the number of tasks that the heuristic may fail to allocate is also presented. Our experiments show that when the real-time constraints are tight, the LR-heuristic achieves 15% off the optimal energy consumption for small size problems, while the performance of a classic greedy heuristic is around 90% off the optimal. A relative performance improvement of up-to 40% over the classic greedy heuristic is also observed for large size problems.
[Real time systems, linearization heuristic, power reduction, processor scheduling, greedy heuristic, resource allocation, heuristic programming, real-time systems, Generalized Assignment Problem, power management, portable systems, Resource management, real-time system]
A parallel implementation of GESPP on a cluster of Silicon Graphics workstations
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
In this paper, we develop and evaluate a parallel algorithm for GESPP on a cluster of Silicon Graphics workstations using PVM as a parallel programming environment. The proposed algorithm reduces the computation time to O(n/sup 3/ / p) and the communication time to O(n) including scaling, searching for the pivot element, row interchanging, and pivot row and multipliers column broadcasting. The numerical experiments show that the proposed parallel algorithm offers a high speedup and efficiency for the test problems.
[workstation clusters, parallel algorithm, PVM, Silicon Graphics workstation cluster, Parallel algorithms, parallel programming, Concurrent computing, communication time, computation time, Clustering algorithms, Computer graphics, Silicon, Workstations, numerical experiments, row interchanging, parallel algorithms, scaling, pivot element, pivot row broadcasting, Application software, Matrix decomposition, Equations, matrix algebra, Load management, parallel programming environment, GESPP, computational complexity, multiplier column broadcasting]
Adaptive matrix multiplication in heterogeneous environments
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
In this paper an adaptive matrix multiplication algorithm for dynamic heterogeneous environments is developed and evaluated. Unlike the state-of-the-art approaches, where load balancing is achieved through unequal distribution of the matrix data among the heterogeneous nodes, the matrices in our approach are partitioned into blocks of equal size. Task allocation and the block size are adapted during run time. Data pre-fetch is used to perform efficient communication. Our approach enables the use of various task scheduling heuristics. Further we show that the control and coordination overheads of this approach are negligible when compared with the overall execution time. The effectiveness of the approach is verified through a configurable simulator developed for understanding the performance of heterogeneous computing environments.
[Costs, block size, Military computing, load balancing, Heuristic algorithms, Communication system control, heterogeneous environments, distributed processing, Linear matrix inequalities, Partitioning algorithms, Distributed computing, distributed heterogeneous computing systems, matrix multiplication, heterogeneous computing, task allocation, resource allocation, High performance computing, distributed resources, Load management, Libraries]
Organization of shared memory with synchronization for multiprocessor-on-a-chip
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
The TSVM is a logical structured memory with a synchronization to improve a performance in a multi-threaded parallel processing. The physical TSVM is realized by the TSVM cache (TC) and a conventional memory in a Multiprocessor-on-a-chip (MOC) system. The L1 cache in a CPU consists of the TC, the General variable cache (GVC) and the instruction cache. The IYA (IY architecture) that is a new architecture divides a conventional data cache into the TC and GVC. The TC caches the shared variables with a synchronization, and the GVC caches other general variables. Regardless of a CPU core, a MOC with the IYA can utilize parallelisms from the instruction level and the statement level to the thread level systematically. To estimate the effect of the TC, preliminary experiments are performed on the multi-chip multiprocessor including the stand-alone TSVM. The result shows that the TSVM cache improves the performance.
[CADCAM, Computer aided manufacturing, Random access memory, logical structured memory, Scheduling, cache storage, multi-threaded parallel processing, shared memory multiprocessor, Yarn, Counting circuits, synchronisation, Associative memory, TSVM, data cache, performance, Parallel processing, shared memory systems, multiprocessor-on-a-chip, Protection]
Toward an improved RBAC model for the organic organization
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
For the smooth and efficient business within the organization of an enterprise today, it is required to utilize the information processing system by using computers, for which the most important factor for the security of organization is the access control for resources. Many access control methods have been studied including DAC and MAC. The role based access control (RBAC), which is based on role, is spotlighted today, and many models have been studied. However, most RBAC models support the bureaucratic organization structure that is mechanical, static, ordinary and of clear vertical relationships. This paper studies the characteristics of the adhocracy organization and the RBAC model to support the adhocracy organization structure that is organic, dynamic and extraordinary such as matrix organization or task force team.
[Access control, project management, adhocracy, data security, enterprise, role based access control, bureaucratic organization structure, project team, Active matrix organic light emitting diodes, Computer science, Authorization, Databases, security of data, Information security, Information processing, organization, Permission, access control, Computer security, information processing system, business data processing, Qualifications]
CoStore: a storage cluster architecture using network attached storage devices
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
We propose a storage cluster architecture CoStore, which evenly distributes system responsibilities across all collaborating cluster members without a separate central file manager. The serverless CoStore architecture has the potential to provide scalable high-performance high-capacity storage services with strong reliability and availability, traditionally only achievable by high-end storage systems. A prototype CoStore system has been implemented using cost-effective COTS-based network attached storage devices on commodity operating systems. The performance of the CoStore prototype has been measured and compared with those of other commonly available distributed file systems. Preliminary test results show that a single node CoStore has a comparable performance to that of NFS and that CoStore outperforms CIFS on Windows 2000.
[network attached storage devices, commodity operating systems, distributed file systems, cost-effective, CoStore, Reliability engineering, availability, Home appliances, File systems, network operating systems, Prototypes, storage reliability, Computer architecture, NFS, Availability, Collaborative software, computer networks, storage cluster architecture, performance evaluation, RAID, Windows 2000, Computer science, high-performance high-capacity storage, Storage area networks, memory architecture, Disk drives, COTS]
Dynamic scheduling of Web server cluster
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Web servers have recently been composed of a server cluster for keeping good quality of service. LVS (Linux Virtual Server) is one method for building a server cluster in Linux and has its own load balancing mechanism. The existing load balancing mechanism, however, often reaches a limit and fails to keep the load in balance, particularly in electronic commerce sites that use typically dynamic script files more than static HTML pages. In this paper, we suggest a new mechanism that overcomes the limit and discuss how to define the load of each real server node in the server cluster and determine the limit of the load. In the suggested mechanism, the weight table of a load balancer is updated according to variation of the load and distribution of service requests is controlled according to the table. The new mechanism shows better results to distribute the load evenly among the real servers than the existing mechanism, when many scripts are included.
[Web server cluster, load balancing, Dynamic scheduling, Routing, HTML, dynamic scheduling, processor scheduling, Linux Virtual Server, Network servers, resource allocation, dynamic script files, Linux, file servers, Load management, Hardware, Internet, Web server, Round robin, Portals]
Slice-and-patch - an algorithm to support VBR video streaming in a multicast-based video-on-demand system
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
In recent years, a number of sophisticated architectures have been proposed to provide VoD service using multicast transmissions. Compared to their unicast counterparts, these multicast VoD systems are highly scalable and can potentially serve millions of concurrent users. Nevertheless, these systems are designed for streaming constant-bit-rate (CBR) encoded videos and thus cannot benefit from the improved visual quality obtainable from variable-bit-rate (VBR) encoding techniques. To tackle this challenge, this paper presents a novel Slice-and-Patch (S&P) algorithm to support VBR video streaming in a multicast VoD system. Extensive trace-driven simulations are conducted to compare performance of the S&P algorithm with two other algorithms based on priority scheduling. Results show that the S&P algorithm outperforms the other two priority scheduling algorithms for most videos. Compared to the CBR counterpart serving videos of the same average bitrate, the S&P algorithm is able to support VBR video streaming with only 50% increase in latency. Given that VBR-encoded video can achieve visual quality comparable to CBR-encoded video at half the bitrate, this S&P algorithm can potentially achieve performance comparable to CBR-based systems when combined with VBR encoding techniques.
[multimedia servers, priority scheduling, VBR video streaming, constant-bil-rate, Multimedia communication, multicast VoD, Scheduling algorithm, Network servers, Multicast algorithms, Bit rate, Layout, video on demand, multicast communication, Streaming media, Broadcasting, scheduling, Motion pictures, video-on-demand, Resource management, multimedia communication]
Three remedied algorithms for advanced waiting time scheduler
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
The advanced waiting time priority scheduler (AWTP), modified from the waiting time priority scheduler (WTP), was proposed to achieve proportional delay differentiation. AWTP not only achieves more accurate delay proportion than the WTP scheduler in short or long timescales, but also greatly reduces the overall queuing delay when the traffic load is moderate. In this paper, we find that AWTP cannot steadily keep the delay ratio under different traffic load distributions because its scheduling sometimes ignores considering packet waiting time. Thus, three remedied algorithms, named minus-WTP (MWTP), existing-WTP (EWTP), and counting-WTP (CWTP), are proposed to resolve this problem. All remedies simultaneously consider the packet waiting time and packet transmission time. Simulation results reveal that these modifications inherit the merit of AWTP and actually alleviate this side effect.
[Protocols, Video on demand, queueing theory, queuing delay, Delay effects, waiting time priority scheduler, proportional delay differentiation, simulation, Telecommunication traffic, Quality of service, Harmonic analysis, Information management, Scheduling algorithm, processor scheduling, delay ratio, advanced waiting time priority scheduler, Diffserv networks, traffic load, packet waiting time, Internet, IP networks, packet transmission time]
Stability aware cluster routing protocol for mobile ad-hoc networks
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
An ad-hoc network is formed by a collection of mobile nodes without any centralized access point or existing infrastructure. Communication between mobile nodes requires routing over the multiple-hop wireless path. Since the mobile nodes could be of high mobility, the effective and adaptive routing protocol must have on-going detail of the topology information. However, it wastes limited bandwidth to keep routing information up-to-date and reliable. Thus, a crucial algorithm design objective to achieve routing responsiveness and updating efficiency is the minimization of reaction to mobility. This paper proposes an efficiently repairable routing protocol, called as gravitational cluster routing (GCR) protocol. It is based on a stable cluster structure that covers dense areas to increase stability, and avoids articulate nodes in the cluster to connect strongly and adopts unicast to minimize reaction to mobility. The active routing paths of GCR can be maintained locally by individual stable clusters and globally by quantifying the corresponding repairable levels to satisfy the distinct demand parameters of QoS. These mechanisms improve the stability of active connections.
[Algorithm design and analysis, Minimization methods, Stability, topology, repairable routing protocol, Mobile communication, wireless networks, Ad hoc networks, network topology, gravitational cluster routing protocol, Delay, mobile ad-hoc network, Network topology, stable cluster structure, Clustering algorithms, routing protocols, Bandwidth, multiple-hop wireless path, Routing protocols, ad hoc networks, mobile nodes]
Architectural level support for dynamic reconfiguration and fault tolerance in component-based distributed software
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
This paper presents a novel architectural approach to support fault tolerance in component-based distributed software (CBDS) through dynamic reconfiguration. Using the graph-oriented programming (GOP) model, the software architecture of CBDS is specified by a logical graph which is reified as an explicit object distributed over the network. Dynamic reconfiguration is implemented by executing a set of operations defined over the graph. The approach supports fault tolerance by dynamically reconfiguring the CBDS upon detection of faults. We describe the basic model, the system architecture and its prototype implementation on top of CORBA.
[logical graph, object-oriented programming, fault tolerance, component-based distributed software, fault detection, architectural level support, explicit object, Application software, Distributed computing, Middleware, software fault tolerance, CORBA, Fault tolerance, software architecture, Runtime, Software architecture, graph-oriented programming model, Fault tolerant systems, Computer architecture, dynamic reconfiguration, Dynamic programming, Internet, visual programming, distributed object management]
Initialization protocols for IEEE 802.11-based ad hoc networks
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Leader election and initialization are two fundamental problems in mobile ad hoc networks (MANETs). The leader can serve as a coordinator in the MANETs and the initialization protocol can assign each host a unique and short id. We know that none of the research on initialization for IEEE 802.11-based MANETs has been done. Here, we propose two contention-based leader election and initialization protocols for IEEE 802.11-based single-hop MANETs. Simulation results show that our protocols are efficient.
[IEEE 802.11, Base stations, Protocols, Nominations and elections, simulation, Ad hoc networks, Relays, Mobile ad hoc networks, telecommunication standards, Computer science, Multicast algorithms, MANET, Network topology, mobile communication, initialization protocols, Feedback, contention-based leader election, mobile ad hoc networks, IEEE standards, ad hoc networks, protocols, wireless LAN]
Performance evaluation of router switching fabrics
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Switching fabrics with distributed control for scalable routers have been proposed recently. Such a fabric consists of small routing units (RU's) interconnected by multistage-based connecting components (CC's) according to grid structures, thereby referred to as a grid-oriented, Multistage-connected RU's, dubbed GMR, and is a direct interconnect with distributed routing. Performance of GMR is evaluated analytically and by simulation, with the packet mean latency being a key performance measure of interest. Under simplified assumptions and uniform traffic distributions, our analytic results are found to be very close to the simulation results (usually within 3% of each other) for a wide range of sizes, providing confidence to our simulation tool. Our simulation study demonstrates that GMR can deliver packets to their destination ports effectively in practical settings even when many ports run at a high-speed rate of 40 Gbps and non-uniform traffic exists. GMR is readily applicable to scalable routers with large numbers of high-speed ports.
[switching, high-speed ports, nonuniform traffic, Scalability, small routing units, simulation, Switches, router switching fabrics, 40 Gbit/s, Distributed computing, Delay, packets, Analytical models, destination ports, Traffic control, packet mean latency, Fabrics, Hardware, network routing, distributed control, performance evaluation, Routing, multistage-based connecting components, multistage interconnection networks, uniform traffic distributions, distributed routing, grid structures, scalable routers, Joining processes]
Gantt Chart visualization for MPI and Apache multi-dimensional trace files
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Trace visualization is a way to understand the behavior of an application on computer systems. In this paper we describe the design and implementation of a Gantt Chart visualization tool for visualizing multi-dimensional trace files, particularly traces from Message Passing Interface (MPI) programs and Apache servers. An MPI tracing library is developed for MPI programs, and an Apache server plug-in is developed to generate Web traces. Since the amount of trace data may be large, utilities are provided to convert and merge multiple event trace files into one scalable, self-defining interval file for visualization. The interval format facilitates the development of multiple time-space diagrams for visualization tools. For MPI traces, each record also includes an instruction address for source code association, which provides a way to display the source code and pin-point the line that generates the event.
[multiple event trace files, application program interfaces, File servers, Yarn, software libraries, scalable self-defining interval file, file servers, time-space diagrams, instruction address, message passing, MPI multi-dimensional trace files, Statistical analysis, Instruments, source code, Apache server plug-in, Apache multi-dimensional trace files, Application software, Web traces, Algorithms, Message passing, Data visualization, Computer applications, Animation, system monitoring, program visualisation, Apache servers, Gantt Chart visualization, MPI tracing library]
Two-layered protocol for a large-scale group of processes
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
A group including a larger number of processes implies larger computation and communication overheads to manipulate and transmit messages. We discuss a group which is composed of subgroups of processes to reduce the overheads. Each subgroup has a gateway process which communicates with the other gateway processes. We propose a protocol to causally deliver messages to processes in a group by using a vector of message sequence numbers whose size is the number of subgroups, smaller than number of processes. We evaluate the protocol.
[Protocols, computation overheads, Communication system control, gateway processes, distributed processing, Routing, Teleconferencing, message sequence numbers, Communication channels, message delivery, communication overheads, Systems engineering and theory, Large-scale systems, IP networks, Telecommunication network reliability, protocols, large-scale process group, two-layered protocol, Clocks]
An efficient real-time scheduler for nested transaction models
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Nested transaction models have been widely adopted in many advanced database applications, such as telecommunications and real-time traffic information systems. However, noticeable studies focus on scheduling flat transactions for the current research of distributed real-time database systems (RTDBS). This paper presents an efficient real-time scheduler consisting of (1) a priority assignment policy called FHRN to schedule real-time nested transactions, and (2) a lock mechanism called 2PL-HPN to resolve the data conflict problem among nested transactions for distributed RTDBS using nested transaction models. The simulation result shows that the FHRN outperforms current priority assignment policies such as ED, HV and HRU.
[Real time systems, transaction processing, distributed real-time database systems, simulation, Telecommunication traffic, Information systems, distributed databases, nested transaction models, Traffic control, scheduling, 2PL-HPN, Database systems, Concurrency control, Transaction databases, Application software, priority assignment policies, real-time scheduler, Computer science, Processor scheduling, advanced database applications, real-time systems, concurrency control, data conflict problem, lock mechanism, priority assignment policy]
TCN: scalable hierarchical hypercubes
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Hierarchical hypercubes, such as extended hypercube (EH), hyperweave (HW), and extended hypercube with cross connections (EHC), have been proposed to overcome the scalability limitation of conventional hypercubes through the use of fixed dimension hypercubes of processing elements (PEs) as basic modules interconnected by network controllers (NC) which are themselves interconnected into hypercubes. The scalability of all these three hierarchical hypercube networks is still limited, because the average network communication load in each NC increases as the number of interconnected PEs become very large. In this work a generalization scheme is proposed for improving network scalability, namely transformer cube network (TCN). For illustration purposes, generalized TCN is presented only for EH, though the same scheme can be applied to HW and EHC as well. Several characteristics of TCN, such as topological properties, message routing complexity, fault tolerance, and scalability are analyzed We present a communication algorithm for one-to-one message passing in a fault-free case. Further, the application of TCN to a class of divide-and-conquer problems is shown to have a time complexity of O(log/sub 2/ N), where N is the total number of PEs.
[divide and conquer methods, Scalability, fault-free case, communication algorithm, Communication system control, Very large scale integration, hypercube networks, communication complexity, scalability, Fault tolerance, scalable hierarchical hypercubes, average network communication load, Hypercubes, network controllers, topological properties, fault tolerance, network routing, message routing complexity, time complexity, Routing, divide-and-conquer problems, one-to-one message passing, Topology, Application software, fixed dimension hypercubes, Computer science, transformer cube network, Message passing, processing elements, fault tolerant computing]
Robust TCP connections for fault tolerant computing
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
When processes on two different machines communicate, they most often do so using the TCP protocol. While TCP is appropriate for a wide range of applications, it has shortcomings in other application areas. One of these areas is fault tolerant distributed computing. For some of those applications, TCP does not address link failures adequately: TCP breaks the connection if connectivity is lost for some duration (typically minutes). This is sometimes undesirable. The paper proposes robust TCP connections, a solution to the problem of broken TCP connections. The paper presents a session layer protocol on top of TCP that ensures reconnection, and provides exactly-once delivery for all transmitted data. A prototype has been implemented as a Java library. The prototype has less than 10% overhead on TCP sockets with respect to the most important performance figures.
[Java, Protocols, session layer protocol, Laboratories, TCP sockets, TCP protocol, exactly-once delivery, Java library, reconnection, Application software, robust TCP connections, Distributed computing, Fault tolerance, Sockets, transport protocols, Prototypes, Robustness, Libraries, fault tolerant computing, fault tolerant distributed computing, link failures]
An improved algorithm for finding k-centrums on weighted trees
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Location theory on networks has been widely investigated by researchers from different fields for more than thirty years due to its significance and practical value. Among various location problems, the p-center and the p-median problems are the most common. The p-facility k-centrum problem, introduced by Slater (1978), is a generalization of the above two problems. The objective is to minimize the sum of the k largest service distances from clients to their nearest servers. When p is an arbitrary integer, the problem is NP-hard on general networks. Therefore, most researchers have devoted to the single-facility case, i.e. p=1, or the case that the networks under consideration are trees. This paper focuses on the single-facility k-centrum problem on a tree. For this problem, Tamir (1996) had an O(nlog/sup 2/ n) time algorithm. In this paper, an O(nlog n) time algorithm with is proposed.
[k-centrums, clients, trees (mathematics), algorithm. time, networks, p-facility k-centrum problem, facility location, weighted trees, Network servers, largest service distances, servers, location theory, single-facility k-centrum problem, computational complexity]
Parallel algorithms for the medial axis transform on linear arrays with a reconfigurable pipelined bus system
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
In this paper based on the advantages of both optical transmission and electronic computation, we first provide an O(log log N) bus cycles parallel algorithm for the medial axis transform of an N/spl times/N binary image on a linear array with a reconfigurable pipelined bus system using N/sup 2/ processors. By increasing the number of processors, the proposed algorithm can be modified to run in O(log log/sub q/ N) and O(1) bus cycles using qN/sup 2/ and N/sup 2+1//spl isin// processors respectively, where 1/spl les/q/spl les//spl radic/N, /spl isin/ is a constant and /spl isin//spl ges/1. These results improve on previously known algorithms developed on various parallel computation models. Key Words: Medial axis transform, image processing, image compression, computer vision, parallel algorithms, linear array with a reconfigurable pipelined bus system.
[Image processing, binary image, parallel algorithm, image compression, Optical computing, system buses, Information management, Parallel algorithms, reconfigurable pipelined bus, Concurrent computing, reconfigurable architectures, image processing, parallel algorithms, data compression, Computational modeling, Phase change random access memory, Educational institutions, medial axis transform, electronic computation, Optical arrays, computer vision, image coding, Pixel, optical transmission, computational complexity]
Consistent LBS solution in next generations of mobile internet
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Location based services (LBSs) have been considered as promising killer applications. There are already some LBSs available in the market while it seems that there is not such a big demand on them. The main reason is that there are many different technology alternatives for providing LBS (WAP and LIF, among others). This problem causes the market fragmentation where different vendors provide LBS applications built with their own protocols and mechanisms. The approach is in detriment of the user experience on LBS. They usually perceive different services via the terminals of different vendors, while the services should be the same from LBS point of view. In addition, this problem dramatically increases the investment and operation costs for operators to provide LBSs for the users with the terminals of various vendors. Therefore, this paper proposes a novel mechanism (a common "LBS" service type and a common LBSP protocol) to create the common LBSs universally understood by different terminals or systems. LBSP works in wired and wireless IP networks for providing LBSs and, reuses existing network elements and protocols.
[Costs, location based services, Wireless application protocol, Proposals, Wire, Next generation networking, Network servers, mobile computing, mobile communication, Space technology, transport protocols, Investments, wired IP networks, wireless IP networks, Internet, IP networks, protocols, next generation mobile Internet]
Modeling for an integration of distributed business information using EPEM
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
In this paper, we show that the IIS-MDR offers the integrated information to the enterprise by making it possible to exchange data between the regionally distributed heterogeneous computers and also to access various types of databases. And the system is modeled using EPEM and is verified in order to verify whether our model satisfies the requirements under the given assumptions. The verification results show that the model satisfies the specified requirements. In future work, we plan to apply the agent concepts to the model and to compare performance with current model.
[meta data, metadata, Object oriented databases, data exchange, Object oriented modeling, distributed heterogeneous computers, Relational databases, Conference management, Application software, Middleware, Information systems, formal verification, Engineering management, Distributed databases, system architecture, Management information systems, distributed databases, metadata registry, middleware]
A mobile-agent based distributed dynamic /spl mu/Firewall architecture
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
With the development of enterprise Intranet and cluster servers, many emerging security challenges could not be solved by conventional firewall due to its inner deficiency. To address these security problems, we present a mobile-agent based distributed dynamic /spl mu/Firewall architecture. In this architecture, special mobile agents implement a dynamic security policy reconfiguration and enhance the scalability. Each /spl mu/Firewall is built with a packet filter and DTE-enhanced evaluator to provide dual fine-grain protection at the individual host level. A distributed intrusion detection and response (DIDR) system provides a fast response to both external and internal attacks, and allows an adaptive change in the security policy in the protected network. The DIDR system provides the infrastructure to support hierarchical intrusion responses and dynamic security capabilities. The distributed security architecture is scalable, topology independent, and intrusion-tolerant.
[/spl mu/Firewall architecture, Scalability, parallel architectures, distributed processing, firewalls, dynamic security policy, local area networks, scalability, Filters, Network topology, security of data, enterprise Intranet, Mobile agents, Intrusion detection, mobile agents, domain type enforcement, Computer architecture, authorisation, distributed intrusion detection response system, Internet, Protection, Web server, distributed dynamic architecture, Mobile computing]
Cell-based positioning method for wireless networks
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
One of the most important applications for mobile commerce is location-based application and the core technology of location-based applications is the location determination technology. In this paper, we present a positioning method, called cell-based positioning method and its positioning accuracy measure for the wireless networks with hexagon structure and mesh structure. In addition, the positioning accuracy is optimized by tuning the base stations' transmission power. In the optimal transmission power an accuracy of within 9.1% (7.2%) cell area for hexagonal structure (mesh structure) can be achieved by cell-based positioning method. We believe that the results are useful to deploy wireless networks for location-based applications.
[mesh structure, Costs, wireless networks, Application software, location-based application, cell-based positioning, Global Positioning System, mobile commerce, Network servers, mobile computing, Wireless networks, Wireless mesh networks, Position measurement, Telephone sets, wireless LAN, positioning method, Mobile computing, Business]
Effective skew handling for parallel sorting in multiprocessor database systems
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
A consensus on a parallel architecture for very large database management has emerged. This architecture is based on a shared-nothing hardware organization. The computation model is very sensitive to skew in tuple distribution, however. The sorting operation is frequently used for database processing. For example sorting may be requested by users through the use of Distinct, Order By and Group By clauses in SQL. Although load balancing incurs processing costs, and therefore can have a profound influence on the optimized execution plan of a query, only few of the existing parallel sorting executions consider this factor. We present two parallel sorting algorithms using the dynamic load balancing technique to address the data skew problem. Our performance study indicates that the proposed parallel sorting techniques can provide very impressive performance improvement over conventional approaches.
[load balancing, Heuristic algorithms, very large database, query execution plan, Distributed computing, query processing, resource allocation, tuple distribution, very large databases, Computer architecture, sorting, distributed databases, Cost function, Database systems, Hardware, skew handling, shared-nothing hardware organization, software performance evaluation, parallel algorithms, multiprocessing systems, multiprocessor database systems, Computational modeling, parallel architecture, Parallel architectures, Sorting, SQL, performance, parallel sorting, Load management]
Cache cooperation for clustered disconnected computers
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
The problem of disconnection from network is one that mobile users always have to face in mobile computing environment. Disconnected operation is one approach to ease this problem by allowing client computers to work solely on cached files in the local disks. However, a cache miss in disconnected operation is often critical, and usually causes the user to stop work on the current task until reconnecting to the server. To avoid such situations, we propose an approach called cache cooperation, which resolves cache misses in a coordinated manner among the clients. Cache cooperation allows each client to retrieve the missing data from the other remote clients when the client is disconnected from the server but not isolated from others. We describe the design of our prototype implementation, its performance measurement, and the results of vast numbers of simulations that we conducted with system-call traces recorded over 3-month period. The measured performance shows that the overhead of our approach is almost comparable to that of the normal client-server file operations. Moreover, the simulation results confirm that our approach can effectively decrease cache misses when multiple client caches can be utilized. Especially, in ideal cases where major files are shared by multiple clients, our approach saves almost all of cache misses for over 70 percent of simulation runs.
[Measurement, client-server systems, missing data retrieval, Computational modeling, disconnected operation, Spine, client server system, Telecommunication traffic, state transition, File servers, Information retrieval, cache cooperation, cache storage, Information systems, mobile computing, Computer networks, cache miss, Virtual prototyping, Mobile computing]
A spread neural network with fuzzy clustering technique applied to color image coding in the MDT domain
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
In this paper an unsupervised parallel approach called fuzzy competitive learning network (FCLN) for vector quantization (VQ) and spread FCLN (SFCLN) for color image compression in the mean value/difference value transform (MDT) domain are proposed. In the FCLN, the codebook design is conceptually considered as a clustering problem. Here, it is a kind of competitive learning network model imposed by the fuzzy clustering strategies working toward minimizing an objective function defined as the average distortion measure between any two training vectors within the same class. The color image information transformed by the MDT operation was separated into RGB 3-plane mean value and detail coefficients. Then the detail coefficients for each plane were trained using the proposed SFCLN method to generate the VQ codebook. The experimental results show that promising codebooks can be obtained using the proposed FCLN and SFCLN for color image compression in the MDT domain.
[Clustering methods, color image compression, Intelligent networks, Image coding, Clustering algorithms, average distortion measure, Fuzzy neural networks, image colour analysis, Distortion measurement, training vectors, objective function, codebook design, RGB 3-plane mean value, Vector quantization, Scattering, fuzzy neural nets, Color, spread neural network, fuzzy competitive learning network, unsupervised learning, vector quantisation, fuzzy clustering technique, unsupervised parallel approach, pattern clustering, Neural networks, mean value/difference value transform, vector quantization, color image coding, image coding, detail coefficients]
Job scheduling policy for high throughput computing environments
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
In high throughput computing environments, an opportunistic scheduling policy is used for placement of batch jobs on idle workstations for execution. In this paper, we propose a new opportunistic scheduling policy with the aim of exploiting the idle resources provided by systems such as Condor in an opportunistic manner. We compared the performance of the proposed scheduling policy with three policies namely the scheduling policy used in Condor, job rotate and round robin policies through simulation. The results show that the proposed scheduling policy offers substantial performance improvements over the exiting approaches. Furthermore, this improved performance is achieved without loss of throughput and it results in a more interactive nature of the system thus increasing its appeal.
[workstation clusters, opportunistic scheduling policy, batch job placement, Condor, round robin policies, Computational modeling, idle workstations, simulation, Throughput, processor scheduling, Computer science, Processor scheduling, High performance computing, job rotate, job scheduling policy, Performance loss, Workstations, Performance analysis, Large-scale systems, Round robin, high throughput computing environments]
Approaches to balancing data load of shared-nothing clusters and their performance comparison
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Using a cluster of PCs or workstations or the like (called nodes) to implement the database server can bring us two great benefits: high scalability and parallel processing capability. Before such a database server can be put into actual use, however two problems have to be solved. The one is how we cope with the data-skew since it can degrade the system performance significantly. The other is how a node is connected to or disconnected from a database server without affecting the users. One general solution to both problems is to redistribute the data. Unfortunately, this would take the data offline for a long time. In fact, numerous applications such as that for reservations, finance, process control, hospitals, police, and armed forces cannot afford the offline data for any significant amount of time. We address the subject of balancing data load online, i.e., balancing data load concurrently with users' reading and writing of the database. The main contributions are an effective approach for this purpose and a comprehensive performance study of the possible alternatives.
[workstation clusters, load balancing, database server, Scalability, Finance, Process control, performance evaluation, parallel processing, workstation cluster, Degradation, Databases, Hospitals, resource allocation, System performance, shared-nothing clusters, distributed databases, Parallel processing, performance comparison, Workstations, data-skew, Personal communication networks, high scalability]
Experiences in building a scalable distributed network emulation system
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Network emulation systems are widely used to explore the behavior of network protocols and to test and evaluate protocol implementations and applications. The major problem of a network emulation system is its scalability in terms of emulation capacity and emulation capability of specific network parameters such as maximum emulated bandwidth and emulated packet delay of the system. In particular, an emulator with a large number of workstations is generally too costly for researchers to afford. We present our experience in building a scalable distributed network emulation system named "EMPOWER". EMPOWER provides the unique feature of precisely emulating multiple nodes with a single workstation, making it possible to support large network emulation with a limited number of commodity computers. We describe some critical design issues such as the system resource competition within an emulator node and its impact on the emulation of network throughput and packet delay. We present our methods to determine the maximum number of virtual routers an emulator node can generate and some techniques to improve maximum throughput and packet delay accuracy of an emulator node. We believe such experiences are valuable for the study, design and implementation, performance tuning of a variety of systems such as network emulators and high performance host-based routers.
[System testing, Protocols, EMPOWER, Scalability, Delay systems, Throughput, network throughput, digital simulation, scalability, virtual routers, Emulation, Bandwidth, Computer networks, Workstations, system resource competition, high performance host-based routers, protocols, performance tuning, Buildings, bandwidth, computer networks, network protocols, workstations, telecommunication network routing, scalable distributed network emulation system, packet delay]
Evaluating the speedup of a MPI-based distributed dispersion simulator for groundwater systems
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
This software version is a distributed implementation of the public domain software of groundwater dispersion simulation of Anderson's book. This simulator proposed in Anderson's book is a sequential program that calculates the concentrations during the dispersion of an aquifer. Basically, the numerical method used in this simulator is finite differences. Depending on the number of NNODEs (number of nodes in the problem domain) used in the grid for the simulation, the simulation time can be prohibitive. The main subject of this paper is to evaluate the performance of a new version of DGWSDS (Distributed Groundwater Simulation Dispersion System). Running the distributed simulator with three different input parameters size NN-ODE=2000, 3000, 4000 and 5000, for a cluster of 8 nodes, the DGWSDS achieves speedups up to 10.73, which is an excellent value if the interconnection and the overhead of the TCP/IP are considered.
[message passing, application program interfaces, concentrations, MPI-based distributed dispersion simulator, public domain software, interconnection, geophysics computing, finite difference methods, speedup, performance evaluation, groundwater, digital simulation, finite difference method, TCPIP, Software systems, TCP/IP, sequential program, Books, groundwater dispersion simulation, aquifer, Finite difference methods]
Reliable message delivery for mobile agents: push or pull
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Two of the fundamental issues in message passing between mobile agents are tracking the migration of the target agent and delivering messages to it. In order to provide reliable message delivery, protocols are needed to overcome message loss caused by asynchronous operations of agent migration and message forwarding. In this paper, two message forwarding approaches, namely push and pull, are explored to design adaptive and reliable message delivery protocols. The pros and cons of these two approaches are evaluated, both qualitatively and quantitatively. The comparative performance evaluation is in terms of network traffic and delay in message processing. We also propose improvements to the pull approach to reduce network traffic and the message delay. We conclude that with different communication and migration patterns and requirements of real-time message processing, specific applications can select different message delivery approaches to achieve the desired level of performance and flexibility.
[message loss, Protocols, Laboratories, Telecommunication traffic, Mobile communication, Communication system security, Relays, pull approach, Mobile agents, mobile agents, reliable message delivery, agent migration, protocols, Target tracking, message passing, push approach, performance evaluation, real-time message processing, message forwarding, Computer science, network traffic, asynchronous operations, delay, real-time systems, Internet]
Module filtering preprocessing for module assignment problems in pipelined computing
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
In this paper, we extend previous works on the problem of mapping pipelined (chain-like) computation modules onto a set of (chain-like) linear array processors. We show how this problem can be solved more effectively by pre-filtering out those bad partition points, which can be determined by the proposed forward-backward filtering scheme. By eliminating those infeasible (bad) partition points, some modules can be merged virtually and thus the total number of modules to be considered is reduced. Consequently, search speed is accelerated Meanwhile, the sub-chain cost matrix, formed by the new module set after above filtering and merging scheme, can be proved being well (lattice-like) ordered. This elegant property avoids the need of matrix rearrangement while applying the two dimension binary search method to find optimal solutions.
[module assignment problems, Costs, pipelined computing, Filtering, Heuristic algorithms, Oceans, Pipelines, forward-backward filtering scheme, Partitioning algorithms, module filtering preprocessing, 2D binary search method, Computer science, Information science, distributed algorithms, linear array processors, sub-chain cost matrix, merging scheme, Polynomials, search speed, pipeline processing, Acceleration]
Sago: a network resource management system for real-time content distribution
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Content replication and distribution is an effective technology to reduce the response time for Web accesses and has been proven quite popular among large Internet content providers. However, existing content distribution systems assume a store-and-forward delivery model and is mostly based on static content. This paper describes the design, implementation, and initial evaluation of a network resource management system for real-time Internet content distribution called Sago, which provides facilities to provision and allocate network resources so that multiple bandwidth-guaranteed and fault-tolerant multicast connections can be multiplexed on a single physical network. Sago includes a novel network resource mapping algorithm that takes into account both physical network topology and dynamic traffic demands, a network-wide fault tolerance mechanism that supports both node-level and link-level fault tolerance, and a hierarchical network link scheduler that provides performance protection among multicast connections sharing the same physical network link. Moreover, Sago does not require any IP multicasting support from underlying network routers because it performs application-level multicasting. The technologies underlying Sago are important building blocks for real-time content distribution networks, end-to-end quality of service guarantee over global corporate intranets, and application-specific adaptation of wide-area network services.
[Real time systems, bandwidth-guaranteed multicast connections, Internet content providers, network resource allocation, Delay, link-level fault tolerance, Fault tolerance, Network topology, application-specific wide-area network service adaptation, Fault tolerant systems, IP multicasting support, multicast communication, network-wide fault tolerance mechanism, dynamic traffic demands, IP networks, performance protection, node-level fault tolerance, Web accesses, real-time content distribution, fault-tolerant multicast connections, Scheduling algorithm, network resource management system, Sago, Multicast algorithms, content replication, global corporate intranets, physical network topology, response time, real-time systems, real-time Internet content distribution, fault tolerant computing, end-to-end quality of service guarantee, Internet, Resource management, hierarchical network link scheduler]
A multi-locking mechanism on shared object DSM
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Shared object Distributed Shared Memory (DSM) minimizes the problem of false sharing by allowing programmer to control the sharing size. This shared object approach for distributed parallel programming works well in task parallelism but not in data parallelism. When the data of a shared object is being modified, a lock on that object must be enforced to exclude any concurrent access on that same object. If the shared data within an object is large, internal false sharing would become a problem. We present a multi-locking mechanism for shared object DSM which allows multiple locks be applied to the different data sets of a shared object and thus enhances its concurrency power.
[Shared Object DSM, Scattering, Distributed Shared Memory, false sharing, Programming profession, parallel programming, concurrency, Concurrent computing, Computer science, task parallelism, Parallel programming, data parallelism, concurrency control, Parallel processing, distributed shared memory systems, Hardware, shared object, Australia, Size control, multi-locking mechanism, Force control, distributed parallel programming]
Pancyclicity of Mobius cubes
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
The problem of containing pancyclic interconnection networks is an important research topic. An n-dimensional Mobius cube, MQ/sub n/, is a variant of hypercubes according to specific rules. In this paper, we prove that Mobius cubes are all pancyclic networks. Similarly, both an n-dimensional crossed cube, CQ/sub n/, and an n-dimensional twisted cube, TQ/sub n/, are also variants of hypercubes according to specific rules. Moreover although the pancyclic property of a crossed cube and a twisted cube had been proved, we propose an alternative proof of this property.
[Multiprocessor interconnection networks, crossed cube, pancyclic interconnection networks, n-dimensional Mobius cube, hypercube networks, hypercubes, Distributed computing, twisted cube, Circuit topology, Network topology, Councils, Hypercubes, Computer networks, Token networks, Data flow computing, Local area networks]
An efficient load balancing strategy for scalable WAP gateways
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
In this paper, we propose a load balancing strategy that has the following features: (1) estimating the potential load of real gateways with low computation and no communication overhead, (2) asynchronous alarm sent when the utilization of a real gateway exceeds a critical threshold, and (3) WAP-awareness. We also propose a scalable WAP gateway (SWG) that consists of a WAP dispatcher and a cluster of real gateways. The WAP dispatcher is a front-end distributor with our load balancing strategy. To prevent the WAP dispatcher from becoming a bottleneck, the WAP dispatcher distributes mobile clients' requests in kernel space and does not process outgoing gateway-to-client responses. Experimental results show that our SWG has better load balancing performance, throughput, and delay compared to the LVS and the Kannel gateway. Although WAP services are not so popular as expected, our load balancing strategy, can be easily adapted to other distributed services.
[network servers, Wireless application protocol, Scalability, front-end distributor, WAP-awareness, Delay, Information science, efficient load balancing strategy, kernel space, resource allocation, Wireless networks, throughput, Kernel, client-server systems, Discussion forums, Access protocols, Reluctance generators, asynchronous alarm, Kannel gateway, mobile communication, delay, mobile client requests, Load management, scalable WAP gateways, WAP dispatcher, Internet, gateway cluster, distributed services]
Distributed parallel metaheuristics based on GRASP and VNS for solving the traveling purchaser problem
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
This paper presents several strategies for parallel implementations of the greedy randomized adaptive search procedure (GRASP) and the variable neighborhood search (VNS) applied to a combinatorial optimization problem known as the traveling purchaser problem (TPP). Parallel algorithms based on master-worker, completely distributed and independent models, using static and dynamic load balance were proposed. The performance of these parallel algorithms was analyzed comparing them among themselves and with their sequential versions.
[Algorithm design and analysis, Costs, static load balance, combinatorial mathematics, combinatorial optimization problem, dynamic load balance, greedy randomized adaptive search procedure, Electronic mail, Parallel algorithms, traveling purchaser problem, Genetic algorithms, optimisation, resource allocation, heuristic programming, Parallel processing, Performance analysis, search problems, parallel algorithms, variable neighborhood search, master-worker models, Traveling salesman problems, randomised algorithms, independent models, Computer science, Upper bound, performance, distributed models, distributed parallel metaheuristics]
On the fault-tolerant pancyclicity of crossed cubes
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
A graph is pancyclic if it contains all cycles from lengths 4 to |V(G)|. An n-dimensional crossed cube, an important variation of hypercube denoted as CQ/sub n/, has been proved to be pancyclic because it contains all cycles whose lengths range from 4 to |V(CQ/sub n/)|. Since vertex and edge faults may occur when a network is used, it is practical and meaningful to evaluate the performance of a faulty network. Moreover the vertex fault-tolerant Hamiltonicity and the edge fault-tolerant Hamiltonicity measure the performances of the Hamiltonian properties in the faulty networks. From this fault-tolerant concept, we propose using the fault-tolerant pancyclicity of networks to measure the performance of faulty networks. In this paper we consider a faulty crossed n-cube with vertex and/or edge faults here. Let the faulty set F be a subset of V(CQ/sub n/)/spl cup/E(CQ/sub n/). We prove that any cycle of length l(4/spl les/l/spl les/|V(CQ/sub n/)|-f/sub /spl nu//) can be embedded into a faulty crossed n-cube CQ/sub n/-F with dilation 1, where |F|=f/sub /spl nu//+f/sub e/ is less than n-2, f/sub /spl nu// is the number of faulty vertices of F, f/sub e/ is the number of faulty edges of F, and n is greater than 2. The results can readily be used in the optimum embedding of a ring of the specified length in a faulty crossed cube.
[Performance evaluation, Algorithm design and analysis, hypercube, optimum ring embedding, Multiprocessor interconnection networks, n-dimensional crossed cube, vertex faults, hypercube networks, vertex fault-tolerant Hamiltonicity, fault-tolerant Hamiltonicity, cycle, Fault tolerance, Information science, fault-tolerant pancyclicity, faulty network, Councils, Management information systems, Hypercubes, edge faults, Computer networks, fault tolerant computing, Contracts]
Evaluating and improving performance of multimedia applications on simultaneous multi-threading
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
This paper presents the study and results of running several core multimedia applications on a simultaneous multithreading (SMT) architecture, including some detailed analysis ranging from memory-bounded kernels to computational-bounded functions. A performance metric to evaluate effective SMT performance gain is introduced, and compared to similar metrics on symmetric multiprocessor (SMP) systems. In addition, we analyze and compare SMT versus SMP systems, and highlight the advantages in the studied applications. The results indicate that sharing the cache in SMT processors can provide better cache locality and thus better performance although sharing the cache can introduce cache conflicts and reduce the actual cache size available for each logical processor. We also propose "mutual prefetching" -a technique to schedule threads so that they prefetch data for each other in order to reduce cache miss penalty.
[simultaneous multithreading architecture, Surface-mount technology, Pipelines, multimedia applications, symmetric multiprocessor systems, cache storage, multimedia computing, Yarn, Delay, processor scheduling, performance metric, memory-bounded kernels, Microprocessors, cache miss penalty, Computer architecture, cache locality, cache conflicts, cache size, Kernel, multi-threading, Prefetching, cache sharing, Decoding, Multithreading, mutual prefetching, computational-bounded functions, thread scheduling]
Dual-Hamiltonian-path-based multicasting on wormhole-routed star graph interconnection networks
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
The star graph interconnection network has been recognized as an attractive alternative to the popular hypercube network. In this paper we first address a dual-Hamiltonian-path-based (DHPB) routing model with two virtual channels based on two Hamiltonian paths and a network partitioning strategy for wormhole-routed star graph networks. Then, we propose three efficient multicast routing schemes on basis of such a model. The proposed schemes are network-selection-based (NSB), heuristic-network-selection-based (HNSB), and two-phase heuristic-network-selection-based (TP-HNSB) dual-path routing. All of the three proposed schemes are deadlock-free. Finally, experimental results are given to show our proposed three routing schemes outperform the unicast-based, the Hamiltonian-path, and the single-Hamiltonian-path-based (SHPB) dual-path routing schemes significantly.
[network-selection-based dual-path routing, Multiprocessor interconnection networks, network routing, multiprocessor interconnection networks, virtual channels, dual-Hamiltonian-path-based multicasting, Routing, network partitioning strategy, Information management, Electronic mail, wormhole-routed star graph interconnection networks, Computer science, two-phase heuristic network-selection-based dual-path routing, Chaotic communication, System recovery, Hypercubes, heuristic-network-selection-based dual-path routing]
A new probabilistic approach for fault-tolerant routing in k-ary n-cubes
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
This paper describes the new fault-tolerant routing algorithm for k-ary n-cubes using the concept of "probability vectors" and conducts an extensive performance analysis for the new algorithm. To compute these vectors, a node determines first its faulty set, which represents the set of all its neighbouring nodes that are faulty or unreachable due to faulty links. Each node then calculates a probability vector, where the l/sup th/ element represents the probability that a destination node at distance l cannot be reached through a minimal path due to a fault node or link. The probability vectors are used by all the nodes to achieve an efficient fault-tolerant routing in the network. The extensive performance analysis conducted in this study reveals that the proposed algorithm exhibits good fault-tolerance properties in terms of the achieved average routing distances.
[Algorithm design and analysis, Multiprocessor interconnection networks, network routing, probability, multiprocessor interconnection networks, faulty links, Probability, Routing, destination node, probability vectors, fault-tolerant routing algorithm, Computational complexity, Delay, Computer science, Fault tolerance, neighbouring nodes, minimal path, Hypercubes, Performance analysis, average routing distances, k-ary n-cubes, performance analysis, faulty set]
Flexible packet scheduling for quality of service provisioning in wireless networks
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
This paper presents a novel wireless scheduling mechanism, called Adaptive Service Curve (ASC). The proposed mechanism increases the flexibility with which network operators can adjust the allocation of resources. A good scheduler should exhibit three kinds of flexibility. First, ASC should be able to differentiate the error resilience requirements due to the impact of location-dependent channel errors in wireless networks. Specifically, ASC can employ link adaptation, enabling a choice to be made between maximizing system throughput and making more link effort on error-prone channels. Second, the above flexibility is not system-wide. Rather users can subscribe to different error resilience policies. ASC utilizes the service curve model that can best meet QoS requirements to provide the third kind of flexibility, and prevents an unacceptable over-allocation of radio resources. Accordingly, a framework is proposed to make the existing service curve model operate effectively in wireless environments. The ASC scheduler combines in a single-framework the three aspects of scheduling, namely, link adaptation, flexibility and a mature traffic characterization model. This design represents a complete solution for wireless resource management.
[wireless scheduling, wireless resource management, Adaptive Service Curve, Quality of service, Telecommunication traffic, Throughput, wireless networks, quality of service, traffic characterization, Scheduling algorithm, Resilience, Intelligent networks, Processor scheduling, Wireless networks, Traffic control, scheduling, error resilience, packet scheduling, Resource management, wireless LAN]
An agent-based approach to enforcing fairness in peer-to-peer distributed file systems
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Peer-to-peer file systems are typically vulnerable to denial-of-service or free-loader problems. Those that address these issues employ approaches that are either simplistic or require a centralized authority. We explore how a zero-sum trading system can provide strong quotas to a peer-to-peer distributed file system without any centralized authority. We treat each member of such a peer-to-peer system as an autonomous agent, interested in preserving its own disk storage. We develop a model for these agents, and present experimental simulation and external emulation results from a multi-agent reinforcement learning model demonstrating the validity of this approach.
[multi-agent systems, denial-of-service, Security, Learning, autonomous agent, File systems, experiment, Fault tolerant systems, Emulation, network operating systems, distributed databases, Broadcasting, Autonomous agents, learning (artificial intelligence), free-loader problems, zero-sum trading system, fairness, Peer to peer computing, disk storage, agent-based approach, security of data, Load management, Internet, peer-to-peer distributed file systems, multi-agent reinforcement learning model]
A channel allocation algorithm for large scale cellular networks
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Due to the insufficiency of available bandwidth resources and the continuously growing demand for cellular communication services, the channel assignment problem becomes increasingly important. To trace the optimal assignment, several heuristic strategies have been proposed. So far, most of them focus on the small-scale systems containing no more than 25 cells and they use an anachronistic cost model that does not satisfy the requirements of most existing cellular operators to measure the solution quality. Solving the small-scale channel assignment problems could not be applied into existing large scale cellular networks' practice. This article proposes a decomposition approach to solve the fixed channel assignment problem (FCAP) for large-scale cellular networks through partitioning the whole cellular network into several smaller sub-networks and then a sequential branch-and-bound algorithm is designed to solve the FCAP for them sequentially. The key issue of partition is to minimize the dependences of the sub-networks so that the proposed heuristics for solving smaller problems will suffer fewer constraints in searching better assignments. The experimental results show that the proposed algorithms perform well and we applied our algorithms in finding better assignments for the cellular network of the Taiwan Cellular Cooperation in ChungLi city.
[Algorithm design and analysis, Costs, frequency allocation, coupling rule, large-scale cellular networks, optimisation, channel allocation algorithm, heuristics, branch-and-bound algorithm, Bandwidth, fixed channel assignment, Large-scale systems, channel allocation, telecommunication network planning, Partitioning algorithms, cellular communication, cellular network partitioning, Cellular networks, Computer science, Land mobile radio cellular systems, Channel allocation, Frequency, Taiwan Cellular Cooperation, cellular radio, decomposition algorithm]
Normality versus system mobility
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Normality, consistency criteria stronger than sequentiality and equivalent to linearizability for the unary operations case, has the main advantage that it avoids the use of the "global real-time ordering". This work presents the first algorithm that implements normality without using strong communication primitives (i.e. atomic broadcast or global clock synchronization). Moreover, our implementation allows the dynamic changes of the system configuration, handles replication and refers the general case of multi-object operations. Although the use of terms as client or server our algorithm is entirely based on a peer-to-peer approach.
[Real time systems, wide area networks, Scalability, consistency criteria, system mobility, peer-to-peer approach, Network servers, Network topology, Distributed databases, Broadcasting, Routing protocols, unary operations, Distributed algorithms, global clock synchronization, multi-object operations, client-server systems, global real-time ordering, Peer to peer computing, client server, synchronisation, atomic broadcast, communication primitives, distributed algorithms, normality, Clocks]
Accelerate the calculation of NURBS curves and surfaces based on parallel architecture
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
The aim of the paper is to propose a three-dimensional graphics chip with parallel architecture, in accordance with the chip of the NURBS algorithm was structured. This architecture presents a regular and easily scalable structure, suitable for VLSI implementation, which can be efficiently exploited for the computing process. This architecture can apply to a peripheral device of a computer and a user can use it for fitting curves and surfaces, convert NURBS curves to Bezier and so on, which have 16 bit precision. In this paper, we first describe the framework of the whole system. Secondly, we introduce the hardware of FPGA and explain the principle. Next, we will also illustrate NURBS, and test and verify using Visual C++ and OpenGL. The performance of the proposed architecture is improved by the use of carry save arithmetic which permits the reduction of the system time cycle.
[splines (mathematics), parallel architectures, field programmable gate arrays, FPGA, Visual C++, Very large scale integration, NURBS surfaces, carry save arithmetic, Spline, surface fitting, Surface reconstruction, NURBS curves, Computer architecture, Computer graphics, 3D graphics chip, OpenGL, parallel architecture, Surface topography, Parallel architectures, peripheral device, computer graphics, Computer peripherals, system time cycle, 16 bit, Surface fitting, curve fitting, Acceleration, VLSI implementation]
Evaluation of a programmable cluster-based IP router
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
A major challenge in Internet edge router design is to support both high packet forwarding performance and versatile and efficient packet processing capabilities. The thesis of this research project is that a cluster of PCs connected by a high speed system area network provides an effective hardware platform for building routers to be used at the edges of the Internet. This paper describes a scalable and extensible edge router architecture called Panama, which supports a novel aggregate route caching scheme, a real-time link scheduling algorithm whose performance overhead is independent of the number of real-time flows, a highly efficient kernel extension mechanism to safely load networking software extensions dynamically, and an integrated resource scheduler which ensures that real-time flows with additional packet processing requirements still meet their end-to-end performance requirements. This paper describes the implementation and evaluation of the first Panama prototype based on a cluster of PCs and Myrinet.
[workstation clusters, integrated resource scheduler, Software performance, Myrinet, Panama, Computer architecture, scheduling, Hardware, IP networks, Kernel, aggregate route caching scheme, Buildings, performance evaluation, packet forwarding performance, programmable cluster-based IP router, Scheduling algorithm, end-to-end performance requirements, PC cluster, performance overhead, Aggregates, transport protocols, telecommunication network routing, real-time systems, Internet, Personal communication networks, Internet edge router design, high speed system area network, packet processing, real-time link scheduling algorithm]
Synchronous/asynchronous switch for a dynamic choice of communication model in distributed systems
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
As the programming of distributed systems becomes more complex, new programming models have been used for creating applications. Communication between different system components is based on two main communication models: communication by messages and communication by remote method invocations. The choice of synchronous communication via remote method invocation or of message-based asynchronous communication still remains an early decision made at the time of the application design. In this article we propose a mechanism for dynamic and transparent switching of the communication model used between the components of a distributed system. This mechanism, aimed especially at large scale applications and mobile systems, offers the possibility to adapt the communication model to the execution context of an application and changes in its communication environment.
[message passing, Switches, Programming, distributed system, Mobile communication, Application software, Communication switching, programming models, Computer science, Asynchronous communication, communication models, remote method invocations, remote procedure calls, distributed systems, messages, Large-scale systems, Internet, distributed programming, synchronous communication, Context modeling]
Design and analysis of a fault-tolerant mechanism for a server-less video-on-demand system
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Video-on-demand (VoD) systems have traditionally been built on the client-server architecture, where a video server stores, retrieves, and transmits video data to video clients for playback This paper investigates a radically different approach to building VoD systems, one where the server, and hence the primary bottleneck, is completely eliminated. This server-less architecture comprises homogeneous hosts, called nodes, which serve both as client and as mini-server. Video data are distributed over all nodes and these nodes cooperatively stream video data to one another for playback. However, unlike traditional video server that runs on high-end server hardware in a carefully controlled and protected data centre, a node in a server less system is likely to be far more unreliable. Therefore it is essential that sufficient data and capacity redundancies are incorporated to maintain an acceptable set-vice reliability. This paper presents and analyzes a fault tolerant mechanism based on inter-node striping and erasure correction codes to tackle this challenge. By formulating the system's reliability as a Markov chain model, we obtain insights into the feasible operating region of the system, such as the amount of redundancy required and the node-level reliability that can be tolerated. Numerical results show that a server-less VoD system of 200 nodes can achieve reliability surpassing that of dedicated video server using a redundancy overhead of only 21.2% even though individual nodes are highly unreliable.
[inter-node striping, mini-server, data redundancies, Markov chain model, redundancy overhead, Control systems, capacity redundancies, Fault tolerance, Fault tolerant systems, video on demand, homogeneous hosts, client, Hardware, playback, redundancy, Protection, client-server systems, Redundancy, Buildings, Information retrieval, fault-tolerant mechanism, Maintenance, cooperative video data streaming, erasure correction codes, service reliability, nodes, server-less video-on-demand system, Streaming media, fault tolerant computing]
Communication pattern based methodology for performance analysis of termination detection schemes
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Efficient determination of processing termination at barrier synchronization points can occupy an important role in the overall throughput of parallel and distributed computing systems. Even though relatively efficient termination detection techniques have been proposed for certain environments, no effective performance analysis methodology has been introduced to determine application attributes that favor the use of a particular termination detection technique. This fact has hindered the adoption and development of termination detection schemes. This paper addresses this problem by developing a communication pattern based methodology to improve the precision of the theoretical performance of termination detection techniques in lieu of laborious experiments or potentially subjective benchmarking studies. By measuring message complexity from the idle period respect, it provides a simple and effective way to evaluate existing termination detection techniques or design new termination detection algorithms.
[Algorithm design and analysis, performance evaluation, Throughput, communication complexity, Distributed computing, parallel processing, Multiprocessing systems, synchronisation, distributed computing systems, idle period, message complexity, termination detection techniques, communication pattern based methodology, barrier synchronization points, parallel computing systems, Computer architecture, Communication channels, Parallel processing, processing termination, Performance analysis, IP networks, Detection algorithms, performance analysis]
Hamiltonian laceability on edge fault star graph
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
The star graph is an attractive alternative to the hypercube graph. It possess many nice topological properties. Edge fault tolerance is an important issue for a network since the edges in the network may fail sometimes. In this paper, we show that the n-dimensional star graph is (n-3)-edge fault tolerant hamiltonian laceable, (n-3)-edge fault tolerant strongly Hamiltonian laceable, and (n-4)-edge fault tolerant hyper Hamiltonian laceable. All these results are optimal in a sense described in this paper.
[Fault tolerance, Network topology, Hamiltonian laceability, multiprocessor interconnection networks, edge fault star graph, Hypercubes, fault tolerant computing, edge fault tolerance, Bipartite graph, topological properties, n-dimensional star graph]
An approach to exploiting skewed associative memories in avionics systems
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
There are two main types of process scheduling algorithms commonly used in aircraft/spacecraft avionics systems. The first category consists of dynamic algorithms, which dynamically assign priorities to processes on the basis of runtime parameters. The second category consists of static algorithms, which statically determine priorities before runtime. The main disadvantage of applying dynamic process scheduling algorithms to avionics systems is the extra runtime overhead produced by these algorithms. This overhead is mainly related to the time required to sort active processes in the ready queue upon each process preemption or the arrival of each new process. The mentioned overhead encourages the use of static algorithms. But static algorithms have their own disadvantages. In fact, these algorithms bound the maximum available CPU utilization and have difficulties with non-periodic processes. This paper proposes and evaluates an approach to exploiting skewed associative memories in order to replace the time-consuming sorting operation by an efficient search operation. Both analytical models and simulation results show that the proposed approach can reduce the time complexity of the runtime overhead of dynamic scheduling algorithms (in terms of n the number of active processes) from O(nlogn) to O(n). This can considerably increase the performance of dynamic scheduling algorithms and make them much more feasible to be used in aircraft/spacecraft avionics systems.
[runtime overhead, skewed associative memories, Heuristic algorithms, simulation, avionics systems, Aerospace electronics, analytical models, processor scheduling, Space vehicles, Associative memory, Analytical models, spacecraft, Runtime, aerospace computing, search problems, process scheduling algorithms, dynamic scheduling algorithms, aircraft, time complexity, Dynamic scheduling, avionics, Scheduling algorithm, Sorting, search operation, Aircraft, content-addressable storage, computational complexity]
The analysis and optimization of collective communications on a Beowulf cluster
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
This paper gives a performance analysis of the all-gather, all-reduce and reduce-scatter collective communication operations on a Beowulf cluster. This cluster has a contention-free switch-based network with multiple network interface cards per node, permitting overlapping of message transmission under certain circumstances. As well as considering traditional algorithms developed previously for parallel computers with vendor-specific networks, we also examine simpler algorithms made up of repeated sub-operations, such as broadcasts. We find that for the kind of network on the Beowulf cluster, a somewhat different performance modelling of the algorithms is required, and that some simple simulation tools had to be developed in order to fully understand some of the algorithms' performance. Our results indicate that the LAM MPI implementations for these operations may be significantly improved, and the algorithms with data exchange and potential contention perform well on the cluster. Furthermore, they indicate that algorithms permitting message overlap are slightly favoured, with a new and simple algorithm which modestly out-performs the best traditional algorithms in the case of Reduce-Scatter. With the exception that the degree of overlapping proved difficult to estimate, our performance models fitted closely with the results, and together with the simulation tools, permit a detailed understanding of the cluster's communication pattern performance.
[workstation clusters, application program interfaces, message transmission, data exchange, Beowulf cluster, simulation tools, Network interfaces, message overlap, all-gather collective communication operations, Concurrent computing, optimization, Clustering algorithms, network interface cards, Broadcasting, parallel computers, Computer networks, Performance analysis, Communication networks, Pattern analysis, contention-free switch-based network, performance modelling, message passing, all-reduce collective communication operations, performance evaluation, broadcasts, Communication switching, Computer science, LAM MPI implementations, reduce-scatter collective communication operations, communication pattern performance, repeated suboperations]
Holoparadigm: a multiparadigm model oriented to development of distributed systems
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
The multiparadigm approach integrates programming language paradigms. We have proposed the Holoparadigm (Holo) as a multiparadigm model oriented to the development of distributed systems. Holo uses a logic blackboard (called history) to implement a coordination mechanism. The programs are organized in levels using abstract entities called beings. First, we describe the principal concepts of the Holoparadigm. After, we propose the Distributed Holo (DHolo), a model to support the distributed execution of programs developed in Holo. DHolo is based on object mobility and blackboards. This distributed model can be fully implemented on the Java platform. Experiments were done using Voyager and Horb to implement mobility. Blackboards were implemented using Java and JavaSpaces.
[experiments, distributed processing, Holo, Voyager, History, DHolo, distributed systems development, Distributed Holo, Computer architecture, multiparadigm model, Parallel processing, Hardware, Libraries, programming language paradigms, Informatics, coordination mechanism, Java, object-oriented programming, Logic programming, Holoparadigm, Object oriented modeling, JavaSpaces, Computer languages, Horb, object mobility, blackboard architecture, logic blackboard]
A scalable broadcast algorithm for multiport meshes with minimum communication steps
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Many broadcast algorithms have been proposed for the mesh over the past decade. However, most of these algorithms do not exhibit good scalability properties as the network size increases. As a consequence, most existing broadcast algorithms cannot support real-world parallel applications that require large-scale system sizes due to their high computational demands. Motivated by these observations, this study proposes a new adaptive broadcast algorithm for the mesh. The unique feature of our algorithm is that it handles broadcast operations with a fixed number of message passing steps irrespective of the network size. Our algorithm is based on the coded path routing, which has been proposed in (Al-Dubai and Ould-Khaous, 2001). Results from extensive comparative analysis reveal that the proposed algorithm exhibits superior performance characteristics over those of the well-known Recursive Doubling and Extending Dominating Node algorithms.
[Algorithm design and analysis, Scalability, multiprocessor interconnection networks, Distributed computing, Extending Dominating Node algorithm, Delay, scalability, large-scale system, Concurrent computing, scalable broadcast algorithm, Broadcasting, Performance analysis, Large-scale systems, message passing, network routing, multiport meshes, coded path routing, performance evaluation, Routing, parallel applications, Partitioning algorithms, network size, performance, distributed algorithms, computational demands, Recursive Doubling]
Performance guarantee for cluster-based Internet services
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
As Web-based transactions become an essential element of everyday corporate and commerce activity, it becomes increasingly important for the performance of Web application services to be predictable and adequate even in the presence of wildly fluctuating input loads. In this work we propose a general implementation framework to provide quality of service (QoS) guarantee for cluster-based Web application services, such as e-commerce or directory services, that is largely independent of the Web application and the hardware/software platform used in the cluster. This paper describes the design, implementation, and evaluation of a Web request distribution system called Gage, which is able to guarantee a service subscriber a pre-defined number of generic Web requests serviced per second regardless of the total input loads at run time. Gage is one of the first, if not the first system that can support QoS guarantees which involves multiple system resources, i.e., CPU, disk, and network. The fully operational Gage prototype shows that the proposed architecture can indeed provide a guaranteed level of service for specific classes of Web accesses according to their QoS requirements in the presence of excessive input loads. In addition, empirical measurement on the Gage prototype demonstrates that the additional performance overhead associated with Gage's QoS guarantee support for Web service is merely 3.06%.
[workstation clusters, Quality of service, Web-based transactions, performance evaluation, quality of service, Web application services, Application software, Web request distribution system, Resource virtualization, Uniform resource locators, Computer science, cluster-based Internet services, Web and internet services, Prototypes, scheduling, performance guarantee, directory services, Gage, Hardware, e-commerce, Internet, Web server, Business, electronic commerce]
Cycle embedding in a faulty hypercube
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
The hypercube is one of the most versatile and efficient interconnection networks yet discovered for parallel computation. We show that a fault-free cycle of length of at least 2n - 2f can be embedded in an n dimensional hypercube with f faulty nodes, where n /spl ges/ 3 and 1 /spl les/ f /spl les/ 2n - 4. The best known results thus far can tolerate only n - 1 faulty nodes.
[parallel computation, Costs, Multiprocessor interconnection networks, Computational modeling, graph theory, hypercube networks, Electronic mail, interconnection networks, Distributed computing, Concurrent computing, Fault tolerance, Intelligent networks, faulty hypercube, n dimensional hypercube, Hypercubes, Computer networks, fault tolerant computing, fault-free cycle, cycle embedding, faulty nodes]
Analysis of system performance by changing the ring architecture on the dual ring CC-NUMA system
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Since NUMA architecture has to access remote memory, the interconnection network determines the performance of the CC-NUMA system. Bus, which has been used as a popular interconnection network, has many limits in a large-scale system because of the limited physical scalability and bandwidth. The dual ring interconnection network, composed of high-speed point-to-point links, is made to resolve the defects of the bus for the large-scale system. However, it also has a problem, in that the response latency is rapidly increased when many nodes are attached to the snooping based CC-NUMA system with the dual ring. In this paper, we propose a chordal ring architecture in order to overcome the problem of the dual ring on a snooping based CC-NUMA system, and design an efficient link controller adapted to this architecture. We will also analyze the effects of chordal ring architecture on the system performance and the response latency by using a probability-driven simulator.
[CC-NUMA system, distributed shared memory, Multiprocessor interconnection networks, Scalability, parallel architectures, multiprocessor interconnection networks, performance evaluation, Delay, Multiprocessing systems, System performance, interconnection network, NUMA architecture, Computer architecture, Bandwidth, distributed shared memory systems, system performance, Frequency, Performance analysis, Large-scale systems, probability-driven simulator, chordal ring architecture, response latency]
Tensor product modeling of fault tolerant multiprocessor architectures
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
This paper presents tensor product formulas for modeling fault tolerant architectures and their corresponding reconfiguration algorithms. In our approaches, a network topology is first described with simple tensor product formulas, and then, by adding a set of permutation matrices and applying the direct sum operation, the reconfigurable architecture can be completely represented. Research results demonstrate three important facts: (1) by providing clarity and simplicity in mathematical form, tensor product representation is well-suited for the modeling of fault tolerant architectures; (2) with the tensor product model, traditional centralized reconfiguration processes can be transformed, in straightforward steps, to execute in a distributed manner; (3) furthermore, with the tensor product algebraic manipulations, complex architectures can be systematically and recursively constructed from simple basic blocks.
[multiprocessing systems, tensor product algebraic manipulations, reconfiguration algorithms, direct sum operation, Multiprocessor interconnection networks, parallel architectures, fault tolerant multiprocessor architectures, centralized reconfiguration processes, network topology, permutation matrices, Fault tolerance, Tensile stress, Network topology, Fault tolerant systems, Computer architecture, Binary trees, Hypercubes, fault tolerant computing, Reconfigurable architectures, Mathematical model, tensor product modeling, mathematical form]
Fault-tolerant broadcasting in hypercubes via local safety information
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
This paper presents a method for fault-tolerant broadcasting in faulty hypercubes using a new metric called local safety. A new concept called broadcast subcube is introduced, according to which various techniques are proposed to improve performance of the broadcast algorithm. An unsafe hypercube can be split into a set of maximal safe subcubes. We show that if these maximal safe subcubes meet certain requirements given in the paper, broadcasting can still be carried out successfully and in some cases optimal broadcast is still possible. The sufficient condition for optimal broadcasting is also presented. Extensive simulation results are presented.
[optimal broadcasting, maximal safe subcubes, simulation, broadcast subcube, hypercube networks, faulty hypercubes, unsafe hypercube, fault-tolerant broadcasting, Fault tolerance, Fault tolerant systems, metric, Broadcasting, Hypercubes, fault tolerant computing, Safety, local safety]
An incremental network topology for contention-free and deadlock-free routing
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Wormhole switching has become the most widely used switching technique for multicomputers. However, the main drawback of wormhole switching is that blocked messages remain in the network, prohibiting other messages from using the occupied links and buffers. To address the deadlock problem without compromising communication latency and the incremental expansion capability that irregular networks can offer, we propose a simple topology called extended incremental triangular mesh (EITM) for switch-based networks. EITM is an extension of a previous ITM (incremental triangular mesh) topology with a more flexible structure. We also show that EITM is highly scalable, allows incremental expansion of systems, has guaranteed deadlock freedom, and can support contention-free multicast. First, we show that for an EITM, any shortest path routing method will not deadlock, therefore EITM networks are ideal for the escape paths in adaptive routing networks. Second, we show that it is possible to arrange the nodes of an EITM in a circular order so that two messages from independent parts of the circular order will not interfere with each other - this is extremely useful for implementing contention-free multicast and other collective communication operations. We also present the results on the relation between ITM/EITM, outer planar graphs and chordal graphs. We show that chordal graphs are strongly related to the freedom of deadlock for shortest path routing, and ITM in our previous paper is indeed maximum outer planar graph.
[contention-free multicast, shortest path routing method, network routing, switch-based networks, multiprocessor interconnection networks, collective communication, chordal graphs, maximum outer planar graph, incremental network topology, outer planar graphs, Routing, deadlock-free routing, contention-free routing, wormhole switching, blocked messages, communication latency, extended incremental triangular mesh, system recovery, incremental expansion capability, Network topology, multicomputers, irregular networks, System recovery, adaptive routing networks]
Safe and complete distributed garbage collection with the train algorithm
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
A new mechanism for achieving isolated train detection in the distributed train algorithm is presented. The train algorithm is a general strategy for garbage collection, and the task of isolated train detection is a portion of the strategy which allows complete collection. When the train algorithm is deployed in a distributed system, a distributed algorithm is required to solve isolated train detection. We analyse the distributed train algorithm in the context of its isolated train detection aspect. We determine how it can be construed within the paradigm underlying distributed termination detection algorithms. From this we form a technical framework which allows any algorithm from this class to be used, essentially unmodified, to solve the distributed isolated train detection problem. We demonstrate this by deriving a new mechanism from a distributed termination detection algorithm.
[Computer science, Algorithm design and analysis, storage management, isolated train detection, distributed train algorithm, Message passing, distributed algorithms, distributed garbage collection, distributed termination detection algorithms, Partitioning algorithms, Detection algorithms, Distributed algorithms]
Group mutual exclusion in tree networks
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
The group mutual exclusion (GME) problem deals with sharing a set of (m) mutually exclusive resources among all (n) processes of a network. Processes are allowed to be in a critical section simultaneously provided they request the same resource. We present three group mutual exclusion solutions for tree networks. All three solutions do not use process identifiers, and use bounded size messages. They achieve the best context-switch complexity, which is O(min (n, m)). The first solution uses a fixed root of the tree and uses 0 to O(n) messages per critical section entry. This solution supports an unbounded degree of concurrency, thus provides the maximum resource utilization. The second solution also uses a fixed root, but uses a reduced number of messages for the critical section entry. It generates an average of O(log n) messages per critical section entry and also allows an unbounded degree of concurrency. However, the concurrency may be limited in some parts of the network. We remove the restriction of using a fixed root in the third solution in addition to maintaining all other desirable properties of the second solution.
[tree networks, mutually exclusive resources, Access protocols, Quality of service, group mutual exclusion, Concurrent computing, Computer science, Intelligent networks, Web and internet services, distributed algorithms, context-switch complexity, concurrency control, Particle measurements, Resource management, Distributed algorithms, Web server, computational complexity]
Providing mobile LAN access capability for Bluetooth devices
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
In this paper, we design and implement a Bluetooth system with mobile LAN access capability. First, by adding a virtual network interface card (NIC) driver between IP and Bluetooth protocol stack, Bluetooth users can use their Bluetooth devices to access the LAN resources via the Bluetooth access points. Besides, to accommodate with the mobility characteristics in Bluetooth network, we also propose a new handoff scheme. Instead of passively waiting for a handoff indication by the expiration of link supervision timer, in this paper, we detect a handoff actively and promptly by measuring the RSSI (receive signal strength indication) value. In addition, our proposed handoff scheme eliminates the inquiry procedure and only requires paging procedure during a handoff. We have implemented a prototype of Bluetooth system with mobile LAN access capability by our proposed scheme. Experimented results show that the handoff duration is significant reduced than the normal scheme.
[Bluetooth, Ethernet networks, handoff scheme, mobile LAN access capability, Access protocols, Switches, IP, Network interfaces, Bluetooth access points, Wireless communication, network interfaces, Information science, link supervision timer, receive signal strength indication, transport protocols, Bluetooth devices, Bluetooth protocol stack, Communications technology, wireless LAN, Local area networks, Mobile computing, virtual network interface card driver, mobility characteristics]
A novel neighbourhood broadcasting algorithm on star graphs
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
The neighbourhood broadcasting problem is defined as sending a message of fixed size from the source node to all its neighbours in an interconnection network, where in one time unit, a node can send to or receive from one and only one of its neighbours a datum item of constant size. In other words, the neighbourhood broadcasting is to simulate a single step of an all-port model on a single-port model. On a star interconnection network, this problem has been recently studied by Fujita (1998) who showed (1) a lower bound of [log n] for an n-star; and (2) a neighbourhood broadcasting algorithm on an n-star that requires 1.5 /spl lceil/log(n - 2)/spl rceil/ + 3 steps. The algorithm is later improved by Mkwawa and Kouvatsos (2001) to require 1.33/spl lceil/log(n - 1)/spl rceil/ + O(1) steps. In this paper we present a novel and interesting O(log n) neighbourhood broadcasting algorithm on an n-star In view of the /spl Omega/ (log n) lower bound, our algorithm is also optimal. Although the actual number of steps required is 4 /spl lceil/log(n/2)/spl rceil/ + 1 (or 4 /spl lfloor/log(n/2)/spl rfloor/ + 1 + x, if 1 /spl les/ x = n mod 2/sup /spl lfloor/log n/spl rfloor// /spl les/ 3), our algorithm is easy to implement since routing for all nodes involved is uniform, and simpler conceptually. It uses the cycle structures of star graphs as well as the standard technique of recursive doubling.
[single-port model, Multiprocessor interconnection networks, network routing, cycle structures, star graphs, multiprocessor interconnection networks, Routing, communication complexity, neighbourhood broadcasting algorithm, message, Communication standards, Computer science, star interconnection network, Network topology, interconnection network, distributed algorithms, Broadcasting, all-port model, source node, recursive doubling]
Self-organization for cache servers in active networks
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
The caching technology, such as proxy servers, has been widely adopted to reduce the bandwidth and the response time for WWW service. In active networks, nodes can execute simple codes to process the passed-through packets and can cache the passed-through data packets like a small transparent cache server How to appropriately organize these distributed cache servers and to decide whether to cache the pass-through data or not are addressed in the paper Simulation results show that the proposed self-organizing scheme is capable of reducing the response time up to 19% and increasing the probability that users get the data before the acceptable response time by 7%.
[active networks, codes, Protocols, simulation, Telecommunication traffic, World Wide Web, cache storage, Delay, distributed cache servers, Network servers, Intelligent networks, WWW service, file servers, Bandwidth, proxy servers, cache servers, small transparent cache server, Workstations, Web server, bandwidth, passed-through data packets, Computer science, response time, Internet, self organizing scheme]
Efficient parallel algorithms for the r-dominating set and p-center problems on trees
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Let T=(V, E) be a tree with vertex set V and edge set E. Let n=|V|. Each e/spl isin/E has a non-negative length. In this paper, we first present an algorithm on the CREW PRAM for solving the V/V/r-dominating set problem on T, where r/spl ges/0 is a real number. The algorithm requires O(log/sup 2/ n) time using O(n log n) work. Applying this algorithm as a procedure for testing feasibility, the V/V/p-center problem on the CREW PRAM is solved in O(log/sup 2/ n) time using O(n log/sup 2/ n) work, where p/spl ges/1 is an integer. Previously, He and Yesha had proposed algorithms on the CREW PRAM for special cases of the V/V/r-dominating set and the V/V/p-center problems, in which r is an integer and the lengths of all edges are 1. Their V/V/r-dominating set algorithm requires O(log n log log n) time using O(n log n log log n) work; and their V/V/p-center algorithm requires O(log/sup 2/ n log log n) time using O(n log/sup 2/ n log log n) work. As compared with He and Yesha's results, ours are more general and more efficient from the aspect of work.
[parallel algorithms, vertex set, testing feasibility, edge set, CREW PRAM, concurrency theory, set theory, Parallel algorithms, computational complexity]
Time-optimal parallel algorithms for constructing optimal virtual cellular networks
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
In a cellular network, the base stations are not necessarily uniformly distributed, and their corresponding cell sizes are not necessarily the same. For example, a cell in a well-populated city cell is usually smaller than a cell in a rural area. To study a cellular network with non-uniform cell sizes, one approach is to use a virtual cellular network with a uniform cell size such that each virtual cell contains at most one base station. This paper has proposed parallel algorithms for meshes with multiple broadcasting to construct virtual mesh and honeycomb cellular networks for non-uniformly distributed base stations. The constructed virtual cellular networks are optimal in the sense that their corresponding uniform cell sizes reach the largest possible. The algorithms run in O(log n) time on a mesh with multiple broadcasting of size n/spl times/n to construct optimal virtual mesh and honeycomb cellular networks for n non-uniformly distributed base stations. Furthermore, those algorithms are time-optimal.
[nonuniformly distributed base stations, Base stations, parallel algorithms, Computational modeling, nonuniform cell sizes, Parallel algorithms, Computer science, cell sizes, virtual mesh networks, Land mobile radio cellular systems, optimal virtual cellular networks, time-optimal parallel algorithms, Computer architecture, Broadcasting, Cities and towns, time, meshes, computational complexity, honeycomb cellular networks]
A scalable core migration protocol for dynamic multicast tree
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
Over the year, researchers have proposed the core based tree (CBT) and protocol independent multicast (PIM) protocols to route multicast data on the Internet. Such protocols need to locate the core of a group to have efficient multicast routing. In this paper, we propose a scalable distributed protocol that can be used to move the core to a near-optimal location for a dynamic multicast tree, and allow the core to migrate efficiently when the multicast tree is expanded or shrunk. In our protocol, it does not require knowledge of the complete network topology, and information of overall members is distributed among local agents; the core only maintains information of agents of the group. Also, only the agents participate in core selection. Therefore, the proposed protocol reduces the runtime overhead and message complexity while doing core migration.
[dynamic multicast tree, runtime overhead, Shape, Multicast protocols, Data engineering, near-optimal location, communication complexity, scalable distributed protocol, Delay, core based tree protocols, Computer science, protocol independent multicast protocols, scalable core migration protocol, message complexity, Runtime, Network topology, routing protocols, multicast data routing, multicast communication, Cost function, Routing protocols, Internet, multicast routing]
Scalability and reliability in a distributed search engine
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
We have developed a distributed search engine, called cooperative search engine (CSE), in order to retrieve fresh information. In CSE, a local search engine located in each Web server makes an index of local pages. A meta search server integrates these local search engines in order to realize a global search engine. In such a way, the communication delay occurs at retrieval time. So, it is thought to be difficult to search quickly. However we have developed several speedup techniques in order to realize real time retrieval. In addition, the meta server is a single point of failure in CSE. So, we propose redundancy of meta search servers in order to increase availability of CSE. In this paper we describe scalability and reliability of CSE and their evaluations.
[search engines, Scalability, local search engine, reliability, local page indexing, Distributed computing, communication delay, scalability, real time retrieval, Network servers, file servers, Search engines, Metasearch, redundancy, Web server, Delay effects, Redundancy, distributed search engine, information retrieval, cooperative search engine, Information retrieval, Web pages, Internet, speedup techniques, meta search server]
Coordinating multi-agents using JavaSpaces
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
In recent years, multiagent systems have become a new attractive paradigm for developing Internet-based enterprise applications. In this paper, we investigate the agent coordination issue of multiagent systems. We explore the emerging JavaSpace technology for achieving coordination within a multi-level supply chain management environment. JavaSpace is a recent realization of the classic Linda model. The tuple space of the Linda model provides a convenient way for agent communication and coordination. The coordination protocol for the supply chain management system is presented. The protocol is designed based on JavaSpace, and is described using colored Petri nets. We argue that the emerging JavaSpaces technology provides a convenient, yet flexible approach to agent coordination in multiagent system environment.
[multiagent systems coordination, multi-level supply chain management environment, Java, JavaSpace, Multiagent systems, Supply chain management, Protocols, multi-agent systems, Scalability, Petri nets, coordination protocol, colored Petri nets, Space technology, tuple space, Mobile agents, supply chain management, agent communication, Linda model, Internet, Manufacturing, protocols, Internet-based enterprise applications, electronic commerce]
Editing any version at any time: a consistency maintenance mechanism in Internet-based collaborative environments
Ninth International Conference on Parallel and Distributed Systems, 2002. Proceedings.
None
2002
This paper investigates the multi-version approach to consistency maintenance in Internet-based real-time collaborative editing systems. It proposes a new multi-versioning scheme that is able to preserve individual users' intentions while guaranteeing convergent document states. The scheme has been implemented in Java in a prototype called POLO.
[Real time systems, document handling, Java, text editing, Collaborative software, consistency maintenance mechanism, Humans, user intentions, POLO, data integrity, Delay, convergent document states, Collaboration, Prototypes, real-time systems, groupware, Internet-based real-time collaborative editing systems, Collaborative work, Internet, Australia, multi-version approach]
Parallel implementation of WAP-tree mining algorithm
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
In this paper, we present parallel algorithms for Web log mining and the performance prediction model. The algorithm, based on WAP-tree, scans dataset only twice and avoids candidate generation process. We parallelized mining part of WAP tree. To balance the workload among processors, we developed a task scheduling strategy. A performance model of parallel Web mining algorithm is also developed to predict the performance of parallel implementation. This model shows that we can get linear speedup for a small number of processors, and a slow down of speedup as the number of processors increases. Using the performance model, we can also estimate the maximum speed up. We implemented the algorithm on a Pittsburg Super Computer Center Lemieux using up to 48 processors. Our benchmark results showed that the performance model correctly predicts the performance of the parallel implementation. We have achieved a good speedup as the size of the dataset is increased.
[Costs, Moon, data mining, parallel algorithm, Predictive models, linear processing speedup, Data mining, Parallel algorithms, processor scheduling, dataset scanning, workload balancing, Information analysis, resource allocation, Web mining, performance prediction model, protocols, parallel algorithms, Pittsburg Super Computer Center Lemieux, performance evaluation, candidate generation process, maximum speed up, Processor scheduling, Web log mining, WAP-tree mining, parallel Web mining, Explosives, Internet, task scheduling, Web sites]
Causally ordered delivery for a hierarchical group
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Large number of peer processes are cooperating in peer-to-peer systems. In this paper, we discuss a hierarchical group protocol aiming at reducing communication and computation overheads for a scalable group of peer processes. A hierarchical group is composed of subgroups each of which is furthermore composed of subgroups. Even if messages are causally ordered in one subgroup, the messages may not be required to be causally ordered in a whole group. We discuss how to globally causally order messages by ordering mechanisms in each subgroup.
[Protocols, peer-to-peer computing, Peer to peer computing, peer process, LAN interconnection, Distributed computing, Mercury (metals), Delay, causally ordered delivery, Broadcasting, peer-to-peer system, Systems engineering and theory, Grid computing, hierarchical group protocol, protocols, Clocks]
Group rekeying with a customer perspective
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
In secure multiparty communications, several solutions have been proposed to deal with group rekeying. However, most of existing solutions including the most efficient ones still are severely lacking with respect to reliability and real customer expectations. Since in these solutions, each rekeying operation requires the update of the keying material of all members alike, frequent rekeying caused by volatile members would strongly affect long-lived members. We thus propose to restructure the logical key hierarchy (LKH) scheme, by separately regrouping members based on their membership duration aiming at preserving members with long duration membership from the impact of rekeying operations caused by arrivals or departures of short-lived members. We designed a hybrid reliability scheme based on a combination of ARQ and FEC that assures a quasi certain delivery of keying material to long-lived members. We then come up with an extensive method to determine the system parameters applicable to each member set based on the target customer satisfaction criteria.
[telecommunication security, hybrid reliability scheme, Scalability, volatile members, system parameters, forward error correction, automatic repeat request, secure multiparty communications, short-lived member, Aerospace industry, customer perspective, customer satisfaction, public key cryptography, Customer satisfaction, logical key hierarchy scheme, Materials reliability, customer satisfaction criteria, Forward error correction, long-lived members, Cryptography, customer expectation, Automatic repeat request, group rekeying]
Hash-based placement and processing for efficient node partitioned intensive databases
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
This paper discusses efficient hash-partitioning using workload access patterns to place and process relations in a cluster or distributed query-intensive database environment. In such an environment, there is usually more than one partitioning alternative for each relation. We discuss a method and algorithm to determine the hash partitioning attributes and placement. Among the alternatives, our algorithm chooses a placement that reduces repartitioning overheads using expected or historical query workloads. The paper includes a simulation study showing how our strategy outperforms ad-hoc placement and previously proposed distributed database strategies.
[hash-based placement, distributed database, ad-hoc placement, node-partitioned database, process relation, hash-partitioning, workload access pattern, query processing, repartitioning overhead, Databases, intensive databases, data placement, distributed databases, query-intensive environment, query workload, file organisation]
Evaluation of a pre-reckoning algorithm for distributed virtual environments
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
The recently introduced pre-reckoning algorithm is an alternative to the traditional dead reckoning algorithm used in DIS-compliant distributed virtual environments. Before the algorithm can be applied to distributed virtual environments, a detailed evaluation is needed. The pre-reckoning algorithm and the general dead reckoning algorithm are implemented within a simple distributed virtual environment system - CUBE. A comparison between the two algorithms is provided and the evaluation of performance and accuracy of two algorithms is presented. The results indicate that the pre-reckoning algorithm achieves a much more accurate model of the actual trajectory of an entity controlled by a remote host than the general dead reckoning algorithm. Moreover, the pre-reckoning algorithm improves accuracy while generating fewer state update packets. Based on the demonstrated performance, the pre-reckoning has a potential to improve scalability of distributed virtual environments and enhance consistency of users' view of the dynamic shared state.
[prereckoning algorithm, Dead reckoning, virtual reality, Virtual environment, Scalability, Telecommunication traffic, Predictive models, distributed processing, performance evaluation, Throughput, CUBE, distributed virtual environment, Convergence, Computer science, dead reckoning algorithm, Prediction algorithms, Error correction, protocols]
A quorum-based extended group mutual exclusion algorithm without unnecessary blocking
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
This paper presents a quorum-based distributed algorithm for the extended group mutual exclusion problem. In the group mutual exclusion problem, multiple processes can enter a critical section simultaneously if they belong to the same group. Processes in different groups cannot enter a critical section at the same time. In the extended group mutual exclusion, each process is a member of multiple groups at the same time. Each process can select which group it belongs at making a request. The algorithm for the group mutual exclusion cannot be applied for this extended problem, since there can be a case that two processes are prevented from entering a critical section simultaneously even when they are capable of doing so. We call the above situation unnecessary blocking. We present a quorum-based algorithm that prevents unnecessary blocking and show its correctness proof.
[program verification, Laboratories, Printers, correctness proof, Telegraphy, Iris, Fault tolerance, multiple processes, distributed algorithms, concurrency control, Communication channels, Telephony, extended group mutual exclusion, unnecessary blocking, quorum-based algorithm, quorum-based distributed algorithm, Distributed algorithms, computational complexity]
Load-balanced anycast routing
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
For fault-tolerance and load-balance purposes, many modern Internet applications may require that a group of replicated servers dispersed widely over the world. The anycast as a new communication style defined in IPv6 provides the capability to route packets to the nearest server. Better quality of service (QoS) can be achieved by this kind of computing paradigm. DNS, Web service, and distributed database system are three most well known examples. However, before anycasting can be realized, more researches need to be done. The anycast routing scheme is one of the most important issues. In this paper, we propose a load-balanced anycast routing scheme based on the WRS (weighted random selection) method. We suggest that the server capability should be propagated along with other fields in the routing tables. An anycast routing algorithm should take into account the network transmission capability as well as the server processing capability for the selection of a target server. Three weight determination strategies are given. We also develop a simple algorithm to calculate the weights of WRS to achieve optimization under both the heavy and the light system traffic environment. Our approach is locally optimized to minimize the average total delay and well balanced for the server load.
[load balancing, replicated servers, IPv6, Quality of service, Telecommunication traffic, Web service, DNS, Delay, Network servers, Fault tolerance, optimisation, resource allocation, QoS, optimization, weighted random selection, Database systems, Web server, packet routing, quality-of-service, distributed database, fault tolerance, anycast routing, Routing, quality of service, Web services, multicast protocols, routing protocols, delays, Internet]
On providing anonymity in wireless sensor networks
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Securing wireless sensor networks against denial of service attacks that disrupt communications or target nodes serving key roles in the network, e.g. sinks or routers, is instrumental to network availability and performance. Particularly vulnerable to these attacks are the components of any communications or operation infrastructure in the network. In this paper, we address a class of wireless sensor networks where network protocols leverage a dynamic general-purpose virtual infrastructure; the core components of that infrastructure are a coordinate system, a cluster structure, and a routing structure. Since knowledge of this virtual infrastructure enables 'smart' cost-effective DOS attacks on the network, maintaining the anonymity of the virtual infrastructure is a primary security concern. The main contribution of this work is to propose an energy-efficient protocol for maintaining the anonymity of the network virtual infrastructure. Specifically, our solution defines schemes for randomizing communications such that the coordinate system, cluster structure, and routing structure remain invisible to an external observer of network traffic during the setup phase of the network.
[telecommunication security, Military computing, wireless sensor networks, Communication system security, Computer crime, DOS attacks, Sensor arrays, Wireless communication, Intelligent networks, network performance, routing structure, computer crime, Computer networks, protocols, coordinate system, network availability, network security, computer networks, network protocols, Computer science, Wireless sensor networks, Information security, cluster structure, network virtual infrastructure, denial-of-service attacks]
Efficient secure multicast with well-populated multicast key trees
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Secure group communications is the basis for many recent multimedia and Web technologies. In order to maintain secure and efficient communications within a dynamic group, it is essential that the generation and management of group key(s) be secure and efficient with realtime response. Typically, a logical key hierarchy is used for distribution of group keys to users so that whenever users leave or join the group, new keys are generated and distributed using the key hierarchy. In this paper, we propose well-populated multicast key tree (WPMKT), an efficient technique to handle group dynamics in the key tree and maintain the tree balanced with minimal cost. In WPKT, subtrees are swapped in a way that keeps the key tree balanced and well populated. A t the same time, rekeying overhead due to reorganization is kept at a minimum. Another advantage of WPKT is that rebalancing has no effect on the internal key structure of the swapped subtrees. Results from simulation studies show that under random user deletion, our approach achieves one order of magnitude in overhead less than existing approaches. Under clustered sequential user deletion, our approach achieves almost a linear growth with tree size under individual rebalancing. For periodic rebalancing, we achieved almost half the overhead introduced by other approaches.
[telecommunication security, Costs, group dynamics, simulation, clustered sequential user deletion, well-populated multicast key trees, Command and control systems, Technology management, public key cryptography, logical key hierarchy, Clustering algorithms, secure group communications, multicast communication, secure multicast, multimedia technology, Cryptography, swapped subtrees, rekeying overhead, group key management, key hierarchy distribution, Data security, key hierarchy generation, key tree balancing, trees (mathematics), Web technology, group key distribution, internal key structure, periodic rebalancing, Computer science, random user deletion, group key generation, Authentication]
Optimal parallel block access for range queries
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Allocation schemes for range queries have been widely used in parallel storage systems to allow fast access to multidimensional data. An allocation scheme distributes data blocks among several devices (e.g. disks) so that the number of parallel block accesses needed per query is minimized. Given a system of k disks, a query that accesses m blocks needs a number of parallel block accesses that is at least OPT = [m/k]. In 2000, Atallah and Prabhakar described an allocation scheme with a guaranteed worst-case performance of OPT + O(log k) parallel block accesses for two dimensions. In this paper, we prove that the scheme of Atallah and Prabhakar has, in fact, guaranteed worst-case performance within an additive constant deviation from OPT: within OPT + 3 parallel block accesses for two dimensions. Also, we identify the type of queries for which the worst-case performance of the scheme is OPT + 1 parallel block accesses.
[Visualization, parallel storage system, Multidimensional systems, Geographic Information Systems, Additives, Optimized production technology, optimal parallel block access, Relational databases, Color, range queries, Spatial databases, multidimensional data access, Visual databases, parallel processing, Computer science, query processing, optimisation, OPT, allocation scheme, worst-case performance, data blocks]
Flexible intrusion tolerant group membership protocol
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Intrusion-tolerant group membership protocols constitute an important part of intrusion-tolerant group communication systems. This protocol maintains a consistent system-wide view of correct group members in the presence of malicious failures. This paper presents a new intrusion-tolerant group membership protocol, which provides two unique features. First, it introduces a new membership state called a suspended membership state. This new state provides a good balance between the amount of time a malicious/compromised group member gets to launch attacks before being removed from the group and the increased vulnerability to denial-of-service attacks if a suspected member is removed too early from the group. Second, it introduces a clean, logical separation between the functionality of detecting malicious processes and removing malicious group members from the group. This logical separation aids in simplifying the group membership protocol design and efficiently detecting suspicious process behaviors.
[intrusion-tolerant group communication system, Protocols, wide area networks, Buildings, Computer crashes, Communication system security, Computer crime, Middleware, Computer science, Prototypes, Detectors, computer crime, Broadcasting, intrusion tolerant group membership protocol, protocols, denial-of-service attack]
QoS and dynamic systems workshop
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
false
[]
Exploiting network locality in a decentralized read-write peer-to-peer file system
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
We have developed a completely decentralized multiuser read-write peer-to-peer file system with good locality properties. In our system all data is contained in blocks stored using the Past distributed hash table (DHT), thus taking advantage of the fault tolerance and locality properties of Past and Pastry. We have also introduced a modification to the Past DHT which allows us to further increase performance when using a relaxed but nevertheless useful consistency model. Authentication and integrity are assured using standard cryptographic mechanisms. We have developed a prototype in order to evaluate the performance of our design. Our prototype is programmed in Java and uses the FreePastry open-source implementation of Past and Pastry. It allows applications to choose between two degrees of consistency. Preliminary results obtained through simulation suggest that our system is approximately twice as slow as NFS. In comparison, Ivy and Oceanstore are between two to three times slower than NFS.
[public domain software, network locality, simulation, Ivy, authoring systems, FreePastry open-source implementation, Intelligent networks, integrity, File systems, Fault tolerant systems, Prototypes, groupware, NFS, Large-scale systems, Cryptography, authentication, Java, decentralized peer-to-peer file system, peer-to-peer computing, fault tolerance, Peer to peer computing, Data security, read-write peer-to-peer file system, cryptography, data integrity, multiuser peer-to-peer file system, Past distributed hash table, Authentication, message authentication, Oceanstore, fault tolerant computing, consistency model, cryptographic mechanism]
A new look at egocentric algorithms
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
With respect to approximate agreement, some categories of voting algorithms have been developed. This research presents an approach to one family of algorithms called egocentric algorithms, and analyzes the conditions under which the approach performs better than the existing egocentric voting algorithms. In addition, the approach provides some insight into the voting process of such algorithms.
[Algorithm design and analysis, egocentric algorithms, synchronous communication system, Sensor systems and applications, distributed agreements, Synchronization, voting algorithms, Delay, Convergence, Computer science, approximate agreement, mean-subsequence-egocentric voting, distributed algorithms, Sampling methods, fault tolerant computing, Performance analysis, Electronic voting, Clocks]
Energy-efficient communication in multi-channel single-hop sensor networks
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Sensor networks are usually designed and optimized for a specific functionality, and the nature of tasks on different nodes and the pattern of communication between nodes is known at design time. We are investigating a design approach for sensor networks that involves the creation of robust and energy-efficient protocols for a library of general communication primitives. Such a library will hide low-level networking details and provide building blocks for the end user who can then focus on high level design of the application. In this paper, we consider single hop sensor networks with multiple data channels, and define and evaluate the performance of a simple and energy-efficient protocol for a general and powerful primitive called (N, p, k/sub 1/, k/sub 2/) routing. This primitive denotes the transfer of N packets among p nodes with each node transmitting at most k/sub 1/ packets and each node expecting to receive at most k/sub 2/ packets. One-to-all (broadcast), all-to-one (gather), many-to-many, etc., are special cases of (N, p, k/sub 1/, k/sub 2/) routing. We use a separate low-power control channel for coordination among nodes. Simulation results show that our protocol leads to very high utilization of the available data bandwidth, and is also energy balanced. Fault-tolerant extensions of the protocol are also discussed.
[Protocols, wireless sensor networks, fault-tolerant protocol, Routing, low-level networking, single hop sensor networks, energy-efficient communication, Intelligent sensors, Design optimization, Intelligent networks, Wireless sensor networks, telecommunication network routing, energy conservation, Energy efficiency, Libraries, Hardware, Robustness, fault tolerant computing, multichannel sensor networks, protocols, energy-efficient protocols]
LASH-TOR: a generic transition-oriented routing algorithm
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Cluster networks are seen as the future access networks for multimedia streaming, e-commerce, network storage, etc. For these applications, performance and high availability are particularly crucial. Regular topologies are preferred when performance is the primary concern. However, due to spatial constraints or fault-related issues, the network structure may become irregular, which makes more difficult to find deadlock-free minimal paths. Over the recent years, several solutions have been proposed. One of them is the LASH routing, which enables minimal routing by assigning paths to different virtual layers. In this paper, we propose an extension of LASH in order to reduce the number of required virtual layers by allowing transitions between virtual layers. Evaluation results show that the new routing scheme (LASH-TOR) is able to obtain full minimal routing with a reduced number of virtual channels. For torus and mesh networks, with only two virtual channels, LASH throughput is increased by an average factor of improvement of 3.30 for large networks. For regular networks with some unconnected (faulty) links, equal performance improvements are achieved. Even for highly irregular networks of size up to 128 switches the new routing scheme only needs three virtual channels for guaranteeing minimal routing. Besides, LASH-TOR performs well compared to dimension order routing for mesh and torus networks.
[workstation clusters, network storage, Laboratories, packet switching, Switches, LASH-TOR, Electronic mail, spatial constraints, multimedia streaming, deadlock-free minimal paths, torus network, Network topology, cluster networks, mesh network, Clustering algorithms, e-commerce, network structure, Availability, virtual channels, Routing, LASH routing, Application software, telecommunication network routing, Streaming media, System recovery, access networks, transition-oriented routing algorithm]
Scalable duplication strategy with bounded availability of processors
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
In this paper we present a task selection scheme and a list-scheduling algorithm for scheduling DAGs onto homogeneous/heterogeneous systems with bounded availability of processors. Specifically, forbidden duplications and superfluous duplications are eliminated in the task selection phase and the processor selection phase respectively. Simulation results show that the proposed algorithm outperforms other high performance algorithms when the availability of processors is restrained.
[Availability, scalable duplication, list-scheduling, multiprocessing systems, homogeneous/heterogeneous systems, simulation, bounded processor availability, Scheduling algorithm, processor scheduling, Processor scheduling, directed graphs, processor selection phase, task selection, DAG scheduling]
Scaling all-to-all multicast on fat-tree networks
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
In this paper, we study the all-to-all multicast operation. Strategies for all-to-all multicast need to be different for small and large messages. For small messages, the major issue is the minimization of software overhead, where as for large messages, the issue is network contention. Many modern large parallel computers use the fat-tree interconnection topology. We therefore analyze network contention on fat-tree networks and develop strategies to optimize collective multicast using known contention free communication schedules on fat-tree networks in the design of two strategies. We evaluate performance of these strategies with up to 256 nodes (1024 processors) on an alpha cluster. We present schemes that perform well when a contiguous chunk of nodes is not available. For large messages, many of our strategies have two times better throughput than native MPI. We also demonstrate that the software overhead of a collective operation is a small fraction of the total completion time in the presence of the communication coprocessor. We therefore compare the performance of the studied strategies using both metrics (i) completion time, and (it) computation overhead.
[workstation clusters, Costs, MPI, network contention, Throughput, communication coprocessor, Design optimization, Concurrent computing, Quantum computing, Network topology, Bandwidth, multicast communication, parallel computers, message passing, contention free communication scheduling, computer networks, trees (mathematics), fat-tree interconnection topology, performance evaluation, telecommunication network topology, collective multicast, software overhead, Computer science, fat-tree networks, alpha cluster, Processor scheduling, all-to-all multicast, Coprocessors]
Coordinated en-route transcoding caching for tree networks
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
As transcoding caching is attracting an increasing amount of attention, it is important and necessary to find methods to distribute multiple versions of the same media object in the Internet. In this paper, we first present a mathematical model for the problem of optimally determining the locations in which to place multiple versions of the same media object in tree networks such that the specified objective is achieved. This problem is formulated as an optimization problem. Second, we propose a low-cost dynamic programming-based solution for solving this problem, by which the optimal locations are obtained. Finally, we evaluate our model on different performance metrics through extensive simulation experiments and compare the results of our model with those of existing models that consider transcoding caching either on a path or at individual nodes only.
[Measurement, Transport protocols, tree networks, trees (mathematics), Transcoding, dynamic programming, Routing, cache storage, transcoding caching, Network servers, Information science, optimization, mathematical model, Internet, Dynamic programming, Mathematical model, IP networks, transcoding]
A framework to support adaptation decisions for dynamic distributed systems
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
The increasing presence of mobile and ubiquitous applications has created a need for distributed systems and applications that are dynamic and can efficiently adapt to changes in quality of service (QoS), resource availability, user requirements and location of mobile nodes. This is done by dynamically reconfiguring system components. Reconfiguration should be done with minimal effect on system performance. A crucial issue is to determine what types of changes and how much change characterizes the need for reconfiguration, and what is the cost (in terms of resource usage) incurred on the system. We propose a reconfiguration framework based on policies and illustrate our initial prototype.
[Availability, adaptation decision, Costs, Quality of service, mobile application, distributed processing, distributed system, resource availability, quality of service, configuration management, mobile computing, System performance, QoS, mobile agent, optimization, system component reconfiguration, Prototypes, mobile agents, ubiquitous application, user requirement]
A time interval based consistency control algorithm for interactive groupware applications
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Traditional concurrency control methods such as locking and serialization are not suitable for distributed interactive applications that demand fast local response. Operational transformation (OT) is the standard solution to concurrency control and consistency maintenance in group editors, an important class of interactive groupware applications. It generally trades consistency for local responsiveness, because human users can often tolerate temporary inconsistencies but do not like their interactions be lost or nondeterministically blocked. This paper presents a time interval based operational transformation algorithm (TIBOT) that overcomes the various limitations of previous related work. Our approach guarantees content convergence and is significantly more simple and efficient than existing approaches. This is achieved in a pure replicated architecture by using a linear clock and by posing some constraints on communication that are reasonable for the application domain.
[operational transformation, Collaborative software, consistency control algorithm, convergence, Concurrency control, Application software, Distributed computing, Sun, Delay, consistency maintenance, Convergence, Computer science, TIBOT, concurrency control, groupware, interactive systems, interactive groupware application, Collaborative work, Computer networks, time interval]
Sporadic communication protocol between clusters of mobile computers
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Recently, mobile computers such as laptop, handheld and parmtop PCs have become to communicate with each other by using wireless LAN protocols, e.g., IEEE802.11 and HIPERLAN. Conventional routing protocols support a mobile network in which mobile computers change their location independently. In case that a mobile computer changes the location with high speed, less messages are exchanged between the mobile computers. In this paper, the authors propose a novel routing protocol for supporting mobile clustered networks in which mobile computers move with almost the same velocity and communicate with each other by multi-hop message transmission form a cluster. Here, communication between clusters is available if at least one mobile computer in the cluster is within the message transmission range of a mobile computer in the other cluster. That is, a communication protocol is required to support sporadic communication. For achieving higher bandwidth even though the clusters move with high speed, a set of gateway mobile computers are introduced in each cluster. We design protocols for switching gateways and for updating routing tables in the clusters according to the movement.
[workstation clusters, Wireless LAN, Portable computers, Wireless application protocol, multihop message transmission, Mobile communication, HIPERLAN, mobile computing, Handheld computers, routing protocols, Spread spectrum communication, Routing protocols, Computer networks, Personal communication networks, wireless LAN, routing tables, IEEE802.11, Mobile computing, mobile computer clusters, sporadic communication protocol]
Analysis of the execution of a next generation application on superscalar and grid processors
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
The astonishing advances in processing speeds and the phenomenal increase in chip densities have enabled the creation of very powerful microprocessors and computer systems. Future high end computing systems are expected to have teraflops of computing capability and massive amounts of storage. Such computers are expected to be important for discovery in the fundamental sciences, pharmaceuticals, and several other causes for the improvement of mankind. In this paper, we analyze a workload that is expected to be instrumental in designing and architecting future computing systems. The workload is a ground motion tracker indication (GMTI) application created by the scientists at the MIT Lincoln Laboratory. The application is being used to drive the design of several advanced future computer systems; hence it is important to understand the computational, memory access and parallelism features of this application. In this paper, we first describe the various components/stages of this application. Then, we perform detailed analysis of the execution of this application. On the basis of profiling the execution of the application, both on actual platforms and with simulations, we show that the parallelism in the several stages of the application is different. The application is seen to contain a large amount of parallelism that can be exploited by spatially or temporally parallel computer architectures. However, the nonuniformities in computing requirements as well as memory access patterns of the different stages are important considerations in the design of spatially/temporally parallel architectures to handle these applications. The execution of the application on a superscalar processor and a grid processor are analyzed.
[parallel computer architectures, Tracking, Instruments, program diagnostics, parallel architectures, memory access patterns, Laboratories, grid computing, simulation, Drives, microprocessor chips, Application software, ground motion tracker indication application, Concurrent computing, Microprocessors, grid processors, computer systems, Parallel processing, teraflops, Performance analysis, microprocessors, application execution profiling, Pharmaceuticals, superscalar processors]
Using structured P2P overlay networks to build content sensitive communities
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
This paper details a proposed peer-to-peer system, which allows a user to join communities of other like-minded users in order to exchange files. Utilising the routing capabilities of Pastry, the proposed system includes an indexing service, which facilitates the creation of virtual rendezvous points for users with similar interests (manifested by shared keywords). Users are described by the content they store. By using vector space modelling techniques users can be grouped together to form content sensitive communities. The system is built to serve as the basis for a distributed archive of research papers. The system is designed to improve the efficiency of file searches over current P2P file sharing applications. Search requests result in a comprehensive set of relevant documents being returned as searching are based on semantic meaning rather than literal matching.
[Scalability, Control systems, search requests, Network servers, shared keywords, Web server, distributed paper archive, peer-to-peer computing, Peer to peer computing, indexing, using vector space modelling, content storage, indexing service, information retrieval, Routing, file exchange, semantic meaning, Surges, file searches, electronic data interchange, structured P2P overlay networks, literal matching, peer-to-peer system, content sensitive communities, Internet, Web sites, Indexing, P2P file sharing applications]
Resource estimation and task scheduling for multithreaded reconfigurable architectures
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Reconfigurable computing is an emerging paradigm of research that offers cost-effective solutions for computationally intensive applications through hardware reuse. There is a growing need in this domain for techniques to exploit parallelism inherent in the target application and to schedule the parallelized application. This paper proposes a method to estimate the optimal number of resources through critical path analysis while keeping resource utilization near optimal. We also propose an algorithm to optimally schedule the parallel threads of execution in linear time. Our algorithm is based on the idea of enhanced partial critical path (ePCP) and handles memory latencies and reconfiguration overheads. Results obtained show the effectiveness of our approach over other critical path based methods.
[parallelized application, memory latency, resource estimation, Yarn, MPEG 4 Standard, Delay, processor scheduling, resource allocation, hardware reuse, reconfigurable architectures, computationally intensive applications, Parallel processing, Hardware, Reconfigurable architectures, resource utilization, parallel execution threads, Pervasive computing, critical path analysis, optimal scheduling, parallel algorithms, multi-threading, Scheduling algorithm, enhanced partial critical path, Processor scheduling, multithreaded reconfigurable architectures, Computer applications, reconfigurable computing, task scheduling, computational complexity, linear time]
Performance of work conserving schedulers and scheduling of some synchronous dataflow graphs
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
We know a lot about competitive or approximation ratios of scheduling algorithms. This, though, cannot be translated into direct bounds on the schedule produced by a scheduling algorithm, because often the optimal solution is intractable. We derive a methodology to find absolute bounds on the scheduling of jobs with precedence constraints on parallel identical machines. Our bounds hold for a large class of online and offline scheduling algorithms: the "work conserving" scheduling algorithms. We apply this methodology to prove that an important class of synchronous dataflow graphs $the parallelized pipelines -has very good performance characteristics when scheduled by a work conserving scheduler. Real time guarantees and granularity design for these dataflow graphs are discussed. We argue that parallelized pipelines should be dynamically scheduled on multiprocessor architectures.
[Algorithm design and analysis, Real time systems, parallel architectures, Pipelines, Optimal scheduling, data flow graphs, work conserving scheduler, real time system, processor scheduling, synchronous dataflow graph, competitive ratio, Program processors, Performance analysis, approximation ratio, parallelized pipeline, parallel system, online scheduling, competitive algorithms, Parallel machines, performance evaluation, Dynamic scheduling, Scheduling algorithm, parallel job scheduling, multiprocessor architecture, Processor scheduling, pipeline processing, performance analysis]
An application using pinwheel scheduling model
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Real-time systems, ranging from small portable devices or embedded systems to more complex general-purpose computers, are designed to satisfy various of real-time computation. It is the most important to guarantee all the requests are satisfied correctly in time. Rate monotonic algorithm is the most well-known real-time scheduling algorithm in the periodic task model. It not only has high schedulability, but also has a tight utilization bound to easily check whether the task set is feasible. In rate monotonic algorithm, a task is executed once in each period, but the start time of execution might be different in each period. On the contrary, for tasks scheduled using pinwheel model, the execution schedule in each period is always the same. Therefore, pinwheel scheduling model is very suitable for real-time systems which need high predictability. The pinwheel scheduling model transforms the task periods into harmonic numbers shorter than or equal to the original periods and provides many advantageous scheduling features. However, not all tasks allow to do the transformation and some would like to be executed according to their original periods. In this paper, we discuss how to solve the problem and give an application in power-aware real-time scheduling, where we can see the pinwheel model is very important in current embedded system design with power-aware requirement.
[Real time systems, Embedded computing, real-time computation, Portable computers, power-aware scheduling, general-purpose computers, task analysis, Scheduling algorithm, processor scheduling, Global Positioning System, Processor scheduling, Operating systems, portable devices, Embedded system, real-time systems, embedded systems, Hardware, pinwheel scheduling, rate monotonic algorithm, real-time scheduling, Kernel]
Divisible load scheduling on arbitrary distributed networks via virtual routing approach
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
In this paper, we propose a distributed algorithm for scheduling divisible loads originating from single site on arbitrary networks. We first propose a mathematical model and formulate the scheduling problem as an optimization problem with an objective to minimize the processing time of the loads. A number of theoretical results on the solution of the optimization problem are derived. On the basis of these results, we propose our algorithm using the concept of virtual routing. The proposed algorithm has three attractive features - distributed working style, simple structure, in terms of implementation ease, and offers generalized approach for handling divisible load scheduling for any network topology. This is the first time in the divisible load scheduling literature that a distributed strategy is attempted.
[optimization problem, computer networks, divisible load scheduling, Routing, arbitrary distributed networks, network topology, Distributed computing, Scheduling algorithm, processor scheduling, Concurrent computing, Mesh networks, distributed algorithm, optimisation, Processor scheduling, Network topology, resource allocation, distributed algorithms, telecommunication network routing, virtual routing, mathematical model, Computer networks, Mathematical model, Distributed algorithms]
Scalable cooperative latency estimation
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
This paper discusses SCoLE, a scalable system to estimate Internet latencies. SCoLE is based on GNP, which models Internet latencies in an n-dimensional Euclidean space. In contrast to GNP and other GNP-based systems, however, SCoLE does not employ any global space whose parameters must typically be negotiated by the participating hosts. Instead, it allows each host to construct its "'private" space and model interhost latencies in that space. The private space parameters as well as the modeling algorithm can be adjusted on a per-host basis, which improves system flexibility. More importantly, the mutual independence of private spaces results in higher SCoLE scalability, which is bound neither by the global negotiation of space parameters nor by global knowledge of any kind. We show that latency estimates performed in different private spaces are highly correlated. This allows SCoLE to be used in large-scale applications where consistent latency estimates need to be performed simultaneously by many independent hosts.
[Solid modeling, Force measurement, Economic indicators, Scalability, Peer to peer computing, SCoLE, n-dimensional Euclidean space, Internet latencies, Delay, GNP-based systems, private space parameters, Computer science, scalable cooperative latency estimation, Internet, Probes, Joining processes, modeling algorithm, computational complexity]
Performance analysis of location-based data consistency algorithms in mobile ad hoc networks
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Many applications of mobile ad hoc networks require real-time data consistency among the moving nodes within a geographical area of interest to function correctly, e.g., battlefield command and control applications. While it is operationally desirable to maintain data consistency among nodes within a large geographical area, the time required to propagate state changes to all mobile nodes in that geographical area limits its size. This paper investigates the notion of location-based data consistency in mobile ad hoc networks, and analyzes the tradeoff between data consistency and timeliness of data exchange among nodes within a location-based group in a geographical area of interest. By utilizing a Petri net performance model, we analyze performance characteristics of location-based data consistency maintenance algorithms and identify design conditions under which the system can best tradeoff consistency for timeliness (reflecting the time to propagate a state change) while satisfying the imposed data consistency requirement, when given a set of parameters characterizing the application in the underlying mobile ad hoc network environment.
[mobile ad hoc network, data exchange, geographical area, battlefield command and control application, Petri nets, Petri net performance model, location-based data consistency algorithm, real-time data consistency, Mobile ad hoc networks, Intelligent networks, electronic data interchange, mobile computing, command and control systems, Performance analysis, ad hoc networks, performance analysis]
A map-growing localization algorithm for ad-hoc wireless sensor networks
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Localization is an important process in deploying ad-hoc wireless sensor networks. Several localization algorithms have been developed. However, they do not achieve satisfactory performance on irregular networks. In this paper, we present a localization algorithm based on an idea of growing local maps. Simulation was performed on networks of different shapes with different connectivity and anchor. Compared with APS DV-distance (Niculescu and Nath, 2001), the map-growing algorithm is about two times more accurate for C-shaped grid or hexagon networks with the same coverage as of APS DV-distance, when the range error is less than or equal to 10%R, where R is the radio range. Compared with APS Euclidean (Niculescu and Nath, 2001), the coverage of map-growing is 100% with low GPS ratio while APS Euclidean requires a higher GPS ratio to get a better coverage.
[APS Euclidean, wireless sensor networks, grid computing, simulation, telecommunication network topology, sensor fusion, APS DV-distance, map-growing algorithm, radio access networks, C-shaped grid, Wireless sensor networks, ad-hoc wireless sensor networks, GPS ratio, map-growing localization, ad hoc networks, hexagon networks]
Load balancing of DNS-based distributed Web server systems with page caching
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
The explosion of WWW traffic has triggered great interest in distributed Web server systems. Among various distributed Web server architectures, the DNS-based distributed system is a promising solution in terms of performance, scalability and availability. DNS (domain name server) name caching has significant effects on the load balance of distributed Web server systems, due to the traffic skewness it causes. Also, there is page caching along whole paths between clients and Web servers, and page caching schemes could affect the system load balancing a lot. In this paper, we examine various caching issues, including the load balancing algorithms of the DNS, the locations of caches, and the page caching policies, for the DNS-based Web server system. We use stochastic processes to model the Web traffic, and compare the load balance performance of these algorithms and policies based on simulation results. We found that, the DNS load balancing algorithm, which takes into account both client domain information and server load information, could yield best load balance performance among various algorithms, and a server-side cache with the policy which only caches popular Web pages could significantly improve both the cache hit ratio and the load balance.
[load balancing, Scalability, Stochastic processes, World Wide Web, cache storage, distributed Web server, resource allocation, Traffic control, stochastic processes, Web server, Availability, client-server systems, server-side cache, Service oriented architecture, Explosions, page caching, domain name server, name caching, server load information, Web pages, client domain information, Load management, Internet, Web sites, Web traffic, telecommunication traffic]
Optimizing parallel multiplication operation for rectangular and transposed matrices
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
In many applications, matrix multiplication involves different shapes of matrices. The shape of the matrix can significantly impact the performance of matrix multiplication algorithm. This paper describes extensions of the SRUMMA parallel matrix multiplication algorithm (Krishnan and Nieplocha, 2004) to improve performance of transpose and rectangular matrices. Our approach relies on a set of hybrid algorithms which are chosen based on the shape of matrices and transpose operator involved. The algorithm exploits performance characteristics of clusters and shared memory systems: it differs from the other parallel matrix multiplication algorithms by the explicit use of shared memory and remote memory access (RMA) communication rather than message passing. The experimental results on clusters and shared memory systems demonstrate consistent performance advantages over pdgemm from the ScaLAPACK parallel linear algebra package.
[Algorithm design and analysis, workstation clusters, Costs, pdgemm, Scalability, Laboratories, transposed matrices, shared memory access communication, rectangular matrices, Access protocols, Mathematics, Distributed computing, parallel processing, SRUMMA parallel matrix multiplication, Concurrent computing, matrix multiplication, optimisation, Aggregates, ScaLAPACK parallel linear algebra package, remote memory access communication, Clustering algorithms, shared memory systems, cluster systems, hybrid algorithms]
On using reputations in ad hoc networks to counter malicious nodes
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Nodes in mobile ad hoc networks have a limited transmission range. Hence the nodes expect their neighbors to relay packets meant for far off destinations. These networks are based on the fundamental assumption that if a node promises to relay a packet, it relays it and does not cheat. This assumption becomes invalid when the nodes in the network have tangential or contradictory goals. The reputations of the nodes, based on their past history of relaying packets, can be used by their neighbors to ensure that the packet is relayed by the node. This paper introduces a reputation scheme for ad hoc networks. Instead of choosing the shortest path to the destination, the source node chooses a path whose next hop node has the highest reputation. This policy, when used recursively, in the presence of 40% malicious nodes, improves the throughput of the system to 65%, from 22 % throughput provided by AODV. This improvement is obtained at the cost of a higher number of route discoveries with a minimal increase in the average hop length according S. Bansal and M. Baker (2003).
[Costs, Throughput, Ad hoc networks, mobile networks, History, Relays, malicious nodes, Counting circuits, Mobile ad hoc networks, Computer science, packets relaying, Intelligent networks, mobile computing, security of data, telecommunication network routing, frame relay, Routing protocols, hop node, ad hoc networks, AODV]
Analysis of an energy efficient optimistic TMR scheme
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
For mission critical real-time applications, such as satellite and surveillance systems, a high level of reliability is desired as well as low energy consumption. In this paper, we propose a general system power model and explore the optimal speed setting to minimize system energy consumption for an optimistic TMR (OTMR) scheme. The performance of OTMR is compared with that of TMR (triple modular redundancy) and duplex with respect to energy and reliability. The results show that OTMR is always better than TMR by achieving higher levels of reliability and consuming less energy. With checkpoint overhead and recovery, duplex is not applicable when system load is high. However, duplex may be more energy efficient than OTMR depending on system static power and checkpointing overhead. Moreover, with one recovery section, duplex achieves comparable levels of reliability as that of OTMR.
[Real time systems, Checkpointing, checkpointing, Energy consumption, Mission critical systems, safety-critical software, system reliability, system recovery, power consumption, satellite system, surveillance system, aerospace computing, surveillance, TMR scheme, real-time system, checkpointing overhead, mission critical system, triple modular redundancy, Redundancy, Power system modeling, system energy consumption, OTMR scheme, Satellites, Surveillance, energy conservation, Energy efficiency, Power system reliability, checkpointing recovery]
A novel packet marking scheme for IP traceback
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Recently, several schemes have been proposed for IP traffic source identification for tracing denial of service (DoS) attacks. Most of these schemes require very large number of packets to conduct the traceback process, which results in lengthy and complicated procedure. In this paper, we address this issue by proposing a scheme, called probabilistic pipelined packet marking (PPPM), which employs the concept of "pipeline" for propagating marking information from one marking router to another so that it eventually reaches the destination. The key benefit of this pipeline process lies in drastically reducing the number of packets that is required for the traceback process. We evaluate the effectiveness of the proposed scheme for various performance metrics through combination of analytical and simulation studies. Our studies show that the proposed scheme offers high attack source detection percentage, and attack source localization distance of less than two hops under different attack scenarios.
[Measurement, telecommunication security, Laboratories, Pipelines, attack source localization distance, packet switching, simulation, Telecommunication traffic, digital simulation, IP traceback, Computer crime, Analytical models, Web and internet services, attack source detection, Computer networks, Performance analysis, marking router, Availability, IP traffic source identification, probability, performance evaluation, denial of service attacks, probabilistic pipelined packet marking, pipeline process, pipeline processing, Internet, telecommunication traffic]
State-on-demand execution for adaptive component-based mobile agent systems
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
The introduction of mobile code in the pervasive computing environment provides a good opportunity for research in ways to enhance execution flexibility. We note that current mobile code is too heavyweight and not adaptive enough to be used in pervasive computing where devices are resource-limited and heterogeneity is the norm. In this paper, we propose a new lightweight, component-based mobile agent system that can adapt to diverse devices and features resource saving as one of its aims. The system supports strong mobility of mobile code, which is a prerequisite for achieving system flexibility and good performance. The system discretize the transmission of code and execution states and relies on a scheme called state-on-demand (SOD) for the execution of the mobile code. We provide performance results to demonstrate the effectiveness of the SOD scheme.
[Pervasive computing, state-on-demand execution, object-oriented programming, pervasive computing environment, code transmission, Mobile handsets, adaptive mobile agent systems, Distributed computing, Information systems, Computer science, Home appliances, mobile computing, Wireless networks, Mobile agents, mobile agents, execution flexibility, mobile code, Personal digital assistants, component-based mobile agent systems, Mobile computing, distributed programming]
Caching support for push-pull data dissemination using data-snooping routers
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Internet applications such as the HTTP-Web, audio-video streaming and file sharing depend on wide-area data dissemination. Clients of these applications suffer from long delays due to network queuing, bandwidth limitations and adverse effects of bandwidth sharing between different traffic. Caching reduces delays and saves network bandwidth by holding the fetched data and responding to the subsequent requests locally. Existing distributed caching solutions are application-specific and do not support delivery in the push-pull directions at the same time. Our proposed architecture, called storage embedded networks, gives application-and direction-independent caching support by using memory-embedded, data-snooping routers. These router caches can act both as a client proxy and a server accelerator. We compare our architecture to the Web caches operating in forward proxy mode. We report additional reductions in client response times and server loads over proxies using the same cache sizes.
[audio-video streaming, client proxy, Telecommunication traffic, storage embedded networks, cache storage, Delay, Network servers, storage management, wide-area data dissemination, bandwidth sharing, Web and internet services, network modeling, embedded systems, Bandwidth, Traffic control, HTTP-Web, IP networks, Web server, data-snooping routing, file sharing, client-server systems, Peer to peer computing, information dissemination, memory-embedded routing, network bandwidth, GUOID, bandwidth limitations, Web caching, server accelerator, telecommunication network routing, delays, application-and-direction-independent caching, Web proxy, Streaming media, network queuing, push-pull directions, Internet, distributed caching, telecommunication traffic]
How to observe real-world events through a distributed world model
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
In this paper we propose an approach for observing real-world events through a distributed model providing dynamic data about the state of the world. Typical examples for real-world events are spatial events, i.e. events that occur when a user enters a certain spatial relationship with other users or his environment. To the best of our knowledge, this paper is the first to propose a uniform approach for observing real-world events that takes the issues related to the distribution of the model into account. The state of the world is only available with limited accuracy in both the value and time dimension, which is due to sensor inaccuracy and the properties of the distributed system. Therefore, it is not always possible to determine for certain, if an event has occurred. We propose to calculate the probability with which an event has occurred. The event is considered to have occurred when the calculated probability is above a threshold probability specified by the user. To realize this approach, we show which system parameters influence the observation, how update protocols provide the data to the observer model, on which the event is actually observed, and how the probability that an event has occurred can be calculated based on this model.
[Protocols, probability, distributed world model, Probability, distributed processing, distributed system, Ubiquitous computing, Sensor systems, Information systems, spatial event, protocols, real-world event, Context modeling]
Cegor: an adaptive distributed file system for heterogeneous network environments
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Distributed file systems have been extensively studied in the past, but they are still far from wide acceptance over heterogeneous network environments. Most traditional network file systems target the tight-couple highspeed networks only, and do not work well in the wide-area setting. Several communication optimization techniques are proposed in the context of wide-area file systems, but these approaches do not take into consideration the file characteristics and may instead introduce extra computing overhead when the network condition is good. We envision that the capability of providing adaptive, seamless file access to personal documents across diverse network connections plays an important role in the success of future distributed file systems. In this paper, we propose to build an adaptive distributed file system which provides the "close and go, open and resume" (Cegor) semantics across heterogeneous network connections, ranging from high-bandwidth local area network to low-bandwidth dial-up connection. Our approach relies on a set of new techniques for managing adaptive access to remote files, including three components: system support for secure, transparent reconnection at different places, semantic-view based caching to reduce communication frequencies in the system, and type-specific communication optimization to minimize the bandwidth requirement of synchronizations between clients and servers.
[Adaptive systems, distributed file system, File servers, Mobile communication, wide-area file systems, cache storage, local area networks, communication optimization, Communication system security, semantic-view based caching, optimisation, personal documents, File systems, remote files, heterogeneous network environments, network operating systems, network connections, Bandwidth, Cegor, Computer networks, Local area networks, close-and-go open-and-resume semantics, Context, client-server systems, adaptive file system, low-bandwidth dial-up connection, file access, clients-and-server synchronization, system support, highspeed networks, computer network management, network file systems, transparent reconnection, Frequency, high-bandwidth local area network, adaptive access, secure reconnection, communication frequencies]
An efficient distributed group key management algorithm
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
A key agreement protocol is an important part of a secure group communication system (SGCS) which provides secure message passing services to its members. Among the various distributed key agreement protocols proposed in the literature, the tree-based group Diffie-Hellman (TGDH) protocol is the most efficient in terms of the number of keys that need to be maintained at each member and distribution of DH exponentiation operations among group members. In TGDH, on a group change, the group members need to perform between one and O(log/sub 2/n) exponentiation operations serially. Also, the messages that are passed during group key agreement must be authenticated using digital signatures. In this paper, we propose a key agreement protocol which minimizes the number of exponentiation operations at each member. The member join operation requires only three members to perform one or two exponentiation operations each while the member leave operation requires only two or five group members to perform one or two exponentiation operations each. This is achieved at the cost of O(log/sub 2/n) causal messages per member leave operation.
[telecommunication security, tree-based group Diffie-Hellman protocol, message passing, trees (mathematics), Conference management, distributed key agreement protocols, digital signature, public key cryptography, distributed group key management, digital signatures, secure group communication system, protocols, DH exponentiation operations, secure message passing services, computational complexity, authentication]
Rate-based and queue-based dynamic load balancing algorithms in distributed systems
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
In this paper, we classify the dynamic, decentralized load balancing algorithms for heterogenous distributed computer systems into three policies: queue adjustment policy (QAP), rate adjustment policy (RAP) and queue and rate adjustment policy (QRAP). We propose two efficient algorithms, referred to as rate based load balancing via virtual routing (RLBVR) and queue based load balancing via virtual routing (QLBVR), which belong to the above RAP and QRAP policies, respectively. We also consider algorithms estimated load information scheduling algorithm (ELISA) that was introduced in the literature, to implement QAP policy. Our focus is to analyze and understand the behaviors of these algorithms in terms of their load balancing ability under varying load conditions (light, moderate, or high) and the minimization of mean response time of jobs. We compare the above classes of algorithms by a number of rigorous simulation experiments to elicit their behavior under some influencing parameters. From these experiments, recommendations are drawn to prescribe the suitability of the algorithms under various situations.
[Algorithm design and analysis, decentralized load balancing, Heuristic algorithms, queue adjustment policy, simulation, RAP policy, Distributed computing, Delay, processor scheduling, resource allocation, System performance, QRAP policy, distributed systems, queue-and-rate adjustment policy, estimated load information scheduling algorithm, Minimization methods, queueing theory, Computational modeling, queue-based dynamic load balancing, Routing, queue based load balancing via virtual routing, heterogenous distributed computer systems, rate-based dynamic load balancing, Scheduling algorithm, rate adjustment policy, telecommunication network routing, Load management, rate based load balancing via virtual routing]
Time-optimal network queue control: the case of multiple congested nodes
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
The fundamental problem of time-optimal queue control in packet-switched networks is how to adjust source rates in time after network disturbances so that the network queue sizes converge to desired values in the minimum time, while ensuring that always at least one link remains fully utilized in every flow's path. This nonlinear feedback control problem had been solved in a previous paper for a single queue in a single congested node and the solution proven robust to queue size and bandwidth estimation errors. In this paper, we generalize that solution to a general network of flows crisscrossing queues, with link delays being arbitrary. The solution, derived for desired queue sizes of 0, turns out to be simple: two computationally simple conditions on the source rate control, viz. QRE-feasibility and maximally utilizing property are sufficient to ensure time-optimality, regardless of the packet scheduling scheme used inside the network nodes.
[Computer aided software engineering, Protocols, telecommunication congestion control, packet switching, Switches, Control systems, Feedback control, network queue control, nonlinear feedback control, USA Councils, network disturbances, optimization, flows crisscrossing queues, scheduling, Computer networks, packet scheduling, Size control, queueing theory, congestion control, flow control, Scheduling algorithm, bandwidth estimation errors, time-optimal queue control, delays, Queueing analysis]
Consistency-preserving neighbor table optimization for P2P networks
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Constructing and maintaining consistent neighbor tables and optimizing neighbor tables to improve routing locality are two important issues in p2p networks. In this paper, we address the problem of preserving consistency while optimizing neighbor tables for p2p networks with node dynamics. We present a general strategy: identify a consistent subnet as large as possible and only replace a neighbor with a closer one if both of them belong to the subnet. We realize the general strategy in the context of hypercube routing. First, we present a join protocol that enables the identification of a large consistent subnet with very low cost when new nodes join. Next, we define an optimization rule to constrain neighbor replacements to preserve consistency, and present a set of optimization heuristics to optimize neighbor tables with low cost. The join protocol is then integrated with a failure recovery protocol. By evaluating the protocols through simulation experiments, we found our protocols and optimization heuristics to be effective, efficient, and scalable to a large number of network nodes.
[Protocols, node dynamics, simulation, hypercube networks, system recovery, consistency-preserving optimization, neighbor replacements, P2P networks, Design optimization, Constraint optimization, optimisation, optimization heuristics, Hypercubes, Cost function, hypercube routing, Computer networks, Large-scale systems, peer-to-peer computing, Peer to peer computing, Buildings, routing locality, network node, Routing, neighbor table optimization, p2p networks, failure recovery protocol, join protocol, routing protocols, consistent subnet]
Energy-aware, load balanced routing schemes for sensor networks
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
This paper presents two energy-aware, load balanced routing schemes, called as maximum capacity path (MCP) scheme and MCP with path switching (MCP-PS) scheme, for sensor networks. In the MCP scheme, the sensor network is constructed into a layered network at first. Based on the layered network, every sensor node selects a shortest path with maximum capacity to sink. In MCP-PS, the node may switch its routing path to its sibling neighbors in order to share the traffic. The simulation results show that our MCP and MCP-PS schemes can achieve a better load-sharing and better endurance on network lifetime.
[path switching, wireless sensor networks, computer networks, Switches, Telecommunication traffic, Routing, Capacitive sensors, Sensor systems, Batteries, load balanced routing, layered network, Relays, Wireless sensor networks, resource allocation, Space technology, telecommunication network routing, maximum capacity path, multipath routing, Space exploration, MCP-PS scheme, energy aware routing]
Efficient scheduling heuristics for GridRPC systems
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
In this paper, we examine several scheduling heuristics for GridRPC middleware relying on the time-shared model (a server can execute more than one task at a time). Our work is based on a forecast module called the 'historical trace manager' (HTM), which is able to predict durations of tasks in the system. We show that the predictions performed by the HTM are very accurate. The five proposed scheduling heuristics use these predictions to map submitted tasks to servers. Experimental simulation results show that they are able to outperform the well-known MCT heuristic for several metrics (makespan but also sumflow, max-stretch, etc.) and therefore provide a better quality of service for the client.
[Computational modeling, grid computing, GridRPC, Standardization, Quality of service, historical trace manager, quality of service, MCT heuristic, Middleware, Network servers, Processor scheduling, Metacomputing, time-shared model, Parallel processing, scheduling, Libraries, Problem-solving, quality-of-service, middleware]
BYPASS: topology-aware lookup overlay for DHT-based P2P file locating services
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
In this paper we propose a peer-to-peer file location system, BYPASS, to enhance the distributed file lookup time in DHT-based P2P systems. BYPASS constructs a secondary overlay in which file location information is distributed considering heterogeneity of peer capabilities and network proximity between requests and results. The secondary overlay in BYPASS allows only high-capability peers to participate in the system-wide file lookup and takes advantage of physical network proximity based on the autonomous system (AS) network topology. We evaluate effectiveness of BYPASS using a set of large-scale simulations. The results show that BYPASS can significantly reduce file lookup latency at the cost of additional yet acceptable storage and network resources.
[BYPASS, network proximity, file lookup latency, Space charge, system-wide file lookup, digital simulation, autonomous system network topology, DHT-based P2P file locating services, Delay, table lookup, Bandwidth, Large-scale systems, Cryptography, file location information, Availability, storage resources, peer-to-peer computing, Peer to peer computing, telecommunication network topology, large-scale simulation, Routing, Data structures, topology-aware lookup overlay, Computer science, DHT-based P2P systems, network resources, file organisation, distributed file lookup time, peer-to-peer file location system]
Performance improvement of congestion avoidance mechanism for TCP Vegas
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
In this paper, we propose a router-based congestion avoidance mechanism (RoVegas) for TCP Vegas. TCP Vegas detects network congestion in the early stage and successfully prevents periodic packet loss that usually occurs in TCP Reno. It has been demonstrated that TCP Vegas outperforms TCP Reno in many aspects. However, TCP Vegas suffers several problems that inhere in its congestion avoidance mechanism, these include issues of rerouting, persistent congestion, fairness, and network asymmetry. By performing the proposed scheme in routers along the round-trip path, RoVegas can solve the problems of rerouting and persistent congestion, enhance the fairness among the competitive connections, and improve the throughput when congestion occurs on the backward path. Through the results of both analysis and simulation, we demonstrate the effectiveness of RoVegas.
[Transport protocols, TCP Reno, telecommunication congestion control, round-trip path, network congestion detection, packet loss, Throughput, Delay, Computer science, Analytical models, transport protocols, telecommunication network routing, Bandwidth, TCP Vegas, Traffic control, RoVegas, Communication system traffic control, Internet, IP networks, network asymmetry, congestion avoidance mechanism]
Role ordering scheduler for concurrency control in distributed objects
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Concepts of roles are significant to design and implement access control models in secure information systems. A role shows a job Junction in an enterprise. In addition to keeping systems secure, objects have to be consistent in presence of multiple transactions. Traditional locking protocols and timestamp ordering schedulers are based on principles "first-comer-winner" and "timestamp order" to make multiple transactions serializable, respectively. Each transaction is associated with a role. We define significancy of role where shows how significant each role is in an enterprise. We discuss a novel type of scheduler for concurrency control so that multiple conflicting transactions are serializable in a significant order of roles of transactions.
[Access control, job junction, Access protocols, Relational databases, distributed processing, Concurrency control, Information systems, secure information system, Design engineering, Processor scheduling, distributed object, access control model, concurrency control, authorisation, Permission, System recovery, scheduling, Systems engineering and theory, locking protocol, information systems, role ordering scheduler, business data processing, timestamp ordering scheduler]
A performance-optimizing scheduling technique for mesh-connected multicomputers based on real-time job size distribution
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Among all non-blocking non-preemptive scheduling techniques for mesh-connected multicomputer system to ensure contiguous processor allocation, largest-job-first (LJF) technique proves to be one of the best in achieving small latency compared to others such as first-come-first-serve (FCFS) and smallest-job-first (SJF). We notice that LJF prevails when there is a significant variance in the number of processors requested among jobs. Instead, if the variance is relatively small, the merit of LJF is overshadowed by its inherent disadvantage in approaching bypass limit faster than the simpler FCFS one. Thus, the size-order used by the LJF for scheduling should be weighed less versus the arriving-order used by the FCFS whenever the variance is smaller. This paper proposes a novel scheduling technique to self-adjust in real-time between the two orders according to the size variance among the jobs in the queue. This technique ensures that the advantage of the LJF is preserved while significantly diminishing the chance for the blocking situation to occur. Our simulation results consistently show an significant improvement from our technique over the LJF and the FCFS ones.
[Real time systems, largest-job-first technique, mesh-connected multicomputers, real-time job size distribution, optimisation, resource allocation, first-come-first-serve technique, performance-optimizing scheduling, multiprocessor interconnection networks, processor allocation, processor scheduling]
Parallelization of Bayesian network based SNPs pattern analysis and performance characterization on SMP/HT
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Single nucleotide polymorphisms (SNPs) is subtle variation in a genomic DNA sequence of individuals of the same species. It plays a key role in the pharmaceutical industry to understand variations in drug treatment responses between individuals at the molecular level. Discovering patterns around SNPs loci is very important for better understanding the possible origin of SNPs in evolution. Bayesian network has been applied to this problem and got promising results. Since Bayesian network based SNPs pattern analysis demonstrates high computational complexity, we parallelized this workload on Intel Xeon SMP systems. SNPs' task level parallelism is exploited. Experiment results show that memory is bottleneck: on 8-way Xeon SMP hyper-threading enabled system, system memory bandwidth is fully saturated and memory load access latency is roughly 50% longer than on single processor system. Another interesting result is that Intel's hyper-threading technology helps improve the multithreaded workload's performance by 1.6X speedup. Workload profiling shows that parallel SNPs' data sharing nature matches hyper-threading's cache sharing mechanism, and thus greatly reduces cache coherency protocol traffic on shared front side bus. Scalability analysis shows that imbalance and locks are two major factors that may limit the parallel workload speedup on more processor platforms.
[Drugs, cache coherency protocol traffic, parallel workload, SMP/HT, pharmaceutical industry, task level parallelism, parallel SNP, ystem memory bandwidth, multithreaded workload performance, Genomics, data mining, cache storage, network parallelization, genomic DNA sequence, genetics, biology computing, hyper-threading cache sharing, workload profiling, SNP pattern analysis, Parallel processing, hyper-threading technology, belief networks, single nucleotide polymorphisms, Pattern analysis, Bioinformatics, Bayesian network, pattern recognition, Sequences, multi-threading, processor platform, memory load access latency, pattern discovery, Xeon SMP hyper-threading enabled system, Computational complexity, drug treatment, single processor system, shared front side bus, Bayesian methods, DNA, scalability analysis, Intel Xeon SMP systems, data sharing, Pharmaceuticals, computational complexity]
A framework for profiling multiprocessor memory performance
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Because of the increasing gap between processor frequency and dynamic random access memory (DRAM) speed, the performance of the memory subsystem typically governs that of the system as a whole. This is especially true for symmetric multiprocessor systems (SMPs). Therefore, performance evaluation methodologies that facilitate the analysis and optimization of the memory subsystem are essential. This paper, describes such a methodology, a performance evaluation framework, and demonstrates its power, speed, and flexibility in the context of a study of the TPC-C benchmark, executed on eight- and 32-processor IBM-pSeries 690 (p690) systems. The framework facilitates analysis of sampled performance monitor event traces that are collected in real time. The analyses are used to characterize the locality of reference exhibited by TPC-C data loads at the various levels of the memory hierarchy and evaluate the efficacy of design aspects of and policies associated with the p690 memory hierarchy w.r.t. workload demands.
[multiprocessing systems, Random access memory, Optimization methods, performance evaluation, Application software, Multiprocessing systems, Computer science, memory architecture, processor frequency, TPC-C benchmark, optimization, Frequency, DRAM chips, Hardware, Performance analysis, symmetric multiprocessor system, DRAM speed, Monitoring, multiprocessor memory performance, dynamic random access memory]
Peer-to-peer Web caching: hype or reality?
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
In this paper, we systematically examine the design space of peer-to-peer Web caching systems in three orthogonal dimensions: the caching algorithm, the document lookup algorithm, and the peer granularity. Based on the observation that the traditional URL-based caching algorithm suffers considerably from the fact of cacheability decrease caused by the fast growing of dynamic and personalized Web content, we propose to use the content-based caching algorithm. In addition to compare two existing document lookup algorithms, we propose a simple and effective geographic-based document lookup algorithm. Four different peer granularities, i.e., host level, organization level, building level, and centralized, are studied and evaluated using a seven-day Web trace collected at a medium-size education institution. Using a trace-driven simulation, we compared and evaluated all design choices in terms of two-performance metrics: hit ratio and latency reduction. Finally, several implications derived from the analysis are also discussed.
[Measurement, Algorithm design and analysis, content-based caching, Cooperative caching, peer-to-peer computing, Peer to peer computing, Buildings, cache storage, digital simulation, Web content, peer-to-peer Web caching, Delay, trace-driven simulation, Uniform resource locators, table lookup, Network servers, geographic-based document lookup, URL-based caching, Web trace, document lookup algorithm, peer granularity, Internet, design space]
Distributed algorithms for balanced zone partitioning in content-addressable networks
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
In this paper, we propose several distributed zone partitioning schemes for content-addressable networks (CAN), that is known as a pure peer-to-peer system based on the distributed hash table (DHT). The main objective of the proposed schemes is to balance the load of nodes in the CAN system, in such a way that every node receives almost the same number of inquiries from the other nodes in the system. The result of simulations implies that, by using the proposed schemes instead of a randomized scheme originally implemented in the CAN system, we could reduce the response time for each inquiry to less than 75%.
[randomized scheme, Content based retrieval, peer-to-peer computing, load balancing, Peer to peer computing, Scalability, Multiprocessor interconnection networks, balanced zone partitioning, multiprocessor interconnection networks, simulation, distributed zone partitioning, Distributed computing, Delay, Intelligent networks, Network servers, resource allocation, distributed algorithms, distributed hash table, content-addressable networks, peer-to-peer system, file organisation, Large-scale systems, Distributed algorithms]
Transient performance model for parallel and distributed systems
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
In studying or designing parallel and distributed systems one should have available a robust analytical model that includes the major parameters that determine the system performance. Jackson networks have been very successful in modeling parallel and distributed systems. However, Jackson networks have their limitations. In particular, the product-form solution of Jackson networks assumes steady state and exponential service centers with certain specialized queueing discipline. In this paper, we present a performance model that can be used to study the transient behavior of parallel and distributed systems with finite workload. When the number of tasks to be executed is large enough, the model approaches the product-form of Jackson networks (steady state solution). We show how to use the model to analyze the performance of parallel and distributed systems. We also use the model to show to what extent the product-form solution of Jackson networks can be used.
[queueing theory, transient performance model, parallel system, Jackson network, performance evaluation, distributed system, Steady-state, parallel processing, Computer science, Design engineering, Analytical models, queueing network, System performance, specialized queueing, performance modeling, product-form solution, system performance, transient analysis, Robustness, Performance analysis, steady state solution, Transient analysis, Queueing analysis]
Loosely-coupled, mobile replication of objects with transactions
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Advances in wireless technology and affordable info-appliances are making mobile computing a reality. However, programmers do have a real hard task while developing mobile distributed applications in which sharing is needed. Such difficulty arises, mainly because programmers are forced to deal with system level issues such as consistency, durability, availability, etc. We designed, implemented and evaluated an object-based platform called M-OBIWAN that releases the programmer from the above mentioned system-level issues. It supports mobile transactions and replication in an integrated way. In contrast with other approaches, M-OBIWAN provides an automatic replication mechanism allowing the creation of dynamic clusters of objects which are accessed within transactions. In addition, the transactional model is adapted to mobile environments. A prototype implementation has been developed. Its performance has been measured with PDAs and desktop machines, linked via Bluetooth.
[transaction processing, mobile transactions, Bluetooth, object-based platform, Power system economics, PDA, Environmental economics, Network servers, mobile computing, Prototypes, Personal digital assistants, middleware, Availability, mobile environments, object-oriented programming, Power generation economics, loosely-coupled replication, M-OBIWAN, mobile distributed applications, Programming profession, automatic replication mechanism, desktop machines, mobile replication, wireless technology, Mobile computing]
One-staged wormhole routing for irregular faulty patterns in meshes
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
A fault-tolerant wormhole routing algorithm in mesh multicomputer is proposed, which enables two good nodes to exchange their messages, except that there is no routing path between them, in one stage without aid of global information. In the method, faulty nodes are enclosed with faulty blocks. And, a message is routed to reach the destination or go out of the faulty block via multiple pre-established intermediate nodes if it is on the inside of a faulty block. A message not on the inside of a faulty block is routed toward the destination till it encounters the faulty block enclosing the destination, and is then routed toward the intermediate node which is able to instruct it to go into the faulty block. Routing algorithm RFB which permits the change of the routing direction is designed for the message not on the inside of a faulty block, which makes it possible that the message reaches the intermediate node in one stage. In our method, each channel is required to be split into two virtual ones.
[Algorithm design and analysis, fault-tolerant routing, message passing, Multiprocessor interconnection networks, multiprocessor interconnection networks, virtual channels, Glass, Routing, Control systems, routing direction, wormhole switching, Computer science, Fault tolerance, Region 2, wormhole routing, Councils, interconnection network, routing algorithm, telecommunication network routing, System recovery, fault tolerant computing, mesh multicomputer, faulty patterns]
Decentralized reactive clustering for collaborative processing in sensor networks
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
A sensor network forms a loosely-coupled distributed environment where collaborative processing among multiple sensor nodes is essential in order to compensate for the limitation of each sensor node in its processing capability, sensing capability, and energy usage, as well as to improve the degree of fault tolerance. Due to the sheer amount of nodes deployed, collaboration is usually carried out among nodes within the same cluster. Different clustering protocols can affect the performance of network to a great extent. Most existing clustering protocols either do not adequately address the energy-constraint problem or derive clusters proactively which may not be suitable for event-driven collaborative processing in sensor networks. This paper focuses on the design of clustering protocols for collaborative processing. We propose a decentralized reactive clustering (DRC) protocol where the clustering procedure is initiated only when events are detected. It uses power control technique to minimize energy usage in forming clusters. We compare the performance of DRC with another popular clustering algorithm, LEACH. Simulation results show considerable improvement over LEACH in energy conservation and network lifetime using DRC.
[workstation clusters, DRC protocol, decentralized reactive clustering, loosely-coupled distributed environment, wireless sensor networks, fault tolerance, sensing capability, simulation, multiple sensor nodes, processing capability, clustering protocols, International collaboration, Sensor systems, sensor networks, Intelligent networks, energy usage, derive clusters, energy conservation, power control, fault tolerant computing, event-driven collaborative processing, protocols, energy-constraint problem]
The impact of failure management on the stability of communication networks
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
In this work we deal with communication networks in which links may fail. We propose an adversarial model for describing the traffic pattern occurring in this type of faulty systems and study properties concerning their stability, especially under (nontrivial) underloaded worse-case scenarios. We show that, depending on how the system is organized and prepared to deal with failures, the dynamics of the system change and thus the conditions for stability. We propose three different ways of failure management and study how they influence on the stability of faulty communication networks under the adversarial model proposed. We show that some failure managements can provoke the instability of even very simple networks.
[Protocols, failure management, Stability, Telecommunication traffic, Control systems, computer network management, Disaster management, Traffic control, faulty communication networks, Performance loss, Communication system traffic control, computer network reliability, communication network stability, Communication networks, Computer network management, traffic pattern, faulty systems, telecommunication traffic]
A parallel algorithm based on search space partition for generating concepts
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Concept lattice, the core data structure in formal concept analysis, has been used widely in machine learning, data mining and knowledge discovery, information retrieval, etc. The main difficulty with concept lattice-based system comes from the lattice construction itself. In this paper, a parallel algorithm based on the closure search space partition for computing concepts is proposed. This algorithm divides the closure search space into several subspaces in accordance with criterions prescribed ahead and introduces an efficient scheme to recognize the valid ones, in which the searching for closures is bounded. An intermediate structure is employed to judge the validity of a subspace and compute closures more efficiently. Since the searching in subspaces are independent tasks, a parallel algorithm based on search space partition can be directly reached.
[parallel algorithms, Machine learning algorithms, core data structure, Lattices, parallel algorithm, Data structures, Information retrieval, lattice theory, concept generation, Partitioning algorithms, Data mining, Parallel algorithms, formal specification, formal concept analysis, Information analysis, Concurrent computing, concept lattice, Machine learning, data structures, lattice construction, closure search space partition, search problems]
Eliminating replica selection - using multiple replicas to accelerate data transfer on grids
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Data-intensive, high-performance computing applications often require the efficient transfer of terabytes or even petabytes of data in wide-area, distributed computing environments. To increase the efficiency of wide area data movement, researchers have devised various techniques such as TCP tuning, multiple streams and asynchronous I/O. This paper adopts an approach to increase performance further by exploiting replica-level parallelism in grids. rFTP, a grid data transferring tool, improves the data transfer rate and reliability on grids by utilizing multiple replica sources concurrently. Experiments on the NPACI grid show as much as a 2.02/spl times/ speedup over a single data source by adaptively retrieving partial data segments from 4 replicas using the data provided by NWS.
[wide area networks, Weather forecasting, grid computing, Throughput, TCP tuning, wide area data movement, data transfer reliability, Distributed computing, parallel processing, distributed computing environment, single data source, multiple streams, NPACI grid, Grid computing, partial data segment retrieval, replica selection, replica-level parallelism, asynchronous I/O, rFTP, grid data transferring tool, Information retrieval, Application software, replica sources, Leg, multiple replicas, Computer science, electronic data interchange, data transfer rate, Computer applications, wide-area computing environment, Acceleration]
A congestion-relieving mechanism for wormhole-routed networks with real-time injection control
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
A performance optimization technique is proposed for wormhole-routed networks. The technique employs a self-feedback-control process to adaptively adjust the packet injection into the system to achieve a real-time performance-optimizing effect. Not having a predetermined goal to reach, as in most feedback control problems, renders this problem almost intractable, especially under a situation where there is an input from a random process. We develop a comprehensive methodology for solving such an optimal-control problem. Several fundamental concepts are fully investigated and various performance measurement options are discussed for this specific issue. Though not yet completely fine-tuned, the proposed preliminary approach already shows a very promising result. We believe that such a real-time performance-optimizing technique can easily be employed in many other areas of applications that involve resource sharing in a random input environment.
[Real time systems, telecommunication congestion control, random processes, feedback control, performance evaluation, Control systems, optimal control, congestion-relieving mechanism, wormhole-routed networks, real-time injection control, optimisation, resource allocation, packet injection, optimal-control problem, telecommunication network routing, resource sharing, performance optimization, self-feedback-control process]
Scaling unstructured peer-to-peer networks with multi-tier capacity-aware overlay topologies
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
The peer to peer (P2P) file sharing systems such as Gnutella have been widely acknowledged as the fastest growing Internet applications ever. The P2P model has many potential advantages due to the design flexibility of overlay networks and the server-less management of cooperative sharing of information and resources. However, these systems suffer from the well-known performance mismatch between the randomly constructed overlay network topology and the underlying IP layer topology for packet routing. This paper proposes to structure the P2P overlay topology using a capacity-aware multi-tier topology to better balance load at peers with heterogeneous capacities and to prevent low capacity nodes from downgrading the performance of the system. To study the benefits and cost of the multi-tier capacity aware topology with respect to basic and advanced routing protocols, we also develop a probabilistic broadening scheme for efficient routing, which further utilizes capacity-awareness to enhance the P2P routing performance of the system. We evaluate our design through simulations. The results show that our multi-tier topologies alone can provide eight to ten times improvements in the messaging cost, two to three orders of magnitude improvement in terms of load balancing characteristics, and seven to eight times lower topology construction and maintenance costs when compared to Gnutellas random power-law topology.
[Costs, multitier overlay topologies, load balancing, capacity-aware overlay topologies, simulation, P2P file sharing systems, Distributed computing, overlay network topology, IP layer topology, Network topology, resource allocation, random power-law topology, topology construction, Broadcasting, Gnutella, capacity-aware multitier topology, Routing protocols, Computer networks, unstructured peer-to-peer networks, packet routing, P2P routing performance, probabilistic broadening scheme, peer-to-peer computing, Peer to peer computing, probability, telecommunication network topology, P2P model, Handheld computers, transport protocols, routing protocols, Internet, Resource management, Internet application, P2P overlay topology]
Admission control algorithms integrated with pricing for revenue optimization with QoS guarantees in mobile wireless networks
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
We propose and analyze call admission control algorithms integrated with pricing for revenue optimization with QoS guarantee to serve multiple service classes in mobile wireless networks. Traditional admission control algorithms make acceptance decision for new and handoff calls to satisfy certain QoS constraints such as the dropping probability of handoff calls and the blocking probability of new calls being lower than a pre-specified threshold. We analyze a class of partitioning and threshold-based admission control algorithms that make acceptance/rejection decision not only to satisfy QoS requirements but also to optimize the revenue of the system, taking into account prices and arrival/departure information of service calls. We show that for a "charge-by-time" pricing scheme, there exist optimal resource allocation settings under which the partitioning and threshold-based admission control algorithm would produce the maximum revenue obtainable by the system without sacrificing QoS requirements. Further, the threshold-based admission control algorithm outperforms the partitioning-based counterpart at optimizing settings over a wide range of input parameters characterizing the operating environment and service workload conditions. Methods for utilizing of the analysis results for real-time admission control for revenue optimization with QoS guarantee are described with numerical data given to demonstrate the applicability.
[Algorithm design and analysis, blocking probability, Telecommunication traffic, handoff call probability, threshold-based admission control, mobile wireless network, multiple service class, Intelligent networks, mobile computing, optimisation, resource allocation, Wireless networks, Pricing, Traffic control, admission control algorithm, Multimedia systems, probability, revenue optimization, Partitioning algorithms, quality of service, QoS guarantee, Admission control, service workload, optimal resource allocation, Mobile computing, pricing]
Wireless support for telemedicine in disaster management
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Disaster response and recovery require timely interaction and coordination of public emergency services in order to save lives and property. An important role in this effort must be played by wireless telemedicine whose mandate is to bring to the scene of the disaster the experience and expertise of medical personnel that can direct and supervise paramedics in providing necessary life-support services. The main contribution of this work is to discuss a novel wireless architecture in support of a recently-proposed telemedicine architecture called WIRM incorporating leading-edge image compression technology, a robust interactive visualization tool, and a high-performance wireless multimedia network.
[telemedicine, data compression, disaster management, WIRM, disasters, image compression, life-support services, surface reconstruction, disaster recovery, public emergency services, wireless architecture, radio access networks, wireless telemedicine, Telemedicine, wireless multimedia network, data visualisation, Disaster management, disaster response, interactive systems, interactive visualization, emergency services, image coding, multimedia communication]
An adaptive scheme for vertical handoff in wireless overlay networks
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Vertical handoff is the switching process between heterogeneous wireless networks. Discovering the reachable wireless networks is the first step for vertical handoff. After discovering the reachable candidate networks, the mobile terminal decides whether to perform handoff or not. We present an adaptive scheme for vertical handoff in wireless overlay networks. Our system discovery method effectively discovers the candidate networks for the mobile terminal. Moreover, we propose two adaptive evaluation methods for the mobile terminal to determine the handoff time that relies on the candidates' resources and the running applications. The simulation results show that the proposed system discovery method can balance the power consumption and the system discovery time. Furthermore, the proposed handoff decision method can decide the appropriate time to perform handoff.
[Base stations, Energy consumption, Stability, wireless sensor networks, handoff decision method, heterogeneous wireless network, Mobile communication, mobile terminal, power consumption, telecommunication switching, Computer science, Intelligent networks, mobile computing, Wireless networks, Councils, switching process, Bandwidth, vertical handoff, Computer science education, system discovery method, wireless overlay network]
Adaptive cache-driven request distribution in clustered EJB systems
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
This paper presents an algorithm for request distribution in clustered EJB systems. A classification for EJB requests is first introduced, on which request distribution is based. The objective is to achieve load balancing and to enhance the caching performance in the EJB containers. The algorithm is based on periodically collecting traffic statistics from the EJB containers, and then solving a constrained optimization problem that finds the best distribution strategies for each request type. An event-driven simulator is built for the performance evaluation, where the TPC-W benchmark is used as the workload. Simulation results show that for relatively small sizes of the containers' caches, the algorithm outperforms the random and round robin algorithms, currently used in clustered EJB systems. In addition, it scales better with the cluster size and can adapt to the varying load patterns.
[workstation clusters, Adaptive systems, load balancing, Containers, cache storage, random algorithm, adaptive request distribution, round robin algorithm, optimisation, Databases, resource allocation, traffic statistics, container caches, Clustering algorithms, Logic, Round robin, Web server, discrete event simulation, constrained optimization problem, event-driven simulator, distributed object management, Java, TPC-W benchmark, EJB containers, performance evaluation, Computer science, clustered EJB systems, Load management, Internet, benchmark testing, cache-driven request distribution]
The net worth of an object-oriented pattern: practical implications of Java RMI
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Debugging distributed applications in their run-time environment is notoriously hard and development and testing of application logic must be completed ahead of this step. Using Java RMI allows a developer to separate the two stages (development of the application logic from the deployment of the application in its distributed run-time environment) but the developer must acknowledge a specific pattern from the outset. We present this pattern below: it allows for a stage of fully carried out development of the application in an isolated run-time environment (no network) and makes the switch to a true networked run-time environment completely transparent.
[Java, Runtime environment, program debugging, object-oriented pattern, application debugging, object-oriented programming, distributed run-time environment, Java RMI, Switches, Debugging, Logic design, Application software, distributed application, Logic testing, Computer science, Network servers, application logic, Sockets, remote method invocation, remote procedure calls]
A probabilistic approach to fault-tolerant routing algorithm on mesh networks
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
In this paper, we propose a novel and efficient fault tolerant routing algorithm for mesh networks based on the concept of k-submesh without virtual channels and not sacrifice non-faulty nodes. Our algorithm is distributed and local information based. Due to the fact that our algorithm is designed based on k-submesh structure, we apply probabilistic analysis on the fault tolerance of our routing algorithm. Suppose that each node has an independent failure probability, we derive the probability that our routing algorithm successfully returns a fault-free routing path. For example, we formally prove that as long as the node failure probability is bounded by 0.35%, our routing algorithm succeeds in finding a fault-free routing path with high probability of 99% on mesh networks with 250000 nodes. Our algorithm runs in liner time and simulation results show that the length of the routing path constructed by our algorithm are very close to the optimal length.
[Algorithm design and analysis, fault-tolerant routing, fault tolerance, Computational modeling, Computer simulation, probability, virtual channels, Routing, probabilistic analysis, fault-free routing path, Computer science, Fault tolerance, Mesh networks, mesh networks, Network topology, Computer network reliability, distributed algorithms, routing algorithm, telecommunication network routing, Computer networks, fault tolerant computing, failure probability]
Routing permutations on optical baseline networks with node-disjoint paths
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Permutation is a frequently-used communication pattern in parallel and distributed computing systems and telecommunication networks. Node-disjoint routing has important applications in guided wave optical interconnects where the optical "crosstalk" between messages passing the same switch should be avoided. In this paper, we consider routing arbitrary permutations on an optical baseline network (or reverse baseline network) with node-disjoint paths. We first prove the equivalence between the set of admissible permutations (or semi-permutations) of a baseline network and that of its reverse network based on a step-by-step permutation routing. We then show that an arbitrary permutation can be realized in a baseline network (or a reverse baseline network) with node-disjoint paths in four passes, which beats the existing results (M. Vaez and C.-T. Lea, 2000 and G. Maier and A. Pattavina, 2001) that a permutation can be realized in an n /spl times/ n banyan network with node-disjoint paths in O(n/sup 1/2/) passes. This represents the currently best-known result for the number of passes required for routing an arbitrary permutation with node-disjoint paths in unique-path multistage networks. Unlike other unique path MINs (such as omega networks or banyan networks), only baseline networks have been found to possess such four-pass routing property. We present routing algorithms in both self-routing style and central-controlled style. Different from the recent work which also gave a four-pass node-disjoint routing algorithm for permutations, the new algorithm is efficient in transmission time for messages of any length, while the algorithm can work efficiently only for long messages. Comparisons with previous results demonstrate that routing in a baseline network proposed in this paper could be a better choice for routing permutations due to its lowest hardware cost and near-optimal transmission time.
[Optical fiber networks, central-controlled style, Distributed computing, self-routing style, unique-path multistage networks, telecommunication networks, parallel computing systems, reverse baseline network, Hardware, guided wave optical interconnects, optical communication, optical crosstalk, omega networks, Optical interconnections, message passing, permutation routing, four-pass node-disjoint routing, Optical switches, Telecommunication switching, Routing, optical waveguides, multistage interconnection networks, Communication switching, distributed computing systems, routing property, Message passing, optical interconnections, node-disjoint paths, telecommunication network routing, optical baseline networks, Optical crosstalk, communication pattern, banyan network, computational complexity]
An efficient parallel collision detection algorithm for virtual prototype environments
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
The automatic recognition of geometric constraints in virtual assembly and maintenance operations relies in the determination of intersecting surfaces between virtual prototypes. This is a key challenge in many virtual prototype applications, where it is necessary to find collisions precisely and interactively. This paper presents an algorithm to determine intersecting surfaces at interactive speed in a virtual prototyping environment. The proposed algorithm is based on the overlapping axis-aligned bounding box (OAABB). The OAABB concept is used effectively to eliminate the number of surfaces that cannot intersect and improve performance. The algorithm also facilitates the implementation using parallel computing methods. OpenMP is used, taking advantage of shared memory multiple processors and reducing the overall time complexity of the collision detection algorithm. To achieve an efficient parallel simulation, it is necessary to provide an efficient load balancing scheme. Our experiences in parallelising the code to achieve a better work distribution are also described. Results show that the proposed collision detection achieves interactive rates in real industrial applications as desired.
[collision avoidance, Design automation, parallelising compilers, open systems, load balancing, production engineering computing, Conference management, computational geometry, virtual prototyping, digital simulation, virtual prototyping environment, parallel programming, parallel collision detection, geometric constraints, Filters, resource allocation, Parallel processing, parallel simulation, shared memory systems, virtual prototype applications, intersecting surfaces, Virtual prototyping, parallel application, parallel computing, Assembly, overlapping axis-aligned bounding box, Software prototyping, parallel algorithms, Virtual environment, code parallelisation, Computational modeling, OpenMP, virtual assembly, CAD, real industrial applications, shared memory multiple processors, work distribution, overall time complexity, Detection algorithms, automatic recognition]
Enhancing locality in structured peer-to-peer networks
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Distributed hash tables (DHTs), used in a number of structured peer-to-peer systems, provide efficient mechanisms for resource location. A key distinguishing feature of current DHT systems such as Chord, Pastry, and Tapestry is the way they handle locality in the underlying network. Topology-based node identifier assignment, proximity routing, and proximity neighbor selection are examples of heuristics used to minimize message delays in the underlying network. While these heuristics are sometimes effective, they rely on a single global overlay that may install the key of a popular object at a node far from most of the nodes accessing it. Furthermore, a response to a lookup does not contain any locality information about the nodes holding a copy of the object. We address these issues by proposing a novel two-level overlay peer-to-peer architecture. In our architecture, local overlays act as locality-aware caches for the global overlay, grouping nodes close together in the underlying network. Local overlays are constructed by exploiting the structure of the Internet as autonomous systems. We present detailed experimental results demonstrating the practicality of the system, and showing performance gains in response time of up to 60% compared to a single global overlay with state-of-the-art localization schemes. We also present efficient distributed algorithms for maintaining local overlays in the presence of node arrivals and departures.
[locality information, autonomous systems, message delays, Performance gain, Distributed computing, Delay, structured peer-to-peer networks, proximity routing, Intelligent networks, distributed hash tables, two-level peer-to-peer architecture, structured peer-to-peer systems, overlay peer-to-peer architecture, Distributed algorithms, local overlays, peer-to-peer computing, Peer to peer computing, telecommunication network topology, proximity neighbor selection, Routing, global overlay, Upper bound, distributed algorithms, resource location, telecommunication network routing, topology-based node identifier assignment, file organisation, Internet, locality-aware caches, Radiofrequency identification]
Challenges: wireless Web services
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
This paper describes the challenges of adapting existing Web-service architecture to the wireless environment. It presents a new, wireless, Web-service architecture based on the smart client model that can address some of the fundamental differences between the wireless and wireline environments. The fundamental differences between these environments can be called the mobile challenges, including (1) the unpredictable nature of the wireless network, (2) the limited processing capabilities and power on mobile devices, and (3) the need for consumer device to have a fast startup time for mobile applications. The smart client model suggests the following modifications to the existing Web-service stack in order to overcome these mobile challenges: (1) a lightweight Web-service container that runs on a resource-limited mobile device (the client), (2) a quality of user experience (QoUE) model based on application response time, application startup time, and power consumption, and (3) an adaptive application configuration algorithm that can exploit the tradeoff among the QoUE parameters to provide the best user experience given preferences and the device platform.
[Chaos, user experience quality, Energy consumption, application response time, Delay, power consumption, Network servers, Wireless networks, wireless environment, Computer architecture, Search engines, wireline environment, wireless network, wireless Web services, client-server systems, QoUE, smart client model, radio access networks, Computer science, application startup time, application configuration algorithm, Web services, XML, mobile devices, Internet]
Distributed and dynamic voronoi overlays for coverage detection and distributed hash tables in ad-hoc networks
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
In this paper we study two important problems - coverage-boundary detection and implementing distributed hash tables in ad-hoc wireless networks. These problems frequently arise in service location and relocation in wireless networks. For the centralized coverage-boundary problem we prove a /spl Omega/(n log n) lower bound for n devices. We show that both problems can be effectively reduced to the problem of computing Voronoi overlays, and maintaining these overlays dynamically. Since the computation of Voronoi diagrams requires O(n log n) time, our solution is optimal for the computation of the coverage-boundary. We present efficient distributed algorithms for computing and dynamically maintaining Voronoi overlays, and prove the stability properties for the latter - i.e., if the nodes stop moving, the overlay stabilizes to the correct Voronoi overlay. Finally, we present experimental results in the context of the two selected applications, which validate the performance of our distributed and dynamic algorithms.
[Stability, Heuristic algorithms, distributed Voronoi overlays, Software algorithms, ad hoc network, coverage-boundary detection, computational geometry, Ad hoc networks, Application software, Distributed computing, Intelligent networks, distributed algorithm, dynamic Voronoi overlays, Voronoi diagram, Wireless networks, distributed algorithms, distributed hash table, file organisation, ad-hoc wireless network, ad hoc networks, Distributed algorithms, Monitoring, coverage detection]
Scheduling distributable real-time threads in Tempus middleware
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
This paper presents the Tempus real-time middleware, which supports real-time CORBA 2.0's distributable threads (DTs) as an end-to-end programming abstraction for distributed real-time systems. DTs in Tempus can have time constraints, including time/utility functions (TUFs), can have resource constraints, including mutual exclusion, and can be scheduled according to utility accrual (UA) disciplines. Tempus propagates the scheduling parameters of DT's as they transit objects and hence perhaps node boundaries. Node-local instances of a UA scheduling algorithm use the propagated parameters to construct local schedules and resolve resource dependencies for local timeliness optimization, toward approximate, system-wide timeliness optimality. Tempus uses an application-level scheduling framework for node-local TUF/UA scheduling on real-time POSIX-compliant operating systems. Our experimental measurements demonstrate the effectiveness of the middleware in scheduling DTs.
[Real time systems, Unix, system-wide timeliness optimality, distributed real-time systems, Laboratories, utility accrual disciplines, mutual exclusion, Yarn, processor scheduling, time/utility functions, Phased arrays, real-time middleware, optimisation, Operating systems, timeliness optimization, UA scheduling, application-level scheduling, Dynamic programming, real-time threads scheduling, distributed object management, real-time CORBA, middleware, resource dependencies, scheduling parameters, multi-threading, resource constraints, Dynamic scheduling, Middleware, Scheduling algorithm, node-local TUF/UA scheduling, end-to-end programming abstraction, Tempus middleware, real-time systems, real-time POSIX-compliant operating systems, Time factors, distributable real-time threads]
Wavelength assignment on bounded degree trees of rings
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
A fundamental problem in computer and communication networks is the wavelength assignment (WA) problem: given a set of routing paths on a network, assign wavelengths (channels) to the paths such that the paths with the same wavelength are edge-disjoint. The optimization problem here is to minimize the number of wavelengths. A popular network topology is a tree of rings. It is known NP-hard to find the minimum number of wavelengths for the WA problem on a tree of rings. Let L be the maximum number of paths on any edge in the network. Then L is a lower bound on the number of wavelengths for the WA problem. We give a polynomial time algorithm which uses at most 3L wavelengths for the WA problem on a tree of rings with node degree at most eight. This improves the previous result of 4L. We also show that some instances of the WA problem require at least 3L wavelengths on a tree of rings, implying that the 3L upper bound is optimal for the worst case instances. In addition, we prove that our algorithm has approximation ratios 2 and 2.5 for a tree of rings with node degrees at most four and six, respectively.
[routing paths, wavelength division multiplexing, optimization problem, NP-hard, Optical fiber networks, WDM networks, edge-disjoint, Wavelength routing, optimisation, Tree graphs, Network topology, Computer networks, Communication networks, Wavelength assignment, communication networks, trees (mathematics), computer networks, wavelength assignment, telecommunication network topology, Wavelength division multiplexing, network topology, polynomial time algorithm, ring tree, bounded degree trees, telecommunication network routing, Approximation algorithms, computational complexity]
Improving cache performance in mobile computing networks through dynamic object relocation
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Caching improves the performance of Web servers by placing frequently accessed data at intermediate nodes close to Web clients. Similarly, in a mobile network, access delay can be reduced by caching data objects near the mobile clients. Existing caching techniques used for Web servers are unsuitable for mobile networks because they do not deal with the issue of client mobility. To ensure cache performance is not affected by client movement, object relocation techniques can be used to dynamically relocate data objects so they remain close to the moving client. Existing relocation techniques rely on path predictions to help make relocation decisions. However, the inaccuracy of path prediction techniques result in high relocation overhead and increased access delay after each handover. In this paper, we propose two new object relocation techniques to deal with the problem of poor path predictions and high object relocation overhead. The first technique called 2PR (two-phase relocation) compensates for the inaccuracy of existing path prediction algorithms by temporarily relocating data objects to a common parent node until the client's location is confirmed. The second technique, called ROLP (return-path object-list passing), reduces the traffic overhead associated with object relocation by using coordination messages between nodes. Test results show that 2PR and ROLP reduce the penalty of poor path predictions and significantly reduces the overhead associated with cache relocation compared to existing schemes.
[mobile computing networks, cache performance, access delay, client location, path predictions, Telecommunication traffic, data object caching, cache storage, Web clients, Delay, Intelligent networks, Network servers, coordination messages, mobile computing, relocation decisions, Wireless networks, file servers, Computer networks, Web server, dynamic object relocation, traffic overhead, Base stations, client-server systems, two-phase relocation, client mobility, return-path object-list passing, Web servers, Computer science, parent node, mobile communication, mobile clients, intermediate nodes, data access, Internet, Mobile computing, telecommunication traffic, mobile network]
Packet routing and selection on the POPS network
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Partitioned optical passive stars (POPS) network has been proposed recently as a desirable model of parallel computing. Many papers have been published that address fundamental problems on these networks. Packet routing is one such important problem. We present a randomized algorithm in this paper that performs better than the best prior algorithms. We also present a randomized algorithm for selection on the POPS network.
[network routing, packet switching, multiprocessor interconnection networks, Optical fiber networks, Routing, Optical receivers, Optical coupling, Partitioning algorithms, randomized algorithm, Optical transmitters, packet selection, randomised algorithms, POPS network, Emulation, optical interconnections, partial permutation routing, Bandwidth, sparse enumeration sort, Parallel processing, Broadcasting, partitioned optical passive stars network, parallel computing, packet routing, computational complexity]
Self-tuning speculation for maintaining the consistency of client-cached data
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
This paper presents a new protocol, self-tuning active data-aware cache consistency (SADCC), which employs parallel communication and self-tuning speculation to improve the performance of data-shipping database systems. Using parallel communication with simultaneous client-server and client-client communication, SADCC reduces the network latency for detecting data conflicts by 50%, while increasing message volume overhead by only about 4.8%. By being aware of the global states of cached data, clients self-tune between optimistic and pessimistic consistency control. The abort rate is reduced by statistically quantifying the speculation cost. We compare SADCC with the leading cache consistency algorithms, active data-aware cache consistency (ADCC) and asynchronous avoidance-based cache consistency (AACC), in a page server DBMS architecture with page-level consistency. The experiments show that, in a non-contention environment, both SADCC and ADCC display a slight reduction (an average of 2.3%) in performance compared to AACC with a high-speed network environment. With high contention, however, SADCC has an average of 14% higher throughput than AACC and 6% higher throughput than ADCC.
[DBMS architecture, Protocols, Costs, Communication system control, Throughput, self-tuning active data-aware cache consistency, Cache storage, cache storage, consistency control, database management systems, parallel processing, Delay, client-server communication, parallel communication, Network servers, Database systems, noncontention environment, protocols, network latency, client-server systems, page-level consistency, client-cached data, self-adjusting systems, Concurrency control, client-client communication, data-shipping database systems, asynchronous avoidance-based cache consistency, Deductive databases, self-tuning speculation]
Message from the General Chair
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Presents the welcome message from the conference proceedings.
[]
Message from the Program Chair
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Presents the welcome message from the conference proceedings.
[]
Conference committee
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Provides a listing of current committee members.
[]
Program Committee
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Provides a listing of current committee members.
[]
Taming lambda's for applications: the OptIPuter system software
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Summary form only given. Dense wavelength-division multiplexing (DWDM), dark fiber, and low-cost optical switches provide the technological capability for private, high bandwidth communication. However, achieving any substantial application benefit from use of these resources is dauntingly complex and error prone. These emerging environments are often called lambda grids. We are developing a simple abstraction called a distributed virtual computer (DVC), which provides convenient application use of dynamic optical resources. DVC descriptions naturally express communication and computation resource requirements, enabling coordinated resource binding. In addition, their shared namespace provides a natural vehicle for incorporating a range of novel capabilities, including novel transport protocols which expose and exploit the capabilities DWDM environment, including efficient multi-point to point (GTP), optical multicast, real-time communication, and fast point to point transports. DVC's also provide a convenient model for integrating a wide array of network-attached instruments and storage. We describe initial experience with DVC's and how they provide an integrating architecture for lambda grids. The OptlPuter project is a large multi-institutional project led by Larry Smarr at the University of California, San Diego (UCSD) and Tom DeFanti at the University of Illinois at Chicago (UIC). Other software efforts include optical signaling software, visualization, distributed configuration management, and two driving applications involving petabytes of data (in conjunction with the Biomedical Informatics Research Network and the Scripps Institute of Oceanography). The project also includes construction of a high-speed OptlPuter testbed spanning UCSD and UIC.
[high bandwidth communication, Biomedical optical imaging, systems software, optical switches, optical multicast, communication resource requirement, dark fiber, Distributed computing, telecommunication computing, network-attached instrument, a high-speed OptlPuter testbed, coordinated resource binding, visualization software, Bandwidth, OptIPuter system software, dynamic optical resources, Optical fiber communication, point-to-point transport, OptlPuter project, System software, optical communication, dense wavelength-division multiplexing, computation resource requirement, low-cost optical switches, private communication, Optical switches, Wavelength division multiplexing, shared namespace, lambda grids, Application software, real-time communication, optical signaling software, network-attached storage, transport protocols, distributed virtual computer, virtual machines, Computer errors, distributed configuration management, High speed optical techniques]
The intersection of grids and networks: where the rubber hits the road
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
Summary form only given. As distributed systems, grids depend completely on networks to provide the communication related services that interconnect the grid system components. Various sorts of grid services need to be able to specify, and request of the network, quality of service, transport services, and monitoring services, etc. Additionally there are services such as identity and rights management, temporary storage and compute relative to network locale, etc., that may be most advantageously provided to grids by the network organization. From the network point of view, the world is fragmented into many different administrative domains (autonomous systems). There may or may not be a common control plane for resource management related to QoS, resource allocation and allocation management are dealt with independently in each different AS, transport service definition and availability are AS dependent, monitoring and measurement may or may not be provided and may or may not be considered a good thing, the parent organization may be long-lived and able to provide highly secure and persistent information services, or it may be a contractor that rotates every few years, etc., etc. This paper discusses some of the issues and approaches to providing network related grid services in the real world of multiple network domains.
[resource management, temporary storage, Roads, network organization, grid computing, Quality of service, resource allocation, QoS, transport service availability, secure information services, Grid computing, distributed systems, Computer networks, Rubber, Monitoring, Availability, persistent information services, grid system components, allocation management, quality of service, grid services, Identity management systems, multiple network domain, computer network management, identity management, Resource management, Computer network management, transport service definition, communication services, rights management]
The speed of light isn't what it used to be [computer design]
Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.
None
2004
As computer designs run up against the limits of physics, we increasingly see communication latencies limited by the speed of light. However, the "speed of light" is far less than the usual rule of thumb, "one nanosecond per foot of distance"; on-chip traces will soon be able to move signals at only 4% of the speed of light. This accelerates the need for ways to cope with an ever-increasing ratio of communication to computation time, cost, and power consumption in modern computer designs. We examine the impact this has on designers and users, and present some novel approaches for dealing with this effect both in hardware and software.
[Energy consumption, Thumb, computer networks, CAD, computer design, communication latency, on-chip trace, Delay, power consumption, Foot, computer communications software, Physics computing, Hardware, Computational efficiency, Acceleration, communication computation time]
Message from the PDES-2005 Chairs
11th International Conference on Parallel and Distributed Systems
None
2005
Presents the welcome message from the conference proceedings.
[]
PDES-2005 Organizing Committee
11th International Conference on Parallel and Distributed Systems
None
2005
Provides a listing of current committee members.
[]
Minimizing Energy via Loop Scheduling and DVS for Multi-Core Embedded Systems
11th International Conference on Parallel and Distributed Systems
None
2005
Low energy consumptions are extremely important in real-time embedded systems, and scheduling is one of the techniques used to obtain lower energy consumptions. In this paper, we propose loop scheduling algorithms for minimizing energy based on rotation scheduling and DVS (dynamic voltage and frequency scaling) for real-time multi-core embedded systems. The experimental results show that our algorithms have better performances than list scheduling and pure ILP (integer linear programming) scheduling with DVS
[Real time systems, Energy consumption, dynamic voltage-frequency scaling, loop scheduling algorithm, integer linear programming scheduling, Dynamic voltage scaling, Voltage control, low energy consumption, Scheduling algorithm, Delay, power consumption, list scheduling, Processor scheduling, Embedded system, Signal processing algorithms, embedded systems, real-time multicore embedded system, energy conservation, scheduling, Threshold voltage, energy minimization, rotation scheduling]
A Dynamic Node Degree Management Scheme for Energy-efficient Routing Protocols in Wireless Ad Hoc Networks
11th International Conference on Parallel and Distributed Systems
None
2005
In mobile devices, the battery-based power is a precious resource. To maximize the network lifetime becomes a challenge issue in MANETs. We found that transmission collision is a problem which affects the energy saving much. If we can reduce the numbers of the transmission collision, we save more power. In this paper, we propose a new mechanism to address this problem and hence prolong network lifetime. Our approach, based on relative neighborhood graph (RNG), adjusts the transmission range according to the degree of the node which is only based on local information. We also suggest appropriate transmission power ranges to some routing protocols such as AODV, DSR and BELLMAN-FORD. Simulation results show that a great improvement of the energy saving for those routing protocols when comparing with the performance of those protocols without applying our mechanism
[Energy consumption, telecommunication congestion control, graph theory, RNG, Batteries, power consumption, Mobile ad hoc networks, Intelligent networks, mobile computing, Broadcasting, relative neighborhood graph, collision., Routing protocols, energy-efficient routing protocol, energy consumption, dynamic node degree management scheme, mobile radio, Peer to peer computing, ad hoc network, Ad hoc networks, transmission collision, mobile device, MANET, battery-based power, wireless ad hoc network, routing protocols, energy conservation, Energy efficiency, ad hoc networks, Energy management]
A Hierarchical Approach for Energy-Aware Distributed Embedded Intelligent Video Surveillance
11th International Conference on Parallel and Distributed Systems
None
2005
Intelligent video surveillance (IVS) offers a large spectrum of different applications that have strict requirements on quality of service (QoS) and energy-efficiency. Recent embedded IVS systems need to deliver compressed video data in high quality while using devices that are partly solar- or battery-powered. In this paper we present PoQoS, a novel hierarchical approach for combined power- and QoS-management in distributed embedded IVS systems. PoQoS allows the implementation of both global and local power- and QoS-adaptation in order to achieve optimal energy/QoS tradeoffs. Furthermore, we present a DSP-based, embedded platform that is used for IVS. It includes QoS-triggered onboard dynamic power management (DPM) that is controlled via Ethernet. We demonstrate the feasibility of PoQoS with a typical IVS-setup and present experimental results
[Algorithm design and analysis, intelligent video surveillance, Quality of service, local area networks, Videoconference, energy-aware distributed embedded system, QoS, Smart cameras, embedded systems, Video compression, Safety, surveillance, Image storage, Availability, data compression, onboard dynamic power management, video data compression, quality of service, video coding, DSP-based embedded platform, Ethernet, Video surveillance, Energy efficiency, low-power electronics, energy-efficiency, digital signal processing chips]
A Code Generation Algorithm for Affine Partitioning Framework
11th International Conference on Parallel and Distributed Systems
None
2005
Multiprocessors are about to become prevalent in the PC world. Major CPU vendors such as Intel and Advanced Micro Devices have recently announced their imminent migration to multicore processors. Affine partitioning provides a systematic framework to find asymptotically optimal computation and data decomposition for multiprocessors, including multicore processors. This affine framework uniformly models a large class of high-level optimizations such as loop interchange, reversal, skewing, fusion, fission, re-indexing, scaling, and statement reordering. However, the resulting code after applying affine transformations tends to contain more loop levels and complex conditional expressions. This impacts performance, code readability and debuggability for both programmers and compiler developers. To facilitate the adoption of affine partitioning in industry, we address the above practical issues by proposing a salient two-step algorithm: coalesce and optimize. The coalescing algorithm maintains valid code throughout and improves readability and debuggability. We demonstrate with examples that the optimization algorithm simplifies the resulting loop structures, conditional expressions and array access functions and generates efficient code
[program debugging, affine partitioning framework, Advanced Micro Devices, code debuggability, optimization algorithm, program compilers, coalescing algorithm, Intel, Concurrent computing, Program processors, optimisation, Microprocessors, Parallel processing, code readability, multicore processor, code generation algorithm, program control structures, multiprocessing systems, Multicore processing, CPU vendor, Educational institutions, Partitioning algorithms, Application software, Programming profession, conditional expression, loop structure, Central Processing Unit, multiprocessor, array access function]
Constructing a Memory-Based Distributed Code Storage on Networked Diskless Embedded Systems
11th International Conference on Parallel and Distributed Systems
None
2005
Advances in networking and microprocessor technology have enabled the development of a variety of small, low-cost, and resource-limited embedded systems. Such an embedded system is often diskless due to the weight, size, and cost of a disk. Because of the availability of inexpensive high-speed network cards, most embedded systems are networked, wired or wireless. We propose and implement a memory-based distributed code storage called NFC (networked file cache) for networked diskless embedded systems. NFC utilizes and integrates all available memory resources in a network to form a large-scaled code storage to which all applications in all nodes are allowed to access. To efficiently locate a file in NFC, we present a one-hop file lookup algorithm and a set of cache management algorithms dealing with file migrations and replications. In comparison with a client-server approach, the experimental results show that NFC successfully integrates memory resources in a network to significantly extend code storage and reduce file-access time of a diskless embedded system
[client-server systems, Costs, Protocols, cache management algorithm, file migration, file replication, File servers, Cache storage, cache storage, disc storage, Embedded software, Network servers, memory architecture, client-server approach, Operating systems, Embedded system, embedded systems, distributed memory systems, memory-based distributed code storage, networked diskless embedded system, Resource management, Kernel, networked file cache, one-hop file lookup algorithm]
Design of a Configurable Embedded Processor Architecture for DSP Functions
11th International Conference on Parallel and Distributed Systems
None
2005
Most of the embedded applications are served today by general-purpose processors or special-purpose ASIC processors containing hundreds to thousands of ALUs. While such solutions are efficient, they lack flexibility and are not feasible for certain embedded applications. ASIP(application specific instruction processor) design methodology can not only satisfy the functionality and performance requirements of the embedded systems but also flexible. So it is widely adopted in embedded processor design domain. For the widely adopting of digital signal processing in the embedded applications, this paper studies a configurable VLIW processor architecture based on TTA(transport triggered architecture) for high performance digital signal processing in embedded systems. The methodology of ASIP design is applied and some handle optimizations are taken. It is shown that it has high performance to run the digital signal processing kernel applications, and its simplicity and flexibility encourages for further development with tuned functionality
[Process design, Design methodology, parallel architectures, high performance digital signal processing, VLIW, reconfigurable architectures, Embedded system, configurable embedded processor architecture design, embedded systems, integrated circuit design, Computer architecture, configurable VLIW processor architecture, Kernel, application specific instruction processor design, special-purpose ASIC processor, Application specific processors, microprocessor chips, transport triggered architecture, Application software, Digital signal processing, general-purpose processor, digital signal processing kernel application, Signal analysis, application specific integrated circuits, digital signal processing chips]
Cramer-Rao Bound Analysis of Quantized RSSI Based Localization in Wireless Sensor Networks
11th International Conference on Parallel and Distributed Systems
None
2005
Localizing sensor nodes in a distributed system of wireless sensors is an essential process for self-organizing wireless sensor networks. Localization is a fundamental problem in wireless sensor networks, and the behavior of localization has not been thoroughly studied. In this paper, we formulate the quantized received signal strength indicator based localization as a parameter estimation problem and derive the Cramer-Rao lower bound for the localization problem. We study the effect of quantization level and network configuration parameters on the lower bound of localization error variance and understand the relationship between network configuration and localization accuracy
[Error analysis, wireless sensor networks, distributed wireless sensor network, Quantization, quantized RSSI-based localization, Sensor systems, Cramer-Rao bound analysis, parameter estimation problem, Information analysis, Computer science, Intelligent networks, Wireless sensor networks, network configuration, localization error variance, Phase estimation, Computer errors, parameter estimation, Hardware, ad hoc networks, error statistics]
Partitioning and Pipelined Scheduling of Embedded System Using Integer Linear Programming
11th International Conference on Parallel and Distributed Systems
None
2005
In this paper, an integer linear programming (ILP) based approach is proposed for integrated hardware/software (HW/SW) partitioning and pipelined scheduling of embedded systems for multimedia applications. The ILP approach not only partitions and maps each computation task of a particular multimedia application onto a component of the heterogeneous multiprocessor architecture, but also schedules and pipelines the execution of these computation tasks while considering communication time. The objective is to minimize the total component cost and the number of pipeline stages subject to the throughput constraint on the pipelined architecture. Experiments on two real multimedia applications are used to demonstrate the effectiveness of the proposed approach
[Costs, multiprocessing systems, integer linear programming, Multimedia systems, integer programming, Pipelines, pipelined scheduling, multimedia systems, linear programming, Application software, integrated hardware/software partitioning, Embedded software, multimedia application, Processor scheduling, Embedded system, embedded systems, Computer architecture, heterogeneous multiprocessor architecture, Integer linear programming, scheduling, embedded system, Hardware, pipeline processing, pipelined architecture]
Design and Integration of Parallel Hough-Transform Chips for High-speed Line Detection
11th International Conference on Parallel and Distributed Systems
None
2005
Line detection is often needed in computer vision applications. The Hough transform processing of image data for line detection is robust but time-consuming. With the use of multiple processors, the processing time for Hough transform can be much reduced. In our research, we design an array processor for line-detection based on Hough transform that performs the line-parameter calculation and accumulation for different angles in parallel. Such an array processor together with its parallel peak extraction circuits have been implemented on a single chip. Based on the TSMC 0.35mum CMOS technology, the fabricated chip (with 10 processors) can be run successfully up to the clock rate of 50MHz. This paper presents the SOC design that can be extended to the integration of multiple chips to form a faster system with more parallel processors
[Process design, Computer vision, TSMC CMOS technology, Image edge detection, Circuits, high-speed line detection, parallel processor, CMOS process, Application software, parallel processing, parallel peak extraction circuit, parallel Hough-transform chip design, Embedded system, Hough transforms, integrated circuit design, computer vision, CMOS technology, Hough transform processing, Robustness, array processor design, SOC design, CMOS integrated circuits, system-on-chip, Clocks]
Hardware-based Packet Classification Made Fast and Efficient
11th International Conference on Parallel and Distributed Systems
None
2005
To achieve fast packet classification, a hardware-based scheme, cross-producting recurrence (CPR), based on a formerly proposed cross-producting scheme is proposed. This scheme simplifies the classification procedure and decrease the distinct combinations of fields by hierarchically decomposing the multi-dimensional space. In the new scheme, the multi-dimensional space is endowed with a hierarchical property which self-divides into several smaller subspaces, whereas the procedure of packet classification is translated into repeatedly searching for matching subspaces. The required storage of the proposed scheme is significantly reduced since the distinct fields of subspaces is controlled by a pre-defined configuration and can be much less than that of the filters. The experimental results demonstrate the effectiveness and scalability of the proposed scheme
[Protocols, Costs, Scalability, Laboratories, packet switching, Data structures, Telecommunications, Information management, Next generation networking, Matched filters, cross-producting recurrence scheme, Clustering algorithms, hardware-based packet classification, IP networks, protocols]
Embedded Fingerprint Verification System
11th International Conference on Parallel and Distributed Systems
None
2005
Fingerprint verification is one of the most reliable personal identification methods in biometrics. In this paper, an effective fingerprint verification system is presented. We describe an enhanced fingerprint verification system consisting of image pre-processing, feature extraction and matching processes. Improved image pre-processing and broken ridge reconnection methods are proposed. In this paper, we also describe the design and implementation of a fingerprint verification system on SoC
[Chaos, Biometrics, fingerprint identification, SoC, Fingerprint recognition, Gray-scale, Reliability engineering, image matching, Computer science, biometrics, embedded fingerprint verification system, broken ridge reconnection methods, Image matching, Computer network reliability, feature extraction, embedded systems, image pre-processing, Gabor filters, system-on-chip, Pixel, personal identification methods]
SaNSO 2005 Foreword
11th International Conference on Parallel and Distributed Systems
None
2005
Presents the welcome message from the conference proceedings.
[]
SaNSO-2005 Program Committee
11th International Conference on Parallel and Distributed Systems
None
2005
Provides a listing of current committee members.
[]
Pulse Position Modulation for Active RFID System
11th International Conference on Parallel and Distributed Systems
None
2005
Radio frequency identification (RFID) enables everyday objects to be identified, tracked, and recorded. This paper presents a design of N-ary pulse position modulation transmission-only active RFID system for monitoring of large number and high density objects. A burst frame from tag consists of multiple duplicated blink units and the interval between them is randomly selected from predefined interval steps to avoid continuous collision. The reader segments composite pulse streams and performs cross-correlation to recover the overlapped blink unit. The tag can be read in registration state or pattern matching state. Simulations show that the system is power efficient, large capacity, and fast reactive
[duplicated blink units, Energy consumption, Active RFID tags, pattern matching, composite pulse stream segmentation, Chirp, radiofrequency identification, Switches, tag burst frame, Educational institutions, radio frequency identification, Pulse modulation, RFID tags, N-ary PPM transmission-only active RFID system, pattern clustering, registration state, Frequency shift keying, Communications technology, pattern matching state, pulse position modulation, Radiofrequency identification]
Novel Anti-collision Algorithms for Fast Object Identification in RFID System
11th International Conference on Parallel and Distributed Systems
None
2005
We propose two ALOHA-based Dynamic Framed Slotted ALOHA algorithms (DFSA) using tag estimation method (TEM), which estimates the number of tags around the reader, and dynamic slot allocation (DSA), which dynamically allocates the frame size for the number of tags. We compare the performance of the proposed DFSA with the conventional Framed Slotted ALOHA algorithm (FSA) using simulation. According to the analysis, two proposed DFSA algorithms show better performance than FSA algorithm regardless of the number of tags
[Algorithm design and analysis, object recognition, Computer vision, Protocols, estimation theory, radiofrequency identification, ALOHA-based Dynamic Framed Slotted ALOHA algorithms, ISO standards, Manuals, Dynamic Slot Allocation, Tag Estimation Method, Sensor systems, DFSA algorithms, RFID system, Radio frequency, anticollision algorithms, Binary trees, object identification, Performance analysis, Radiofrequency identification, FSA algorithm]
A node-based available bandwidth evaluation in IEEE 802.11 ad hoc networks
11th International Conference on Parallel and Distributed Systems
None
2005
We propose a new technique to estimate the available bandwidth of wireless nodes and by extension of one-hop links in IEEE 802.11-based ad hoc networks. Our technique exploits the fact that a node can estimate the channel occupancy by monitoring its environment. It provides a non-intrusive estimation meaning that it doesn't generate additional traffic to perform the evaluation. We show by simulations that our technique provides an accurate estimation of available bandwidth on wireless links in mobile ad hoc environment
[Performance evaluation, radio links, Base stations, Peer to peer computing, Quality of service, IEEE 802.11-based ad hoc networks, Throughput, Ad hoc networks, one-hop links, channel occupancy estimation, wireless links, bandwidth allocation, Intelligent networks, channel estimation, node-based available bandwidth evaluation, Bandwidth, Routing protocols, nonintrusive estimation, ad hoc networks, wireless LAN, mobile ad hoc environment, Monitoring]
An Efficient Authentication and Access Control Scheme Using Smart Cards
11th International Conference on Parallel and Distributed Systems
None
2005
In this paper, we propose a novel integrated authentication and access control scheme using smart cards. A list of accessible resources with privileges is encrypted in the smart card issued to the user. Without storing access control information, a server can authenticate each user, realize resources to be accessed, and determine access privileges. We propose the use of card identifiers to prevent privilege elevation attacks and to protect the privacy of access requests. Our scheme has the following merits: low communication and computational cost, no access control information in the server, prevention of privilege elevation attack, multiple-access requests, privacy protection of access requests, mutual authentication, and session key agreement
[Access control, Smart cards, privilege elevation attack prevention, card identifiers, session key agreement, smart cards, cryptography, Reflection, access request privacy, Information management, multiple-access requests, mutual authentication, Privacy, Network servers, Authentication, message authentication, access control scheme, authorisation, Computational efficiency, Cryptography, Protection, access privileges]
Secure extensible type system for efficient embedded operating system by using metatypes
11th International Conference on Parallel and Distributed Systems
None
2005
In the context of extensible system for small secure embedded devices, we present an extensible type system for typed intermediate languages that unifies in a unique hierarchy type systems from various source high level languages and ensures integrity and confidentiality. To increase execution efficiency and use flexibility, we propose a dynamic binding mechanism that allows the programmer to describe the bindings of his code without breaking the type system
[Smart cards, typed intermediate languages, high level languages, data integrity, Security, Application software, High level languages, Programming profession, embedded operating system, secure extensible type system, Runtime, Operating systems, unique hierarchy type systems, Embedded system, embedded systems, operating systems (computers), Hardware, metatypes, dynamic binding mechanism, reasoning about programs, Reliability]
Lower and Upper Bounds for Minimum Energy Broadcast and Sensing Problems in Sensor Networks
11th International Conference on Parallel and Distributed Systems
None
2005
In this paper, we study the problem of power transmission and sensing ranges assignment wireless sensor network nodes so that to minimize power consumption while ensuring broadcasting task or sensing process. A first novelty is that our model takes into account both the transmission and the reception costs when evaluating the energy consumption of a broadcasting task. We establish a new analytical model and derive lower and upper bounds on region covering. Moreover, we show that the lower bound is asymptotically optimal and can be approached up to epsi
[radio broadcasting, Energy consumption, Costs, Protocols, wireless sensor networks, Power transmission, power transmission, Batteries, wireless sensor network nodes, power consumption, Intelligent networks, Wireless sensor networks, Analytical models, Upper bound, power consumption minimization, Broadcasting, minimum energy broadcast]
Multicast Scaling Properties in Massively Dense Ad Hoc Networks
11th International Conference on Parallel and Distributed Systems
None
2005
We study the benefits of multicast routing in the performance of mobile ad hoc networks. In particular we show that if a node wishes to communicate with n distinct destinations, multicast can reduce the overall network load by a factor O (radicn), when used instead of unicast. One of the implications of this scaling property consists in a significant increase of the total capacity of the network for data delivery. We discuss how these results can be taken into consideration in the operation of a multicast protocol using overlay multicast trees
[Costs, mobile radio, mobile ad hoc network performance, overlay multicast trees, Multicast communication, Multicast protocols, Ad hoc networks, Mobile ad hoc networks, Intelligent networks, multicast scaling properties, Unicast, Network topology, telecommunication network routing, multicast communication, Routing protocols, Communication system traffic control, ad hoc networks, multicast routing]
Message from the PMWMNC-2005 Chairs
11th International Conference on Parallel and Distributed Systems
None
2005
Presents the welcome message from the conference proceedings.
[]
PMWMNC-2005 Workshop Organizers
11th International Conference on Parallel and Distributed Systems
None
2005
Provides a listing of current committee members and society officers.
[]
Choosing an Accurate Network Model using Domain Analysis
11th International Conference on Parallel and Distributed Systems
None
2005
Network link simulation is perhaps the most common method for evaluating application and network protocol designs. In modeling realistic networks, researchers face measurements whose characteristics experience non-stationarity (time variability) and complex patterns due to a number of factors, including both internal network elements and external events. In this paper we introduce a methodology we call domain analysis to quantify the accuracy of different models, and show how to use it to choose the best model for a given set of network characteristics. Our work seeks to aid network and application protocol developers in developing and choosing appropriate models for network simulation. We introduce our data preconditioning methodology for modelling non-stationary datasets, and present the new multiple states MTA model (MMTA). We show that it is better in capturing error burst statistics than classical models and more consistently accurate across different networks that our previous MTA model
[network link simulation, telecommunication links, Protocols, telecommunication network planning, application protocol developers, Educational institutions, Time measurement, domain analysis, network protocol developers, Data mining, Milling machines, multiple states MTA model, transport protocols, Hidden Markov models, Data visualization, data preconditioning method, Error correction, Automatic repeat request, network protocol designs, nonstationary dataset modelling, Digital audio players, error statistics]
An Analytical Model for Performance Evaluation of Network Traffic Analysis Systems
11th International Conference on Parallel and Distributed Systems
None
2005
Simulation models have been developed in order to foresee characteristics of networks, systems or protocols when doing tests in laboratories are very expensive or even impossible. This paper presents a simulation model of a multiprocessor network traffic analysis system. The model, which is based on closed networks of queues, evaluates the efficiency of the system depending on the hardware/software platform features. Therefore, this model is able to estimate performance early in the design and development stages simulating a multiprocessor architecture in charge of analysing network traffic. The goodness of the model is checked by comparing analytical results with practical ones obtained in laboratory using a traffic analysis system that runs in a multiprocessor platform
[System testing, Gold, Costs, multiprocessing systems, queueing theory, Laboratories, computer networks, Telecommunication traffic, queue network, performance evaluation, Telecommunication standards, multiprocessor architecture, Analytical models, Traffic control, multiprocessor network traffic analysis system, Computer networks, Performance analysis, telecommunication traffic]
ModellingWeb transfer Performance over Asymmetric Networks
11th International Conference on Parallel and Distributed Systems
None
2005
TCP over bandwidth asymmetric networks such as cable TV, asymmetric digital subscriber loop (ADSL) and wireless networks exhibit different characteristics from TCP on symmetric links. A number of techniques have been proposed to address this problem. However, previous research has been largely focused on bulk transfers. This paper investigates the effects of bandwidth asymmetry on Web-like short-lived transfers. A close-form prediction model is presented for TCP transfers over asymmetric links. The Web transfer model is then derived from it. Simulations based on ns-2 show that the model can give predictions for TCP transfers with a high degree of accuracy
[TCP, telecommunication links, bandwidth asymmetric networks, Web transfer performance, Cable TV, Predictive models, asymmetric digital subscriber loop, Throughput, cable TV, wireless networks, Helium, Delay, cable television, Computer science, bandwidth allocation, Wireless networks, transport protocols, Bandwidth, telecommunication network reliability, Internet, DSL, digital subscriber lines, Contracts]
Analysis of Discrete-Time Queues with Space and Service Priorities for Arbitrary Arrival Processes
11th International Conference on Parallel and Distributed Systems
None
2005
Finite buffer queues with service and space priorities are of great importance towards effective congestion control and quality of service (QoS) protection in high speed networks. This paper presents closed-form analytical solutions, based on the principle of maximum entropy (ME), for the performance evaluation of single server finite capacity G/G/1/N censored queues with R (R&gt;1) distinct priority classes under preemptive-resume (PR) or head-of-line (HoL) service disciplines. Buffer management for these queues employs either partial buffer sharing (PBS) or complete buffer sharing (CBS) schemes. A vector, N, representing a sequence of thresholds (N<sub>1</sub>, N<sub>2</sub>,... ,N<sub>R</sub>) for each priority class jobs, is used to implement these buffer management schemes in order to provide quality of service constraints. Closed-form expressions for various performance metrics have been derived
[Measurement, maximum entropy principle, telecommunication congestion control, Quality of service, Closed-form solution, Entropy, Network servers, High-speed networks, discrete-time queues, Performance analysis, partial buffer sharing, Protection, Quality management, buffer storage, queueing theory, computer networks, finite buffer queues, preemptive-resume service, performance evaluation, congestion control, quality of service, head-of-line service, complete buffer sharing scheme, maximum entropy methods, QoS protection, Queueing analysis]
Systematic Performance Modeling and Characterization of Heterogeneous IP Networks
11th International Conference on Parallel and Distributed Systems
None
2005
Accurate measurement and modeling of IP networks is essential for network design, planning, and management. Efforts are being made to detect the state of the network from end-to-end measurements using different techniques and paradigms. In this paper we present a novel concept to use in the modeling of real network scenarios under measurement and analysis. We called this new concept service condition. We explain our proposal's motivations and we show how to apply the service condition concept to the study of real heterogeneous network scenarios. To show the real applicability of our proposal results from a performance evaluation study over real heterogeneous networks (where the integration of LAN, UMTS and GPRS is present) are given
[Measurement, Wireless LAN, Ground penetrating radar, Packet switching, telecommunication network planning, 3G mobile communication, Quality of service, performance evaluation, quality of service, GPRS, Proposals, network management, UMTS, computer network management, packet radio networks, LAN, IP networks, wireless LAN, network design, Local area networks, Monitoring, service condition, heterogeneous IP network, network planning]
On the Performance of Probabilistic Flooding in Mobile Ad Hoc Networks
11th International Conference on Parallel and Distributed Systems
None
2005
This paper investigates using extensive simulations the effects of a number of important system parameters in a typical MANETs, including node speed, pause time, traffic load, and node density on the performance of probabilistic flooding. The results reveal that most of these parameters have a critical impact on the reachability and the number of saved rebroadcast messages achieved by probabilistic flooding, prompting the need for dynamically adjusting nodal retransmission probabilities depending on the current state of the network
[Algorithm design and analysis, probabilistic flooding, message passing, Computational modeling, probability, Telecommunication traffic, performance evaluation, Mobile ad hoc networks, broadcasting, Intelligent networks, Analytical models, mobile computing, MANET, Storms, mobile ad hoc networks, Broadcasting, Computer networks, Performance analysis, nodal retransmission probabilities, ad hoc networks, telecommunication traffic]
Clustering Overhead and Convergence Time Analysis of the Mobility-based Multi-Hop Clustering Algorithm for Mobile Ad Hoc Networks
11th International Conference on Parallel and Distributed Systems
None
2005
With the emergence of large mobile ad hoc networks, the ability of existing routing protocols to scale well and function satisfactorily comes into question. Clustering has been proposed as a means to divide large networks into groups of suitably smaller sizes such that prevailing MANET routing protocols can be applied. However, the benefits of clustering come at a cost. Clusters take time to form and the clustering algorithms also introduce additional control messages that contend with data traffic for the wireless bandwidth. In this paper, we aim to analyze a distributed multi-hop clustering algorithm, mobility-based D-hop (MobDHop), based on two key clustering performance metrics and compare it with other popular clustering algorithms used in MANETs. We show that the overhead incurred by multi-hop clustering has a similar asymptotic bound as 1-hop clustering while being able to reap the benefits of multi-hop clusters
[Algorithm design and analysis, workstation clusters, Costs, distributed multihop clustering algorithm, communication complexity, Convergence, Mobile ad hoc networks, mobile computing, distributed algorithms, Clustering algorithms, routing protocols, mobile ad hoc networks, mobility-based D-hop algorithm, Spread spectrum communication, Bandwidth, Routing protocols, Communication system traffic control, Performance analysis, ad hoc networks, telecommunication traffic, MANET routing protocols]
Performance Evaluation for Self-Healing Distributed Services
11th International Conference on Parallel and Distributed Systems
None
2005
Distributed applications, based on internetworked services, provide users with more flexible and varied services and developers with the ability to incorporate a vast array of services into their applications. Such applications are difficult to develop and manage due to their inherent dynamics and heterogeneity. One desirable characteristic of distributed applications is self-healing, or the ability to reconfigure themselves "on the fly" to circumvent failure. In this paper, we discuss our middleware for developing self-healing distributed applications. We present the model we adopted for self-healing behaviour and show as case study the reconfiguration of an application that uses networked sorting services and an application for networked home appliances. We discuss the performance benefits of self-healing property by analyzing the elapsed time for automatic reconfiguration without user intervention. Our results show that a distributed application developed with our self-healing middleware is able to perform smoothly by quickly reconfiguring its services upon detection of failure
[self-healing distributed services, internetworking, Educational institutions, Application software, Middleware, Distributed computing, Sorting, configuration management, Home appliances, networked home appliances, networked sorting services, Web and internet services, sorting, fault tolerant computing, Performance analysis, Personal digital assistants, Assembly, software performance evaluation, middleware, internetworked services]
The Impact of Random Waypoint Mobility on Infrastructure Wireless Networks
11th International Conference on Parallel and Distributed Systems
None
2005
In this paper we report on the impact of the random waypoint model on infrastructure wireless networks that use multi-rate schemes. The waypoint model causes a higher concentration of nodes near the centre of the mobility area. Consequently, we show that a wireless network could appear to achieve higher capacity, and that base stations near the centre of the mobility area experience higher frequencies of transmission rate change. These aberrations could affect simulation results that should be interpreted with care
[Base stations, Wireless application protocol, random waypoint model, random processes, Throughput, infrastructure wireless networks, mobile computing, Network topology, Wireless networks, Land mobile radio cellular systems, Chromium, Frequency, Australia, wireless LAN, Motion analysis]
Performance Modeling of Hand Off calls in 4G Networks
11th International Conference on Parallel and Distributed Systems
None
2005
In this paper, an analytic framework is devised, based on the principle of maximum entropy (ME), for the performance modelling of wireless 4G cellular networks with bursty multimedia traffic under an efficient buffer threshold-based generalized partial sharing (GPS) traffic handling scheme. Analytical closed-form expressions for the state probabilities have been characterized, subject to appropriate GE-type queueing and delay theoretic constraints and new expressions for the aggregate state and blocking probabilities are determined. Typical numerical examples are included to validate the ME performance metrics against Java-based simulation results and also to study the effect of bursty multiple class traffic upon the performance of the cell
[wireless 4G cellular networks, Java-based simulation, maximum entropy principle, blocking probabilities, Telecommunication traffic, Closed-form solution, Entropy, Delay, delay theoretic constraints, Traffic control, generalized partial sharing traffic handling scheme, Constraint theory, Performance analysis, multimedia communication, Java, bursty multimedia traffic, queueing theory, probability, performance evaluation, analytical closed-form expressions, Global Positioning System, 4G mobile communication, Land mobile radio cellular systems, maximum entropy methods, Queueing analysis, telecommunication traffic, cellular radio]
An Evaluation Mechanism for QoS Management in Wireless Systems
11th International Conference on Parallel and Distributed Systems
None
2005
The evaluation of QoS requirements is one of the critical functions that span both the design and the run-time phases of QoS management. This paper presents an architecture for QoS evaluation and admission control, based on the modelling of both system behaviour and QoS requirements. Two aspects are considered. The first refers to QoS management, and to the component-based architecture for QoS evaluation. The second relates to the approach and its illustration by a case study, based on a personal area network. The proposed approach relies on the instantiation of models for representing both the behaviour and the QoS aspects of the system in terms of timed automata. The compatibility of the evaluation mechanism with architectures with a defined role for a QoS manager, such as ITSUMO, is also highlighted
[open systems, telecommunication congestion control, automata theory, Quality of service, admission control, Personal area networks, QoS management, Information science, Runtime, Technology management, mobile computing, Computer architecture, personal area network, timed automata, wireless systems, Laser sintering, quality of service, Computer science, personal area networks, component-based architecture, Admission control, Automata, ITSUMO, wireless LAN, QoS requirements]
Experimental Performance Modeling of MANET Interconnectivity
11th International Conference on Parallel and Distributed Systems
None
2005
The proliferation of mobile wireless computing devices and the increasing usage of wireless networking has motivated substantial research in mobile ad hoc networks (MANETs). In addition, much has also been done to link autonomous MANETs to the Internet, and as MANETs become more prevalent, the need to interconnect multiple MANETs becomes increasingly important too. However, direct interconnection of MANETs has rarely been studied. In this paper, we report an experimental study on the performance of interconnected MANETs running two different routing protocols, viz., the ad hoc on-demand distance vector (AODV) and optimized link state routing (OLSR) protocols, which represent the two major categories, and show that with the use of multiple gateways, it is possible to viably interconnect multiple networks running different MANET routing protocols
[Base stations, mobile ad hoc network, MANET interconnectivity, Telecommunication traffic, ad hoc on-demand distance vector routing protocol, Mobile ad hoc networks, Network topology, mobile communication, routing protocols, Spread spectrum communication, mobile wireless computing device, Routing protocols, Computer networks, optimized link state routing protocol, Internet, IP networks, ad hoc networks, Mobile computing]
Message from the INVITE-2005 Chairs
11th International Conference on Parallel and Distributed Systems
None
2005
Presents the welcome message from the conference proceedings.
[]
INVITE 2005 Organizing Committee
11th International Conference on Parallel and Distributed Systems
None
2005
Provides a listing of current committee members.
[]
Collaborative Virtual Traditional Japanese Crafting System over the Japan Gigabit Network II
11th International Conference on Parallel and Distributed Systems
None
2005
Virtual reality technology performs very important role to design and construct desired buildings houses. In this paper, we introduce an extended virtual Japanese traditional crafting system to collaborate by many users such as end consumers, craftsmen, coordinator and salesperson each other. The high immersive system in consideration of the collaboration support over the next-generation network (JGN II) is proposed. We are building the presentation system which takes sensitivity into consideration to virtual reality environment. It has so far mounted by proposing three systems of an Internet version (Java+VRML and Java+Java3D) and a CAVE version. We think that collaboration support in share space is required of remote users. Multiple CAVE systems in different locations are interconnected to organize a collaborative virtual reality environment over Japan gigabit network (JGN II) which is a testbed network in Japan with several ten Gigabit/sec. A larger number of traditional Japanese fittings in several local cities are redesigned by 3DCG computes into CAD data and stored in 3D database systems over JGN II. Users can share the virtual space, interactively collaborate, retrieve form the distributed DBs to realize the desired interior space in the house for consumers. We implemented a 3D presentation system by OpenGL and IRIS performer in CAVE environment on JGN II and built a prototype system. As a result, we could verify the usefulness of the suggested system since it provided not only higher potential on deign capability and productive efficiency but also made user's satisfaction much better on build a new house
[virtual reality languages, distributed database, Java, System testing, IRIS performer, Buildings, Fitting, CAD, OpenGL, Distributed computing, CAVE version, Next generation networking, virtual traditional Japanese crafting system, Japan Gigabit Network II, 3D database system, Collaboration, Virtual reality, distributed databases, groupware, Cities and towns, virtual reality technology, Internet, solid modelling]
Interactive Controller for 3D Contents with Omni-directional Display
11th International Conference on Parallel and Distributed Systems
None
2005
We are interested in the omni-directional display system as one of 3D displays. The omni-directional display is good to observe displaying objects by some users. Therefore, a special controller for the omni-directional display is necessary to realize interactive operations from any positions around the display system. So far, we have developed a PDA (personal digital assistants) based controller for the omni-directional display system. However, we consider that users can observe and operate 3D images effectively by using the 2D display properly in addition to the omni-directional display system, as the controller can't display the displaying object on the system. In this research, we developed a new controller with mobile PC to solve the problems, and implemented high performance functions. Moreover, in order to utilize the controller effectively, we created the 3D contents "Vizoo". In this paper, we describe the developed controller and the created contents
[virtual reality, graphical user interfaces, mobile PC, Omni-directional Display System, Control systems, omni-directional display system, interactive controller, mobile computing, computer displays, Virtual reality, Three dimensional displays, interactive devices, Personal digital assistants, personal digital assistants, Two dimensional displays, User Interface, three-dimensional displays, Vizoo 3D content, Digital control, Computer displays, Mobile PC, User interfaces, Control engineering education, Mice, Zoo, Virtual Reality]
Constructing real time simulation using by familiar PC
11th International Conference on Parallel and Distributed Systems
None
2005
Recently, the virtual reality has progressed rapidly in the field of the science and technology. The coverage includes extremely various fields such as communication, design, entertainment, etc. Various VR space have been proposed and constructed by present. However, there are no VR system that realizes real time simulation based on the same physical law as it works in the real world. Then, we construct a virtual air hockey game based on rigid dynamics in this research. Moreover, to provide the presence with users, two sensations (the visual and auditory sensation) are synchronized. This paper describes various problems and their solutions in construction of the system, and utilities for synchronizing the two sensations
[Real time systems, microcomputers, virtual reality, Computational modeling, Object oriented modeling, Educational institutions, real time simulation, digital simulation, Physics, personal computer, Graphics, virtual air hockey game, Space technology, computer games, real-time systems, Virtual reality, Solids, Libraries]
Proposing Internet Analysis for Improvement of Usability on the 3D virtaul shared Space
11th International Conference on Parallel and Distributed Systems
None
2005
Recently many communication systems based on three dimensional spaces are proposed. In those systems, in order to support many users in the same space, QoS control function based on the distance among the avatars is mainly applied. For this reason, the computing and network resources in the communication systems are wasted. In this paper, we suggest a new QoS control to take account of user's interest and status information in addition to the distance among the avatars. Using this method, unnecessary communication and resources can be reduced and more interested communication can be attained with higher priority in the large communication space
[virtual reality, resource allocation, 3D virtual shared space, QoS control function, network resources, Internet, quality of service, multimedia communication system, Usability, multimedia communication, avatars]
Position-Based Keyframe Selection for Human Motion Animation
11th International Conference on Parallel and Distributed Systems
None
2005
This paper proposes a method for keyframe selection of captured motion data. Motion capture systems have been widely used in movies, games and human motion analysis. Most of the previous methods make use of the rotation angles directly and measure a cost of the rotation curves for selection of keyframes. One drawback of these methods is that they do not directly control the positions of the joints in the 3D space. Our method proposes a position-based keyframe detection scheme. In our framework, frames are decimated one by one with measuring the positions of all the joints. We make use of the cost that is the sum of all the joint position differences. Then, a frame with the lowest cost is decimated after measuring those costs about frames. We demonstrate it in experimental section by several typical motions
[Costs, Humans, image motion analysis, position-based keyframe selection, computer animation, human motion animation, Goniometers, captured motion data, Position measurement, Animation, Motion pictures, Skeleton, Joints, Motion analysis, Rotation measurement]
The Method to Evaluate a Similarity of Two Dimensional Data Using Critical Point Graph
11th International Conference on Parallel and Distributed Systems
None
2005
According to increase in use frequency of computer simulation, the system is needed that classifies a large amount of these results into database and retrieves it. To classifies and retrieves simulation data, it is necessary to evaluate the similarity between characters of data. It has been reported that a method using "critical point graph (CPG)" as an index of data is effective in evaluating. However, a past method using CPG has a question of not corresponding to affine transformation of data. In this paper, we propose the evaluation method corresponding to affine transformation using CPG in two dimensional data. In proposal method, we normalize datasets by using principal component analysis. Actually using the proposal method, we evaluated the similarity between several data. From the result, one can safety state that this method is corresponding to affine transformation
[virtual reality, critical point graph, Computer simulation, Computational modeling, graph theory, visual databases, Information retrieval, Data engineering, digital simulation, Proposals, Databases, database indexing, data indexing, Virtual reality, Frequency, computer simulation, Computer performance, two dimensional data, principal component analysis, Principal component analysis]
Verbal/Nonverbal Communication Permitting User to Communicate with Virtual Environment without Special Instrument
11th International Conference on Parallel and Distributed Systems
None
2005
In this research, a dialog environment between human and virtual environment has been constructed. At preset, if a person wants to interact with virtual environment, special device such a data glove is required, but it makes difficult for general users to manipulate virtual objects. When we cannot manipulate objects directly, it is natural that we ask someone with a privilege to do the operation in place of us. In the case, it is convenient if methods used in daily life are allowed. That is, the method to be proposed here is based on the integration between verbal information through the utterances and non-verbal information by the gestures such as finger pointing. The experimental results have proved the effectiveness of this approach in terms of facilitating man-machine interaction and communication. The environment constructed in this research allows a user to communicate by talking and showing gestures to a personified agent in virtual environment. A user can use his/her finger to point at a virtual object and ask him to manipulate it
[Assembly systems, virtual reality, Virtual environment, Yagi-Uda antennas, Instruments, graphical user interfaces, Humans, Data gloves, Helium, verbal-nonverbal communication, dialog environment, man-machine interaction, gesture recognition, Fingers, Speech recognition, Systems engineering and theory, human computer interaction]
Peer Collaboration Modes and Private Room Services in TOMSCOP
11th International Conference on Parallel and Distributed Systems
None
2005
This paper presents the implementation of peer group management services in a pure P2P groupware framework called TOMSCOP (technology of multiuser synchronous collaboration platform). To support a variety of computer-based collaborative activities and styles, a user in a group can be assigned a role or identity such as a discusser peer, a chair peer, a presenter peer, a player peer, or an observer peer-Based on the above identities and their control service, we developed three collaboration modes, i.e., discussion mode, presentation mode and rotation mode, for smoothly manipulating shared applications in their different usages. Moreover we designed and implemented the public and private room services for users to create suitable collaboration rooms that match theirs privacy requirements. These modes and services not only provide more functions to TOMSCOP users, but also support software developers for easily and flexibly making P2P shared applications with different purposes
[virtual reality, peer group management services, peer-to-peer computing, TOMSCOP groupware framework, Collaborative software, Peer to peer computing, Collaborative tools, private room services, Topology, Application software, Privacy, Technology management, Message passing, multiuser synchronous collaboration platform technology, Collaboration, groupware, Collaborative work, peer collaboration modes]
GeDA-3D Agent Architecture
11th International Conference on Parallel and Distributed Systems
None
2005
This paper presents an agent-architecture useful to generate suitable behaviors to virtual creatures participating in virtual environments created by a declarative description. This architecture is used in a platform to design and run dynamic virtual environments that allows users to generate virtual scenes involving two phases: 1) description of the attributes and intentions of the characters participants throughout a declarative description, and 2) graphical simulation of a dynamic scene during which the characters interact with each other in order to achieve individual and/or collective goals
[Multiagent systems, virtual reality, Virtual environment, graphical user interfaces, Displays, software architecture, GeDA-3D agent architecture, Layout, Character generation, mobile agents, virtual environment, Rendering (computer graphics), virtual creatures]
Level of Detail Control for Texture on 3D Maps
11th International Conference on Parallel and Distributed Systems
None
2005
This paper proposes a method to control texture resolution levels for large-scale 3D maps of urban areas. Recently, performance of computers has been developed rapidly, but it is still hard to directly handle the large-scale 3D maps on PCs, because of its high requirement for hardware capability. To render it, the 3D data reduction is needed during rendering process. On the 3D maps, texture data, in general, tend to be far larger than geometry data. So reduction of rendering texture data by using the LOD (level of detail) is effective to reduce whole data size. To achieve that, we propose a method to control the resolution level of texture. In this method, appropriate texture levels are decided offline according to their rendered sizes on a display and importance of the texture images. This determination is done in advance to reduce processing load during rendering process
[LOD, Virtual environment, urban area, Urban planning, level-of-detail control, computational geometry, Displays, cartography, Application software, image texture, town and country planning, 3D map texture resolution, Geometry, data reduction, Rendering (computer graphics), Hardware, Large-scale systems, rendering process, Size control, Personal communication networks, rendering (computer graphics), image resolution]
A Study of data reduction method with data accuracy for triangle data
11th International Conference on Parallel and Distributed Systems
None
2005
We propose a data reduction technique with data accuracy for large polygonal data or iso-surface data. We utilize a voxel data structure to keep the original shape. Our decimation process checks each triangle inside the reduction width that user specified. We also improved the quality by moving the vertex position in the direction of the original model by using direction vector. We show the results of our techniques, and evaluated the effectiveness
[Shape, voxel data structure, computational geometry, accuracy, triangle data, decimation, polygonal data, Information science, data reduction, Computed tomography, Computer graphics, Laser modes, data structures, edge detection, Data communication, data accuracy, isosurface data, data reduction method, decimation process, Data structures, vectors, voxelc data structure, Magnetic resonance imaging, direction vector, Character generation, Rendering (computer graphics), distance image]
Virtual Object Reconstruction in Real World
11th International Conference on Parallel and Distributed Systems
None
2005
A large majority of the communication on the tele-immersive environment does not include a real objective presentation. This paper introduces a method of reconstructing a real object to the communication on the tele-immersive environment by using toy blocks. As the similar but an opposite direction approach, our method consists of three stages: (1) 3D object reconstruction using a 3D modeling system with multi-viewpoint cameras; (2) block arrangement; and (3) block construction. In our knowledge, our proposal is the first approach to realize the virtual objects to the real. In this paper we describe our new realization method with using toy blocks and reveal the immanent difficulties to achieve virtual objects reconstruction in real world
[object recognition, virtual reality, toy block, 3D modeling system, tele-immersive environment, Humans, multiviewpoint camera, image reconstruction, Proposals, Printers, Image reconstruction, real world, block construction, Communication industry, Prototypes, virtual object reconstruction, User interfaces, Cameras, Hardware, Computer performance, block arrangement, 3D object reconstruction, solid modelling]
3D visualization techniques and its usefulness of eyeground images data sets generated by Optical Coherence Tomography
11th International Conference on Parallel and Distributed Systems
None
2005
Tomogram of eyeground images by optical coherence tomography (OCT) are a set of the echo information on a minute portion by the scanning beam. These images contain many random noises and gap between images. Images of OCT are strongly influenced of the motion-induced artifacts during the image acquisition, pulsation by the living body organization. Therefore, it is difficult to form 3D volume images. In this research, a noise removal of 2D images and position alignment processing were performed as a pretreatment of 3D visualization. 3D images contribute to medical treatment of eyeground disease and informed consent. We utilized a texture-based volume rendering technique, and evaluated the usefulness of the 3D eyeground model in the medical treatment of eyeground disease
[light coherence, OCT, Biomedical optical imaging, medical treatment, Noise reduction, optical coherence tomography, Retina, optical tomography, position alignment processing, living body organization, patient treatment, data visualisation, 3D visualization technique, Tomography, Aging, texture-based volume rendering technique, rendering (computer graphics), medical image processing, motion-induced artifact, image pulsation, Medical treatment, Inspection, diseases, echo information, image acquisition, eyeground disease, image denoising, image texture, Diseases, eye, 2D image, Data visualization, Image generation, 3D volume image, eyeground image data set]
Using a Declarative Language to Describe the Interactions in Virtual Scenes
11th International Conference on Parallel and Distributed Systems
None
2005
A generic platform useful to design and run virtual scenes as a result of a human-like description is introduced. The present article describes the operation of one of the main components of such platform: the virtual-environment (VE) editor. The main goal of the VE editor is to allow a human description of a virtual scene and translate it, if errors-free, into a set of low-level commands. The modeler is provided a declarative language useful to: a) arrange the virtual entities using geometric constraints instead of geometrical data, b) assign personalities to virtual humans, and c) specify goals to be fulfilled by virtual humans. A scene takes place in a specific world. For every different world to model a set of rules must be defined. The VE editor allows the specification of some features related to a particular virtual world, including the following: actions and skills that can be performed, virtual entities acceptable, pre-and post- conditions for interactions, effect of natural laws (gravity, collisions), and a set of rules, which reject or allow the assignment of goals according to the state of virtual humans
[Real time systems, virtual entity, Solid modeling, natural scenes, Multiagent systems, Uncertainty, Humans, declarative language, human-like description, computational geometry, geometric constraint, virtual world, visual languages, virtual scene, Layout, Virtual reality, Computer graphics, Distributed control, virtual-environment editor, rendering (computer graphics), virtual human, Gravity, avatars, VE editor]
Message from the SNDS05 Chairs
11th International Conference on Parallel and Distributed Systems
None
2005
Presents the welcome message from the conference proceedings.
[]
SNDS05 Organizing Committee
11th International Conference on Parallel and Distributed Systems
None
2005
Provides a listing of current committee members.
[]
Generating authentication data without keeping a private key for Mobility
11th International Conference on Parallel and Distributed Systems
None
2005
In the near future, people will wish to access many kinds of heterogeneous networks to use their services anytime and anywhere. Owing to the heterogeneity of networks, there must be many kinds of protocols to guarantee secure services. The mobile device can depend on a middleware for accessing services in the heterogeneous networks and the middleware helps the mobile device to communicate with services without knowing concrete protocols. If a secure channel is necessary, the middleware may access a private key in the mobile device to perform a security protocol. In this paper, we focus on the security of a private key in the mobile device against malicious middlewares. To do so, we introduce two models for a user to protect his/her private key against malicious middlewares by generating authentication data (e.g., digital signatures) without keeping the private key in the mobile device
[security protocol, Java, Data security, Access protocols, Mobile communication, cryptography, mobility management (mobile radio), Middleware, private key, malicious middleware, Computer science, mobile device, mobile computing, digital signature, heterogeneous network, Authentication, message authentication, computer crime, Concrete, Computer networks, authentication data generation, protocols, Digital signatures, middleware]
Query Forwarding Algorithm Supporting Initiator Anonymity in GNUnet
11th International Conference on Parallel and Distributed Systems
None
2005
Anonymity in peer-to-peer network means that it is difficult to associate a particular communication with a sender or a recipient. Recently, anonymous peer-to-peer framework, called GNUnet, was developed. A primary feature of GNUnet is resistance to traffic-analysis. However, Kugler analyzed a routing protocol in GNUnet, and pointed out traceability of initiator. In this paper, we propose an alternative routing protocol applicable in GNUnet, which is resistant to Kugler's shortcut attacks
[telecommunication security, initiator anonymity, peer-to-peer computing, Peer to peer computing, Telecommunication traffic, query forwarding algorithm, routing protocol, Kugler shortcut attack, cryptography, Electric resistance, Information science, Intelligent networks, GNUnet, Intrusion detection, routing protocols, computer crime, Bismuth, Routing protocols, Cryptography, peer-to-peer network, Testing]
Supporting E.cient Authorization in Delegation with Supervision
11th International Conference on Parallel and Distributed Systems
None
2005
Delegation is commonly used in organizations to transfer some permission by one user to another user. However, most existing delegation schemes do not support supervision, which allows the delegators to retain control over how the delegated permission can be exercised. In this paper, we describe how to support efficient authorization in delegation with supervision using proxy signature techniques
[Software maintenance, Technological innovation, proxy signature technique, Programming, delegation scheme, Application software, Software development management, Authorization, Computer science, authorization, Councils, authorisation, Permission, digital signatures, protocols, Human resource management]
UNIDES: An Efficient Real-Time System to Detect and Block Unauthorized Internet Access
11th International Conference on Parallel and Distributed Systems
None
2005
In the audit process of Internet activities of users in a local area network, one of the most difficult problems is the problem of source authentication i.e. establishing securely the identity of the owner of the records. The problem cannot be solved by user authentication only because of the inherited security deficiencies in the structure of Internet protocols. To achieve secure and reliable source authentication, we propose an efficient proxy based architectural solution called UNIDES which detects and blocks unauthorized Internet access in realtime
[Real time systems, telecommunication security, proxy based architectural solution, network security., local area networks, authorisation, computer crime, source authentication, user authentication, UNIDES, IP networks, Protection, real-time system, Local area networks, security management, Data security, Internet protocol, Access protocols, audit process, TCP/IP security, Computer science, Real-time communication security, unauthorized Internet access, transport protocols, Authentication, Information security, message authentication, real-time systems, local area network, Internet]
Path Analysis: Detection of Triangle Routing Attacks in IPv6
11th International Conference on Parallel and Distributed Systems
None
2005
Triangle routing is one of the serious attacks to the Internet infrastructure. It can be caused by malicious routers which misroute packets to wrong directions. This kind of attacks creates network problems such as network congestion, denial of service and network partition and results in degrade of network performance. This paper gives a comprehensive study on how the path analysis combats the triangle routing attacks. We discuss the method, implementation and limitation of path analysis to detect triangle routing in IPv4 network. We also discuss the implementation of path analysis in IPv6 by proposing a new extension header
[Availability, path analysis, Data security, IPv6, malicious router, Routing, Computer crime, Degradation, Intelligent networks, IPv4 network, Information security, Authentication, telecommunication network routing, computer crime, triangle routing attack detection, Internet, IP networks, Protection]
A Differentiated Message Delivery Architecture to Control Spam
11th International Conference on Parallel and Distributed Systems
None
2005
Unsolicited bulk electronic mail (spam) is increasingly plaguing the Internet email system and deteriorating its value as a convenient communication tools. In this paper we argue that the difficulties in controlling spam can be attributed to the lack of receiver control on how different email messages should be delievered on the Internet. In the current email delivery architecture, a user can send messages to another at will, regardless of whether or not the latter is willing to accept the message. Based on this observation, we propose a differentiated message delivery architecture - DiffMail. In DiffMail, a user can classify email senders into multiple classes and handle messages from each class differently. For example, although a receiver may directly accept messages from the regular correspondents, he may selectively ask other senders to store messages on the senders' own mail servers, and pull the messages only if and when he wants to. In this paper we present the DiffMail architecture and illustrate some of the appealing advantages using real-world email archives
[message passing, email delivery architecture, Unsolicited electronic mail, Communication system control, Control systems, unsolicited e-mail, DiffMail architecture, Knowledge management, Electronic mail, Postal services, differentiated message delivery architecture, Computer science, message authentication, Computer architecture, unsolicited bulk electronic mail, Internet, Web server, Internet email system]
The Security Requirement for off-line E-cash system based on IC Card
11th International Conference on Parallel and Distributed Systems
None
2005
An offline E-cash system is presented that offers appreciably greater security and better privacy than currently considered E-cash system with similar functionality. Most off-line E-cash systems use the temper-resistant IC card which controls an E-cash issued by the card issuer. Offline E-cash system based on IC card has the threats of overspending, double spending, forgery E-cash, altering/eavesdropping transaction contents, etc. To prevent the above threats, there have been a lot of technical discussions of the security requirements for theoretical offline E-cash protocols based on IC card. However, there has been little attention paid to the security requirements for practical offline E-cash system based on IC card including entity authentication, key management, implementation of cryptographic algorithm, etc. Thus, this paper describes the security requirements for cryptographic algorithms, integrity for implementation of cryptographic algorithm, authentication module, key management, and E-cash protocols
[smart cards, Control systems, Mathematics, key management, electronic money, Privacy, security requirement, E-cash protocol, Forgery, Cryptography, National security, E-cash system, authentication module, off-line E-cash system based on IC Card, cryptography, IC card system, Cryptographic protocols, Payment system, Authentication, Information security, message authentication, eavesdropping transaction content, Load management, offline E-cash system, cryptographic algorithm]
An Efficient and Practical Defense Method Against DDoS Attack at the Source-End
11th International Conference on Parallel and Distributed Systems
None
2005
Distributed Denial-of-Service (DDoS) attack is one of the most serious threats to the Internet. Detecting DDoS at the source-end has many advantages over defense at the victim-end and intermediate-network. One of the main problems for source-end methods is the performance degradation brought by these methods, which discourages Internet service providers (ISPs) to deploy the defense system. We propose an efficient detection approach, which only requires limited fixed-length memory and low computation overhead but provides satisfying detection results. The low cost of defense is expected to attract more ISPs to join the defense. The experiments results show our approach is efficient and feasible for defense at the source-end
[telecommunication security, distributed Denial-of-Service attack, defense method, Internet service provider, Telecommunication traffic, Data structures, source-end method, Information filtering, quality of service, Helium, Computer crime, Distributed computing, Degradation, security of data, Web and internet services, Information filters, Internet, Monitoring]
Delegatable Access Control for Fine-Grained XML
11th International Conference on Parallel and Distributed Systems
None
2005
The access control mechanisms are critical to ensure security in XML (extensible markup language). Several such mechanisms have been used or proposed; however, the notion of delegation in XML has not been studied in the literature. In this paper, we propose an access control model encapsuling delegation authorization rules for XML documents that allow flexible data granularity and limited inference protection. Our access control policy specification is basically DTD-based. It can also be considered to be document-based
[Access control, Tree data structures, fine-grained XML document, encapsuling delegation authorization rules, Data security, Standardization, Authorization, Markup languages, access control model, XML, authorisation, Permission, DTD-based policy, tree data structures, extensible markup language, Australia, Protection, data encapsulation]
Separable and Anonymous Identity-Based Key Issuing
11th International Conference on Parallel and Distributed Systems
None
2005
In identity-based (ID-based) cryptosystems, a local registration authority (LRA) is responsible for authentication of users while the key generation center (KGC) is responsible for computing and sending the private keys to users and therefore, a secure channel is required. For privacy-oriented applications, it is important to keep in secret whether the private key corresponding to a certain identity has been requested. All of the existing ID-based key issuing schemes have not addressed this anonymity issue. Besides, the separation of duties of LRA and KGC has not been discussed as well. We propose a novel separable and anonymous ID-based key issuing scheme without secure channel. Our protocol supports the separation of duties between LRA and KGC. The private key computed by the KGC can be sent to the user in an encrypted form such that only the legitimate key requester authenticated by LRA can decrypt it. and any eavesdropper cannot know the identity corresponding to the secret key
[identity-based cryptosystem, Technological innovation, anonymous identity-based key issuing scheme, privacy-oriented application, Identity-based encryption, Protocols, local registration authority, Security, Computer science, Councils, public key cryptography, Public key, Authentication, message authentication, key generation center, authorisation, Public key cryptography, data privacy]
Group key distribution scheme for reducing required rekey message size
11th International Conference on Parallel and Distributed Systems
None
2005
Generally, systems, such as the pay-per-view TV, require secure multi-party communication. In these systems, group key is required, and members participate in and leave from party frequently, so that, group key materials of all members of the network must be updated. Therefore, an approach which tries to distribute a key materials effectively is proposed from Onen-Molva, the approach divides the members of the network into the two sub-groups according to the relative length in the period of the operation of the each member, and the approach uses FEC (forward error correction) and an ARQ (automatic repeat request) in the key update to the group of the long network-connection period. There are some issues in their proposal. When the communication quality is good, its efficiency is less, and there is an overhead in the bandwidth. Therefore, we propose an efficient group key distribution method using M-ary coding for a key message without using FEC and an ARQ, and transmitting this in parallel with the non-key message
[Tree data structures, telecommunication security, TV, M-ary coding, Scalability, forward error correction, automatic repeat request, Proposals, group key distribution method, Information technology, Information science, public key cryptography, secure multiparty communication, rekey message size, Bandwidth, Forward error correction, Automatic repeat request, Personal digital assistants]
An Efficient Distributed Key Generation Protocol for Secure Communications with Causal Ordering
11th International Conference on Parallel and Distributed Systems
None
2005
In this paper, we propose an elliptic curve based distributed key generation protocol in communication systems with causal ordering semantics on broadcast messages
[telecommunication security, Protocols, message passing, elliptic curve, broadcast message, Galois fields, secure communication, Variable structure systems, Computer science, Interpolation, distributed key generation protocol, Elliptic curves, public key cryptography, Broadcasting, Elliptic curve cryptography, Public key cryptography, Polynomials, protocols]
Key Agreement for Heterogeneous Mobile Ad-Hoc Groups
11th International Conference on Parallel and Distributed Systems
None
2005
In this paper we propose an efficient key agreement protocol suite for heterogeneous mobile ad-hoc groups, whose members use mobile devices with different performance limitations, e.g., laptops, PDAs, and mobile phones. Absence of a trusted central authority in ad-hoc groups requires contributory computation of the group key by interacting members. We introduce a performance ratio parameter to quantify the performance of a mobile device. Our protocols are based on elliptic curve cryptography (ECC) to achieve better computation efficiency and are proven secure
[telecommunication security, Portable computers, Nominations and elections, Mobile communication, Mobile handsets, Cryptographic protocols, mobile device, mobile computing, mobile communication, public key cryptography, Authentication, Information security, Elliptic curve cryptography, heterogeneous mobile ad-hoc group, ad hoc networks, protocols, Personal digital assistants, key agreement protocol, Mobile computing, elliptic curve cryptography]
REAL TIME DIGITAL VIDEO WATERMARKING FOR DIGITAL RIGHTS MANAGEMENT VIA MODIFICATION OF VLCS
11th International Conference on Parallel and Distributed Systems
None
2005
Digital watermarking is a data hiding technology that can be used for digital rights management. It embeds an invisible signal including owner identification and copy control information into multimedia data for copyright protection against illegal copying and distribution. In this paper, we embed the watermark in I-pictures of the MPEG video sequence by modifying VLCs (variable length codes) directly to avoid inverse DCT (discrete cosine transform) and inverse quantization. The modification of VLCs is intended to minimize the perceptual degradation of video quality caused by the embedded watermark. Because we select VLCs with the same run value and code word length to be modified, the compressed video stream does not increase in size. Furthermore, our proposed watermarking scheme protects the embedded information by means of a secret key and a PRNG (pseudo random number generator) to generate a random binary sequence for watermarking. We apply three MPEG-1 video sequences to test our proposed algorithm. Since the watermark can be extracted by simple table lookup without using original video sequences, the computational overhead is very small. Therefore, the decoding of the watermarked video sequence can be real time for viewing. The proposed algorithm can be easily applied to MPEG-2 and MPEG-4 videos also for digital rights management
[digital rights management, Watermarking, Quantization, multimedia systems, random number generation, data hiding technology, table lookup, discrete cosine transform, Technology management, variable length codes, embedded systems, Video compression, Copyright protection, Data encapsulation, video streaming, Discrete cosine transforms, Random number generation, discrete cosine transforms, multimedia data, Video sequences, MPEG-1 video sequences, inverse quantization, watermarking, copyright protection, Signal processing, real time digital video watermarking, data encapsulation, pseudo random number generator]
Sharing a Secret Two-Tone Image in Two Gray-Level Images
11th International Conference on Parallel and Distributed Systems
None
2005
In this paper, we shall propose two spatial-domain image hiding schemes with the concept of secret sharing. The two new schemes use the two-out-of-two visual secret sharing technique to generate two shares for hiding a secret two-tone image. These two secret shares are embedded into two gray-level cover images by the proposed embedding scheme. To decode the hidden messages, we can superimpose the extracted shares from the secret-share-carrier images (namely the embedding images). The advantages of our scheme are simple computation and good security, and thus it is very suitable for applications involving low power verification systems. Besides, our scheme can support two participants to share one secret two-tone image. According to our experimental results, the two proposed schemes are capable of offering satisfactory embedding image quality
[data encryption, Digital images, low power verification system, visual secret sharing technique, cryptography, Decoding, Power system security, gray-level images, Computer science, Image quality, Steganography, image quality, image enhancement, feature extraction, spatial-domain image hiding scheme, Computer networks, Cryptography, Data communication, Protection, data encapsulation, image coding, visual cryptography]
Universal Designated Multi Verifier Signature Schemes
11th International Conference on Parallel and Distributed Systems
None
2005
The notion of universal designated-verifier signatures was put forth by Steinfeld et. al. in Asiacrypt 2003. This notion allows a signature holder to designate the signature to a desired designated-verifier. In this paper, we extend this notion to allow a signature holder to designate the signature to multi verifiers, and hence, we call our scheme as universal designated multi verifier signatures. We provide security proofs for our schemes based on the random oracle model
[security proof, Licenses, cryptography, biometrics (access control), Voltage control, Information technology, Cryptographic protocols, Computer science, Privacy, Information security, bilinear pairing, Public key cryptography, random oracle model, digital signatures, Australia, Digital signatures, universal designated multiverifier signature]
GBR: Grid Based Random Key Predistribution for Wireless Sensor Network
11th International Conference on Parallel and Distributed Systems
None
2005
Resource constraints of sensor nodes make infeasible to use traditional key management techniques such as public key cryptography for security service of distributed sensor network. Pairwise key establishment is one of the efficient ways to achieve security service. To alleviate the study of novel pairwise key predistribution techniques, this paper presents an efficient framework for establishing pairwise keys based on polynomial key pre-distribution adopting the idea of random key predistribution scheme and grid based scheme. We named our scheme as grid based random (GBR) scheme. The concept and analysis of this scheme explains its better improved resilience to node capture than the existing schemes along with a very high probability to establish pairwise keys between nodes in an efficient way
[telecommunication security, wireless sensor networks, grid computing, Key pre-distribution, wireless sensor network, Security, Distributed computing, grid based random key predistribution, sensor nodes, Engineering management, public key cryptography, Polynomials, Computer networks, Computer security, polynomial key predistribution, distributed sensor network, data security, resource constraint, resiliency, Resiliency., Resilience, Wireless sensor network, Wireless sensor networks, Public key cryptography, Computer network management, Resource management]
Message from the HiPCoMB-2005 Chairs
11th International Conference on Parallel and Distributed Systems
None
2005
Presents the welcome message from the conference proceedings.
[]
HiPCoMB-2005 Organizing Committee
11th International Conference on Parallel and Distributed Systems
None
2005
Provides a listing of current committee members.
[]
A Parallel Implementation of 2-D/3-D Image Registration for Computer-Assisted Surgery
11th International Conference on Parallel and Distributed Systems
None
2005
This paper presents the design and implementation of a parallel two-dimensional/three-dimensional (2-D/3-D) image registration method for computer-assisted surgery. Our method exploits data and speculative parallelism, aiming at making computation time short enough to carry out registration tasks during surgery. Our experiments show that exploiting both parallelisms reduces computation time on a cluster of 64 PCs from a few tens of minutes to less than a few tens of seconds
[Image registration, computer-assisted surgery, Two dimensional displays, image registration, Optical imaging, X-ray imaging, parallel programming, Concurrent computing, Ultrasonic imaging, Computed tomography, data parallelism, Surgery, Parallel processing, Robustness, speculative parallelism, medical image processing, surgery]
Rotational and Translational Alignment Errors in 3D Reconstruction of Virus Structures at High Resolution
11th International Conference on Parallel and Distributed Systems
None
2005
3D reconstruction of virus structures at high resolution using CryoTEM data requires a very accurate rotational and translational alignment of individual views obtained experimentally. We discuss the geometrical foundations and the computational problems raised by rotational and translational alignment. We also outline the basic ideas for CTF correction
[Discrete Fourier transforms, Reconstruction algorithms, computational geometry, image reconstruction, Electron microscopy, Image reconstruction, Computer science, rotational alignment error, Electron beams, 3D reconstruction, CryoTEM data, translational alignment error, biology computing, microorganisms, Computer errors, Genetics, transmission electron microscopy, virus structure, CTF correction, Spatial resolution, Viruses (medical), image resolution]
Multiple Sequence Alignment on an FPGA
11th International Conference on Parallel and Distributed Systems
None
2005
Molecular biologists frequently compute multiple sequence alignments (MSAs) to identify similar regions in protein families. Progressive alignment is a widely used approach to compute MSAs. However, aligning a few hundred sequences by popular progressive alignment tools requires several hours on sequential computers. Due to the rapid growth of biological sequence databases biologists have to compute MSAs in a far shorter time. In this paper we present a new approach to MSA on reconfigurable hardware platforms to gain high performance at low cost. To derive an efficient mapping onto this type of architecture, fine-grained parallel processing elements (PEs) have been designed. Using this PE design as a building block we have constructed a linear systolic array to perform a pairwise sequence distance computation using dynamic programming. This results in an implementation with significant runtime savings on a standard off-the-shelf FPGA
[Costs, field programmable gate arrays, FPGA, Performance gain, systolic arrays, sequences, parallel processing, Proteins, Databases, biology computing, reconfigurable architectures, protein family, progressive alignment, proteins, Computer architecture, reconfigurable hardware, Parallel processing, Hardware, Systolic arrays, multiple sequence alignment, dynamic programming, biological sequence database, Biology computing, Field programmable gate arrays, linear systolic array]
Exploiting Multi-level Parallelism for Homology Search using General Purpose Processors
11th International Conference on Parallel and Distributed Systems
None
2005
New biological experimental techniques are continuing to generate large amounts of data using DNA, RNA, human genome and protein sequences. The quantity and quality of data from these experiments makes analyses of their results very time-consuming, expensive and impractical. Searching on DNA and protein databases using sequence comparison algorithms has become one of the most powerful techniques to better understand the functionality of particular DNA, RNA, genome, or protein sequence. This paper presents a technique to effectively combine fine and coarse grain parallelism using general purpose processors for sequence homology database searches. The results show that the classic Smith-Waterman sequence alignment algorithm achieves super linear performance with proper scheduling and multi-level parallel computing at no additional cost
[multiprocessing systems, homology database search, Smith-Waterman sequence alignment algorithm, RNA, Genomics, Humans, multilevel parallel computing, sequences, parallel processing, Scheduling algorithm, general purpose processor, Databases, Processor scheduling, genetics, biology computing, DNA, proteins, Parallel processing, human genome, scheduling, Protein sequence, Bioinformatics, protein sequence]
Concurrent Numerical Simulation of Flow and Blood Clotting using the Lattice Boltzmann Technique
11th International Conference on Parallel and Distributed Systems
None
2005
In this paper we describe a novel approach for a concurrent numerical simulation of the unsteady flow within an idealised stenosed artery and a simplified blood clotting process based on a residence time model. The applied numerical scheme is the lattice Boltzmann technique, which proved to be highly efficient particularly for transient flows and complex or varying geometries
[transient flow, concurrent numerical simulation, lattice Boltzmann technique, Computational modeling, Laboratories, Coagulation, idealised stenosed artery, blood clotting process, Blood, Arteries, Boltzmann machines, Lattice Boltzmann methods, haemorheology, Geometry, biology computing, numerical analysis, Numerical simulation, Cardiovascular diseases, unsteady blood flow, residence time model, Distribution functions]
Protein Fold Recognition by Mixed Environment-Specific Amino Acid Substitution Mapping Using Neural Networks
11th International Conference on Parallel and Distributed Systems
None
2005
Protein fold recognition programs align a probe amino acid sequence onto a library of representative folds of known structure to identify structural similarities. Substitution matrix is the key to detect the aligned protein similarities. In this paper, a new mixed environment-specific substitution mapping (MESSM) is designed for fold recognition. It has two features: first, with amino acid residue level environmental description, structurally-derived substitution scores are generated using neural networks. The substitution probability of each pair of amino acids at any chosen structural environment can be instantly generated; second, the structurally-derived substitution score is linearly combined with sequence profile from traditional sequences substitution matrices to obtain a positive consensus for fold recognition. By fitting a single parameter in the combined substitution score, benchmark problems have been carried out to test the ability of the MESSM model. The results show that the new fold recognition model with mixed substitution mapping has a better performance than the one with either structure or sequence profile only. Moreover, it is comparable with those more computational intensive, energy potential based fold recognition models
[Potential energy, Solvents, structurally-derived substitution score, residue level environmental description, dynamic programming, Amino acids, neural network, matrix algebra, Proteins, mixed environment-specific substitution mapping, biology computing, amino acid sequence, Neural networks, proteins, Benchmark testing, Matrices, Libraries, Computer networks, Probes, neural nets, protein fold recognition, substitution matrix, pattern recognition]
PRec-I-DCM3: A Parallel Framework for Fast and Accurate Large Scale Phylogeny Reconstruction
11th International Conference on Parallel and Distributed Systems
None
2005
Accurate reconstruction of phylogenetic trees very often involves solving hard optimization problems, particularly the maximum parsimony (MP) and maximum likelihood (ML) problems. Various heuristics have been devised for solving these two problems; however, they obtain good results within reasonable time only on small datasets. This has been a major impediment for large-scale phylogeny reconstruction, particularly for the effort to assemble the Tree of Life - the evolutionary relationship of all organisms on earth. Roshan et al. recently introduced Rec-I-DCM3, an efficient and accurate meta-method for solving the MP problem on large datasets of up to 14,000 taxa. Nonetheless, a drastic improvement in Rec-I-DCM3's performance is still needed in order to achieve similar (or better) accuracy on datasets at the scale of the Tree of Life. In this paper, we improve the performance of Rec-I-DCM3 via parallelization. Experimental results demonstrate that our parallel method, PRec-I-DCM3, achieves significant improvements, both in speed and accuracy, over its sequential counterpart
[Algorithm design and analysis, Data analysis, evolution (biological), biological evolution, Organisms, Phylogeny, parallelization, parallel processing, phylogenetic tree reconstruction, Earth, maximum likelihood problem, optimisation, maximum parsimony problem, biology computing, optimization, Biology computing, Polynomials, Large-scale systems, Impedance, Assembly]
Numerical Solutions of Master Equation for Protein Folding Kinetics
11th International Conference on Parallel and Distributed Systems
None
2005
Numerical solution of a master equation involves the calculation of eigenpairs for the corresponding transition matrix. In this paper, we computationally study the folding rate for a kinetics problem of protein folding by solving a large-scale eigenvalue problem. Three numerical methods, the implicitly restarted Arnoldi, the Jacobi-Davidson, and the QR methods are applied to solve the corresponding large scale eigenvalue problem of the transition matrix of master equation. Comparison among three methods is performed in terms of the computational efficiency. It is found that the QR method demands tremendous computing resource when the length of sequence L&gt;10 due to the extremely large size of matrix and CPU time limitation. The Jacobi-Davidson method may encounter convergence issue, for some testing cases with L&gt;9. The implicitly restarted Arnoldi method is suitable for solving the problem among three solution methods. Numerical examples with various residues are investigated
[implicitly restarted Arnoldi method, Computational modeling, Lattices, Sparse matrices, protein folding kinetics, Equations, Nanoelectronics, eigenvalues and eigenfunctions, matrix algebra, Proteins, Jacobian matrices, numerical solution, master equation, biology computing, transition matrix, QR method, proteins, Eigenvalues and eigenfunctions, Kinetic theory, Jacobi-Davidson method, eigenpair, Testing, eigenvalue problem]
Message from the HWISE-2005 Chairs
11th International Conference on Parallel and Distributed Systems
None
2005
Presents the welcome message from the conference proceedings.
[]
HWISE-2005 Organizing Committee
11th International Conference on Parallel and Distributed Systems
None
2005
Provides a listing of current committee members.
[]
A Two-Level Strategy for Topology Control in Wireless Sensor Networks
11th International Conference on Parallel and Distributed Systems
None
2005
This paper presents a two-level strategy for topology control in wireless sensor networks. The energy saving methods in most of the existing research work can be categorized into two types: active subnetwork and short hops. The active subnetwork and short hops methods have different network conditions for them to perform well. We propose a two-level topology control strategy which integrates the above two methods to achieve further energy saving. The simulation results show that the two-level topology control strategy achieves better performance in terms of energy saving than both active subnetwork and short hops methods in networks with a wide range of node densities
[Algorithm design and analysis, active networks, Energy consumption, wireless sensor networks, short hops method, Heuristic algorithms, Spine, telecommunication network topology, two-level topology control strategy, Computer science, Intelligent networks, Wireless sensor networks, Network topology, energy saving methods, energy conservation, Energy efficiency, active subnetwork, Power generation]
Avoiding the Bottlenecks in the MAC Layer in 802.15.4 Low Rate WPAN
11th International Conference on Parallel and Distributed Systems
None
2005
We analyze the operation of the MAC layer of an IEEE 802.15.4 compliant network cluster in the beacon enabled mode with bidirectional traffic. We identify certain issues in the standard that lead to serious performance limitations, and suggest some simple modifications of the coordinator function that allows the network to handle higher traffic loads
[workstation clusters, Telecommunication traffic, performance evaluation, Throughput, Batteries, access protocols, bidirectional traffic loads, WPAN, Degradation, Computer science, Intelligent networks, Wireless sensor networks, personal area networks, Network topology, Wireless personal area networks, IEEE 802.15.4, Energy efficiency, IEEE standards, network cluster, wireless LAN, wireless personal area networks, telecommunication traffic, MAC layer]
A Configurable Time-Controlled Clustering Algorithm for Wireless Sensor Networks
11th International Conference on Parallel and Distributed Systems
None
2005
Future large-scale sensor networks may comprise thousands of wirelessly connected sensor nodes that could provide an unimaginable opportunity to interact with physical phenomena in real time. These nodes are typically highly resource-constrained. Since the communication task is a significant power consumer, there are various attempts to introduce energy-awareness at different levels within the communication stack. Clustering is one such attempt to control energy dissipation for sensor data routing. Here, we propose the time-controlled clustering algorithm to realise a network-wide energy reduction by the rotation of clusterhead role, and the consideration of residual energy in its election. A realistic energy model is derived to accurately quantify the network's energy consumption using the proposed clustering algorithm
[workstation clusters, wireless sensor networks, Communication system control, Sensor phenomena and characterization, sensor fusion, power consumption, clusterhead election, mobile computing, Network topology, sensor data routing, network-wide energy reduction, Clustering algorithms, Spread spectrum communication, Large-scale systems, energy dissipation control, Monitoring, power consumer, Base stations, network energy consumption, time-controlled clustering algorithm, communication stack, Wireless sensor networks, telecommunication network routing, energy conservation, energy-awareness, Australia]
Learning patterns in wireless sensor networks based on wavelet neural networks
11th International Conference on Parallel and Distributed Systems
None
2005
In this paper it is demonstrated how some of the algorithms developed within the artificial neural-networks tradition can be simply adopted to wireless sensor network platforms and still meet most of the requirements for sensor networks. Neural-networks clustering algorithms also provide dimensionality reduction which further leads to lower communication costs and thus bigger energy savings. Two different data aggregation architectures are presented. They both utilize algorithms which apply wavelets for initial data-processing of the sensory inputs at different resolutions. Artificial neural-networks which make use of unsupervised learning methods are used for categorization of the sensory inputs. These architectures are tested on a data obtained from a set of several motes, equipped with several sensors each. Results from simulations of intentionally made defective sensors demonstrate the data robustness of these architectures
[Multidimensional systems, wireless sensor networks, discrete wavelet transforms, wavelet neural-networks, Artificial neural networks, Sensor phenomena and characterization, artificial neural-networks clustering algorithm, unsupervised learning, Computer science, Intelligent networks, Wireless sensor networks, Sociotechnical systems, sensory inputs, data aggregation architecture, unsupervised learning method, pattern clustering, Neural networks, Clustering algorithms, Robustness, data-processing, neural nets, pattern learning]
Evolution of Learning Parameters in a Team of Mobile Agents
11th International Conference on Parallel and Distributed Systems
None
2005
Most of everyday life environments are unknown and dynamic. Therefore, the artificial agents living in such environments must adapt their policy based on the environment conditions. In this paper, we consider a team of mobile agents that learns to survive by capturing the active battery packs. In our method, evolution considered metaparameters of an actor-critic reinforcement learning algorithm. Results show that after some generations the agents were able to survive and increase the energy level. In addition, the evolved metaparameters helped the agent to adapt much faster during the first stage of life and find an important relation between exploration-exploitation and energy level
[mobile agents team, active battery packs, metaparameters., metaparameter evolution, learning, Batteries, Mobile robots, Learning, evolutionary computation, surviving behaviour, Evolution (biology), actor-critic reinforcement learning algorithm, Mobile agents, neurocontrollers, mobile agents, Team of agents, Chromium, Acoustic sensors, Robot sensing systems, Infrared sensors, artificial agents, Energy states, learning (artificial intelligence)]
A Simple Two-Dimensional Coded Detection Scheme in Wireless Sensor Networks
11th International Conference on Parallel and Distributed Systems
None
2005
Recently an approach that combined the techniques in distributed classification and the ones in error-correcting codes has been proposed to design a fault-tolerant classification system in wireless networks. The codeword implemented at the fusion center is assigned based on each phenomenon and each detection result of every sensor. A local decision represented by one bit is made according to the codeword and the detection result. Thus, the length of the codeword designed is equal to the number of sensors. In this paper, we propose a new distributed detection scheme combined with two-dimensional channel coding. Multiple detections are conducted by each sensor for the same phenomenon. A two-dimensional code that combines the designed code and a repetition code is implemented at the fusion center for decoding. The virtual code length of this two-dimensional code is the code length of the designed code multiplied with the number of detections conducted at each sensor. Compared with the previous methods, the new scheme has higher classification reliability and lower sensor complexity
[sensor complexity, error correction codes, wireless sensor networks, channel coding, distributed detection scheme, Sensor phenomena and characterization, Electrical fault detection, sensor fusion, Decoding, two-dimensional coded detection scheme, two-dimensional channel coding, Intelligent networks, Wireless sensor networks, Design engineering, error-correcting codes, Fault tolerant systems, fusion center, fault tolerant computing, codeword length, Error correction codes, Optical noise, distributed fault-tolerant classification system, virtual code length, Signal to noise ratio]
WISER: Cooperative Sensing Using Mobile Robots
11th International Conference on Parallel and Distributed Systems
None
2005
This paper presents an overview of WISER which is a communication architecture for multiple mobile robots with short-range wireless links aiming at cooperative sensing. WISER deals with several features which were not observed in conventional wireless ad hoc networks: intentional mobility and allowance for disconnection. In this paper we describe the design and the prototype implementation of WISER
[Actuators, wireless sensor networks, WISER cooperative sensing, wireless ad hoc networks, multi-robot systems, Mobile communication, multiple cooperated robots, mobile robots, Mobile robots, communication architecture, Delay, Mobile ad hoc networks, Wireless communication, Wireless sensor networks, short-range wireless links, Prototypes, Robot sensing systems, Error correction, ad hoc networks]
A Hybrid Wireless Network Approach to Support QoS Data Transmission
11th International Conference on Parallel and Distributed Systems
None
2005
Wireless sensor networks are emerging as a popular research issue and tool for many applications. However, nodes in a sensor network are severely constrained by energy, storage capacity and computing power. In this paper, we propose an integrated network environment (sensor and ad hoc network) with IEEE 802.15.4a, ultra-wideband (UWB) and two-tier data dissemination (TTDD) routing protocol. We wish that sensor network could support fast dynamic location-tracking, information gathering and response. And base station use theses information to compute an optimal path for ad hoc mobile devices to offer good QoS level and reliable data transmission according to different requests
[ad hoc mobile devices, wireless sensor networks, ultra wideband communication, Capacitive sensors, tracking, Wireless networks, dynamic location-tracking, ultra-wideband, Computer networks, Routing protocols, IEEE standards, Data communication, base station, Ultra wideband technology, two-tier data dissemination routing protocol, Base stations, paged storage, ad hoc network, Ad hoc networks, IEEE 802.15.4a, quality of service, Wireless sensor networks, integrated network environment, routing protocols, QoS data transmission, storage capacity, ad hoc networks, wireless LAN, information gathering, Energy storage]
Routing through Backbone Structures in Sensor Networks
11th International Conference on Parallel and Distributed Systems
None
2005
Virtual infrastructures or backbones in wireless sensor networks reduce the communication overhead and energy consumption. In this paper, we present backbone routing (BBR), a novel fully distributed protocol for construction and rotation of backbone networks. BBR reduces energy consumption without significantly diminishing the capacity or connectivity of the network. Another key feature of BBR is its energy balancing nature by distributing the role of being backbone node among all the nodes. BBR builds on the observation that when a region of a shared-channel wireless network has a sufficient density of nodes, only a small number of them need be on at any time to forward traffic for active connections. Improvement in system lifetime due to BBR increases as the ratio of idle-to-sleep energy consumption increases, and increases as the density of the network increases. Our experiments show that BBR is more efficient in saving energy and extending network life without deteriorating network performance when compared with 802.11
[Energy consumption, shared-channel wireless network, wireless sensor networks, backbone networks, Spine, Mobile ad hoc networks, Intelligent networks, Network topology, network performance, Wireless networks, Routing protocols, protocols, energy consumption, virtual infrastructures, performance evaluation, telecommunication network topology, backbone routing, distributed protocol, Computer science, Wireless sensor networks, Sleep, distributed algorithms, telecommunication network routing, wireless LAN, telecommunication traffic]
Emergency Broadcast Protocol for Inter-Vehicle Communications
11th International Conference on Parallel and Distributed Systems
None
2005
The most important goal in transportation systems is to reduce the dramatically high number of accidents and fatal consequences. One of the most important factors that would make it possible to reach this goal is the design of effective broadcast protocols. In this paper we present an emergency broadcast protocol designed for sensor inter-vehicle communications and based in geographical routing. Sensors installed in cars continuously gather important information and in any emergency detection raise the need for immediate broadcast. The highway is divided in virtual cells, which moves as the vehicles moves. The cell members choose a cell reflector that behaves for a certain time interval as a base station that handle the emergency messages coming from members of the same cell, or close members from neighbor cells. Besides that the cell reflector serves as an intermediate node in the routing of emergency messages coming from its neighbor cell reflectors and does a prioritization of all messages in order to decide which is the first to be forwarded. After this the message is forwarded through the other cell reflectors. Finally the destination cell reflector sends the message to the destination node. Our simulation results show that our proposed protocol is more effective compared to existing inter-vehicles protocols
[wireless sensor networks, destination cell reflector, cell reflector, Mobile communication, Floods, geographical routing, Road transportation, Wireless networks, road vehicles, Road vehicles, Broadcasting, Routing protocols, base station, inter-vehicle communications, message passing, transportation systems, virtual cells, emergency broadcast protocol, emergency messages, transportation, Computer science, sensors, Vehicle safety, routing protocols, automated highways, ad hoc networks, broadcast channels, Accidents]
Sensor Network Design and Implementation for Health Telecare and Diagnosis Assistance Applications
11th International Conference on Parallel and Distributed Systems
None
2005
The attempts to develop a ubiquitous health care monitoring system arisen from the need of automatic real-time medical services for emergent diseases. Besides, the physiological statuses gathered and maintained by this system are very helpful for diagnosis and early warning. To improve the medical services and diagnosis accuracy, a Wireless Health Advanced Mobile Bio-diagnostic System (abbreviated as WHAM-BioS) is proposed. This study focuses on network/communication technology in the WHAM-BioS and proposes a novel clustered sensor network (CSN) architecture for long-term periodical telecare applications. In the proposed CSN architecture, most network functions are concentrated in a special purpose device called the human body gateway (HBG). The sensor nodes focus on detecting and reporting their detection results to their HBG. To reduce the design complexity and the implementation cost for the sensor nodes, the proposed architecture proposed several protocols to help each HBG to provide a contention free environment for their sensor nodes. The contention free environment significantly reduces the power consumption in data retransmission. Besides, to further reduce the power consumption of the sensor nodes, this study also proposes a power saving mechanism, which reduces the power consumption in idle listening. Based on the proposed network architecture and protocols, a prototype system is implemented
[Real time systems, Energy consumption, Protocols, wireless sensor networks, Medical services, power consumption, mobile computing, health telecare, contention free environment, Communications technology, protocols, Wireless Health Advanced Mobile Bio-diagnostic System, health care, telemedicine, Computerized monitoring, clustered sensor network architecture, Diseases, diagnosis assistance application, Wireless sensor networks, WHAM-BioS, ubiquitous health care monitoring system, Biosensors, human body gateway, Biomedical monitoring, patient diagnosis]
A Smart Ambient Sound Aware Environment for Be Quiet Reminding
11th International Conference on Parallel and Distributed Systems
None
2005
This paper presents a prototype of a smart sound aware environment that captures ambient sounds, analyzes sound situations and gives a be-quiet reminder to someone who generates sound exceeding certain volume level and disturbing others in the same site. It shows the microphone net to sense sound data and the speaker net to send be-quiet reminders. All computers are interconnected with a P2P network atop of the TOMSCOP platform. It discusses how sound data is captured and processed, and when a be-quiet reminder decision is made with analyzing contexts of sounds and users in a room. Some preliminary experimental results are given
[Embedded computing, peer-to-peer computing, TOMSCOP platform, microphone net, P2P network, Intelligent actuators, speaker net, noise pollution, Ambient intelligence, sound data processing, Intelligent sensors, Information analysis, Microphones, Loudspeakers, smart ambient sound aware environment, Prototypes, be-quiet reminder, Computer networks, intelligent sensors, Joining processes, sound data capture]
Low Power Platform for Wireless Sensor Network
11th International Conference on Parallel and Distributed Systems
None
2005
Forming from diversified and mass of sensors, the wireless sensor network (WSN) has become a major field and diversified trend for research gradually in these years for its applications. However, in the wireless environment, the lifetime of network would be limited by the supply of battery devices. Among these, protocol has huge influences to the lifetime of WSN. This paper discusses the advantages of two protocols: routing communication and direct communication and try to use the remaining power of nodes as the determined index to switch between those two protocols which further recommends a new protocol to improve network efficiency. Besides, this paper proposes a self designed sensor node hardware structure with low power and low cost advantages, which would combine the sensor nodes with new protocol into a WSN system with fully functional and human oriented monitoring interface. Via WSN, not only local digital data and image can be obtained, it can also combine with Internet, which can be efficiently used on remote environment control
[human oriented monitoring interface, wireless sensor networks, Humans, Switches, wireless sensor network, Sensor systems, Batteries, Communication switching, power consumption, low power platform, direct communication protocol, Wireless sensor networks, mobile computing, routing communication protocol, Cost function, Routing protocols, Hardware, Internet, remote environment control, Remote monitoring]
A Security Solution of WLAN Based on Public Key Cryptosystem
11th International Conference on Parallel and Distributed Systems
None
2005
The security of WLAN is becoming a bottleneck for its further applications. At present, many standard organizations and manufacturers of WLAN are trying to solve this problem. However, owing to the serious secure leak in IEEE 802.11 standard, it is impossible to utterly solve the problem by simply adding some remedies. Based on the analysis on the security mechanism of WLAN, this paper takes the advantages of latest techniques of WLAN security, and presents a solution to WLAN security. The solution will make preparation for the further combination of WLAN and Internet
[telecommunication security, Wireless LAN, Identity-based encryption, Data security, Wireless application protocol, IEEE 802.11 standard, WLAN security, telecommunication standards, wireless local area network, USA Councils, public key cryptography, Information security, Authentication, Public key cryptography, Internet, IEEE standards, wireless LAN, National security, public key cryptosystem]
Message from the RAMPDS-2005 Chairs
11th International Conference on Parallel and Distributed Systems
None
2005
Presents the welcome message from the conference proceedings.
[]
RAMPDS-2005 Organizing Committee
11th International Conference on Parallel and Distributed Systems
None
2005
Provides a listing of current committee members.
[]
Comparing Persistent Computing with Autonomic Computing
11th International Conference on Parallel and Distributed Systems
None
2005
This paper presents a comparative study to examine the relationship between autonomic computing and persistent computing from the various aspects including motivation problems, ideas, purposes, goals, underlying principles, design methodologies, and architectures
[Computer vision, Design methodology, persistent computing, Security, ubiquitous computing, Distributed computing, Concurrent computing, Engineering management, Computer architecture, Biology computing, autonomic computing, Protection, Software engineering]
A Simulation-Based Framework for Autonomic Web Services
11th International Conference on Parallel and Distributed Systems
None
2005
A possible solution to guarantee critical requirements in Web services designs is the use of an autonomic architecture, able to auto-configure and to auto-tune. This paper presents an innovative approach for the development of self-optimizing autonomic systems for Web services architectures, based on the adoption of a simulation engine for obtaining performance predictions. MAWeS (MetaPL/HeSSE Autonomic Web Services) is a framework whose aim is to support the development of self-optimizing predictive autonomic systems for Web service architectures. It adopts a simulation-based methodology, which allows to predict system performances in different status and load conditions. The predicted results are used for a feedforward control of the system, which self-tunes before the new conditions and the subsequent performance losses are actually observed
[open systems, self tuning, Computational modeling, Service oriented architecture, self-optimizing autonomic system, Predictive models, Control systems, digital simulation, interoperability, MetaPL, feedforward control, HeSSE, Tuning, Engines, performance prediction, Web services, simulation engine, Computer architecture, Performance loss, Large-scale systems, Internet, autonomic Web service architecture]
Management of Event Domains - The MEDiator-Approach
11th International Conference on Parallel and Distributed Systems
None
2005
In this paper we describe our software component MEDiator. Our development is based on OMG's Management of Event Domains specification. It allows the efficient management and simplified operation of different CORBA notification services running concurrently. After a brief introduction into the specifications of the notification service and the management of event domains, we describe their architecture and discuss the most important interfaces. Following, we review the shortcomings of the current specification and delineate our approach to solve the problems that result from these deficiencies
[event domain management, message passing, object-oriented programming, Object oriented modeling, Object Management Group, OMG, CORBA notification service, Topology, formal specification, Computer languages, Filters, Common Object Request Broker Architecture, Operating systems, MEDiator, Collaboration, Computer architecture, Hardware, event domain specification, Joining processes, distributed object management, Context modeling]
Group Communication Protocol for Autonomic Computing
11th International Conference on Parallel and Distributed Systems
None
2005
We discuss a group communication protocol, which supports applications in change of QoS supported by networks and required by applications. An autonomic group protocol is realized by cooperation of multiple autonomous agents. Each agent autonomously takes a class of each protocol function. Classes taken by an agent are required to be consistent with but might be different from the others. We make clear what combination of classes can be autonomously taken by agents. We also present how to change retransmission ways
[message passing, peer-to-peer computing, Peer to peer computing, Quality of service, Multicast protocols, quality of service, Application software, software agents, group communication protocol, Business communication, autonomous agent, QoS, Computer architecture, Broadcasting, Systems engineering and theory, Autonomous agents, Computer networks, autonomic computing, protocols]
Secure Group Communication Based Scheme for Differential Access Control in Dynamic Environments
11th International Conference on Parallel and Distributed Systems
None
2005
Applications like e-newspaper or interactive online gaming typically have more than one resource and a large number of users. There is a many-to-many relationship between users and resources, each user can access multiple resources and each resource can be accessed by multiple users. Each resource needs to be encrypted by a different resource encryption key (REK). Each REK needs to be distributed to all subscribers of the resource and each subscriber must get all the REKs he/she subscribes to. We term this as problem of differential access control. Also this environment is very dynamic in terms of subscription changes by users and resource changes by service providers. Conventional ways of access control are not sufficient for this problem of differential access control. In this paper, we propose a new scheme of access control, based on secure group communication framework that is efficient, scalable, practical and dynamic
[Access control, secure group communication, differential access control, Subscriptions, Differential Access Control, Dynamic Conferencing, Communication system control, Key Management., cryptography, Multimedia communication, Application software, resource encryption key, Centralized control, Information science, resource allocation, dynamic conferencing, authorisation, Broadcasting, Secure Group Communication, Internet, Access Control, Cryptography]
Distributed Computer Systems Reliability Considering Imperfect Coverage and Common-Cause Failures
11th International Conference on Parallel and Distributed Systems
None
2005
We analyze the reliability of distributed computer systems (DCS) with imperfect fault coverage (IPC) and common-cause failures (CCF). Our analysis approach based on reduced ordered binary decision diagrams (ROBDD) is separable and computationally efficient. The DCS without IPC or CCF appear to be special cases of our approach. The application and advantages of the approach are illustrated through a concrete analysis of an example distributed computer system
[Redundancy, distributed processing, Data structures, Application software, Distributed computing, system recovery, distributed computer system reliability, binary decision diagrams, Boolean functions, reduced ordered binary decision diagram, Fault detection, Failure analysis, Distributed control, Concrete, fault tolerant computing, common-cause failure, Reliability, imperfect fault coverage]
Fault Tolerance Mechanisms for SLA-aware Resource Management
11th International Conference on Parallel and Distributed Systems
None
2005
Future grid systems will demand for properties like runtime responsibility, predictability, and a guaranteed service quality level. In this context, service level agreements have central importance. Many ongoing research projects already focus on the realization of required mechanisms at grid middleware layer. However, only concentrating on grid middleware is not enough. Also the underlying resource management systems have to provide an increased QoS level, since they provide their resources to grid environments. The EU-funded project HPC4U aims at realizing an SLA-aware resource management system. It allows the grid user to negotiate on SLAs, assuring the adherence with agreed SLAs by means of application-transparent checkpointing, snapshotting, and migration
[Checkpointing, checkpointing, fault tolerance mechanism, grid computing, Quality of service, SLA, contracts, Fault tolerance, Runtime, resource allocation, QoS, Parallel processing, Grid computing, application-transparent checkpointing, Business, middleware, Context-aware services, service quality level, grid middleware layer, grid system, quality of service, Middleware, software fault tolerance, resource management system, service level agreement, Resource management]
Challenges of Developing New Classes of NASA Self-Managing Missions
11th International Conference on Parallel and Distributed Systems
None
2005
NASA is proposing increasingly complex missions that require a high degree of autonomy and autonomicity. These missions pose hereto unforeseen problems and raise issues that have not been well-addressed by the community. Assuring success of such missions require new software development techniques and tools. This paper discusses some of the challenges that NASA and the rest of the software development community are facing in developing these ever-increasingly complex systems. We give an overview of a proposed NASA mission as well as techniques and tools that are being developed to address autonomic management and the complexity issues inherent in these missions
[Productivity, autonomic management, NASA, safety-critical software, Programming, Software development management, self-managing mission, complex mission, software development technique, Space vehicles, Mars, Space technology, aerospace computing, Hardware, Space exploration, Software engineering, software metrics]
Reliability Improvement and Models in Autonomic Computing
11th International Conference on Parallel and Distributed Systems
None
2005
The rapidly increasing complexity of computing systems is driving the movement towards autonomic systems that are capable of managing themselves without the need for human intervention. Without autonomic technologies, many conventional systems suffer reliability degradation due to the accumulation of errors. The autonomic management techniques break the traditional reliability degradation trend. This paper comprehensively describes the roles and functions of various autonomic components, and systematically reviews past and current technologies that have been/are being developed to address the specific areas of the autonomic computing environment. The effort to identify those ideas can lead to the design of more advanced autonomic computing that support highly reliable systems, as briefly proposed in the conclusion
[computing system complexity, Costs, software reliability, Humans, Power system economics, autonomic management technique, Environmental management, Degradation, configuration management, Information science, autonomic system, Technology management, reliable system, System performance, Biology computing, system monitoring, fault tolerant computing, Power system reliability, autonomic computing environment, software metrics]
Towards Autonomic Management of NASA Missions
11th International Conference on Parallel and Distributed Systems
None
2005
Increasingly, NASA relies on autonomous systems concepts, not only in the mission control centers on the ground, but also on spacecraft and on rovers and other assets on extraterrestrial bodies to achieve the full range of advanced mission objectives. While autonomy cost-effectively supports mission goals, autonomicity supports survivability of remote missions, especially when human tending is not feasible. Analysis of two prototype NASA agent-based systems and of a proposed mission involving numerous cooperating spacecraft illustrates how autonomous and autonomic system concepts may be brought to bear on future space missions
[Costs, multi-agent systems, autonomic management, Humans, safety-critical software, Control systems, Space vehicles, software architecture, spacecraft, Automatic control, aerospace computing, Software prototyping, autonomous system, NASA, NASA mission, Lighting control, space vehicles, mission control center, software maintenance, software agents, autonomic system, Satellites, Space missions, space mission, agent-based system, extraterrestrial bodies]
Towards an Autonomic Cluster Management System (ACMS) with Reflex Autonomicity
11th International Conference on Parallel and Distributed Systems
None
2005
Cluster computing, whereby a large number of simple processors or nodes are combined together to apparently function as a single powerful computer, has emerged as a research area in its own right. The approach offers a relatively inexpensive means of providing a fault-tolerant environment and achieving significant computational capabilities for high-performance computing applications. However, the task of manually managing and configuring a cluster quickly becomes daunting as the cluster grows in size. Autonomic computing, with its vision to provide self-management, can potentially solve many of the problems inherent in cluster management. We describe the development of a prototype autonomic cluster management system (ACMS) that exploits autonomic properties in automating cluster management and its evolution to include reflex reactions via pulse monitoring
[Availability, workstation clusters, reflex reaction, Power system management, Scalability, NASA, autonomic cluster management system, grid computing, fault-tolerant environment, reflex autonomicity, cluster computing, self-management, Concurrent computing, High performance computing, Space technology, Prototypes, mobile agents, pulse monitoring, Computer networks, fault tolerant computing, autonomic computing, computational capability, Energy management, high-performance computing]
Optimality of Control-Limit Type of Software Rejuvenation Policy
11th International Conference on Parallel and Distributed Systems
None
2005
Software rejuvenation is a preventive and proactive maintenance policy that is particularly useful for counteracting the phenomenon of software aging. In this paper we consider an operational software system with multiple degradations and derive the optimal software rejuvenation policy minimizing the expected operation cost per unit time in the steady state, via the dynamic programming approach. Especially, we show analytically that the control-limit type of software rejuvenation policy is optimal. A numerical example is presented to make a decision table and to perform the sensitivity analysis of cost parameters
[Software maintenance, Sensitivity analysis, control-limit type, sensitivity analysis, preventive maintenance, dynamic programming, performance evaluation, software aging, Steady-state, software maintenance, Preventive maintenance, software fault tolerance, decision tables, Degradation, proactive maintenance policy, operational software system, decision table, Optimal control, Aging, Software systems, Cost function, software rejuvenation policy, Dynamic programming]
Comparison of Software Reliability Assessment Methods for Open Source Software
11th International Conference on Parallel and Distributed Systems
None
2005
IT (information technology) advanced with steady steps from 1970's is essential in our daily life. As the results of the advances in high-speed data-transfer network technology, software development environment has been changing into new development paradigm. In this paper, we propose software reliability assessment methods for concurrent distributed system development by using the analytic hierarchy process. Also, we make a comparison between the inflection S-shaped software reliability growth model and the other models based on a nonhomogeneous Poisson process applied to reliability assessment of the entire system composed of several software components. Moreover, we analyze actual software fault count data to show numerical examples of software reliability assessment for the open source project. Furthermore, we investigate an efficient software reliability assessment method for the actual open source system development
[public domain software, information technology, software reliability, Programming, Reliability engineering, Open source software, Information systems, software component, open source system development, software development environment, stochastic processes, distributed programming, nonhomogeneous Poisson process, Object oriented modeling, Collaborative software, open source software, S-shaped software reliability growth model, software reliability assessment method, Software reliability, software fault count data, high-speed data-transfer network technology, Information technology, concurrent distributed system, Systems engineering and theory, hierarchy process, Internet]
A Dynamic Programming Algorithm for Software Rejuvenation Scheduling under Distributed Computation Circumstance
11th International Conference on Parallel and Distributed Systems
None
2005
Recently, a complementary approach to handle transient software failures, called software rejuvenation, is becoming popular as a proactive fault management technique in operational software systems. In this paper, we consider a scheduling problem of software rejuvenation for a distributed computation. Based on the dynamic programming approach, we derive the optimal software rejuvenation schedule which minimizes the expected total time of computation. In numerical examples, we examine the sensitivity of model parameters characterizing failure phenomenon to the resulting optimal rejuvenation schedule
[Heuristic algorithms, Software algorithms, distributed processing, dynamic programming, software rejuvenation scheduling, proactive fault management technique, Dynamic scheduling, distributed computation, Application software, software maintenance, Distributed computing, Scheduling algorithm, software fault tolerance, Processor scheduling, dynamic programming algorithm, operational software system, transient software failure, Aging, scheduling, Software systems, Dynamic programming]
Message from the PMAC-PDG&amp;#146;05 Co-Chairs
11th International Conference on Parallel and Distributed Systems
None
2005
Presents the welcome message from the conference proceedings.
[]
PMAC-PDG&amp;#146;05 Organizing Committee
11th International Conference on Parallel and Distributed Systems
None
2005
Provides a listing of current committee members.
[]
What are Cycle-Stealing Systems Good For? A Detailed Performance Model Case Study
11th International Conference on Parallel and Distributed Systems
None
2005
The idea of stealing cycles has been hyped for some years, boasting unlimited potential by tapping the computational power of millions of under utilized PCs connected to the Internet. Despite a few spectacular success stories (e.g. SETI@HOME), cycle-stealing is today not a widely used technology. We believe two principal impediments need to be overcome. The first is ease of development and use. Most of the problems faced in developing cycle stealing applications are not specific to those applications, so generic cycle stealing frameworks such as our G2 framework can play a vital role in this regard. The second is uncertainty. Potential developers don't know whether if they went to the effort of developing a parallel application for a cycle stealing environment, it would pay off, i.e. whether they would get a reasonable speedup. To minimize this risk, we propose the development and use of detailed performance models
[Computer aided software engineering, queueing theory, G2 framework, Predictive models, performance evaluation, performance model, Discrete event simulation, Power system modeling, Analytical models, cycle-stealing system, computational power, Internet, Performance analysis, Personal communication networks, Impedance, Queueing analysis]
Performance Evaluation of Flooding in MANETs in the Presence of Multi-Broadcast Traffic
11th International Conference on Parallel and Distributed Systems
None
2005
Broadcasting has many important uses and several mobile ad hoc networks (MANETs) protocols assume the availability of an underlying broadcast service. Applications, which make use of broadcasting, include LAN emulation, paging a particular node. However, broadcasting induces what is known as the "broadcast storm problem" which causes severe degradation in network performance, due to excessive redundant retransmission, collision, and contention. Although probabilistic flooding has been one of the earliest suggested approaches to broadcasting. There has not been so far any attempt to analyse its performance behaviour in MANETs. This paper investigates using extensive ns-2 simulations the effects of a number of important parameters in a MANET, including node speed, pause time and, traffic load, on the performance of probabilistic flooding. The results reveal that while these parameters have a critical impact on the reachability achieved by probabilistic flooding, they have relatively a lower effect on the number of saved rebroadcast packets
[Availability, LAN emulation, probabilistic flooding, ns-2 simulation, Telecommunication traffic, multibroadcast traffic, performance evaluation, Mobile ad hoc networks, broadcasting, Degradation, mobile computing, MANET, Storms, network performance, mobile ad hoc network protocol, broadcast storm problem, Broadcasting, Routing protocols, Computer networks, Performance analysis, ad hoc networks, protocols, telecommunication traffic]
Performance evaluation of fully adaptive routing under different workloads and constant node buffer size
11th International Conference on Parallel and Distributed Systems
None
2005
In this paper, the performance of some popular direct interconnection networks, namely the mesh, torus and hypercube, are studied with adaptive wormhole routing for different traffic patterns. We investigate the effect of the number of virtual channels and depth of their buffers on the performance of such strictly orthogonal topologies under uniform, hot-spot and matrix-transpose traffic patterns for generated messages, while the total buffer size associated to each physical channel is kept constant. In addition we analyze the effect of escape channel buffer length on the performance of a fully adaptive routing algorithm. It is shown that the optimal number of virtual channels and buffer depth strongly depends on the assumed traffic pattern.
[Multiprocessor interconnection networks, parallel architectures, Telecommunication traffic, hypercube network, hypercube networks, Discrete event simulation, buffer size, uniform traffic pattern, Analytical models, torus network, Network topology, mesh network, direct interconnection network, Traffic control, Hypercubes, Performance analysis, buffer storage, orthogonal topology, adaptive wormhole routing, Routing, mesh generation, hot-spot traffic pattern, matrix-transpose traffic pattern, Computer science, virtual channel]
On the performance of routing algorithms in wormhole-switched multicomputer networks
11th International Conference on Parallel and Distributed Systems
None
2005
This paper presents a comparative performance study of adaptive and deterministic routing algorithms in wormhole-switched hypercubes and investigates the performance vicissitudes of these routing schemes under a variety of network operating conditions. Despite the previously reported results, our results show that the adaptive routing does not consistently outperform the deterministic routing even for high dimensional networks. In fact, it appears that the superiority of adaptive routing is highly dependent to the broadcast traffic rate generated at each node and it begins to deteriorate by growing the broadcast rate of generated message.
[multiprocessing systems, Computational modeling, network routing, wormhole-switched hypercube network, Telecommunication traffic, performance evaluation, Routing, hypercube networks, broadcast traffic rate, adaptive routing algorithm, Intelligent networks, Analytical models, Unicast, network performance, wormhole-switched multicomputer network, deterministic routing algorithm, Broadcasting, Traffic control, Hypercubes, Computer networks]
BET: a hybrid bandwidth estimation tool
11th International Conference on Parallel and Distributed Systems
None
2005
In this paper we propose a tool that aims to integrate and improve existing tools for the available bandwidth estimation. We discuss our proposal analyzing each component of our architecture. Results from experimental analysis of our architecture and several comparative analyses with other existing and spread used tools are shown. Finally, overall conclusions are discussed and indications for future improvements are given.
[Protocols, telecommunication congestion control, congestion control, Routing, File servers, quality of service, Proposals, bandwidth allocation, BET, transport protocols, QoS, Intrusion detection, Bandwidth, TCP/IP, Communication system traffic control, Monitoring, telecommunication traffic, hybrid bandwidth estimation tool, Testing, Capacity planning]
On the use of fuzzy logic to enhance mobile agents in distributed e-commerce
11th International Conference on Parallel and Distributed Systems
None
2005
In recent years, the sales of products throughout the Internet have increased significantly. However, experiences with client/server architectures applied to developing e-commerce systems have shown some limitations related to system's needs, such as product choosing process, pricing process, and decision making while negotiating the price. These limitations might have profound impact on regular customers interested in purchasing the products. This paper presents two approaches on how one can use both fuzzy logic and agent-based technologies to solve these issues. Throughout this logic we have an acceptable intelligence level of agents in these systems. This paper proposes a new approach to solve data integrity in the workflow of distributed e-commerce systems using intelligent mobile agents through the fuzzy logic concept. Our main contributions rely on our new model that is based upon the aggregation of ERP to B2B and B2C. To the best of our knowledge, this is the first time such approach has been used. We discuss our preliminary experiments and show how our model can significantly increase the performance of e-commerce workflow using mobile agents, fuzzy logical and ERR.
[ERP, purchasing, distributed e-commerce, Enterprise resource planning, enterprise resource planning, B2B, B2C, Mobile agents, Pricing, Marketing and sales, client/server architecture, Web server, Intelligent systems, electronic commerce, pricing process, client-server systems, Decision making, fuzzy logic, product choosing process, data integrity, software agents, Intelligent agent, Fuzzy logic, intelligent mobile agent, decision making, Internet, pricing]
Efficient polynomial root finding using SIMD extensions
11th International Conference on Parallel and Distributed Systems
None
2005
In this paper, the parallel implementations of different iterative polynomial root finding methods on a processor with SIMD processing capability are reported. These methods are based on the construction of a sequence of approximations that converge to the set of roots. We have chosen four widely used methods namely Newton's, Durand-Kerner's, Aberth-Ehrlich's, and QD and implemented them using the SIMD instruction set of the Pentium processor with C++ and assembly language. Experiments show that a speedup of 3 or higher can be achieved, depending on the order of polynomial, required accuracy, and the method employed.
[instruction sets, assembly language, iterative methods, approximation theory, multiprocessing systems, SIMD parallel processing, Registers, C++ language, approximation sequence, parallel processing, Convergence, Computer science, SIMD instruction set, Microprocessors, Computer applications, iterative polynomial root finding method, Polynomials, Iterative algorithms, Iterative methods, Vector processors, Pentium processor, Assembly]
BECP: the scheduling policy based on best-effort in clustered VOD servers
11th International Conference on Parallel and Distributed Systems
None
2005
The increasing process power of personal terminates enable VOD server to have more flexible schedule scheme. This paper proposed a server-push scheduling strategy that based on best-effort transmission, called BECP (best-effort concurrent push) scheduling scheme. Different to traditional video servers schedule policy, all server nodes in the cluster send out the media data blocks as soon as they are retrieved from disks without long time buffering. We made an analysis of the system performance and made a simulation. The simulation results show that, with BECP scheduling policy, a clustered VOD server with 128 nodes and 4 disks per nodes will support over 3160 concurrent streams, and it occupies small buffering space.
[buffer storage, Scalability, Information retrieval, Displays, video servers schedule policy, disc storage, video servers, Scheduling algorithm, processor scheduling, clustered VOD server, Network servers, server-push scheduling strategy, concurrency control, best-effort concurrent push scheduling scheme, Streaming media, Workstations, Performance analysis, Personal communication networks, Web server, best-effort transmission]
Message from the DPNA-2005 Chairs
11th International Conference on Parallel and Distributed Systems
None
2005
Presents the welcome message from the conference proceedings.
[]
DPNA-2005 Organizing Committee
11th International Conference on Parallel and Distributed Systems
None
2005
Provides a listing of current committee members.
[]
The security analyses of RosettaNet in grid
11th International Conference on Parallel and Distributed Systems
None
2005
The RosettaNet is an e-commerce standard over the Internet, and grid is a computing infrastructure for resource sharing. How to build e-commerce applications based on the RosettaNet over the grid infrastructure is what we are interested in this paper. We first introduce the RosettaNet, and discuss the security strategy of the RosettaNet. Then, we look into the security issues of the grid. Finally, we present how the security of RosettaNet can be improved, particularly, in the grid environment.
[Dictionaries, RosettaNet security analysis, grid computing, Companies, Electronic commerce, Security, Sun, standards, e-commerce standard, security of data, resource sharing, Grid computing, Internet, Resource management, Communication networks, Business, electronic commerce]
Dynamic RWA mechanism for group-based protection on IP over WDM networks
11th International Conference on Parallel and Distributed Systems
None
2005
To provide the robust message delivery capability with traffic flows in the networks, survivability becomes a key issue for the service provider to guarantee the service level agreement to the customers. In this paper, we propose the dynamic group-based routing and wavelength assignment policy for dynamic traffic demands under WDM optical networks which equipped with the capability to convey IP flows under GMPLS control plane over optical layer environment. Simulation results show that the proposed mechanism can achieve high resource utilization and restoration efficiency. In particular, it provides the capability to perform RWA for distinct connection demands in a parallel manner, which is different with traditional event-driven batch processing for the requests.
[optical layer environment, wavelength division multiplexing, Telecommunication traffic, WDM networks, Optical fiber networks, restoration efficiency, WDM optical network, parallel processing, Wavelength routing, resource allocation, batch processing (computers), telecommunication network reliability, Traffic control, dynamic traffic demands, event-driven batch processing, Robustness, Communication system traffic control, IP networks, resource utilization, Protection, IP flows, dynamic group-based routing, Wavelength assignment, network routing, dynamic RWA mechanism, Optical control, service level agreement, network traffic flow, wavelength assignment policy, group-based protection, telecommunication network routing, optical fibre networks, survivability, telecommunication traffic, GMPLS control plane]
Automated discovery of brand piracy on the Internet
11th International Conference on Parallel and Distributed Systems
None
2005
The Internet has become a key instrument for establishing and promoting product brands. Companies are very interested in detecting any misuse of their logos, plagiarism, grey imports, and slander campaigns to protect the reputation of their brands. So-called brand protection is very costly and requires a great expenditure of human resources especially when concerning electronic media. This article describes a software architecture that enables flexible observation and examination of Internet content regarding e.g. unauthorized use of logos, damage to brand names or shady sales offers. These algorithms check Web content over several steps including a semantic analysis. The described system can be flexibly adjusted to specific monitoring tasks and inquiry orders to serve Internet detectives as an effective tool.
[brand protection, Instruments, Humans, content management, human resource, electronic media, software architecture, Software architecture, Plagiarism, Computer graphics, authorisation, semantic analysis, Marketing and sales, data privacy, Internet, Protection, Monitoring, Consumer electronics]
Universal routing in distributed networks
11th International Conference on Parallel and Distributed Systems
None
2005
We show that universal routing can be achieved with low overhead in distributed networks. The validity of our results rests on a new network called the fat-stack. We show that from a routing perspective the fat-stack is efficient and is suitable for use as a baseline distributed network We prove that the fat-stack is universal by routing efficiency. A requirement for the fat-stack to be universal is that link capacities double up the levels of the network. We use methods developed in the areas of VLSI and processor interconnect for much of our analysis. Our universality proof shows that a fat-stack of area /spl Theta/(A) can simulate any competing network of area A with O(log/sup 3/2/ A) overhead independently of wire delay. The universality result implies that the fat-stack of a given size is nearly the best routing network of that size. The fat-stack is also the minimal universal network for an O(log/sup 3/2/ A) overhead in terms of number of links.
[Multiprocessor interconnection networks, network routing, VLSI, fat-stack network, multiprocessor interconnection networks, Telecommunication traffic, Very large scale integration, Routing, Wire, processor interconnection, Distributed computing, Computer science, Intelligent networks, wire delay, baseline distributed network, universal routing network, Computer networks, Hardware]
Causally ordered delivery with global clock in hierarchical group
11th International Conference on Parallel and Distributed Systems
None
2005
In peer-to-peer (P2P) networks, large number of peer processes are cooperating. In this paper, we discuss a scalable group of processes where processes are widely distributed in networks. Clocks of computers in every local network are synchronized by using the network time protocol (NTP) with a GPS time server. We discuss a global clock group (GCG) protocol where messages are causally ordered by using the physical time stamps. Messages not to be ordered by physical clock are furthermore ordered by using linear clock. We evaluate the protocol in terms of the number of messages ordered compared with the vector clock.
[Protocols, peer-to-peer computing, Delay effects, Peer to peer computing, message time stamping, network time protocol, Vectors, Synchronization, Global Positioning System, synchronisation, Network servers, Intelligent networks, Computer networks, protocols, global clock group protocol, message switching, GPS time server, peer-to-peer network, Clocks, global clock synchronization]
MeshTree: reliable low delay degree-bounded multicast overlays
11th International Conference on Parallel and Distributed Systems
None
2005
We study decentralised low delay degree-constrained overlay multicast tree construction for single source real-time applications. This optimisation problem is NP-hard even if computed centrally. We identify two problems in traditional distributed solutions, namely the greedy problem and delay-cost trade-off. By offering solutions to these problems, we propose a new self-organising distributed tree building protocol called MeshTree. The main idea is to embed the delivery tree in a degree-bounded mesh containing many low cost links. Our simulation results show that MeshTree is comparable to the centralised compact tree algorithm, and always outperforms existing distributed solutions in delay optimisation. In addition, it generally yields trees with lower cost and traffic redundancy.
[Costs, Laboratories, Videoconference, Delay, optimisation problem, optimisation, Unicast, Traffic control, centralised compact tree algorithm, Water resources, overlay multicast tree construction, Buildings, Redundancy, trees (mathematics), Routing, self-organising distributed tree building protocol, real-time application, NP-hard problem, multicast protocols, delays, real-time systems, MeshTree protocol, telecommunication traffic, computational complexity, greedy problem]
Enabling requirements-based programming for highly-dependable complex parallel and distributed systems
11th International Conference on Parallel and Distributed Systems
None
2005
The manual application of formal methods in system specification has produced successes, but in the end, despite any claims and assertions by practitioners, there is no provable relationship between a manually derived system specification or formal model and the customer's original requirements. Complex parallel and distributed systems present the worst case implications for today's dearth of viable approaches for achieving system dependability. No avenue other than formal methods constitutes a serious contender for resolving the problem, and so recognition of requirements-based programming has come at a critical juncture. We describe a new, NASA-developed automated requirements-based programming method that can be applied to certain classes of systems, including complex parallel and distributed systems, to achieve a high degree of dependability.
[distributed system validation, Automatic programming, parallel system, NASA, Natural languages, system specification, Formal specifications, automatic code generation, formal specification, parallel processing, program compilers, Research and development, Information systems, NASA-developed automated requirements-based programming, Parallel programming, formal verification, Space technology, systems analysis, aerospace computing, Systems engineering and theory, Timing]
A route secure parallel dispatch model for mobile agents security
11th International Conference on Parallel and Distributed Systems
None
2005
Mobile agent technology offers a computing paradigm in which a software agent can transfer itself to other hosts on the network, and execute on the new host. During the self initiated migration, the agent carries all its code and the complete execution state with it. Mobile agent systems build the environment in which mobile agents can exist. Security is a very important issue for widespread deployment of mobile agent systems. Without proper counter measures, use of agent based systems will be severely impeded. However, not all applications require the same set of counter-measures, nor can they depend entirely on the agent system to provide them. Dispatching scheme followed to transfer the agents can also contribute to security features in a mobile agent system. Countermeasures are applied according to the threat profile and intended security objectives of the application. In this paper we discuss parallel mobile agent dispatch model (ROSPAD) to enhance agent as well as route security.
[Resumes, software agent, Application software, Electronic commerce, parallel mobile agent dispatch model, Counting circuits, security of data, route secure parallel dispatch model, Mobile agents, Information security, mobile agents, Software agents, Computer networks, Dispatching, Impedance, mobile agent system security]
An architecture for multi-agent COTS software integration systems
11th International Conference on Parallel and Distributed Systems
None
2005
Commercial off-the-shelf (COTS) software products are increasingly used as software components in large-scale systems. We had proposed an approach for distributed COTS software integration by using the concepts of multi-agent system and distributed scripting mechanism [J.M. Lin et al., (2002)]. To describe the experience in the COTS software integration and facilitate the reuse of the software integration procedure, this paper presents a multi-agent architecture for the COTS software integration systems. This architecture is of a three-layered structure and is described with the agent UML (AUML). Since the interaction and internal processing of agents is clearly described in the proposed architecture, programmers may have a guide to build a software system and implement the protocols and behaviors of agents according to layered description.
[multi-agent systems, distributed scripting mechanism, Unified modeling language, multi-agent distributed COTS software integration system, large-scale system, software component, software architecture, commercial off-the-shelf software product, Computer architecture, software packages, Cities and towns, Software agents, Large-scale systems, agent UML, Multiagent systems, object-oriented programming, Unified Modeling Language, Application software, software agents, Intelligent agent, Computer science, integrated software, software reusability, Software systems, multiagent software architecture]
An image refining method using digital watermark via vector quantization
11th International Conference on Parallel and Distributed Systems
None
2005
In this paper, we shall propose an image refining method that incorporates the digital watermarking technique and vector quantization to recover an image after transmission on a network unclean or tampered by people. In our method, we embed the codevector indices of the original image into the original image itself via the watermarking technique. Then, in the refining phase, the indices are retrieved and employed to detect and recover the supposedly partially damaged image block. The experimental results show that the performance of our method is excellent in suppressing block noise.
[image recovery, Vector quantization, Image retrieval, image transmission, Watermarking, visual communication, Decoding, Electronic mail, block noise suppression, image denoising, image refining method, Computer science, watermarking, vector quantisation, discrete cosine transform, Image coding, digital watermarking, image restoration, vector quantization, Frequency, Computer science education, Discrete cosine transforms, image coding, multimedia communication]
Zone-based traffic load balancing to ensure quality of service (QoS) in distributed systems
11th International Conference on Parallel and Distributed Systems
None
2005
We propose a method for the sharing of capacity among zones of a communication network. Our approach can be employed either when a zone owner wishes to sell capacity for a specified period of time to a number of different zones, or when zones cooperate to build a network to be shared among themselves. We show how load balancing can be performed to mediate between rapidly fluctuating costs and the capacity of zone, which might be traded. Depending on the density of the zone, an important property of the process is that it prevents zone from congestion.
[telecommunication congestion control, Quality of service, Telecommunication traffic, distributed processing, distributed system, Distributed computing, Quality of Service (QoS), resource allocation, QoS, Bandwidth, Zone density and Zone-based Load-balancing., Computer networks, Communication system traffic control, Augmenting Path Selection, Communication networks, zone-based traffic load balancing, communication network, congestion control, Routing, path selection, quality of service, Congestion Control, Computer science, telecommunication network routing, Load management, telecommunication traffic]
A technique to analyse session initiation protocol traffic
11th International Conference on Parallel and Distributed Systems
None
2005
The session initiation protocol (SIP) will provide the signaling network in the next generation networks, such as UMTS and cdma2000. Here we study the traffic load generated by the use of this protocol from a methodological point of view. We propose an analytical technique to simulate SIP finite state machine (FSM) in an IP network by means of the theory of queuing networks. Based on this approach, we derive a simple model which can be applied to different network scenarios for performance evaluation and engineering purposes. We conclude this study by showing some results about call dropping rate in a wireless access network.
[cdma2000, wireless access network, Protocols, 3G mobile communication, NS-2 simulator, Telecommunication traffic, finite state machines, Next generation networking, Analytical models, traffic load, Wireless networks, Traffic control, call dropping rate, IP networks, protocols, queuing network theory, queueing theory, next generation network, performance evaluation, IP network, UMTS, signaling network, Automata, session initiation protocol traffic, finite state machine, Queueing analysis, telecommunication traffic]
Two hops backup routing protocol in mobile ad hoc networks
11th International Conference on Parallel and Distributed Systems
None
2005
Routing protocols play an important role in mobile ad hoc networks (MANET) [J. Macker et al. (1997)]. Sungle-Ju Lee and Mario Gerla proposed AODV-BR [2000] to improve AODV [C.E. Perkins et al. (1999)], [C.E. Perkins et al. (2001)] by providing multiple alternate routes. However, the node which detected link failure maybe move to the position where no alternate routes can be used or nodes on the alternate paths have to transmit other data packets. Therefore, the effect of multiple alternate routes will be reduced. In this paper, we propose two hops backup routing scheme to provide more alternate routes with little setup overhead, resulting in better performance in high mobility and heavy traffic load environment.
[Base stations, mobile ad hoc network, packet switching, Telecommunication traffic, network traffic load, Mobile ad hoc networks, Intelligent networks, mobile computing, MANET, Network topology, mobile communication, Wireless networks, data packet transmission, routing protocols, Broadcasting, data communication, Routing protocols, Computer networks, ad hoc networks, Mobile computing, telecommunication traffic, hops backup routing protocol]
A dynamic caching mechanism for mobile ad hoc networks
11th International Conference on Parallel and Distributed Systems
None
2005
Nowadays people could travel around for work and conferences by using wired or wireless communication to receive information and data immediately. A mobile ad hoc network (MANET) is a self-organizing and dynamically reconfigurable wireless network without base stations or access points. Due to the high mobility of mobile nodes (MNs), the topology of a MANET frequently changes. A dynamic caching mechanism is proposed in this paper to cope with the intrinsic properties of MANETs. With the aid of the proposed mechanism, the repetition of data and data path occurring in a MANET could be cached. The data reusable rate is enhanced to reduce the bandwidth and the power consumption of MNs; routes and time span to access data are also shortened.
[Energy consumption, mobile ad hoc network, bandwidth reduction, Data engineering, Mechanical factors, cache storage, reconfigurable wireless network, power consumption, Mobile ad hoc networks, Computer science, bandwidth allocation, mobile computing, MANET, Network topology, mobile communication, Wireless networks, routing protocols, Bandwidth, Routing protocols, Distributed Bragg reflectors, ad hoc networks, dynamic caching mechanism]
An application layer protocol interoperability design model for content evolution in the mobile Internet
11th International Conference on Parallel and Distributed Systems
None
2005
Interoperability is one of the top priority issues in the computer communication standardizations. In the content side, the evolution is not continuous, therefore, the interoperability needs to be embedded in the communication protocol design. In the past, the communication protocol design is done in an ad hoc manner in each sequence of protocol evolution. It leads to the lack of the design strategy reuse among the different communication protocol domains. In order to provide a general guideline for the interoperable communication protocol design, a general methodology to pursue the interoperability considering the key aspects of the content evolution. The author analyzes the interoperability-aware protocol design and proposes an interoperability design model. First, the author proposes a communication protocol design framework to clarify the components to be considered in the interoperable communication protocol design. Then, the author describes the different patterns from the past lessons learned in the communication protocol design in each aspect: evolution patterns, underlying assumptions, communication architecture, and protocol processing models.
[open systems, Design methodology, interoperable communication protocol design, mobile Internet, Standardization, Access protocols, Mobile communication, Mobile handsets, protocol processing models, Application software, communication architecture, Research and development, Guidelines, mobile computing, Internet, protocols, Mobile computing, computer communication standardizations, application layer protocol]
Parity placement schemes with generalized fault-tolerant technique in disk array systems
11th International Conference on Parallel and Distributed Systems
None
2005
In order to achieve high reliability in disk array systems, an efficient and simple fault-tolerant method for tolerating disk failures based on parity placement scheme needs to be explored. In this paper, we proposed a simple recovery algorithm (or decoding method) to deal with the faulty disks problem (given k, generally less or equal than 3) occurring frequently in disk array system (given the number of disks N). It is based on modulo 2 arithmetic, parity and exclusive-OR operations which make the recovery speed of our purposed method faster than other schemes that require computation over finite fields.
[Linear systems, disk array systems, parity check codes, Reliability engineering, Encoding, Decoding, fault-tolerant method, RAID, Galois fields, exclusive-OR operations, decoding, parity placement scheme, Fault tolerant systems, Performance loss, fault tolerant computing, Error correction, modulo 2 arithmetic, disk failures, decoding method, Logic arrays, Arithmetic]
Cycle Embedding on the M &#168; obius Cube with Both Faulty Nodes and Faulty Edges
11th International Conference on Parallel and Distributed Systems
None
2005
A graph G = (V,E) is said to be pancyclic if it contains fault-free cycles of all lengths from 4 to |V | in G. Let Fv and Fe be the sets of faulty nodes and faulty edges of an n-dimensional M&#168;obius cube MQn, respectively, and let F = Fv U Fe. In this paper, we show that MQn - F contains a fault-free Hamiltonian path when |F| \\le n - 1 and n \\ge 1. We also show that MQn - F is pancyclic when |F| \\le n - 2 and n \\ge 2. Since MQn is regular of degree n, both results are optimal in the worst case.
[Computer science, Network topology, Terminology, Multiprocessor interconnection networks, Length measurement, Hypercubes]
A declustering scheme with guaranteed worst-case additive error O(k/sup 1/(d-1)/) for d-dimensional range queries
11th International Conference on Parallel and Distributed Systems
None
2005
Declustering schemes for range queries have been widely used in parallel storage systems to allow fast access to multidimensional data. A declustering scheme distributes data blocks among several devices (e.g., disks) so that the number of parallel block accesses needed per query is minimized. Given a system of k disks, a query that accesses m blocks needs a number of parallel block accesses that is at least OPT=/spl lceil/m/k/spl rceil/. In literature, the performance of any declustering scheme is measured by its worst-case additive deviation from OPT. A number of asymptotically optimal declustering schemes are known for 2-dimensional range queries. The case of higher dimensions appears intrinsically very difficult. None of the proposed schemes provide any non-trivial performance guarantees in higher dimensions. In this paper, we describe a declustering scheme which has guaranteed worst-case performance of OPT+O(k/sup 1/(d-1)/) parallel block accesses for d dimensions. Our scheme is a generalization of a 2-dimensional scheme proposed by Atallah and Prabhakar in 2000.
[parallel memories, Visualization, Multidimensional systems, Geographic Information Systems, multidimensional data, d-dimensional range queries, Optimized production technology, Relational databases, parallel databases, Spatial databases, Visual databases, declustering schemes, Delay, Computer science, query processing, parallel block accesses, guaranteed worst-case additive error, Computer errors, parallel storage systems, computational complexity]
A GPS-less self-positioning method for sensor networks
11th International Conference on Parallel and Distributed Systems
None
2005
One challenging issue in sensor networks is to determine where a given sensor node (SN) is physical located. This problem is especially crucial for very small SNs. In this paper, we present a GPS-less self-positioning method for sensor networks. In our method, a set of nodes, called as reference points (RPs), are deployed in the sensor network with overlapping regions of coverage. The RP periodically broadcasts beacon frames which contain localization data. The SN collects the beacon frames from RPs and processes the data in the frame, and then it can localize itself simply. The analysis of positioning accuracy is also given to show how well a SN can correctly localize itself.
[Costs, wireless sensor networks, reference points, Delay effects, beacon frames, GPS-less self-positioning method, Sensor systems, sensor networks, Global Positioning System, Wireless sensor networks, Transmitters, Fires, Position measurement, Tin, Computer networks, sensor node]
The development of the adaptive traffic signal control system
11th International Conference on Parallel and Distributed Systems
None
2005
Agents refer to self-contained and identifiable computer programs that can migrate along the network and can act on behalf of the user or other agents. Agents often work on heterogeneous network and operating system environment. Therefore, an integrated logical process to access physical structure via agent application is become more and more urgent. The adaptive urban traffic signal control (TSC) system has become a development trend of intelligent transportation system (ITS). In this paper, we proposed a tracking and persistent agent-based mobility management system in case of adaptive urban traffic signal control (TSC) system. We investigated the vision based surveillance and to keep sight of the unpredictable and hardly measurable disturbances may perturb the traffic flow. We integrated and performed the vision based methodologies that include the object segmentation, classify and tracking methodologies to know well the real time measurements in urban road. According to the real time traffic measurement, we derived agent communication and the adaptive traffic signal control strategy to adapt the traffic signal time. By comparing the experimental result obtained by traditional traffic signal control system which improves the traffic queuing situation, we confirmed the efficiency of our vision based adaptive TSC approach.
[image classification, Control systems, object detection, intelligent transportation system, Adaptive control, Mobile radio mobility management, tracking, Intelligent transportation systems, Operating systems, adaptive urban traffic signal control system, image segmentation, heterogeneous network, mobile agents, Traffic control, agent-based mobility management system, Communication system traffic control, Computer networks, surveillance, signalling, real time traffic measurement, object tracking methodologies, adaptive control, traffic engineering computing, Application software, transportation, Programmable control, object segmentation, real-time systems, computer vision, automated highways]
Static scheduling of periodic tasks in a decentralized real-time control system using an ILP
11th International Conference on Parallel and Distributed Systems
None
2005
In this paper, we introduce a novel integer linear program (ILP) to derive a static global schedule for a decentralized real-time control system consisting of periodic tasks while considering the resource, the dependency and the timing constraints. The ILP always finds a feasible schedule if one exists.
[Real time systems, integer programming, Communication system control, Control systems, linear programming, decentralised control, processor scheduling, static global schedule, decentralized real-time control system, periodic tasks, Intelligent networks, integer linear program, Processor scheduling, resource allocation, real-time systems, Broadcasting, Software systems, Timing, Resource management, Communication networks]
Transactional agent on distributed objects
11th International Conference on Parallel and Distributed Systems
None
2005
A transactional agent is a mobile agent to manipulate distributed objects. A transactional agent is composed of routing and manipulation subagents. After visiting computers, a routing agent makes a decision on commitment by using its commitment condition. Objects in each computer are manipulated in a manipulation agent. In addition, objects obtained from a computer have to be delivered to other computers where the transactional agent is performed. A schedule to visit computers is made from the input-output relation of manipulation agents. We discuss how to implement a transactional agent in Aglets for database servers. We evaluate the transactional agent model in terms of accessing time compared with the client-server model.
[transaction processing, client-server systems, client-server model, Routing, Mobile communication, Transaction databases, Application software, Distributed computing, Network servers, transactional agent, Processor scheduling, Computer network reliability, Mobile agents, mobile agent, mobile agents, distributed databases, distributed objects, manipulation agent, Aglets, database servers, Computer networks, distributed object management]
An efficient long-lived adaptive collect algorithm
11th International Conference on Parallel and Distributed Systems
None
2005
Adaptive algorithms, whose step complexity adjusts to the number of actually participating processors, are attractive in distributed systems where the number of processors is highly variable. Long-lived adaptive algorithms moreover enable processors to repeatedly execute operations "from scratch\
[Algorithm design and analysis, Adaptive algorithm, adaptive shared memory collect algorithm, shared memory registers, Read-write memory, long-lived adaptive collect algorithm, Registers, Distributed computing, Computer science, Writing, distributed shared memory systems, distributed systems, computational complexity]
A middleware for distributed system in heterogeneous wireless networks
11th International Conference on Parallel and Distributed Systems
None
2005
Ubiquitous access of the resource is the greatest challenge in the world of distributed mobile computing. This paper discusses the framework of agent based middleware to provide efficient, flexible and scalable system support for configuration and communication in heterogeneous wireless networks. The proposed middleware satisfies the requirement of ubiquitous access of the resource in multilayer modular manner by hiding the platform heterogeneity.
[distributed mobile computing, agent based middleware, distributed system, Educational institutions, Middleware, Distributed computing, Intelligent networks, Network servers, mobile computing, resource allocation, Wireless networks, mobile agents, Computer architecture, Bandwidth, ubiquitous resource access, Computer networks, heterogeneous wireless networks, Mobile computing, middleware]
A distributed computing approach for marketable quality and profitability of corporations
11th International Conference on Parallel and Distributed Systems
None
2005
The main objective of this paper is to provide an evaluation method for both: the quality aspect of corporations (marketable quality) and the profitability. First, we define the corporation rate of operation and profitability. Based on these definitions, we present a model to identify the profitability function considering the rate of operation at the break-even point. This function is considered as the marketable quality indicator from the viewpoint of a fair relationship between producers and consumers. In this paper, we consider only the fair relationship. We apply the real values of some leading manufacturing corporations in Japan to our proposed model to analyze its accuracy. From the analysis we concluded that the theoretical and real standard values of the marketable quality indicator were both 0.6 (that is 60%).
[profitability, Costs, Profitability, marketable quality indicator, distributed processing, Distributed computing, distributed computing, break-even point, marketing, Technology management, quality management, Engineering management, Production, Japan manufacturing corporation, Marketing and sales, Virtual manufacturing, Acceleration, corporation profitability, Quality management, electronic commerce]
A simple quorum reconfiguration for open distributed environments
11th International Conference on Parallel and Distributed Systems
None
2005
Synchronizations adopting quorum consensus are the well-known solutions of some fundamental issues in the study of distributed mutual exclusion and replica control problems. Mechanisms to reconfigure quorum structure in open distributed environments are necessary since the membership changes of such systems (i.e., the joining and leaving members) may decrease quorum availability. Many algorithms have been proposed to this problem, however, they mostly change the quorum system totally thus any operation cannot be performed while system in the reconfiguration process. This paper presents a simple quorum reconfiguration algorithm in open distributed computing systems that can evolve their behavior based on membership changes in the environment. The algorithm is easy to use since it simply implements the two quorum operations called join-replace and join-cross. The join-replace operation is used when a set of nodes have leaved from the system while some others are joining, and the join-cross is defined and used if there is only a set of joining nodes enter the system. The great advantages of the algorithm are the ability to complete any operation before a new quorum structure is fully constructed during reconfiguration thus system does not enter the halt state with a wait-avoidance characteristic, and it directly adopts quorum consensus in the static environments without any change to the protocol. Moreover, an extra mapping procedure is unnecessary to be given since the algorithm only works in the logical space.
[Availability, Protocols, open systems, open distributed environment, join-cross quorum operations, distributed mutual exclusion, Electronic mail, Partitioning algorithms, Distributed computing, synchronisation, join-replace quorum operations, resource allocation, replica control problem, Permission, Distributed control, synchronization, Database systems, Safety, quorum reconfiguration algorithm, Informatics]
An on-line page-structure approximation scheme for Web proxies
11th International Conference on Parallel and Distributed Systems
None
2005
To render a Web page, a browser must first download an HTML document, parse it, and then issue a sequence of additional requests to fetch the embedded objects according to the content of the HTML document. Therefore, it should be straightforward for Web proxies to accurately predict future client requests by considering the characteristics of such regular behavior. However, the strong bindings between embedded objects and their containing documents are often ignored by modern Web proxies because there still exists no efficient solution for Web proxies to obtain the knowledge of page structures without performing the computation-intensive operations of HTML parsing. In this paper, we propose an effective and low-overhead scheme for Web proxies to approximate page structures and refine the approximation as new client requests arrive. The results of simulation show that the approximation converges quickly and reaches high accuracy after a relatively small number of incoming requests have been processed.
[Embedded computing, Content based retrieval, online page-structure approximation scheme, Computational modeling, Containers, Predictive models, HTML, content management, Uniform resource locators, HTML parsing, embedded object, Web browser, Web pages, Web page, HTML document, online front-ends, Web proxy, Internet, Web server, Pattern matching, hypermedia markup languages]
On-demand service in grid: architecture, design and implementation
11th International Conference on Parallel and Distributed Systems
None
2005
Service oriented architecture becomes the mainstream of grid research today. On-demand service, which can provides better flexibility for grid users, is purposed in this paper. According to the research on different implementation policies, on-demand service architectures for virtual machine based on-demand service and service specification reference based on-demand service is proposed. Furthermore, it is designed and implemented based on Java virtual machine and Groovy script language. Performance testing to the implementation shows that on-demand service can provide better flexibility for grid by low cost, and can speedup mass data processing services usually.
[Java, Costs, open systems, Service oriented architecture, grid computing, Grid, Groovy Script Language, Virtual machining, Helium, formal specification, service specification reference, Java Virtual Machine, Computer science, Computer languages, On-Demand Service, Web services, Computer architecture, virtual machines, service oriented architecture, Java virtual machine, on-demand service architectures, Groovy script language, Testing]
Design and implementation of mobile grid middleware for handsets
11th International Conference on Parallel and Distributed Systems
None
2005
This paper proposes mobile grid middleware based on an adaptive QoS framework to conceal the instability and resource constraints of mobile server hosts (MSHs). In a mobile grid system, data and codes on an MSH are replicated and synchronized on a fixed grid proxy for enhanced availability. However, operation time of MSH is limited by battery consumption due to the synchronization needed to maintain the freshness of replicas and the session management needed to maintain responsiveness of the MSH. According to application QoS requests, the framework optimizes synchronization scheduling and polling intervals to maximize sustainability. It also adapts to dynamic resource changes including low residual battery and poor wireless connectivity, by decreasing the target QoS and by reducing the aggregation period for synchronization. We built a prototype of mobile grid middleware on a J2ME handset and grid proxy on PC running Globus toolkit. Experiments showed that the framework enables MSH to increase its operation time while meeting diverse application QoS requests.
[session management, grid computing, Globus toolkit, quality of service, mobile handset, Middleware, J2ME handset, synchronisation, synchronization scheduling, mobile computing, resource allocation, scheduling, mobile grid middleware, Telephone sets, mobile server host, middleware, mobile handsets, grid proxy]
Validating the distributed ontology framework for deployment onto the semantic grid environment
11th International Conference on Parallel and Distributed Systems
None
2005
As the vision of the semantic grid slowly becomes a reality, more and more people start migrating their applications onto the semantic grid environment as grid resources. Their applications have the ability to use a wide range of grid resources as well as have exposure to a larger population of potential users. This paper explores some of the main obstacles that obstruct the deployment of the distributed ontology framework onto the semantic grid environment. The results are a set of validation criteria that can be used to determine if particular systems are ready to be deployed on the semantic grid.
[semantic grid environment, semantic Web, Taxonomy, Service oriented architecture, grid computing, Ontologies, grid resources, Semantic Web, Web services, resource allocation, High performance computing, Web and internet services, distributed ontology framework, Grid computing, ontologies (artificial intelligence), Hardware]
Towards a service-based collaborative framework for data-intensive grid applications
11th International Conference on Parallel and Distributed Systems
None
2005
As data-intensive applications increase continuously in various domains, scientists nowadays need to save, retrieve and analyze rapidly increasing large datasets. The long latency of data transfer over Internet results in a serious challenge on ensuring high-performance access to large quantities of data. Furthermore, the vision of grid researches is to provide a collaborative space where scientists can share their data, resources and experiences with others. These issues motivate us to develop a service-based collaborative framework for data-intensive grid applications. In this paper, we describe our initial work towards this goal. We plan to provide an integrated and virtual infrastructure, which manages distributed and diverse data resources and services. By means of Web Services technologies, we can realize the interoperability among various types of resources and integrate computing services with storage services to serve data-intensive applications. By making use of portals and data-intensive applications built on the top of the framework, users will be able to manage and analyze large quantities of data with high performance and share useful data with others in a way of forming virtual groups to complete cooperative work.
[distributed data resources, open systems, service-based collaborative framework, grid computing, portals, information retrieval, Information retrieval, interoperability, data-intensive grid application, Delay, Web Services technology, Web services, resource allocation, Collaboration, Virtual groups, groupware, distributed databases, Collaborative work, Internet, Performance analysis, Resource management, Portals]
MPI-I/O operations to a remote computer using Java
11th International Conference on Parallel and Distributed Systems
None
2005
A communication library named Stampi realizes MPI communications among different computers, and it is supported in many kinds of parallel computers. In addition, I/O operations using MPI-I/O APIs among different computers are available with it. In recent parallel computation, Java has been focused for its portable and flexible interfaces, and it has been widely used in applications for parallel computation. For data-intensive applications, MPI-I/O is useful in data handling. To realize MPI-I/O operations to a remote computer in a Java application, Stampi introduces Java into several kinds of its MPI-I/O interfaces. This mechanism enables a Java application to play MPI-I/O operations which are compatible with MPI-I/O operations using C or Fortran. In this paper, details of the mechanism and preliminary performance results are described.
[Computer interfaces, Java, Stampi communication library, Portable computers, message passing, application program interfaces, Application software, Distributed computing, parallel processing, Concurrent computing, Computer science, TCPIP, Computer applications, parallel computers, Libraries, MPI-I/O operations]
POERS: a performance-oriented energy reduction scheduling technique for a high-performance MPSoC architecture
11th International Conference on Parallel and Distributed Systems
None
2005
Continuous improvements in semiconductor technology are supporting new classes of multi-processor system-on-a-chip (MPSoC) architectures that combine extensive processing logic with high-density memory. Such architectures are generally colled processor-in-memory (PIM) or intelligent memory (I-RAM) and can support high-performance computing by reducing the performance gap between the processor and the memory. The PIM architecture combines various processors in a single chip. These processors are characterized by their computation, memory-access, and power consumption capabilities. Therefore, a novel parallelizing system, SAGE II, has be developed to identify their capabilities and dispatch the most appropriate jobs to them in order to exploit the advantages of PIM architectures. This paper provides a new low-power transformation mechanism, called performance-oriented energy reduction scheduling (POERS), to extend the capability of SAGE II system. It can reduce the energy consumption for the processor-in-memory system without losing execution performance. The detailed POERS transformation technique is presented later. The experimental results of several benchmarks are also discussed.
[Energy consumption, high-performance MPSoC architecture, parallel architectures, Random access memory, SAGE II, Delay, processor scheduling, storage management, Computer architecture, SAGE II system, a performance-oriented energy reduction scheduling, multiprocessor system-on-a-chip, processor-in-memory, System-on-a-chip, Logic, Embedded computing, multiprocessing systems, intelligent memory, Processor-in-Memory, Memory architecture, MPSoC, Energy Reduction, POERS, POERS technique, Processor scheduling, High performance computing, system-on-chip]
An optimized algorithm of high spatial-temporal efficiency for Megablast
11th International Conference on Parallel and Distributed Systems
None
2005
BLAST (basic local alignment search tool), as a heuristic algorithm, is one of the most widely used sequence similarity search tools. MegaBlast, as an improved version of BLAST, speeds up the searches and improves the total throughput owing to greedy algorithm and batch processing. However, MegaBlast consumes a great deal of memory, which is proportional to the product of the size of the query file and database file. This paper proposes an optimized MegaBlast algorithm based on MegaBlast. The new algorithm exchanges the query and subject sequences, and builds a hash table based on new subject sequences. The optimized algorithm overlaps I/O with computation, further decreases the overall time and the cost of memory, which is only proportional to the size of the database file. The optimized algorithm is suitable to be parallelized on cluster systems. As our experiments shown, the parallel program, which is implemented with MPI, achieves high speedup.
[Greedy algorithms, Computers, application program interfaces, Heuristic algorithms, Throughput, Parallel algorithms, parallel programming, query processing, heuristic algorithm, Databases, MegaBlast algorithm, Clustering algorithms, Cost function, Dynamic programming, search problems, message passing, hash table, message passing interface, greedy algorithms, parallel databases, Sun, spatiotemporal phenomena, basic local alignment search tool, file organisation, parallel program]
Message from the Workshop Chair
12th International Conference on Parallel and Distributed Systems -
None
2006
Presents the welcome message from the conference proceedings.
[]
Message from the PMAC-2WN&amp;#146;06 Co-Chairs
12th International Conference on Parallel and Distributed Systems -
None
2006
Presents the welcome message from the conference proceedings.
[]
Program Committee PMAC-2WN 2006
12th International Conference on Parallel and Distributed Systems -
None
2006
Provides a listing of current committee members.
[]
Performance modeling of communication and computation in hybrid MPI and OpenMP applications
12th International Conference on Parallel and Distributed Systems -
None
2006
Performance evaluation and modeling is a crucial process to enable the optimization of parallel programs. Programs written using two programming models, such as MPI and OpenMP, require an analysis to determine both performance efficiency and the most suitable numbers of processes and threads for their execution on a given platform. To study both of these problems, we propose the construction of a model that is based upon a small number of parameters, but is able to capture the complexity of the runtime system. We must incorporate measurements of overheads introduced by each of the programming models, and thus need to model both the network and computational aspects of the system. We have combined two different techniques: static analysis, driven by the OpenUH compiler, to retrieve application signatures and a parallelization overhead measurement benchmark, realized by Sphinx and Perfsuite, to collect system profiles. Finally, we propose a performance evaluation measurement to identify communication and computation efficiency. In this paper we describe our underlying framework, the performance model, and show how our tool can be applied to a sample code
[parallelization overhead measurement benchmark, message passing, Instruments, program diagnostics, OpenMP, parallel program optimization, MPI, Predictive models, static analysis, Supercomputers, Application software, Yarn, parallel programming, OpenUH compiler, Computer science, Analytical models, Sphinx, Perfsuite, Computer networks, Performance analysis, Large-scale systems, software performance evaluation]
Analytical modeling of communication latency in multi-cluster systems
12th International Conference on Parallel and Distributed Systems -
None
2006
This paper addresses the problem of performance modeling of large-scale distributed systems with emphasis on communication networks in heterogeneous multi-cluster systems. The study of interconnection networks is important because the overall performance of a distributed system is often critically hinged on the effectiveness of this part. We present an analytical model to predict message latency in multi-cluster systems in the presence of processor heterogeneity. The model is validated through comprehensive simulation, which demonstrates that the proposed model exhibits a good degree of accuracy for various system sizes and under different operating conditions
[workstation clusters, Java, multicluster systems, message passing, Multiprocessor interconnection networks, Computational modeling, multiprocessor interconnection networks, large-scale distributed systems, interconnection networks, Delay, Analytical models, analytical communication latency modeling, High performance computing, message latency prediction, Computer networks, Performance analysis, Large-scale systems, Communication networks]
An accurate communication model of a heterogeneous cluster based on a switch-enabled Ethernet network
12th International Conference on Parallel and Distributed Systems -
None
2006
The paper presents a communication model of a set of heterogeneous processors interconnected via a switch-enabled Ethernet network. The goal of the model is to accurately predict the contribution of communication operations into the total execution time of parallel applications running on the platform. The presented model takes into account the impact of the heterogeneity of processors on the performance of communication operations. In this paper, we give analytical models for a single point-to-point communication, multiple independent point-to-point communications, multiple one-to-many point-to-point communications, and for a broadcast. Experimental results are presented demonstrating the accuracy of the analytical models
[workstation clusters, switch-enabled Ethernet network, Ethernet networks, multiple independent point-to-point communications, multiple one-to-many point-to-point communications, heterogeneous cluster communication model, Predictive models, Parallel machines, broadcast communication, Power system modeling, parallel processing, Communication switching, telecommunication switching, Analytical models, Operating systems, Bandwidth, Parallel processing, single point-to-point communication, Workstations]
Analytical performance comparison of deterministic, partially- and fully-adaptive routing algorithms in binary n-cubes
12th International Conference on Parallel and Distributed Systems -
None
2006
In this paper, we study the effect of adaptivity of routing algorithm on the overall performance in a hypercube multicomputer using wormhole switching. To this end, we use three accurate analytical models proposed for deterministic, fully-adaptive, and partially-adaptive routing algorithms in hypercube. Surprisingly, our analysis shows that under uniform traffic load, the partially-adaptive routing exhibits a lower performance compared to the deterministic routing with less adaptivity
[Algorithm design and analysis, partially-adaptive routing algorithm, Packet switching, multiprocessing systems, Multiprocessor interconnection networks, Switches, performance evaluation, Routing, hypercube networks, wormhole switching, Telecommunication network topology, hypercube multicomputer, Analytical models, fully-adaptive routing algorithm, binary n-cubes, deterministic routing algorithm, System recovery, Hypercubes, Performance analysis]
Performance evaluation of a fault-tolerant switch for next generation computer networks
12th International Conference on Parallel and Distributed Systems -
None
2006
In this paper, the architecture of a parallel- plane deflection-routed circular banyan (PDCB) network based switching fabric is presented. The PDCB switch has a cyclic, regular, self-routing, simple architecture and fairly good performance. Its performance is improved due to reduction in blocking by two-dimensional path-multiplicity of the proposed architecture. A simple analytical model based on Markov chain has been used to evaluate the performance of the switch under uniform traffic condition. Further, the fault-tolerance and reliability aspect has also been examined
[Packet switching, Switches, performance evaluation, Educational institutions, multistage interconnection networks, parallel-plane deflection-routed circular banyan network based switching fabric, Next generation networking, Delay, telecommunication switching, Markov chain, Fault tolerance, Analytical models, self-routing, Computer network reliability, telecommunication network routing, Computer architecture, Markov processes, next generation computer networks, Computer networks, fault tolerant computing, computer network reliability, fault-tolerant switch]
Non-contiguous processor allocation strategy for 2D mesh connected multicomputers based on sub-meshes available for allocation
12th International Conference on Parallel and Distributed Systems -
None
2006
Contiguous allocation of parallel jobs usually suffers from the degrading effects of fragmentation as it requires that the allocated processors be contiguous and has the same topology as the network topology connecting these processors. In non-contiguous allocation, a job can execute on multiple disjoint smaller sub-meshes rather than always waiting until a single sub-mesh of the requested size is available. Lifting the contiguity condition in non-contiguous allocation is expected to reduce processor fragmentation and increase processor utilization. However, the communication overhead is increased because the distances traversed by messages can be longer. The extra communication overhead depends on how the allocation request is partitioned and allocated to free sub-meshes. In this paper, a new noncontiguous processor allocation strategy, referred to as greedy-available-busy-list, is suggested for the 2D mesh network, and is compared using simulation against the well-known non-contiguous and contiguous allocation strategies. To show the performance improved by proposed strategy, we conducted simulation runs under the assumption of wormhole routing and all-to-all communication pattern. The results show that the proposed strategy can reduce the communication overhead and improve performance substantially in terms of turnaround times of jobs and finish times
[Context, multiprocessing systems, greedy-available-busy-list, greedy algorithms, Parallel machines, Routing, parallel processing, Concurrent computing, Degradation, Mesh networks, 2D mesh connected multicomputers, Network topology, resource allocation, noncontiguous processor allocation, Computer networks, Joining processes]
Performance evaluation of a new end-to-end traffic-aware routing in MANETs
12th International Conference on Parallel and Distributed Systems -
None
2006
There has been a lot of research effort on developing reactive routing algorithms for mobile ad hoc networks (MANETs) over the past few years. Most of these algorithms consider finding the shortest path from source to destination in building a route. However, this can lead to some network nodes being more overloaded than the others. In MANETs resources, such as node power and channel bandwidth are often at a premium and, therefore, it is important to optimise their use as much as possible. Consequently, a traffic-aware technique to distribute the load is very desirable in order to make good utilisation of nodes' resources. Therefore a number of end-to-end traffic aware techniques have been proposed for reactive routing protocols to deal with this challenging issue. In this paper we contribute to this research effort by proposing a new traffic aware technique that can overcome the limitations of the existing methods. Results from an extensive comparative evaluation show that the new technique has superior performance over similar existing end-to-end techniques in terms of the achieved throughput, end-to-end delay and routing overhead
[load balancing, Peer to peer computing, end-to-end traffic-aware routing, Telecommunication traffic, performance evaluation, routing protocol, Throughput, Ad hoc networks, reactive routing protocols, Mobile ad hoc networks, Intelligent networks, MANET, Network topology, mobile communication, traffic-aware load distribution, routing protocols, telecommunication network routing, mobile ad hoc networks, Bandwidth, Routing protocols, Computer networks, ad hoc networks, traffic]
The algorithm to enhance the security of multi-agent in distributed computing environment
12th International Conference on Parallel and Distributed Systems -
None
2006
The usages of public key infrastructure (PKI) in secure e-mail service, e-commerce service, client authentication service with SSL, etc. have increased. However, a PKI faces many challenges in the practice, especially the scalability of the infrastructure. ID-based cryptosystem (ID-C) has been proposed to solve the problems of PKI by eliminating the necessity for the infrastructure to authenticate public keys and manage directories to store certificates. But, the key escrow is integrated in this setting such that private key generator can easily threaten security of agents. In this paper, to enhance the security of multi-agent in distributed computing environment, we first propose an ID-based threshold decryption scheme without key escrow which has a lost share recovery property. Also, the proposed scheme can provide the group division/merge and key update scheme for a dynamic group membership
[Identity-based encryption, multi-agent systems, distributed processing, multiagent security, cryptography, Electronic mail, Dynamic group management, Security, Distributed computing, distributed computing, Postal services, Privacy, lost share recovery, Wireless networks, Public key, Authentication, ID-based threshold decryption(ID-ThD), escrow, Wireless network, Cryptography, ID-based threshold decryption, Key]
Analysis of queueing networks with blocking under active queue management scheme
12th International Conference on Parallel and Distributed Systems -
None
2006
This paper presents a framework for the performance analysis of queueing networks with blocking under active queue management scheme. The analysis is based on a queue-by-queue decomposition technique where each queue is modelled as a GE/GE/1/N queue with single server, R (R ges 2) distinct traffic classes and {N = N<sub>1</sub>, N<sub>2</sub>, ..., N<sub>R</sub>} buffer threshold values per class under first-come-first-serve (FCFS) service rule. The use of queue thresholds is a well known technique for network traffic congestion control. The external traffic is modelled using the generalised exponential (GE) distribution which can capture the bursty property of network traffic. The analytical solution is obtained using the maximum entropy (ME) principle. The forms of the state and blocking probabilities are analytically established at equilibrium via appropriate mean value constraints. The initial numerical results demonstrate the credibility of the proposed analytical solution
[telecommunication congestion control, queue thresholds, queue-by-queue decomposition, first-come-first-serve service rule, Quality of service, Telecommunication traffic, network traffic congestion control, Entropy, Telecommunication congestion control, appropriate mean value constraints, Traffic control, Communication system traffic control, Computer networks, queueing networks, queueing theory, buffer threshold values, performance evaluation, maximum entropy, generalised exponential distribution, maximum entropy methods, active queue management, Internet, Computer network management, Queueing analysis, performance analysis]
Message from the MLMD&amp;#146;06 Chair
12th International Conference on Parallel and Distributed Systems -
None
2006
Presents the welcome message from the conference proceedings.
[]
Technical Program Committee: MLMD 2006
12th International Conference on Parallel and Distributed Systems -
None
2006
Provides a listing of current committee members.
[]
Joint optimization of MAC and network coding for cooperative and competitive wireless multicasting
12th International Conference on Parallel and Distributed Systems -
None
2006
In this paper, we address the problem of cross-layer optimization in medium access control (MAC) and network layers for wireless multicasting with multiple cooperative or competitive source nodes in a simple tandem network. We consider scheduled or random access in MAC layer and model network layer operations as network coding or plain routing. We separately look at the cooperative and competitive operation with total and individual performance objectives. We evaluate the resulting cross-layer interactions between MAC and network layers and specify throughput optimization trade-offs intertwined with energy efficiency objectives. We follow a game-theoretic analysis to point at the inefficiency of the non-cooperative equilibrium with selfish nodes competing for limited network resources. In this context, we introduce distributed cooperation stimulation mechanisms to improve the non-cooperative network operation to the cooperative equilibrium performance
[plain routing, radio networks, MAC optimization, Energy measurement, network coding optimization, Interference, game theory, medium access control, Throughput, Routing, Educational institutions, simple tandem network, distributed cooperation stimulation, Wireless networks, cooperative competitive wireless multicasting, telecommunication network routing, authorisation, multicast communication, Network coding, cross-layer optimization, Energy efficiency, Performance loss, Computer networks, game-theoretic analysis]
A cross-layer optimized error recovery mechanism for real-time video in ad-hoc networks
12th International Conference on Parallel and Distributed Systems -
None
2006
The increase in the bandwidth of wireless channels and the computing power of mobile devices increase the interest in video communications over ad-hoc wireless networks. However, the high error rate and the rapidly changing quality of the radio channels can be devastating for the transport of compressed video. In addition, the bounded playout delay for interactive video limits the effectiveness of retransmission-based error control. In this paper, we propose an error recovery mechanism for real-time video that combines forward error correction (FEC), and multi-path retransmission. Based on the sender to receiver route obtained from lower layers, as well as the video data content, the mechanism determines the allocation of error correction code, as well as the retransmission criteria. We evaluated the effectiveness of the mechanism under different network conditions. Simulation results show that the proposed hybrid error recovery mechanism maintains the video quality under different loss rates and mobility speeds, with less overhead compared to error recovery methods that depend only on fixed FEC allocation
[Error analysis, cross-layer optimized error recovery, Mobile communication, Ad hoc networks, forward error correction, real-time video, ad-hoc wireless networks, multipath retransmission, ad-hoc networks, Wireless networks, video communications, Bandwidth, Video compression, Forward error correction, mobile devices, Computer networks, Error correction, wireless channels, ad hoc networks, video communication, Mobile computing, interactive video]
Energy efficiency in multi-hop CDMA networks: a game theoretic analysis
12th International Conference on Parallel and Distributed Systems -
None
2006
A game-theoretic analysis is used to study the effects of receiver choice on the energy efficiency of multi-hop networks in which the nodes communicate using direct-sequence code division multiple access (DS-CDMA). A Nash equilibrium of the game in which the network nodes can choose their receivers as well as their transmit powers to maximize the total number of bits they transmit per unit of energy is derived. The energy efficiencies resulting from the use of different linear multiuser receivers in this context are compared, looking at both the non-cooperative game and the Pareto optimal solution. For analytical ease, particular attention is paid to asymptotically large networks. Significant gains in energy efficiency are observed when multiuser receivers, particularly the linear minimum mean-square error (MMSE) receiver, are used instead of conventional matched filter receivers
[Pareto optimisation, linear minimum mean-square error receiver, direct-sequence code division multiple access, game theory, Pareto optimal solution, Nash equilibrium, Electrical capacitance tomography, code division multiple access, Multiaccess communication, Game theory, Intelligent networks, linear multiuser receivers, game theoretic analysis, Spread spectrum communication, Pricing, multihop CDMA network energy efficiency, Energy efficiency, Routing protocols, Error correction, mean square error methods, noncooperative game]
Message from the Program Chair
12th International Conference on Parallel and Distributed Systems -
None
2006
Presents the welcome message from the conference proceedings.
[]
Program Committee
12th International Conference on Parallel and Distributed Systems -
None
2006
Provides a listing of current committee members.
[]
Real-time scheduling in heterogeneous dual-core architectures
12th International Conference on Parallel and Distributed Systems -
None
2006
With high computational application, embedded systems are becoming more complex. To achieve high performance in the midst of increased complexity, dual-core SoC (system-on-chip) is used. Of many dual-core architectures, a general purpose CPU and DSP are most widely used. But only a few scheduling policies for this heterogeneous architectures exist to guarantee real-time character. This paper discusses scheduling policy for heterogeneous dual-core architectures. We explore the problem of previous scheduling policy (Gai et al., 2002) based on DPCP (distributed priority ceiling protocol) (Rajkumar et al., 1988; Saewong et al., 1999; Sha et al., 1990) and provide a solution of strict schedulability bound model
[Real time systems, Job shop scheduling, parallel architectures, Digital signal processing chips, heterogeneous dual-core architectures, distributed priority ceiling protocol, Processor scheduling, Embedded system, embedded systems, Computer architecture, Digital signal processing, Computer applications, Streaming media, scheduling, real-time scheduling, system-on-chip, dual-core system-on-chip, Coprocessors]
On-demand high performance computing: image guided neuro-surgery feasibility study
12th International Conference on Parallel and Distributed Systems -
None
2006
The emerging cyberinfrastructure holds the promise of providing on-demand access to high performance network, compute and data resources. Image guided neurosurgery is one of many applications that requires such on-demand access to resources. In this paper, we have studied the feasibility of accessing such resources on-demand. An experiment was designed and carried out across five TeraGrid clusters for this study. This paper provides an analysis of the results and draws some conclusion regarding feasibility of on-demand access to high performance resources
[workstation clusters, Brain, grid computing, Anatomical structure, Supercomputers, Neurosurgery, Finite element methods, image guided neurosurgery, Neoplasms, Hospitals, High performance computing, Surgery, Data visualization, medical computing, on-demand high performance computing, TeraGrid clusters, surgery]
On-line evolutionary resource matching for job scheduling in heterogeneous grid environments
12th International Conference on Parallel and Distributed Systems -
None
2006
In this paper, we describe a resource matcher (RM) developed for the on-line resource matching in heterogeneous grid environments. RM is based on the principles of evolutionary algorithms (EA) and supports dynamic resource sharing, job priorities and preferences, job dependencies on multiple resource types, and resource specific and site-wide policies. We describe the evolutionary algorithm and the models used for representing the resource requirements, preferences, and policies. We evaluate three different methods for bootstrapping RM. We then describe a evolutionary matcher (EM) service $a Web service implementation of RM for on-line resource matching. Preliminary performance results indicate that the EM service is efficient in speed and accuracy and can keep up with high job arrival rates - an important criterion for on-line resource matching systems. The service oriented architecture makes the EM service scalable and extensible and can be integrated with already existing grid services in a straightforward manner
[Service oriented architecture, grid computing, Evolutionary computation, evolutionary algorithms, Web service, Throughput, heterogeneous grid environment, Environmental management, processor scheduling, dynamic resource sharing, Computer science, evolutionary computation, Web services, resource allocation, Prototypes, Load management, online resource matching, Large-scale systems, Internet, evolutionary matcher service, Resource management, job scheduling, online evolutionary resource matching]
Ensuring fairness among participating clusters during multi-site parallel job scheduling
12th International Conference on Parallel and Distributed Systems -
None
2006
Multi-cluster schedulers can dramatically improve average job turn-around time performance by making use of fragmented node resources available throughout the grid. By carefully mapping job's across potentially many clusters, jobs that would otherwise wait in the queue for local cluster resources can begin execution much earlier; thereby improving system utilization and reducing average queue waiting time. In this paper, we demonstrate that these multi-site scheduling techniques can be successfully integrated with fairness policies to ensure that participation in the multi-cluster is beneficial under extremely disparate workload intensities. Furthermore, we demonstrate that the trade-off between fairness and performance is relatively small
[workstation clusters, Job shop scheduling, Switches, multisite scheduling, parallel processing, processor scheduling, multicluster schedulers, multisite parallel job scheduling, Concurrent computing, fairness policy, Processor scheduling, Bandwidth, Grid computing, Computer industry, Computer networks, Internet, Resource management]
Flexible, low-overhead event logging to support resource scheduling
12th International Conference on Parallel and Distributed Systems -
None
2006
Flexible resource management and scheduling policies require detailed system-state information. Traditional, monolithic operating systems with a centralized kernel derive the required information directly, by inspection of internal data structures or maintaining additional accounting data. In systems with distributed or multi-level resource managers that reside in different subsystems and protection domains, direct inspection is unfeasible. In this paper, we present how system event logging - a mechanism usually used in the context of performance analysis and debugging - can also be used for resource scheduling. Event logs provide accumulated, pre-processed, and structured state information independent of the internal structure of individual system components or applications. We describe methods of low-overhead data collection and data analysis and present a prototypical application to multiprocessor scheduling of virtual machines
[distributed resource management, Data analysis, Debugging, Inspection, Data structures, flexible low-overhead event logging, multilevel resource management, resource scheduling, processor scheduling, flexible resource management, system event logging, resource allocation, Operating systems, scheduling policy, Performance analysis, Resource management, Virtual prototyping, Kernel, Protection]
Scheduling tradeoffs for heterogeneous computing on an advanced space processing platform
12th International Conference on Parallel and Distributed Systems -
None
2006
Future spaceborne platforms will require expanded onboard processing payloads to meet increasing mission performance and autonomy requirements. Recently proposed spacecraft systems plan to deploy networked processors configured much like commodity clusters for high-performance computing (HPC). Just as robust job management services have been developed and are required to optimize the performance of ground-based systems, so too will spaceborne clusters require similar management services, especially to meet real-time mission deadlines. In order to gain insight into how best to address the challenge of job management in high-performance, embedded space systems, a management service has been developed for a NASA New Millennium Program (NMP) experiment for the ST-8 mission slated for launch in 2009. This paper presents an overview and analysis of the effects on overall mission performance of adding priority and preemption to a baseline gang scheduler employing opportunistic load-balancing (OLE) on a heterogeneous processing system for space. Experiments are conducted with two mission scenarios including planetary mapping and object tracking
[Real time systems, Preemption, networked processors, heterogeneous processing system, spaceborne platforms, distributed processing, Heterogeneous Computing, Space vehicles, heterogeneous computing, resource allocation, aerospace computing, scheduling, Delta modulation, spacecraft systems, Power generation, Gang Scheduling, Embedded computing, NASA New Millennium Program, NASA, embedded space systems, opportunistic load-balancing, Systems, Embedded Space, Processor scheduling, Real-time Systems, Space missions, High performance computing, real-time systems, gang scheduling, advanced space processing platform, Field programmable gate arrays]
A workflow editor and scheduler for composing applications on computational grids
12th International Conference on Parallel and Distributed Systems -
None
2006
A grid allows dynamic integration of disparate resources. A number of grid platforms like CONDOR, Net-Solve, ISS Cornell are available to harness computing power. However, for grid to be pervasive and usable by a novice user, integration at application level is required. We have designed and developed a scalable infrastructure to support workflow composition. The entire workflow can be saved and reused as a compound application. Our workflow engine has been designed to be independent of the underlying grid infrastructure. It has only a small layer which interfaces with the grid infrastructure. We have implemented and integrated our infrastructure with Sun Grid Engine Enterprise Edition 5.3. Applications like document search and matrix multiplication were composed. We observe increase in performance with increase in number of compute servers during experiments
[workflow scheduling, Sun Grid Engine Enterprise Edition 5.3, grid computing, workflow editor, File servers, Application software, ubiquitous computing, Sun, Engines, computational grids, Computer science, grid infrastructure, Power engineering computing, Processor scheduling, resource allocation, Computer applications, workflow engine, Grid computing, Power engineering and energy]
A DRMAA-based target system interface framework for UNICORE
12th International Conference on Parallel and Distributed Systems -
None
2006
The UNICORE grid technology provides a seamless, secure, and intuitive access to distributed grid resources. UNICORE is a full-grown and well-tested grid middleware system that is used in daily production and research projects worldwide. The success of the UNICORE technology can at least partially be explained by the fact that UNICORE consists of a three-tier architecture. In this paper, we present the evolution of one of its tiers that is mainly used for job and resource management. This evolution integrates the distributed resource management application API (DRMAA) of the Global Grid Forum providing UNICORE with a standardized interface to underlying resource management systems and other grid systems
[Computer interfaces, Java, Production systems, Torque, resource management systems, grid computing, grid middleware system, Supercomputers, Mathematics, Middleware, distributed grid resources, distributed resource management application API, Computer science, three-tier architecture, resource allocation, Grid computing, DRMAA-based target system interface framework, UNICORE grid technology, Resource management, middleware]
Message from the general chair
2007 International Conference on Parallel and Distributed Systems
None
2007
This paper describes the considerations taken in designing cost effective embedded processor development kits to support the take-home self-practice pedagogical strategy. Since each student is issued with such a take-home development kit, special design considerations were given to the choice of processor, the on-board peripheral support, the mode of software development and the development tool support. Two embedded processor development platforms of different complexity are described. One is an 8051 based kit and the other is based on the ARM RISC processor.
[computer science education, Microcontrollers, software development, Laboratories, reduced instruction set computing, Debugging, embedded processor development, Programming, microprocessor chips, onboard peripheral support, take-home self-practice pedagogies, Education, ARM RISC processor, Computer architecture, Software, software engineering, computational complexity]
Message from the honorary chair
2007 International Conference on Parallel and Distributed Systems
None
2007
In early 2003, the Singapore Economic Development Board identified Embedded Systems as a major 'new growth area' for the Singapore economy, building upon the existing infrastructure of technological companies, and proven ability for companies both local and overseas, to conduct advanced research and development, as well as specialist production, in Singapore. In response to this, Nanyang Technological University School of Computer Engineering proposed, and deployed, a part-time graduate masters' programme in embedded systems. This paper discusses the need for such an embedded education in Singapore, the syllabus and course coverage which has been developed, and the response of students and industry to the initiative. Pitfalls and problems are identified at each stage.
[Industries, Computers, computer science education, Singapore economy, Laboratories, Singapore Economic Development Board, Nanyang Technological University, Educational institutions, part-time graduate masters programme, School of Computer Engineering, graduate education, Embedded system, Education, educational courses, embedded systems, Software]
Message from the program chair
2007 International Conference on Parallel and Distributed Systems
None
2007
Embedded systems are dedicated computers that have fixed functions. Although they have been around for sometime, they have explosive growth recently due to the ability of making smaller and faster semiconductor chips. Embedded systems can be divided into two categories. One has the operating system, the other one is without the operating system. When the functions of embedded systems are getting more and more sophisticated, especially when networking is involved, the operating system is needed to make the embedded system design easier. Although there are several embedded operating systems available, the embedded Linux has gained wide popularity. This paper is about the curriculum design of how to teach an embedded Linux system course. It also gives several references which are helpful for the course.
[computer science education, Debugging, Media, embedded operating systems, educational course, teaching, embedded Linux system, Linux, Operating systems, Embedded system, educational courses, embedded systems, Kernel, curriculum design, Driver circuits]
Organizing committee
2007 International Conference on Parallel and Distributed Systems
None
2007
Operating systems play an important role in interacting with hardware and software, while embedded operating systems deal more with hardware-specific functions, optimization and customization. To emphasize the education in embedded operating systems, in this paper, we present the design methodologies of two courses in this area: embedded operating systems and real-time embedded operating systems for SoC. The former course focus on the basic concepts of embedded kernel primitives. The latter one emphasizes more on the insight of the real-time kernel and its scheduling protocols. Both courses consist of comprehensive hands-on practices to provide students more opportunities to fully participate in this blooming field.
[computer science education, operating system kernels, hardware-specific function, Conferences, Finance, real-time embedded operating system, SoC, scheduling protocol, Scheduling, Parallel algorithms, USA Councils, optimization, educational courses, embedded systems, Nickel, Resource management, system-on-chip]
ICPADS 2007 PC members
2007 International Conference on Parallel and Distributed Systems
None
2007
This paper proposes a vision-based fuzzy 8051 surveillance system which emphasizes on designing the image processing function to detect the invaded states and the 8051 fuzzy servomotors controller to track the invader region. The techniques include discrete wavelet transformation (DWT) image data reducing, edge detection, motion detection, and fuzzy servomotor control. In this paper, image processing techniques are used for invader detection. Using a two-axis camera mount controlled by fuzzy controller, this system could make the camera exactly keeping the invader object on the center of scene. Also, the proposed tracking system has high potential in the guard of security management, the transaction authentication, the residential monitoring.
[Process design, security management, discrete wavelet transformation, Image processing, Image edge detection, discrete wavelet transforms, fuzzy controller, Control systems, Discrete wavelet transforms, transaction authentication, fuzzy control, Fuzzy control, vision-based fuzzy surveillance system, image processing function, Surveillance, fuzzy servomotor controller, servomotors, Cameras, residential monitoring, Servomotors, video surveillance, invader detection, Fuzzy systems]
Evaluation and modeling of power consumption of a heterogeneous dual-core processor
2007 International Conference on Parallel and Distributed Systems
None
2007
Embedded systems such as mobile phones widely adopt heterogeneous dual-core processors to improve the performance and reduce the cost and power consumption. Considering mobile devices that are usually battery-operated, one of the critical issues for such an embedded system is software design which must fully utilize the computation power and minimizes the power consumption. A number of power consumption models were proposed to facilitate designers to evaluate and diagnose the power consumption of the software. Unfortunately, previous studies only investigated single-core processors, but have not yet developed power consumption models for heterogeneous dual-core processors. In this paper, a power consumption model for the dual-core processor based on TI OMAP5912 is proposed for evaluating the power consumption of embedded software. This tool not only estimates the energy consumed by embedded software but also analyzes the energy distributions of the software on each processor core. With the aid of this tool, designers could diagnose the power consumption of dual-core embedded software efficiently and easily.
[Energy consumption, Embedded computing, Costs, multiprocessing systems, dual-core embedded software, energy consumption estimation, Mobile handsets, embedded software power consumption, Power system modeling, Embedded software, Software design, power aware computing, power consumption modeling, Embedded system, embedded systems, power consumption evaluation, embedded system, energy distribution, heterogeneous dual-core processor, Software tools, low-power electronics, Mobile computing]
SSEST: Summer school on embedded system technologies
2007 International Conference on Parallel and Distributed Systems
None
2007
In recent years, embedded system technologies are evolving rapidly and in short-handedness of experienced engineers has become a serious problem. To solve this problem from the point of view of students and young engineers, a committee is organized by those members. We, the committee members, planned to provide a new educational material and a curriculum for students and young engineers to have an opportunity to learn embedded system technologies during summer time as summer school on embedded system technologies (SSEST). In this school, we aim to provide an education for basic knowledge and techniques about embedded systems through a whole development process and an interchange among people of different universities and companies through the processes. In this paper, we introduce the plan of summer school, and its educational material and the curriculum of SSEST in 2005 and 2006.
[Educational programs, educational material, summer school curriculum, Government, Educational technology, Educational institutions, educational technology, Embedded software, Information science, Materials science and technology, Embedded system, educational courses, embedded systems, Information processing, Hardware, embedded system technology]
Ksensor: Multithreaded kernel-level probe for passive QoS monitoring
2007 International Conference on Parallel and Distributed Systems
None
2007
Traffic monitoring is an increasingly important discipline for nowadays networking, as Accounting, Security and Traffic Engineering lay on it. Besides, traffic bandwidth has increased exponentially in the last few years, and high-speed network monitoring has become a challenging task. Performance requirements are highly relevant for passive QoS monitoring systems. A low-level study of the capturing and processing stages on a traffic analysis system (TAS) has shown room for improvement. We provide an architecture able to cope with high-speed traffic monitoring using commodity hardware. Our system is intended to exploit the parallelism available in up-to- date workstations, which also introduces constraints for multithreaded QoS analysis. This paper presents a kernel-level framework (ksensor) that, keeping the previous requirements, removes some issues from user-level processing and effectively integrates QoS algorithms, improving the overall performance.
[Ethernet networks, multi-threading, traffic analysis system, Telecommunication traffic, Quality of service, ksensor multithreaded kernel-level probe, network traffic monitoring, quality of service, Information analysis, user-level processing, computer network management, passive QoS monitoring, Operating systems, Intrusion detection, Bandwidth, Hardware, Probes, Monitoring, telecommunication traffic]
A curriculum design on embedded system education for first-year graduate students
2007 International Conference on Parallel and Distributed Systems
None
2007
The embedded system is blooming. A lot of undergraduates are absorbed to devote themselves to this domain in recent years. Generally speaking, most undergraduates are deficient in the background knowledge about embedded system science. To build the essential background knowledge, we have a curriculum on embedded systems for first-year graduate students in their first summer vacation. We surveyed the curriculum of many universities and found them focuses on programming skills training. The students didn't interest in learning through this kind of curriculum in our empirical results. As reforming the curriculum year by year, we have devised a curriculum delivering the background knowledge to students by effective methods. The curriculum contains two major parts. The first part is embedded system guidance courses. It navigates the students in the embedded system science. Students will understand the applications domains, development methods, research issues, and challenges of embedded systems through these courses. The lecturer must issue some open-ended questions and assign homework of paper reading to help students thinking. The second part is the programming skills training. It is the hardest challenge for most students because they are deficient in the background knowledge of development tools as well as the hardware. Lab practice is an efficient way familiar with development tools and hardware for students. We carry on the teaching activities in an iterative and incremental method. The lecturer must depend on each student's learning status to adjust the quantities of Lab units. According to a survey of students went through this curriculum, it shows the teaching effect is quite good for most students.
[Electrical engineering, computer science education, embedded system education, Navigation, embedded system guidance course, Application software, Programming profession, Embedded software, Computer science, Embedded system, educational courses, embedded systems, Hardware, Systems engineering education, programming skills training, Iterative methods, curriculum design, embedded system science, programming]
Loop recreation for thread-level speculation
2007 International Conference on Parallel and Distributed Systems
None
2007
For some sequential loops, existing techniques that form speculative threads only at their loop boundaries do not adequately expose the speculative parallelism inherent in them. This is because some inter-iteration dependences, which translate into inter-thread dependences at run time, are too costly to synchronize or speculate. This paper presents a novel compiler technique, called loop recreation, to transform a loop into a prologue, a kernel loop - formed with instructions from two adjacent iterations, and an epilogue so that the inter-iteration dependences in the kernel are less costly to enforce at run time than those in the original loop. We prove the concept by giving an algorithm for finding an optimal loop recreation with respect to a simple misspeculation cost model and by demonstrating performance advantages of loop recreation over two recent techniques for speculative multi-core systems running four irregular applications with indirect array accesses.
[loop recreation, Optimizing compilers, multi-threading, Light rail systems, Data structures, inter iteration dependences, Yarn, program compilers, compiler technique, Multithreading, Microprocessors, Cost function, Hardware, thread-level speculation, Australia, Kernel]
Teaching embedded systems software: The HKUST experience
2007 International Conference on Parallel and Distributed Systems
None
2007
Trends in embedded systems indicate a growing importance of the software component of these systems, and the concomitant increase in the importance of formal design and development of embedded software. In this paper we share our experience with designing and offering courses related to embedded software in the Department of Computer Science and Engineering at the Hong Kong University of Science and Technology. We give a detailed overview of the courses, the structure and the teaching/learning methodology followed in our courses, followed by some discussion and reflections on the courses.
[computer science education, courses, embedded systems software teaching, teaching, embedded software development, Embedded software, software component, Hong Kong University of Science and Technology, Computer science, Design engineering, Software design, Operating systems, Education, Embedded system, educational courses, embedded systems, Computer architecture, Software systems, formal design, Hardware, software engineering, educational institutions]
An integration platform for developing digital life applications
2007 International Conference on Parallel and Distributed Systems
None
2007
This paper describes how we develop a digital life service system by integrating developed separately applications. A lot of Internet appliances are implemented and integrated to evaluate this integration platform. Users can control these appliances by a web browser. We integrated seamlessly these appliances as a whole on the web platform. A home gateway is used as the integration platform. A GPRS modem and a Bluetooth module are connected to the home gateway. The home gateway can thus communicate with all the appliances through the Bluetooth module, and provide a way to approach each appliance from Internet. For each appliance there is a corresponding CGI program to provide as the user interface. Users of the appliance can control each appliance through the web browser. Users can access the appliance through Internet at any place and any time. The GPRS modem can send SMS to user when there are any emergency events.
[Bluetooth, Ground penetrating radar, TV, electronic messaging, internetworking, Control systems, user interfaces, Internet appliance, Home appliances, packet radio networks, Digital Life, Modems, integration platform, GPRS modem, digital life application, user interface, home computing, home automation, Bluetooth module, Web browser, Web pages, User interfaces, CGI program, SMS, home gateway, Internet, Information Appliance, Portals, Home Gateway]
Embedded education for Computer Rank Examination
2007 International Conference on Parallel and Distributed Systems
None
2007
Embedded system has become one of the most important directions in computer education. Embedded system is at the intersection of control system, command and control, wireless data systems, real-time system and so on. The Computer Rank Examination (CRE) is designed to promote the certification for computer application. In this paper, we describe the efforts on embedded education for CRE including the curriculum design and practice.
[Embedded computing, computer science education, Educational products, control engineering education, Educational technology, Computer Rank Examination, Educational institutions, Application software, computer education, Industrial training, embedded education, Embedded system, educational courses, embedded systems, Computer applications, embedded system, Control engineering education, Computer science education, curriculum design]
Constructing double- and triple-erasure-correcting codes with high availability using mirroring and parity approaches
2007 International Conference on Parallel and Distributed Systems
None
2007
With the rapid progress of the capacity and slow pace of the speed/MTTF of hard disks, and increasing size of storage systems, the reliability and availability of storage systems become more and more serious. This paper discusses the method of constructing double- and triple-erasure-correcting codes via combining mirroring and parity approaches in details, and presents a double-erasure code MPDC and a triple-erasure code MPPDC based on one-factorizations of complete graphs. The two codes are simple, easy to implement, and have no disk number limitation. They achieve perfect fault-free load balance and approximately optimal reconstruction load balance. The simulation results show that, compared with other double- and triple-erasure codes, MPDC and MPPDC have comparative light-load and moderate-load performance and better heavy-load performance in fault-free mode. Because parity declustering is used, the two codes are far superior to the other double- and triple-erasure codes in degraded- and reconstruction-mode performance.
[Availability, optimal reconstruction load balance, error correction codes, parity approaches, graph theory, hard disks, Educational institutions, Decoding, performance degradation, RAID, fault-free mode, fault-free load balance, Degradation, Linear code, Fault tolerance, Analytical models, triple-erasure-correcting codes, storage systems, Hard disks, Cost function, Parity check codes, mirroring approaches]
Morphisms from IEEE 802.11 DCF specifications to its EDCA QoS practice with cross-layer interface
2007 International Conference on Parallel and Distributed Systems
None
2007
The IEEE 802.11e EDCA function differentiates traffics via parameterized backoff mechanism. However, many studies shown that QoS cannot be assured by only using the EDCA MAC layer function. Cross-layer controls are proven to be generally essential in QoS management. Moreover, unlike legacy DCF functional having specification and description language (SDL) descriptions in the standard, EDCA does not have standard SDL specifications. Therefore, we propose the SDL behavior model of the EDCA with cross-layer interface (EDCA-XLI) so that students and researchers are able to practice their cross-layer design of real-time applications in wireless networks. We demonstrate the deduction processes to show how the legacy DCF specifications are extended into EDCA-XLI. We call the deductions the morphisms from the DCF to the EDCA-XLI. The message sequence chart (MSC) simulations and a fuzzy QoS control implementation example show that the EDCA- XLI SDL not only realize the behavior defined in the IEEE 802.11e standard but also support generic cross-layer signaling design including receiving and feedback signals to protocols at different layers.
[Chaos, Cross layer design, Protocols, generic cross-layer signaling design, access protocols, QoS management, MAC layer function, fuzzy control, Fuzzy control, EDCA-XLI, Wireless networks, message sequence chart, fuzzy QoS control implementation, feedback signals, specification languages, Signal design, specification-description language, EDCA QoS, telecommunication signalling, quality of service, Standards publication, cross-layer interface, parameterized backoff mechanism, Upper bound, IEEE 802.11 DCF specifications, Streaming media, wireless LAN, Collision avoidance, telecommunication traffic]
Energy-efficient anonymous multicast in mobile ad-hoc networks
2007 International Conference on Parallel and Distributed Systems
None
2007
Protecting personal privacy and energy efficiency are two primary concerns for mobile ad hoc networks. However, no energy-efficient multicast algorithm designed for preserving anonymity has been proposed to date. At the same time, existing approaches cannot be applied to anonymous routing due to their incapability of preserving anonymity. To solve this critical issue, we propose an energy-efficient anonymous multicast algorithm (EEAMA,), which relies only on the statistical properties of the wireless network. This not only makes EEAMA suitable to preserving anonymity, but also reduces its execution time significantly. The complexity of EEAMA increases polynomially with the size of the multicast group, as opposed to the size of the network which determines the complexity of all approaches in the literature. Extensive simulation results show that compared to anonymous unicast, EEAMA offers both better performance (in terms of packet delivery ratio, end-to-end delay and network throughput) and significant energy savings.
[Algorithm design and analysis, telecommunication security, energy-efficient anonymous multicast algorithm, network throughput, packet delivery ratio, Mobile ad hoc networks, Privacy, end-to-end delay, mobile ad-hoc networks, anonymous routing, Wireless networks, mobile ad hoc networks, multicast communication, preserving anonymity, Polynomials, statistical properties, Protection, mobile radio, Routing, Ad hoc networks, multicast, Multicast algorithms, telecommunication network routing, Anonymous routing, Energy efficiency, ad hoc networks, statistical analysis, energy-efficiency]
Teaching complete embedded systems design process with graphical system design methodologies
2007 International Conference on Parallel and Distributed Systems
None
2007
Embedded systems design is becoming essential for engineering students of all majors, not just electrical engineering or computer science. However, using traditional techniques to teach the embedded design process - including the process to design, prototype and deploy an actual embedded system in a semester with traditional approaches presents substantial challenges to educators, especially teaching non-EE or CS majors. In this paper, we will explore an increasingly popular embedded design methodology that employs graphical design techniques to teach students the complete embedded systems design process in a semester and illustrate how graphical system design influences each of the three stages - design, prototype and deploy with real-world examples.
[Process design, computer science education, Chemical engineering, Humans, complete embedded systems design process teaching, electrical engineering, teaching, graphical system design methodologies, Automotive engineering, engineering education, Computer science, computer science, Education, Embedded system, Prototypes, embedded systems, engineering students, Field programmable gate arrays, Biomedical engineering]
Early containment of worms using dummy addresses and connection trace back
2007 International Conference on Parallel and Distributed Systems
None
2007
Most of existing network worms have used address scanning to find vulnerable hosts. Recently, however, worms with more effective propagation strategies have emerged. Among the worms, we focus on the worms that exploit address lists obtained from infected hosts to find other vulnerable hosts effectively. In this paper, we propose a method to detect and contain such worms that try to infect all hosts in an enterprise network. In our method, a detection system inserts some dummy addresses into the address lists of hosts in the network. Then, the system detects the existence of worms when a host tries to open a connection to a dummy address, and then traces back the connection logs to find potentially infected hosts and removes them from the network. Computer simulation results showed our method detected and contained worms with less than 1% infected hosts and less than 5% removed hosts.
[Network servers, invasive software, infected hosts, Computer worms, early worm containment, Computer simulation, enterprise network, Cities and towns, dummy addresses, Viruses (medical), connection trace back, network worms]
A QoS-aware service selection algorithm for multimedia service overlay networks
2007 International Conference on Parallel and Distributed Systems
None
2007
Multimedia applications are becoming more and more popular on today's Internet. Given the enormous development costs and less flexibility, traditional monolithic based approaches are not suitable for building large-scale multimedia systems. By composing of distributed, autonomous services dynamically to provide more complex tasks, service composition provides an attractive way for building large-scale Internet applications. So, multimedia service composition provides a viable solution to large-scale complex multimedia systems. One of the challenging issues of multimedia service composition is how to find service paths to route the data flows through while meeting the applications' resource requirements and specific QoS constraints. However, QoS-aware service routing problem is typically NP-hard. In this paper, we propose a heuristic algorithm named greedy-EF to solve this problem more effectively. More specially, greedy-EF uses an aggregate function to evaluate the QoS conditions for each service instance, and a hop-by-hop service selection approach to explore the proper service path. Simulations show that greedy-EF algorithm can achieve desired QoS assurances as well as load balancing in multimedia service overlay networks.
[Costs, Heuristic algorithms, communication complexity, greedy-EF algorithm, Delay, Web and internet services, multimedia service overlay networks, hop-by-hop service selection approach, large-scale complex multimedia systems, Large-scale systems, multimedia communication, Availability, Multimedia systems, greedy algorithms, multimedia service composition, QoS-aware service selection algorithm, quality of service, large-scale multimedia system, NP-hard problem, Aggregates, QoS-aware service routing problem, telecommunication network routing, Streaming media, Load management, Internet]
SWEVM: a software EVM for embedded system programming
2007 International Conference on Parallel and Distributed Systems
None
2007
EVM (evaluation module), also known as EVB, has become an important tool in developing embedded systems because it can shorten the development cycle. It is also very useful in course modules which cover embedded system programming. However, an EVM with the configuration suitable for the diversity in embedded system programming is quite expensive such that setting up a laboratory for students to practice programming embedded system sums a large amount of funding. This paper presents the design and the implementation of a software EVM, namely SWEVM, which serves as an effective tool in teaching embedded system programming. SWEVM can emulate the main functionality of Creator PreSOC kit which is an EVM product of Mircotime Inc., a Taiwanese company. Its design is highly modularized with good configurability for both flexibility and expandability. Students can easily exercise the common make-load-execute cycle of embedded system programming using SWEVM with no hardware device other than an ordinary PC. Therefore, SWEVM serves as a good alternative to the costly hardware EVM for education purposes.
[System testing, course module, educational computing, Object oriented modeling, Laboratories, Programming profession, Embedded software, Embedded system, Education, educational courses, embedded systems, evaluation module, Hardware, embedded system programming, System-on-a-chip, Software tools]
Diameter bounds of cubelike recursive networks
2007 International Conference on Parallel and Distributed Systems
None
2007
The cubelike recursive networks is a special sub family of the binary interconnection networks. Typical cubelike recursive networks include the hypercube, the crossed cube, the Mobius cube, the generalized twisted cube, the twisted n-cube and the twisted-cube connected network. In a general sense, lots of their topological properties and network parameters are identical, but their diameters are quite different. This work makes the following contributions: Firstly, the definitions of sub-network and super-network are introduced to explain the recursive nature on structure of the cubelike recursive networks. Secondly, the supremum and infimum of the cubelike recursive networks' diameters are n and [~(n +1)/2] respectively, which are proved according to these definitions. Finally, a routing algorithm of cubelike recursive networks is proposed, with an example presented to explain how the algorithm works.
[cubelike recursive networks, hypercube, Multiprocessor interconnection networks, network routing, hypercube network, supernetwork, Routing, hypercube networks, network topology, Sun, Computer science, Network topology, binary interconnection networks, generalized twisted cube connected network, interconnection network, sub-network, routing algorithm, Hypercubes, crossed cube network, Computer networks, diameter bounds, Mobius cube network]
From ISA to application design via RTOS&#x2014; a course design framework for embedded software
2007 International Conference on Parallel and Distributed Systems
None
2007
Embedded systems have pervaded every aspect of our daily lives, however their design and verification are often accomplished using ad hoc and trial-and-error methods. Courses introducing systematic and more formal methods are required. However, currently there is little consensus on what a standard syllabus for an undergraduate course on embedded software design should cover. This paper proposes a course design that have undergone thorough experimentations and evaluations through the last four years in actual classes. The course starts from the ARM instruction set architecture and concludes with an introduction of Java- based wireless application design. The design of standalone, as well as, RTOS-based embedded software are all introduced. The course has culminated in the generation of embedded software engineers that significantly contribute to the technical industry in Taiwan, spanning from handheld devices to home appliances and from networked systems to personal computer accessories. We hope the proposed curriculum becomes a standard effort at trainingembedded software engineers in both theory and practice.
[RTOS, Java, embedded software design, Instruction sets, instruction set architecture, Application software, Embedded software, Software design, Handheld computers, Embedded system, embedded systems, systems analysis, ISA, Computer architecture, Taiwan, Java based wireless application design, Computer industry, Software standards]
Design and analysis of a low power wireless portable media player
2007 International Conference on Parallel and Distributed Systems
None
2007
For most portable media player (PMP), the demands for large storage space and longer working time usually can not be satisfied at the same time. In this paper, a low power wireless LAN based PMP called WiPMP is proposed. WiPMP is basically a diskless PMP. Instead, it can retrieve a large number of multimedia data from a remote storage device wirelessly. A prototype of WiPMP has been realized on an Intel Xscale embedded platform with embedded Linux as the operating system. A systematic power measurement and analysis method is used in this paper to evaluate the prototype from the perspective of power consumption. The result shows that WiPMP does consume less power and is able to access much more multimedia data than the hard disk based PMP.
[Energy consumption, multimedia data, Multimedia systems, operating system, low power wireless portable media player, embedded Linux, remote storage device, Portable media players, Power measurement, Space technology, Operating systems, Wireless networks, Linux, Prototypes, Intel Xscale embedded platform, Hard disks, operating systems (computers), WiPMP, wireless LAN, multimedia communication]
Sharing information of three-dimensional geographic locations through mobile devices
2007 International Conference on Parallel and Distributed Systems
None
2007
People who travel to a place may not know well about the information of the sites. Fortunately, people may have a personal device such as a mobile phone which has a digital camera and broadband wireless accesses, and the device can serve as a convenient equipment to access the information of geographic locations when people visit the place. This paper presents a system, called the GLOBE (GLobe On moBilE), consisting of a mobile, a client and a server. The GLOBE mobile is a mobile phone equipped with GPS, height, direction, angle sensors which are used to estimate the three-dimensional (3D) geographic location on the earth of the objects in the photo. The GLOBE client and server can manage these photos with 3D location information, annotate the objects on the photo, associate the information and travel notes with the photos, and share their photos with people on the Internet. Therefore, people just use the GLOBE mobile to take a picture on the object that they feel interested during a trip, and can immediately obtain the information of the object by searching on the GLOBE server. The proposed system helps people to learn information behind sites and objects they visited, and helps users to manage and share the information during atrip.
[client-server systems, globe-on-mobile system, Image recognition, mobile radio, information sharing, Poles and towers, geographic information systems, Mobile handsets, Digital cameras, GPS, Global Positioning System, GLOBE system, Earth, mobile computing, 3D geographic locations, mobile phone, Object detection, mobile devices, Internet, Web server, Personal digital assistants, client-server system]
Design of a dynamic distributed mobile computing environment
2007 International Conference on Parallel and Distributed Systems
None
2007
Many of the embedded systems have integrated the features of multimedia, networking, and mobility in a device. With the capability of wireless communication, mobile devices can be connected to share information, and even more, work together to accomplish a more complex job. In this paper, a novel distributed mobile computing model based on the concept of dynamic task group is proposed. Through this dynamic distributed mobile computing (Dynamic-DMC) model, computing resources can be added and removed dynamically according to the availability of the resources. An environment supporting the Dynamic-DMC model, called D2MCE, has also been implemented. In D2MCE, a shared memory abstraction layer is provided for the purpose of easy programming. An example D2MCE program is also given in this paper. The Dynamic-DMC model and the D2MCE implementation can also be used in traditional cluster computing. Initial result shows that the performance of D2MCE is acceptable.
[Embedded computing, shared memory abstraction layer, Peer to peer computing, cluster computing, Distributed computing, wireless communication, Concurrent computing, Wireless communication, mobile computing, Embedded system, Parallel processing, mobile devices, shared memory systems, Computer networks, dynamic task group, dynamic distributed mobile computing, Personal digital assistants, Mobile computing]
A scalable parallelization of all-pairs shortest path algorithm for a high performance cluster environment
2007 International Conference on Parallel and Distributed Systems
None
2007
We present a parallelization of the Floyd-Warshall all pairs shortest path algorithm for a distributed environment. A lot of versions of the Floyd-Warshall algorithm have been proposed for a uniprocessor environment, optimizing cache performance and register usage. However, in a distributed environment, communication costs between nodes have to be taken into consideration. We present a novel algorithm, Phased Floyd-Warshall, for a distributed environment, which optimally overlaps computation and communication. Our algorithm is compared with a register optimized version of the blocked all pairs shortest path algorithm [6, 4, 1] which is adapted for a distributed environment. We report speedups of 2.8 in a 16-node cluster and 1.2 in a 32-node cluster for a matrix size of 4096.
[workstation clusters, mainframes, parallel algorithms, cache performance, Costs, graph theory, Educational institutions, Supercomputers, cache storage, distributed environment, Registers, Distributed computing, parallel machines, scalable parallelization, Computer science, Concurrent computing, Shortest path problem, all-pairs shortest path algorithm, Clustering algorithms, Parallel processing, high performance cluster environment, register usage]
An MCU description methodology for initialization code generation software
2007 International Conference on Parallel and Distributed Systems
None
2007
Due to the widespread use of microcontroller unit (MCU) in application and education areas, there is a need for initialization code generation for MCUs to reduce developers' effort and to aid beginners' learning of MCU programming. A common realization for initialization code generation software is a graphical user interface (GUI) with all the available initialization settings for user selection, and the initialization code can be generated based on the user's configuration. This has been achieved by several software using different implementations. This paper evaluates two current initialization code generation software to identify the existing problems and requirements. It then presents a methodology to describe MCU structure to support the software's initialization code generation functionality to achieve flexibility and effectiveness. Our approach observes low development cost, enables user contribution to define and update MCU structure, and provides suitable guide for user to avoid faulty initialization settings.
[Educational programs, Costs, Microcontrollers, graphical user interfaces, graphical user interface, Educational technology, microcontroller unit, Application software, initialization code generation software, program compilers, MCU description methodology, Programming profession, XML, GUI, Computer science education, Assembly, Graphical user interfaces, microcontrollers]
Speculative and distributed simulation of many-particle collision systems
2007 International Conference on Parallel and Distributed Systems
None
2007
As problem size increases dramatically and quickly, many scientific computing applications, such as molecular dynamics and many-particle collision simulations, exhibit high demand on data capacity and computability of the hosting computer systems. Scalable solutions are on demand. Neither well optimized sequential algorithms nor expensive shared memory parallel computers could be the ultimate choice for such large-scale problems. This paper intends to solve this scalability problem by taking advantage of the availability of commodity computers. Aggregated memory and networked computers satisfy the capacity and computability requests. To overcome the strong event dependency nature in particle collision simulations, a speculative execution scheme is proposed to exploit parallelism. Performance analysis and experiments are provided to show the balance between scalability and performance gains.
[Scientific computing, Scalability, digital simulation, parallel programming, speculative execution scheme, Concurrent computing, X), speculative simulation, Computer networks, Large-scale systems, networked computer, Availability, sequential algorithm, Computational modeling, Computer simulation, Particle collisions, scientific computing application, aggregated memory computer, (HI, distributed simulation, Application software, physics computing, molecular dynamics system, relativistic heavy ion collision, many-particle collision system, shared memory parallel computer, performance analysis]
Linux as a teaching aid for embedded systems
2007 International Conference on Parallel and Distributed Systems
None
2007
Linux, an operating system kernel with a heritage derived from the room-sized mainframes of the 1970s, has seen a trememdous amount of multidisciplinary and worldwide development which has resulted in its ability to operate on some of the smallest and lowest power microprocessors available today. With its ongoing penetration into mass market embedded systems such as smart phones, automotive and aeronautical equipment, multimedia entertainment devices and personal digital assistants, versions of Linux are now the operating system of choice for many embedded developers. However Linux tends to be overlooked in the more traditional embedded electronics education. This paper explores the characteristics of embedded Linux that affect the education of embedded systems, and describes a tested educational model for its teaching within a general embedded systems education course.
[computer science education, operating system kernels, Multimedia systems, operating system kernel, embedded electronics education, teaching, Automotive engineering, power microprocessors, Linux, Operating systems, Microprocessors, teaching aid, Education, Embedded system, embedded systems, computer aided instruction, Kernel, Personal digital assistants, Smart phones]
Computation and communication schedule optimization for jobs with shared data
2007 International Conference on Parallel and Distributed Systems
None
2007
Almost every computation job requires input data in order to find the solution, and the computation cannot proceed without the required data becoming available. As a result a proper interleaving of data transfer and job execution has a significant impact on the overall efficiency. In this paper we analyze the computational complexity of the shared data job scheduling problem, with and without consideration of storage capacity constraint. We show that if there is an upper bound on the server capacity, the problem is NP-complete, even when each job depends on at most three data. On the other hand, if there is no upper bound on the server capacity, we show that there exists an efficient algorithm that gives optimal job schedule when each job depends on at most two data. We also give an efficient heuristic algorithm that gives good schedule for cases where there is no limit on the number of data a job may access.
[Heuristic algorithms, Optimal scheduling, communication schedule optimization, Data engineering, server capacity, NP-complete, Computational complexity, Scheduling algorithm, Delay, job execution, Computer science, Upper bound, electronic data interchange, Processor scheduling, Clustering algorithms, scheduling, data transfer, shared data job scheduling problem, computational complexity]
Adaptive computation offloading for energy conservation on battery-powered systems
2007 International Conference on Parallel and Distributed Systems
None
2007
This paper considers the problem of extending the battery lifetime for a portable computer by off loading its computation to a server. Depending on the inputs, computation time for different instances of a program can vary significantly and they are often difficult to predict. Different from previous studies on computation off loading, our approach does not require estimating the computation time before the execution. We execute the program initially on the portable client with a timeout. If the computation is not completed after the timeout, it is off loaded to the server. We first set the timeout to be the minimum computation time that can benefit from off loading. This method is proved to be 2- competitive. We further consider collecting online statistics of the computation time and find the statistically optimal timeout. Finally, we provide guidelines to construct programs with computation off loading. Experiments show that our methods can save up to 17% more energy than existing approaches.
[Portable computers, online statistics, adaptive computation, Batteries, computation off loading, Distributed computing, Guidelines, Computer science, Network servers, computer power supplies, Energy conservation, energy conservation, portable computer, Computer networks, Hardware, statistical analysis, battery-powered systems, portable computers, Power engineering and energy]
Message from APESER organization Committee
2007 International Conference on Parallel and Distributed Systems
None
2007
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Mining temporal mobile sequential patterns in location-based service environments
2007 International Conference on Parallel and Distributed Systems
None
2007
In recent years, a number of studies have been done on location-based service (LBS) due to the wide applications. One important research issue is the tracking and prediction of users' mobile behavior. In this paper, we propose a novel data mining algorithm named TMSP-Mine for efficiently discovering the temporal mobile sequential patterns (TMSPs) of users in LBS environments. To our best knowledge, this is the first work on mining the mobile sequential patterns associated with moving paths and time intervals in LBS environments. Furthermore, we propose novel location prediction strategies that utilize the discovered TMSPs to effectively predict the next movement of mobile users. Finally, we conducted a series of experiments to evaluate the performance of the proposed method under different system conditions by varying various parameters.
[location-based service environments, Switching systems, Location-based services, data mining, Mobile handsets, Data mining, Application software, Biological cells, Genetic algorithms, Computer science, data mining algorithm, Accuracy, mobile computing, location prediction strategies, Temporal mobile sequential patterns, mining temporal mobile sequential patterns, Mobile computing, Business]
Message from P2P-NVE Program Chair
2007 International Conference on Parallel and Distributed Systems
None
2007
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Parallel Minimum Spanning Tree Heuristic for the steiner problem in graphs
2007 International Conference on Parallel and Distributed Systems
None
2007
Given an undirected graph with weights associated with its edges, the Steiner tree problem consists of finding a minimum weight subtree spanning a given subset of (terminal) nodes of the original graph. Minimum spanning tree heuristic (MSTH) is a heuristic for solving the Steiner problem in graphs. In this paper we first review existing algorithms for solving the Steiner problem in graphs. We then introduce a new parallel version of MSTH on three dimensional mesh of trees architecture. We describe our algorithm and analyze its time complexity. The time complexity analysis shows that the algorithm's running time is O(lg2 n) which is comparable with other existing parallel solutions.
[Steiner trees, Algorithm design and analysis, parallel algorithms, trees (mathematics), Very large scale integration, Routing, Steiner tree problem, Delay, Multicast algorithms, Tree graphs, parallel minimum spanning tree heuristic, trees architecture, Cost function, Polynomials, minimum weight subtree spanning, Communication networks, undirected graph, computational complexity]
Improving quality-of-service of file migration policies in high-performance servers
2007 International Conference on Parallel and Distributed Systems
None
2007
Previous work demonstrates that migrating files between disks may induce idleness in servers and facilitate power management. However, these studies focus on steady-state power savings and do not consider the energy consumption and performance overhead of migrating files between disks. In this study, we construct a constrained optimization problem of file location for high-bandwidth applications by considering file migration energy, as well as steady-state energy consumption. We present an algorithm that improves quality-of-service by 63% for a streaming video server compared with existing techniques while maintaining comparable energy saving.
[Energy consumption, Power system management, Quality of service, File servers, Steady-state, quality of service, video servers, Delay, File systems, Bandwidth, file migration energy, Streaming media, steady-state energy consumption, video streaming, high-performance servers, file migration policies, steady-state power savings, Energy management, quality-of-service]
Message from PMAC-2WN Program Co-Chairs
2007 International Conference on Parallel and Distributed Systems
None
2007
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from SRMPDS Program Chair
2007 International Conference on Parallel and Distributed Systems
None
2007
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
RAASP: Resource allocation analysis service and portal
2007 International Conference on Parallel and Distributed Systems
None
2007
Understanding how resources are allocated in a cluster or grid environment can be a daunting task. The number of schedulable processors has increased dramatically, resulting in a greater need for resource allocation analysis tools. This increase comes from hardware developments as well as inter-domain resource sharing. This paper describes a toolset for monitoring the allocation of resources in a high performance computing cluster or grid environment. Specifically, we are interested in monitoring and analyzing the allocation of resources in a batch scheduled universe. The aim is to categorize the data so that it is useful for understanding particular questions about resource allocation.
[High Performance Computing, grid computing, portals, interdomain resource sharing, schedulable processor, Resource description framework, Statistics, RAASP, Information analysis, Grid Computing, Computer science, Processor scheduling, resource allocation, High performance computing, resource allocation analysis service-portal, Cluster Computing, Batch Schedulers, scheduling, grid environment, Grid computing, Resource management, Portals, Monitoring]
The First Asia-Pacific Workshop on Embedded System Education and Research (APESER)
2007 International Conference on Parallel and Distributed Systems
None
2007
false
[]
An FPGA implementation of a snoop cache with synchronization for a multiprocessor system-on-chip
2007 International Conference on Parallel and Distributed Systems
None
2007
FPGA based multiprocessor SoC (MPSoC) is an on-chip multiprocessor with fully programmable feature which can reduce development cost and achieve performance requirement. In order to provide an MPSoC with the low-overhead communication and synchronization methods, this paper attempts to introduce the TSVM (tagged shared variable memory) cache to a snooping cache on the MPSoC. The TSVM cache can improve a performance by combining communication and synchronization with the coherence maintenance. Using an FPGA, we evaluate how extending a conventional snooping cache affects circuitries and clock speed. As a result, the growth of hardware amount and the degradation of clock speed are only 5% and 2% respectively. It is also confirmed that the TSVM cache improves significantly performance and energy efficiency by stalling in synchronization.
[Energy consumption, Costs, Computational modeling, field programmable gate arrays, multiprocessor system-on-chip, Circuits, FPGA, SoC, cache storage, coherence maintenance, Synchronization, low-overhead communication, Multiprocessing systems, synchronisation, tagged shared variable memory cache, snoop cache, synchronization, clock speed, Hardware, Central Processing Unit, system-on-chip, Field programmable gate arrays, Clocks]
2007 International Workshop on Peer-to-Peer Networked Virtual Environments (P2P-NVE 2007)
2007 International Conference on Parallel and Distributed Systems
None
2007
false
[]
Optimizing Katsevich image reconstruction algorithm on multicore processors
2007 International Conference on Parallel and Distributed Systems
None
2007
The Katsevich image reconstruction algorithm is the first theoretically exact cone beam image reconstruction algorithm for a helical scanning path in computed tomography (CT). However, it requires much more computation and memory than other CT algorithms. Fortunately, there are many opportunities for coarse-grained parallelism using multiple threads and fine-grained parallelism using SIMD units that can be exploited by emerging multi- core processors. In this paper, we implemented and optimized Katsevich image reconstruction based on the previously proposed pi- interval method and cone beam cover method and parallelized them using OpenMP API and SIMD instructions. We also exploited symmetry in the backprojection stage. Our results show that reconstructing a 1024 times 1024 times 1024 image using 5120 512 times 128 projections on a dual-socket quad-core system took 23,798 seconds on our baseline and 642 seconds on our final version, a more than 37 times speedup. Furthermore, by parallelizing the code with more threads we found that the scalability is eventually hinged by the limited front-side bus bandwidth.
[application program interfaces, Katsevich image reconstruction algorithm, Scalability, Optimization methods, multiple threads, X-rays, SIMD, helical scanning path, Yarn, Image reconstruction, Computed tomography, Parallel processing, Attenuation, OpenMP API, computed tomography, backprojection stage, Multicore processing, Filtering, multi-threading, image reconstruction, dual-socket quad-core system, computerised tomography, pi- interval method, cone beam image reconstruction algorithm, fine-grained parallelism, multicore processors, coarse-grained parallelism]
Communication performance of a modular high-bandwidth multiprocessor system
2007 International Conference on Parallel and Distributed Systems
None
2007
This article deals with communication performance of a multiprocessor system implemented using award-wining BCM 1480 multi-core chips. Our system uses high-performance HyperTransport links to interconnect constituent chips, realizing cache-coherent non-uniform memory access. It takes advantage of hardware support from the BCM 1480 chip to attain very impressive communication performance among constituent BCM 1480 chips. This is achieved via an extension to global memory, so that small messages can be pushed quickly across chips in less than one us by the CPU cores through DMA to achieve zero-copy message buffering. It eliminates all overhead associated with the kernel and protocol processing for the utmost interconnect bandwidth in data transfers.
[multiprocessing systems, Multicore processing, Scalability, zero-copy message buffering, kernel processing, performance evaluation, Educational institutions, CPU core, multi-core chips, central processing unit, communication performance, Delay, Multiprocessing systems, protocol processing, non-uniform memory access, High performance computing, Prototypes, data transfer, HyperTransport link, Hardware, Joining processes, Testing, modular high-bandwidth multiprocessor system]
The Third International Workshop on Performance Modelling and Analysis of Communication in Wired and Wireless Networks (PMAC-2WN)
2007 International Conference on Parallel and Distributed Systems
None
2007
false
[]
Prohibition-based MAC protocols for QoS-enhanced mesh networks and high-throughput WLANs
2007 International Conference on Parallel and Distributed Systems
None
2007
To achieve high throughput in wireless networks, collision rate must be very small, while communication overheads (e.g., RTS/CTS dialogues) and channel idleness (e.g., due to backoff) should be both relatively small as compared to data packet durations. Low collision rate is also essential to QoS provisioning in any network employing exponential backoff or a similar strategy. Our proposed solution to the preceding contradicting requirements is to employ prohibition-based mechanisms, which replace the functionality of RTS/CTS dialogues that have been shown to suffer from high communication overhead but only provide limited protection against the hidden terminal problem while in multihop networking environments. The resultant prohibition-based MAC protocols combine binary countdown with busytone, thus inheriting important advantages from both worlds including collision freedom/controllability, prioritization capability, and elimination of hidden terminals. They can also support asynchronous operations which are of practical importance.
[prohibition-based MAC protocol, Wireless application protocol, Communication system control, Access protocols, telecommunication network topology, Throughput, multihop networking environment, WLAN, RTS/CTS dialogues, access protocols, quality of service, Mesh networks, Spread spectrum communication, Media Access Protocol, Controllability, Computer networks, QoS-enhanced mesh network, wireless LAN, Protection]
The Third Workshop on Scheduling and Resource Management for Parallel and Distributed Systems (SRMPDS&#x2019;07)
2007 International Conference on Parallel and Distributed Systems
None
2007
The following topics are dealt with: multiprocessor architecture; grid and cluster computing; information retrieval and mining; overlay and network architecture; power-aware architecture; mobile computing; parallel and distributed systems; security and trustworthy computing; wireless networks; resource scheduling; fault tolerance; peer-to-peer computing; wireless communication; parallel algorithms.
[radio networks, power-aware architecture, parallel algorithms, fault tolerance, peer-to-peer computing, grid computing, data mining, information retrieval, distributed processing, wireless networks, cluster computing, network architecture, resource scheduling, multiprocessor architecture, parallel systems, mobile computing, security, overlay architecture, distributed systems]
FairTrust: Toward secure and high performance P2P networks
2007 International Conference on Parallel and Distributed Systems
None
2007
Peer-to-peer (P2P) networks are becoming increasingly popular and are an exciting new class of innovative, internet-based resource sharing systems. In a P2P network, a reputation system is essential to evaluate the trustworthiness of participating peers and to combat the selfish, dishonest, and malicious peer behaviors. The system collects locally-generated peer feedbacks and aggregates them to yield the global reputation values to represent peer trustworthiness. Reputation system guides peers in choosing a server for services/resources. Most of the approaches for peers to exploit reputation metrics are to select the one with the highest reputation value as a providing server. However, it may lead to unexpectedly low efficiency for high-reputed peers, and prevents P2P networks from taking full advantage of all resources for high performance. Other approaches restrict a peer to select a server in the same or lower reputation values. However, these approaches prevent P2P networks from achieving their goal of widely resource sharing and service exchanges. In this paper, we introduce a trust-based fairness-oriented server selection policy, FairTrust, for a peer to choose another peer to interact. FairTrust takes into account both reputation and capacity factors in server selection. It helps to create a secure P2P communication environment, and meanwhile to take full advantage of system resources for high performance by fair load distribution. Simulation results show the superiority of the FairTrust policy in achieving both high security and high performance in P2P networks in comparison with other related policies.
[FairTrust, peer-to-peer computing, Peer to peer computing, Scalability, peer-to-peer networks, locally-generated peer feedbacks, Telecommunication traffic, File servers, Application software, Computer science, fair load distribution, Network servers, security of data, Internet-based resource sharing systems, Feedback, Internet, Resource management, peer trustworthiness]
Scalable networked virtual environments using unstructured overlays
2007 International Conference on Parallel and Distributed Systems
None
2007
Large-scale Networked Virtual Environments (NVEs) are used to enable rich interactive experiences such as multiplayer games, military and industrial training, and collaborative engineering systems. In these applications, hosts must exchange data updates to maintain a consistent view of the virtual environment. In large-scale environments, the volume of data places a heavy load on the networking and computational resources available to each host. To improve scalability, systems have employed a variety of techniques, including data subscription, data segmentation, and multicast. In this paper, we describe a communication architecture for Networked Virtual Environments that takes advantage of unstructured peer-to-peer (P2P) overlay networks for message distribution. The overlay network design provides scalability while supporting easy deployment over today's Internet. The approach, when combined with data segmentation and data subscription, enables Internet deployment of large-scale NVEs.
[message passing, virtual reality, unstructured peer-to-peer overlay network, Virtual environment, Military computing, peer-to-peer computing, Scalability, Subscriptions, Maintenance engineering, data segmentation, Industrial training, large-scale networked virtual environment communication architecture, interactive experience, message distribution, Collaboration, multicast communication, Systems engineering and theory, Computer networks, Large-scale systems, data subscription]
S/Kademlia: A practicable approach towards secure key-based routing
2007 International Conference on Parallel and Distributed Systems
None
2007
Security is a common problem in completely decentralized peer-to-peer systems. Although several suggestions exist on how to create a secure key-based routing protocol, a practicable approach is still unattended. In this paper we introduce a secure key-based routing protocol based on Kademlia that has a high resilience against common attacks by using parallel lookups over multiple disjoint paths, limiting free nodeld generation with crypto puzzles and introducing a reliable sibling broadcast. The latter is needed to store data in a safe replicated way. We evaluate the security of our proposed extensions to the Kademlia protocol analytically and simulate the effects of multiple disjoint paths on lookup success under the influence of adversarial nodes.
[telecommunication security, parallel lookups, peer-to-peer computing, Peer to peer computing, Data security, cryptography, Topology, reliable sibling broadcast, Resilience, Analytical models, decentralized peer-to-peer systems, Kademlia, routing protocols, Broadcasting, Telematics, Routing protocols, Internet, secure key-based routing protocol, crypto puzzles, Cryptography]
Persistence and communication state transfer in an asynchronous pipe mechanism
2007 International Conference on Parallel and Distributed Systems
None
2007
Emergent wide-area distributed systems like computational grids present opportunities for large scientific applications. On these systems, communication mechanisms have to deal with dynamic resource availability and occurrence of network failures. In this paper, we present the design and implementation of an asynchronous and persistent pipe mechanism, called pi-channels. These communication issues are addressed by combining adaptive caching with data streaming for efficient and fault-tolerant communication. We present the underlying distributed algorithm that implements (a) caching of pipe data segments; (b) asynchronous operation; and (c) re-establishment of connections when a peer leaves and rejoins the computation - part of a communication state transfer mechanism. This makes it possible for different segments (from cache and from writer) of the pipe data to be concurrently streamed to the migrated reader, reducing the retrieval time. Finally, we present some performance results showing the benefits of asynchronous operation.
[Protocols, Pipelines, network failure occurrence, grid computing, cache storage, Distributed computing, computational grids, fault-tolerant communication, Fault tolerant systems, TCPIP, Grid computing, Computer networks, Distributed algorithms, persistence state transfer, Availability, data streaming, dynamic resource availability, asynchronous operation, asynchronous pipe mechanism, Peer to peer computing, wide-area distributed systems, pipe data segments, communication state transfer mechanism, distributed algorithm, distributed algorithms, fault tolerant computing]
ECHoP2P: Emergency call handling over peer-to-peer overlays
2007 International Conference on Parallel and Distributed Systems
None
2007
The impact of the peer-to-peer paradigm increases both in research and in industry. Still, serious applications for P2F'-based systems are rare. On the other hand, emergency call handling (ECU) is (or will be) a mandatory function for VoIP services. In this paper we investigate international legal and technical requirements of ECU and present ECHoPIP, a solution that fulfills these requirements. Based on Globase.KOM and HiPNOS.KOM, ECHoP2P provides the functionality to determine the closest and (geographically) responsible emergency station to a calling peer. Further, emergency calls are processed with highest priority in the overlay, so that quality of service guarantees are given. We evaluated ECHoPIP thoroughly and present the quality and costs analysis, identified tradeoffs and effects of optimization parameters. ECHoPIP provides a fully evaluated solution for emergency call handling and for further location-aware applications.
[Law, peer-to-peer computing, Peer to peer computing, emergency call handling, ECHoP2P, VoIP services, Quality of service, FCC, location-aware application, quality of service, Application software, Multimedia communication, HiPNOS.KOM, Globase.KOM, Communication industry, peer-to-peer overlays, Cost function, Internet telephony, emergency services, Legal factors]
Extended Prefix Hash Trees for a distributed phone book application
2007 International Conference on Parallel and Distributed Systems
None
2007
IP telephony has become one of the most successful applications of the peer-to-peer paradigm. However, providing a scalable peer-to-peer-based distributed telephone directory for searching user entries is still an unsolved challenge. In this paper we present the Extended Prefix Hash Tree algorithm that can be used to implement an indexing infrastructure supporting range queries on top of Distributed Hash Tables. The algorithm supports hierarchical keywords, and is especially designed for situations where a few keywords are very common while others are very uncommon. The design of our algorithm perfectly matches the requirements found in a distributed phone book scenario. We point out the difficulties that previous algorithms had with data that is distributed that way, and evaluate the performance of Extended Prefix Hash Trees with a real-world phone book. The results can be transferred to other scenarios where support for range queries is needed in combination with the decentralization, self-organization, and resilience of an underlying peer-to-peer infrastructure.
[Algorithm design and analysis, peer-to-peer computing, IP telephony, Peer to peer computing, extended prefix hash tree algorithm, distributed phone book application, hierarchical keywords, Application software, Resilience, Computer science, query processing, database indexing, distributed hash tables, distributed databases, indexing infrastructure, Communications technology, tree data structures, Books, Communication networks, user entry searching, Internet telephony, Indexing, peer-to-peer-based distributed telephone directory]
Statistical trust establishment in wireless sensor networks
2007 International Conference on Parallel and Distributed Systems
None
2007
We present a new distributed approach that establishes reputation-based trust among sensor nodes in order to identify malfunctioning and malicious sensor nodes and minimize their impact on applications. Our method adapts well to the special characteristics of wireless sensor networks, the most important being their resource limitations. Our methodology computes statistical trust and a confidence interval around the trust based on direct and indirect experiences of sensor node behavior. By considering the trust confidence interval, we are able to study the tradeoff between the tightness of the trust confidence interval with the resources used in collecting experiences. Furthermore, our approach allows dynamic scaling of redundancy levels based on the trust relationship between the nodes of a wireless sensor network. Using extensive simulations we demonstrate the benefits of our approach over an approach that uses static redundancy levels in terms of reduced energy consumption and longer life of the network. We also find that high confidence trust can be computed on each node with a relatively small memory overhead and used to determine the level of redundancy operations among nodes in the system.
[redundancy levels dynamic scaling, wireless sensor networks, Computational modeling, Redundancy, Power generation economics, Sensor phenomena and characterization, Data processing, Sensor systems, statistical trust establishment, Environmental economics, Distributed computing, Wireless sensor networks, malicious sensor nodes, Computer networks, statistical analysis, reputation-based trust]
A clustering model for memory resource sharing in large scale distributed system
2007 International Conference on Parallel and Distributed Systems
None
2007
As an application of large scale distributed network computing system, RAM Grid tries to solve the problem of memory resource sharing and utilization. Due to the special properties of memory, traditional resource information management approaches cannot be adapted easily. This paper proposes a clustering based resource aggregating scheme under the background of RAM Grid, which can reduce the scale of resource information management efficiently. With analogy to the force field and potential energy theory in physics, the basic model, the force field-potential energy model, and the corresponding distributed algorithms are proposed, respectively. The model and algorithms are also evaluated by real network topologies based simulation.
[Potential energy, large scale distributed network computing system, information management, Random access memory, RAM grid, Read-write memory, resource aggregating scheme, Information management, Distributed computing, force field potential energy model, memory resource sharing, Memory management, distributed algorithms, distributed memory systems, network topologies, Grid computing, Computer networks, Large-scale systems, Resource management, resource information management]
Delay sensitive identity protection in peer-to-peer online gaming environments
2007 International Conference on Parallel and Distributed Systems
None
2007
Peer-to-peer computing overcomes communication bottleneck problems associated with centralized game servers and provides an alternative mechanism to support massively multi-player online gaming (MMOG) applications. A potential security problem associated with this approach relates to player identity. In an MMOG, users from different parts of the network interact with each other in a virtual world. In a peer-to-peer model, these interactions happen directly between the peers and this leads to the IP addresses of peers being available in the network packets. Malicious users can extract the IP addresses of their opponents and the information can be used to gain unfair advantage by compromising their opponents' computers. The well known approach to this problem is the use of anonymizing networks (onion routing and mixing), that anonymize the sender and the destination IP addresses from the peers along a path. However this approach introduces significant delays which are not desirable in MMOG applications. This paper proposes the use of a secret shared key to reduce computational delay and also provides a theoretical framework for trading off the strength of anonymity for reduced delay with respect to classes of interactions in the MMOG. The results suggest that appropriately low delays can be achieved with small reductions in anonymity strength.
[telecommunication security, onion routing, cryptographic protocols, massively multiplayer online gaming applications, security problem, IP addresses, anonymizing networks, Relays, Delay, computer games, secret shared key, Cryptography, IP networks, Protection, Web server, anonymity protocol, onion mixing, peer-to-peer computing, Peer to peer computing, MMOG applications, delay sensitive identity protection, Routing, Application software, Computer science, routing protocols, Games, private key cryptography, peer-to-peer online gaming environments]
Finding candidate spots for replica servers based on demand fluctuation
2007 International Conference on Parallel and Distributed Systems
None
2007
Many service providers distribute various kinds of content over the Internet. They often use replica servers to provide stable service. To position them appropriately, service providers must predict the demands for their services and provide computing capacity sufficient for servicing the demands. Unfortunately, predicting demands is difficult because demand for a service usually fluctuates. Our research group is developing ExaPeer, an infrastructure that apportions computing capacity to services running on hundreds or thousands of trusted machines all over the Internet. In this paper, we describe ExaPeer's approach to dynamically selecting candidate spots for replica servers. The runtime system in ExaPeer detects fluctuations in demand and then dynamically selects candidate spots on which replica servers should be placed to best meet the demand. Experimental results demonstrate that the candidate spots selected by ExaPeer work better than manually selected ones even if the scale of demand changes rapidly.
[ExaPeer, Cloud computing, Fluctuations, network servers, Access protocols, demand fluctuation, Electronic mail, replica servers, Computer science, Uniform resource locators, Web and internet services, Web pages, candidate spots, service providers, Streaming media, Internet, Web server]
Effect of unstable routing in location-aware mobile ad hoc networks on a geographic DHT protocol
2007 International Conference on Parallel and Distributed Systems
None
2007
Geographic distributed hash table (DHT) protocols assume that the set of langkey, valuerang pairs, called indexes, should be distributed among nodes. In geographic DHT protocols, the overhead of index redistribution due to node mobility may be high enough to impact the normal lookup operation if each node contains a large number of indexes. In our previous work, we proposed an efficient lookup protocol, called double indirect access (DIA), that dispenses with index redistribution to improve lookup performance. We also evaluated the performance of DIA by using our own simulator which could show only limited simulation results with the assumptions that all nodes are connected and the packets are transferred instantly without delay. In this paper, we implement DIA into the packet-level simulator ns2 in order to evaluate how DIA is affected by the underlying unstable routing in MANET. The metrics of the effectiveness are considered lookup success rate and bandwidth consumption, and the simulation results show that DIA performs better than a conventional geographic DHT protocol.
[double indirect access, Access protocols, geographic DHT protocol, packet-level simulator ns2, Delay, Mobile ad hoc networks, Computer science, Network servers, mobile computing, mobile communication, Cellular phones, distributed hash table, routing protocols, Bandwidth, location-aware mobile ad hoc networks, Routing protocols, ad hoc networks, Personal digital assistants, unstable routing, Digital audio players]
On incorporating an on-line strip packing algorithm into elastic Grid reservation-based systems
2007 International Conference on Parallel and Distributed Systems
None
2007
In grid systems, users may require assurance for completing their jobs on shared resources. Such guarantees can only be provided by reserving resources in advance. In this paper, we introduce an elastic reservation model, where users can query about a resource availability on a given time interval. They can also provide a reservation duration time and/or number of compute nodes needed as soft constraints to the query. Next, we provide an adapted version of an on-line strip packing algorithm, that takes into a consideration of resource utilization when processing reservation requests. We evaluate our algorithm with a real workload trace and show that the proposed algorithm manages a higher resource utilization and number of acceptance compared to an ad-hoc rigid approach.
[Availability, Strips, Peer to peer computing, grid computing, elastic grid reservation-based system, Quality of service, ad-hoc rigid approach, on-line strip packing algorithm, Distributed computing, Processor scheduling, resource allocation, Computer applications, Grid computing, Computer networks, Resource management, resource utilization]
Directions for Peer-to-Peer based mobile pervasive augmented reality gaming
2007 International Conference on Parallel and Distributed Systems
None
2007
Pervasive augmented reality gaming and mobile P2P are both attracting significant focus and much progress has occurred recently. Augmented reality games extend reality with virtual elements to enhance game experience and mobile P2P networks provide autonomous, self-organizing, scalable and robust communication platforms. We review the technological challenges in both mobile P2P computing and AR domains by investigating some existing related works. Then we propose possible directions of the mobile AR collaborative gaming over P2P networks and show its advantages over other AR gaming and discuss the feasibility and future requirements regarding the support for this type of game.
[Pervasive computing, gaming, peer-to-peer computing, Peer to peer computing, peer-to-peer based mobile pervasive augmented reality gaming, mobile P2P networks, augmented reality, robust communication platforms, Mobile handsets, Application software, pervasive computing, ubiquitous computing, Distributed computing, mobile P2P, Augmented reality, Computer displays, mobile computing, mixed reality, computer games, Virtual reality, Computer networks, Mobile computing]
Virtual Exclusion: An architectural approach to reducing leakage energy in caches for multiprocessor systems
2007 International Conference on Parallel and Distributed Systems
None
2007
This paper proposes virtual exclusion, an architectural technique to reduce leakage energy in the L2 caches for cache-coherent multiprocessor systems. This technique leverages two previously proposed circuits techniques - gated V<sub>dd</sub> and drowsy cache, and proposes a low cost, easily implementable scheme for cache-coherent multiprocessor systems. The virtual exclusion scheme saves leakage energy by keeping the data portion of repetitive cache lines off in the large higher level caches while still manages to maintain multi-level Inclusion, an essential property for an efficient implementation of conventional cache coherence protocols. By exploiting the existing state information in the snoop-based cache coherence protocol, there is almost no extra hardware overhead associated with our scheme. In our experiments, the SPLASH-2 multiprocessor benchmark suite was correctly executed under the new Virtual Exclusion policy and showed an up to 72% savings of leakage energy (46% for SMP and 35% for multicore in L2 on average) over a baseline drowsy L2 cache.
[Energy consumption, multiprocessing systems, Multicore processing, leakage energy reduction, L2 cache, Circuits, Random access memory, multiprocessor systems, Turning, cache storage, virtual exclusion, Multiprocessing systems, snoop-based cache coherence protocol, power aware computing, CMOS technology, Threshold voltage, Leakage current, protocols, Power engineering and energy]
P2P-based geometric computation method for extracting ROI from ubiquitous video cameras
2007 International Conference on Parallel and Distributed Systems
None
2007
This paper proposes a method for extracting the ROI using geometric computations, to obtain information of commonly-interested objects by multiple users. The authors use P2P Delaunay Network to share the regional information shot by cameras with peers, and extract the most intersecting area of camera ranges as ROI. In P2P Delaunay Network, nodes and edges refer to as computers and their connections. This proposed method can construct P2P Delaunay Network using locations of mobile devices as GPS phones or PDAs equipped with a camera, and process computations distributively with these devices. Thereby, a P2P Delaunay network can be constituted, without using the computer resource of other settlements. The method first partitions the plane and assigns the partitions to nodes according to their locations using P2P Delaunay Network. Then, each camera propagates its camera range information to nodes, and each node thus processes computation distributively. The node which has received multiple camera range information computes the intersecting area within its Voronoi region. Finally, the ROI can be determined, by computing the most intersecting area among all nodes. The authors examine the proposed method by evaluating the frequency of region partitioning and data transfer for each node through numerical simulation.
[Pervasive computing, peer-to-peer computing, Peer to peer computing, Video sharing, peer-to-peer-based geometric computation method, computational geometry, video cameras, ubiquitous video camera, Data mining, ubiquitous computing, GPS phone, PDA, Distributed computing, Global Positioning System, mobile device, ROI extraction, Cameras, Computer networks, P2P Delaunay network, Personal digital assistants, video signal processing, Mobile computing, Voronoi region]
Performance study of data stream approximation algorithms in wireless sensor networks
2007 International Conference on Parallel and Distributed Systems
None
2007
Reducing amount of data transmitted enables conserving scarce battery power in wireless sensor networks. In our previous work, we propose two data approximation algorithms for data reduction in sensor networks, maintaining the accuracy of query results within certain bounds. In this paper, we provide a performance study and analysis of these algorithms with emphasis on the types of data for which the algorithms are appropriate. We experimented with different data sets to determine the reduction ratios achieved , energy consumed, errors introduced, complexity of query answering obtained. We provide comparison of our algorithms with related methods. The presented results indicate the superiority of our methods in terms of data reduction and accuracy of query results.
[Algorithm design and analysis, Wavelet transforms, approximation theory, Data analysis, wireless sensor networks, scarce battery power conservation, Sensor phenomena and characterization, Information technology, query processing, query answering, Wireless sensor networks, Histograms, data reduction, Approximation algorithms, Sampling methods, Data communication, data stream approximation algorithms]
A comparative study on Peer-to-Peer failure rate estimation
2007 International Conference on Parallel and Distributed Systems
None
2007
The robustness of Peer-to-Peer systems is challenged by its highly dynamic nature. Frequent peer failure and departure events introduce uncertainty for which is considered exceptional in traditional distributed systems. The difficulty of monitoring such large scale networks is further exacerbated because it has to be done in a completely decentralized way for both scalability and reliability concerns. Some methods for estimating peer failure rate have been applied in Peer-to-Peer systems, however their comparative performance has not yet been reported in the literature. We simulate three different failure rate estimation methods and compare their accuracy and response time with respect to sample size, stabilization interval and neighbour set size. We conclude that the Maximum Likelihood Method introduced is better than the Failure Frequency based Methods commonly used in current Peer-to-Peer systems.
[Checkpointing, Costs, peer-to-peer computing, Peer to peer computing, Scalability, Routing, peer-to-peer failure rate estimation, maximum likelihood estimation, system recovery, Delay, Computer science, peer-to-peer systems, maximum likelihood method, distributed systems, failure frequency based methods, Robustness, Large-scale systems, Software engineering]
ADENS: Efficient address determination for mobile grids
2007 International Conference on Parallel and Distributed Systems
None
2007
This article deals with distributed address determination for mobile Grids, realized by ADENS (address determination via neighboring states), where a new mobile host (MH) determines a conflict-free address for itself efficiently according to state information only from neighboring MHs. With low traffic overhead, ADENS achieves higher address space utilization than the best known approach. The optimal design of basic ADENS has been derived analytically for the first time. Enhanced ADENS can be achieved by designating appropriate MHs (instead of permitting all MHs) to respond to address requests of newly arrived MHs, further improving address space utilization markedly while lowering traffic overhead drastically. Our simulation results reveal that enhanced ADENS enables a mobile grid to operate practically as long as it needs.
[Multicore processing, mobile grids, Computational modeling, grid computing, WiMAX, Routing, Network servers, mobile computing, mobile host, ADENS, Collaboration, Computer applications, Traffic control, Grid computing, address determination via neighboring states, Mobile computing, traffic overhead]
Peer-to-Peer AOI voice chatting for massively multiplayer online games
2007 International Conference on Parallel and Distributed Systems
None
2007
In recent years, massively multiplayer online games (MMOGs) have become more and more popular. Many techniques have been proposed to enhance the experience of using MMOGs, such as realistic graphics, vivid animations, and player communication tools, etc. However, in most MMOGs, communication between players is still based on text, which is unnatural and inconvenient. In this paper, we propose the concept of AOI voice chatting for MMOGs. The term AOI stands for the area of interest; a player in the MMOG only pays attention to his/her AOI. By AOI voice chatting, a player can easily chat by voice with other plays in the AOI. This improves the way players communicate with one another and provides a more realistic virtual environment. We also propose two peer-to-peer schemes, namely QuadCast and SectorCast, to achieve efficient AOI voice chatting for MMOGs. We perform simulation experiments to show that the proposed schemes have reasonable end-to-end delay and affordable bandwidth consumption.
[Virtual environment, peer-to-peer computing, realistic graphics, SectorCast, Peer to peer computing, Avatars, massively multiplayer online games, Delay, area of interest, Computer science, Network servers, computer games, peer-to-peer AOI voice chatting, Bandwidth, vivid animations, Animation, QuadCast, Internet, voice communication, Web server]
Efficient search in file-sharing networks
2007 International Conference on Parallel and Distributed Systems
None
2007
Currently, the most popular file-sharing applications have used either centralized or flood-based search algorithms. Napster has been successful in providing a centralized index with presumably perfect query recall. Succeeding, more distributed protocols like Gnutella and Kazaa have used flood-based search procedures in which a query is propagated through an unstructured network. Query result quality in such networks suffers as queries for items that are rare have high probabilities of not being found as the entire corpus is not covered during a search. In this paper, we present a new and improved implementation of a distributed file-sharing system yielding (1) query result quality better than flooding and close to a centralized index, and (2) low-maintenance network overhead. These improvements result from our optimized approaches to (a) high churn rates (clients and servers frequently entering and leaving the system) and (b) skewed workloads (high variation in access frequencies vs. key). High churn rates are addressed by keeping all data in soft state, which is periodically refreshed, such that the loss of a server or client is quickly reflected in the indexes; higher refresh rates imply fewer false positives. Skewed workloads are load balanced with the use of a layer of indirection for placing and locating data, such that data is partitioned and distributed based on the frequency of use. A trace-driven prototype evaluation based on Gnutella system traces shows that our prototype implementation achieves a low network bandwidth, attains max-average load ratios within a factor of three across all servers, and has positive recall values for over 90% of all queries, despite a high churn rate; the recall would be 100% absent churn.
[Protocols, peer-to-peer computing, telecommunication network management, Peer to peer computing, flood-based search algorithm, query result quality, distributed file-sharing network, Application software, distributed protocol, Computer science, Network servers, Publishing, distributed algorithms, Prototypes, low-maintenance network overhead, Load management, Frequency, Internet, protocols, search problems]
A fast joining operation for highly dynamic chord system
2007 International Conference on Parallel and Distributed Systems
None
2007
Highly dynamic characteristic is one of the most important characteristics of P2P system in nature. Nodes may join in or leave the P2P system at any moment. Frequently joining or leaving must increase the maintenance overhead greatly in DHT system. To decrease the cost, we introduce a new join operation for Chord, named F-Join that is suitable for highly dynamic environments and can fast join in the system. F-Join builds finger table of node by the support of the fingers of the node's successor and predecessor. So, it decreases the lookup cost of building node's finger table and decreases the maintenance overheads finally. The theory and simulations show that F-Join can decrease the maintenance overheads greatly and improve the lookup performance.
[Algorithm design and analysis, finger table nodes, F-Join, Costs, peer-to-peer computing, lookup performance, Buildings, Routing, Proposals, Delay, distributed hash table system, Computer science, table lookup, highly dynamic chord system, P2P system, Fingers, DHT system, Internet, Logic]
Parallel association rule mining based on FI-growth algorithm
2007 International Conference on Parallel and Distributed Systems
None
2007
Association rule mining is one of the most important techniques in data mining. It extracts significant patterns from transaction databases and generates rules used in many decision support applications. Many organizations such as industrial, commercial, or even scientific sites may produce large amount of transactions and attributes. Mining effective rules from such large volumes of data requires much time and computing resources. In this paper, we propose a parallel Fl-growth association rule mining algorithm for rapid extraction of frequent itemsets from large dense databases. We also show that this algorithm can efficiently be parallelized in a cluster computing environment. The preliminary experiments provide quite promising results, with nearly ideal scaling on small clusters and about half of ideal (15 fold speedup) on a thirty-two processor cluster.
[decision support applications, data mining, Data engineering, FI-growth algorithm, transaction databases, cluster computing, Transaction databases, Partitioning algorithms, Association rules, Data mining, database management systems, decision support systems, Concurrent computing, Itemsets, Clustering algorithms, thirty-two processor cluster, Grid computing, Marketing and sales, parallel association rule mining, frequent itemsets]
A forwarding model for Voronoi-based Overlay Network
2007 International Conference on Parallel and Distributed Systems
None
2007
An approach to build highly scalable and robust networked virtual environments (NVEs) is using the peer-to-peer overlay networks. Voronoi-based overlay network (VON) has been proposed to maintain a highly overlay topology consistency in a bandwidth- efficient manner. However, the related research requires all nodes to connect directly with their relevant neighbors. This limits the number of neighbors should appear within the area of interest (AOI) of a given node. A new forwarding model for VON is proposed to solve this problem by connecting only with the nearest neighbors that is called the enclosing neighbors (EN), and propagating the position updates information to other nodes by message forwarding. In this way, the AOI of a given node may be more flexibly expanded and different bandwidth capacities may be more efficiently utilized.
[Virtual environment, peer-to-peer computing, Peer to peer computing, Scalability, overlay network, Peer-to-peer (P2P), Electronic mail, Voronoi-based overlay network, Proposals, bandwidth compression, forwarding model, enclosing neighbors, Nearest neighbor searches, area of interest, Computer science, Network servers, Network topology, networked virtual environments, Voronoi-based Overlay Network (VON), networked virtual environment (NVE), Bandwidth, peer-to-peer overlay networks, overlay topology]
Data representations for mobile devices
2007 International Conference on Parallel and Distributed Systems
None
2007
Data sharing and synchronisation involving mobile devices has been a significant challenge to research. The methods for tracking changes and conflicts often require the involvement of a server or desktop PC with the available storage or processing power to do so, reducing the flexibility and potential of mobile device applications. This paper presents storage mechanisms for storing data on a mobile device which allow data to be related and connected and yet easily synchronised within logical structures. This data can then be synchronised with other hosts with ease. This research bases data on serialisation to allow more logical linking of PIM data among mobile data applications.
[storage allocation, Protocols, data representation, Multiaccess communication, synchronisation, Network servers, mobile computing, File systems, Databases, Web and internet services, Insurance, mobile devices, serialisation, data structures, Australia, Web server, Joining processes, data sharing]
A Peer-To-Peer platform for simulating distributed virtual environments
2007 International Conference on Parallel and Distributed Systems
None
2007
The current expansion of multi-player online games has promoted the growth of large scale distributed virtual environments (DVEs). In these systems, peer-to-peer architectures have been proved as an efficient scheme for supporting massively multi-player applications. In order to research on this type of architecture, stand-alone simulators do not take into account inconsistencies due to network latency, and it is necessary to develop a distributed tool that allows to simulate large-scale DVEs in an efficient way. In this paper, we propose a distributed platform for simulating the behavior of peer-to-peer DVEs. This simulator is implemented following a modular architecture. It is capable of providing the main performance metrics in distributed systems, and it contains all the elements involved in real DVE simulations like the awareness method and the graphic interface. As a result, this tool can be used in real simulations of peer-to-peer DVEs, becoming an invaluable tool for capturing the behavior of this kind of systems.
[Measurement, virtual reality, Virtual environment, peer-to-peer computing, Peer to peer computing, Computational modeling, Computer simulation, Avatars, large scale distributed virtual environments, Distributed computing, Delay, multiplayer applications, peer-to-peer architectures, software architecture, multiplayer online games, computer games, Computer architecture, Large-scale systems, network latency]
Network energy consumption in ad-hoc networks under different radio models
2007 International Conference on Parallel and Distributed Systems
None
2007
In this work, we consider the behavior of a wireless ad- hoc network under TwoRayGround and Shadowing radio models. By means of simulations, we analyze the performance of three routing protocols: AODV, DSR, and DSDV, and study the global energy depletion inside the network. We found that the energy consumption due to AODV and DSR can be very different with respect to the radio model. In particular, we found a maximum relative difference of 70%. On the other hand, for the proactive DSDV routing protocol the energy consumption rate seems to be independent of the radio model, at least for moderate size of the network (256 nodes). Therefore, for small networks, although proactive protocols offer a great advantage in term of delay, the energy consumption is independent against link failures due to the irregularity of the radio medium.
[Energy consumption, radio models, routing protocols, proactive protocols, Ad hoc networks, wireless ad-hoc networks, ad hoc networks, network energy consumption]
Access point selection algorithms for maximizing throughputs in wireless LAN environment
2007 International Conference on Parallel and Distributed Systems
None
2007
In wireless LAN technology, access point selection at each station is a critical problem in order to obtain satisfactory throughputs. The current protocol for access point selection is based on the received signal strength, and a concentration of stations causes a degradation of the entire wireless network. In the present paper, we propose two access point selection algorithms for maximizing two types of throughputs. The first algorithm is proposed for maximizing the average throughput of stations, and the second algorithm is proposed for maximizing the minimum throughput of stations. The experimental results of the proposed algorithms indicate that the proposed algorithms achieve a number of performance improvements compared with previous algorithms.
[Wireless LAN, Wireless application protocol, throughput maximization, Access protocols, Educational technology, Throughput, access protocols, Centralized control, Degradation, Computer science, protocol, Wireless networks, access point selection algorithms, Internet, wireless LAN]
Secure random number agreement for peer-to-peer applications
2007 International Conference on Parallel and Distributed Systems
None
2007
We propose a protocol for a group of peers in a peer- to-peer network to securely generate an agreed random value without the use of a central authority. We can vary the security parameters to maintain security (to a desired probability) in the presence of a high percentage of corrupt and colluding peers. We envision using this protocol to generate random content in a peer-to-peer game. It could also be used for generating input into peer-to-peer protocols that require random values, such as group selection.
[telecommunication security, Protocols, peer-to-peer computing, peer-to-peer network protocol, Peer to peer computing, group selection, Application software, peer-to-peer game, Delay, Computer science, Voting, computer games, secure random number agreement, Cryptography, protocols, National security, Random number generation, Software engineering]
Using synchronized atoms to check distributed programs
2007 International Conference on Parallel and Distributed Systems
None
2007
The execution of a distributed program generates a large state space which needs to be checked in testing and debugging. Atoms are useful abstractions in reducing the state lattice of a distributed computation; we refer to the reduced lattice as the atomic state lattice. However, general predicates remain difficult to check if they are asserted over all states. This paper presents a formulation to attack this problem involving separation of two different concerns: (a) order/synchronization requirement, and (b) computational dependency among atoms. Order requirement is modeled by the serialization of the global states reached by a synchronized set of atoms. Synchrony among atoms is specified by a synchronization predicate. Computational dependency among synchronized states is modeled by a general predicate. With this modeling assumption, the number of the states where a general predicate needs to be checked will be bounded by the number of atoms executed. Two efficient algorithms for checking a general predicate, in the cases where the synchronization predicate is conjunctive or disjunctive, are presented along with their proof of correctness.
[Software testing, program debugging, distributed program execution, program testing, program verification, atomic state lattice, Lattices, Debugging, Interference, State-space methods, program correctness proof, Distributed computing, synchronisation, Computer science, Message passing, system monitoring, synchronized atom, synchronization predicate, Monitoring, distributed programming, Software engineering]
CAVENET: Description and analysis of a toolbox for vehicular networks simulation
2007 International Conference on Parallel and Distributed Systems
None
2007
Simulation of vehicular ad hoc networks is an important aspect of the analysis and design of intelligent routing and broadcasting protocols. In this regard, we propose a lightweight simulator, which separates the mobility model of the vehicles from the details of communication protocols. The mobility model is written in a high-level computing language, and the movement trace of nodes can be exported in a format compatible with standard network simulators. As a proof of concept, by means of extensive simulations, we review the properties of the generated mobility trace by measuring some parameters of interest, such as the average velocity. We show that, in general, the average velocity is a long-range dependent variable, and this fact poses some doubts on how to set the length of the simulation. Accordingly, we discuss the problem of transient data and stationarity, which is well understood only in the deterministic case.
[vehicular ad hoc networks, Protocols, mobile radio, Computational modeling, broadcasting protocols, network simulators, Telecommunication traffic, Ad hoc networks, intelligent routing, Electronic mail, telecommunication computing, Vehicles, Information analysis, Analytical models, Automata, routing protocols, mobility model, Traffic control, vehicular networks simulation toolbox, ad hoc networks, high-level computing language]
Parallel algorithms for chains and anti-chains of points on a plane
2007 International Conference on Parallel and Distributed Systems
None
2007
In this paper we describe efficient parallel algorithms for computing canonical chains and canonical anti-chains partition of a set of points on a plane. The problem to compute chain and anti-chain partition is of interest in VLSI design, computational geometry and in computational molecular biology. A new affine transformation on the set of points is defined which transforms chains in the original point set into anti-chains in the transformed point set.
[parallel algorithms, Sequences, parallel algorithm, k-Chains, Very large scale integration, computational geometry, Partitioning algorithms, set theory, point set transformation, Parallel algorithms, affine transforms, affine transformation, Computer science, Concurrent computing, Computational geometry, canonical chain/antichain partition, Biology computing, Skeleton, Computational biology, Parallel Algorithms]
Cost evaluation on secure routing over powerlines for WMNs
2007 International Conference on Parallel and Distributed Systems
None
2007
In Wireless Mesh Networks (WMNs), wireless nodes exchange routing messages among themselves to build routing tables. Since routing messages are sensitive and critical information, they should be kept confidential and protected. However, the existing WMNs convey the routing messages in an open medium (air) where the messages can be accessed publicly. By abusing the messages, attackers could launch various kinds of attack such as black hole, wormhole, reply, denial of services, and routing table poisoning. In this paper, we present a secure routing architecture for WMNs in which the routing messages are protected by making use of powerline as the routing medium. We also propose three routing protocols for the proposed architecture. The implementation costs of these protocols are evaluated under different scenarios.
[telecommunication security, radio networks, Costs, Wireless application protocol, Mobile communication, Topology, Communication system security, Computer crime, carrier transmission on power lines, cost evaluation, powerline, Wireless mesh networks, routing protocols, Spread spectrum communication, open medium, Routing protocols, wireless mesh network, Protection, routing protocol security]
Modeling and analysis of regional registration based mobile multicast service management
2007 International Conference on Parallel and Distributed Systems
None
2007
We propose and analyze a regional registration based mobile multicast service management scheme which we call URRMoM for efficiently supporting mobile multicast in mobile IP networks. URRMoM extends the design concept of remote subscription (RS) by using mobile multicast agents (MMAs), each covering a number of MHs in its service area through tunneling multicast packets received from the multicast source to them. URRMoM adopts a user-central design allowing each mobile host to determine its optimal MMA service area dynamically depending on its service and mobility characteristics. We develop an analytical model to describe the behavior of a MH operating under URRMoM and derive the optimal service area of the MMA based on the MH's service and mobility characteristics to minimize the total network traffic generated due to multicast packet delivery and multicast tree maintenance. We demonstrate the benefit of our proposed regional registration based mobile multicast service management scheme over existing schemes including RS and RBMoM.
[Subscriptions, Telecommunication traffic, Information analysis, mobile computing, Unicast, remote subscription, multicast communication, Tunneling, mobility handoff, IP networks, regional registration, Mobile IP, URRMoM, mobile multicast agents, multicast packet delivery, Multicast protocols, multicast tree maintenance, mobile multicast service management, multicast, network traffic, mobile IP networks, Character generation, Frequency, user-central design, Mobile computing, telecommunication traffic, performance analysis]
A group-based multi-channel MAC protocol for wireless ad hoc networks
2007 International Conference on Parallel and Distributed Systems
None
2007
When we exploit multiple channels in MAC protocol, we can achieve a higher network throughput than using one single channel due to that multiple transmissions can take place simultaneously. In this paper, we proposed a novel group-based multichannel MAC protocol which cannot only utilize multiple channels to transmit data packets but allow using multiple channels to propagate control packets. The protocol we presented is simple and suitable for wireless ad hoc networks with multiple available channels. The simulation results show that our protocol has the superior performances in network ' to previous work.
[Wireless application protocol, Communication system control, Access protocols, Switches, wireless ad hoc networks, Throughput, group-based multi-channel MAC protocol, Transceivers, access protocols, Mobile ad hoc networks, Computer science, MAC protocol, Media Access Protocol, Performance analysis, wireless channels, ad hoc networks, performance analysis]
New power aware routing algorithm for MANETs using gateway node
2007 International Conference on Parallel and Distributed Systems
None
2007
Mobile Adhoc Network (MANET) is a collection of wireless mobile nodes. Each node works as a router and forms a dynamic topology. Energy is an important resource in the MANET, since every node has equipped with limited battery-power. In this paper, we propose a model called a power-aware routing algorithm for MANET using gateway node. Focus of this work is to minimize number of control message packets, energy consumption and increases the throughput.
[mobile adhoc network, Energy consumption, mobile radio, Telecommunication traffic, Routing, Mobile communication, Batteries, Mobile ad hoc networks, power aware computing, MANET, Network topology, telecommunication network routing, Spread spectrum communication, Competitive intelligence, ad hoc networks, control message packets, energy consumption, Power engineering and energy, power aware routing algorithm, gateway node]
A cooperative game theoretical replica placement technique
2007 International Conference on Parallel and Distributed Systems
None
2007
Creating replicas of frequently accessed data objects across a read intensive network can lead to reduced communication cost and end-user response time. On the contrary, data replication in the presence of writes incurs extra cost due to multiple updates. The selection of data objects and servers requires solving a constraint optimization problem, which is NP-complete in general. A majority of the current state-of-the-art replica placement techniques suffer from high computational complexity issues. To circumvent such issues, we propose a cooperative game theoretical technique in which the servers (players) in the system collectively deliberate to converge at a replica schema that is beneficial to the system as a whole. In particular we make use of the Aumann-Shapley mechanism of cooperative game theory to propose an effective replica placement technique that yields good solutions when the system is very heavily loaded. Experimental comparisons are made against: (1) branch and bound, (2) greedy, (3) genetic, (4) Dutch auction, and (5) English auction. As demonstrated by the experimental results, the proposed technique maintains superior solution quality in terms of lower communication cost and reduced execution time.
[constraint theory, replica placement technique, game theory, distributed processing, Data engineering, constraint optimization problem, NP-complete, Game theory, Computational complexity, Distributed computing, Delay, distributed computing, Computer science, Constraint optimization, read intensive network, Network servers, optimisation, Aumann-Shapley mechanism, data replication, Cost function, Computer networks, cooperative game theory, computational complexity]
Lifetime analysis of the logical topology constructed by homogeneous topology control in wireless mobile networks
2007 International Conference on Parallel and Distributed Systems
None
2007
Topology control protocols construct a logical topology out of the physical communication graph. Logical topology is maintained by logical neighbor lists in every node. Logical topology is used by several upper-layer protocols as a substantial communication map and is prone to link breakages due to node mobility which compels the periodic re-execution of the topology control protocol in so called "Hello" intervals. The problem addressed in this paper is determining the maximum "Hello " interval preserving the connectivity with high probability which is not extensively concerned yet. The simplest form of topology control, homogeneous topology control, is chosen for start. Two connectivity requirements and statistical topology lifetime (STL) are defined. Then, temporal properties of the topology are studied in terms of STL analysis. Finally, an estimation method for evaluation of STL is proposed and based on the method the STL of several scenarios is estimated. The results are compared to the results of extensive simulations which confirm the accuracy of the proposed method.
[wireless sensor networks, Wireless application protocol, Communication system control, topology control protocols, Probability, telecommunication network topology, Mobile communication, physical communication graph, Mobile ad hoc networks, Computer science, Wireless sensor networks, logical topology, Network topology, mobile communication, wireless mobile networks, homogeneous topology control, Broadcasting, statistical topology lifetime, Mobile computing]
On the relative value of local scheduling versus routing in parallel server systems
2007 International Conference on Parallel and Distributed Systems
None
2007
We consider a system with a dispatcher and several identical servers in parallel. Task processing times are known upon arrival. We first study the impact of the local scheduling policy at a server. To this end, we study random routing followed by a priority scheme at each server. Our numerical results show that the performance (mean waiting time) of such a policy could be significantly better than the best known suggested policies that use FCFS at each server. We then propose to use multi-layered round robin routing, which is shown to further improve system performance. Our analysis involves a combination of comparing analytic models, heavy traffic asymptotic and numerical work.
[queueing theory, local scheduling, parallel server system, multilayered round robin routing, network routing, first come first served, Routing, Concurrent computing, Processor scheduling, System performance, priority scheme, Traffic control, scheduling, Internet, Autocorrelation, task processing, Round robin, Web server, Queueing analysis]
Analytical modelling of communication in the rectangular mesh NoC
2007 International Conference on Parallel and Distributed Systems
None
2007
Networks on chip (NoC) emerged as a packets switched, structured communication medium for development of the future systems on chip (SoC). Due to its unique features in terms of scalability and ease of synthesis, the (rectangular) mesh topology is regarded as an appropriate candidate for on-chip network development. This paper presents an analytical model of the average message latency for rectangular mesh topology. The validity of the analysis is verified by comparing the model against the results produced by a discrete-event simulator.
[average message latency, network-on-chip, Scalability, multiprocessor interconnection networks, packet switching, rectangular mesh topology, network topology, Delay, integrated circuit interconnections, Analytical models, Network topology, Network-on-a-chip, Wires, communication modelling, Computer networks, Network synthesis, System-on-a-chip, Integrated circuit modeling, rectangular mesh NoC, wormhole swiching]
Minimum-delay energy-efficient source to multisink routing in wireless sensor networks
2007 International Conference on Parallel and Distributed Systems
None
2007
This paper proposes a simple and scalable approach to multisink routing scheme in wireless sensor networks. Wireless sensor network is a rapidly growing discipline, with new technologies emerging and new applications under development. In addition to providing light and temperature measurements, wireless sensor nodes have applications such as security surveillance, environmental monitoring, and wildlife watching. One potential problem in a sensor network is how to transmit packets efficiently from Single-Source to Multi-Sinks, i.e., to gather data from a single sensor node and to deliver it to multiple clients who are interested in the data. The difficulty of such a scenario is finding the minimum-cost multiple transmission paths. Many routing algorithms have been proposed to solve this problem. Most current algorithms address the reduction of power consumption, and potentially introduce a large delay. This paper proposes a novel multi- path routing algorithm, called Hop-Count based routing (HCR) algorithm, which considers energy cost and transmission delay simultaneously. A hop count vector (HCV) is introduced to support routing decision. Moreover, an additional pruning vector (PV) can further enhance energy efficiency.
[wireless sensor networks, Data security, security surveillance, hop count vector, Routing, pruning vector, Communication system security, Delay, Temperature measurement, Temperature sensors, Wireless sensor networks, minimum-delay energy-efficient source, Surveillance, minimum-cost multiple transmission paths, telecommunication network routing, Energy efficiency, hop-count based routing algorithm, Monitoring, multisink routing]
Accelerating 3-D capacitance extraction in deep sub-micron VLSI design using vector/parallel computing
2007 International Conference on Parallel and Distributed Systems
None
2007
The widespread application of deep sub-micron and multilayer routing techniques makes the interconnection parasitic influence become the main factor to limit the performance of VLSI circuits. Therefore, fast and accurate 3D capacitance extraction is essential for ultra deep sub-micron design (UDSM) of integrated circuits. Parallel processing provides an approach to reducing the simulation turn-around time. In this paper, we present parallel formulations for 3D capacitance extraction based on P-FFT algorithm, on a personal computer (PC) or on a network of PCs. We implement both vector and parallel versions of 3D capacitance extraction algorithm simultaneously and evaluate our implementation quality in terms of speed up achieved.
[fast Fourier transforms, 3D capacitance extraction, capacitance, multilayer routing techniques, Computational modeling, Circuit simulation, VLSI, integrated circuits, deep submicron VLSI design, Integrated circuit interconnections, Very large scale integration, Microcomputers, Nonhomogeneous media, Routing, parallel processing, integrated circuit interconnections, vector computing, interconnection parasitic influence, integrated circuit design, Parallel processing, P-FFT algorithm, circuit CAD, Acceleration, Parasitic capacitance]
Novel critical-path based low-energy scheduling algorithms for heterogeneous multiprocessor real-time embedded systems
2007 International Conference on Parallel and Distributed Systems
None
2007
In this paper, we propose novel low-energy static and dynamic scheduling algorithms with low computational complexities, for heterogeneous multiprocessor real-time embedded systems. We consider task graphs with deadlines and precedence relationships to satisfy. We propose a novel scheme, referred to as "critical-path information track-and update\
[Real time systems, low-energy scheduling algorithms, Energy consumption, multiprocessing systems, Heuristic algorithms, dynamic scheduling algorithms, graph theory, Dynamic scheduling, static scheduling algorithm, critical-path information track-and update, Computational complexity, Scheduling algorithm, processor scheduling, Information analysis, task graphs, Runtime, Embedded system, embedded systems, heterogeneous multiprocessor real-time embedded systems, Timing, energy consumption, computational complexity]
Mathematical performance analysis of product networks
2007 International Conference on Parallel and Distributed Systems
None
2007
In this paper, we propose the first comprehensive mathematical performance model for product networks where fully adaptive routing is applied. Besides the generality of this model which makes it suitable to be used for any product graph, our analysis shows that the proposed model exhibits high accuracy. Simulation results show the validity and accuracy of the model even in heavy traffic and saturation region, where other models have severe problems for prediction.
[saturation region, Adaptive systems, Multiprocessor interconnection networks, adaptive routing, multiprocessor interconnection networks, Telecommunication traffic, Predictive models, performance evaluation, Routing, network topology, product graph, mathematical performance analysis, Analytical models, Network topology, product networks, Traffic control, Performance analysis, Mathematical model]
A two-tier semantic overlay network for P2P search
2007 International Conference on Parallel and Distributed Systems
None
2007
This paper proposes a two-tier semantic peer-to-peer network that facilitates efficient search for context information in wide-area networks. Context data with the same semantics are grouped together into a one-dimensional semantic ring space in the upper-tier network. This is achieved by applying an ontology- based semantic clustering technique and dedicating part of node identifiers to correspond to their data semantics. In the lower-tier network, peers in each semantic cluster are organized as Chord identifier space. Thus, all the nodes in the same semantic cluster know which node is responsible for storing context data triples they are looking for, and context queries can be efficiently routed to those nodes. Through the simulation studies, we demonstrate the effectiveness of our proposed scheme.
[Semantic Peer-to-Peer Network, peer-to-peer computing, wide area networks, Peer to peer computing, Scalability, chord identifier space, query routing, Context Ontology, Ontologies, Resource description framework, P2P search, wide-area network, Data mining, query processing, semantic networks, context information, Context Search, Search engines, ontologies (artificial intelligence), two-tier semantic overlay network, peer-peer network, ontology-based semantic clustering technique, Context modeling]
Performance evaluation of dynamic probabilistic flooding under different mobility models in MANETs
2007 International Conference on Parallel and Distributed Systems
None
2007
In mobile ad hoc networks (MANET), broadcasting is widely used in route discovery and many other network services. The efficiency of broadcasting protocol can affect the performance of the entire network. As such, the simple flooding algorithm aggravates a high number of unnecessary packet rebroadcasts, causing contention and packet collisions. Proper use of probabilistic method can reduce the number of rebroadcasting, therefore reduce the chance of contention and collision among neighbouring nodes. A good probabilistic broadcast protocol can achieve high save rebroadcast and low collision. In this paper, we propose a dynamic probabilistic approach when nodes move according to different mobility models and compare it with simple flooding AODV and fixed probabilistic scheme. Our approach dynamically set the rebroadcast probability according to the number of neighbour nodes distributed in the ad hoc network. Simulation results show our approach performs better than both simple flooding and fixed probabilistic flooding.
[mobile radio, mobility models, dynamic probabilistic flooding, performance evaluation, Topology, Delay, Mobile ad hoc networks, Storms, Wireless networks, mobile ad hoc networks, MANETs, ad-hoc on demand distance vector, Broadcasting, probabilistic broadcast protocol, Routing protocols, Computer networks, ad hoc networks, Informatics, Mobile computing, AODV]
A faster closure algorithm for pattern matching in partial-order event data
2007 International Conference on Parallel and Distributed Systems
None
2007
When working with large sets of partial-order event data, typically collected by monitoring a distributed application, it is often desirable to search for specific patterns of events within this data. When performing such a search, we must be able to identify temporal relationships between complex sets of events. Searching for hierarchically-specified event patterns requires that a convex-closure algorithm be applied to the partial matches at each stage in the computation and the performance of such a closure algorithm can strongly influence the overall speed of pattern matching. In this paper, we propose improvements to an existing closure algorithm that significantly improve its efficiency, and also briefly discuss other possible approaches for improving performance.
[Algorithm design and analysis, convex-closure algorithm, pattern matching, Computerized monitoring, distributed application monitoring, Debugging, distributed processing, convex programming, Application software, Distributed computing, Computer science, Runtime, Interleaved codes, system monitoring, Computer networks, partial-order event data, Pattern matching, temporal relationships]
Extending GridSim with an architecture for failure detection
2007 International Conference on Parallel and Distributed Systems
None
2007
Grid technologies are emerging as the next generation of distributed computing, allowing the aggregation of resources that are geographically distributed across different locations. However, these resources are independent and managed separately by various organizations with different policies. This will have a major impact to users who submit their jobs to the Grid, as they have to deal with issues such as policy heterogeneity, security and fault tolerance. Moreover, the changes of Grid conditions, such as resources that may become unavailable for a period of time due to maintenance and/or suffer failures, would significantly affect the quality of service (QoS) requirements of users. Therefore, it is essential for users to take into account the effects of resource failures during jobs execution. In this paper, we present our work on introducing resource failures and failure detection into the GridSim simulation toolkit. As we need to conduct repeatable and controlled experiments, it is easier to use simulation as a means of studying complex scenarios. We also give a detailed description of the overall design and a use case scenario demonstrating the conditions of resources varied over time.
[fault tolerance, Peer to peer computing, grid computing, Quality of service, GridSim, quality of service, Security, Distributed computing, grid technologies, software fault tolerance, distributed computing, Fault tolerance, Computer architecture, Grid computing, Software, Resource management, failure detection, Testing]
A Legal Information flow (LIF) scheduler for distributed systems
2007 International Conference on Parallel and Distributed Systems
None
2007
In formation systems have to be kept consistent and secure in presence of multiple conflicting transactions and security threats. The role-based access control (RBAC) model is widely used to make systems secure. Here, a subject s is allowed to issue a method op to an object o only if an access right (o, op) is included in the roles granted to the subject s. Even if every access request issued by every subject is authorized in the roles, illegal information flow might occur as well known confinement problem. In this paper, we newly define a legal information flow (LIF) relation (R<sub>1</sub> rArr R<sub>2</sub>) among a pair of role families R<sub>1</sub> and R<sub>2</sub> to prevent illegal information flow. Here, the relation R<sub>1</sub> rArr R<sub>2</sub> shows that no illegal information flow occur if a transaction T\\ with a role family R\\ is performed prior to another transaction T<sub>2</sub> with R<sub>2</sub>. In addition, we discuss an illegal information flow (IIF) relation R<sub>1</sub> rarr R<sub>2</sub>, i.e. illegal information flow necessarily occur if every transaction T<sub>1</sub> with R<sub>1</sub> is performed before T<sub>2</sub> with R<sub>2</sub>. The more significant transaction, the more prior performed. We discuss a legal information flow (LIF) scheduler to synchronize transactions so as to prevent illegal information flow and to serialize conflicting methods from multiple transactions in terms of significancy and information flow relation of roles families.
[Access control, Protocols, legal information flow scheduler, Law, Data security, distributed processing, Concurrency control, Information systems, role-based access control, Information security, authorisation, Permission, distributed systems, Database systems, Legal factors]
A semantic overlay network for unstructured peer-to-peer protocols
2007 International Conference on Parallel and Distributed Systems
None
2007
Peer-to-peer computing has become a popular networking paradigm for file sharing, distributed computing, collaborative working, etc. The widely used unstructured peer-to-peer protocols mainly face two problems affecting their working efficiency: 1) inefficient flooding-based search, 2) topology mismatch between the overlay network and its underlying network. In this paper, we propose to organize nodes into a semantic overlay network called CON which is composed of special interest groups based on nodes' contents. CON guides the content search with semantic information so that it avoids most of the flooding cost. In order to alleviate the mismatch problem, nodes in CON initialize links according to their underlay proximity. Simulation results show that our mechanism efficiently increases the query success rate and reduces the traffic cost and query latency. We also compare CON with the similar work, which illuminates that CON performs better in many aspects.
[semantic overlay network, Protocols, Costs, queueing theory, peer-to-peer computing, Peer to peer computing, Collaborative software, unstructured peer-to-peer protocols, File servers, underlay proximity, flooding-based search, Distributed computing, Delay, distributed computing, Network servers, query success rate, Network topology, semantic networks, Collaborative work, protocols, file sharing]
Federated clusters using the transparent remote Execution (TREx) environment
2007 International Conference on Parallel and Distributed Systems
None
2007
Due to the increasing complexity of scientific models, large-scale simulation tools often require a critical amount of computational power to produce results in a reasonable amount of time. For example, multi-system wireless network simulations involve complex algorithms of traffic balancing and communication control on large geographical areas. Moreover many of these intensive applications are designed for single sequential machines and large sums of money are spent on purchasing powerful servers that can give results in a satisfactory amount of time. The aim of this paper is to introduce a general-purpose tool, dubbed transparent remote execution (TREx), which avoids resorting to expensive servers by providing a cost effective, high performance, distributed solution. TREx is a daemon that dynamically exploits idle operational in-use workstations. Based on elaborate rules of computational resource management, this daemon permits a master to scan workstations within a predefined subnetwork and share the workload among the least occupied processing elements. It also provides a clear framework for parallelization that applications can exploit. By providing a simple way of federating computational resources, such a framework could drastically reduce hardware investments.
[workstation clusters, Costs, multi-system wireless network simulations, Computational modeling, federated clusters, communication control, purchasing powerful servers, transparent remote execution environment, Network servers, traffic balancing, sequential machines, resource allocation, Wireless networks, Clustering algorithms, Traffic control, large-scale simulation tools, Communication system traffic control, computational resource management, Workstations, Large-scale systems, Resource management, IP networks, telecommunication traffic]
Scheduling algorithm for multi-item requests with time constraints in mobile computing environments
2007 International Conference on Parallel and Distributed Systems
None
2007
On-demand broadcast is an effective wireless data dissemination technique to enhance system scalability and capability to handle dynamic user access patterns. Previous studies on time-critical on-demand data broadcast were under the assumption that each client requests only one data item at a time. Little work, however, has considered the on- demand broadcast with time-critical multi-item requests. In this paper, we study the problem arising in this new environment and observe that existing single item based scheduling algorithms are unable to manage multi-item requests efficiently. Thus, a new scheduling algorithm that combines the benefit of data item scheduling and request scheduling is proposed. The performance results show that the proposed algorithm is superior to other classical algorithms under a variety of factors. Our algorithm not only reduces deadline-missing ratio of requests, but also saves broadcast channel bandwidth.
[Real time systems, scheduling algorithm, Application software, Scheduling algorithm, Computer science, mobile computing, Processor scheduling, Bandwidth, Broadcasting, scheduling, wireless data dissemination technique, broadcast channel bandwidth, Database systems, Time factors, on-demand broadcast, Mobile computing, multiitem requests]
Efficient generation of stream programs from loops
2007 International Conference on Parallel and Distributed Systems
None
2007
The efficiency of scientific applications on the Imagine stream processor is increasingly concerned by researchers. One of the obstacles is that the programming language of Imagine does not target the scientific computing. This paper introduces a program transformation algorithm to automatically transform loops to the stream programs executed on Imagine. The optimization for memory accessing is also considered during the transformation. We have implemented the transformation and optimization algorithm with the GFORTRAN frontend. Preliminary results over benchmark kernels show that our approach is a convenient and efficient solution to develop scientific applications on the Imagine stream processor.
[program control structures, stream programs, Scientific computing, Optimization methods, Application software, program compilers, program transformation algorithm, Programming profession, Computer science, Imagine stream processor, Computer languages, Computer architecture, Streaming media, FORTRAN, Page description languages, programming language, Kernel]
Fault-free Hamiltonian cycles in locally twisted cubes under conditional edge faults
2007 International Conference on Parallel and Distributed Systems
None
2007
The locally twisted cube is a variation of hypercube, which possesses some properties superior to the hypercube. In this paper, we investigate the edge-fault-tolerant Hamiltoncity of an n-dimensional locally twisted cube, denoted by LTQ<sub>n</sub>. We show that for any LTQ<sub>n</sub> (n ges 3) with at most 2n - 5 faulty edges in which each node is incident to at least two fault-free edges, there exists a fault-free Hamiltonian cycle. We also demonstrate that our result is optimal with respect to the number of faulty edges tolerated.
[fault tolerance, Multiprocessor interconnection networks, graph theory, conditional edge faults, hypercube network, fault-free Hamiltonian cycles, Routing, hypercube networks, Computer science, Fault tolerance, Network topology, locally twisted cubes, Broadcasting, Hypercubes, edge-fault-tolerant Hamiltoncity, Flexible manufacturing systems]
Resource reclamation using meta-events in a real time java system
2007 International Conference on Parallel and Distributed Systems
None
2007
The objective of this work is to optimize the behavior of a real-time Java system, by identifying and then reaffecting resources which have been allocated to tasks but unused. The tasks in question are periodic tasks which produce aperiodic events such as exceptions. In a real-time system, guaranteeing the task response time leads to resource reservation (CPU, Memory) necessary to their execution. Resource reservation is nevertheless essential because the worst case scenario, where all events are produced at their maximum frequency, is the one that must be catered for. In many cases the worst case scenario never materialises and a part of the resources is wasted. The objective of this work is to present a reclamation mechanism for unused resources. This applies to asynchronous events which have not been activated and other events which we are certain will remain inactivated. An asynchronous event is, at the same time, an object (the AsyncEvent) and the associated handler of that object (AsyncEventHandler). A meta-event is an object used to specify the relationship between asynchronous events. It is this relationship which will enable us to predict the resources which will remain unused and can thus be reallocated. In this paper we will discuss three points. What quantity of resources can be saved from each task ? When can they be collected and for how long will they be available ?
[Real time systems, resource reservation, Java, Costs, real-time Java system, asynchronous event, meta-event, Yarn, Sun, formal specification, Delay, resource reclamation, resource allocation, Operating systems, Fires, Frequency, Resource management]
A dual scale heterogeneous organizational network model
2007 International Conference on Parallel and Distributed Systems
None
2007
Organizational network is one type of complex network featured distinct hierarchical structure, and the kernel of e-Government is information sharing and exchange based on organizational network. Therefore, the establishment of a manageable, scaleable and well- performed network structure becomes one of the crucial parts of the solution to the core problem in e-Government. This paper analyzes the characteristics of administrative organizational network, explores suitable strategies for optimization of organizational network model, defines a organizational tree model, and accordingly proposes a new organizational network model-DSHON (a Dual Scale Heterogeneous Organizational Network model). The proposed model is based on the structure of organizational tree, and the network performance is successfully promoted by adding shortcuts to the original tree according to the organizational distance between and the fitness of the nodes in the network. Simulation results show that the load allocation, transmission efficiency and network connectivity robustness are effectively improved in this model.
[Tree data structures, information sharing, load allocation, Routing, Computer science, Network topology, information exchange, e-government, optimization, organizational tree model, Complex networks, dual scale heterogeneous organizational network model, Robustness, Large-scale systems, Kernel, Erbium, government data processing, organisational aspects, Electronic government]
Enhanced resource management capabilities using standardized job management and data access interfaces within UNICORE Grids
2007 International Conference on Parallel and Distributed Systems
None
2007
Many existing Grid technologies and resource management systems lack a standardized job submission interface in Grid environments or e-Infrastructures. Even if the same language for job description is used, often the interface for job submission is also different in each of these technologies. The evolvement of the standardized Job Submission and Description Language (JSDL) as well as the OGSA - Basic Execution Services (OGSA-BES) pave the way to improve the interoperability of all these technologies enabling cross-Grid job submission and better resource management capabilities. In addition, the BytelO standards provide useful mechanisms for data access that can be used in conjunction with these improved resource management capabilities. This paper describes the integration of these standards into the recently released UNICORE 6 Grid middleware that is based on open standards such as the Web Services Resource Framework (WS-RF) and WS-Addressing (WS-A).
[resource management, Torque, Protocols, OGSA, Web services resource framework, application program interfaces, open systems, description language, grid computing, standardized job submission, Production facilities, Mathematics, basic execution service, Web service addressing, Environmental management, open standard, Technology management, resource allocation, specification languages, middleware, UNICORE grid, Europe, interoperability, Middleware, Web services, data access interface, Resource management]
Optimization on distributed user management in Wireless Sensor Networks
2007 International Conference on Parallel and Distributed Systems
None
2007
In this paper, we address one of the wireless sensor network (WSN) management problems - optimization on the execution of multiple commands. The objective of the paper is to provide efficient support for pre-processing a set of commands before disseminating into the sensor network. It is important that only necessary work will be assigned to the sensor network by virtue of strict energy constraint. The problem is NP-hard. We divide the problem into a series of tractable sub-problems along with their solutions. We present a novel hierarchical quadrant-based field partition mechanism to virtually divide the sensor field. We also classify and model WSN management commands. We then identify merging possibilities for a given command, which results in several merging rules and constraints. Lastly, we evaluate a simulated annealing based search algorithm for finding optimal merge order. The results show that energy can be significantly saved within short time-delay while the overall effects of the final command set still satisfies the users' requirements.
[hierarchical quadrant-based field partition mechanism, simulated annealing, wireless sensor networks, Merging, Multi-Command Optimization, Heuristics, simulated annealing based search algorithm, Partitioning algorithms, communication complexity, Wireless Sensor Networks, Environmental management, Distributed computing, Hierarchical Quadrant-based, Computer science, Wireless sensor networks, NP-hard problem, Engineering management, optimization, Simulated annealing, Computer networks, Computer network management, search problems, distributed user management]
Grid scheduling simulations with GSSIM
2007 International Conference on Parallel and Distributed Systems
None
2007
Grid simulation tools provide frameworks for simulating application scheduling in various grid infrastructures. However, while experimenting with many existing tools, we have encountered two main shortcomings: (i) there are no tools for generating workloads, resources and events ; (ii) it is difficult and time consuming to model different grid levels, i.e. resource brokers, and local level scheduling systems. In this paper we present the grid scheduling simulator (GSSIM), a framework that addresses these shortcomings and provides an easy-to-use Grid scheduling framework for enabling simulations of a wide range of scheduling algorithms in multi-level, heterogeneous grid infrastructures. In order to foster more collaboration in the community at large, GSSIM is complemented with a portal (http://www.gssim.org) that provides a repository of grid scheduling algorithms, synthetic workloads and benchmarks for use with GSSIM.
[Algorithm design and analysis, grid scheduling simulation tool, Computational modeling, grid computing, digital simulation, Scheduling algorithm, Technology management, Processor scheduling, resource allocation, scheduling, Grid computing, Computer networks, Computer network management, Portals, Mesh generation, multilevel heterogeneous grid infrastructure]
Supporting deadline monotonic policy over 802.11 average service time analysis
2007 International Conference on Parallel and Distributed Systems
None
2007
In this paper, we propose a real time scheduling policy over 802.11 DCF protocol called Deadline Monotonic (DM). We evaluate the performance of this policy for a simple scenario where two stations with different delay constraints contend for the channel. For this scenario a Markov chain based analytical model is proposed. From the mathematical model, we derive expressions of the average medium access delay called service time. Analytical results are validated by simulation results using the ns-2 network simulator.
[Wireless LAN, Delay effects, Access protocols, Quality of service, deadline monotonic policy, Scheduling, Multiaccess communication, delay constraint, real time scheduling policy, Markov chain, Analytical models, 802.11 DCF protocol, Markov processes, Delta modulation, wireless channels, Mathematical model, protocols, wireless LAN, distributed coordination function, Collision avoidance, 802.11 average service time analysis]
A new MAC protocol design for long-term applications in wireless sensor networks
2007 International Conference on Parallel and Distributed Systems
None
2007
This paper presents the design, implementation and performance evaluation of a new MAC protocol, called A- MAC, for wireless sensor networks. A-MAC combines the strengths of TDMA and CSMA to achieve the goal of low power transmissions for long-term surveillance and monitoring applications, where sensor nodes are typically vigilant for a long time and inactive most of the time until some event is detected. A-MAC employs an advertisement mechanism to eliminate collisions and reduce the overhearing and idle listening, which are the major energy wastes in wireless sensor networks. The distinctive feature of A-MAC is that a node needs to be active only when necessary as it is the transmitter or the receiver. During other times it can safely turn off its radio. Furthermore, to meet different application requirements, A-MAC supports two operation modes by which nodes can adaptively switch their operation modes according to the instant requirements and conditions of the network. A-MAC is implemented in TinyOS. By comparing A-MAC with existing MAC protocols, we show that A-MAC presents significant improvements in terms of power consumption and throughput.
[Event detection, wireless sensor networks, TDMA, Wireless application protocol, Power transmission, Switches, wireless sensor network, access protocols, Multiaccess communication, MAC protocol design, Wireless sensor networks, Time division multiple access, Surveillance, advertisement mechanism, time division multiple access, Media Access Protocol, CSMA, Monitoring, carrier sense multiple access]
RTMG: Scheduling real-time distributable threads in large-scale, unreliable networks with low message overhead
2007 International Conference on Parallel and Distributed Systems
None
2007
We consider scheduling real-time distributable threads in the presence of node/link failures, message losses, and dynamic node joins and departures. We present a distributed scheduling algorithm called RTMG. The algorithm uses gossip-based communication for discovering eligible nodes. Traditionally, gossip protocols incur high message overhead. We explain that this problem is not that serious. We present a hybrid message propagation protocol with lower message overhead, and improve it by evenly distributing the overhead into all gossip rounds. In scheduling local thread sections, RTMG exploits slacks to optimize gossip time utilization. Thereby, it satisfies end-to-end time constraints with probabilistic assurance. Our simulation studies verify our analytical results.
[Real time systems, Protocols, Uncertainty, gossip-based communication, multi-threading, Subscriptions, message losses, Dynamic scheduling, Yarn, Scheduling algorithm, realtime distributable threads, Multicast algorithms, distributed algorithms, real-time systems, scheduling, message propagation protocol, RTMG, Large-scale systems, gossip protocols, Time factors, distributed scheduling algorithm, node failures, link failures, gossip time utilization]
Comparative evaluation of the non-contiguous processor allocation strategies based on a real workload and a stochastic workload on multicomputers
2007 International Conference on Parallel and Distributed Systems
None
2007
The performance study of the existing noncontiguous processor allocation strategies has been traditionally carried out by means of simulation based on a stochastic workload model to generate a stream of incoming jobs that are submitted to and run on a given message passing parallel machine for a period of time. To validate the performance of the existing allocation algorithms, there has been need to evaluate the algorithms' performance based on a real workload trace. In this study, we evaluate the performance of several well-known processor allocation and job scheduling strategies based on a real workload trace and compare the results against those obtained from using a stochastic workload. Our results reveal that the conclusions reached on the relative performance merits of the allocation strategies when a real workload trace is used are in general compatible with those obtained when a stochastic workload is used.
[Shape, noncontiguous processor allocation strategies, Exponential distribution, Computational modeling, Multiprocessor interconnection networks, Stochastic processes, real workload, message passing parallel machine, Parallel machines, performance evaluation, processor scheduling, Concurrent computing, stochastic workload, Processor scheduling, Network topology, resource allocation, Message passing, multicomputers, job scheduling]
Comparative evaluation of multi-core cache occupancy strategies
2007 International Conference on Parallel and Distributed Systems
None
2007
Intelligent sharing cache space among multiple cores on a Chip Multiprocessor (CMP) has become an important research topic. There are many design options to trade off and many possible performance metrics to evaluate. It generally requires costly simulations to gain insights over a wide-spectrum of cache sharing and partitioning methods. In this paper, we use an efficient single-pass stack simulation method to understand the effectiveness of cache sharing through natural competition (i. e. shared cache) and through static or dynamic cache partitioning, such as equal partition, utility-based partition, etc. The results demonstrate that cache occupancy through natural competition favors the core with more frequently misses. It may take away the needed space for other cores and increase the overall miss ratio. Furthermore, we find that the existing cache partitioning schemes based on fixed-length portioning phases may not be optimal compared with the scheme that allows variable phase lengths.
[Measurement, multiprocessing systems, cache sharing, microprocessor chips, Virtual machining, cache storage, partitioning methods, Delay, Wiring, Information science, Analytical models, Runtime, multi-core cache occupancy strategies, Frequency, comparative evaluation, chip multiprocessor]
Formal verification of concurrent scheduling strategies using TLA
2007 International Conference on Parallel and Distributed Systems
None
2007
There is a high demand for correctness for safety critical systems, often requiring the use of formal verification. Simple, well-understood scheduling strategies ease verification but are often very inefficient. In contrast, efficient concurrent schedulers are often complex and hard to reason about. This paper shows how the temporal logic of action (TLA) can be used to formally reason about a well-understood scheduling strategy in the process of implementing a more efficient one. This is achieved by formally verifying that the efficient strategy preserves all properties, in particular the behaviour, of the simpler strategy. The approach is illustrated with the Hume programming language, which is based on concurrent rich automata. We introduce an efficient extension to the Hume scheduler, and prove that it preserves the behaviour of the standard Hume scheduler.
[concurrent scheduling strategies, safety-critical software, temporal logic, Calculus, Hume programming language, scheduling strategies, Concurrent computing, Computer languages, Processor scheduling, formal verification, Automata, scheduling, standard Hume scheduler, Safety, Logic, Standards development, temporal logic of action, Pattern matching, safety critical systems, Formal verification]
Performance analysis on mobile agent-based parallel information retrieval approaches
2007 International Conference on Parallel and Distributed Systems
None
2007
The main concern of the Internet user-base has shifted from what kind of information are available to how to find the desired information on the Internet thanks to the explosive growth of the WWW and the increasing amount of data available via the Internet. Since mobile agent technology is expected to be a promising technology for information retrieval, there are a number of mobile agent based-information retrieval approaches have been proposed in recent years. For a better understanding and efficiency improvement of these approaches, performance evaluation of great importance. However, most of existing evaluation results are experimental and there is a lack of theoretical performance analysis which is helpful to reveal the insight of the working mechanisms. In this paper, we further the study in W. Qu et al. (2007) in which some primary studies on the performance of several mobile agent-based information retrieval approaches are provided and provide the exact probability distributions of execution time for each approach. Our results reveal the insight of mobile agent-based information retrieval approaches and our analytical method provides a useful tool for further research to information retrieval.
[probability, information retrieval, performance evaluation, Information retrieval, World Wide Web, mobile agent-based parallel information retrieval approaches, Information analysis, Mobile agents, probability distributions, mobile agents, Bandwidth, Search engines, Explosives, performance comparison, Performance analysis, Internet, IP networks, performance analysis]
Optimizing resource allocation for multiple concurrent jobs in grid environment
2007 International Conference on Parallel and Distributed Systems
None
2007
In a dynamic environment like grid, it is difficult to manage resources at application or user level. In order to support application execution in the context of grid, a resource broker is essential and the task of resource brokering in such a heterogeneous, fast changing, distributed environment is non-trivial. In this paper we present the design and implementation of resource brokering strategies within a multi-agent framework. These strategies help in finding out an optimal allocation of resources for executing multiple concurrent jobs in a grid environment. We discuss the different stages in resource brokering and their implementation within the framework. The paper also presents results of a preliminary implementation and demonstrates the effectiveness of our strategies.
[Costs, multi-agent systems, resource allocation optimization, grid computing, multiagent framework, Scheduling, distributed environment, Application software, Environmental management, resource brokering strategy, Computer science, optimisation, resource allocation, Engineering management, concurrency control, Computer applications, grid environment, Grid computing, Performance analysis, Resource management, multiple concurrent job]
Effect of number of faults on NoC power and performance
2007 International Conference on Parallel and Distributed Systems
None
2007
According to international technology roadmap for semiconductors (ITRS), before the end of this decade, we will be entering the era of a billion transistors on a single chip. The major threat toward the achievement of billion transistor on a chip is poor scalability of current interconnect infrastructure. With the advent of "network on chip (NoC)" various characters and methodologies of traditional networks were hardly considered on-chip. Failure, power and area are the major concepts that should be considered when migrating from traditional interconnection networks to NoCs. In this paper we study the effects of faulty links and nodes on power and performance of mesh based NoC, Also several routing algorithms have been implemented and simulated using a cycle accurate VHDL model of NoC.
[Costs, network-on-chip, fault tolerance, mesh based NoC performance analysis, Routing Algorithm, network routing, Integrated circuit interconnections, performance evaluation, Routing, Interconnection Network, Fault, Circuit faults, Network on Chip, failure analysis, Delay, Power engineering computing, power aware computing, Network-on-a-chip, Bandwidth, fault tolerant routing algorithm, Performance, Power, Clocks, Power engineering and energy]
A scheduling algorithm for revenue maximisation for cluster-based Internet services
2007 International Conference on Parallel and Distributed Systems
None
2007
This paper proposes a new priority scheduling algorithm to maximise site revenue of session-based multi-tier Internet services in a multicluster environment. This research is part of a larger study in support of large-scale online trading systems and, as a result, this case study is chosen as a demonstrator for the techniques presented in this paper. The trading system is partitioned into a number of operations (trade, query etc.), which by their very nature are divided into orders of importance in terms of transactional response. The algorithm in this paper is based on Mean Value Analysis (MVA), which is used for the calculation of performance metrics concerning the queuing networks and workload allocation decision support in the multicluster. In addition to this, the priority assignment is based on combination of three attributes of any given request: (i) the sender class; (ii) the operation and, (Hi) the status of the user's portfolio (i.e.number of items in the user's portfolio). A discrete event simulator has been developed to evaluate the performance of the priority scheduling scheme with different combinations of request attributes in various experimental scenarios. Our study aims to develop a dynamic scheduling policy, which takes into account real-time system parameters and optimises the site revenue. Although our priority scheduling algorithm is designed for an online trading system, it can be applied to most e-Commerce systems in which differentiated services are required.
[Algorithm design and analysis, Measurement, priority scheduling algorithm, revenue maximisation, transactional response, mean value analysis, Discrete event simulation, discrete event simulator, optimisation, Web and internet services, e-commerce system, scheduling, queuing network, Large-scale systems, Performance analysis, discrete event simulation, Portfolios, electronic trading, queueing theory, large-scale online trading system, workload allocation decision support, Partitioning algorithms, Scheduling algorithm, session-based multi tier Internet service, cluster-based Internet service, Internet, Queueing analysis]
Ants vs. faults: A swarm intelligence approach for diagnosing distributed computing networks
2007 International Conference on Parallel and Distributed Systems
None
2007
Although much is known about the nature of testing structures for t-diagnosable systems, the problem of efficiently identifying the set of faulty units of a system in which the fault situation is known to be diagnosable remains an outstanding research issue. In this paper, we propose and evaluate an approach, based on swarm intelligence, to identify the set of faulty units in diagnosable systems. We consider t-diagnosable systems under the PMC model, where each node is capable of testing a particular subset of the other nodes in the system. We show that the ant-colony- based fault diagnosis algorithm is efficient, in that, it is able to diagnose a faulty situation in very short periods of time even if the number of faults is around the bound t, and with very few number of ants. The simulation results show that the new adaptive fault identification approach constitutes an addition to existing diagnosis algorithms.
[System testing, swarm intelligence approach, fault diagnosis, PMC model, Military computing, adaptive fault identification approach, Particle swarm optimization, Distributed computing, Information technology, ant-colony-based fault diagnosis algorithm, Fault diagnosis, Fault tolerance, optimisation, Wireless mesh networks, Signal processing algorithms, fault situation, distributed computing network diagnosis, computer network reliability, IP networks, t-diagnosable systems]
Scheduling multiple divisible loads on a linear processor network
2007 International Conference on Parallel and Distributed Systems
None
2007
Min, Veeravalli, and Barlas have recently proposed strategies to minimize the overall execution time of one or several divisible loads on a heterogeneous linear network, using one or more installments [18, 19]. We show on a very simple example that their approach does not always produce a solution and that, when it does, the solution is often suboptimal. We also show how to find an optimal scheduling for any instance, once the number of installments per load is given. Then, we formally prove that any optimal schedule has an infinite number of installments under a linear cost model as the one assumed in [18, 19]. Such a cost model cannot be used to design practical multi-installment strategies. Finally, through extensive simulations we confirmed that the best solution is always produced by the linear programming approach.
[multiinstallment strategy, Computational modeling, Optimal scheduling, Multimedia databases, distributed processing, linear cost model, Linear programming, linear programming approach, linear programming, optimal scheduling problem, Distributed computing, Processor scheduling, resource allocation, execution time minimization strategy, heterogeneous linear processor network, Linear algebra, scheduling, Cost function, Books, multiple divisible load scheduling problem, Load modeling]
A dual-time vector clock based synchronization mechanism for key-value data in the SILENUS file system
2007 International Conference on Parallel and Distributed Systems
None
2007
The SILENUS federated file system was developed by the SORCER research group at Texas Tech University. The federated file system with its dynamic nature does not require any configuration by the end users and system administrators. The SILENUS file system provides support for disconnected operation. To support disconnected operation a relevant synchronization mechanism is needed. This mechanism must detect and order events properly. It must detect also possible conflicts and resolve these in a consistent manner. This paper describes the new synchronization mechanism needed for providing data consistency. It introduces dual-time vector clocks to order events and detect conflicts. A conflict resolution algorithm is defined that does not require user interactions. It introduces the switchback problem and how it can be avoided. The synchronization mechanisms presented in this paper can be adapted to synchronize any key-value based data in any distributed system.
[switchback problem, Event detection, Memory, conflict resolution algorithm, distributed system, data integrity, SILENUS file system, Synchronization, data consistency, dual-time vector clock based synchronization mechanism, File systems, Metacomputing, NIST, Grid computing, Hardware, user interactions, Standards development, key-value data, key-value based data, Clocks]
A weighted interference estimation scheme for interface switching wireless mesh networks
2007 International Conference on Parallel and Distributed Systems
None
2007
The number of available channels in a specific wireless network is bounded. Therefore, co-channel interference is inevitable in most wireless networks. The co-channel interference problem in wireless mesh networks is more serious than that in single-hop wireless networks. It is preferable for mesh nodes to dynamically adjust channels to elevate the interference level of networks. In this paper, we introduce the conception of communication constraint to denote the strategies that decide the mode of interface switching. We develop an interference estimation scheme. The proposed scheme uses a weight to represent the effects of loads distribution and relative distance between nodes on the interference level of network. We compare the performance of our solution with the model presented by Gupta and Kumar [14] through both graph-based and NS2 simulations. Extensive results show that our solution achieves better performance than the compared scheme when the number of available channels is small.
[graph-based simulation, NS2 simulation, Spine, graph theory, Ad hoc networks, Interference constraints, Communication switching, cochannel interference, Wireless communication, weighted interference estimation, Wireless networks, Aggregates, Wireless mesh networks, Spread spectrum communication, interface switching wireless mesh networks, wireless channels, Interchannel interference]
CrossTree: A new HTC architecture with high reliability and scalability
2007 International Conference on Parallel and Distributed Systems
None
2007
HTC (high throughput computing) is a environment that can provide large amounts of processing capacity over long periods of time. To HTC, users are more concerned about how many jobs can be completed in a long period, but not how fast can a single job be finished. Condor, an implementation of HTC, is constructed by commodity CPUs and memory. As long as the Condor nodes are controlled by the Central Management Node, its reliability and scalability had been restricted. Based on the concept of DHT (distributed hash table), this paper presents a new distributed HTC architecture, named CrossTree, which has no central parts, and its metadata is distributed across all nodes in the system. Theoretical analysis and the simulation results proved CrossTree to be an efficient architecture with high scalability and reliability.
[meta data, Condor, CrossTree, Scalability, software reliability, high throughput computing, distributed processing, Throughput, Educational institutions, Distributed computing, Centralized control, Central Management Node, Information science, Analytical models, software architecture, distributed hash table, Computer architecture, file organisation, Internet, Personal communication networks]
Virtualization aware job schedulers for checkpoint-restart
2007 International Conference on Parallel and Distributed Systems
None
2007
Application checkpoint and restart has been a widely studied problem over the last several decades. Despite immense volume of theory and several research project level implementations, there is very little by way of working solutions for the case of parallel distributed applications (such as MPI programs on a cluster). We describe our experiences in enhancing a job scheduler to leverage mechanisms of a virtual machine environment to support checkpoint-restart. We also describe the basic coordinated checkpoint-restart framework that we implemented on which this solution is based.
[Checkpointing, checkpointing, message passing, Virtual machining, Middleware, checkpoint-restart framework, virtual machine environment, Application virtualization, parallel distributed application, Processor scheduling, Operating systems, Linux, virtualization aware job scheduler, virtual machines, Chromium, scheduling, Libraries, Kernel]
A push-based prefetching for cooperative caching RAM Grid
2007 International Conference on Parallel and Distributed Systems
None
2007
As an innovative distributed computing technique for sharing the memory resources in high-speed network, RAM Grid exploits the distributed free nodes, and provides remote memory for the nodes which are short of memory. One of the RAM Grid systems named DRACO, tries to provide cooperative caching to improve the performance of the user node which has mass disk I/O but lacks local memory. However, the performance of DRACO is constrained with the network communication cost. In order to hide the latency of remote memory access and improve the caching performance, we proposed using push- based prefetching to enable the caching providers to push the potential useful memory pages to the user nodes. Specifically, for each caching provider, it employs sequential pattern mining techniques, which adapts to the characteristics of memory page access sequences, on locating useful memory pages for prefetching. We have verified the effectiveness of the proposed method through system analysis and trace-driven simulations.
[high-speed network, Costs, Cooperative caching, random-access storage, Prefetching, Laboratories, Random access memory, data mining, grid computing, Read-write memory, sequential pattern mining techniques, cache storage, Delay, Distributed processing, memory resource sharing, High-speed networks, File systems, push-based prefetching, distributed computing technique, distributed shared memory systems, cooperative caching RAM grid systems]
Subscription-aware publish/subscribe tree construction in mobile ad hoc networks
2007 International Conference on Parallel and Distributed Systems
None
2007
A publish/subscribe system consists of publishers, subscribers, and a delivery infrastructure, where publishers produce events, subscribers declare their interests in receiving events via subscriptions, and the delivery infrastructure forwards subscribed events from publishers to corresponding subscribers. The paper describes a distributed, subscription-aware publish/subscribe tree (PST) construction protocol, termed DSAPST, for MANET. DSAPST specifically takes subscribers' subscriptions into account so that subscribers sharing common subscriptions that are closer in terms of hop count would be connected together in the constructed PSTs, resulting in lower event delivery overhead in terms of the number of wireless transmissions. In addition, DSAPST adapts to mobility of nodes, allows existing nodes to join and leave as subscribers, and accommodates brand new nodes joining a MANET as subscribers. Simulation results show that DSAPST outperforms a MAODV-based PST construction protocol which is not subscription-aware.
[Tree data structures, Protocols, Subscriptions, trees (mathematics), Telecommunication traffic, subscription-aware publish-subscribe tree construction protocol, Routing, Mobile ad hoc networks, MANET, Network topology, Unicast, mobile communication, mobile ad hoc networks, Computer networks, ad hoc networks, protocols, wireless transmissions, Joining processes]
Stepping-stone detection algorithm based on order preserving mapping
2007 International Conference on Parallel and Distributed Systems
None
2007
Intruders often do not attack victim hosts directly from their own hosts so as not to reveal their identity. Instead, intruders perform their attacks through a sequence of intermediary hosts before attacking the target. This type of attack is known as a "stepping-stone attack". Stepping-stone detection is to determine if a host machine is being used as a stepping-stone by attackers. In this paper, we propose an algorithm for stepping-stone detection using a pervious mapping-based detection method. The technique reduces the detection problem to finding a mapping between two streams of packets. If our algorithm cannot find the mapping, then no such mapping exists. But if there is a mapping, then the proposed algorithm is guaranteed to find one and the solution will always be the one with minimum indexed. We provide the proof of the correctness of the algorithms. Furthermore, the algorithm has a low time complexity. The paper also discusses the effect of chaff packets on the ability to detect stepping-stones.
[Stepping-stone, Design methodology, stepping-stone detection algorithm, intrusion detection, connection chain, Helium, Computer science, Upper bound, mappings, security of data, order preserving mapping, Intrusion detection, chaff packets, Delta modulation, Timing, Cryptography, IP networks, Detection algorithms, computational complexity, algorithm]
Privacy-preserving logical vector clocks using secure computation techniques
2007 International Conference on Parallel and Distributed Systems
None
2007
Systems of logical clocks are commonly found in distributed systems for establishing causality between events occurring in concurrent communicating processes. Vector clocks are a popular type of logical clocks which require processes to attach to each message a logical timestamp that contains information about the sender process's view of the state of the distributed computation at the time of message sending. Causality between two events can then be determined by comparing their two logical timestamps. However, by doing so, processes leak potentially sensitive information about the advancement of their computation and the computations performed by the processes they communicate with. The contribution presented in this paper is a protocol, based on secure computation techniques, which preserves the privacy of each process's local logical clock while being strictly equivalent to regular vector clocks.
[Protocols, distributed processing, Synchronization, Distributed computing, Counting circuits, Concurrent computing, secure computation techniques, Privacy, concurrent communicating processes, message sending, Message passing, privacy-preserving logical vector clocks, data privacy, Australia, logic circuits, Clocks]
LORM: Supporting low-overhead P2P-based range-query and multi-attribute resource management in grids
2007 International Conference on Parallel and Distributed Systems
None
2007
Resource management is critical to the usability and accessibility of grid computing systems. Conventional approaches to grid resource discovery are either centralized or hierarchical, and these prove to be inefficient as the size of the grid system increases. The peer-to-peer (P2P) paradigm has been applied to grid systems as a mechanism for providing scalable range- query and multi-attribute resource management. However, most current P2P-based resource management approaches support multi-attribute range queries at a high cost. They either depend on multiple P2P networks with each P2P network responsible for a single attribute, or they keep the resource information of all attributes in a single node. This paper presents a low-overhead range-query multi-attribute P2P-based resource management approach, LORM. Unlike other P2P-based approaches, it relies on a single P2P network and allocates resource information to different nodes based on resource attributes and values. Moreover, it has high capability to handle the large-scale and dynamic characteristics of resources in grids. Simulation results demonstrate the efficiency of LORM in comparison with other resource management approaches in terms of resource management overhead and resource discovery efficiency.
[Costs, peer-to-peer computing, low-overhead P2P-based range-query, Peer to peer computing, grid computing, grid computing systems, Quality of service, multi-attribute resource management, Application software, Floods, resource allocation, Grid computing, Large-scale systems, Performance analysis, Resource management, Usability, peer-to-peer paradigm]
Embedding cycles and paths in a k-ary n-cube
2007 International Conference on Parallel and Distributed Systems
None
2007
The k-ary n-cube, denoted by Q<sub>n</sub> k, has been one of the most common interconnection networks. In this paper, we study some topological properties of Q<sub>n</sub> k. Given two arbitrary distinct nodes x and y in Q<sub>n</sub> k, we show that there exists an x-y path of every length from [k/2]n to kn - 1, where n ges 2 is an integer and k ges 3 is an odd integer. Based on this result, we further show that each edge in Q<sub>n</sub> k lies on a cycle of every length from k to kn. In addition, we show that Q<sub>n</sub> k is both bipanconnected and edge-bipancyclic, where n ges 2 is an integer and k ges 2 is an even integer.
[edge-bipancyclicity, Codes, embedding cycle, Multiprocessor interconnection networks, graph theory, edge-pancyclicity, hypercube networks, Graph theory, hypercubes, network topology, panconnectivity, Delay, topological property, k-ary n-cube network, Computer science, Network topology, interconnection network, Emulation, graph-theoretic interconnection networks, Hypercubes, k-ary n-cubes, bipancyclicity, bipanconnectivity]
Conditional edge-fault-tolerant Hamiltonian cycle embedding of star graphs
2007 International Conference on Parallel and Distributed Systems
None
2007
The star graph has been recognized as an attractive alternative to the hypercube. In this paper, we investigate the hamiltoncity of a n-dimensional star graph. We show that for any n-dimensional star graph (n ges 4) with at most 3n - 10 faulty edges in which each node is incident to at least two fault-free edges, there exists a fault-free Hamiltonian cycle. Our result improves the previously best known result where the number of tolerable faulty edges is bounded by 2n - 7. We also demonstrate that our result is optimal with respect to the worst case scenario where every other node of a six-length cycle is incident to exactly n - 3 faulty non-cycle edges.
[Multiprocessor interconnection networks, graph theory, multiprocessor interconnection networks, conditional edge-fault-tolerant Hamiltonian cycle, Routing, Multiprocessing systems, Resilience, Computer science, Fault tolerance, Network topology, Chaotic communication, star graph, Hypercubes, fault tolerant computing, Large-scale systems, tolerable faulty edge, multiprocessor interconnection network]
A Voronoi dEtection Range Adjustment (VERA) approach for energy saving of wireless sensor networks
2007 International Conference on Parallel and Distributed Systems
None
2007
Since the batteries in a wireless sensor networks cannot be replaced, efficient power management becomes an important research issue. If we can largely reduce the overlaps among detection ranges and decrease the amount of duplicate data then we can save energy more efficiently. Meguerdichian et al. exploit the coverage problems in wireless ad-hoc sensor networks in terms of Voronoi diagram and Delaunay triangulation. In this paper, we propose a Voronoi detection range adjustment (VERA) method that utilizes distributed Voronoi diagram to delimit the area of responsibility for each sensor. We then use genetic algorithm to optimize the most suitable detection range for each sensor. Simulations show that VERA outperforms maximum detection range, K-covered, and greedy methods in terms of reducing the overlaps among detection ranges, minimizing energy consumption, and prolonging the lifetime of the whole network.
[Energy consumption, Minimization methods, Event detection, wireless sensor networks, computational geometry, Delaunay triangulation, genetic algorithms, energy saving, efficient power management, Genetic algorithms, Computer science, genetic algorithm, Wireless sensor networks, Voronoi diagram, Battery management systems, greedy methods, Voronoi detection range adjustment, maximum detection range, K-covered method, Data communication, Computer network management, Energy management]
Message from the General and Program Committee Chairs
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Presents the introductory welcome message from the conference proceedings.
[]
Keynote speakers
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Provides an abstract for each of the keynote presentations and may include a brief professional biography of each
[]
Conference organizers
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Provides a listing of current committee members and society officers.
[]
Supporting Distributed Application Workflows in Heterogeneous Computing Environments
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Next-generation computation-intensive applications in various fields of science and engineering feature large-scale computing workflows with complex structures that are often modeled as directed acyclic graphs. Supporting such task graphs and optimizing their end-to-end network performances in heterogeneous computing environments are critical to the success of these distributed applications that require fast response. We construct analytical models for computing modules, network nodes, and communication links to estimate data processing and transport overhead, and formulate the task graph mapping with node reuse and resource sharing for minimum end-to-end delay as an NP-complete optimization problem. We propose a heuristic approach to this problem that recursively computes and maps the critical path to the network using a dynamic programming-based procedure. The performance superiority of the proposed approach is justified by an extensive set of experiments on simulated data sets in comparison with existing methods.
[optimization problem, complex structures, Graph mapping, large-scale computing workflows, task graph mapping, NP-complete, Distributed computing, distributed application workflows, Concurrent computing, minimum end-to-end delay, heuristic algorithm, Analytical models, Network topology, Physics computing, Computer networks, NP-complete optimization problem, directed acyclic graphs, heterogeneous computing environments, next-generation computation-intensive applications, Computational modeling, Delay estimation, dynamic programming, Application software, directed graphs, Resource management, computational complexity]
MRBench: A Benchmark for MapReduce Framework
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
MapReduce is Google's programming model for easy development of scalable parallel applications which process huge quantity of data on many clusters. Due to its conveniency and efficiency, MapReduce is used in various applications (e.g., Web search services and online analytical processing). However, there are only few good benchmarks to evaluate MapReduce implementations by realistic testsets. In this paper, we present MRBench that is a benchmark for evaluating MapReduce systems. MRBench focuses on processing business oriented queries and concurrent data modifications. To this end, we build MRBench to deal with large volumes of relational data and execute highly complex queries. By MRBench, users can evaluate the performance of MapReduce systems while varying environmental parameters such as data size and the number of (map/reduce) tasks. Our extensive experimental results show that MRBench is a useful tool to benchmark the capability of answering critical business questions.
[Costs, TPC-H, Data engineering, Application software, parallel programming, MapReduce framework, online analytical processing, MapReduce, Computer science, query processing, Fault tolerance, Parallel programming, Google programming model, pattern clustering, Benchmark testing, Parallel processing, critical business questions, Benchmark, business oriented queries, Functional programming, concurrent data modifications, Web search, Web search services]
Contention-Free Scheduling in a Dynamic Context
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
In this paper, we present an adaptive method for scheduling parallel applications on heterogeneous multi-processor platforms, in a dynamic context. When executing divisible load applications according to a master-worker model, this method delivers the workload through multiple rounds and can avoid contentions in the use of the network. Such contentions avoidance is a means to avoid idle time, thus to maximize the throughput. Before presenting the proposed scheduling method, the paper revisits a runtime method which motivated its development. The method presented in this paper can be used to schedule parallel applications whose total workload is large but unknown a priori.
[multiprocessing systems, dynamic context, scheduling, adaptive method, Dynamic scheduling, heterogeneous multiprocessor, contention-free scheduling, runtime method, parallel application, master-worker model, parallel processing]
Parallel Architecture Implementation of a Reliable (k,n) Image Sharing Scheme
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
This paper presents a hardware implementation of a secure and reliable k-out-of-n threshold based secret image sharing method. The secret image is divided into n image shares so that any k image shares are sufficient to reconstruct the secret image in a lossless manner, but (k-1) or fewer image shares cannot reveal anything about the secret image. This secret sharing method comprises multiple independent computations which are conducive to parallel processing architectures. Fine-grained field programmable gate array (FPGA) architectures are the near optimal hardware platform for performing parallel processing. This paper illustrates the design and implementation of the secret image sharing method for 8-bit grayscale images on an FPGA which enhances execution time. On average, it was found that the FPGA executes image sharing and reconstruction approximately 300 times faster than a microprocessor operating on the same image.
[k-out-of-n threshold based secret image sharing method, field programmable gate arrays, parallel architectures, FPGA, Gray-scale, secret image reconstruction, Parallel Processing, Parallel architectures, image reconstruction, parallel processing architectures, Image reconstruction, matrix algebra, Concurrent computing, Microprocessors, fine-grained field programmable gate array architectures, Image Sharing, Computer architecture, grayscale images, Parallel processing, Hardware, Cryptography, Field programmable gate arrays, image sharing scheme]
Distributed RankBoost Acceleration Using FPGA and MPI for Web Relevance Ranking
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Web search engine ranks web pages according to their relevance to user queries, which is critical for the success of commercial search engines. Rank Boost algorithm is promising in Web relevance ranking area, while its computation complexity makes our existing implementations (including single node software-based implementation and a FPGA-based accelerator) too slow to reflect the dynamics of the Web. Moreover, previous implementations can not handle the huge web-scale data. As such, in this paper, we present the RankBoost implementation on a MPI-based distributed FPGA-based accelerators. Our results show that the combination of the coarse parallel efficiency of distributed system and the fine parallel efficiency of reconfigurable hardware accelerators can significantly increase the computing performance.
[RankBoost, search engines, application program interfaces, field programmable gate arrays, FPGA, MPI, distributed RankBoost acceleration, Distributed computing, Concurrent computing, computation complexity, Web search engine, Web pages, Search engines, Parallel processing, FPGA acceleration, Cost function, Hardware, Acceleration, Web relevance ranking, Field programmable gate arrays, Web search, computational complexity]
Allocating Series of Workflows on Computing Grids
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
In this paper, we focus on scheduling jobs on computing Grids. In our model, a Grid job is made of a large collection of input data sets, which must all be processed by the same task graph or workflow, thus resulting in a series of workflow problem. We are looking for an efficient solution with regard to throughput and latency, while avoiding solutions requiring complex control. We thus only consider single-allocation strategies. We present an algorithm based on mixed linear programming to find an optimal allocation, and this for different routing policies depending on how much latitude we have on communications. Then, using simulations, we compare our allocations to reference heuristics. The results show that our algorithm almost always finds an allocation with good throughput and low latency, and that it outperforms the reference heuristics, especially under communication-intensive scenarios.
[mixed linear programming, Laboratories, computing grids, Communication system control, grid computing, latency, Throughput, linear programming, Distributed computing, Delay, Concurrent computing, resource allocation, single-allocation strategies, DAGs, scheduling, computing Grid, Grid computing, throughput, job scheduling, computing Grids, Workflows, Routing, Dynamic scheduling, workflow allocation, heterogeneity, Processor scheduling, Grid job]
Using Dedicated and Opportunistic Networks in Synergy for a Cost-Effective Distributed Stream Processing Platform
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
This paper presents a case for exploiting the synergy of dedicated and opportunistic network resources in a distributed hosting platform for data stream processing applications. Our previous studies have demonstrated the benefits of combining dedicated reliable resources with opportunistic resources in case of high-throughput computing applications, where timely allocation of the processing units is the primary concern. Since distributed stream processing applications demand large volume of data transmission between the processing sites at a consistent rate, adequate control over the network resources is important here to assure a steady flow of processing. In this paper, we propose a system model for the hybrid hosting platform where stream processing servers installed at distributed sites are interconnected with a combination of dedicated links and public Internet. Decentralized algorithms have been developed for allocation of the two classes of network resources among the competing tasks with an objective towards higher task throughput and better utilization of expensive dedicated resources. Results from extensive simulation study show that with proper management, systems exploiting the synergy of dedicated and opportunistic resources yield considerably higher task throughput and thus, higher return on investment over the systems solely using expensive dedicated resources.
[data stream processing, Distributed stream processing, Multimedia databases, distributed processing, Bi-modal architecture, Throughput, opportunistic networks, Distributed computing, Centralized control, dedicated networks, Network servers, Investments, high-throughput computing applications, Streaming media, Computer networks, Internet, Resource management, decentralized algorithms, Web server, cost-effective distributed stream processing]
Designing and Experimenting with a Distributed Tracking System
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Tracking objects is an important activity in many applications such as Augmented Reality, Visual Servoing, and Tangible Interfaces. Most of these applications are inherently dynamic and demand realtime response while tracking. Also, due to the issues of economics, such applications can benefit from tracking systems that do not depend on well-engineered setups and instead use inexpensive sensors for tracking. These requirements make distributed tracking a challenging problem that needs a thorough investigation. In this paper an approach to tracking, which uses the concepts of dynamic discovery, in a distributed environment containing autonomous vision-based trackers that use inexpensive sensors (such as Web cameras) is presented along with its empirical validation. The empirical results obtained from experiments are promising and validate the premise that Distributed Tracking Systems can be built using inexpensive vision-based sensors and concepts of dynamic resource discovery.
[Computer interfaces, object recognition, distributed processing, Radar tracking, augmented reality, Visual servoing, Distributed computing, tracking, Concurrent computing, Information science, Filters, Software Services, visual servoing, object tracking, distributed tracking system, dynamic resource discovery, vision-based sensors, Application software, Augmented reality, Dynamic Discovery, image sensors, autonomous vision-based trackers, Vision-based trackers, Cameras, tangible interfaces, Distributed Tracking]
Parallel Large Scale Inference of Protein Domain Families
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
The resolution of combinatorial assortments of protein sequences into domains is a prerequisite for protein sequence interpretation. However the recognition and clustering of homologous domains from sequence databases typically scales quadratically with respect to their size which grows exponentially, making it essential to parallelize these complex bioinformatics applications. Here we demonstrate the parallelization of MKDOM2, the sequential program that has been instrumental in the construction of the PRODOM database of protein domain families. This was challenging because of (1) dependencies between program iterations, (2) their extremely heterogeneous run times and (3) communication bottlenecks that could arise because of the large size of the data. A large scale test of the new program, MPI_MKDOM2, demonstrated its robustness against heterogeneous run times, preparing the grounds for future releases of PRODOM that would otherwise be out of reach with MKDOM2 by several orders of magnitude.
[protein domain families, MPI_MKDOM2, Genomics, grid computing, parallel large scale inference, MKDOM2, parallel programming, Grid Computing, Databases, protein sequence interpretation, proteins, Protein Domains, Protein sequence, Robustness, Large-scale systems, Bioinformatics, PRODOM database, Testing, sequence databases, Protein engineering, message passing, Instruments, complex bioinformatics applications, Message passing, bioinformatics, Iterative algorithms, Sequence clustering]
Scheduling Techniques for Effective System Reconfiguration in Distributed Storage Systems
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
In this paper, we present a new algorithm for disk reconfiguration in the context of Vespa, a scalable platform developed by Yahoo! Technologies Norway for storing, retrieving, processing and searching large amounts large amounts of data. The corresponding scheduling problem is closely related to independent tasks scheduling on heterogeneous platforms, when communication costs are taken into account, and when each task can only be processed on a prescribed set of processors. We prove how to derive from a linear programming formulation in rational numbers an approximation algorithm whose approximation ratio is close to 1 in the condition of use o/Vespa. By performing an extensive set of simulations using SIMGRID, we also show the proposed algorithm is in fact optimal under Vespa conditions of use.
[Algorithm design and analysis, grid computing, linear programming, distributed storage systems, Complexity, Distributed Storage Systems, storage management, System performance, Clustering algorithms, scheduling, approximation algorithm, Hardware, Vespa, Linear Programming, Context, approximation theory, Redundancy, disk reconfiguration, SIMGRID, Information retrieval, Linear programming, Scheduling, Approximation Algorithms, linear programming formulation, Processor scheduling, Approximation algorithms, scheduling techniques]
GiFT: Automating FTPA Implementation for MPI Programs
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Fault tolerance is a critical issue in the arena of large-scale computing. The fault-tolerant parallel algorithm (FTPA) is an application-level technique for tolerating hardware failures. FTPA achieves fast failure recovery making use of parallel recomputing. However, it complicates the coding of the application program. This paper uses compiler technology to automate the design of FTPA, and introduces the implementation of a tool called GiFT (Get it Fault-Tolerant). GiFT utilizes the extended data-flow analysis to choose the state needed by failure recovery, exploits the parallel recomputing time model to compute the optimal number of recomputing processes, and uses parallelization technologies to generate parallel recomputing codes. The experimental results show that original MPI programs can be transformed into the FTPA counterparts by GiFT correctly, and the performance of GiFT-generated FTPA programs is comparable to the performance of hand-modified FTPA programs.
[checkpointing, hardware failures tolerance, parallel algorithms, Data analysis, message passing, Scientific computing, application program interfaces, fault tolerance, failure recovery, data-flow analysis, Application software, Get it Fault-Tolerant, Parallel algorithms, Distributed computing, fault-tolerant parallel algorithm, Concurrent computing, Fault tolerance, GiFT, MPI programs, High performance computing, parallel recomputing, Hardware, fault tolerant computing, Large-scale systems]
Energy Efficient Register File with Reduced Window Partition
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
As power dissipation of the register file in modern processor designs tends to dominate, measures must be taken to keep it under control. This paper introduces an approach for reducing the SPARC windowed register file power based on the operation of the Partitioned Register File Inspector added in the decode stage of the pipeline. The power savings show that, when the size of the Register File Inspector is properly fixed, the average saving on the energy consumption of the windowed register file could be up to 77% compared with the traditional register file control scheme.
[Process design, energy efficient register file, Energy consumption, SPARC, Energy measurement, Registers, Decoding, Microelectronics, processor, power consumption, Radiofrequency interference, reduced window partition, register file, program processors, Power measurement, partitioned register file inspector, Energy efficiency, Power dissipation, power, window, energy consumption, power dissipation]
A Novel Network RAID Architecture with Out-of-Band Virtualization and Redundant Management
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
The paper presents a novel network RAID storage system based on the out-of-band virtualization architecture and the backend centralized redundant management. The application servers can inquire the mapping information of virtual disk from the out-of-band virtualization server and directly access the storage nodes. The read request can fetch the data from special storage node, while the write request is not only stored into the storage node, and also mirrored into the redundant server by the storage node. The redundant server can cache updated data in local disks with log-structured mode, and calculate the parity of RAID5 in the background process when the system is idle. It relieves the bottle problem of I/O performance and low reliability danger of single controller in the front-end centralized management system. And the layout of RAID1/RAID5 on the data block has acquired the trade-off among performance, reliability and cost.
[local disk, log-structured mode, Redundancy, virtual storage, Conference management, Control systems, out-of-band virtualization server, cache storage, RAID, Centralized control, network RAID storage system, cache management, Network servers, Technology management, network RAID, out-of-band virtualization, out-of-band virtualization architecture, backend centralized redundant management, front-end centralized management system, Computer architecture, Bandwidth, Computer networks, redundant management, Computer network management]
Object-Oriented Parallelisation: Improved and Extended Parallel Iterator
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
The need to parallelise desktop applications is becoming increasingly essential with the mainstream adoption of multi-cores. In object-oriented languages, sequential iterators handle iterative computations of a sequential program; similarly, the parallel iterator was developed to handle the iterative computations of a parallel program. This paper presents the progress of the parallel iterator concept. New features, such as support for reductions and global break semantics, allow the parallel iterator to undertake more situations. With a slight contract modification, the parallel iterator interface now imitates that of the sequential iterator. All these features combine together to promote minimal, if any, code restructuring. The reduction frequently outperforms related work and the importance of providing simple and flexible fine-tuning capability is affirmed.
[reductions, Yarn, parallel programming, Concurrent computing, contract modification, parallel iterator concept, Parallel processing, Libraries, global break semantics, object-oriented methods, sequential program, Object oriented programming, parallel computing, Contracts, Java, object-oriented programming, parallel iterator interface, multicores, parallel iterator, object-oriented parallelisation, sequential iterator, Application software, Processor scheduling, parallelise desktop applications, flexible fine-tuning capability, loop scheduling, Lakes, object-oriented languages, parallel program, iterative computations]
Extending Inter-process Synchronization with Robust Mutex and Variants in Condition Wait
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Condition variables and mutexes are synchronization mechanisms defined in the POSIX Pthreads specification and there are several implementations that address these requirements. Applications wanting to use them for inter-process communication could suffer from process hangs, in the event of unexpected termination of a mutex owning process. Also, if condition waiters need to have fairness associated with them across processes, threads should enable priority scheduling with contention scope set to system level. This could prevent a thread in another process, or in the kernel, from running and lead to starvation. To address these limitations, we present a mechanism to implement FIFO condition wait, signaling and robust mutex using system V semaphores. The paper also describes lockless condition wait, a simple mechanism implemented using compare-and-swap (CAS) atomic primitive. Since semaphores are known to be heavy weight objects, we evaluate the performance of the proposed base implementation in comparison with process shared mutexes and condition variables provided by pthread library on major UNIX platforms. The results indicate that the overheads associated with semaphores even out gradually under heavy loads (with the exception of Native POSIX Thread Library (NPTL) on Linux) on multi-CPU machines lending support to the idea that the implementation could be widely deployed by applications wanting prioritized and robust synchronization.
[Unix, Event detection, System V semaphores, Yarn, condition variables, Robustness, Libraries, Kernel, Protection, extending interprocess synchronization, Signaling and Synchronization, condition waiters, Context-aware services, Condition Wait, multi-CPU machines, Application software, mutexes, POSIX, Content addressable storage, Linux, Mutexes, Semaphores, FIFO condition, compare-and-swap atomic primitive, FIFO Waiters, UNIX platforms]
Asymptotic Analysis of Load Distribution for Size-Interval Task Allocation with Bounded Pareto Job Sizes
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Server farms, consisting of a collection of hosts and a front-end router that dispatches incoming jobs to hosts, are now commonplace. When job service requirements (job sizes) are highly variable, the Size-Interval task assignment policy is an excellent rule for assigning jobs to hosts. For a long time it was believed that the size cutoff separating "short" jobs from "long " ones should be chosen to balance the load at the hosts in the server farm. However, recent literature has provided evidence that load balancing is not always optimal for minimizing mean response time. Previous collaborative work by the author presented simple sufficient criteria for determining which host should be underloaded in a two host system using Size-Interval task assignment. In this paper, we derive asymptotic closed-form expressions for the optimal size cutoff, separating short jobs from large jobs, and the optimal load point at the separation in a two-host system. The asymptotic expressions are for when the maximum job size in a Bounded Pareto distribution approaches infinity and are derived using perturbation analysis. We show that the asymptotics can be one of three forms depending on whether the system load (mean number of busy hosts) is less than 1, equal to 1 or greater than 1 with different mechanisms dominating in each case. Our results allow the effects of both load and tail index to be easily determined. Numerical analysis shows that the asymptotic expressions in most cases are very accurate.
[load balancing, Task allocation, Closed-form solution, Probability distribution, asymptotic load distribution analysis, Distributed computing, Delay, asymptotic closed-form expression, optimal size cutoff, resource allocation, perturbation theory, Tail, bounded Pareto distribution, scheduling, Pareto analysis, Size-Interval, bounded Pareto job size-interval task allocation, perturbation analysis, H infinity control, Scheduling, Asymptotics, Bounded Pareto, Numerical analysis, Pareto distribution, Load management, Collaborative work, mean response time minimization]
GMIP: A Novel Optical Interconnect Gridded Memory Service Protocol
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Dynamic self-organized computer architecture (DSAG) based on Grid-components departs computer components to grid components, and dynamically aggregates and organizes these components to realize architecture-on-demand. We take the important feature, that CPU centered design principle should possibly become memory centered design and optimization. We propose a novel computer architecture Gridded Memory Service (GMS), which is based on DSAG. We design and implement a serial, light weight and packet switching optical interconnect protocol, Gridded Memory Interconnect Protocol (GMIP), which is featured as high bandwidth and low latency protocol. It optimizes the link and physical layer to take advantage of very short reach optical interconnect technology. At last we study interconnect effects, and propose the main evaluation principles of bandwidth latency compensation.
[memory centered design, Optical interconnections, Protocols, Packet switching, optical interconnect gridded memory service protocol, grid computing, memory protocols, dynamic self-organized computer architecture, GMIP, Delay, Design optimization, Optical design, Aggregates, optical interconnections, computer architecture, Computer architecture, Bandwidth, Grid computing, CPU centered design principle]
A Purpose-Based Synchronization Protocol of Multiple Transactions
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
In a peer-to-peer (P2P) system, multiple peer processes issue transactions to manipulate objects. A transaction issued by a process is assigned with a subfamily of roles named purpose, which are granted to the process. If a transaction reads an object o<sub>1</sub> and then writes another object o<sub>2</sub>, information in o<sub>1</sub> might flow into o<sub>2</sub>. Suppose a transaction T<sub>1</sub> with a purpose R<sub>1</sub> precedes another transaction T<sub>2</sub> with a purpose R<sub>2</sub> in a schedule. Here, if the legal information flow relation R<sub>1</sub>  R<sub>2</sub> holds, no illegal information flow occur. Otherwise, illegal information flow might occur. In order to prevent illegal information flow, if T<sub>1</sub> writes an object o, the object o is marked with the purpose R<sub>1</sub>. The transaction T<sub>2</sub> is allowed to read the object o if R<sub>1</sub>  R<sub>2</sub>. Even if T<sub>1</sub> commits, the object o isstill marked with the purpose R<sub>1</sub>. Hence, every transaction T<sub>3</sub> with such a purpose R<sub>3</sub> that R<sub>1</sub> does not legally flow into R<sub>3</sub> is aborted if T<sub>3</sub> reads the object o. In result, the throughput is degraded. In this paper, we discuss how to release purpose marks on objects. Objects whose information may flow into an object o are source objects of o. If the source objects are changed, a purpose mark on the object o is released. In addition, an object o might have some lifetime when o's data has to be secure since the data iscreated. If it takes time since the object o is marked, the purpose mark is released. While there occur no illegal information flow in our purpose marking (PM) protocol, transactions which imply illegal information flow are aborted. We evaluate the PM protocol in terms of how many transactions are aborted.
[transaction processing, Protocols, Law, peer-to-peer computing, Peer to peer computing, Throughput, Purpose, purpose-based synchronization protocol, Degradation, security of data, Distributed systems, illegal information flow, Role-based access control model, peer-to-peer system, multiple transactions, Information flow control, purpose marking protocol, protocols, Legal factors]
RUFT: Simplifying the Fat-Tree Topology
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
The fat-tree is one of the most widely-used topologies by interconnection network manufacturers. Recently, a deterministic routing algorithm that optimally balances the network traffic in fat--trees was proposed. It can not only achieve almost the same performance than adaptive routing, but also outperforms it for some traffic patterns. Nevertheless, fat-trees require a high number of switches with a non-negligible wiring complexity. In this paper, we propose replacing the fat-tree by an unidirectional multistage interconnection network referred to as reduced unidirectional fat-tree (RUFT) that uses a a simplified version of the aforementioned deterministic routing algorithm. As a consequence, switch hardware is almost reduced to the half, decreasing, in this way, power consumption, arbitration complexity, switch size, and network cost. Evaluation results show that RUFT obtains lower latency than fat-tree for low and medium traffic loads. Furthermore, in large networks, it obtains almost the same throughput than the classical fat-tree.
[Energy consumption, Costs, Multiprocessor interconnection networks, Switches, Telecommunication traffic, unidirectional multistage interconnection network, Traffic Balancing, power consumption, reduced unidirectional fat-tree topology, Wiring, Network topology, Hardware, Manufacturing, Fat--tree, network routing, MINs, trees (mathematics), Routing, multistage interconnection networks, network topology, network traffic, deterministic routing algorithm, Deterministic Routing, Cost-efficiency, computational complexity]
Energy-Efficient Task Partition for Periodic Real-Time Tasks on Platforms with Dual Processing Elements
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Modern computing systems often adopt multiple processing elements to enhance the computing capability or reduce the power consumption, especially for embedded systems. Such configurations impose challenges on energy efficiency in hardware and software implementations. This paper targets energy-efficient task partitioning for real-time tasks on a platform with two heterogeneous processing elements (processors), in which each one has its own characteristics on power consumption and job execution. This paper proposes a general framework for different hardware configurations in energy/power consumption. The framework provides a fully polynomial-time approximation scheme (FPTAS) to derive a solution with energy consumption very close to the optimal energy consumption in tolerable time/space complexity. Experimental results reveal that the proposed framework is effective in energy efficiency.
[Real time systems, Energy consumption, energy-efficient task partition, periodic real-time task scheduling, Voltage control, Distributed computing, processor scheduling, power consumption, job execution, Energy-efficient task partition, Concurrent computing, power aware computing, heterogeneous processing element, DualCore systems, embedded systems, embedded system, Hardware, Computer networks, Real-time systems, Embedded computing, approximation theory, Partitioning algorithms, hardware configuration, Energy efficiency, fully polynomial-time approximation scheme, dual processing element, computational complexity]
Save Watts in Your Grid: Green Strategies for Energy-Aware Framework in Large Scale Distributed Systems
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
While an extensive set of research project deals with the saving power problem of electronic devices powered by electric battery, few have interest in large scale distributed systems permanently plugged in the wall socket. However, a rapid study shows that each computer, member of a distributed system platform, consume a substantial quantity of power especially when those resources are idle. Today, given the number of processing resources involved in large scale computing infrastructure, we are convinced that we can save a lot of electric power by applying what we called green policies. Those policies, introduced in this article, propose to alternatively switch On and Off computer nodes in a clever way.
[resource management, Energy consumption, on-off computer node, grid computing, Switches, Batteries, Distributed computing, power aware computing, electric battery, Grid computing, Large-scale systems, Monitoring, Testing, green strategy, energy efficient models, environmental factors, large scale distributed systems, scale computing infrastructure, watts saving, green policy, large scale distributed system, Sockets, grid, green computing, Mobile computing, energy-aware framework]
Enhancement of Anticipative Recursively-Adjusting Mechanism for Redundant Parallel File Transfer in Data Grids
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
In data grid, co-allocation architecture can be used to enable parallel transferring of data file from multiple replicas which stored in the different grid sites. Some schemes base on co-allocation model were proposed and used to exploit the different transfer rates among various client-server network links and to adapt dynamic rate fluctuations by dividing data into fragment. These schemes showed the more fragments used the more performance conducted when data transfer in parallel with evidence. In our previous work, we propose a scheme named anticipative recursively-adjusting mechanism (ARAM) in previous research work. The best thing is performance tuning through the alpha value, its rely on special feature to adapt different network situations in a data grid environment. In this paper, the TCP bandwidth estimation model (TCPBEM) is used to evaluate dynamic link state by detect TCP throughput and packet lost rate between grid nodes. We integrate the model into ARAM, called anticipative recursively-adjusting mechanism plus (ARAM+), that can be more reliable and reasonable then previous one. In the meanwhile, we also design a burst mode which could increase transfer rate of ARAM+. This approach not only adapts worst network link but also speedup the overall performance.
[client-server systems, Parallel File Transfer, grid computing, Co-allocation, Throughput, File servers, data grids, client-server network, Distributed computing, Network servers, Bandwidth, Grid computing, anticipative recursively-adjusting mechanism, Computer networks, bandwidth estimation model, Data Grids, Internet, Recursively-Adjusting Mechanism, Web server, Bioinformatics, redundant parallel file transfer]
A Meta-scheduler with Auction Based Resource Allocation for Global Grids
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
As users increasingly require better quality of service from grids, resource management and scheduling mechanisms have to evolve in order to satisfy competing demands on limited resources. Traditional schedulers for grids are system centric and favour system performance over increasing userpsilas utility. On the other hand market oriented schedulers are price-based systems that favour users but are based solely on user valuations. This paper proposes a novel meta-scheduler that unifies the advantages of both the systems for benefiting both users and resources. In order to do that, we design a valuation metric for userpsilas applications and computational resources based on multi-criteria requirements of users and resource load. The meta-scheduler maps user applications to suitable distributed resources using a continuous double auction (CDA). Through simulation, we compare our scheduling mechanism against other common mechanisms used by current meta-schedulers. The results show that our meta-scheduler mechanism can satisfy more users than the others while still meeting traditional system-centric performance criteria such as average load and deadline of applications.
[resource management, meta-scheduler maps, Laboratories, grid computing, Quality of service, system-centric performance criteria, Environmental economics, Cost accounting, auction, resource allocation, meta-scheduler mechanism, price-based systems, scheduling, Grid computing, global grids, Computational modeling, scheduling mechanisms, meta-scheduler, quality of service, market oriented schedulers, Computer science, Processor scheduling, grid, distributed resources, system performance, multicriteria requirements, Resource management, Software engineering]
Mapping Linear Workflows with Computation/Communication Overlap
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
This paper presents theoretical results for mapping and scheduling linear workflows onto heterogeneous platforms. We use a realistic architectural model, representative of current multi-threaded systems. Our model has bounded communication capabilities and full computation/communication overlap. In these workflow applications, the goal is often to maximize throughput or to minimize latency. We present several complexity results, and approximation algorithms, for these two criteria. We also consider the implications of adding feedback loops to linear chain applications.
[complexity, bounded multi-port, multithreaded systems, graph theory, latency, Throughput, communication/computation overlap, approximation algorithms, Distributed computing, Delay, linear workflow scheduling, architectural model, Concurrent computing, feedback, feedback loop, Feedback loop, USA Councils, computation/communication overlap, scheduling, linear workflow mapping, Polynomials, throughput, linear chain applications, mapping, multi-threading, feedback loops, complexity results, linear workflow, Processor scheduling, Streaming media, Approximation algorithms, computational complexity]
Dynamic Resource Allocation in Enterprise Systems
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
It is common that Internet service hosting centres use several logical pools to assign server resources to different applications, and that they try to achieve the highest total revenue by making efficient use of these resources. In this paper, multi-tiered enterprise systems are modelled as multi-class closed queueing networks, with each network station corresponding to each application tier. In such queueing networks, bottlenecks can limit overall system performance, and thus should be avoided. We propose a bottleneck-aware server switching policy, which responds to system bottlenecks and switches servers to alleviate these problems as necessary. The switching engine compares the benefits and penalties of a potential switch, and makes a decision as to whether it is likely to be worthwhile switching. We also propose a simple admission control scheme, in addition to the switching policy, to deal with system overloading and optimise the total revenue of multiple applications in the hosting centre. Performance evaluation has been done via simulation and results are compared with those from a proportional switching policy and also a system that implements no switching policy. The experimental results show that the combination of the bottleneck-aware switching policy and the admission control scheme consistently outperforms the other two policies in terms of revenue contribution.
[Measurement, Bottleneck, switching engine, telecommunication congestion control, Internet service hosting centre, multiclass closed queueing network, Switches, multitiered enterprise system, Network servers, Server Switching, Runtime, resource allocation, System performance, Web and internet services, Resource Allocation, Web server, logical pool, server resource assignment, queueing theory, proportional switching policy, dynamic resource allocation, performance evaluation, Power system modeling, telecommunication switching, bottleneck-aware server switching policy, admission control scheme, Queueing Network, Admission control, Revenue, Internet, Resource management]
A Cost-Aware Resource Exchange Mechanism for Load Management across Grids
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Numerous Grids have been created during the last years. Most of these Grids work in isolation and with different utilisation levels. As the resource utilisation within a Grid has fixed and operational costs, there can be benefits for a Grid to offload requests to another Grid or provide spare resources, thus reducing the cost of over-provisioning. In this work, we enable load management across Grids through resource exchange between them considering the cost for one Grid to acquire resources from another. However, enabling resource exchange amongst Grids is a challenging task: a Grid should not compromise the performance of its local user communities' applications, yet benefit from providing spare resources to other Grids. The load management mechanism and related policies take into consideration the economic compensation of providers for the resources allocated. Experimental results show that the mechanism achieves its goal in redirecting requests, increasing the number of user requests accepted and balancing the load amongst Grids.
[cost reduction, Costs, economic compensation, load sharing, Laboratories, grid computing, intergrid resource allocation, cost-aware resource exchange mechanism, load management, Computer science, resource allocation, Load management, Grid computing, Resource management, IP networks, Contracts, Software engineering, Capacity planning]
snapPVFS: Snapshot-Able Parallel Virtual File System
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
In this paper, we propose a modified parallel virtual file system that provides snapshot functionality. Because typical file systems are exposed to various failures, taking a snapshot is a good way to enhance the reliability of file systems. The PVFS, which is one of the famous parallel file systems deployed in cluster systems, is vulnerable to system failures or users mistakes; however, there is a scarcity of research on snapshots or online backup for the PVFS. Because a PVFS consists of multiple servers on a network, snapshots should be generated properly in each server in the system. Furthermore, before snapshots are generated, the status of each PVFS server must be checked to guarantee sound operation. To demonstrate our approach, we implemented two prototypes of a snapshot-able PVFS (snapPVFS). The performance measurements indicate that an administrator can take snapshots of an entire parallel file system and properly access any previous versions of files or directories in the future without serious performance degradation.
[Measurement, software reliability, parallel file system, virtual storage, file system reliability, File servers, performance degradation, snapshot-able parallel virtual file system, ext3cow, versioning file system, Degradation, Computer science, Concurrent computing, Network servers, Storage area networks, File systems, Prototypes, snapshot functionality, PVFS, snapPVFS, Large-scale systems, cluster systems, snapshot]
Scheduling Mixed Tasks with Deadlines in Grids Using Bin Packing
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
This paper addresses the problem of scheduling independent tasks with different priorities and deadline constraints in computational grids. The problem is first formulated as a bin packing problem. We propose a heuristic algorithm named Residual Capacity Maximization Scheduling (RCMS), which integrates the ideas of a classical bin packing algorithm (Best Fit) and a mixed integer quadratic programming modeling approach. RCMS is highly scalable as it does not need to know the global state of the grid. Simulation results based on a real-world trace demonstrate that with respect to the total number of schedulable tasks meeting deadlines, RCMS outperforms existing approaches by 8%. With respect to the number of the highest priority tasks meeting deadlines, RCMS outperforms them by over 20% on average. Moreover, RCMS is a fully distributed algorithm with a low complexity.
[residual capacity maximization scheduling, Scalability, grid computing, Quality of service, mixed integer quadratic programming, Scheduling, Quadratic programming, quadratic programming, grids, mixed tasks, Scheduling algorithm, bin packing, Computer science, heuristic algorithm, Processor scheduling, Clustering algorithms, scheduling, Grid computing, Dispatching, Software engineering]
Orchestra: Extensible Block-Level Support for Resource and Data Sharing in Networked Storage Systems
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
High-performance storage systems are evolving towards decentralized commodity clusters that can scale in capacity, processing power, and network throughput. Building such systems requires: (a)Sharing physical resources among applications; (b)Sharing data among applications; (c) Allowing customized views of data for applications. Current solutions satisfy typically the first two requirements through a distributed file-system, resulting in monolithic, hard-to-manage storage systems. In this paper, we present Orchestra, a novel storage system that addresses all three above requirements below the file-system by extending the block layer. To provide customized views, Orchestra allows applications to create semantically-rich virtual block devices by combining simpler ones. To achieve efficient resource and data sharing it supports block-level allocation and byte-range locking as in-band mechanisms. We implement Orchestra under Linux and use it to build a shared cluster file-system. We evaluate it on a 16-node cluster, finding that the flexibility offered by Orchestra introduces little overhead beyond mandatory communication and disk access costs.
[Costs, distributed file system, semantically-rich virtual block devices, Throughput, File servers, scalability, shared virtual disk, block-level allocation, Prototypes, decentralized commodity clusters, storage virtualization, Image storage, Cluster storage, networked storage systems, Orchestra, Application software, block-level I/O, Computer science, extensible storage stack, Storage area networks, Linux, resource sharing, byte-range locking, file organisation, extensible block-level support, Resource management, data sharing]
Optimal Placement of Pipeline Applications on Grid
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Computation and communication intensive applications such as scientific data analysis and data visualization are commonly found in grid computing environment.These applications can be divided into a sequence of pipeline stages which could be executed concurrently on different grid resources to achieve high performance. Finding the optimal placement of pipeline stages on grid is a difficult problem due to the aggregation of computation and communication cost involved. This paper proposes a solution to such problem that allows the maximum application throughput by the integration of pipeline placement and data routing. The proposed solution, on one hand, minimizes the computation bottleneck of a pipeline and, on the other hand, prevents the communication cost between successive stages from dominating the entire processing time. Our proposed solution consists of two novel methods. The first method is the single path pipeline execution that fully exploits temporal parallelism and the second method is the multipath pipeline execution which can leverage both temporal and spatial parallelism inherent in any pipeline applications. We evaluate our proposed methods using a set of experiments running in a real grid environment. When compared with the results from several traditional placement methods, our proposed methods give the highest throughput.
[Pipelines, grid computing, Throughput, pipeline applications, grid resources, Concurrent computing, resource allocation, data visualisation, Parallel processing, Grid computing, Cost function, Computational efficiency, data visualization, pipeline scheduling, Data analysis, data analysis, pipeline placement, data routing, High performance computing, scientific data analysis, Data visualization, pipeline mapping, pipeline processing, multipath pipeline execution, grid computing environment]
AHPIOS: An MPI-Based Ad Hoc Parallel I/O System
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
This paper presents the design and implementation of a portable ad-hoc parallel I/O system (AHPIOS). AHPIOS virtualizes on-demand available distributed storage resources and allows the files to be striped over several storage devices. Additionally, the design unifies the configuration of the MPI-IO library and the AHPIOS data servers. By a strong integration of the application, MPI-IO library and file system, a significant performance improvement can be achieved. The experimental section shows that the full MPI-IO integrated AHPIOS implementation of file access operations outperforms the existing MPI-IO implementation by as much as 495% for file writes and 522% for file reads.
[shared-disk architecture, message passing, Stability, application program interfaces, virtualization, parallel file system, portable ad-hoc parallel I/O system, MPI-IO library, Supercomputers, cluster computing, parallel processing, parallel I/O, Resource virtualization, distributed storage resources, Concurrent computing, data servers, File systems, Cost function, shared memory systems, Libraries, parallel file systems]
A Backward Algorithm in a Distributed Agreement Computing
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
In traditional agreement protocols, multiple processes just aim at agreeing on one value out of values shown by the processes. In meetings of human societies, agreement procedures are often used and are so flexible that persons can change their opinions and can use various types of agreement conditions like majority-condition. We discuss a flexible agreement protocol of multiple peers by taking into account human behaviors in social agreement procedures in a fully unstructured peer-to-peer (P2P) model. We define a recoverable cut which is a satisfiable set of previous values which every peer can back. We discuss constraints on values which each peer can take at each round and a backward algorithm to find a satisfiable cut. If each peer takes a previous value in a recoverable cut, the peer can make an agreement. We discuss a protocol for multiple peers to find a recoverable cut.
[Protocols, Humans, behavioural sciences, flexible agreement protocol, agreement protocol, History, Distributed computing, P2P networks, Concurrent computing, backward algorithm, distributed agreement computing, unstructured peer-to-peer model, distributed systems, Computer networks, protocols, peer-to-peer computing, Peer to peer computing, Social network services, computer networks, Application software, social agreement procedures, peer-to-peer systems, social sciences, Collaborative work, human behaviors]
Bootstrapping in Peer-to-Peer Systems
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Peer-to-Peer systems have become a substantial element in computer networking. Distributing the load and splitting complex tasks are only some reasons why many developers have come to adopt this technology. However, all of them face a severe problem at the very beginning: setting up an overlay network, such that other clients can easily join it. With an empty peer cache common bootstrapping methods require some manually triggered actions for discovering a peer on the overlay. We therefore introduce an approach for an automated bootstrapping based on DDNS. In this paper we give detailed information about our protocol and document its efficiency and scalability.
[Protocols, Terminology, Scalability, overlay network, Domain Name System, Distributed computing, Concurrent computing, protocol, System performance, dynamic domain name system, Computer networks, DDNS, Communication networks, protocols, peer-to-peer computing, Peer to peer computing, load distribution, computer networking, Peer-to-Peer, P2P, Bootstrapping, automated bootstrapping method, computer bootstrapping, peer-to-peer system, Internet]
SkyEye.KOM: An Information Management Over-Overlay for Getting the Oracle View on Structured P2P Systems
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
In order to ease the development and maintenance of more complex P2P applications, which combine multiple P2P functionality (e.g. streaming and dependable storage), we suggest to extend structured P2P systems with a dedicated information management layer. This layer is meant to generate statistics on the whole P2P system and to enable capacity-based peer search, which helps the individual functionality layers in the P2P application to find suitable peers for layer-specific role assignment. We present in this paper SkyEye.KOM, an information management layer applicable on DHTs, which fulfills these desired functionality. SkyEye.KOM builds an over-overlay, which is scalable by leveraging the underlying DHT, easy to deploy as simple add-on to existing DHTs and efficient as it needs O(log N) hops per query and to place peer-specific information network wide accessible. Evaluation shows that SkyEye.KOM has a good query performance and that the costs for maintaining the over-overlay are very low.
[Protocols, query performance, Information management, Multimedia communication, information management over-overlay, DHT, Self-X, complex P2P applications, Cost function, System Statistics, Image storage, Kalman filters, Oracle, Monitoring, information management layer, peer-to-peer computing, Peer to peer computing, Multimedia systems, information management, Peer-to-Peer, software maintenance, Statistics, SkyEye.KOM, capacity-based peer search, Streaming media, statistical analysis, Capacity-based Peer Search]
Performance Analysis of Peer-to-Peer Video Streaming Systems with Tree and Forest Topology
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Peer-to-peer networks are an increasingly popular solution for the distribution of media content to a large number of users, with limited investments for network infrastructures. The distribution of a real time video stream imposes strict performance requirements such as small playback delays and few frame losses. In this paper, we focus on peer-to-peer video streaming systems with tree or forest content distribution structure and we provide a sensitivity analysis to investigate the impact of three critical parameters - rejoin time, average permanence time of peers and playback threshold - over the quality of the video stream received by users. The study, carried out through simulation, considers a general peer-to-peer video streaming reference model with tree/forest topology.
[Performance evaluation, sensitivity analysis, forest content distribution structure, Analytical models, Network topology, System performance, video streaming, Performance analysis, playback threshold delay, Sensitivity analysis, peer-to-peer computing, Peer to peer computing, Delay effects, trees (mathematics), telecommunication network topology, computer network performance evaluation, peer-to-peer, forest topology, performance, tree topology, Streaming media, Performance loss, peer-to-peer video streaming system, peer-to-peer network, performance analysis]
Locality-Aware Peer-to-Peer SIP
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
SIP (Session Initiation Protocol) is a signaling protocol widely used in multimedia communication. Recently, P2PSIP (Peer-to-Peer SIP), which combines DHT (distributed hash table) and SIP, has been proposed to overcome the drawbacks of traditional CS SIP (client/server architecture SIP). However, the introduction of DHT increases the registration overhead and, session setup overhead/latency. These problems become unbearable when P2PSIP overlay grows huge. To address these problems, we propose locality-aware P2PSIP in this paper. In the P2PSIP context, we design a locality-aware approach, which can be applied to most DHTs. This locality-aware approach reduces both DHT routing hop count and latency per DHT hop. Performance evaluation shows that registration overhead and session setup overhead/latency are reduced dramatically in locality-aware P2PSIP.
[P2PSIP overlay, Protocols, Costs, Multimedia communication, Delay, locality-aware, Network servers, DHT, Robustness, client/server architecture SIP, multimedia communication, peer-to-peer computing, Peer to peer computing, Multimedia systems, P2PSIP, performance evaluation, Routing, locality-aware peer-to-peer SIP overlay, DHT routing hop count, signalling protocols, session initiation protocol, distributed hash table, telecommunication network routing, file organisation, Internet, SIP, signaling protocol]
Improving Web Server Performance Through Main Memory Compression
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Current Web servers are highly multithreaded applications whose scalability benefits from the current multi-core/multiprocessor trend. However, some workloads cannot capitalize on this because their performance is limited by the available memory and/or the disk bandwidth, which prevents the server from taking advantage of the computing resources provided by the system. To solve this situation we propose the use of main memory compression techniques to increment the available memory and mitigate the disk band-width problem, allowing the Web server to improve its use of CPU system resources. In this paper we implement to the Linux OS a full SMP capable main memory compression subsystem to increase the performance of a Web server running the SPEC Web 2005 benchmark. Although main memory compression is not a new technique per-se, its use in a multicore environment running heavily multithreaded applications like a Web server introduces new challenges in the technique, such as scalability issues and the trade-off between the compressed memory size and the computational power required to achieve it. Finally, the evaluation of our implementation shows promising results such as a 30% Web server throughput improvement and a 70% reduction in the disk bandwidth usage.
[Scalability, Data compression, page cache, cache storage, Proposals, Delay, CPU system resource, Multiprocessing systems, Web server performance improvement, Linux OS, file servers, Bandwidth, Hardware, multithreading, multicore system, Web server, data compression, paged storage, multiprocessing systems, Multicore processing, multi-threading, main memory compression, Linux, SMP, multiprocessor system, Internet]
OID: Optimized Information Discovery Using Space Filling Curves in P2P Overlay Networks
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
In this paper, we present the system design and evaluation of a space filling curve (SFC)-based P2P information discovery system OID. The OID system uses multiple SFCs to significantly optimize the performance of multi-attribute range queries, particularly for applications with a large number of data attributes where a single big SFC-based index is inefficient. The basic idea is to have multiple SFC based indices and select the best one to perform a query. We also introduce two tree-based query optimizations that increase the scalability of the system.
[peer-to-peer computing, Scalability, tree-based query optimizations, Space-Filling Curves, P2P overlay networks, space filling curves, Resource Discovery, Design optimization, P2P information discovery system, P2P, query processing, Fault tolerance, Query processing, optimized information discovery, multiattribute range queries, Overlay Networks, Grid computing, Filling, Complex Queries]
Integrating Sensor Streams in pHealth Networks
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Personal Health (pHealth) sensor networks are generally used to monitor the well being of both athletes and the general public to inform health specialists of future and often serious ailments. The problem facing these domain experts is the scale and quality of data they must search in order to extract meaningful results. By using peer-to-peer sensor architectures and a mechanism for reducing the search space, we can, to some extent, address the scalability issue. However, synchronisation and normalisation of distributed sensor streams remains a problem in many networks. In the case of pHealth sensor networks, it is crucial for experts to align multiple sensor readings before query or data mining activities can take place. This paper presents a system for clustering and synchronising sensor streams in preparation for user queries.
[expert systems, Humans, data mining, peer-to-peer sensor architectures, Sensor phenomena and characterization, Sensor systems, Data mining, Integrating sensor streams, pHealth networks, Universal Serial Bus, search space, data mining activity, health care, search problems, distributed sensor streams, peer-to-peer computing, Peer to peer computing, sensor readings, distributed sensors, Heart rate, Wireless sensor networks, domain experts, Collaboration, personal health sensor networks, Biomedical monitoring]
A Performance Study of Clustering Web Application Servers with Distributed JVM
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
A Distributed Java Virtual Machine (DJVM) is a cluster-wide set of extended JVMs that enables parallel execution of a multithreaded Java application. It has proven effectiveness for scaling scientific applications. However, leveraging DJVMs to cluster real-life web applications with commercial server workloads has not been well studied. This paper presents a new generic clustering approach based on DJVMs that promote user transparency and global object sharing for web application servers. We port Apache Tomcat to our JESSICA2 DJVM and study the performance of a wide range of web applications running on the server. Our experimental results show that this approach can scale better than the traditional clustering approach, particularly for cache-centric web applications.
[Tomcat, JESSICA2 DJVM, Turning, Apache Tomcat, Yarn, parallel processing, Global Object Space, file servers, Distributed Java Virtual Machine, Libraries, Application Server, Java, cache-centric Web applications, Cooperative Caching, Virtual machining, Application software, Middleware, Web application servers clustering, Computer science, Bridges, distributed Java virtual machine, Server Clustering, multithreaded Java application parallel execution, virtual machines, Tin, Internet]
QoS-Aware Service Selection Using QDG for B2B Collaboration
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Collaboration among enterprises through Web service has become a hot topic. Before the collaboration, how to select the most appropriate enterprise to collaborate with, from a set of enterprise candidates that provide similar functions, is an important issue. Existing work focus on proposing evaluation rules, and aggregating these rules to evaluate a service, where subjectiveness is usually involved. In this paper, we propose to utilize serve, be served relationship to evaluate the quality of services. In more detail, we use quality dependency graph (QDG) method model the relationship among enterprises, and then, by traveling the built QDG, an analytic hierarchy process (AHP) model is used to calculate the evaluation result of each candidate organization. Our method provides a more objective way for collaboration on enterprise level.
[Chaos, Service Composition, Costs, Laboratories, graph theory, Quality of service, AHP, Quality Dependency Graph, analytic hierarchy process, B2B, QoS, QDG, Collaborative software, International collaboration, quality of service, QoS-aware Web service selection, B2B collaboration, Web services, service dependency, decision making, Collaborative work, Australia, Acceleration, quality dependency graph, business data processing]
Maximizing Download Bandwidth for File Sharing in BitTorrent-like Peer-to-Peer Networks
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Peer-to-peer file sharing applications are major proportion of traffic in Internet. Among P2P file sharing applications, BitTorrent is known to be the most popular system, in which peers can download pieces of file proportional with upload bandwidth shared with others. Therefore, for such a system, bandwidth adjustment is very significant for peers. In this paper we aim at modeling this as a solution to an optimization problem which maximizes the portion of download bandwidth constrained by the average download time of peers. To solve this, we propose an iterative algorithm which can be implemented in a distributed manner with low computation and communication overhead.
[iterative methods, optimization problem, Protocols, Scalability, Telecommunication traffic, Iterative Algorithm, Distributed computing, Optimization, optimisation, Bandwidth, Traffic control, file sharing application, BitTorrent, Mathematical model, peer-to-peer computing, Peer to peer computing, Peer-to-Peer, Application software, iterative algorithm, BitTorrent system, Computer science, bandwidth allocation, network traffic, Fluid Model, Internet, telecommunication traffic, peer-to-peer network, download bandwidth maximization]
Mining Community Structures in Peer-to-Peer Environments
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Most social networks exhibit community structures, in which nodes are tightly connected to each other within a community but only loosely connected to nodes in other communities. Researches on community mining have received a lot of attention; however, most of them are based on a centralized system model and thus not applicable to the distributed model of P2P networks. In this paper, we propose a distributed community mining algorithm, namely Asynchronous Clustering and Merging scheme (ACM), for computing environments. Due to the dynamic and distributed nature of P2P networks, The ACM scheme employs an asynchronous strategy such that local clustering is executed without requiring an expensive global clustering to be performed in a synchronous fashion. Experimental results show that ACM is able to discover community structures with high quality while outperforming the existing approaches.
[peer-to-peer computing, Peer to peer computing, Social network services, peer-to-peer environments, peer-to-peer networks, Merging, Communities, data mining, social networks, centralized system model, Educational institutions, Distributed node clustering, asynchronous clustering scheme, local clustering, Distributed computing, P2P networks, asynchronous merging scheme, USA Councils, Clustering algorithms, community structures mining, Hardware, IP networks, connected graph]
Continuous Range Search Query Processing in Mobile Navigation
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Range search query processing has become one of the most important technologies in spatial and mobile databases. Most literature focuses on static range search extended from one point on both Euclidean distance and actual network distance, but there are only a few methods which can properly solve the problem for moving users, such as searching objects of interest on a road within a certain range. Although there are some techniques, which address continuous search, most approaches are absorbed in the KNN (k nearest neighbour) queries which are a different research area. In this paper, we propose two new methods to process continuous range search query in mobile computing. One is constructed using R-tree index based on Euclidean distance, and the other addresses the requirement on actual network distance.
[Roads, visual databases, Mobile Navigation, Information systems, query processing, mobile navigation, mobile computing, k nearest neighbour query, database indexing, Spatial Databases, Distributed databases, Range Query, Continuous Range Search, tree data structures, Navigation, Continuous Range Query, Mobile Query, Spatial databases, Information technology, actual network distance, Global Positioning System, continuous range search query processing, spatial database, Query processing, R-tree index, Euclidean distance, Mobile Databases, Mobile computing, mobile database]
Automatic Partitioning of Object-Oriented Programs for Resource-Constrained Mobile Devices with Multiple Distribution Objectives
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
We describe a system that takes monolithic Java programs as its input and automatically converts them into distributed Java programs. Our research is situated in the context of resource-constrained mobile devices, in which there are often multiple distribution objectives, such as minimizing energy consumption on mobile devices by off loading workloads to a stationary server, or minimizing total execution time. Our method initially constructs an object relation graph (ORG), using a combination of static analysis and offline profiling. Instead of directly partitioning this ORG, we then transform it into a target graph (TG) to abstract from concrete distribution infrastructures and objectives. By applying this two-layer graph modeling, we achieve a unified strategy for different partitioning goals. Preliminary benchmarks for our prototype implementation are highly promising, with an average speedup factor of almost 1.5 and an average energy savings of 83.5% for the beneficial benchmarks.
[Energy consumption, automatic object-oriented program partitioning, Mobile handsets, Distributed computing, mobile computing, USA Councils, distributed Java programs, Automatic Partitioning, Personal digital assistants, Objected-Oriented, object relation graph, Java, object-oriented programming, Resource-Constrained, Object oriented modeling, program diagnostics, Data structures, monolithic Java programs, resource-constrained mobile devices, multiple distribution objectives, Computer science, target graph, stationary server, two-layer graph modeling, Mobile Devices, Mobile computing]
Virtual Ring-Based Hole Avoiding Routing in Mobile Ad Hoc Networks
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Greedy routing protocols provide a scalable and cost effective solution for routing packets in mobile ad-hoc networks (MANETs). However, such routing protocols can not be used in the networks where holes exist. We propose a virtual ring-based hole avoiding routing protocol (VRHAR) in MANETs where holes exist. Simulation studies show that the proposed protocol can improve the routing performance more effectively than existing greedy routing protocols.
[Costs, mobile radio, hole avoiding routing, Scalability, greedy algorithms, greedy routing protocols, Ad hoc networks, virtual ring, virtual ring-based hole avoiding routing protocol, Mobile ad hoc networks, Global Positioning System, Computer science, Information science, MANET, Network topology, greedy forwarding algorithm, routing protocols, mobile ad hoc networks, Routing protocols, Mobile ad-hoc networks, ad hoc networks, Mobile computing]
An Efficient Policy System for Body Sensor Networks
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Body sensor networks (BSNs) have become a promising technology for healthcare, in which biosensors continuously monitor physiological parameters of a user. Distinct from conventional sensor networks for environmental monitoring, such networks need to be adaptive and can therefore be easily managed. In addition, security becomes a necessity. To this end, we design a policy system that implements policy-driven management on the sensor level. Biosensor adaptability is realized through support of dynamic loading, enabling and disabling of policies without shutting down nodes. In addition, fine-grained access control becomes possible through authorization policies on biosensors. Design and implementation details of the policy system are presented. Experimental results demonstrate that the policy system is viable and can accelerate application development of biosensor networks for healthcare.
[Access control, wireless sensor networks, Medical services, biosensor adaptability, dynamic loading, Sensor systems, Environmental management, patient monitoring, Condition monitoring, Authorization, authorization, authorisation, policy-driven management system, access control, Policy System, health care, Body Sensor Networks, data security, body area networks, biomedical telemetry, Body sensor networks, body sensor network, Biosensors, Acceleration, Biomedical monitoring]
Achieving Ubiquitous Network Connectivity Using an RFID Tag-Based Routing Protocol
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Ubiquitous network connectivity is challenged in areas outside of the range of wireless base stations and cellular networks. In such networks the mobile nodes are disconnected for significantly long periods of time. In this paper, we propose an approach where mobile nodes deposit/retrieve messages to/from known point locations in the geographic region. Messages are delivered from a source by being deposited at one or more locations that are later visited by the destination. We propose the use of passive, read/writable radio frequency identification tags, one per point location. Our simulation results indicate that our approach can achieve competitive message latency and delivery rates. We also provide techniques for optimizing the tag placement, including tag pruning, probability based tag distribution and a genetic algorithm; the genetic algorithm is shown to provide the best solutions to this problem.
[Costs, radiofrequency identification, ubiquitous network connectivity, TBR protocol, ubiquitous computing, wireless base station, Mobile ad hoc networks, Genetic algorithms, mobile node communication, routing, tag-based routing protocol, mobile ad hoc networks, Routing protocols, protocols, Base stations, mobile radio, sparse networks, probability, cellular network, genetic algorithms, radio frequency identification tag, RFID tags, message delivery rate, Computer science, Land mobile radio cellular systems, telecommunication network routing, genetic algorithm approach, geographic region, probabilistic distribution, RFID tag, Radiofrequency identification, Software engineering, cellular radio]
Sleeping Schedule Aware Minimum Transmission Broadcast in Wireless Ad Hoc Networks
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
As a fundamental operation of wireless ad hoc networks (WANET), broadcast has been widely studied in the past ten years. However, most existing broadcasting strategies assumed non-sleeping wireless devices. Little attention has been paid to broadcast in WANETs with sleeping schedule, which is a promising power-saving method in wireless networks. In this paper we study the sleeping schedule aware minimum transmission broadcast problem in WANETs (MTB-SA problem) and prove its NP-hardness. Both centralized and distributed approximation algorithms are presented to solve the problem. The centralized algorithm SchmM-Cent has an approximation ratio of 3(ln+1) and time complexity of O(n^3). The distributed algorithm SchmM-Dist has a constant approximation ratio of at most 20, while time and message complexity are both O(n). In addition, we provide theoretical analysis and simulations to evaluate the performance of the approximation algorithms.
[Algorithm design and analysis, Minimum transmission broadcast, sleeping schedule aware, wireless ad hoc network (WANET), Broadcast technology, wireless ad hoc networks, Scheduling algorithm, Mobile ad hoc networks, Analytical models, optimisation, Processor scheduling, distributed approximation algorithms, NP-hard problem, sleeping schedule, Broadcasting, approximation algorithm, Approximation algorithms, minimum transmission broadcast, Performance analysis, ad hoc networks, Distributed algorithms, computational complexity]
TCP Enhancement in IEEE 802.11e Wireless Networks
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Wireless networks and multimedia applications are two rapidly emerging technological trends. The IEEE 802.11e protocol has been developed to support QoS for such delay-sensitive applications in wireless networks. This protocol has also been extensively studied in the literature over the past few years, and several mechanisms to improve its performance have been proposed concentrating on ways to reinforce QoS guarantees for delay-sensitive applications. While the Internet traffic is still dominated by TCP-based applications, the negative effects of IEEE 802.11e service differentiation scheme on the performance of TCP have not received enough attention. In this paper, we first, as an attempt to highlight these effects, evaluate the performance of TCP in 802.11e WLANs when competing with high priority VoIP traffic. We then evaluate the enhancement achievable by our proposed cross-layer schemes, IEDCA and RE-TCP, which exploit the TCP bidirectional nature to alleviate TCP starvation and consequently improving its performance. Our simulation results show significant improvements in terms of TCP goodput, retransmissions and segment delay (without any negative effect on the performance of delay-sensitive applications).
[TCP, Wireless LAN, Distributed computing, multimedia application, Concurrent computing, Analytical models, IEEE 802.11e protocol, Wireless networks, QoS, delay-sensitive application, Traffic control, Computer networks, improved enhanced distributed channel access, wireless network, Internet VoIP traffic, multimedia communication, resource efficient TCP enhancement, Multimedia systems, Delay effects, IEEE 802.11e, performance evaluation, WLAN, quality of service, DiffServ networks, transport protocols, wireless LAN, Internet telephony, telecommunication traffic, Context modeling]
A Novel Distributed Index Approach for Service Discovery in MANETs
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Efficiently discovering services in terms of diversified service constraints in a dense MANET is a challenging issue. This paper proposes to build a distributed suffix tree on backbone nodes as XML-based services index to provide a concise profile for service descriptions. Moreover, a content-addressable P2P overlay and corresponding fault-tolerance mechanisms are introduced to support the distributed suffix tree and deal with the changes in network topology. In such a way, the precision and recall for service discovery are guaranteed and can also be degraded gracefully in the face of node failures. Experimental results show that our approach can increase at least 30% precision in comparison with existing distributed index solutions, and our newly-added messages for every 100 queries are only 22% as many as the ones of the compared solutions.
[XML-based service description, Spine, Drives, Mobile communication, diversified service constraints, Personnel, Mobile ad hoc networks, Recruitment, Degradation, Wireless communication, Fault tolerance, Network topology, distributed index approach, fault-tolerance mechanisms, content-addressable P2P overlay, distributed suffix tree, mobile radio, peer-to-peer computing, fault tolerance, telecommunication network topology, network topology, service discovery, MANET, Web services, XML, service descriptions, ad hoc networks, XML-based services index]
Link Availability Prediction in Ad Hoc Networks
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Since mobility may cause radio links to break frequently, one pivotal issue for routing in mobile ad hoc networks is how to select a reliable path that can last longer. Several metrics have been proposed in previous literatures, including link persistence, link duration, link availability, link residual time, and their path equivalents. In this paper, we present a novel algorithm for predicting continuous link availability between two mobile ad hoc nodes. By a rough estimation of the distance between two nodes, our approach is able to accurately predict link availability over a short period of time. Simulation results are given to verify our approach. This study could serve as groundwork for further ad hoc network researches including analyzing and optimizing other network protocols.
[Availability, Protocols, mobile radio, continuous link availability prediction, Peer to peer computing, link residual time, Predictive models, Radio link, link availability, Routing, Ad hoc networks, link persistence, Mobile ad hoc networks, network protocols, telecommunication network routing, mobile ad hoc networks, link duration, Prediction algorithms, Computer networks, ad hoc networks]
GWRR: Greedy Weighted Region Routing in Wireless Sensor Networks
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Wireless sensor networks (WSNs) consist of large number of sensor nodes with limited sensing, processing and communication capabilities that cooperatively fulfill environmental sensing and monitoring tasks. WSNs are meant to be deployed in large numbers in various environments, including remote and more importantly harsh environments ensuing potential hardware or software faults which consequently may cause temporal unavailability of some sensor nodes. Geographic routing algorithms owing to low overhead of message passing and state preserving are very promising candidate for such environments. In this paper, we propose greedy weighted region routing (GWRR) algorithm that addresses message loss tolerability in harsh and hostile environments by assigning higher weights to harsher regions and then we present a nearly-optimal routing in dense WSNs. Moreover, we demonstrate that GWRR has low computational overhead. Simulation experiments confirm the validity of proposed algorithm with high degree of accuracy.
[Event detection, wireless sensor networks, Computerized monitoring, greedy algorithms, geographic routing algorithms, Sensor fusion, Sensor systems, greedy weighted region routing, Computer science, Wireless sensor networks, computational overhead, Fault detection, Voting, harsh environments, telecommunication network routing, message loss tolerability, communication capabilities, Routing protocols, Hardware]
Optimal Updating Time Using Theory of Reliability
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
In this paper, we use reliability theory to determine the answer of an important question about how often we need to update a piece of secure information. For example, one may need to determine an appropriate time for updating a new password. To generalize, we compute an efficient updating time for a dependable secure computing (DSC) system which stores secret information in a reliable and secure fashion among n servers. The secret information can be reconstructed by successfully connecting to any k servers (k  n), but cannot be revealed by using (k - 1) or fewer servers. Generally, the secret information has to be updated in the system in order to prevent adversaries from learning secret information by breaking into k servers in a DSC system. There are serval proposed procedures to update secret information, but few has been able to determine what an appropriate updating time on a DSC system is. To achieve the most reliable way to safeguard the secret information, we developed an interesting approach to compute the updating time on a DSC system from the reliability point of view.
[Reliability theory, Reliability engineering, dependable secure computing system, reliability theory, Distributed computing, Concurrent computing, optimal updating time, security of data, Information security, Time sharing computer systems, Collaborative work, Polynomials, Cryptography, Joining processes, secure information]
Integrity-Preserving Replica Coordination for Byzantine Fault Tolerant Systems
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
The use of good random numbers is essential to the integrity of many mission-critical systems. However, when such systems are replicated for Byzantine fault tolerance, a serious issue arises, i.e., how do we preserve the integrity of the systems while ensuring strong replica consistency? Despite the fact that there exists a large body of work on how to render replicas deterministic under the benign fault model, the solutions regarding the random number control are often overly simplistic without regard to the security requirement, and hence, they are not suitable for practical Byzantine fault tolerance. In this paper, we present a novel integrity-preserving replica coordination algorithm for Byzantine fault tolerant systems. The central idea behind this algorithm is that all random numbers to be used by the replicas are collectively determined, based on the contributions made by a quorum of replicas, at least one of which is correct. We have implemented the algorithm in Java and conducted extensive experiments, in both a LAN testbed and an emulated WAN environment. We show that our algorithm is particularly suited for Byzantine fault tolerant systems operating in the LAN environment, or where replicas are connected by high-speed low-latency networks.
[Availability, Wide area networks, Java, high-speed low-latency networks, random number control, Mission critical systems, WAN, Replica Consistency, Entropy, data integrity, byzantine fault tolerant systems, System Integrity, Security, Fault tolerant systems, distributed algorithms, LAN, integrity-preserving replica coordination, fault tolerant computing, Byzantine Fault Tolerance, Random Numbers, system integrity, Local area networks, Random number generation, Testing]
A Dynamic Load Balancing Mechanism for New ParaLEX
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
ParaLEX, developed recently by the authors, is a parallel extension for the CPLEX mixed integer optimizer which is known as one of the fastest commercial solvers for the mixed integer programming problems. In our previous work, we showed that ParaLEX could efficiently perform 30 solver parallelizations. On the other hand, the simple load balancing mechanism of ParaLEX did not obviously have scalability. In this paper, we propose a load balancing mechanism for a new version of ParaLEX. Preliminary computational results show that the load balancing mechanism is quite efficient in solving a lot of classes of problem instances.
[solver parallelizations, load balancing, Scalability, integer programming, Linear programming, Partitioning algorithms, parallel processing, Equations, parallel programming, resource allocation, ParaLEX, CPLEX mixed integer optimizer, Web pages, mixed integer programming, Load management, Agriculture, Dynamic programming, Large-scale systems, dynamic load balancing, Mathematical programming]
Network Fault Diagnosis: An Artificial Immune System Approach
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Artificial immune systems (AIS) have been widely used in many fields such as data analysis, multimodal function optimization, error detection, etc. In this paper, we introduce a novel artificial immune systems approach for diagnosing faults in a network of processors under the PMC model. We investigate how AIS can be used for system-level fault diagnosis. Our theoretical analysis and experimental results demonstrate the effectiveness of the AIS-based diagnosis approach for small and large class of networks in both the worst and average cases, making it a viable alternative to traditional fault diagnosis approaches.
[System testing, network fault diagnosis, Data analysis, fault diagnosis, multiprocessing systems, PMC model, multiprocessor systems, Educational institutions, Data engineering, Sequential diagnosis, processors, Information technology, Multiprocessing systems, artificial immune systems, Fault diagnosis, Computer science, Artificial immune systems, artificial immune system, system-level fault diagnosis]
Scheduling Peers Based on Credit Construction Period in Peer-to-Peer Networks
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
In Peer-to-Peer (P2P) networks if adversaries such as Sybil attackers have got most identities in the network, they will control routing table or traffic. In this paper, we propose a framework based on two complementary techniques to defense malicious node after they transmit data to other malicious peers instead of honest peers. The first approach, based on behaviors of destination nodes, is used to count the local nodes credit values in a period of credit construction time. The second approach is updating the global nodes credit values between schedulers. We propose scheduler to maintain the trust relationship between global nodes. In order to assess the effectiveness of the above techniques, we extend Eclipse attack code in simulator p2pSim-0.3 and implement scheduler defense malicious attack behavior algorithm based on both credit and proximity. We adopt Chord protocol and Euclidean topology to implement scheduling algorithm, but the same methodology can be applied to other protocols and topologies as well.
[telecommunication security, Protocols, Scheduler, Control systems, Credit Construction Period, p2pSim-0.3 simulator, Security, Chord protocol, Network topology, scheduling, Communication system traffic control, protocols, peer-to-peer computing, Peer to peer computing, Instruments, network routing, telecommunication network topology, Routing, Euclidean topology, Scheduling algorithm, YouTube, P2P, network traffic, scheduler defense malicious attack behavior algorithm, telecommunication network routing, Internet, peer scheduling, credit construction period, telecommunication traffic, peer-to-peer network, trust relationship]
Area Difference Based Recovery Information Placement for Mobile Computing Systems
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
In a mobile computing system, mobile hosts may move around cells, resulting in a considerable cost for locating and retrieving the recovery information, which is necessary for fault tolerance. To speed up the recovery, traditionally, recovery information is migrated according to the location of the mobile host. In this paper, a scheme for efficiently handling the recovery information is proposed. When a mobile host moves out of a certain range, only partial recovery information of the mobile host needs to be migrated to mobile support stations. It can avoid the unnecessary migration of recovery information. Moreover, the performance of the proposed scheme is evaluated and compared with the traditional movement based scheme.
[Checkpointing, checkpointing, Costs, message passing, fault tolerance, Information retrieval, Mobile communication, mobile support station, Distributed computing, mobile computing system, Recovery information placement, Message logging, Concurrent computing, Computer science, pessimistic message logging, mobile computing, Fault tolerant systems, area difference based recovery information placement, Frequency, Fault-tolerance, Mobile computing, independent checkpointing technique]
An Algorithm to Detect Stepping-Stones in the Presence of Chaff Packets
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
A major concern for network intrusion detection systems is the ability of an intruder to evade the detection by routing through a chain of the intermediate hosts to attack a target machine and maintain the anonymity. Such an intermediate host is called a stepping-stone. The intruders have developed some evasion techniques such as injecting chaff packets. A number of algorithms have been proposed to detect stepping-stones, but some of them failed to detect correctly when the network traffic is somehow corrupted or with the chaff packets. We discuss the viability of solving those issues by improving a previous methodology. The algorithm is based on finding as many matched pairs of incoming and outgoing packets on the same host as possible and then decide whether it is a stepping-stone connection by the mismatched rate. We examine a number of tradeoffs in choosing the threshold values by simulating network traffic. Our experiments report a very good performance with very low false detection rates when using carefully selected parameter values.
[telecommunication security, stepping-stone detection, Telecommunication traffic, false detection rate, intrusion detection, network intrusion detection system, Delay, USA Councils, Intrusion detection, Delta modulation, chaff packet, Cryptography, Stepping-stone, evasion technique, network routing, network security, computer networks, Routing, connection chain, Computer science, network traffic, telecommunication network routing, Timing, Detection algorithms, telecommunication traffic, chaff]
An Online Model Checking Tool for Safety and Liveness Bugs
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Modern software model checkers are usually used to find safety violations. However, checking liveness properties can offer a more natural and effective way to detect errors, particularly in complex concurrent and distributed e-business systems. Specifying global liveness properties which should always eventually be true proves to be more desirable, but it is hard for existing software model checkers to verify liveness in real codes because doing so requires finding an infinite execution. For solving such a challenge, this paper proposes an online checking tool to verify the safety and liveness properties of complex systems. We adopt the linear temporal logic to describe the semantics of the finite model checking, use binary instrumentation to obtain the distribute states and apply a checking engine to dynamically verify the finite trace linear temporal logic properties. At last, we demonstrate the method in a distributed system using distributed protocol Paxos and achieve good results by experiments.
[program debugging, online model checking tool, Protocols, Surface-mount technology, program testing, program verification, Laboratories, distributed processing, temporal logic, distributed system, safety violations, Software safety, Liveness Bugs, Engines, finite model checking, Model Checking, Logic, liveness bugs, Instruments, safety bugs, software model checkers, linear temporal logic, distributed protocol, complex concurrent e-business systems, Computer science, Computer bugs, Program Analysis, Computer errors, distributed e-business systems]
Towards Effective Defense Against Insider Attacks: The Establishment of Defender's Reputation
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
We address issues related to the establishment of defender's reputation in anomaly detection against insider attacks. We consider two types of attackers: smart insiders, which learn from historic attacks and adapt their strategies to avoid detection/punishment, and naive attackers, which blindly launch their attacks. We introduce two novel reputation-establishment algorithms for systems with solely smart insiders and systems with both smart insiders and naive attackers, respectively. Theoretical analysis and simulation results show that our reputation-establishment algorithms can significantly improve the performance of anomaly detection against insider attacks in terms of the tradeoff between detection and false positives.
[Algorithm design and analysis, Costs, smart insider attack, Change detection algorithms, anomaly detection, learning, Game theory, Surges, naive attacker, Mobile ad hoc networks, Analytical models, security of data, Intrusion detection, Performance analysis, Computer security, defender reputation establishment algorithm]
Tolerating Temporal Correlated Failures from Cyclic Dependency in High Performance Computing Systems
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Correlated failures have recently gained more attention in the research of failures in large scale systems. Recent studies have pointed out the negative effect of ignoring such failures when designing a fault tolerant scheme for large scale systems. In this paper, we explore the behaviors of temporal correlated failures arising from cyclic dependency among task nodes via an abstract model. Using this model, we find that fast failure propagation and slow recovery from failures are two dominant factors which make recovering from such failures much difficult. To efficiently stop failure propagation and shorten the total recovering time, we propose a recovery protocol called GCCTS (group-based coordinated checkpointing and task suspending) against temporal correlated failures.
[Checkpointing, checkpointing, Protocols, task suspending, distributed processing, high performance computing systems, availability, Concurrent computing, cyclic dependency, recovery protocol, Fault tolerant systems, Hardware, Large-scale systems, temporal correlated failure tolerance, Correlated failures, Availability, fault tolerant scheme, dependency, failure propagation, Application software, Helium, group-based coordinated checkpointing, High performance computing, large scale systems, fault tolerant computing]
The Feasibility of Launching Reduction of Quality (RoQ) Attacks in 802.11 Wireless Networks
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
In this paper, we discuss wireless reduction of quality (RoQ) attacks against the transmission control protocol (TCP). RoQ attacks can dramatically degrade the TCP performance with a less number of wireless jamming attacking packets, which makes them rather difficult to detect. We propose a RoQ attack model which exposes the possibility to launch a RoQ attack and illustrates attack conditions. A CTS jamming method is proposed to make it possible to launch RoQ attacks in 802.11b/g wireless networks. The wireless RoQ attacks are evaluated in both NS2 simulation environment and practical wireless networks. Experimental results demonstrate that it is possible to degrade wireless TCP throughput through RoQ attacks with undetectable low-rate attacking traffic.
[telecommunication security, IEEE 802.11 wireless network, wireless CTS jamming attack, Wireless application protocol, wireless TCP throughput, Telecommunication traffic, Throughput, Communication system security, Jamming, Computer crime, Degradation, wireless RoQ attack model, Wireless networks, Wireless Security, Traffic control, wireless reduction of quality attack, 802.11, RoQ Attacks, 802.11g networks, Access protocols, NS2 simulation environment, transmission control protocol, TCP Performance, network traffic, 802.11b networks, transport protocols, wireless LAN, telecommunication traffic]
hFT-FW: Hybrid Fault-Tolerance for Cluster-Based Stateful Firewalls
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Failures are a permanent menace for the availability of Internet services. During the last decades, numerous fault-tolerant approaches have been proposed for the wide spectrum of Internet services, including stateful firewalls. Most of these solutions adopt reactive approaches to mask failures by replicating state-changes between replicas. However, reactive replication is a resource consuming task that reduces scalability and performance: the amount of computational and bandwidth resources to propagate state-changes among replicas might be high. On the other hand, more and more commercial off-the-shelf platforms provide integrated hardware error-detection facilities. As a result, some current fault-tolerance research works aim to replace the reactive fault-handling with proactive fault-avoidance. However, pure proactive approaches are risky and they currently face serious limitations. In this work, we propose a hybrid proactive and reactive model that exploits the stateful firewall semantics to increase the overall performance of cluster-based fault-tolerant stateful firewalls. The proposed solution reduces the amount of resources involved in the reactive state-replication by means of Bayesian techniques to perform lazy replication while, at the same time, benefits from proactive fault-tolerance. Preliminary experimental results are also provided.
[cluster-based stateful firewall, fault-tolerance, bandwidth resources, error detection, lazy replication, state changes, reactive state replication, Fault tolerance, Runtime, Internet service availability, resource allocation, Web and internet services, Bandwidth, authorisation, Hardware, Large-scale systems, Web server, Availability, hybrid fault tolerance, stateful firewalls, computer networks, proactive model, Bayesian technique, high availability, reactive fault handling, computational resources, Computer errors, fault tolerant computing, Error correction, Bayes methods, Internet, distributed firewalls]
Constructing Double-Erasure HoVer Codes Using Latin Squares
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Storage applications are in urgent need of multi-erasure codes. But there is no consensus on the best coding technique. Hafner has presented a class of multi-erasure codes named HoVer codes [1]. This kind of codes has a unique data/parity layout which provides a range of implementation options that cover a large portion of the performance/efficiency trade-off space. Thus it can be applied to many scenarios by simple tuning. In this paper, we give a combinatorial representation of a family of double-erasure HoVer codes - create a mapping between this family of codes and Latin squares. We also present two families of double-erasure HoVer codes respectively based on the column-Hamiltonian Latin squares (of odd order) and a family of Latin squares of even order. Compared with the double-erasure HoVer codes presented in [1], the new codes enable greater flexibility in performance and efficiency trade-off.
[codes, even-order Latin squares, combinatorial mathematics, disc storage, Distributed computing, Concurrent computing, Reed-Solomon codes, Linear code, Fault tolerance, storage management, parity layout, Space technology, HoVer code, performance trade-off, disk storage, Educational institutions, Latin square, Application software, Computational complexity, Information technology, combinatorial representation, double-erasure HoVer code construction, one-factorization, multierasure code, column-Hamiltonian Latin squares, erasure code]
Transparent and Autonomic Rollback-Recovery in Cluster Systems
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Cluster systems provide an excellent environment to run computation hungry applications. However, due to being created using commodity components they are prone to failures. To overcome these failures we propose to use rollback-recovery, which consists of the checkpointing and recovery facilities. Checkpointing facilities have been the focus of many previous studies; however, the recovery facilities have been overlooked. This paper focuses on the requirements, concept and architecture of recovery facilities. The synthesized fault tolerant system was implemented in the GENESIS system and evaluated. The results show that the synthesized system is efficient and scalable.
[Checkpointing, checkpointing, workstation clusters, Cluster systems, recovery facilities, Application software, Information technology, Distributed computing, cluster system rollback-recovery, Concurrent computing, Fault tolerance, Operating systems, Fault tolerant systems, Rollback-recovery, Computer applications, GENESIS system, fault tolerant computing, Australia, fault tolerant system]
UDB: Using Directional Beacons for Localization in Underwater Sensor Networks
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Underwater sensor networks (UWSN) are widely used in many applications, such as oceanic resource exploration, pollution monitoring, tsunami warnings and mine reconnaissance. In UWSNs, determining the location information of each sensor node is a critical issue, because many services are based on the localization results. In this paper, we introduce a novel underwater localization approach based on directional signals, which are transmitted by an autonomous underwater vehicle (AUV). Our method utilizes directional beacons (UDB) to replace traditional omni-directional localization which provides more accurate and efficient ways to locate the sensors themselves by simple calculations. The advantage of this novel scheme is that the communications between AUV and sensors are not necessary because the AUV broadcasts signals and sensors only need to passively listen to the signals. Since the energy consumption for transmissions in underwater environments is a nontrivial factor, our localization scheme not only supports accurate positioning, but also reduces energy consumption of sensors. We evaluate our scheme by simulations. The results show that our new approach is very precise in a strap area. At the same time, we minimize the number of beacons issued from the AUV.
[AUV broadcasts, Chemical sensors, Energy consumption, Positioning, wireless sensor networks, mine reconnaissance, Underwater localization approach, Directional Beacons, Sensor phenomena and characterization, Mobility, Underwater vehicles, Temperature sensors, UDB, tsunami warnings, Acoustic sensors, Localization, directional beacons, Sensor systems and applications, acoustic communication, Underwater Sensor Networks, underwater acoustic communication, pollution monitoring, Computer science, Wireless sensor networks, underwater sensor networks, autonomous underwater vehicle, underwater vehicles, Underwater acoustics, oceanic resource exploration, sensor node]
A Constant Factor Localized Algorithm for Computing Connected Dominating Sets in Wireless Sensor Networks
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Connected dominating sets (CDSs) are probably the most common way of constructing virtual backbones for broadcasting operation in wireless sensor networks. This is because such backbones guarantee to reduce unnecessary message transmissions or flooding in the network. In this paper we propose a simple localized algorithm to construct a small-sized CDS. Considering the sensors deployed in the plane, our main idea is based on the computation of convex hulls of sensor nodes (nodes are considered points in the plane) in a localized manner and a simple coloring scheme, which produces a CDS in unit disk graphs whose size is at most 38*|MCDS| where |MCDS| is the size of a minimum CDS. To the best of our knowledge, this is a significant improvement over the best published results in the same context [5]. We also analyze grids and trees to compute the exact approximation ratios for the problem. We show that our algorithm produces an optimal CDS if the graph is a tree and in the case of grids the approximation factor is 2.
[constant factor localized algorithm, wireless sensor networks, broadcasting operation, Spine, Maximal Independent Set, Connected Dominating Set, Distributed computing, Connectors, Concurrent computing, Wireless sensor networks, connected dominating sets computation, Convex hull, Tree graphs, Broadcasting, Sensor networks, Grid computing, Approximation algorithms, Computer networks, information theory, convex hull computation]
A Simulation System for Routing Efficiency in Wireless Sensor-Actor Networks: A Case Study for Semi-automated Architecture
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Wireless networks have become increasingly popular and advances in wireless communications and electronics have enabled the development of different kind of networks such as mobile ad-hoc networks (MANETs), wireless sensor networks (WSNs) and wireless sensor-actor networks (WSANs). These networks have different kind of characteristics, therefore new protocols that fit their features should be developed. We have developed a simulation system to test MANETs, WSNs and WSANs. In this paper, we consider the performance behavior of two protocols: AODV and DSR using TwoRayGround model and Shadowing model for lattice and random topologies. We study the routing efficiency and compare the performance of two protocols for different scenarios. By computer simulations, we found that for large number of nodes when we used TwoRayGround model and random topology, the DSR protocol has a better performance. However, when the transmission rate is higher, the routing efficiency parameter is unstable.
[System testing, DSR protocol, Protocols, wireless sensor networks, TwoRayGround model, Lattices, Wireless communication, routing efficiency, performance behavior, mobile ad-hoc networks, random topology, Wireless Sensor-Actor Networks, AODV, mobile radio, AODV protocol, DSR, Computational modeling, telecommunication network topology, Routing, Ad hoc networks, Routing Efficiency, Topology, Shadow mapping, wireless sensor-actor networks, Wireless sensor networks, MANET, Shadowing model, lattice topology, routing protocols, semi automated architecture, ad hoc networks, simulation system]
Response Time Constrained Top-k Query Evaluation in Sensor Networks
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Existing solutions for top-k queries in wireless sensor networks mainly focused on energy efficiency and little attention has been paid to the response time to answer a top-k query as well as the relationship between the response time and the network lifetime. In this paper we address this issue explicitly by studying the top-k query problem in sensor networks with the response time constraint. We aim at finding an energy-efficient routing tree and devising an evaluation algorithm for top-k queries on the tree such that the network lifetime is significantly prolonged, provided that the query response time constraint is met too. To do so, we propose a novel joint optimization framework of finding a routing tree and devising a filter-based evaluation algorithm on the tree. We also conduct extensive experiments by simulation to evaluate the performance of the proposed algorithms. The experimental results showed that the joint optimization framework prolongs the network lifetime significantly under a given response time constraint.
[Energy consumption, wireless sensor networks, WSN lifetime, wireless sensor network, energy-efficient routing tree, sensor networks, Delay, Constraint optimization, optimisation, Fires, telecommunication network reliability, top-k query, Monitoring, trees (mathematics), Routing, response time constrained top-k query evaluation, Wireless sensor networks, joint optimization framework, query optimization, Query processing, response time, distributed algorithms, telecommunication network routing, Energy efficiency, Australia]
Secure RFID Identification and Authentication with Triggered Hash Chain Variants
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
In this paper, we propose two RFID identification and authentication schemes based on the previously proposed triggered hash chain scheme by Henrici and Muller. The schemes are designed to mitigate the shortcomings observed in the triggered hash chain scheme and to ensure privacy preserving identification, tag-reader mutual authentication, as well as forward-privacy in the case of RFID tags that have been compromised. The first scheme uses a challenge-response mechanism to defend against an obvious weakness of the triggered hash chain scheme. The second scheme uses an authenticated monotonic counter to defend against a session linking attack that the first scheme is vulnerable to. We compare the level of security offered by our proposed schemes against other previous schemes and find that the schemes perform well, while keeping within reasonable overheads in terms of computational, storage and communication requirements.
[telecommunication security, tag-reader mutual authentication, radiofrequency identification, RFID secure identification, triggered hash chain variants, Hash chain, Security, Counting circuits, Privacy, privacy preserving identification, RFID authentication, challenge-response mechanism, authentication schemes, authenticated monotonic counter, cryptography, Transponders, RFID tags, Secure storage, Cryptographic protocols, session-linking attack, Authentication, message authentication, communication security, Joining processes, Radiofrequency identification]
Bouncing Tracks in Sensor Networks
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Recent work in building data-centric sensor networks treats a sensor network as a pool of information. Sensor nodes generate, store, and retrieve information as both data producers and consumers. The intensive demand of data exchange within the network leads previous approaches with sensor-to-sink transmission model inefficient. In-network data storage schemes, such as geographical hash table (GHT) and double-rulings, have been accordingly proposed to structure the information storage among the network so as to facilitate the consumers to efficiently discover and retrieve data. Under those approaches, however, each sensor node needs to publish and retrieve data from different routes calculated every time, introducing unnecessary computation and communication overhead. This paper proposes anew approach that stores and queries data through bouncing tracks. Sensors are able to publish replica of generated data along their bouncing tracks and successfully retrieve data from other sensors along the same tracks, which largely simplifies data exchange and improves efficiency. The strengths of this design also include distance-bounded data retrieval. We conducted extensive simulations and the results show that this approach outperforms existing designs, including rumor routing and double-rulings, in terms of communication efficiency and cost.
[data-centric wireless sensor network, Costs, Protocols, wireless sensor networks, data exchange, sensor-to-sink transmission model, Memory, information retrieval, data storage schemes, distance-bounded data retrieval, Information retrieval, Routing, Sensor systems, geographical hash table, Degradation, electronic data interchange, Bouncing tracks, Publishing, WSN, data center storage, telecommunication network routing, Load management, bouncing tracks, Joining processes]
Efficient Data Suppression for Wireless Sensor Networks
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Due to critical resource restrictions, wireless sensor networks (WSNs) often face a trade-off between the cost of data transmission and the accuracy of event detection. By exploring the potential spatial and temporal correlations among sensory data, a WSN may intelligently select only a subset of nodes, whose data can still keep the major properties of those collected by the whole network, to transmit. Two important issues are examined in this study. First, which of those sensors should be selected? Second, how can the lifetime of the selected sensors be maximized? We propose a Singular Value Decomposition (SVD) based Sensory Data Suppression (SSS) Mechanism, which removes unnecessary data transmissions and prolong the lifetime of sensor networks. We also balance transmission duties among sensor nodes by leveraging the load balancing algorithms with both one-attribute and multi-attribute scenarios.
[Costs, Event detection, wireless sensor networks, load balancing, wireless sensor network, Data engineering, wireless sensor network lifetime, resource allocation, sensory data suppression mechanism, telecommunication network reliability, Computer networks, Data communication, singular value decomposition, Singular value decomposition, data suppression, multiattribute scheme, temporal correlation, spatial correlation, spatiotemporal phenomena, Intelligent sensors, Ocean temperature, load balancing algorithm, Computer science, Singular Value Decomposition, Wireless sensor networks, data transmission, correlation methods]
Mobility-Assisted Position Estimation in Wireless Sensor Networks
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Wireless sensor networks (WSNs) have been proposed for a multitude of location-dependent applications. To stamp the collected data and facilitate communication protocols, it is necessary to identify the location of each sensor. In this paper, we discuss the performance of a novel received signal strength indicator (RSSI) positioning scheme, which uses a generalized geometrical location algorithm to achieve an accurate estimation based on mean received signal strength measurements. In order to improve the network performance and address limitations of static WSNs position estimation, mobile sensors are utilized effectively and an attractive movement strategy with mobile elements is designed. The effectiveness of our approach is validated and compared with the traditional RSSI method by extensive simulations.
[generalized geometrical location algorithm, mobile sensor, Protocols, wireless sensor networks, location-dependent application, received signal strength indicator positioning scheme, communication protocol, Computer science, Industrial electronics, Wireless sensor networks, RSSI, USA Councils, Geometrical location, Electronics industry, Additive white noise, Computer industry, protocols, State estimation, mobility-assisted position estimation, mean received signal strength measurement, Monitoring]
Time-Based Privacy Protection for Multi-attribute Data in WSNs
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Wireless sensor networks become ubiquitous to collect people's information in many people-centric applications, such as, health care, smart space and public safety. Because any misusage of these personal data might result in the leakage of privacy, it is expected that the data requesters can only access to the data what they are entitled to read. Based on a revised hash chain technique, we proposed a novel time-based privacy protection (TPP) scheme for multi-attribute data in WSNs. In the scheme, all the personal data are divided into 2-D subspaces representing data attribute and generation time. Data in each subspace is encrypted with a sub-key before its transmission to the sink. Anyone who wants to read data attribute at a particular time must get the corresponding sub-key from the sender node. TPP can generate a sub-key for data in each subspace in an efficient manner in terms of less sub-key generation time and low memory space usage. The simulation results show that the schemes can be applied to the resource limited WSNs efficiently.
[Pervasive computing, Data privacy, wireless sensor networks, Medical services, hash chain technique, cryptography, ubiquitous computing, Distributed computing, Concurrent computing, Wireless sensor networks, Space technology, multiattribute data, time-based privacy protection, Computer networks, data privacy, Health and safety, Protection]
Using Cable-Based Mobile Sensors to Assist Environment Surveillance
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
In wireless sensor networks, mobile sensors are often employed for enhancing the sensing coverage and detection accuracy. Current approaches assume mobile sensors with the capability of arbitrary movement. The usage of such sensors with unlimited mobility, however, requires complicated sensor manufactures and high intelligence of movement which are practically unrealistic in many practical applications. We investigate the usage of cable-based mobile sensors which move along pre-deployed cables to accomplish sensing tasks at different positions. A target area is said to be reachable, if for any point in this area, at least one mobile sensor can move along the cable and achieve coverage to the point within a specified delay bound. We propose to achieve k reachability for the sensing field with minimum mobile sensors along the cable. Further, during special events, mobile sensors need to move and help surveillance. We need adjust the positions of the rest of mobile sensors accordingly to balance the reachability within the area. We prove the NP-hardness of the targeted problems and give heuristic approaches. Through comprehensive simulations, we evaluate the performance of this design and show its effectiveness.
[cable-based mobile sensor, mobile sensor, mobile radio, wireless sensor networks, wireless sensor network, Sensor systems, Osmosis, Intelligent sensors, Delay, Cables, NP-hardness, Wireless sensor networks, Surveillance, telecommunication cables, Manufacturing, surveillance, reachability, cable, Monitoring, Gas detectors, computational complexity, environment surveillance, heuristic approach]
Fault-Tolerant Algoritms for Detecting Event Regions in Wireless Sensor Networks Using Statistical Hypothesis Test
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Detecting event regions in a monitored environment is a canonical task of wireless sensor networks (WSNs). It is a hard problem because sensor nodes are prone to failures and have scarce energy. In this paper, we seek distributed and localized algorithms for fault-tolerant event region detection. Most existing algorithms only assume that events are spatially correlated, but we argue that events are usually both spatially and temporally correlated. By examining the temporal correlation of sensor measurements, we propose two detection algorithms by applying statistical hypothesis test (SHT). Our analyses show that SHT-based algorithm is more accurate in detecting event regions. Moreover, it is more energy efficient since it gets rid of frequent measurement exchanges. In order to improve the capability of fault recognition, we extend SHT-based algorithm by examining both spatial and temporal correlations of sensor measurements, and our analyses show that extended SHT-based algorithm can recognize almost all faults when sensor network is densely deployed.
[Algorithm design and analysis, distributed-localized algorithms, Event detection, wireless sensor networks, statistical distributions, Condition monitoring, Fault tolerance, sensor nodes, sensor measurements, Temporal correlation examining, Statistical hypothesis test, statistical hypothesis test, statistical testing, SHT, Testing, detection algorithms, fault tolerance, Energy measurement, fault location, Event region detection, spatiotemporal phenomena, spatially-temporally correlated events, Wireless sensor network, Wireless sensor networks, Fault detection, Energy efficiency, fault-tolerant event region detection, fault recognition capability, Detection algorithms]
Integrating RFID with Wireless Sensor Networks for Inhabitant, Environment and Health Monitoring
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Radio frequency identification (RFID) and wireless sensor networks (WSN) have been popular in industrial and academic field due to their ability in identity identification and data transmission respectively. Both RFID and wireless sensor networks have undergone huge development separately. In this paper, it has been discussed how helpful and effective it will be when RFID and WSN have been combined together. It also discusses about some applications of this integrated technology in the field of precious animal and patient health monitoring where real time information is of utmost importance. A system structure of the integrated RFID and WSN has been introduced and the simulation results show that the new system outperforms traditional RFID monitory system in terms of the cost of deployment, updating delay and tag capacity requirement.
[Costs, Supply chain management, Tracking, radiofrequency identification, wireless sensor networks, Computerized monitoring, RFID integration, identity identification, wireless sensor network, RFID, biomedical communication, radio frequency identification, RFID tags, patient monitoring, animal health monitoring, Wireless sensor networks, Patient monitoring, routing, Animals, patient health monitoring, data transmission, Data communication, Radiofrequency identification]
A Distributed Area-Based Guiding Navigation Protocol for Wireless Sensor Networks
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
One of the major applications of wireless sensor networks is guiding navigation service whose goal is to find a way to guide moving objects across a hazardous region covered by sensors. Sensor networks maintain safe paths by which the moving objects can be guided safely to exits. In this paper, we propose a distributed guiding navigation protocol for constructing area-to-area optimal guiding paths that do not traverse through the hazardous areas and to guide the moving objects to escape from this area safely and quickly. Our protocol also allows multiple exits and multiple emergency events in the sensor networks. Moreover, we propose a load-dispersion algorithm including an additional AP layer for moving objects registration and path assignment in multi-exits scenario. Hence, the moving objects can be properly dispersed to multiple paths leading to multiple exits to avoid congestion in the same exit. Simulation results show that our protocol can guide moving objects along shorter paths to reach nearest exits and operate in different scenarios or environments.
[Navigation, wireless sensor networks, Wireless application protocol, distributed area-based guiding navigation protocol, Indoor environments, Application software, load-dispersion algorithm, Computer science, Hazardous areas, Wireless sensor networks, moving objects registration, AP layer, Manufacturing, path assignment, protocols, Distributed algorithms, Monitoring, area-to-area optimal guiding paths construction]
CyclicMAC: A Cyclic MAC Scheduling Scheme for Gathering Data from Heterogeneous Sensors
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Many data gathering systems require the deployment of heterogeneous sensors to detect various aspects of the target and the use of wireless networks to transmit sensed data to the control center. The data rates of the heterogeneous sensors may range from a few bytes per minute to megabytes per second. To support the transmissions of such diverse rates of data through the wireless network, we propose a novel MAC scheduling scheme, CyclicMAC. CyclicMAC assigns each sensor a cyclic MAC schedule according to the data rates of its sensed as well as relaying data, without causing any channel contention. As far as we know, CyclicMAC is the first to address at the same time all three design issues in MAC scheduling: heterogeneous data rate, end-to-end latency, and storage for scheduling table, for multihop wireless sensor networks.
[wireless sensor networks, TDMA, CyclicMAC scheduling scheme, Sensor phenomena and characterization, Control systems, Scheduling, Sensor systems, access protocols, cyclic MAC schedule, Delay, Wireless sensor networks, Time division multiple access, heterogeneous sensors, target detection, time division multiple access, data gathering, distance constrained task set, data gathering system, Acoustic sensors, scheduling, Sampling methods, multihop wireless sensor network, Monitoring]
Pipelined Parallel AC-Based Approach for Multi-String Matching
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
New applications such as real-time packet processing require high-speed string matcher, and the number of strings in pattern store is increasing to tens of thousands, which requires a memory efficient solution. In this paper, a pipelined parallel approach for hardware implementation of Aho-Corasick (AC) algorithm for multiple strings matching called P2-AC is presented. P2-AC organizes the transition rules in multiple stages and processes in pipeline manner, which significantly simplifies the DFA state transition graph into a character tree that only contains forwarding edges. In each stage, parallel SRAMs are used to store and access transition rules of DFA in memory. Transition rules can be efficiently stored and accessed in one cycle. The memory cost is less than 47% of the best known AC-based methods. P2-AC supports incremental update and scales well with the increasing number of strings. By employing two-port SRAMs, the throughput of P2-AC is doubled with little control overhead.
[Real time systems, Costs, finite automata, pipelined processing, Pipelines, deterministic finite automata state transition rule graph, Tree graphs, deterministic automata, Intrusion detection, Hardware, pipelined parallel Aho-Corasick algorithm, parallel algorithms, character tree, trees (mathematics), Doped fiber amplifiers, Application software, real-time packet processing, parallel two-port SRAM, multistring matching algorithm, Computer science, SRAM chips, memory-efficient solution, network intrusion detection, parallel SRAMs, deterministic finite automata, pipeline processing, string matching, Pattern matching, computational complexity, hardware implementation]
Automatic Implementation of Multi-partitioning Using Global Tiling
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Strategies for partitioning an applications data and computation play fundamental role in determining the efficiency of parallelization. This paper describes a sophisticated strategy for partitioning data and computation known as multi-partitioning, which can support the best parallelization for some applications such as the line sweep computations. However, the implementation of multi-partitioning is very difficult and, as we know, there is none automatic parallelizing compiler supports such partitioning strategy. Though the dHPF compiler implemented multi-partitioning as a special extension for block style HPF partitioning, it still needs the programmers participation to analyze the application and decide the data distribution scheme. In this paper, we present a global tiling transformation algorithm and a tile-to-processors mapping strategy called hyper-diagonal modular mapping, to implement the multi-partitioning strategy. The experimentation with NPB2.3-serial SP shows that the code generated by the compiler achieves scalable performance.
[parallelising compilers, Partial differential equations, Laboratories, dHPF compiler, parallelizing compiler, hyper-diagonal modular mapping, Partitioning algorithms, Distributed computing, Concurrent computing, global tiling transformation algorithm, Program processors, code generation, Tiles, Computer applications, Computer architecture, data partitioning, data distribution scheme, automatic multipartitioning, Navier-Stokes equations, tile-to-processor mapping strategy]
Clustered Decoupled Software Pipelining on Commodity CMP
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
With the prevalence of chip multiprocessor (CMP) on server and client computers, it becomes an important issue to use the multicores to speedup existing sequential programs. Decoupled software pipelining (DSWP) is a recent proposed technique that extracts non-speculative threads from sequential programs for higher performance. However, this technique is not effective on commodity CMP architecture, because the inter-thread communication and synchronization overhead often offset the benefit from the parallelization. To reduce the overhead without modification to CMP architecture, this paper presents a clustered DSWP (CDSWP) technique that is an extension to DSWP. By communicating a dependent data set instead of a single dependent data, this technique transforms sequential program into a clustered thread pipeline. The meaning of "clustered" is that some dependent data items are clustered together as a communication unit. The advantage of this technique is that it can eliminate false sharing and reduce the average cache latency, and thus the overhead is reduced greatly. According to the preliminary experiments on some commodity CMP architectures, we have achieved loop speedup ranging from 16% to 58% on some SPEC2000 benchmark programs.
[client-server systems, multiprocessing systems, Commodity CMP, decoupled software pipelining, interthread communication, Communication Overhead, Data mining, Thread Pipelining, Yarn, Distributed computing, Pipeline processing, Concurrent computing, Information science, synchronization overhead, Computer architecture, Parallel processing, Software systems, chip multiprocessor, TLP, Hardware, pipeline processing, Clustered Data Size]
A Quantitative Study of the On-Chip Network and Memory Hierarchy Design for Many-Core Processor
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
In this paper, we will study the on-chip network and memory hierarchy design of the Godson-T - a homogeneous many-core processor. Godson-T has 64 cores (with private L1 cache), and 16 global L2 cache banks. All these on-chip units are connected by a 2D 8  8 mesh network. Our study reveals that:(a) Global on-chip L2 cache can effectively alleviate the memory pressure caused by the data-thirsty on-chip computing engines. However, its potential is still limited by both the off-chip and the in-chip bandwidth, especially when increasing the number of active threads.(b) On-chip traffic congestion is largely caused by the intensive memory access requests issued from the on-chipcores. Therefore, the design of the on-chip network must consider the available performance of the datapath that connects the processor to the main memory. (c) In theory, different applications have different communication patterns (Berkeley's view). However, the application's runtime communication pattern is only determined by the design of the underlying memory hierarchy and on-chip interconnection. These conclusions are generally applicable to a wide variety of many-core processors with similar design.
[Process design, data-thirsty on-chip computing engines, on-chip network, homogeneous many-core processor, cache, Telecommunication traffic, microprocessor chips, Godson-T, memory hierarchy, cache storage, Yarn, Engines, Concurrent computing, Mesh networks, Runtime, on-chip interconnection, global on-chip L2 cache, many-core processor, Network-on-a-chip, Bandwidth, memory hierarchy design, communication pattern, on-chip traffic congestion, System-on-a-chip]
Experimental Study of Thread Scheduling Libraries on Degraded CPU
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
In this paper, we compare four libraries for efficiently running threads when the performance of a CPU cores are degraded. First, we are interested by 'brute performance' of the libraries when all the CPU resources are available and second, we would like to measure how the scheduling strategy impacts also the memory management in order to revisit, in the future, scheduling strategies when we artificially degrade the performance in advance. It is well known that work stealing, when done in an anarchic way, may lead to poor cache performance. It is also known that the migration of threads may induce penalties if they are too frequent. We study, at the processor level, the memory management in order to find trade-offs between active thread number that an application should start and the memory hierarchy. Our implementations, coded with the different libraries, were compared against a Pthread one where the threads are scheduled by the Linux kernel and not by a specific tool. Our experimental results indicate that scheduler may perfectly balance loads over cores but execution time is impacted in a negative way. We also put forward a relation between the L1 cache misses, the number of steals and the execution time that will allow to focus on specific points to improve 'work stealing' schedulers in the future.
[work stealing, degraded CPU, Linux kernel, work stealing schedulers, brute performance, Yarn, software libraries, Degradation, storage management, memory management, heterogeneous computing, thread scheduling libraries, Parallel processing, scheduling, Libraries, operating system kernels, Job shop scheduling, Multicore processing, multi-threading, Thread scheduling, Processor scheduling, Linux, Memory management, Central Processing Unit, Resource management]
Quarc: A Novel Network-On-Chip Architecture
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
This paper introduces the Quarc NoC, a novel NoC architecture inspired by the Spidergon NoC. The Quarc scheme significantly outperforms the Spidergon NoC through balancing the traffic which is the result of the modifications applied to the topology and the routing elements.The proposed architecture is highly efficient in performing collective communication operations including broadcast and multicast. We present the topology, routing discipline and switch architecture for the Quarc NoC and demonstrate the performance with the results obtained from discrete event simulations.
[network-on-chip, Quarc, Spidergon NoC, NoC, network routing, Switches, Telecommunication traffic, Quarc network-on-chip architecture, network traffic balance, Routing, Spidergon, network topology, network routing element, Communication switching, Delay, broadcast communication operation, network topology element, Network topology, Network-on-a-chip, Collective communication, Computer architecture, Broadcasting, Computer networks, multicast communication operation]
An Efficient Switching Technique for NoCs with Reduced Buffer Requirements
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Networks on chip (NoCs) communicate the components located inside a chip. Overall system performance depends on NoC performance, that is affected by several factors. One of them is the network clock frequency, imposed by the critical path delay. Recent works show that switch critical path includes buffer control logic. Consequently, by removing switch buffers, switch frequency can be doubled. In this paper, we exploit this idea, proposing a new switching technique for NoCs which requires a reduced amount of storage at the switches. It is based on replacing switch port buffers by single latches. By doing so, network cycle can be reduced, which reduces packet latency. On the other hand, power and area consumption requirements can be reduced. However, since there are no buffers at the switch ports, packets can not be stopped. Stopped packets due to contention are dropped and reinjected from their senders via negative acknowledgments. Packet dropping is strongly reduced by exploiting NoCs wiring capability.
[networks on chip, Multiprocessor interconnection networks, Switches, Delay, Wiring, critical path delay, Wires, Logic, Buffers, Packet switching, network-on-chip, buffer control logic, performance evaluation, Space Division Multiplexing, network clock frequency, switching technique, Switching, network cycle, Networks on-chip, Network-on-a-chip, Frequency, Power efficiency, reduced buffer requirements, packet dropping, Clocks]
Processor, Assembler, and Compiler Design Education Using an FPGA
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
This paper reports the design of two courses, "embedded hardware'' and "embedded software" offered in 2008 spring semester at Hiroshima University. These courses use 16-bit processor TINYCPU, cross assembler TINYASM, and cross compiler TINYC. They are designed very simple and compact: The total number of lines of the source code is only 427. Thus, students can understandthe entire design easily, and can learn the basics of computer and embedded system, including processor architecture, assembler and compiler design, assembler programming in a unified way by experiment.
[Assembly systems, field programmable gate arrays, assembler design education, FPGA, processor architecture, compiler design education, program compilers, Embedded software, Program processors, Hiroshima University, Embedded system, computer architecture, embedded systems, cross assembler TINYASM, Computer architecture, Hardware, cross compiler TINYC, Springs, Embedded computing, computer science education, processor design education, program assemblers, microprocessor chips, 16-bit processor TINYCPU, Programming profession, educational courses, Field programmable gate arrays]
Secure Embedded Systems: The Threat of Reverse Engineering
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Companies releasing newly designed embedded products typically recoup the cost of development through initial sales, and thus are unlikely to welcome early competition based around rapid reverse engineering of their products. By contrast, competitors able to shorten time-to-market though reverse engineering will gain design cost and market share advantages. Reverse engineering for nefarious purposes appears to be commonplace, and has significant cost impact on industry sales and profitability. In the Embedded Systems MSc programme at Nanyang Technological University, we are aiming to raise awareness of the unique security issues related to the reverse engineering of embedded systems. This effort is largely through devoting 50% of the secure embedded systems course, ES6190 to reverse engineering (the remainder to traditional security concerns). This paper covers the reverse engineering problem scope, and our approach to raising awareness through the secure embedded systems course. A classification of hardware reverse engineering steps and mitigations is also presented for the first time, with an overview of a reverse engineering curriculum. Since the quantity of published literature related to the reverse-engineering of embedded systems lies somewhere between scarce and nonexistent, this paper presents a full overview of the topic before descussing educational aspects related to this.
[computer science education, Costs, Profitability, Reverse engineering, secure embedded systems course, reverse engineering, Product design, embedded security, Security, Design engineering, security of data, design reuse, Embedded system, educational courses, embedded systems, User interfaces, Marketing and sales, electronic engineering education, Manufacturing]
Real-Time Enhancements for Embedded Linux
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
With the popularity of using Linux for embedded systems, its real-time performance is increasingly becoming an importance issue for applications that require short latency and task execution predictability as commonly encountered in many embedded systems. This paper presents a survey of the approaches used by commercial vendors and open source community to enhance the real-time performance of the Linux kernel. It outlines some of the main factors that affect the real-time performance of the Linux kernel and describes the two general approaches that are used to improve the real-time performance of the kernel, follows by a comparative study of the various real-time enhanced versions of the Linux that are created for embedded systems.
[Real time systems, Embedded computing, operating system kernels, real-time enhancements, Job shop scheduling, Linux kernel, Switches, embedded Linux, Delay, Linux, Operating systems, real-time, Embedded system, embedded systems, linux, Timing, Kernel, embedded]
A Short Course on Implementing FPGA Based Digital Systems
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
The rapid advances in the FPGA technology along with high-levels of system integration have made FPGAs the preferred platform not only for rapid prototyping but also for production of digital embedded systems. This paper presents the experience of a team of instructors in designing and conducting a short course on implementing FPGA-based digital systems for industry professionals. The selection of topics, course organization, the issues involved in designing effective hands-on exercises and the response of the students to the course are discussed.
[Production systems, Electronic design automation and methodology, Digital systems, field programmable gate arrays, rapid prototyping, FPGA based digital systems, FPGA design, professional aspects, Course design, system integration, digital embedded systems, Embedded system, Programmable logic devices, Employment, Prototypes, educational courses, Digital arithmetic, electronic engineering education, Field programmable gate arrays, Clocks]
An Intensive Curriculum on Embedded Software Design for Vocational-Purpose Training
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
This paper presents the design and implementation of an intensive curriculum on embedded software design for university or college graduates of engineering background, who are seeking job opportunities in developing software for embedded system products. It is part of a government-conducted project intending to resolve the shortage of embedded software engineers in SoC (System-on-Chip) and related industry. In order to balance among the required expertise in developing embedded software, background diversity of the enrolled students, and the 300-hour limit on total course hours imposed by the project, the curriculum is designed towards delivering ready-to-use skills and aimed at establishing a solid common ground as required by embedded software engineers. The curriculum is composed of 9 courses which are divided into foundation modules, core modules, and practicing modules. More than 75% of the students were employed to work in embedded software and related industry within 6 months after completing this curriculum. This curriculum, although it has its limitation, demonstrates an effective alternative to supplying industry with trained manpower of embedded software.
[computer science education, embedded software design, embedded software engineers, SoC, Educational institutions, Job design, ready-to-use skills, vocational training, Embedded software, Design engineering, Software design, intensive curriculum, embedded software, Embedded system, educational courses, embedded systems, systems analysis, vocational curriculum, Vocational training, Computer industry, Software systems, Systems engineering and theory, curriculum design, vocational-purpose training]
The Organization of Intel Cup Undergraduate Embedded System Electronic Design Contest
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
The Intel Cup Undergraduate Embedded System Electronic Design Contest is an embedded system design invitational contest in China. This contest is held every two years since 2002. The contest focuses on the embedded system design and implementation. And it gives undergraduates a chance to show their creativity and learn teamwork. The influence of the contest is keeping increasing. Now it is an international invitational contest. And some universities from the countries which are advanced in computing begin to take part in the contest. In this paper we describe the experiences in such a contest for embedded system.
[Embedded computing, computer science education, Educational products, Microcomputers, Educational institutions, Intel Cup Undergraduate Embedded System Electronic Design Contest, Reduced instruction set computing, Microprocessors, Embedded system, international invitational contest, student creativity, embedded systems, Collaborative work, student teamwork, Hardware, electronic engineering education, Computer science education]
VIP: A Flexible Virtual Integrated Platform for EVM Emulation
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
EVM (evaluation module) has become an important and useful tool both in developing embedded systems and in teaching course modules which cover embedded system programming. However, an EVM with the configuration suitable for the diversity in embedded system programming is usually quite expensive such that setting up a laboratory for students to practice programming embedded system sums a large amount of funding. This paper presents the development of VIP (virtual integrated platform) to provide flexibility in EVM emulation. VIP has notable features of flexibility in configuration, dynamic loading of device emulation modules, and visual device emulation. It integrates the emulation of configured devices at runtime to build an emulator of the target EVM. The development of VIP is composed of a framework to emulate a bus-structured platform as the base EVM and the device emulation modules connected to the bus, which are loaded at run time. It therefore has the advantages of not only its capability of flexible and extensible configuration and dynamically loading of device modules, but also the capability of catching up with the changes to new platforms or new devices by replacing device modules, in addition to being a low cost educational tool.
[Costs, Laboratories, virtual integrated platform, Emulator, Distributed computing, system development tool, Concurrent computing, bus-structured platform, Runtime, Virtual Integrated Platform, virtual platform, Emulation, Embedded system, Education, EVM emulation, embedded systems, Hardware, embedded system programming, software tools, computer science education, device emulation modules, low cost educational tool, visual device emulation, emulation, course modules, Programming profession, embedded software, virtual machines, evaluation module, computer aided instruction]
Adapting Experiments of Embedded System Curriculum Designed Based on Embedded IA to Atom
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
As the rapidly development of embedded systems, it is a challenge for universities to fill the gap between education and industry. In this paper, we introduce the experiment designed for embedded system curriculum based on embedded IA first, and then adapt the design to the new promising processor-Atom - developed by Intel. At last, we will share our experiences in the experiment teaching of embedded system.
[IEEE news, Atom processor, education, Educational technology, Educational institutions, microprocessor chips, Mobile handsets, curriculum, Washing machines, Intel, Atom, Computer science, embedded system curriculum, Partial response channels, Embedded system, Electronics industry, embedded systems, Computer science education, EIA]
A Practical Implementation Course of Operating Systems: Curriculum Design and Teaching Experiences
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
The embedded software engineers are highly demanded in recent several years in order to support fast development of SoCs and embedded systems. These engineers need both strong hardware/software knowledge and hands-on experiences of system-level software. Unfortunately, the practical training of the system software development such as the OS design and implementation is often insufficient for computer science students in Taiwan recently. To minimize the gap between theory and practical implementation, a practical implementation course of OSs is thus developed in National Chiao Tung University. The course provides students trainings on the design details of modern OSs and comprehensive hands-on practices on OS implementation. In this paper, the curriculum and hands-on lab design, the teaching experiences and student feedbacks for the trial run are presented.
[Knowledge engineering, embedded software engineering, SoC, teaching, Embedded software, Operating systems, Education, Embedded system, computer based training, embedded systems, Hardware, software engineering, System software, system software development training, curriculum design, computer science education, Teaching Experiences, Operating Systems, Computer science, hardware/software knowledge, computer science student, educational courses, system-level software, Systems engineering and theory, Software systems, operating systems (computers), Curriculum, teaching experience, operating system course]
Project Based Learning Curriculum in Microelectronics Engineering
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
In this paper, the student-centered pedagogical program used in the School of Electrical Engineering with a case study illustrating and describing the project-based curriculum in microelectronics is described. A case study project also described in the paper shows that the project-based learning method engages students in a deeper learning where not only emphasis is given to the final product, but also the process to get there.
[Educational programs, student-centered pedagogical program, Industrial relations, integrated circuits, Curriculum Design, Educational institutions, school of electrical engineering, Mathematics, teaching, Microelectronics, Electronic mail, Engineering education, Project-Based Learning, Learning systems, educational courses, microelectronics engineering, electronic engineering education, Logic, project based learning curriculum, Power engineering and energy]
Delaunay State Management for Large-Scale Networked Virtual Environments
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Peer-to-Peer (P2P) networks have been proposed as one promising approach to provide better scalability for Networked Virtual Environment (NVE) systems, but P2P-NVE also increases the probability of cheating by allowing users to manage the states of objects. In this paper, we propose Delaunay State Management (DSM), a P2P-NVE state management scheme that divides the whole virtual world into many triangular regions by Delaunay triangulation. In DSM, each region is managed by three super-peers, whose collective decisions determine how states will change. Assuming that at most one of the three super-peers is malicious, effective anti-cheating can be provided. Additionally, we also describe how DSM provides the essential features for a state management system, such as consistency, load-balancing, and fault-tolerance. Finally, we discuss some of DSM's potential applications.
[virtual reality, Virtual environment, peer-to-peer computing, delaunay, Peer to peer computing, peer-to-peer networks, fault-tolerance, load-balancing, large-scale networked virtual environments, Conference management, mesh generation, mmog, Environmental management, P2P networks, peer-to-peer, Network servers, Fault tolerance, Delaunay state management, Engineering management, Second Life, Large-scale systems, Computer network management, nve, anti-cheating]
Plug: Virtual Worlds for Millions of People
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
We propose the design of Plug, an application to find and keep contacts with friends within many inter-connected 3D virtual worlds. Users use an instant messenger (IM)-like interface to converse with friends and find new contacts, using a virtual representation of the user's self. It consists of three parts: a plug is an automatous agent / avatar situated at a user's computer that reflects and mimics its owner's behaviors and interests; a plugspace is a virtual environment that can be inter-connected, with scales ranging from a room to an entire virtual universe; and plugtalk is a set of packet formats and protocols that allow plugspaces to inter-connect, and individual plugs to navigate. Plug utilizes standards whenever possible, and is designed to be scalable, extensible, and customizable for various uses such as distance learning, virtual shopping, or online gaming. By combining the looks of 3D virtual worlds and the accessibility of IMs, we envision Plug as a step towards common virtual world experiences sharable by all Internet users.
[virtual reality, Avatars, Internet users, Plug, HTML, virtual worlds, virtual universe, plugspace, instant messenger, automatous agent, Space technology, groupware, plugtalk, Hardware, Second Life, MMOGs, virtual environments, protocols, Web server, Plugs, Virtual environment, virtual representation, Access protocols, millions of people, avatar, peer-to-peer, 3D streaming, packet formats, interconnected 3D virtual worlds, Internet]
NL-DHT: A Non-uniform Locality Sensitive DHT Architecture for Massively Multi-user Virtual Environment Applications
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
DHT networks offer a scalable structure for use in massively multi-user virtual environments (MMVEs). However, an issue with DHT structures is their use of uniform location-independent ID assignment. This conflicts with the locality-sensitive non-uniform ID assignment needed to achieve efficient latency-aware routing in MMVE applications. Our proposed solution is to use a modified version of the Hilbert space-filling curve in order to map users locations from three dimensional VE space into single dimensional DHT ID space in the best locality preserving way. The need for such modified Hilbert curve is due to the fact that users are not typically homogeneously distributed in MMVEs while Hilbert curves fill space in a uniform manner. Generation of, and mapping to, an entire Hilbert curve can be very expensive in terms of computational complexity. Thus we present a fast dynamic mapping of MMVE locations to the modified Hilbert curve to reduce the mapping complexity.
[Virtual environment, Buildings, uniform location-independent ID assignment, Massively Multi User Virtual Environments, Telecommunication traffic, massive multiuser virtual environment application, Hilbert spaces, International collaboration, nonuniform locality sensitive DHT architecture, Computational complexity, Information technology, Hilbert curve mapping, DHT, latency-aware routing, Hilbert space-filling curve, distributed hash table, telecommunication network routing, Computer architecture, file organisation, Hilbert space, curve fitting, Internet, IP networks, computational complexity]
Towards Benchmarking of Structured Peer-to-Peer Overlays for Network Virtual Environments
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Network virtual environments (NVE) are an evolving trend combining millions of users in an interactive community. A distributed NVE platform promises to lower the administration costs and to benefit from research done in the peer-to-peer (p2p) domain. In order to reuse existing mature p2p overlays for NVEs, a comparative evaluation has to be done in the same environment (e.g. resources of peers, peer behavior, churn, etc.), using appropriate test cases (scenarios) and observing relevant performance metrics. In this paper we present a benchmarking approach for p2p overlays in the context of NVEs. We define related quality attributes, scenarios, and metrics and use them to evaluate Chord and Kademlia as most popular p2p overlays and assess their suitability to NVE.
[Measurement, quality metric, benchmarking, performance metric, Network servers, benchmarking approach, groupware, Benchmark testing, structured peer-to-peer overlay network, Robustness, Large-scale systems, Kalman filters, Chord, quality attribute, Virtual environment, peer-to-peer computing, Peer to peer computing, Routing, computer network performance evaluation, distributed network virtual environment platform, peer-to-peer, Graphics, interactive community, Kademlia, benchmark testing, NVE]
CSLive: A Live Streaming Overlay with Cooperative Swarming
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Peer-to-peer (P2P) live streaming systems gain a lot of attention recently. Among the various architectures, the gossip-based approach holds great potential and has been used in real systems. In a gossip-based P2P streaming system, peer nodes are grouped together to exchange pieces of the live stream with each other. Maintaining the group and pulling the streaming data from other peers incur nontrivial overhead. This paper proposes CSLive, a P2P streaming system which gathers nodes that have good network proximity into clusters and lets nodes in the same cluster cooperate with each other to pull the stream. Each node in the cluster is responsible for pulling a subset of the stream, while pushing the subset to other nodes in the cluster. With the cooperative operations, a node can receive most streaming data from other nodes in the same cluster, which have good communication quality. The proposed system is evaluated with simulation.
[Stability, peer-to-peer computing, Peer to peer computing, Information retrieval, peer-to-peer live streaming systems, P2P, Computer science, cooperative swarming, Embedded system, Communication industry, CSLive, Computer architecture, Bandwidth, Computer industry, Internet, cooperative systems, Live streaming]
GP3 - A Distributed Grid-Based Spatial Index Infrastructure for Massive Multiuser Virtual Environments
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Massive Multiuser Virtual Environments (MMVEs) and especially the idea of a ''3D Web'' as a combination of a MMVE and today's WWW currently attracts a lot of attention. The realization of such a vision on a global scale though poses severe technical challenges to the underlying network infrastructure. It is generally accepted that such a global scale scenario can only be realized in a distributed fashion. The HyperVerse project aims at the provision of a federated global scalable infrastructure for such a ''3D Web'' scenario. We propose a two-tier Peer-To-Peer infrastructure that combines a loosely structured overlay network of user clients with a highly-structured overlay network of reliable so-called public servers constituting the backbone of our architecture. This paper presents the Grid-based plane Partitioning Protocol (GP3), a structured peer-to-peer overlay network for the interconnection of the public servers that realizes a spatial index in order to allow fast location based queries.
[Protocols, virtual reality, Avatars, Scalability, Spine, grid computing, visual databases, loosely structured overlay network, World Wide Web, federated global scalable infrastructure, Spatial Index, Spatial indexes, 3D Web, Network servers, database indexing, protocols, two-tier peer-to-peer infrastructure, client-server systems, public server, Virtual environment, peer-to-peer computing, Peer to peer computing, grid-based plane partitioning protocol, location-based query, GP3, MMVE, distributed grid-based spatial index infrastructure, massive multiuser virtual environment, Internet, hyperverse project, P2P Overlay]
GROUP: Dual-Overlay State Management for P2P NVE
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Peer-to-peer (P2P) architectures have recently become a popular design choice to build scalable Networked Virtual Environments (NVEs). While P2P architectures offer better scalability than server-based architectures, efficient distribution and management of avatar and object states remains a highly challenging issue. In this paper, we propose GROUP, a fully-distributed P2P architecture for NVEs that addresses this issue by combining a structured P2P overlay, used for object state management, with a Voronoi-based overlay, used for avatar state and group membership management. The resulting dual overlay architecture enables efficient and fully distributed management of state updates for P2P-based NVEs.
[Avatars, Scalability, data distribution, Conference management, computational geometry, interest management, state management, Environmental management, Networked virtual environments, Network servers, Engineering management, distributed hash tables, fully-distributed peer-to-peer architecture, Computer architecture, group membership management, peer-to-peer networked virtual environment, avatars, Voronoi-based overlay, voronoi diagram, Virtual environment, peer-to-peer computing, avatar management, object state management, Computer science, peer-to-peer systems, Computer network management, dual-overlay state management]
Textures in Second Life: Measurement and Analysis
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
We collected packet traces from second life client sessions and analyzed the packet contents. We observed that textures constitute a majority of the network traffic. We further characterized the textures from three selected regions in second life in terms of their size and spatial distributions. We found that textures in these regions exhibit a different size distribution from files on a file system or documents on the Web. We also verified the intuition that texture objects are spatially non-uniformly distributed. Surprisingly, we found that the selected second life regions can contain up to hundreds of megabytes of textures, and there exist locations in these regions that encompass a large portion of these textures within their area-of-interest. Our work motivates the need to manage textures carefully and efficiently in the design of networked virtual environments such as second life, and hints at the amount of storage and bandwidth required at a peer if peer-to-peer techniques are applied for texture caching. Our traces are useful for simulation studies and can lead to a model to generate realistic workload for networked virtual environments.
[Measurement, virtual reality, Telecommunication traffic, peer-to-peer technique, cache storage, statistical distributions, Environmental management, spatial distribution, Textures, File systems, texture caching, Electric variables measurement, Bandwidth, networked virtual environment, Second Life, size distribution, Networked Virtual Environments, Virtual environment, peer-to-peer computing, Peer to peer computing, texture object, image texture, Computer science, network traffic, Layout, telecommunication traffic, second life]
Peer-to-Peer Based Version Control
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Many software projects are developed by globally distributed teams. The nature of the peer-to-peer paradigm fits to such an application scenario. However, existing tools to support developer require a central instance, like it is the case in version control, which is crucial for software development. This paper address the challenges version control faces in a purely peer-to-peer environment and presents a peer-to-peer based version control system we developed. We will evolve this basic approach, adding more features like change-set version control.
[project management, peer-to-peer computing, software development, Peer to peer computing, Scalability, peer-to-peer based version control, Communication system control, vcs, Programming, Control systems, Application software, Delay, Centralized control, peer-to-peer, p2p, Robust control, configuration management, change-set version control, Distributed control, software engineering, software projects]
Query Management in a Sensor Environment
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Traditional sensor network deployments consisted of fixed infrastructures and were relatively small in size. More and more, we see the deployment of ad-hoc sensor networks with heterogeneous devices on a larger scale, posing new challenges for device management and query processing. In this paper, we present our design and prototype implementation of XSense, an architecture supporting metadata and query services for an underlying large scale dynamic P2P sensor network. We cluster sensor devices into manageable groupings to optimise the query process and automatically locate appropriate clusters based on keyword abstraction from queries. We present experimental analysis to show the benefits of our approach and demonstrate improved query performance and scalability.
[Biometrics, cluster, metadata, wireless sensor networks, keyword abstraction, query performance, Conference management, sensor environment, Sensor systems, Environmental management, query processing, Prototypes, Motion pictures, query services, Large-scale systems, Performance analysis, query management, XSense, peer-to-peer computing, ad hoc sensor networks, query, sensor network deployments, sensor device management, P2P, Query processing, peer-to-peer system, dynamic P2P sensor network, sensor, Biosensors, ad hoc networks]
Enhancing the Performance of Locating Data in Chord-Based P2P Systems
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
In peer-to-peer (P2P) networks, how to efficiently locate data on distributed hash tables (DHTs) is challenging and has attracted much research attention in recent years. Two measurements are usually considered when discussing the efficient location of data items: the path length (the hop count for resolving a lookup request on the overlay network)and the latency (the actual time period between the issuing and completion of a lookup). In this paper, we propose two protocols based on Chord which is well-known and basic for locating data on DHTs. The first approach, enhanced bidirectional chord (EB-Chord), keeps the finger tables on two directions and uses a heuristic to select the next hop to achieve a short path length. By combining the idea of the Lookup-Parasitic Random Sampling (LPRS), the second approach, enhanced bidirectional chord with lookup-parasitic random sampling (EB-Chord-LPRS), can greatly reduce the average latency. We last validate our protocols through extensive experiments. In comparison with other algorithms, such as Chord, Bidirectional Chord, Low Latency Chord, and LPRS-Chord, our experimental results show that the proposed approaches reduces the path length and latency more than 33% respectively.
[Protocols, Peer-to-Peer networks, graph theory, Quality of service, latency, Length measurement, enhanced bidirectional chord, Delay, table lookup, protocol, Fingers, protocols, sampling methods, short path length, peer-to-peer computing, finger table, Peer to peer computing, chord-based P2P system, random processes, Routing, Information retrieval, Time measurement, distributed hash table, path length, Sampling methods, file organisation, data location, peer-to-peer network, lookup-parasitic random sampling]
Quality of Service for Peer-to-Peer Based Networked Virtual Environments
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
This paper describes how quality of service (QoS) enabled overlay multicast architectures using peer-to-peer (P2P) networks can enhance the experience of end-users in networked virtual environments (NVE). We show how IP multicast, which offers an easy to use API for implementing NVE but is not widely deployed, can be made available to end-users by bridging it transparently with P2P networks. We describe how different P2P and application level multicast (ALM) architectures can be extended with QoS mechanisms using our proposed OM-QoS (overlay multicast QoS) architecture. The presented approach allows users to experience QoS for NVE such as group-based multimedia broadcasting and distributed multiplayer games.
[ALM, Quality of service, Overlay Multicast, Network interfaces, Multimedia communication, Delay, Digital multimedia broadcasting, Application Layer Multicast, QoS, TCPIP, groupware, multicast communication, IP networks, QoS Framework, Kernel, IP multicast middleware, peer-to-peer based networked virtual environment, middleware, Networked Virtual Environments, application level multicast architecture, Virtual environment, peer-to-peer computing, Peer to peer computing, quality of service, Peer-to-Peer, Middleware, Quality of Service, group-based multimedia broadcasting, distributed multiplayer game, P2P, overlay multicast QoS architecture, API, NVE]
A Framework for Scalable Virtual Worlds Using Spatially Organized P2P Networks
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
The general craze for virtual environments, the potential of augmented reality applications and the announced revolution of the Internet world (Web 2.0, Web 3D.0) are key points for the emergence of an 'ambient' Web which will make it possible for users to communicate, collaborate, entertain, work and exchange content. In this context, content storage, delivery, and reproduction are among the essential points for the deployment of a highly scalable platform of wide reality. In this paper, we propose a self-scalable peer-to-peer architecture for the navigation in network-based virtual worlds. To reach this goal, we propose a fully distributed and adaptive streaming method that quickly adapts the reproduced content according to user interaction. Our content delivery strategy has been implemented and tested on a dedicated simulator with a large 3D city model. The presented results show the efficiency of our strategy in very critical conditions.
[Context, Costs, Virtual environment, peer-to-peer computing, scalable virtual worlds, augmented reality, International collaboration, Augmented reality, spatially organized P2P networks, peer-to-peer, peer-to-peer architecture, Network servers, Internet world, adaptive streaming method, self-repartition, Cities and towns, network-based virtual worlds, Collaborative work, self-adaptation, Hardware, virtual environments, IP networks, self-scalabity, self-distribution]
Measures for Inconsistency in Distributed Virtual Environments
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
In distributed virtual environments, hosts typically have to react to events within a time span which is less than the network latency. As a consequence, hosts do routinely take actions although the system is in an inconsistent state. This has a noticeable influence on the perceived quality of these actions and their effect on the application. We argue that the level of this influence depends on the degree of inconsistency. In this paper, we tackle two fundamental questions: How does the degree of inconsistency influence the perceived quality of the users' actions? How can the degree of inconsistency be quantified? We propose a benchmark test for comparing different consistency algorithms with each other which consists of two measures of inconsistency and a sample scenario. For two different consistency algorithms, we compare the results of our benchmark test with the results of a user evaluation test and a simple yield measure.
[virtual reality, Virtual environment, Delay effects, Avatars, Standardization, Time measurement, Mathematics, History, consistency, distributed virtual environments, Optical propagation, Benchmark testing, Internet, consistency algorithms, network latency]
DualCast: Protocol Design of Multiple Shared Trees Based Application Layer Multicast
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
This paper introduces a new approach to build application layer multicast overlay - multiple shared trees. Multiple shared trees' approach makes tradeoffs between traditional source-based trees and single-shared tree, and between transmission efficiency and protocol overheads. Based on this, we propose two protocols to build ALM overlay among end users and media-forwarding-gateways respectively. The latter references the design thought of aggregated multicast to share the multicast trees among groups.
[application layer multicast overlay, source-based trees, Video sharing, trees (mathematics), Programming, Multicast protocols, transmission efficiency, Application software, Videoconference, Delay, Computer science, Design engineering, protocol overheads, multiple shared trees, single-shared tree, multicast protocols, media-forwarding-gateways, Bidirectional control, DualCast, protocol design, Data communication, application layer multicast, video conferencing]
Performance of a Traffic-Based Handover Method in High-Mobility Scenarios
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
A handover priority method for cellular systems based on statistical traffic profile estimates is evaluated in this work through simulation. The scheme is of practical interest as it is simple to compute, easy to implement and it provides network operators with a wide operating interval. The method is examined in scenarios with high mobility and large handover (i.e. overlapping) areas because handover policies are very important for providing QoS in these conditions. Handover performance degradation due to mobility is analysed. The implications of the overlapping area - a larger area decreases the probability of forced termination of ongoing calls to a greater extent than it increases the probability of blocking of new calls - are of practical interest, particularly in the presence of heavy traffic and mobility conditions.
[Call admission control, estimation theory, telecommunication congestion control, Telecommunication traffic, cellular system, traffic-based handover method, Degradation, System performance, QoS, statistical traffic profile estimation, Traffic control, Communication system traffic control, Computer networks, Performance analysis, high-mobility scenario, call admission control, traffic estimates, cellular networks, call admission control (CAC) methods, handover, Computational modeling, carried traffic, probability, quality of service, Land mobile radio cellular systems, QoS metrics, statistical analysis, cellular radio]
Efficient Broadcasting in Networks with Weighted Nodes
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
In this paper the problem of broadcasting in networks with weighted nodes is considered. This model is defined on networks in which nodes are assigned some weights representing the internal process or delay that they should perform before sending data to their neighboring nodes. This model has real world applications in parallel computation and satellite terrestrial networks. The problem is shown to be NP-complete. In this paper we present three algorithms to find near optimal broadcast time of an arbitrary network with weighted nodes. We also present our simulation results to compare the performance of these algorithms.
[parallel computation, Costs, Delay effects, Computational modeling, Satellite broadcasting, NP-complete problem, networks, weighted nodes, broadcasting, Computer science, Concurrent computing, optimisation, Tree graphs, Computer applications, network broadcasting, Computer networks, weighted-vertex graphs, Joining processes, satellite terrestrial networks, computational complexity]
CLM-TCP: TCP Congestion Control Mechanism Using Cross Layer for Wireless Mesh Networks
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
The Transmission Control Protocol - TCP has been widely researched in many different networking technologies such as wireless, optical, satellite, sensor, and wired since it is the most used protocol on the Internet. In Wireless mesh networks, this protocol still has challenges due to its origin and implementations for wired networks. This paper proposes an adaptation on the congestion control mechanism using information from the network layer to improve TCP performance through the vertical calibration across layer technique. The result is the Cross Layer Mesh - Transmission Control Protocol (CLM-TCP). To show the efficiency of the proposal, simulations in the NS-2 were carried out.
[Cross layer design, radio networks, telecommunication congestion control, Wireless application protocol, Optical fiber networks, Transmission Control Protocol, Calibration, wireless mesh networks, NS-2, Optical control, cross layer mesh, Wireless sensor networks, Satellites, transport protocols, Wireless mesh networks, TCP congestion control, Optical sensors, IP networks, CLM-TCP]
Bounding Communication Energy Overhead in Parallel Networks with Power-Delay Scalability
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
We propose the energy scalability and power-delay product scalability as new metrics for calculating and estimating boundaries on the energy overhead of parallel processing systems. Because both properties consider network topology and the algorithm to be implemented, they are shown to be effective in predicting the energy scalability of a multiprocessor in the most general case. Power-delay scalability is then shown to provide upper bound and lower bound estimates when analyzing the energy scalability of a multiprocessor while significantly reducing the computational cost of directly calculating the energy scalability. Energy scalability is an effective metric for predicting the energy overhead cost when using a parallel processor versus a single processor. Power-delay scalability is a means for estimating energy scalability on a system when exact calculation may be difficult due to variation in power levels which have become common with dynamic voltage frequency scaling.
[multiprocessing systems, Scalability, power-delay scalability, power-delay product scalability, dynamic voltage frequency scaling, energy overhead, network topology, parallel processing, communication overhead, bounding communication energy overhead, power aware computing, multiprocessor energy scalability, parallel processing system, energy scalability]
The Application of &#x003BC;u-Law Companding to the WiMax IEEE802.16e Down-Link PUSC
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
This paper presents a simulation investigation of the PSD, BER and PAPR for the IEEE802.16a mobile WiMax standard Down Link PUSC when &#x003BC;-law companding is applied. Results are presented for &#x003BC; values of 0.1, 1, 3, 10, 30, 100, 255 and 1000, for QPSK, 16-QAM and 64-QAM modulation, and for both fixed pilot and randomly phased BPSK pilot situations. The simulations demonstrate that significant BER and PAPR improvement can result through &#x003BC;-Law companding but at the cost of increased transmitter power and increased PSD frequency splatter. Optimized &#x003BC; settings for best BER performance are determined along with the corresponding PAPR level probability curves. The simulation results also allow decisions to be taken regarding the level of companding applied and the compromise desired between any required BER and PAPR levels.
[IEEE802.16a mobile WiMax standard down link PUSC, mobile radio, Bit error rate, Peak to average power ratio, WiMAX, WiMax, Amplitude modulation, Physical layer, quadrature amplitude modulation, Binary phase shift keying, phased BPSK pilot situations, PSD frequency splatter, quadrature phase shift keying, BER, Quadrature phase shift keying, QAM modulation, QPSK, Nonlinear distortion, PAPR, Bandwidth, OFDM modulation, Companding, binary phase shift keying, &#x003BC;-law companding]
Reducing the PAPR of OFDM Using a Simplified Scrambling SLM Technique with No Explicit Side Information
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Selected Mapping (SLM) is a popular technique used for Peak-to Average Power (PAPR) reduction in OFDM systems. Normally SLM techniques require explicit side information to be transmitted to allow decoding at the receiver. Alternative SLM techniques use shift register scramblers to embed the side information into the transmitted OFDM symbols. In this paper the concept of an SLM Simplified Scrambler is introduced. The advantages of this technique are (a) no explicit side information is transmitted, and (b) the simplified scrambler helps to avoid the problem of shift register error propagation in the receiver descrambler. The Simplified Scrambler SLM-OFDM approach is investigated in terms of PAPR reduction, Power Spectral Density (PSD) and Bit-Error Rate (BER) performance. It is shown that the Simplified Scrambler SLM-OFDM technique exhibits the same PSD and BER as general OFDM as well as improving the BER performance over SLM shift-register scrambler techniques.
[power spectral density, orthogonal frequency division multiplexing, OFDM, Partial transmit sequences, Bit error rate, Peak to average power ratio, Shift registers, Educational institutions, Physical layer, receiver descrambler, Decoding, encoding, shift register error propagation scrambler, peak-to average power reduction, PAPR, Selected Mapping, OFDM modulation, OFDM system, bit-error rate, simplified scrambling selected mapping technique, Digital video broadcasting, Power engineering and energy, error statistics]
A Performance Model of Communication in the Quarc NoC
2008 14th IEEE International Conference on Parallel and Distributed Systems
None
2008
Networks on-chip (NoC) emerged as a promising communication medium for future MPSoC development. To serve this purpose, the NoCs have to be able to efficiently exchange all types of traffic including the collective communications at a reasonable cost. The Quarc NoC is introduced as a NOC which is highly efficient in performing collective communication operations such as broadcast and multicast. This paper presents an introduction to the Quarc scheme and an analytical model to compute the average message latency in the architecture. To validate the model we compare the model latency prediction against the results obtained from discrete-event simulations.
[communication medium, Costs, network-on-chip, Quarc, NoC, Telecommunication traffic, multicast operations, Predictive models, performance model, Quarc networks on-chip, model latency prediction, Discrete event simulation, broadcast operations, MPSoC development, Delay, Analytical models, Network-on-a-chip, Perdormace Evaluation, Computer architecture, Traffic control, Broadcasting]
Message from the General and Program Co-Chairs
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Presents the welcome message from the conference proceedings.
[]
Keynote
2009 15th International Conference on Parallel and Distributed Systems
None
2009
We will share or experience at simplifying the programming of applications that are distributed on local area network (LAN), on cluster of workstations, or grids, and of course, clouds. We will promote a kind of approach, network on chip, to cope seamlessly with both distributed and shared-memory multi-core machines. A theoretical foundation ensures constant behavior, whatever the environment.
[network-on-chip, Clouds, Laboratories, grid computing, Standardization, network on chip, Distributed computing, distributed computing, workstation cluster, Concurrent computing, shared-memory multicore machines, Network-on-a-chip, local area network, distributed shared memory systems, LAN, Libraries, Workstations, Local area networks, Graphical user interfaces]
Prediction-Based Prefetching to Support VCR-like Operations in Gossip-Based P2P VoD Systems
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Supporting free VCR-like operations in P2P VoD streaming systems is challenging. The uncertainty of frequent VCR operations makes it difficult to provide high quality realtime streaming services over distributed self-organized P2P overlay networks. Recently, prefetching has emerged as a promising approach to smooth the streaming quality. However, how to efficiently and effectively prefetch suitable segments is still an open issue. In this paper, we propose PREP, a PREdiction-based Prefetching scheme to support VCR-like operations over gossip-based P2P on-demand streaming systems. By employing the reinforcement learning technique, PREP transforms users' streaming service procedure into a set of abstract states and presents an online prediction model to predict a user's VCR behavior via analyzing the large volumes of user viewing logs collected on the tracker. We further present a distributed data scheduling algorithm to proactively prefetch segments according to the predicted VCR behavior. Moreover, PREP takes advantage of the inherent peer collaboration of gossip protocol to optimize the response latency. Through comprehensive simulations, we demonstrate the efficiency of PREP by gaining the accumulated hit ratio close to 75% while reducing the response latency close to 70% with only less than 15% extra stress on the server side.
[Uncertainty, Protocols, Predictive models, PREdiction based prefetching scheme, Delay, gossip based P2P VoD system, Learning, video on demand, scheduling, distributed data scheduling algorithm, video streaming, video tape recorders, learning (artificial intelligence), online prediction model, VCR-like operation support, peer-to-peer computing, high quality realtime streaming service, Prefetching, prediction theory, Video recording, Scheduling algorithm, Stress, reinforcement learning technique, distributed self-organized P2P overlay networks, Collaboration, P2P VoD streaming system]
Quality of Service in Peer-to-Peer IPTV Networks
2009 15th International Conference on Parallel and Distributed Systems
None
2009
In Peer-to-peer (P2P) IPTV networks, jitter rate is one of the most important metrics for Quality of Service. In this paper, we develop a new approach to estimate jitter rate during playback. Unlike traditional approaches that focus on the download speed, our method relies on the distribution of peer download latencies. We demonstrated how to apply the proposed methodologies in a real-life environment. We report simulation results over a P2P IPTV network with 5 seeds and 2,000 clients. Experimental results not only validated the theoretical projections, but also show that our model captures impacts of many design factors. It is thus suggested to use the proposed method as a guiding tool to design P2P IPTV networks with low jitter rate.
[Computers, peer-to-peer computing, Peer to peer computing, peer-to-peer IPTV network, real-life environment, jitter rate, chunking schemes, Quality of service, Jitter, download latency, IPTV, quality of service, Delay, P2P networks, Network servers, peer download latencies, Upper bound, streaming goodput, P2P IPTV network, Bandwidth, download speed, Streaming media]
Decentralized Adaptive Routing for Reliability in Event Broker Networks
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Guaranteeing quality of service (QoS) for event delivery has been recognized as an important but challenging issue in event based middleware (EBM), that is responsible for routing events from publishers to subscribers over an event broker network. Amongst the numerous QoS parameters, in our work, we focus on reliability as a service guarantee to subscribers in an EBM. We add to the existing body of work in this area by investigating reliability needs of type-specific subscriptions and comparing them with type-agnostic subscriptions. The broker network establishes routes by maintaining event-type specific path-quality information at every broker node occurring in the route to the destination. Each broker node measures the drop probability of a particular event type. We prove that the drop probabilities experienced by individual event types are proportional to the ratio of their inter-arrival times at the broker. Based on this, we present the TSAR (Type Specific Adaptive Reliability) algorithm. where route establishment is done in an adaptive and decentralized fashion using persistent type-specific path quality information stored as a matrix of reliability estimates. Our results show that TSAR (1) reduces the overall message complexity as compared to existing efforts in this area (2) provides subscribers with a higher level of granularity when subscribing to events and adapts to the dynamics of the broker network with varying reliabilities of broker nodes.
[event broker networks, Adaptive systems, type-agnostic subscriptions, path-quality information, Subscriptions, Quality of service, Reliability engineering, Discrete event simulation, event delivery, Analytical models, type specific adaptive reliability, telecommunication network reliability, TSAR, event based middleware, Large-scale systems, Event Broker Network, middleware, message passing, decentralized adaptive routing, Routing, quality of service, Middleware, Computer science, telecommunication network routing, Reliability, type-specific subscriptions]
On the Robust Mapping of Dynamic Programming onto a Graphics Processing Unit
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Graphics processing units (GPUs) have been widely used to accelerate algorithms that exhibit massive data parallelism or task parallelism. When such parallelism is not inherent in an algorithm, computational scientists resort to simply replicating the algorithm on every multiprocessor of a NVIDIA GPU, for example, to create such parallelism, resulting in embarrassingly parallel ensemble runs that deliver significant aggregate speed-up. However, the fundamental issue with such ensemble runs is that the problem size to achieve this speed-up is limited to the available shared memory and cache of a GPU multiprocessor. An example of the above is dynamic programming (DP), one of the Berkeley 13 dwarfs. All known DP implementations to date use the coarse-grained approach of embarrassingly parallel ensemble runs because a fine-grained parallelization on the GPU would require extensive communication between the multiprocessors of a GPU, which could easily cripple performance as communication between multiprocessors is not natively supported in a GPU. Consequently, we address the above by proposing a fine-grained parallelization of a single instance of the DP algorithm that is mapped to the GPU. Our parallelization incorporates a set of techniques aimed to substantially improve GPU performance: matrix re-alignment, coalesced memory access, tiling, and GPU (rather than CPU) synchronization. The specific DP algorithm that we parallelize is called Smith-Waterman (SWat), which is an optimal local-sequence alignment algorithm. We then use this SWat algorithm as a baseline to compare our GPU implementation, i.e., CUDA-SWat, to our implementation on the cell broadband engine, i.e., Cell-SWat.
[NVIDIA GPU multiprocessor, tiling, cache storage, coprocessors, parallel processing, Engines, Concurrent computing, Computer architecture, Computer graphics, Parallel processing, GPU synchronization, shared memory systems, SWat algorithm, Robustness, Dynamic programming, matrix realignment, coalesced memory access, optimal local-sequence alignment algorithm, cache, dynamic programming, Smith-Waterman algorithm, Synchronization, task parallelism, computer graphics, data parallelism, graphics processing unit, Central Processing Unit, Acceleration, shared memory]
CUDA Accelerated LTL Model Checking
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Recent technological developments made available various many-core hardware platforms. For example, a SIMD-like hardware architecture became easily accessible for many users who have their computers equipped with modern NVIDIA GPU cards with CUDA technology. In this paper we redesign the maximal accepting predecessors algorithm for LTL model checking in terms of matrix-vector product in order to accelerate LTL model checking on many-core GPU platforms. Our experiments demonstrate that using the NVIDIA CUDA technology results in a significant speedup of verification process.
[SIMD-like hardware architecture, parallel architectures, LTL model checking, many-core hardware platforms, temporal logic, coprocessors, NVIDIA GPU cards, Parallel algorithms, maximal accepting predecessors algorithm, GPU, Concurrent computing, formal verification, many-core GPU platforms, Computer architecture, Parallel processing, Hardware, Informatics, computers, matrix-vector product, Pervasive computing, LTL Model Checking, computer aided analysis, Power system modeling, CUDA, computer graphics, Automata, Acceleration]
Improving Performance of Matrix Multiplication and FFT on GPU
2009 15th International Conference on Parallel and Distributed Systems
None
2009
In this paper we discuss about our experiences in improving the performance of two key algorithms: the single-precision matrix-matrix multiplication subprogram (SGEMM of BLAS) and single-precision FFT using CUDA. The former is computation-intensive, while the latter is memory bandwidth or communication-intensive. A peak performance of 393 Gflops is achieved on NVIDIA GeForce GTX280 for the former, about 5% faster than the CUBLAS 2.0 library. Better FFT performance results are obtained for a range of dimensions. Some common principles are discussed for the design and implementation of many-core algorithms.
[fast Fourier transforms, memory bandwidth intensive, Laboratories, Software performance, coprocessors, communication-intensive, computer speed 393 GFLOPS, Yarn, GPU, computation-intensive, FFT, Bandwidth, NVIDIA GeForce GTX280, Matrix Multiplication, Libraries, Hardware, Computer science education, Testing, single-precision FFT, Educational technology, Programming profession, single-precision matrix-matrix multiplication subprogram, CUDA, matrix multiplication, computer graphics]
Flexible Multi-link Ethernet Binding System for PC Clusters with Asymmetric Topology
2009 15th International Conference on Parallel and Distributed Systems
None
2009
In current high-performance PC clusters, the performance and cost of interconnection network are essential issues. Very cost-effective Ethernets, such as Gigabit Ethernet, as well as high performance SANs, such as Infiniband and Myrinet, are still widely used. The authors have been developing a multi-link binding network system for Ethernet, called RI2N, for high-throughput and fault-tolerant interconnection with Gigabit Ethernet. It can be used both for internode communication in MPI programs and traditional UNIX network services such as NFS. In this paper, the authors propose an optimized version of RI2N, called RI2N+, that allows asymmetrical multi-link connection for fitting to various cost-effective system configurations. Such a configuration cannot be supported by Linux Channel Bonding, which is widely used in standard Linux distributions. RI2N+ automatically detects the asymmetric network configuration and controls the traffic distribution to multiple links. In the basic performance evaluation under a high traffic rate, it was confirmed that the throughput of the network with the proposed scheme is improved by approximately 30% compared with the original RI2N. RI2N+ also maintains high performance even in asymmetric configurations, that is up to 86% of the relative performance compared with the symmetric case.
[workstation clusters, Ethernet networks, Costs, Multiprocessor interconnection networks, asymmetric network configuration, Throughput, Ethernet binding system, gigabit Ethernet, Network topology, Fault tolerant systems, Linux channel bonding, Automatic control, Communication system traffic control, asymmetric channel binding, Bonding, RI2N+, UNIX network services, PC clusters, telecommunication network topology, internode communication, asymmetric topology, LAN interconnection, redundant interconnection network, PC cluster, fault tolerant network, Linux, Ethernet]
uStream: A User-Level Stream Protocol over Infiniband
2009 15th International Conference on Parallel and Distributed Systems
None
2009
As one of the most popular high speed networks, InfiniBand demonstrates several enhanced features, such as RDMA and zero-copy mechanisms, which offer high bandwidth and low latency. Communication stacks IPoIB and SDP (Sockets Direct Protocol) have been proposed on InfiniBand for sockets based applications to take advantage of these features. However, these protocols are inefficient to utilize the performance capabilities provided by the physical network. In order to fully exploit the high performance of InfiniBand, we present uStream, a relatively simple and efficient protocol on the user-level socket layer with a stream interface to enable RDMA capability and zero-copy mechanism. Experiment results have shown that the performance of uStream is comparable to raw InfiniBand Verbs/RDMA interface, and uStream outperforms SDP with 7.9 s minimum latency and 10.4 Gbps peak bandwidth in our testbed. In addition, a Java communication library, jStream, is built upon uStream to enable Java to use RDMA directly. Preliminary results have shown Java clusters communication performance has been improved to a level similar to C programs over InfiniBand through combined uStream and jStream.
[Protocols, InfiniBand, jStream, time 7.9 mus, Switches, user-level stream protocol, C language, Delay, Infiniband, High-speed networks, Bandwidth, media streaming, Computer networks, Libraries, protocols, bit rate 10.4 Gbit/s, SDP, RDMA, Java, uStream, IPoIB, Communication Protocol, Communication switching, Java clusters communication, Sockets, Java communication library, sockets direct protocol, C programs]
WormCircle: Connectivity-Based Wormhole Detection in Wireless Ad Hoc and Sensor Networks
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Wormhole attack is a severe threat against wireless ad hoc and sensor networks. It can be launched without compromising any legitimate node or cryptographic mechanisms, and often serves as a stepping stone for many serious attacks. Most existing countermeasures often make critical assumptions or require specialized hardware devices in the network. Those assumptions and requirements limit the applicability of previous approaches. In this work, we explore the impact of wormhole attacks on network connectivity topologies, and develop a simple distributed method to detect wormholes, called WormCircle-. WormCircle relies solely on local connectivity information without any requirements on special hardware devices or making any rigorous assumptions on network properties. We establish the correctness of this design in continuous geometric domains and extend it into discrete networks. We evaluate the effectiveness in randomly deployed sensor networks through extensive simulations.
[deployed sensor networks, wireless sensor networks, wireless ad hoc networks, Mobile communication, cryptography, specialized hardware devices, Sensor systems, wormcircle, Computer science, Wireless sensor networks, Network topology, Hardware, Communication system traffic control, Computer networks, cryptographic mechanisms, Cryptography, ad hoc networks, National security, local connectivity information, connectivity-based wormhole detection]
Broadcasting with Optimized Transmission Efficiency in 3-Dimensional Wireless Networks
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Broadcasting is one of the most important operations in the wireless network for disseminating information throughout the entire network. Flooding is a simple mechanism to realize broadcasting, but it has high redundancy of retransmissions, leading to low transmission efficiency. Many broadcast protocols have been proposed for pursuing optimized transmission efficiency for wireless networks hypothetically deployed on the 2-dimesional (2D) plane. In the real world, wireless networks are deployed in the 3D space. In this paper, we derive the upper bound of 3D transmission efficiency and propose a 3D broadcast protocol with optimized transmission efficiency by partitioning the 3D space into multi-layer hexagonal prisms of a hexagon ring pattern in each layer. As we will show, the transmission efficiency of the proposed protocol can reach 1/, which is better than those of other polyhedron-filling approaches using cubes, hexagon prisms, rhombic dodecahedrons, and truncated octahedrons.
[broadcasting operations, radio networks, flooding mechanism, transmission efficiency, 2-dimesional plane, retransmissions redundancy, 3-Dimensional Wireless Network, broadcasting, 3-dimensional wireless networks, Wireless networks, routing protocols, multilayer hexagonal prisms, hexagon ring pattern, Broadcasting, optimized transmission efficiency, 3D broadcast protocol]
Contour-cast: Location-free Data Dissemination and Discovery for Wireless Sensor Networks
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Data dissemination and discovery is critical for ad-hoc wireless sensor networks. Most existing research depends on location information that is not always obtained easily, efficiently and accurately. We propose the concept of Contour-cast, a location-free data dissemination and discovery approach for large-scale wireless sensor networks. One important property of Contour-cast is that it does not depend on physical position or accurate localization services. The other advantage is that each node needs not maintain too much topology information. We evaluate Contour-cast thoroughly using metrics including data retrieval success ratio, storage cost, and load balance. Evaluation results show that Contour-cast can reach comparable functionalities and performance as the other approaches with physical location information.
[wireless sensor networks, Routing, Data engineering, mobility management (mobile radio), contour-cast, location-free data dissemination, Distributed computing, Computer science, Concurrent computing, Wireless sensor networks, Network topology, data discovery, Computer networks, Large-scale systems, Monitoring]
Optimizing Base Station Deployment in Wireless Sensor Networks Under One-hop and Multi-hop Communication Models
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Sensor network lifetime is largely affected by the energy consumption for data transmission from sensor nodes to a base station. We generalize and solve the problems of deploying multiple base stations in sensor networks using one-hop and multi-hop communication models to maximize network lifetime. Under the one-hop communication model, the sensors far away from base stations always deplete their energy much faster than others. We propose an optimal solution for small-scale networks and a heuristic approach for large-scale ones based on the smallest enclosing circle algorithm that deploys a base station at the geometric center of each cluster. Under the multi-hop communication model, both the base station locations and the data routing scheme need to be considered in maximizing network lifetime. We propose an iterative algorithm based on rigorous mathematical derivations and use linear programming to compute the optimal routing path for benchmark purposes. Extensive simulation results show superior network lifetime performance of the proposed deployment algorithms in comparison with existing ones.
[Energy consumption, wireless sensor networks, one hop communication model, small-scale network, linear programming, complex networks, sensor nodes, sensor placement, Clustering algorithms, Spread spectrum communication, data routing scheme, smallest enclosing circle algorithm, Large-scale systems, Data communication, energy consumption, heuristic approach, Base stations, base station deployment, Routing, Linear programming, multihop communication model, iterative algorithm, Wireless sensor networks, telecommunication network routing, data transmission, Iterative algorithms, p-center, network lifetime, rigorous mathematical derivations]
Minimizing Memory Access Schedule for Memories
2009 15th International Conference on Parallel and Distributed Systems
None
2009
According to the characteristics of the "3-D" structure of contemporary DRAM chips, the row first column ordered (RFCO) algorithm is proposed in this paper to minimize memory access schedule length. In memory systems with a single memory controller, assuming that the memory access trace is known before scheduling, the RFCO algorithm can generate schedules which are 7.89% shorter than burst scheduling on average. If memory accesses are coming to the single memory controller in real time, the RFCO algorithm can generate schedules which are 8.03% shorter than burst scheduling on average.
[row first column ordered algorithm, single memory controller, memory access schedule minimization, Random access memory, Control systems, memory access schedule length, burst scheduling, Delay, Scheduling algorithm, processor scheduling, Computer science, Fabrication, Processor scheduling, Bandwidth, Frequency, DRAM chips, Clocks]
When Misses Differ: Investigating Impact of Cache Misses on Observed Performance
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Although modeling of memory caches for the purpose of cache design and process scheduling has advanced considerably, the effects of cache sharing are still not captured by common approaches to modeling of software performance. One of the obstacles is lack of information about the relationship between cache misses, which the cache models usually describe, and the timing penalties, which the performance models require. Following earlier work that has shown how cache misses do not quite account for timing penalties, we report on extensive experiments that investigate the connection between cache sharing and observed performance in more depth on a real computer architecture.
[Process design, Software performance, Switches, Mathematics, cache storage, cache design, memory caches, performance modeling, Computer architecture, scheduling, Hardware, Mathematical model, cache misses, processor caches, cache sharing, observed performance, performance models, cache models, resource sharing, process scheduling, Timing, timing penalties, software performance, Context modeling, Software engineering]
A Light-weight Code Cache Design for Dynamic Binary Translation
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Interpretation and basic block translation (BBT) are two typical strategies for cold code emulation in a dynamic binary translation (DBT) system. More and more DBT systems employ BBT as the generated native code runs more efficient than the interpretation routines. We observe that BBT's high efficiency is based on those special hardware assists. With certain simple hardware techniques, interpretation could outperform BBT. In our pervious work, we proposed a hardware interpreted code cache (Pcache) mechanism to speedup interpretation by saving the decoded instruction information during interpretation. This light-weight code cache design could be extended to assist the hotspots translation, thus further reduce the DBT systems' overhead. We add the translation entry into the Pcache design thus saving most decoding operations during translation. We use eight SPEC 2000 integer benchmarks on our DBT simulator. Results show that the modified Pcache design causes a speedup of 1.94 according to the referenced DBT with basic interpretation and the interpretation based DBT system assisted by the modified Pcache performs more efficiently than the DBT system which employs BBT for the cold code.
[hotspots translation, Instruction sets, light-weight code cache design, cache storage, VLIW, Distributed computing, decoded instruction information, Concurrent computing, Microarchitecture, Emulation, cold code emulation, block codes, Pcache, design, Hardware, instruction sets, Technological innovation, binary codes, Pcache design, Decoding, decoding, performance, interpretation, basic block translation, dynamic binary translation system, Acceleration, hardware interpreted code cache]
Snooping and Ordering Ring - An Efficient Cache Coherence Protocol for Ring Connected CMP
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Ring is a promising on-chip interconnection for CMP. It is more scalable than bus and much simpler than packet-switched networks. The ordering property of ring can be used to optimize cache coherence protocol design. Existing ring protocols, such as the snooping ring protocol and the ring-order protocol need a retry and acknowledgement scheme or use the ordering property of the ring respectively to resolve conflict memory requests. A cache coherence protocol named SOR (Snooping and Ordering Ring) is developed for ring connected CMP in this paper. This protocol is based on the snooping ring protocol. But instead of using the acknowledgement and retry scheme, it uses the ordering property of the ring to resolve conflicts, thus can avoid unnecessary retries to improve performance and power efficiency. The L1 snooping results are sent with the requests instead of being delayed, so that many useless snoops can be avoided. Simulation result shows that the average probe slot transports and snoop operations can be reduced by SOR are 47% and 48.9%. The average and maximum performance improvements by SOR are 3.33% and 6%.
[Energy consumption, Laboratories, ring, multiprocessor interconnection networks, cache storage, Distributed computing, Delay, Multiprocessing systems, Concurrent computing, Bandwidth, Computer architecture, Broadcasting, memory requests conflict, protocols, chip multiprocessor systems, multiprocessing systems, ring ordering property, Access protocols, CMP, snooping and ordering ring protocol, snooping ring protocol, on-chip interconnection, ring connected CMP, cache coherence protocol, cache coherence protocol design]
Feedback-Control-Based Performance Regulation for Multi-Tenant Applications
2009 15th International Conference on Parallel and Distributed Systems
None
2009
The ability to deliver different performance levels based on tenant-specific service level agreements (SLAs) is a key requirement for multi-tenant Internet applications. However, workload variations and extensive resource sharing among tenants make this goal hard to achieve. We address the issue with a performance regulator based on feedback-control. The regulator has a hierarchical structure, with which a high-level controller manages request admission rates to prevent overloading and a low-level controller manages resource allocation for admitted requests to track a specified level of service differentiation between the cohosted tenants. A prototype implementation of the performance regulator based on Tomcat and MySQL is provided and a multi-tenant version of RUBBoS benchmark is used for evaluation. Experimental results indicate that the regulator effectively bounds the response time for each tenant while maintaining high resource utilization levels.
[Computers, multi-tenant applications, Software prototyping, Regulators, Costs, Tomcat, Quality of service, feedback control, distributed processing, Security, Yarn, performance regulation, tenant-specific service level agreements, feedback, multi-tenant, resource allocation, Web and internet services, resource sharing, MySQL, Internet, Resource management, RUBBoS, Testing]
Allocation of Clients to Multiple Servers on Large Scale Heterogeneous Platforms
2009 15th International Conference on Parallel and Distributed Systems
None
2009
We consider the problem of allocating a large number of independent, equal-sized tasks to a heterogeneous large scale computing platform. We model the platform using a set of servers (masters) that initially hold (or generate) the tasks to be processed by a set of clients (slaves). All resources have different speeds of communication and computation and we model contentions using the bounded multi-port model. This model corresponds well to modern networking technologies, but for the sake of realism, another parameter needs to be introduced in order to bound the number of simultaneous connections that can be opened at a server node. We prove that unfortunately, this additional parameter makes the problem of maximizing the overall throughput NP-complete. On the other hand, we also propose a polynomial time algorithm, based on a slight resource augmentation, to solve this problem. More specifically, we prove that, if d <sub>j</sub> denotes the maximal number of connections that can be opened at server node S <sub>j</sub>, then the throughput achieved using this algorithm and d <sub>j</sub> + 1 simultaneous connections is at least the same as the optimal one with d <sub>j</sub> simultaneous connections. This algorithm also provides a good approximation for the dual problem of minimizing the maximal number of connections that need to be opened in order to achieve a given throughput, and it can be turned into a standard approximation algorithm (i.e., without resource augmentation). Finally, we also propose extensive simulations to assess the performance of the proposed algorithm.
[Master-slave, client allocation, Throughput, NP-complete, Steady-state, approximation algorithms, task analysis, Distributed computing, heterogeneous large scale computing platform, Concurrent computing, Network servers, resource allocation, large scale platforms, standard approximation algorithm, Large-scale systems, modern networking technologies, approximation theory, client-server systems, multiple servers, slight resource augmentation, Computational modeling, polynomial time algorithm, bounded multiport model, task allocation, Processor scheduling, Approximation algorithms, computational complexity]
Dynamic Load Balancing for Parallel Numerical Simulations Based on Repartitioning with Disturbed Diffusion
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Load balancing is an important requirement for the efficient execution of parallel numerical simulations. In particular when the simulation domain changes over time, the mapping of computational tasks to processors needs to be modified accordingly. State-of-the-art libraries for this problem are based on graph repartitioning. They have a number of drawbacks, including the optimized metric and the difficulty of parallelizing the popular repartitioning heuristic Kernighan-Lin (KL). In this paper we further explore the very promising diffusion-based graph partitioning algorithm DIBAP by adapting DIBAP to the related problem of load balancing and improving its implementation. The presented experiments with graph sequences that imitate adaptive numerical simulations are the first using DIBAP for load balancing. They demonstrate the applicability and high quality of DIBAP for load balancing by repartitioning. Compared to the faster state-of-the-art repartitioners PARMETIS and parallel JOSTLE, DIBAP's solutions have partitions with significantly fewer external edges and boundary nodes and the resulting average migration volume in the important maximum norm is also the best in most cases.
[Partial differential equations, boundary nodes, graph theory, graph repartitioning, Finite element methods, parallel processing, Concurrent computing, diffusion-based graph partitioning algorithm, resource allocation, average migration volume, numerical analysis, Libraries, PARMETIS, Load balancing, graph partitioning, parallel adaptive numerical simulations, disturbed diffusion, Computational modeling, libraries, Partitioning algorithms, Application software, DIBAP, parallel JOSTLE, disturbed diffusive repartitioning algorithm, Computer science, adaptive graph sequences, parallel numerical simulations, Load management, Numerical simulation, heuristic Kernighan-Lin repartitioning, dynamic load balancing]
Market-Based Load Balancing for Distributed Heterogeneous Multi-Resource Servers
2009 15th International Conference on Parallel and Distributed Systems
None
2009
To cope with rapidly increasing Internet usage nowadays, providing Internet services using multiple servers has become a necessity. To ensure sufficient service quality and server utilization at the same time, effective methods are needed to spread load among servers properly. Existing load balancing methods often assume servers are homogeneous and consider only one type of resource, such as CPU. Such methods suffer from the fact that different requests often demand multiple types of resources with different requirements; trying to balance the usage of only one resource type may induce an inadvertent performance bottleneck, leading to low resource utilization and service quality. To address this problem, we propose a load balancing method based on the concept of distributed market mechanism, where requests are priced with respect to the load of multiple resources on each server. By migrating jobs among servers to balance inter-server load and minimize intra-server job cost at the same time, our method shows significant improvement in terms of load imbalance degrees, server utilization, and response time when compared to other published methods, especially when server heterogeneity increases.
[distributed heterogeneous multiresource servers, Internet services, Costs, service quality, low resource utilization, Scheduling, Delay, intraserver job cost, Computer science, Network servers, distributed market mechanism, market mechanisms, interserver load, resource allocation, market based load balancing, Web and internet services, multi-resource scheduling, Load management, Hardware, Internet, Resource management, Load balancing, Web server, server utilization]
A Decentralized Storage Scheme for Multi-Dimensional Range Queries over Sensor Networks
2009 15th International Conference on Parallel and Distributed Systems
None
2009
This paper presents the design of a decentralized storage scheme to support multi-dimensional range queries over sensor networks. We build a distributed k-d tree based index structure over sensor network, so as to efficiently map high dimensional event data to a two-dimensional space of sensors while preserving the proximity of events. We propose a dynamic programming based methodology to control the granularity of the index tree in an optimized approach, and an optimized routing scheme for range query processing to achieve best energy efficiency. The simulation results demonstrate the efficiency of the design.
[indexing, Laboratories, distributed k-d tree based index structure, Optimization methods, trees (mathematics), optimized routing scheme, Sensor phenomena and characterization, dynamic programming, Routing, Sensor systems, sensor networks, Paper technology, distributed sensors, Temperature sensors, Computer science, query processing, storage management, data storage, Query processing, sensor network, optimization, Distributed control, multidimensional range queries, decentralized storage scheme]
Polygon-Based Tracking Framework in Surveillance Wireless Sensor Networks
2009 15th International Conference on Parallel and Distributed Systems
None
2009
We propose a new tracking framework by organizing nodes into a polygonal spatial neighborhood in order to detect and track unauthorized traversals in surveillance wireless sensor networks. During a tracking, the neighborhood is further constructed ahead of the target traversal, and this features a timely forwarding with guaranteed delivery property. Instead of estimating future movement and position separately in a polygon, we find an original target tracking path in a graph, and create a brink on the graph called "critical region'' by introducing a brink detection algorithm to know a target's route, and to also achieve reliable inter-node communications. In addition to the basic design, an optimal sensor selection algorithm was developed to select which sensors to query, and dynamically guide the target information to a sink. Simulation results validated that the proposed approach has better tracking accuracy, reduced localization error, and is robust to strong environment noise, while using a minimum number of sensors.
[Shape, wireless sensor networks, brink detection algorithm, Brink construction, target localization, surveillance wireless sensor networks, Computer networks, surveillance, critical region graph, polygonal neighborhood, Target tracking, sensor selection, future movement estimation, polygon-based tracking framework, polygonal spatial neighborhood, Routing, inter-node communications, Face detection, Organizing, Wireless sensor networks, optimal sensor selection algorithm, Surveillance, Working environment noise, telecommunication network routing, target tracking, Detection algorithms]
Constructing Optimal Clustering Architecture for Maximizing Lifetime in Large Scale Wireless Sensor Networks
2009 15th International Conference on Parallel and Distributed Systems
None
2009
In large scale wireless sensor networks, clustering is an effective technique for the purpose of improving the utilization of limited energy and prolonging the network lifetime. However, the problem of unbalanced energy dissipation exists in the multi-hop clustering model, where the cluster heads closer to the sink have to relay heavier traffic and consume more energy than farther nodes. Unequal clustering has proved to be able to achieve more uniform energy dissipation among cluster heads; however, how to quantify the size for the aim of scalability is a challenging problem. In this paper, we investigate the theoretical aspects of the unequal clustering strategy for minimizing and balancing the energy consumption of cluster heads in large uniform distributed sensor networks. Based on the proposed optimal clustering architecture, we design simple energy-aware head rotation and routing protocols to further balance the energy consumption among all nodes. Extensive simulations show that the proposed architecture can maximize the network lifetime defined from different aspects, and therefore, are suitable for the design and deployment of large scale sensor networks.
[Energy consumption, lifetime prolonging, wireless sensor networks, Scalability, Data compression, relay heavier traffic, large scale wireless sensor networks, Relays, distributed sensor networks, Wireless sensor networks, energy dissipation, pattern clustering, optimal clustering architecture, Energy dissipation, Traffic control, Energy efficiency, Corona, multihop clustering model, structuralized clustering approaches, Large-scale systems, energy consumption, energy-efficiency, telecommunication traffic]
CCQR: Constant Cost Quality-based Routing Protocol in Delay Tolerant Networks
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Quality-based routing protocols are proposed to restrict message flooding within only high quality nodes in delay tolerant networks (DTNs). However, different quality threshold mechanisms have diverse impact on the network and we investigate this issue in the paper. Through theoretical analysis we show that heterogeneous threshold mechanism suffers a severe nodal cost imbalance problem. Furthermore, for both homogeneous and heterogeneous mechanisms, routing cost depends on the total number of nodes, and becomes significant in large-scale networks. To address these issues, we propose Constant Cost Quality-based Routing (CCQR) protocol, which not only retains the favorable features of quality-based routing protocols, but also achieves a constant routing cost. More importantly, thanks to the wise message distribution rules, CCQR protocol is capable of alleviating nodal cost imbalance problem effectively. We also conduct extensive simulations to verify our conclusions and the efficacy of the new protocol.
[Disruption tolerant networking, Costs, Laboratories, delay tolerant networks, Floods, Communication system software, constant cost quality-based routing protocol, Chaotic communication, mobile communication, Wireless networks, routing protocols, Software quality, Routing protocols, Large-scale systems]
An Improved Parallel Access Technology on Routing Table for Threaded BGP
2009 15th International Conference on Parallel and Distributed Systems
None
2009
The stringent requirement for the high efficiency of routing protocol on Internet will be satisfied by exploiting the threaded border gateway protocol (TBGP) on multi-cores. Since the TBGP performance is restricted by a mass of contentions when racing to access the routing table, a highly-efficient parallel access approach is originally proposed to achieve the ultra-high route processing speed. In this paper, a novel routing table structure consisting of two-level tries is presented for fast parallel access, and a heuristic-based divide-and-recombine algorithm is devised to balance the table accesses and release the contentions, thereby accelerating the parallel route update of multi-threading. By modifying the typical table operations such as lookup, insert, etc., the correctness of two-level tries table is validated according to the operation behaviors of traditional routing table. Experimental results on dual quad-core Xeon server show that the parallel access contentions decrease sharply by 92.5% versus traditional routing table, and the maximal update time per thread is obviously reduced by 56.8% on average with little overhead. Then, the throughput of BGP update message is measured to be improved by about 169%, delivering significant performance improvement of BGP.
[parallel access technology, dual quad-core Xeon server, network servers, Peer to peer computing, Delay effects, internetworking, Access protocols, routing protocol, ultra-high route processing speed, Yarn, Distributed computing, Concurrent computing, heuristic-based divide-and-recombine algorithm, routing protocols, Parallel processing, Streaming media, routing table, Routing protocols, threaded border gateway protocol, Internet]
Exploiting TLS Parallelism at Multiple Loop-Nest Levels
2009 15th International Conference on Parallel and Distributed Systems
None
2009
As the number of cores integrated onto a single chip increases, architecture and compiler designers are challenged with the difficulty of utilizing these cores to improve the performance of a single application. Thread-level speculation (TLS) can potentially help by allowing possibly dependent threads to speculatively execute in parallel. Extracting speculative thread from sequential applications is key to efficient TLS execution. Previous work on thread extraction has focused on parallelizing iterations from a single loop-nest level or function continuation. However, the amount of parallelism available at a single loop-nest level is sometimes limited, and we are forced to look for parallelism across multiple loop-nest levels. In this paper we propose SpecOPTAL - a compiler algorithm that statically allocates cores to threads extracted from different levels of loop-nests. We show that, a subset of SPEC 2006 benchmarks are able to benefit from the proposed technique.
[Out of order, program control structures, thread extraction, multiprocessing systems, SpecOPTAL compiler algorithm, Multicore processing, multi-threading, Merging, parallel execution, Yarn, program compilers, Delay, function continuation, Computer science, Degradation, Runtime, resource allocation, Multi-core, multiple loop-nest level, Parallel processing, Hardware, thread-level speculation, compiler optimization, thread-level speculation parallelism]
Modeling Data Access Contention in Multicore Architectures
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Multicore processors are now part of mainstream computing. However, data access contention among multiple cores is a significant performance bottleneck in utilizing these processors. Typically, memory hierarchies in multicore architectures use shared last level cache or shared memory. As multiple cores concurrently send requests to access data from these shared memory hierarchy levels, their capacity to serve all the requests is overwhelming and causes performance bottlenecks. In this paper, we introduce simple analytical models for predicting the occurrence of data access contention and provide a guideline for choosing optimal number of cores in running an application without causing data access contention. We verify our models by comparing the predicted optimal number of cores without causing data contention with the measured value in running MIMD Lattice Computation (MILC) application. The proposed analytical models are accurate and promising in guiding data access optimizations to improve multicore utilization.
[multiprocessing systems, data access optimization, data access contention, Multicore processing, MIMD lattice computation application, modeling, parallel architectures, multicore architecture, Predictive models, data access contention modeling, Yarn, Modeling, Delay, Concurrent computing, performance prediction, Analytical models, Bandwidth, Computer architecture, Parallel processing, multicore processors, multicore processor]
Towards High-Performance Network Intrusion Prevention System on Multi-core Network Services Processor
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Network intrusion prevention system (NIPS) becomes more complex due to the rapid growth of network bandwidth and requirement of network security. However existing solutions, either hardware-based or software-based cannot obtain a good tradeoff between performance and flexibility. In this paper, we propose a parallel NIPS architecture using emerging network services processor. To resolve the problems and bottlenecks of high-speed processing, we investigate the main design aspects which have dramatic impacts on most parallel network security system implementations: efficient and flexible pipeline and parallel processing, flow-level packet-order preserving, and latency hiding of deep packet inspection. To these key points, we address several optimizations and modifications with an architecture-aware design principle to guarantee high performance and flexibility of the NIPS on a network services processor implementation. Performance evaluation shows that, our prototype NIPS on Cavium OCTEON3860 processor can reach line-rate stateful inspection and multi-Gbps deep inspection performance.
[high-performance network intrusion prevention system, Costs, Automation, multiprocessing systems, Data security, parallel architectures, Inspection, architecture-aware design principle, multicore network services processor, network bandwidth, network security requirement, parallel processing, flow-level packet-order preserving, Application specific integrated circuits, High-speed networks, security of data, deep packet inspection, parallel NIPS architecture, Prototypes, Bandwidth, parallel network security system, latency hiding, Field programmable gate arrays, Software engineering]
A Novel Optimization Method to Improve De-duplication Storage System Performance
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Data De-duplication has become a commodity component in data-intensive storage systems. But compared with other traditional storage paradigms, de-duplication system achieves elimination of data duplications or redundancies at the cost of bringing several additional layers or function components into the I/O path, and these additional components are either CPU-intensive or I/O intensive, largely hindering the overall system performance. Direct against the above potential system bottlenecks, this paper quantitatively analyzes the overhead of each main component introduced by de-duplication, and then proposes two performance optimization methods. The one is parallel calculation of content aware chunk identifiers, which fully utilizes the parallelism both inter and intra chunks by using a certain task partition and chunk content distribution algorithm. Experiments demonstrate that it can improve up to 150% of the system throughput, and at the same time much better utilize the multiprocessor resources. The other one is storage pipelining, which overlaps the CPU-bound, I/O-bound and network communication tasks. Through a dedicated five-stage storage pipeline design for file archival operations, experimental results show that the system throughput can increase up to 25% according to our workloads.
[storage allocation, parallel calculation, data compression, performance optimization methods, Performance Optimization, Laboratories, Storage Pipeline, Optimization methods, Throughput, parallel programming, task partition algorithm, chunk content distribution algorithm, Pipeline processing, Computer science, Information science, Data De-duplication, System performance, data de-duplication, Parallel Hash, Cost function, Performance analysis, Cryptography, data-intensive storage systems]
Scalability of Macroblock-level Parallelism for H.264 Decoding
2009 15th International Conference on Parallel and Distributed Systems
None
2009
This paper investigates the scalability of macroblock (MB) level parallelization of the H.264 decoder for high definition (HD) applications. The study includes three parts. First, a formal model for predicting the maximum performance that can be obtained taking into account variable processing time of tasks and thread synchronization overhead. Second, an implementation on a real multiprocessor architecture including a comparison of different scheduling strategies and a profiling analysis for identifying the performance bottlenecks. Finally, a trace-driven simulation methodology has been used for identifying the opportunities of acceleration for removing the main bottlenecks. It includes the acceleration potential for the entropy decoding stage and thread synchronization and scheduling. Our study presents a quantitative analysis of the main bottlenecks of the application and estimates the acceleration levels that are required to make the MB-level parallel decoder scalable.
[Video coding, Codecs, Multicore processing, Scalability, entropy decoding stage, Decoding, video coding, H.264/AVC, Yarn, decoding, multiprocessor architecture, high definition applications, thread synchronization, thread scheduling, Video compression, Multicores, macroblock-level parallelism, Video CODEC Parallelization, Performance analysis, H.264 decoding, Acceleration, High definition video, Chip-multiprocessors, parallel scalability]
A Cache-Efficient Parallel Gauss-Seidel Solver with Alternating Tiling
2009 15th International Conference on Parallel and Distributed Systems
None
2009
We present a new cache-efficient parallel multilayer Gauss-Seidel algorithm to solve 2D diffusion equations on distributed memory machines, by focusing on improving its cache behaviour and parallelism simultaneously. The novelty of our parallel multi-layer algorithm lies in performing Gauss-Seidel in two alternating sweeping directions (with multiple layers, i.e., iterations per direction) and applying alternating tiling strategies in two opposite sweeping directions to the subdomain allocated to every processor. As a result, its efficiency comes from a significant reduction in two sources of overhead: data cache misses and communication costs. In comparison with two commonly used parallel Gauss-Seidel algorithms, our algorithm has good performance and scalability in a cluster computing environment.
[cache-efficient parallel Gauss-Seidel solver, loop parallelization, Gaussian distribution, multilayer Gauss-Seidel algorithm, cache storage, Convergence, Concurrent computing, cluster computing environment, Clustering algorithms, communication costs, Parallel processing, alternate tiling, alternating tiling, distributed memory machines, data cache misses, parallel algorithms, PDE solver, Pipeline processing, parallel multilayer algorithm, Tiles, Gaussian processes, distributed memory systems, Iterative algorithms, Australia, 2D diffusion equations]
Optimizing End-to-end Performance of Distributed Applications with Linear Computing Pipelines
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Supporting high-performance computing pipelines in wide-area networks is crucial to enabling large-scale distributed scientific applications that require minimizing end-to-end delay for fast user interaction or maximizing frame rate for smooth data flow. We formulate and categorize the linear pipeline configuration problems into six classes with two mapping objectives, i.e. minimum end-to-end delay and maximum frame rate, and three network constraints, i.e. no, contiguous, and arbitrary node reuse. We design a dynamic programming-based optimal solution to the problem of minimum end-to-end delay with arbitrary node reuse and prove the NP-completeness of the rest five problems, for each of which, a heuristic algorithm based on a similar optimization procedure is proposed. These heuristics are implemented and tested on a large set of simulated networks of various scales and their performance superiorities are illustrated by extensive experimental results in comparison with existing methods.
[Algorithm design and analysis, wide area networks, Heuristic algorithms, Pipelines, distributed processing, dynamic programming, NP-complete, Distributed computing, distributed applications, Delay, Design optimization, minimum end-to-end delay, end-to-end delay, frame rate, Computer applications, NP-completeness, Computer networks, Data flow computing, Large-scale systems, end-to-end performance, linear computing pipelines, data flow, optimization procedure, computational complexity]
DBS: A Bit-level Heuristic Packet Classification Algorithm for High Speed Network
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Packet classification is one of the most critical techniques in many network devices such as firewall, IDS and IPS, etc. In order to meet the performance requirement for high speed Internet (even higher than 10 Gbps), practical algorithms must keep better spatial and temporal performance. Moreover, as the size of rule set is increasing to tens of thousands, novel packet classification algorithms must have good scalability. In this paper, we propose a novel packet classification algorithm named DBS (discrete bit selection) which takes a bit level heuristic design to partition the rule set effectively. To the best of our knowledge, DBS is the first try to design a heuristic classification algorithm at bit-level. To evaluate the performance of our algorithm, DBS is deployed on a popular multi-core network processor platform, compared with two existing well-known algorithms. Experimental results show that DBS achieves 300% higher throughput than HiCuts and HSM, while the memory requirement is reduced to about 10% averagely. DBS works well especially with large rule set (10K), which trends a good scalability.
[Algorithm design and analysis, pattern classification, Heuristic algorithms, Scalability, multicore network processor platform, Satellite broadcasting, bit level heuristic design, bit-level heuristic packet classification algorithm, Throughput, discrete bit selection, high speed Internet, Classification algorithms, Partitioning algorithms, High-speed networks, Intrusion detection, Internet]
Network Interface Architecture for Scalable Message Queue Processing
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Most of scientists except computer scientists do not want to make efforts for performance tuning with rewriting their MPI applications. In addition, the number of processing elements which can be used by them is increasing year by year. On large-scale parallel systems, the number of accumulated messages on a message buffer tends to increase in some of their applications. Since searching message queue in MPI is time-consuming, system side scalable acceleration is needed for those systems. In this paper, a support function named LHS (Limited-length Head Separation) is proposed. Its performance in searching message buffer and hardware cost are evaluated. LHS accelerates searching message buffer by means of switching location to store limited-length heads of messages. It uses the effects such as increasing hit rate of cache on host with partial off-loading to hardware. Searching speed of message buffer when the order of message reception is different from the receiver's expectation is accelerated 14.3 times with LHS on FPGA-based network interface card (NIC) named DIMMnet-2. This absolute performance is 38.5 times higher than that of IBM BlueGene/P although the frequency is 8.5times slower than BlueGene/P. Hardware cost of LHS is significantly lower than that of ALPU, which is a hardware accelerator for searching message buffer. LHS has higher scalability than ALPU in the performance per frequency. Therefore, LHS is more suitable for larger parallel systems.
[Costs, Buffer storage, Scalability, field programmable gate arrays, MPI, network interface architecture, Network interfaces, parallel processing, scalability, DIMMnet-2 interface card, field programmable gate array, Computer architecture, Hardware, Large-scale systems, message passing, message passing interface, Architecture, NIC, message queue processing, Application software, limited-length head separation function, PC cluster, parallel systems, network interface card, Frequency, Acceleration]
Field-Based Branch Prediction for Packet Processing Engines
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Network processors have exploited many aspects of architecture design, such as employing multi-core, multi-threading and hardware accelerator, to support both the ever-increasing line rates and the higher complexity of network applications. Micro-architectural techniques like superscalar, deep pipeline and speculative execution provide an excellent method of improving performance without limiting either the scalability or flexibility, provided that the branch penalty is well controlled. However, it is difficult for traditional branch predictor to keep increasing the accuracy by using larger tables, due to the fewer variations in branch patterns of packet processing. To improve the prediction efficiency, we propose a flow-based prediction mechanism which caches the branch histories of packets with similar header fields, since they normally undergo the same execution path. For packets that cannot find a matching entry in the history table, a fallback gshare predictor is used to provide branch direction. Simulation results show that the our scheme achieves an average hit rate in excess of 97.5% on a selected set of network applications and real-life packet traces, with a similar chip area to the existing branch prediction architectures used in modern microprocessors.
[microcomputers, branch prediction, Scalability, Pipelines, packet switching, Quality of service, History, Engines, packet processing engines, Cyclic redundancy check, Design engineering, flow-based prediction mechanism, computer architecture, Hardware, microprocessors, network processors, network processor, fallback gshare predictor, cache, Application software, network applications, network traffic, packet branch histories, real-life packet traces, Multithreading, branch prediction architectures, packet flow, field-based branch prediction, microarchitectural techniques]
RankBoost Acceleration on both NVIDIA CUDA and ATI Stream Platforms
2009 15th International Conference on Parallel and Distributed Systems
None
2009
NVIDIA CUDA and ATI Stream are the two major general-purpose GPU (GPGPU) computing technologies. We implemented RankBoost, a web relevance ranking algorithm, on both NVIDIA CUDA and ATI Stream platforms to accelerate the algorithm and illustrate the differences between these two technologies. It shows that the performances of GPU programs are highly dependent on the utilization of GPU's hardware memory architectural features. In this work, we accelerated RankBoost algorithm on both platforms, and we achieved 22.9X speedup on CUDA and 9.2X speedup on ATI Stream respectively. Then we made a comparison on the differences of memory architecture between NVIDIA CUDA and ATI Stream.
[Machine learning algorithms, ATI Stream platforms, general-purpose GPU computing technology, Boosting, coprocessors, GPGPU, Concurrent computing, CUDA, relevance feedback, NVIDIA CUDA platforms, Clustering algorithms, Search engines, Parallel processing, RankBoost acceleration, Hardware, Large-scale systems, Internet, ATI Stream, Acceleration, Web relevance ranking, Web search, hardware memory architectural features]
Program Optimization of Array-Intensive SPEC2k Benchmarks on Multithreaded GPU Using CUDA and Brook+
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Graphic Processing Unit (GPU), with many light-weight data-parallel cores, can provide substantial parallel computing power to accelerate several general purpose applications. Both the AMD and NVIDIA corps provide their specific high performance GPUs and software platforms. As the floating-point computing capacity increases continually, the problem of ``memory-wall'' becomes more serious, especially for array-intensive applications. In this paper, we optimize and implement two SPEC2k benchmarks mgrid and swim on multithreaded GPU using CUDA and Brook+. In order to reduce the pressure on off-chip memory, we make use of data locality in multi-level memory hierarchies and hide long memory access latency via double-buffers. To balance inter-thread parallelism and intra-thread locality, we further tune thread granularity for each kernel and empirically study the best equilibrium point for this problem. Flow control instruction can significantly impact the effective instruction throughput. Oriented to this problem, we introduce a diverge elimination technology to convert condition expression into computing operation. Through all the optimizations, we gain the speedup of 10-34 to the CPU implementation on the GPUs of AMD and NVIDIA respectively. Finally, we summarize and compares the GPUs from AMD and NVIDIA in hardware and software.
[mgrid, graphic processing unit, memory access latency, Software performance, Throughput, coprocessors, Yarn, Delay, GPGPU, data locality, optimization, array intensive SPEC2k benchmark, memory wall problem, Parallel processing, intrathread locality, multithreaded GPU, lightweight data parallel core, Kernel, parallel computing, floating point computing, multi-threading, interthread parallelism, Brook+, multilevel memory hierarchy, Application software, program optimization, Graphics, CUDA, computer graphics, flow control instruction, off-chip memory pressure reduction, Central Processing Unit, Acceleration, swim]
Optimal Data Distribution for Versatile Finite Impulse Response Filtering on Next-Generation Graphics Hardware Using CUDA
2009 15th International Conference on Parallel and Distributed Systems
None
2009
In this paper, we investigate discrete finite impulse response (FIR) filtering of images, while harnessing the powerful computational resources of next-generation GPUs. These novel platforms exhibit a massive data parallel architecture with an advanced SIMT execution model and thread management, to enable designers to better cope with the infamous memory wall, i.e. the growing gap between the cost of data communication and computational processing. However, the concerning platforms still have hard constraints that prevent trivial optimization of convolution filtering. Although automatic (compiler) optimization is available, we investigate and explain the speedup potential considering manual intervention, given the context of FIR kernels. Furthermore, we present multiple convolution implementation techniques that are able to cope with the hard platform constraints in different situations, while still being able to optimize the implementation to the underlying architecture. Utilizing the acquired insights, a view is given on the impact for possible optimization when loosening these hard constraints in the near future.
[massive data parallel architecture, Costs, parallel architectures, data distribution, coprocessors, Yarn, Constraint optimization, FIR filters, Convolution, FIR kernel, convolution, Hardware, compiler optimization, optimal data distribution, thread management, multiple convolution, Filtering, Parallel architectures, FIR, SIMT execution model, Graphics, CUDA, computer graphics, next-generation graphics hardware, Memory management, Finite impulse response filter, finite impulse response filtering]
Pancyclicity and Panconnectivity in Augmented k-ary n-cubes
2009 15th International Conference on Parallel and Distributed Systems
None
2009
The augmented k-ary n-cube AQ<sub>n,k</sub> is a recently proposed interconnection network that incorporates an extension of a k-ary n-cube Q<sub>n</sub> k inspired by the extension of a hypercube Q<sub>n</sub> to the augmented hypercube AQ<sub>n</sub> (as developed by Choudom and Sunita). We extend a recent topological investigation of augmented k-ary n-cubes by proving that any augmented k-ary n-cube AQ<sub>n,k</sub> is edge-pancyclic and that AQ<sub>2,k</sub> is panconnected.
[Costs, Hamming distance, Multiprocessor interconnection networks, augmented k-ary n-cubes, graph theory, edge-pancyclic, augmented hypercube networks, pancyclicity, Routing, hypercube networks, Mathematics, panconnectivity, interconnection networks, Computer science, interconnection network, augmented k-ary n-cube, Parallel processing, Broadcasting, Hypercubes, parallel computing]
A Fault-Free Unicast Algorithm in Twisted Cubes with the Restricted Faulty Node Set
2009 15th International Conference on Parallel and Distributed Systems
None
2009
The dimensions of twisted cubes in the original definition of twisted cubes are only limited to odd integers. In this paper, we first extend the dimensions of twisted cubes to all the positive integers. Then, we introduce the concept of the set of restricted faulty nodes into twisted cubes. We further prove that under the condition that each node of the n-dimensional twisted cube TQ<sub>n</sub> has at least one fault-free neighbor its restricted connectivity is 2n - 2, which is almost as twice as that of TQ<sub>n</sub> under the condition of arbitrary faulty nodes, the same as that of the n-dimensional hypercube. Moreover, we give an O(N log N) fault-free unicast algorithm, where N denotes the node number of TQ<sub>n-1</sub>. Finally, we give the simulation result of the expected length of the fault-free path gotten by our algorithm.
[fault diagnosis, positive integer, Multiprocessor interconnection networks, graph theory, hypercube networks, unicast, twisted cubes, fault-free unicast algorithm, Concurrent computing, Unicast, Computer network reliability, Hypercubes, Polynomials, Data communication, hypercube, restricted faulty node set, set of restricted faulty nodes, fault-free path, fault-free, Computer science, Binary trees, Connectivity, Telecommunication network reliability, odd integer, computational complexity]
Routing, Broadcasting, Prefix Sums, and Sorting Algorithms on the Arrangement Graph
2009 15th International Conference on Parallel and Distributed Systems
None
2009
The (n, k)-arrangement graph A<sub>n,k</sub> is a generalization of the well known star graph and the alternating group graph. We first present a constant time routing algorithm that allows two groups of A<sub>n-1,k-1</sub> 's to exchange their data in a one-to-one fashion. We then use this routing algorithm to develop an optimal broadcasting algorithm, an optimal algorithm for computing the general prefix sums as well as an efficient sorting algorithm on the arrangement graph. Consequently, all of our algorithms are applicable to the star and the alternating group graphs.
[Multiprocessor interconnection networks, Scalability, network routing, multiprocessor interconnection networks, Routing, sorting algorithms, Sorting, broadcasting, Computer science, Fault tolerance, prefix sums, routing, optimal broadcasting algorithm, Tree graphs, Network topology, star graph, interconnection network, routing algorithm, sorting, Broadcasting, Hypercubes, arrangement graph]
A Functional Classification Based Inter-VM Communication Mechanism with Multi-core Platform
2009 15th International Conference on Parallel and Distributed Systems
None
2009
With the resurgence of virtualization technologies and the development of multi-core technologies, the combination of the two becomes a trend. Therefore, inter-VM communication becomes a key part in how to improve the performance of virtual machines (VMs) basing on multi-core platform. In this paper, we first analyze the characteristics of multi-core tasks and the properties of virtual machine environment, and then classify processor cores into two categories basing on their different functions. According to the classification, we design an inter-VM communication mechanism with multi-core platform. It discards the traditional communication path between VMs which needs to via a trusted VM, sets up communication channels between virtual CPUs in different VMs and uses shared memory space to implement high-throughput communication of inter-VM. Experiment results have proved the efficiency of them.
[multicore platform, Platform virtualization, multiprocessing systems, interVM communication mechanism, virtualization, Microcomputers, Virtual machining, Application software, virtual machine environment, Voice mail, high-throughput communication, Virtual machine monitors, Space technology, virtualization technologies, virtual machines, Isolation technology, multicore technologies, Hardware, functional classification, Virtual manufacturing, communication]
PaS: A Preemption-aware Scheduling Interface for Improving Interactive Performance in Consolidated Virtual Machine Environment
2009 15th International Conference on Parallel and Distributed Systems
None
2009
As virtualization technology is used widely in cloud computing, there are more and more interactive workloads being deployed on virtual machine (VM) environment. Although improving interactive performance has been heavily studied in operating system area, in consolidated VM environment, the improvements of guest OS are usually offset by the more coarse-grained VM scheduler, which may cause poor interactive performance. The guest OS scheduler and VM scheduler are totally independent with each other, which leads to the so called 'semantic gap'. To reduce this semantic gap, this paper presents PaS (Preemption-aware Scheduling) as an extension of VM scheduling interface. PaS introduces only two interfaces: one to register VM preemption conditions, the other to check if a VM is preempting. Thanks to the sophisticated techniques of interactive-process identification and optimization in traditional OS, it is trivial for guest OS to use the new interfaces: only 10 lines of code are added into Linux 2.6.18.8. The evaluation results show that PaS can significantly improve the interactive performance of consolidated VMs while keeping the fairness and performance isolation.
[Cloud computing, preemption-aware scheduling interface, semantic gap, interactive performance improvement, Registers, Voice mail, Delay, resource allocation, Microprocessors, virtualization technology, operating system scheduler, scheduling, Virtual manufacturing, cloud computing, Kernel, operating system kernels, interactive-process identification, Virtual machining, interactive workload, consolidated virtual machine environment, Processor scheduling, virtual machine, Linux, virtual machines, interactive performance, performance isolation]
CFS Optimizations to KVM Threads on Multi-Core Environment
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Multi-core architecture provides more on-chip parallelism and powerful computational capability. It helps virtualization achieve scalable performance. KVM (kernel based virtual machine) is different from other virtualization solutions which can make use of the Linux kernel components such as completely fair scheduler (CFS). However, CFS treats the KVM threads as normal tasks without considering about their unique features such as thread allocation mechanism and lock inside guest virtual machine, which may harm the KVM virtualization performance. In this paper, we analyze a phenomenon that some guest multi-threaded applications have very low performance when scheduled by CFS. As a solution to this problem, we introduce two kinds of optimizations in CFS: (1) configuration optimizations (2) lock optimizations. Our contributions are: (1) implement 5 original and 2 newest proposed optimizations in the newest Linux kernel. (2) Classify and compare them, a brief analysis is also given. They are all very simple and general to other virtual machine monitors such as Xen and schedulers as O(1). The performance of our CFS optimizations to KVM threads is measured by running some well-known benchmarks in two guest virtual machines on an 8-core server which models the real world applications. The results indicate our scheduling optimizations can improve the overall system performance. This paper can provide useful advices to KVM developers and virtualization data center administrators.
[Xen, KVM, parallel architectures, Multi-Core, 8-core server, Yarn, Concurrent computing, lock optimizations, Computer architecture, Parallel processing, scheduling, Performance analysis, Kernel, Lock Holder Preemption, completely fair scheduler optimization, operating system kernels, multi-threading, on-chip parallelism, multicore architecture, Virtual machining, Scheduling, configuration management, Virtual machine monitors, Processor scheduling, Linux, kernel based virtual machine threads, virtual machines, Linux kernel components, scheduling optimizations, multithreaded applications, Performance, Virtualization, configuration optimizations]
Optimizing Inter-Domain Communication
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Virtual machine technology has played an important role in data center. Distributed services deployed in multiple virtual machines, may reside on one physical machine. This situation requires an efficient inter-domain communication channel with transparency and security principles ensured. Although current inter-domain mechanism has gained a much better performance compared to traditional inter-domain path offered by hypervisor, shared data channel size limitation and additional copy are still two restrains against a higher performance efficiency. In this paper, we give an analysis on these limitations, overcome these shortcomings, and achieve a higher efficient inter-domain communication channel. By applying virtual address protection mechanism during channel bootstrap, we enlarge the maximum size of shared data channel. By pointing network packet structure to the buffer in the shared data channel, we avoid an extra data copy on the receiver VM side. In our evaluation using a number of standard benchmarks, we have reduced the latency by nearly 40%, increased the throughput by approximately 45% and cut down more than 3500 CPU cycles per packet.
[Xen, Costs, Protocols, Data security, channel bootstrap, Virtual machine, Throughput, Inter-domain communication, Virtual machining, Voice mail, Virtual machine monitors, virtual machine, computer bootstrapping, Communication channels, virtual machines, hypervisor, distributed shared memory systems, Hardware, Virtual manufacturing, interdomain communication channel, distributed services, virtual address protection mechanism]
ERN: Emergence Rescue Navigation with Wireless Sensor Networks
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Navigation with wireless sensor networks (WSNs) can help people escape safely from an emergency. Previous navigation algorithms attempt to find safe and efficient escape paths for individuals under various environmental dynamics but ignore possible congestion caused by the individuals rushing for the exits. Moreover, all the previous works have overlooked the fact that the emergency rescue force can take actions strategically in order to save people out of danger. We propose ERN, Emergence Rescue Navigation algorithm by treating WSNs as navigation infrastructure. ERN takes both pedestrian congestion and rescue force flexibility into account. A directed graph is used to model the emergency regions. Human's movements are regarded as network flows on the graph. By calculating the maximum flow and minimum cut on the graph, the system can provide firemen rescue commands to eliminate key dangerous areas, which may significantly reduce congestion and save trapped people. We have performed extensive simulations under dynamic environments to evaluate the effectiveness and response time of ERN. Simulation results show that with ERN people in emergency are evacuated much faster and less congestion is observed.
[Performance evaluation, Navigation, wireless sensor networks, Roads, Laboratories, graph theory, emergence rescue navigation, Sensor systems, cyber-physical system, Software safety, pedestrian congestion, Delay, Hazardous areas, Wireless sensor networks, navigation, rescue force flexibility, Fires, directed graph, firemen rescue commands, emergency services]
Traffic-Known Urban Vehicular Route Prediction Based on Partial Mobility Patterns
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Travel route analysis and prediction are essential for the success of many applications in Vehicular Ad-hoc Networks (VANETs). Yet it is quit challenging to make accuracy route prediction for general vehicles in urban settings due to several practical issues such as very complicated traffic networks, the highly dynamic real-time traffic conditions and their interaction with drivers' route selections. In this paper, we undertake a systematic study on the vehicular route prediction in urban environments where the traffic conditions on complicated road networks keep changing from time to time. Inspired by the observation that a vehicle often has its own route selection flavor when traversing between its sources and destinations, we define a mobility pattern as a consecutive series of road segment selections that exhibit frequent appearance along all the itineraries of the vehicle. We further leverage Variable-order Markov Models (VMMs) to mine mobility patterns from the real taxi GPS trace data collected in Shanghai. In addition, considering the tremendous impact of dynamic traffic conditions to the accuracy of route prediction, we deploy multiple VMMs differentiating different traffic conditions in daytime. Our extensive trace-driven simulation results show that notable patterns can be mined from routes of common vehicles though they usually have no constraints when selecting routes. Given a specific taxi, around 40% next road segments are predictable using our model with a confidence weight of 60%. With multiple VMMs a high route prediction accuracy is achievable from the real traffic trace.
[Vehicular Ad-hoc Networks, vehicular ad-hoc networks, Telecommunication traffic, Predictive models, vehicles, drivers' route selections, Vehicle dynamics, traffic-known urban vehicular route prediction, Accuracy, Road vehicles, Traffic control, taxi GPS trace data, mobility pattern, travel route analysis, mobile radio, Vehicle driving, Shanghai, VANET, Ad hoc networks, Global Positioning System, route prediction, Virtual machine monitors, partial mobility patterns, telecommunication network routing, Markov processes, variable-order Markov models, ad hoc networks, Variable-order Markov Models, telecommunication traffic]
An Optimized String Transformation Algorithm for Real-Time Group Editors
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Operational transformation (OT) is an optimistic consistency control method for supporting collaboration over high-latency networks. The technique lies in the heart of many recent products such as Google Wave. It replicates the shared data and allows the users to concurrently modify any part of a shared document in a nonblocking manner. Most of the published results only support two characterwise primitives and take O(|H|2) or even longer time to integrate one remote characterwise operation, where H is the operation history. However, as the history grows long and operations are integrated in batches, the high complexity can make an algorithm easily exceed the 100 ms responsiveness threshold that is critical for interactive applications. This paper proposes a new OT algorithm that supports string primitives and reduces the time complexity to O(|H|). The result can be used in a range of parallel and distributed applications that can be abstracted as realtime group editors.
[Real time systems, Heart, Availability, operational transformation, collaborative systems, Optimization methods, time complexity, Control systems, International collaboration, History, group editing, optimized string transformation algorithm, Delay, real-time group editors, data consistency, Computer science, Google Wave, groupware, data replication technique, Collaborative work, data handling, optimistic consistency control method, computational complexity]
Modified Simultaneous Algebraic Reconstruction Technique and its Parallelization in Cryo-electron Tomography
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Three-dimensional reconstruction of cryo-electron tomography (cryo-ET) has emerged as the leading technique in analyzing structures of complex pleomorphic cellulars. A classical iterative method, simultaneous algebraic reconstruction technique (SART), has been employed to reconstruct volume images in cryo-ET. However, SART starts with an arbitrary approximation and takes into account only a weighted factor when updating density value in every error-correction iterative procedure, thus limits the improvement of the reconstruction resolution. Facing these problems, we present a modified simultaneous algebraic reconstruction technique (MSART) which applies several key techniques, a back projection technique (BPT) and an adaptive adjustment of corrections. Experimental results show that MSART can improve significantly the quality of reconstruction. Additionally, in order to address the computational requirements demanded by the reconstruction of large volumes, we have presented and implanted a strategy to parallel the MSART algorithm on DAWNING 4000H cluster system, and obtained a good computational performance.
[iterative methods, cellular biophysics, parallel algorithm, 3D reconstruction, Fault tolerant systems, Prototypes, tomography, Tomography, electron microscopy, back projection technique, Hardware, cryo-electron tomography, Image storage, Protection, medical image processing, simultaneous algebraic reconstruction technique, volume image reconstruction, error correction, modified simultaneous algebraic reconstruction technique (MSART), adaptive correction adjustment, error-correction iterative method, Virtual machining, image reconstruction, iterative method, complex pleomorphic cellular structure, Computer science, Virtual machine monitors, stereo image processing, Frequency synchronization]
An Evaluation of Communication Factors on an Adaptive Control Strategy for Job Co-allocation in Multiple HPC Clusters
2009 15th International Conference on Parallel and Distributed Systems
None
2009
To more effectively use a network of high performance computing clusters, allocating multi-process jobs across multiple connected clusters, i.e., job co-allocation, offers the possibility of more efficient use of computer resources, reduced turn-around time and computations using numbers of processes larger than processors on any single cluster. Effective co-allocation, ultimately, depends on the inter-cluster communication cost. We previously introduced a scalable co-allocation strategy - maximum bandwidth adjacent cluster set (MBAS) strategy. It made use of two thresholds to control job co-allocation - one dealing with inter-cluster links and one controlling job partitioning. We subsequently introduced the adaptive threshold control system (ATCS), which used a fuzzy control approach to dynamically adjust these thresholds within MBAS. Results suggested that using ATCS during MBAS job co-allocation could achieve an overall performance improvement. However, these results only considered jobs that involved either master-slave or all-all communications among constituent processes. In this paper, we extend this analysis by also considering jobs that exhibit 2D-mesh communication patterns and evaluate ATCS further.
[workstation clusters, resource management, Costs, Adaptive systems, Communication system control, maximum bandwidth adjacent cluster set, Control systems, job co-allocation, intercluster link, communication factor, Adaptive control, fuzzy control, scalable coallocation strategy, Bandwidth, Computer networks, job coallocation, multiple HPC cluster, telecommunication control, adaptive threshold control system, adaptive control, job partitioning, 2D-mesh communication pattern, Programmable control, High performance computing, high-performance computing clusters, high performance computing, Resource management]
Real-time Task Assignment with Replication on Multiprocessor Platforms
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Fault tolerance is a very important aspect in critical real-time task scheduling. On multiprocessor systems, executing tasks with replication provides an additional reliability to resist potential processor failures and computing faults. For assigning real-time tasks on such systems, there must be requirements that all tasks assigned on the system meet their timing constraints, and all replicas of the same task are assigned to distinct processors. Obviously, such a reliability requirement could overload the system. In these situations, how to assign the tasks on processors to achieve the highest benefit poses a challenge. In this paper, we consider the problem of maximizing the number of successfully assigned tasks on a homogeneous distributed multiprocessor system, while satisfying the real-time constraint and system reliability requirement. Exact, greedy approximation and polynomial time approximation scheme (PTAS) algorithms are developed for the problem. Theoretical analysis, necessary proofs and experimental results that support our claims are all given.
[Real time systems, task analysis, Distributed computing, processor scheduling, Multiprocessing systems, Fault tolerance, homogeneous distributed multiprocessor system, critical real-time task scheduling, approximation theory, multiprocessing systems, fault tolerance, greedy algorithms, real-time task assignment, Partitioning algorithms, Application software, Scheduling algorithm, multiprocessor platforms, system reliability requirement, timing constraints, real-time systems, polynomial time approximation scheme algorithm, Approximation algorithms, fault tolerant computing, greedy approximation, Timing, Reliability, real-time constraint, computational complexity]
A Heuristic Energy-aware Scheduling Algorithm for Heterogeneous Clusters
2009 15th International Conference on Parallel and Distributed Systems
None
2009
With the rapid development of supercomputers, the power consumption by large scale computer systems has become a big concern. How to reduce the power consumption is now a critical issue in designing high performance computers. Energy-aware scheduling for large scale clusters, especially the high performance heterogeneous ones, is one of the strategies for energy saving. Proposed in this paper is a novel energy-aware task scheduling algorithm (EAMM) for heterogeneous clusters, which is based on the general adaptive scheduling heuristics min-min algorithm. The algorithm is evaluated on a simulated heterogeneous cluster. The experiment results show that the new energy-aware algorithm can achieve a good time-energy trade-off and outperform the original min-min algorithm under various conditions.
[Algorithm design and analysis, workstation clusters, Energy consumption, supercomputer, large scale computer system, energy-aware task scheduling, parallel machines, power consumption, power aware computing, min-min algorithm, Clustering algorithms, scheduling, adaptive scheduling heuristics, Large-scale systems, Mathematical model, heuristic energy-aware scheduling, Supercomputers, Application software, energy saving, Scheduling algorithm, large scale clusters, Processor scheduling, High performance computing, heterogeneous clusters, high performance computer]
Data-bandwidth-aware Job Scheduling in Grid and Cluster Environments
2009 15th International Conference on Parallel and Distributed Systems
None
2009
This paper introduces techniques in scheduling jobs on a master/workers platform where the bandwidth is shared by all workers. The goal is to minimize the total makespan. The jobs are independent and each job requires a fixed amount of bandwidth to download input data before execution. The master can communicate with multiple workers simultaneously, provided that the bandwidth used by the master and the workers do not exceed their bandwidth limits. We proposed two models for this limited-bandwidth problem. If the data transfer cannot be interrupted, then we prove that the scheduling problem is NP-complete. Nevertheless we propose heuristic algorithms and experimentally test their performance. If the data transfer can be interrupted, we propose an algorithm that produces optimal makespan. The algorithm is based on a binary search on the completion time, and an efficient feasibility verification process for a given completion time.
[workstation clusters, cluster, grid computing, Biomedical computing, Data engineering, processor scheduling, grid environments, makespan minimization, limited-bandwidth problem, Bandwidth, Grid computing, Computer networks, search problems, Load modeling, Wide area networks, binary search, data-bandwidth-aware job scheduling, Job shop scheduling, cluster environments, heuristic algorithms, communication bandwidth, Job scheduling, NP-complete problem, master/workers platform, Computer science, bandwidth allocation, Processor scheduling, grid, data transfer, computational complexity]
Efficient Update Propagation by Speculating Replica Locations on Peer-to-Peer Networks
2009 15th International Conference on Parallel and Distributed Systems
None
2009
As demand for high fidelity multimedia content has soared, content distribution has emerged as a critical application. Large multimedia files require effective content distribution services such as content distribution networks (CDNs). A recent trend in CDN development is the use of peer-to-peer (P2P) techniques to enhance scalability, fault resilience, and cost-effectiveness. Unfortunately, P2P-based content distribution poses a crucial problem in that update propagation is quite difficult to accomplish. This is because peers cannot obtain a global view of replica locations on the network. In this paper, we propose speculative update, which quickly propagates an update to all replicas in a pure P2P fashion. Each server attempts to determine the directions in which there will be replicas with high probability based on server's local state used for replica repositioning. Then, it relays update messages speculatively in those directions. Simulation results demonstrate that our mechanism propagates an update to all replicas faster than the current pure P2P-based approaches.
[Protocols, content distribution services, replica repositioning, peer-to-peer computing, replicated databases, Peer to peer computing, Scalability, peer-to-peer networks, multimedia files, fault resilience, multimedia computing, Relays, Delay, Resilience, Computer science, Network servers, update propagation, Internet, high fidelity multimedia content, content distribution networks, Web server, replica locations]
A Study of Group Size Effects on a Hybrid P2P System
2009 15th International Conference on Parallel and Distributed Systems
None
2009
P2P network is the one of the most important application models in Internet. Numerous structured and unstructured P2P models have been proposed in the last ten years, and both models do have distinctive advantages and disadvantages. We proposed a hybrid model to strike a balance between these two models using peer grouping. Since the size of peer groups is essential to performance, we analyze the effect of group size on the maintenance cost, which is measured in terms of the number of maintenance messages. Experimental results suggest that the group size obtained from our theoretical analysis is very close to the actual best group size obtained from simulation, and the hybrid model uses less messages in maintaining a P2P network than Chord does.
[peer-to-peer computing, Peer to peer computing, P2P network, Telecommunication traffic, hybrid P2P system, Control systems, Application software, Centralized control, Computer science, Network topology, group size effects, application model, peer grouping, P2P systems, Load management, Performance analysis, Internet, IP networks]
Papnet: A Proximity-aware Alphanumeric Overlay Supporting Ganesan On-Line Load Balancing
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Many structured Peer-to-Peer-Systems (P2P) have been developed over the past years. However, most of them rely on hash-functions and thus put major restrictions on applications being implemented on top of them. In this paper we present a very detailed description of Papnet, a hash-free P2P overlay-network that supports range-queries and realizes an infinite alphanumeric address space that can be used to store arbitrarily skewed data. We show how Papnet is able to distribute load amongst its nodes using the Ganesan On-Line Load Balancing providing a constant imbalance ratio while still being able to perform proximity routing, reaching each participating node with a latency being only roughly twice the direct latency. Further, we introduce a join-algorithm that provides Papnet with high fault-tolerance properties. We evaluated Papnet in a real distributed environment by setting up a network consisting of 50,000 nodes.
[constant imbalance ratio, Alphanumerical Overlay, high fault tolerance properties, proximity aware alphanumeric overlay, peer-to-peer-systems, Load Balancing, Delay, proximity routing, resource allocation, store arbitrarily skewed data, Intrusion detection, hash-free P2P overlay-network, Ganesan online load balancing, supports range queries, peer-to-peer computing, Peer to peer computing, Range-Queries, hash-free, Routing, Peer-to-Peer, computer network security, P2P, Papnet, Load management, participating node latency]
Towards Keyword Search over Relational Databases in DHT Networks
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Recent research has shown that keyword search is a friendly and potentially effective way to retrieve information of interest over relational databases. Existing work has generally focused on implementing keyword search in centralized databases. This paper addresses keyword search over distributed databases. We adopts distributed hash tables (DHTs) - a peer-to-peer inspired overlay network technology - as the infrastructure to implement keyword search over relational databases. For this end, we combine IR-based ranking techniques with a P2P-based indexing strategy, and propose an effective approach. Extensive experiments over real-world datasets show that our approach is effective and efficient.
[peer-to-peer networks, Relational databases, keyword search, Peer-to-Peer Networks, query processing, Distributed Hash Tables, Intelligent networks, IR-based ranking techniques, distributed databases, P2P-based indexing strategy, peer-to-peer computing, Peer to peer computing, Keyword search, DHT networks, overlay network technology, information retrieval, Information retrieval, cryptography, relational database, relational databases, Computer science, Query processing, distributed hash table, Relational Databases, Internet, Deductive databases, Keyword Searching, Indexing]
Reliable Software Distributed Shared Memory Using Page Migration
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Reliability has recently become an important issue in PC cluster technology. This research proposes a software distributed shared memory system, named SCASH-FT, as an execution platform for high performance and highly reliable parallel system for commodity PC clusters. To achieve fault tolerance, each node has redundant page data that allows recovery from node failure using SCASH-FT. All page data is checkpointed and duplicated to another node when a user explicitly calls the checkpoint function. When failure occurs, SCASH-FT invokes the rollback function by restarting an execution from the last checkpoint data. SCASH-FT takes charge of processes such as detecting failure and restarting execution. So, all you have to do is just adding checkpoint function calls in the source code to determine the timing of each checkpoint. Evaluation results show that the checkpoint cost and the rollback penalty depend on the data access pattern and the checkpoint frequency. Thus, users can control their application performance by adjusting checkpoint frequency.
[checkpointing, fault-tolerant systetem, rollback penalty, software distributed shared memory, software reliability, Software performance, Reliability engineering, execution platform, Distributed computing, Fault tolerance, data access pattern, reliable software distributed shared memory, Cost function, Functional programming, checkpoint function calls, parallel computing, reliable parallel system, node failure recovery, fault tolerance, checkpoint frequency, rollback function, checkpoint data, Application software, page data checkpointing, PC cluster technology, High performance computing, distributed memory systems, Frequency, Software systems, fault tolerant computing, page migration, redundant page data]
MPI-NeTSim: A Network Simulation Module for MPI
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Programs that execute in parallel across a network often use the message passing interface (MPI) library for communication. The network requirements of an MPI program are often unclear because of the difficulty in exploring alternative network configurations as well as obtaining packet level information about the communication. MPI-NeTSim is an execution environment to emulate MPI programs on simulated networks to allow users to better explore the impact of the network on the performance of MPI programs. We describe the design of MPI-NeTSim and the integration of OMNeT++'s INET framework into MPICH2's MPI middleware. We introduce a novel technique for uniformly slowing down the execution of the system to allow the discrete event network simulator to keep up with the execution and provide a consistent view of the communication. We validate our technique with synthetic programs as well as the standard NAS benchmarks. We demonstrate MPI-NeTSim's usefulness in analyzing the effect of the network on communication by using our environment to study the impact of a slow-link on the NAS benchmarks.
[Transport protocols, simulation, MPI, Discrete event simulation, slowdown, Delay, network configuration, Bandwidth, Permission, MPI-NeTSim, Computer networks, Hardware, network simulation, discrete event simulation, middleware, MPI program, MPI middleware, message passing, message passing interface, Computational modeling, OMNeT++ INET framework, discrete event network simulator, emulation, SCTP, Middleware, packet level information, MPI library, NAS benchmark, Message passing]
Accelerating MapReduce with Distributed Memory Cache
2009 15th International Conference on Parallel and Distributed Systems
None
2009
MapReduce is a partition-based parallel programming model and framework enabling easy development of scalable parallel programs on clusters of commodity machines. In order to make time-intensive applications benefit from MapReduce on small scale clusters, this paper proposes a new method to improve the performance of MapReduce by using distributed memory cache as a high speed access between map tasks and reduce tasks. Map outputs sent to the distributed memory cache can be gotten by reduce tasks as soon as possible. Experiment results show that our prototype's performance is much better than that of the original on small scale clusters. To our knowledge, this is the first effort to accelerate MapReduce with the help of distributed memory cache.
[Computers, partition-based parallel programming model, high speed access, cache storage, cluster computing, Distributed computing, Programming profession, Delay, parallel programming, MapReduce, Concurrent computing, map tasks, scalable parallel programs, Fault tolerance, Parallel programming, distributed memory cache, Prototypes, reduce tasks, distributed memory systems, Large-scale systems, Acceleration]
Quantification of Security for Compute Intensive Workloads in Clouds
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Cloud computing is a promising technology to facilitate development of large-scale, on-demand, flexible computing infrastructures. However, improving dependability of cloud computing is critical for realization of its potential. In this paper, we describe our efforts to quantify security for Clouds to facilitate provision of assurance for quality of service, one of the factors contributing to dependability. This has profound implications for delivering customized security solutions such as effective intrusion prevention and detection which is the overall objective of our research. In order to demonstrate the applicability of our research, we have incorporated these requirements in the resource acquisition phase for Clouds. We also present experiments to demonstrate the effectiveness of our approach to address the random migration problem for virtualized computing environments.
[Cloud computing, cloud security, Memory, Quality of service, Virtual machining, intrusion detection, quality of service, Security, Distributed computing, random migration problem, Concurrent computing, resource acquisition phase, security of data, High performance computing, virtualized computing environments, Grid computing, intrusion prevention, Large-scale systems, Internet, cloud computing]
Energy-Efficient Sensor Data Acquisition Based on Periodic Patterns
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Wireless sensor networks have received considerable attention in recent years and played an important role in data collection applications. Sensor nodes usually have limited supply of energy. Therefore, a major consideration for developing sensor network applications is to conserve the energy for sensor nodes. In this paper, we propose a novel energy-efficient data acquisition algorithm based on the periodic patterns derived from past sensor readings. Our key observation is that sensor readings often exhibit periodic patterns, e.g., the daily cycle of temperature readings, and the patterns provide opportunities for reducing energy consumption for sensor data acquisition. We exploit the patterns and use the patterns to build a statistic model for predicting sensor readings. In our approach, sensor data acquisition is needed only when acquired readings are unpredictable. Therefore the energy for sensor data acquisition and the associated radio communications can be conserved. The experiments performed with real data validate the effectiveness and efficiency of our approach.
[Acquisitions, Energy consumption, Base stations, Voltage measurement, Costs, wireless sensor networks, data collection application, Data acquisition, Sensor Networks, wireless sensor network, Sensor systems, Data, periodic pattern prediction, Temperature sensors, power aware computing, Particle measurements, Energy efficiency, data acquisition, Query Processing, energy efficient sensor data acquisition, energy consumption reduction, Monitoring]
tk-coverage: Time-Based K-Coverage for Energy Efficient Monitoring
2009 15th International Conference on Parallel and Distributed Systems
None
2009
K-coverage is a classic issue in wireless sensor network (WSN) deployment. Existing works typically assumes that every position in the monitoring field is covered by at least k sensor nodes at any given time. This may not always be necessary because certain events to be monitored will last only for a short period of time. Based upon such observation, we include the time dimension in the original k-coverage problem, and denote it as tk-coverage In the context of tk-coverage, sensor nodes can apply periodical sleeping strategies to save energy use. A corresponding tk-coverage (TKC) model is proposed to analyze the energy consumption and detection delay., The proposed tk-coverage can balance energy consumption and detection delay by adjusting the sleeping strategies of sensor nodes. Comprehensive simulations have been conducted to validate the effectiveness and demonstrate the efficiency of this solution.
[coverage, Energy consumption, energy efficient monitoring, Event detection, wireless sensor networks, time-based coverage, Delay effects, Humans, wireless sensor network, Sensor systems, Discrete event simulation, Bridges, Wireless sensor networks, WSN, time-based K-coverage, Energy efficiency, tk-coverage, Monitoring]
Energy Minimization and Latency Hiding for Heterogeneous Parallel Memory
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Many high-performance DSP processors employ multi-module on-chip memory to improve performance and power consumption. This paper studies the scheduling and assignment problem that minimizes the total energy while satisfying performance for applications with loops. An algorithm, LSAMEM (Loop Scheduling and Assignment to Minimize Energy for Memory), is proposed. The algorithm attempts to maximum energy saving while satisfying timing constraint with guaranteed probability. The experimental results show that the average improvement on energy-saving is significant by using LSAMEM.
[Real time systems, parallel memories, heterogeneous parallel memory, Energy consumption, memory, timing constraint, assignment problem, heterogeneous, Delay, processor scheduling, power consumption, loop scheduling and assignment to minimize energy for memory, Embedded system, parallel, multimodule on-chip memory, Memory architecture, probability, DSP processors, energy saving, Scheduling algorithm, Processor scheduling, Signal processing algorithms, Digital signal processing, latency hiding, Timing, LSAMEM, minimisation, guaranteed probability, digital signal processing chips, energy minimization]
A Power-Aware Routing Protocol with Signal Strength Variation in Mobile Ad Hoc Networks
2009 15th International Conference on Parallel and Distributed Systems
None
2009
In this paper, we proposed bandwidth based power-aware routing protocol with signal strength detection instead of using GPS. In our proposed routing protocol, we used the variation of received signal strength to predict transmission bandwidth and lifetime of a link. Accordingly, the possible data transmission amount and remaining power after data transmission can be predicted. With the prediction of transmission data amount and remaining power after data transmission, we can design a bandwidth-based power-aware routing protocol to have power efficiency and prolong the network lifetime.
[Energy consumption, Costs, mobile radio, remaining power, bandwidth, Batteries, signal strength variation, GPS, Mobile ad hoc networks, Global Positioning System, power-aware routing protocol, signal strength detection, Computer science, signal strength, routing protocols, mobile ad hoc networks, Bandwidth, data transmission, Routing protocols, Data communication, ad hoc networks, transmission bandwidth prediction, Power engineering and energy, power efficiency]
Multiprocessor System-on-Chip Profiling Architecture: Design and Implementation
2009 15th International Conference on Parallel and Distributed Systems
None
2009
With the growing needs for advanced functionalities in modern embedded systems, it is now necessary to integrate multiple processors in the system, preferably on a single chip, to support the required computing complexity. The problem is that such multiprocessor system-on-chip (MPSoC) architecture is very complex and its internal behavior is very difficult to track. An effective tool for profiling the behavior of the MPSoC system is in great need. Such a tool is very useful during system design for exploiting various options and identifying potential bottlenecks. In this paper, we introduce the multiprocessor profiling architecture (MPPA) - a general framework for profiling MPSoC embedded systems. The MPPA framework entails the use of FPGA emulation for the target system, the embedding of performance counters for recording system events, and the development of OS drivers for collecting the profiled data. To demonstrate its use, we show the implementation of an MPSoC emulation system based on Leon3 cores following the MPPA framework. We also show how the MPPA framework and the emulator help the designers to identify performance problems and improve their MPSoC embedded system design.
[field programmable gate arrays, multiple processor integration, Multiprocessing systems, Counting circuits, Operating systems, Embedded system, Emulation, Prototypes, embedded systems, Computer architecture, FPGA emulation, design, Hardware, multiprocessor system-on-chip profiling architecture, Monitoring, architecture, Leon3 cores, MPSoC, microprocessor chips, monitor, MPSoC architecture, profiling, multiprocessor, system-on-chip, Field programmable gate arrays]
Exploiting Parallelism through High Level Optimization on a Heterogeneous Multicore SoC
2009 15th International Conference on Parallel and Distributed Systems
None
2009
This paper describes a heterogeneous multicore SoC named EVMP-SoC, which is composed of a RISC host processor and two minor-different SIMD synergistic processor that are specially optimized for embedded visual media applications. By using on chip memory and the multi-channel memory access unit, this chip achieved several different level of parallelism, such as single-instructionstream-multiple-datastream (SIMD) data-level parallelism (DLP), multicore thread-level parallelism (TLP) and memory tile pipeline parallelism. We used an affine transformation framework called PLuTo on code optimization for EVMPSoC and explored multiple level parallelism on this chip. We found that lacking of processor performance model, the general polyhedral affine transformation framework could not generate efficient parallel code for heterogeneous architectures. Tile scheduling and pipelining techniques are adopted to make a full use of process cores and memory bandwidth. The experiment results showed that tile schedule and pipeline is effective. This chip gained a very good accelerate ratio after all the parallel optimizations. Finally, the chip was proved to be high efficiency and availability through a case study (a typical application of three dimensional reconstruction from multi images).
[PLuTo framework, Nonhomogeneous media, RISC host processor, heterogeneous multicore SoC, Pluto, affine transforms, Reduced instruction set computing, Bandwidth, scheduling, high level optimization, heterogeneous architecture, multiple level parallelism, Availability, multicore thread-level parallelism, Multicore processing, multi-threading, reduced instruction set computing, EVMP-SoC, tile scheduling, Pipeline processing, memory bandwidth, SIMD synergistic processor, visual media application, Processor scheduling, chip memory, single-instructionstream-multiple-datastream, Tiles, data-level parallelism, polyhedral affine transformation, tile pipelining, multichannel memory access unit, pipeline processing, Acceleration, system-on-chip, memory tile pipeline parallelism, code optimization]
An Application Mapping Scheme over Distributed Reconfigurable System
2009 15th International Conference on Parallel and Distributed Systems
None
2009
The developing semiconductor technology enables the reconfigurable hardware such as FPGA. Distributed reconfigurable system is a FPGA-based hardware accelerated system with network. Applications can be accelerated as hardware module by FPGA in a distributed system. There are few works about application mapping on distributed reconfigurable system. In this paper an application mapping scheme for reconfigurable hardware accelerated distributed system is introduced. By extending the link state routing algorithm, this scheme can maintain the network map with low overhead to ensure the flexibility of the algorithm. The LEFM-NH algorithm proposed in this paper analyzes the current network status and the current application task graphs, and then finds appropriate hosts to map tasks to reduce the average network latency. This paper uses NetFPGA as the reference platform. The LEFM-NH algorithm can reduce the average network latency by 64% compared with simple random mapping algorithm. Experimental results show that the LEFM-NH algorithm of our scheme has good scalability and its execution time has a linear growth with task number.
[Algorithm design and analysis, reconfigurable hardware accelerated distributed system, NetFPGA, mapping, Scalability, field programmable gate arrays, Performance gain, distributed processing, distributed system, Routing, application task graphs, Application software, semiconductor technology, Delay, application mapping scheme, Runtime, link state routing algorithm, network status, distributed reconfigurable system, reconfigurable hardware, Hardware, LEFM-NH algorithm, Acceleration, Field programmable gate arrays]
Speculation with Little Wasting: Saving Cost in Software Speculation through Transparent Learning
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Software speculation has shown promise in parallelizing programs with coarse-grained dynamic parallelism. However, most speculation systems use offline profiling for the selection of speculative regions. The mismatch with the input-sensitivity of dynamic parallelism may result in large numbers of speculation failures in many applications. Although with certain protection, the failed speculations may not hurt the basic efficiency of the application, the wasted computing resource (e.g. CPU time and power consumption) may severely degrade system throughput and efficiency. The importance of this issue continuously increases with the advent of multicore and parallelization in portable devices and multiprogramming environments. In this work, we propose the use of transparent statistical learning to make speculation cross-input adaptive. Across production runs of an application, the technique recognizes the patterns of the profitability of the speculative regions in the application and the relation between the profitability and program inputs. On a new run, the profitability of the regions are predicted accordingly and the speculations are switched on and off adaptively. The technique differs from previous techniques in that it requires no explicit training, but is able to adapt to changes in program inputs. It is applicable to both loop-level and function-level parallelism by learning across iterations and executions, permitting arbitrary depth of speculations. Its implementation in a recent software speculation system, namely the behavior-oriented parallelization system, shows substantial reduction of speculation cost with negligible decrease (sometimes, considerable increase) of parallel execution performance.
[Energy consumption, software speculation, Costs, Profitability, Multicore processing, Transparent Learning, Throughput, Application software, parallel processing, Degradation, Multicore, transparent statistical learning, Power system protection, Computer applications, coarse-grained dynamic parallelism, Parallel processing, behavior-oriented parallelization system, function-level parallelism, Adaptive Speculation, learning (artificial intelligence), statistical analysis, loop-level parallelism, Parallelization]
Partitioned Computation to Accelerate Scalar Multiplication for Elliptic Curve Cryptosystems
2009 15th International Conference on Parallel and Distributed Systems
None
2009
The scalar multiplication is the dominant operation in Elliptic Curve Cryptosystems (ECC). It consists of a series of point additions and point doublings. A number of algorithms have been proposed to accelerate the scalar multiplication. Most of the algorithms demand high complexity which makes scalar multiplication hard to implement. In this paper, we propose an efficient algorithm for computing scalar multiplication based on partitioning scalar and propositional logic theory to address the trade-offs between speed and complexity. Our algorithm remains low complexity compared to existing accelerated scalar multiplication algorithms, whilst it is suitable for parallel processing systems.
[elliptic curve, propositional logic, propositional logic theory, Security, formal logic, Elliptic curves, public key cryptography, elliptic curve cryptosystem, algorithm theory, Logic, Personal digital assistants, linear algebra, classic binary algorithm, Partitioning algorithms, Parallel architectures, Elliptic curve cryptosystems, Galois fields, prepositional logic theory, matrix multiplication, Content addressable storage, accelerated scalar multiplication algorithm, Elliptic curve cryptography, scalar multiplication, Acceleration, partitioned computation]
DVM-MAC: A Mandatory Access Control System in Distributed Virtual Computing Environment
2009 15th International Conference on Parallel and Distributed Systems
None
2009
We design and implement a Mandatory Access Control (MAC) system in distributed virtual computing environment, named DVM-MAC, aiming to provide distributed trust through enforcing MAC policies. In DVM-MAC, Prioritized Chinese Wall (PCW) model is implemented to control potential covert channels between VMs in both single node and distributed environment. A policy enforcement module locates inside Xen VMM for better enforcing MAC locally rather than outside the VMM. DVM-MAC adopts centralized architecture for multi-level management and secure transmission of inter-node policy information. For performance consideration, a specific policy decision and enforcement module for controlling inter-node behaviors is moved out of Xen VMM and up to user space. DVM-MAC authorizes a specific center node named Central Security Server (CSS) to be responsible for the decision making between the nodes as well as leaves the inter-node policy enforcement module in each node. Through our experiments and data analysis, we verify the correctness, effectiveness, and efficiency in our prototype when implementing PCW model.
[Access control, mandatory access control system, internode policy information transmission, Xen VMM, Distributed computing, Voice mail, Resource virtualization, Concurrent computing, software architecture, central security server, Operating systems, authorisation, Mandatory Access Control, Grid computing, Hardware, Virtual manufacturing, multilevel management, internode policy enforcement module, Distributed System, DVM-MAC, Virtual machining, Prioritized Chinese Wall Model, prioritized chinese wall model, virtual machines, distributed virtual computing environment, Virtual Machine]
Is Straight-line Path Always the Best for Intrusion Detection in Wireless Sensor Networks
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Intrusion detection is prominently important for civil and military applications in wireless sensor networks (WSNs). It is defined as the mechanism to detect moving intruder(s) by single-sensing or jointly-sensing detection. To date, related works focus on the problem of network configuration for detecting intruder(s) within a pre-specified time/distance threshold by assuming a straight line intrusion path of the intruder. However, straight line intrusion path is often not the case in reality. An intruder can invade the network following a curved path or even a random walk in order to improve its attacking probability. Therefore, the results based on the assumption of a straight line intrusion path may not hold. In view of this, we are to address the problem from another angle, i.e., the effects of different intrusion paths on the intrusion detection probability in an arbitrary wireless sensor network. First, we propose a novel Sine Curve mobility model that can simulate different intrusion paths by adjusting its features such as amplitude, frequency, and phase. Next we are to explore the effects of different intrusion paths of the intruder on the intrusion detection probability in an arbitrary wireless sensor network. In other words, we are to investigate the following question theoretically and experimentally: Is straight-line path always optimal for intruder(s) to adopt for minimizing the detection probability of an arbitrary WSN, so as to maximize its attacking probability? Simulation outcomes are shown to match well with the analytical results, and therefore validate our analysis and conclusion.
[Military computing, wireless sensor networks, Quality of service, network deployment, intrusion detection, Rivers, quality of service, Computer science, Wireless sensor networks, security of data, Intrusion detection, attacking probability, Frequency, Optical sensors, Biomedical monitoring, Remote monitoring]
Privacy Reference Monitor - A Computer Model for Law Compliant Privacy Protection
2009 15th International Conference on Parallel and Distributed Systems
None
2009
The Internet and computers did not invent or even cause privacy issues. The issues existed long before the creation of computers and Internet. The existence of the Internet, computers and large data storage make it possible to collect, process and transmit large volumes of data, including personal data. In this paper, we shall study the privacy from following two different views, namely legal framework and computer security model, and attempt to identify the difference between them. Because of the difference, we further argue that the current computer security model is not sufficient to support the privacy requirements in the legal framework. We propose a computer model privacy reference monitor to handle those unsupported requirements. The design of the privacy reference monitor is privacy policy neutral with a small number of functions. With minimal functionalities, we believe that it is possible to implement a verifiable privacy reference monitor.
[Pervasive computing, Data privacy, computer model, Law, law, Computerized monitoring, legal framework, Privacy Protection, Educational institutions, Privacy reference monitor, Computer Model, Computer science, data storage, security of data, law compliant privacy protection, data privacy, Internet, Protection, Computer security, computer security model, Legal factors]
Early Performance Evaluation of Dawning 5000A and DeepComp 7000
2009 15th International Conference on Parallel and Distributed Systems
None
2009
In this paper, we present our early performance evaluation results with the NPB benchmark and two scientific computing applications program, i.e., a HFFT package developed by our lab and a CFDO application software, on two 100 Teraflops-scale Dawning 5000A and DeepComp 7000. We compared the NPB performance evaluation results of Dawning 5000A and DeepComp 7000, with their corresponding predecessor, Dawning 4000A and DeepComp 6800, which demonstrating their performance improvements across a variety of NPB benchmark problems. From our evaluation results, we can find that the NPB benchmark can keep its scalability up to 16384 cores on Dawning 5000A while on DeepComp 7000 this number becomes 4096. We also find that the HFFT scales well over 2048 cores while the CFDO scales well over 512 cores on Dawning 5000A.
[Energy consumption, Scientific computing, Scalability, scientific computing applications, Laboratories, performance evaluation, Supercomputers, AHFFT package, 100Teraflops-scale Dawning 5000A, Application software, Sun, parallel machines, DeepComp 7000, Computer architecture, Packaging, NPB performance evluation]
Understanding Network Saturation Behavior on Large-Scale Blue Gene/P Systems
2009 15th International Conference on Parallel and Distributed Systems
None
2009
As researchers continue to architect massive-scale systems, it is becoming clear that these systems will utilize a significant amount of shared hardware between processing units. Systems such as the IBM Blue Gene (BG) and Cray XT have started utilizing flat (i.e., scalable) networks, which differ from switched fabrics in that they use a 3D torus or similar topology. This allows the network to grow only linearly with system scale, instead of the super linear growth needed for full fat-tree switched topologies, but at the cost of increased network sharing between processing nodes. While in many cases a full fat-tree is an over estimate of the needed bisectional bandwidth, it is not clear whether the other extreme of a flat topology is sufficient to move data around the network efficiently. In this paper, we study the network behavior of the IBM BG/P using several application communication kernels, and we monitor network congestion behavior based on detailed hardware counters. Our studies scale from small systems to 8 racks (32,768 cores) of BG/P and provide insights into the network communication characteristics of the system.
[Costs, telecommunication congestion control, network saturation behavior, Torus, parallel machines, Counting circuits, Network topology, similar topology, fat-tree switched topologies, Bandwidth, Hardware, Fabrics, Large-scale systems, large-scale Blue Gene/P systems, Kernel, Petascale, Blue Gene/P, Monitoring, massive-scale systems, network sharing, Saturation, network congestion, telecommunication network topology, Cray XT, Communication switching, Fat Tree, large-scale systems, computer network management, IBM Blue Gene, 3D torus, Cray computers]
Providing Responsiveness Requirement Based Consistency in DVE
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Consistency and responsiveness are two important factors in providing the sense of reality in Distributed Virtual Environment (DVE). However, it is not easy to optimize both aspects because of the trade-off between these two factors. As a result, most existing consistency maintenance methods ignored the responsiveness requirements, or just assumed a simple responsiveness requirement model which cannot meet the real need of DVE systems. In this paper, we first present a new responsiveness requirement model. The model can describe requirement satisfaction situation of each node. Base on this model, we propose a responsiveness requirement based consistency method. The method can adjust the utilization of time resource according to the requirements of different nodes and improve the overall responsiveness performance by at least 20%. Therefore, it provides a good support to increase the applicability of DVE systems.
[Real time systems, requirement satisfaction, responsiveness requirement model, virtual reality, Virtual environment, Distance learning, Delay effects, Electronic commerce, distributed virtual environment, Research and development, Computer science, Distributed processing, Games, Safety, consistency maintenance methods]
A Population Dynamics Model for Data Streaming over P2P Networks
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Data streaming (DS) over peer-to-peer (P2P) networks has been intensively studied in recent years and there have been various schemes proposed already. To evaluate these schemes, either measurement in experimental implementations, or simulation and theoretical analysis have been used. The former is inadequate as data are collected from different experiments, while the latter lacks a proper theoretical dynamics model. Our research aims at providing a general theoretical model to evaluate DS over P2P systems and analyze their dynamic behaviors. In this paper, with the analysis and abstraction of the characteristics of peers and their organization in DS over P2P, we propose a general population dynamics model for DS over P2P with fixed population. The model depicts the dynamic distribution of peers as a closed Markov queuing network. In particular, the model is scheme-independent and can be used with various schemes. Through theoretical analysis, we prove the model has equilibrium and only one closed-form solution. Besides, we verify the model through simulations, and show that it is a helpful analytical tool with a case study.
[data streaming, Costs, peer-to-peer computing, Peer to peer computing, Scalability, peer-to-peer networks, Closed-form solution, population dynamics model, Multimedia communication, closed-form solution, P2P networks, peer-to-peer, Analytical models, system dynamics, Markov queuing network, Streaming media, Markov processes, closed Markov queuing network, Performance analysis, IP networks, Stock markets]
JOR: A Journal-guided Reconstruction Optimization for RAID-Structured Storage Systems
2009 15th International Conference on Parallel and Distributed Systems
None
2009
This paper proposes a simple and practical RAID reconstruction optimization scheme, called JOurnal-guided Reconstruction (JOR). JOR exploits the fact that significant portions of data blocks in typical disk arrays are unused. JOR monitors the storage space utilization status at the block level to guide the reconstruction process so that only failed data on the used stripes is recovered to the spare disk. In JOR, data consistency is ensured by the requirement that all blocks in a disk array be initialized to zero (written with value zero) during synchronization while all blocks in the spare disk also be initialized to zero in the background. JOR can be easily incorporated into any existing reconstruction approach to optimize it, because the former is independent of and orthogonal to the latter. Experimental results obtained from our JOR prototype implementation demonstrate that JOR reduces reconstruction times of two state-of-the-art reconstruction schemes by an amount that is approximately proportional to the percentage of unused storage space while ensuring data consistency.
[Availability, storage system, Laboratories, Switches, reliability, Reconstruction algorithms, data storage systems, performance evaluation, Paper technology, RAID, data consistency, Computer science, Degradation, storage space utilization, Computer displays, RAID reconstruction optimization scheme, System performance, Prototypes, journal-guided reconstruction scheme, reconstruction, data structures, disk array]
A Parallel SystemC Environment: ArchSC
2009 15th International Conference on Parallel and Distributed Systems
None
2009
In domains of VLSI and the rising SoC, the system design exceedingly depends on the simulation and modeling. Conventional HDLs have some weakness including the extravagant precision and the slow speed. On the other hand, the system-level modeling, such as SystemC, has been widely used on all kinds of projects and achieved favorable results. When the target system scales excessively up, the simulation speed also drops to the most extern. So, it is very important to speed up by parallelizing the system-level modeling over HPC such as cluster. This paper introduces a novel parallel SystemC environment named ArchSC which is constructed on a large-scale system-level parallel simulation platform, ArchSim. The test results demonstrate that ArchSC has some advantages of high scalability, good generality and preferable acceleration performance.
[Scalability, SoC, Very large scale integration, parallel SystemC, hardware description languages, parallel processing, Engines, HDL, TCPIP, Benchmark testing, parallel simulation, Large-scale systems, system-level modeling, ArchSim, Computational modeling, VLSI, ArchSC, system design, Sockets, Life estimation, HPC, Hardware design languages, system-on-chip, parallel engine, parallel channel, parallel SystemC environment]
A Hybrid Parallel Framework for the Cellular Potts Model Simulations
2009 15th International Conference on Parallel and Distributed Systems
None
2009
The cellular Potts model (CPM) has been widely used for biological simulations. However, most of current implementations are either sequential or approximate, which cannot be used for large scale complex 3D simulation. In this paper we present a hybrid parallel framework for CPM simulations. The time-consuming partial differential equation (PDE) solving, cell division, and cell reaction operation are distributed to clusters by using the message passing interface (MPI). The Monte Carlo lattice update is parallelized on shared-memory SMP system by using OpenMP. Since the Monte Carlo lattice update is much faster than the PDE solving and SMP systems are more and more common, this hybrid approach achieves good performance and high accuracy at the same time. Based on the parallel cellular Potts model, we have studied the avascular tumor growth by using a multiscale model. The application and performance analyses demonstrate that the hybrid parallel framework is quite efficient. The hybrid parallel CPM can be used for the large scale simulation (~ 108 sites) of complex collective behavior of numerous cells (~ 106).
[message passing, application program interfaces, message passing interface, shared-memory SMP system, Biological system modeling, Computational modeling, Computer simulation, Laboratories, Lattices, biological simulations, MPI, Monte Carlo lattice update, Helium, Neoplasms, Computer science, time-consuming partial differential equation, cellular Potts model simulations, Monte Carlo methods, biology computing, Large-scale systems, partial differential equations]
E-TCP: Enhanced TCP for IEEE802.11e Mobile Ad Hoc Networks
2009 15th International Conference on Parallel and Distributed Systems
None
2009
TCP, the facto standard used in today's Internet, has been found to perform poorly in Mobile Ad Hoc Networks (MANETs). This is exacerbated by contention with increasing UDP-based high priority multimedia traffic and the class differentiation introduced in current QoS protocols, which results into TCP starvation and increased spurious timeouts. In this paper, we propose a cross-layer TCP enhancement (E-TCP) that makes use of TCP bidirectionality to avoid TCP traffic starvation and spurious retransmissions in presence of high priority traffic. E-TCP is based on prioritizing TCP acknowledgement packets and on adjusting TCP retransmission timer based on the medium contention. Our simulation results reveal considerable improvements in TCP performance in terms of gooput, delay and retransmission efficiency, while the better channel utilization results into even better high priority traffic performance.
[TCP, Protocols, Spurious, QoS protocol, Telecommunication traffic, cross-layer TCP enhancement, 802.11e, class differentiation, Distributed computing, Delay, Mobile ad hoc networks, Concurrent computing, high priority traffic, UDP-based high priority multimedia traffic, QoS, TCP retransmission, mobile ad hoc networks, Traffic control, Computer networks, IP networks, channel allocation, EDCA, multimedia communication, mobile radio, Computational modeling, IEEE 802.11e, quality of service, TCP bidirectionality, MANET, TCP traffic starvation, delay, transport protocols, delays, channel utilization, acknowledgement packets, gooput, ad hoc networks, wireless LAN, TCP starvation, telecommunication traffic]
SCN4M-DL: An Adaptive Directory-Less Service Discovery System for MANETs
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Service discovery systems over MANETs are demanded to find available services at an appropriate cost, even if nodes in MANETs frequently move, randomly join or leave, and services may be revoked by their owners. SCN4M-DL is an adaptive directory-less service discovery system for MANETs, and it meets the above challenges by deeply coupling with underlying geographic routing algorithm and setting configurable service renewal. The storage consumption and network traffic are under control due to the adopted ring-based service register and discovery mechanism. Extensive experiments are also conducted. The result data, i.e. the discovery request's success rates and the corresponding costs, show that SCN4M-DL can adapt well to MANET's characteristics and the changes of service states.
[Availability, Costs, Adaptive systems, mobile radio, SCN4M-DL, Routing, configurable service renewal, Delay, storage consumption, Mobile ad hoc networks, geographic routing algorithm, network traffic, MANET, Software architecture, adopted ring-based service register, telecommunication network routing, mobile ad hoc networks, Computer architecture, Communication system traffic control, Resource management, ad hoc networks, telecommunication traffic, adaptive directory-less service discovery system]
Queuing Based Traffic Model for Wireless Mesh Networks
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Wireless mesh networks (WMN) provide network access for mobile users. Therefore, most traffic flows in WMNs are to and from the wired networks. Some mesh nodes, called Gateways connect directly to the wired networks, through which mesh clients can access the resources that reside on the wired networks. However, there are usually only a few of Gateways in a WMN. The packet processing ability of every wireless node is limited. As a result, traffic loads of mesh nodes affect greatly the network performance. In this paper, we put forward a queuing based traffic model for WMNs. In the model, both Gateways and mesh nodes at the largest hop count from the gateways are regarded as service stations with infinite capacity, whereas the other mesh nodes are modeled as service stations with finite capacity. The model also takes into account impacts of interference. We then analyze the network throughput, average packet loss and packet delay on each hop nodes using the proposed traffic model. Results show that the proposed model is accurate in modeling the characteristics of traffic loads in WMNs.
[Wireless LAN, gateways, queueing theory, traffic loads, Queuing Theory, Telecommunication traffic, Interference, wireless node, Throughput, Educational institutions, network throughput, Relays, wireless mesh networks, Computer science, packet radio networks, Wireless mesh networks, Computer applications, Traffic control, average packet loss, WMN, queuing based traffic model, packet delay, telecommunication traffic, Traffic Model]
Implementation of the AdaBoost Algorithm for Large Scale Distributed Environments: Comparing JavaSpace and MPJ
2009 15th International Conference on Parallel and Distributed Systems
None
2009
This paper presents the parallelization of a machine learning method, called the AdaBoost algorithm. The parallel algorithm follows a dynamically load-balanced master-worker strategy, which is parameterized by the granularity of the tasks distributed to workers. We first show the benefits of this version with heterogeneous processors. Then, we study the application in a real, geographically distributed environment, hence adding network latencies to the execution. Performances of the application using more than a hundred processes are analyzed in both JavaSpace and P2P-MPI. We therefore present an head-to-head comparison of two parallel programming models. We study for each case the granularities yielding the best performance. We show that current network technologies enable to obtain interesting speedups in many situations for such an application, even when using a virtual shared memory paradigm in a large-scale distributed environment.
[parallel programming models, large scale distributed environments, MPJ, parallel algorithm, load-balanced master-worker strategy, machine learning method, parallel programming, Grid Computing, resource allocation, AdaBoost algorithm, virtual shared memory paradigm, Large-scale systems, learning (artificial intelligence), network latency, Adaboost, Java, parallel algorithms, JavaSpace, message passing, peer-to-peer computing, P2P-MPI, parallelization, heterogeneous processors, network technology, geographically distributed environment, distributed shared memory systems]
A Tabu Search for the Heterogeneous DAG Scheduling Problem
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Scheduling parallel applications on heterogeneous processors/architectures with different computational speed is a difficult problem. Here, a tabu search metaheuristic is developed to improve the schedule generated by list scheduling. Three neighbourhoods variants are proposed and examined, including a novel neighbourhood that takes the shape of the task graph into account. The effectiveness is evaluated based on a set of modified random benchmark graphs, including task graphs of real-world applications. Factors affecting algorithm performance are also examined. We have found that the variants proposed were able to reduce the schedule length produced by HEFT up to an average of 30% and up to on average 20% for the standard graphs. The results also show that using information about the shape of the task graph is a viable strategy.
[task graph, Shape, Multicore processing, Optimal scheduling, HEFT, DAG, Distributed computing, parallel processing, heterogeneous, Scheduling algorithm, Concurrent computing, list scheduling, metaheuristic, Processor scheduling, High performance computing, directed graphs, Computer architecture, scheduling, tabu search, directed acyclic graphs, Field programmable gate arrays, search problems, heterogeneous DAG scheduling problem]
Double-layer Scheduling Strategy of Load Balancing in Scientific Workflow
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Complicated method and technology have been introduced into business workflow to solve their cooperation and load balancing. However, as combination of data management and flow management, scientific workflow's load balancing is less researched. So the phenomena of conflicting scheduling and load unbalance often occur. In this paper, we analyze functional framework and load consumption of scientific workflow engine in detail, and then introduce the load models of workflow application, workflow runtime engine and grid node. More importantly, utility function quantifying various loads and performance is introduced to lay foundation for engine selection and task scheduling. The collection and evaluation of real-time status are implemented by monitor and high-layer planner. Considering load balancing of both scientific workflow engine and grid node, we propose a double-layer scheduling algorithm based on performance model. Results from experiments demonstrate that our proposed models and double-layer scheduling algorithm can well avoid load unbalancing and conflicting scheduling in engine and grid node. Moreover, completion time of scientific workflow application can reduce greatly owning to full and even utilization of engine and grid node.
[scientific workflow, Costs, load balancing, workflow application model, workflow runtime engine model, utility function, engine selection, Application software, Engines, Scheduling algorithm, Processor scheduling, resource allocation, double-layer scheduling algorithm, scheduling, Load management, Collaborative work, task scheduling, Resource management, workflow management software, grid node model, Workflow management software, Load modeling]
A Supply-and-Demand Model for On-Demand Streaming in Networked Virtual Environments
2009 15th International Conference on Parallel and Distributed Systems
None
2009
In a large scale networked virtual environment, users may request various multimedia data in streaming. Although adopting a peer-to-peer (P2P) architecture can make the transmission of on-demand streaming more efficient, most existing research assume that the peers have infinite buffer to cache the entire content and supply the data from there. In this paper, we consider the practical case in which the peers have limited buffer space to cache only a portion of the multimedia content. Due to limited buffer space, peers need effective chunk selection strategies to fetch and discard chunks. We propose a supply-and-demand strategy to estimate the usability of chunks. Peers then select the chunks with a higher usability to fetch and discard the chunks that are useless. Simulation results show that the supply-and-demand strategy selects the chunks accurately and enhances the continuity of playing the stream.
[multimedia data, Virtual environment, peer-to-peer computing, Peer to peer computing, Scalability, Multimedia systems, P2P architecture, peer-to-peer (P2P), on-demand streaming, Computer science, peer-to-peer architecture, supply-and-demand model, chunk selection, streaming, networked virtual environments, buffer management, video-on-demand (VoD), Computer architecture, Streaming media, media streaming, Computer industry, Large-scale systems, Usability]
Group-Based Peer-to-Peer 3D Streaming Authentication
2009 15th International Conference on Parallel and Distributed Systems
None
2009
In this paper, we present an authentication scheme for group-based peer-to-peer 3D streaming that takes advantage of secure group communication to reduce computation overheads of authentication. Users of the same interest first form a group, and one of the users is then elected to be the trusted leader, who is to download 3D contents, to verify their authenticity and integrity, and to send group members a checksum value for each 3D content piece via group secure channels. By the encrypted checksum values, a group member can authenticate 3D contents downloaded from any source. Since checksum is cheap to compute, much computation is saved. We have evaluated the computation saving of the proposed scheme for the case of progressive meshes based on the hash chain signature 3D streaming authentication scheme. We have also evaluated the rendering quality when embedding hash chain signatures into the least significant bits (LSBs) of the mesh data to make the signatures imperceptible.
[Concurrent computing, Bandwidth, authorisation, media streaming, Second Life, group communication security, mesh data least significant bits, authentication, Virtual environment, peer-to-peer computing, Peer to peer computing, 3D contents downloaded authentication, rendering quality, cryptography, encrypted checksum values, group secured channel, peer-to-peer, Computer science, 3D streaming, Chaotic communication, Layout, Authentication, Streaming media, digital signatures, group based peer-to-peer 3D streaming authentication, hash chain signature 3D streaming authentication scheme]
Evaluation of the HyperVerse Avatar Management Scheme Based on the Analysis of Second Life Traces
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Massive multiuser virtual environments (MMVEs) and the idea of a global scale 3D Web have grown popular in recent years. While commercial precursors of such environments for the most part rely on centralized client/server architectures, it is commonly accepted that a global scale virtual online world can only be realized in a distributed fashion. Within the HyperVerse project, we have developed and recently presented a two-tier Peer-to-Peer (P2P) architecture that incorporates a loosely structured P2P overlay of user peers and a highly structured overlay of server machines constituting a reliable backbone service. In such a distributed environment, an essential question is how avatars are tracked and interconnected in order to allow mutual rendering and interaction. We have previously proposed a hybrid avatar management scheme that utilizes the backbone service for avatar tracking if necessary, but handles tracking in a P2P fashion when peers can track each other to reduce the backbone load. This paper presents a detailed performance analysis of this algorithm under a realistic scenario, using traces from a large scale MMVE called Second Life. Moreover this paper presents and evaluates an optimization for the hybrid avatar tracking scheme that can be utilized under a weaker condition.
[second life traces analysis, Avatars, Spine, Conference management, Environmental management, 3D Web, software architecture, optimisation, centralized client/server architectures, optimization, Controllability, Second Life, Performance analysis, Large-scale systems, avatars, client-server systems, Virtual environment, peer-to-peer computing, Peer to peer computing, massive multiuser virtual environments, MMVE, P2P, peer-to-peer architecture, DVE, HyperVerse avatar management, Internet]
A Service-oriented Programming Platform for Internet-Based Virtual Computing Environment
2009 15th International Conference on Parallel and Distributed Systems
None
2009
This paper describes a service oriented programming platform for Internet-based virtual computing environment (iVCE), which is targetted for enabling collaboration among autonomous entities over Internet. This platform provides mechanisms and facilities to help iVCE programmers write and run applications in a relatively stable and trusted environment. In this platform, autonomous entities first voluntarily join in virtual community attracted by common interests. Then entities will selectively play specific roles predefined in collaboration process which is specified in a declarative language named APEL. Capability requirements and environment specifications are also provided to enable negotiation and adaptation while process is executed. All business interfaces and negotiation interfaces are published using standard Web services to maximize reuse and interoperation.
[service-oriented programming platform, APEL declarative language, Virtual colonoscopy, open systems, iVCE program, interoperation, negotiation interface, autonomous, Distributed computing, formal specification, Concurrent computing, software architecture, Web and internet services, groupware, autonomous entity collaboration, capability requirement, Runtime environment, virtual community, Collaborative software, Programming profession, Identity management systems, business interface, Parallel programming, Web services, Collaboration, service, collaboration, software reusability, environment specification, Internet-based virtual computing environment]
Experiences Running OGSA-DQP Queries against a Heterogeneous Distributed Scientific Database
2009 15th International Conference on Parallel and Distributed Systems
None
2009
This paper explores the use of emerging Grid technologies for the manipulation of a scientific dataset with complex schema. We focus on the potential of a well-known grid middleware-OGSA-DQP-for querying such datasets. In particular, we investigate running OGSA-DQP queries against SQL Server and Oracle database to access data in a complex schema of a real scientific database-the Sloan digital sky survey (SDSS) database. We used two common databases which support complex schemas and very large datasets: SQL Server and Oracle. The paper briefly provides the background information on the SDSS database. It then examines the running of a few OGSADQP queries and the query plans. Various join queries run successfully through OGSA-DQP after we optimized the implementation of some of the OGSA-DQP operations.
[Oracle database, grid computing, Relational databases, Data mining, Uniform resource locators, query processing, very large databases, Distributed databases, distributed databases, OGSA-DQP queries, heterogeneous distributed scientific database, Photometry, open grid services architecture-distributed query processing, OGSA-DQP, middleware, Sloan Digital Sky Survey (SDSS), Scientific Database, Spectroscopy, Sloan digital sky survey database, OGSA-DAI, Grid middleware, SQL Server, relational databases, Middleware, Complex Database Schemas, SQL, Computer science, Image databases, Query processing, grid middleware, scientific datasets]
Cyberinfrastructure on Tumor Gene Expresstion Data Processing
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Malignant tumor or cancer is one of serious diseases threatening human health. It is a challenging issue on how to diagnose the malignant tumor earlier and more accurately. A feasible way on cancer diagnosis is efficiently to share and analyze tumor related data. We work on a cyberinfrastructure which presents data sharing and data analysis solutions to reveal the relationship between tumor and gene.
[Data analysis, Malignant tumors, Data processing, cancer diagnosis, Gene expression, Neoplasms, human health, Diseases, Tumor Gene Expression Data Processing, Pathology, Cyberinfrastructure, cancer, data handling, medical administrative data processing, malignant tumor, tumours, Medical diagnostic imaging, patient diagnosis, Cancer, Biomedical imaging]
Towards Hybrid Grid Infrastructure for Large Simulations
2009 15th International Conference on Parallel and Distributed Systems
None
2009
The last decade has witnessed an explosion of interest in technology of large simulation with the rapid growth of both the complexity and the scale of problem domains. A variety of techniques have been developed for fostering large simulations over the Internet. Along this direction, existing work normally only suit for coarse-grained models. In this paper, we present a hybrid grid infrastructure which applies to both coarse-grained and fine-grained models. A gateway approach has been proposed to present simulation models of an individual administrative domain for fine-grained problems, and it also bridges simulation models operating in multiple administrative domains to form large simulations for studying problems of different grains. A prototype infrastructure has been realized with the support of federated simulation technology.
[coarse-grained models, Large Simulation, Computational modeling, Computer simulation, Neurons, Time warp simulation, grid computing, Grid, Explosions, Discrete event simulation, Security, Distributed computing, Parallel and Distributed Computing, hybrid grid infrastructure, Brain modeling, Modelling &#x00026; Simulation, Internet, fine-grained models]
Towards a Scalable Real-Time Cyberinfrastructure for Online Computer Games
2009 15th International Conference on Parallel and Distributed Systems
None
2009
We propose a novel cyberinfrastructure for an emerging class of Internet-based real-time online interactive applications (ROIA). The most challenging representative of this application class are massively multi-player online games. We present the results of the European project Edutain@Grid on the development of efficient cyberinfrastructure and scalable applications. We report experimental results demonstrating the performance and scalability of our approach.
[Real time systems, Virtual environment, Scalability, Computational modeling, Grid middleware, scalable real-time cyberinfrastructure, Scalability of distributed systems, Application software, Distributed computing, Middleware, online computer game, Multi-player online games, Concurrent computing, Runtime, multiplayer online game, Cyberinfrastructure, computer games, interactive systems, Internet, Real-time middleware, real-time online interactive application]
A Transmit Antenna Selection for MIMO Wireless Ad-Hoc Networks in Lossy Environment
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Because of rapid channel attenuation in lossy environment, there is a severe packet drop loss problem in wireless ad hoc networks. Though MIMO (Multi-Input Multi-Output) technology can alleviate this effect greatly, it will result in higher transmission energy depletion as using more transmit antennas. Lots of schemes have been proposed to reduce such energy cost by transmit antenna selection. However, most of those only consider QoS requirements between neighbor nodes while ignoring QoS requirements for entire network. In this paper, we investigate an energy-efficient transmit antenna selection problem for many-to-one communications, while guaranteeing a certain packet drop loss ratio from any node to the sink. We prove that this problem (Transmit Antenna Selection Problem, TASP) is NP-hard. Thus, a greedy algorithm TASA (Transmit Antenna Selection Algorithm) is proposed to solve this problem. This paper also prove the correctness of the proposed algorithm and the time complexity of TASA algorithm is O(|E|+|V|-M<sub>T</sub>), where |V| represents the number of nodes, |E| indicates the number of links and M<sub>T</sub> is the number of transmit antennas on each node, respectively. Our numerical results show that in a 100 node network, TASA can reduce transmission energy cost by 44.3% on average.
[Costs, transmitting antennas, MIMO wireless ad hoc networks, Transmitting antennas, Antenna accessories, Mobile ad hoc networks, electromagnetic wave transmission, channel attenuation, transmission energy depletion, Propagation losses, Attenuation, MIMO, greedy algorithm, MIMO communication, multi input multi output technology, transmit antenna selection problem, time complexity, multi-input multi-output (MIMO), Ad hoc networks, quality of service, packet drop loss ratio, Computer science, transmit antenna selection, energy-efficient transmit antenna selection, absorbing media, Energy efficiency, ad hoc networks, QoS requirements, computational complexity]
BER Performance Evaluation of Tail-Biting Convolution Coding Applied to Companded QPSK Mobile WiMax
2009 15th International Conference on Parallel and Distributed Systems
None
2009
A Bit-Error-Rate (BER) performance evaluation is presented for mu-Law companded QPSK modulated Mobile WiMax (IEEE802.16e) employing tail-biting convolutional coding (TBCC). The Down Link (DL) Partially Used Subcarrier (PUSC) deployment has been considered for this investigation. Equalized symbol power transmissions for uncompanded and mu-Law companded WiMax scenarios employing mu = 0.1, 1, 3, 10, 30, 100, 255, 1000, and 3000 has been undertaken. It is shown that for 30 clusters each carrying 48 bits of random data, the improvement in SNR for TBCC using the convolutional code (7, 171, 133) is best for uncompanded WiMax and deteriorates slightly for companded WiMax situations. The maximum improvement in SNR at the 0.001 BER probability level is ~1.7 dB, whilst at the 0.0001 BER probability level the improvement is ~2.9 dB. As companded BER rates for smaller values of mu are only slightly degraded over normal WiMax BER rates, then TBCC for small values of mu may be used to provide improved SNR performance over normal convolutional coding (CC) WiMax situations. Importantly, small values of mu can also provide improved Peak-to-Average Power Ratio (PAPR) but for larger values of mu, where significant PAPR reduction can be obtained, the improved performance of TBCC does not provide sufficient SNR gain to improve upon the standard CC WiMax performance.
[Convolutional codes, Bit error rate, Mobile WiMax, Power transmission, Peak to average power ratio, Performance gain, companding, quadrature phase shift keying, down link partially used subcarrier deployment, mu-Law companded QPSK modulated mobile WiMax, Degradation, Tail-biting, Convolution, SNR, Modulation coding, error statistics, mobile radio, peak-to-average power ratio, WiMAX, WiMax, TBCC, DL PUSC, BER probability level, BER, Quadrature phase shift keying, tail-biting convolution coding, convolutional codes, BER performance evaluation]
Broadcasting in Fully Connected Trees
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Broadcasting is an information dissemination problem in a connected network, in which one node, called the originator, disseminates a message to all other nodes by placing a series of calls along the communication lines of the network. Once informed, the nodes aid the originator in distributing the message. Finding the minimum broadcast time of a vertex in an arbitrary graph is NP-complete. The problem is solved polynomially only for trees, unicyclic graphs, and tree of cycles. In this paper we consider broadcasting in a new class called the Fully Connected Trees (FCT). We present a O(n log n) algorithm to find the broadcast time of any originator in an arbitrary FCT.
[Multiprocessor interconnection networks, vertex, network theory (graphs), O(n log n) algorithm, NP-complete, originator, Concurrent computing, connected network, communication lines, Tree graphs, Broadcasting, Parallel processing, vertex functions, Polynomials, Computer networks, unicyclic graphs, arbitrary graph, trees (mathematics), information dissemination problem, broadcasting, Computer science, fully connected trees, Approximation algorithms, Software engineering, computational complexity, minimum broadcast time]
Optimizing Unstructured Peer-to-Peer Overlays with Topology Awareness
2009 15th International Conference on Parallel and Distributed Systems
None
2009
In an unstructured peer-to-peer (P2P) network (e.g., Gnutella), participating peers choose their neighbors randomly such that the resultant P2P network mismatches its underlying physical network, resulting in the lengthy communication between the peers and redundant network traffics generated in the underlying network. Previous solutions to the topology-mismatch problem in the literature are far from the optimum. In this paper, we propose a novel topology-matching algorithm based on the Metropolis-Hastings method. Through extensive simulations, we show that our proposal constructs an unstructured P2P network where a broadcast message, originated by any node v, reaches any other node u by taking approximately the only physical end-to-end delay between v and u.
[unstructured peer-to-peer overlays, topology-mismatch problem, Metropolis-Hastings method, peer-to-peer computing, Peer to peer computing, Telecommunication traffic, telecommunication network topology, Routing, Proposals, Floods, topology-matching algorithm, Delay, network traffics, Computer science, end-to-end delay, Network topology, Broadcasting, Traffic control, topology awareness, unstructured P2P network]
Anadem: A Hybrid Overlay Network for Content-Based Data Distribution
2009 15th International Conference on Parallel and Distributed Systems
None
2009
As an infrastructure for data distribution, overlay networks have to feature efficient routing and adequate robustness to achieve fast and accurate data distribution in the environment with node churn. Considering that the existing overlay networks mostly focus on single optimization objective and fail to ensure routing efficiency and robustness simultaneously, a hybrid overlay network for content-based data distribution - Anadem is proposed in this paper. Anadem achieves a better compromise between routing efficiency and robustness by combining the inter-cluster multiple structured topologies with the intra-cluster unstructured topologies. Anadem also provides mechanisms for dynamic concurrent cluster creation, cluster departure and load balance to make data distribution more adaptive to the dynamic network environment. Experimental results reveal that compared with existing overlay networks, Anadem can support fast and accurate content-based data distribution even when large amount of nodes fail in the system.
[workstation clusters, Costs, hybrid overlay network, load balancing, data distribution, Distributed computing, content-based data distribution, Concurrent computing, routing efficiency, Distributed processing, Network topology, resource allocation, Disaster management, dynamic concurrent cluster creation, hierarchical, Robustness, Computer networks, intracluster unstructured topology, publish/subscribe, peer-to-peer computing, cluster departure, Peer to peer computing, network routing, overlay, telecommunication network topology, Routing, intercluster multiple structured topology, peer-to-peer, content-based, hybrid, telecommunication network routing, Anadem]
A Replica Placement Algorithm for Hybrid CDN-P2P Architecture
2009 15th International Conference on Parallel and Distributed Systems
None
2009
The Hybrid CDN-P2P architecture, or HCDN, which combines the complementary advantages of CDN and P2P networks, has been proposed to reduce the deployment cost and to improve the quality of service in file sharing and video streaming applications. A replica placement algorithm (RPA) decides where to replicate the specific data. Existing RPAs for pure CDN do not work efficiently in the HCDN architecture because they do not take into consideration the contribution of the peers at the P2P distribution level. In this article, a heuristic RPA that takes into account the effects of P2P distribution is proposed for HCDN. The performance of our proposed algorithm is evaluated and the impact of some key metrics is analyzed. The experimental result shows the clear performance benefits of our approach.
[Costs, peer-to-peer computing, Heuristic algorithms, Peer to peer computing, Quality of service, hybrid CDN-P2P architecture, P2P distribution level, Replica Placement Algorithm, quality of service, Peer-to-Peer, Delay, replica placement algorithm, P2P networks, video streaming application, content distribution network, Computer architecture, Bandwidth, Streaming media, Internet, Web server, file sharing, Hybrid Content Distribution Network]
Measuring the Influence of Active Measurement on Unstructured Peer-to-Peer Network
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Although intensive researches have been performed regarding P2P network measurement, it is still unknown to what extent the measurement system influences the final measurement results. As an initial study, we investigated the influence of a measurement system on degree distribution of a P2P network. Theoretical analysis and simulation results suggest an interesting phase-transition phenomena when the size of the measurement system increases. A P2P network mixed with a small active measurement system remains as a scale-free network; however, the mixture P2P network will not remain as scale-free when the size of the measurement system exceeds a threshold. We also observed that the number of measuring peers usually has stronger influence than the number of connections among these peers. Briefly speaking, a small but dense active measurement is better than a large but loose measurement system.
[Performance evaluation, measurement systems, active measurement, Phase measurement, peer-to-peer computing, unstructured peer-to-peer network, Peer to peer computing, scale-free, Crawlers, Extraterrestrial measurements, Size measurement, scale-free network, P2P network degree distribution, Analytical models, computer network management, Network topology, Complex networks, Sampling methods, P2P network measurement, degree distribution, phase transition phenomena, peer-to-peer network, active measurement system]
Mobility of Internet-Based Virtual Computing Environment
2009 15th International Conference on Parallel and Distributed Systems
None
2009
The Internet-based virtual computing environment (iVCE) provides on-demand aggregation and autonomic collaboration mechanisms to facilitate the utilization of autonomous and dynamic Internet resources. Load balancing and fault tolerance are important issues when scheduling those transient resources. In this paper, we propose a mobility mechanism for the migration of various roles of agents in the iVCE platform. The mobility mechanism involves two parts of the iVCE platform: role container layer and event service layer. At the role container layer, a novel approach is proposed to handle the code and data mobility issue. At the event service layer, an efficient routing reconfiguration protocol is proposed based on a publish/subscribe system over DHTs to facilitate task migrations. Certain conditions must be satisfied before the migration of an agent to ensure the correctness of the whole process. Experiments are conducted to evaluate the performance of the mobility mechanism, and the experimental results show that it is suitable for implementing load balancing and fault tolerance in the iVCE.
[virtual reality, mobility, agent, load balancing, publish-subscribe system, roles, Containers, event service layer, data mobility, Distributed computing, Concurrent computing, Fault tolerance, DHT, resource allocation, mobility mechanism, iVCE platform, Routing protocols, role container layer, on-demand aggregation, iVCE, middleware, message passing, fault tolerance, International collaboration, Middleware, software fault tolerance, Computer languages, Internet-based virtual computing, Load management, Internet, autonomic collaboration mechanisms, routing reconfiguration protocol]
Performance under Failure of Multi-tier Web Services
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Performance issues of multi-tier Web services have been studied extensively in recent years. Performance modeling and prediction under failure of multi-tier architectures, however, is not well addressed yet. We propose a novel model named Performance under failure of multi-tier architecture, or PerFAMA in short, to address this issue. We first show that the multi-tier architecture with failure considerations is a product-form network, and then analyze and model the failure impact. By applying the PerFAMA model, we are able to predict the end-to-end response time of multi-tier Web services under failures. We have simulated two representative Web services architectures and various failure scenarios to verify the proposed PerFAMA model. The experimental results show that the proposed model works well and the prediction accuracy is up to 98%.
[prediction under failure, Service oriented architecture, end-to-end response time, Predictive models, multitier Web services, Sun, system recovery, Delay, multitier architecture, Computer science, software architecture, Web services, Web services architectures, System performance, performance modeling, product-form network, Failure analysis, Computer architecture, performance under failure, Multi-tier Web Services, Performance analysis, Performance, Failure, PerFAMA]
A Peer-to-Peer Media Streaming System Based on the iVCE Platform
2009 15th International Conference on Parallel and Distributed Systems
None
2009
With the advancement of peer-to-peer technology, media streaming applications become more and more popular in the Internet. However, the traditional development methods for this kind of applications need developers not only to consider the application logic but also to manage the dynamics of Internet resources, thus increasing the difficulty of development and limiting the deployment of personal video distribution applications. In this paper, we design and implement a peer-to-peer streaming system in a much easier way. In this way we can concentrate on the application itself without distraction from the dynamics of Internet resources. Such simplification owes to the Internet-based Virtual Computing Environment (iVCE), which provides programming abstractions and runtime utilities that can encapsulate the complexity of managing transient resources into the platform, thus facilitating the construction of Internet applications. When we build our streaming application based on the iVCE, we only need to define the interaction protocols among distributed nodes with the Owlet programming language. Also, we implement a JavaBean, which can be used by the Owlet program, to assist the transferring and rendering of the content. Our implementation shows that peer-to-peer applications such as media streaming, can be elegantly built using the iVCE platform, and it can serve as a reference implementation for developing similar applications.
[peer-to-peer technology, Protocols, programming abstractions, interaction protocols, programming languages, Environmental management, content management, content rendering, media streaming, personal video distribution, Logic, iVCE, Runtime environment, peer-to-peer computing, Peer to peer computing, runtime utilities, Internet resources, Internet based virtual computing environment, peer-to-peer, Computer languages, content transfer, Utility programs, Owlet programming language, JavaBean, Streaming media, Internet, Resource management]
Measurement and Analysis of BitTorrent Availability
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Of the many peer-to-peer (P2P) systems in existence, BitTorrent is one of the most appealing that has managed to attract millions of users in the past few years. In this paper we present an extensive study of BitTorrent availability through measurement and analysis. Previous studies are limited to tracker without distributed hash tables (DHT), which have been used widely already. We first discuss the difference of measurement methods, and then develop a new parallel method based on threshold with bounding measurement errors. Afterwards, we focus on three issues: the availability of tracker and DHT, the availability of pieces and the variability of availability over time. Our analysis provides several new findings: (1) Overall dynamics of peers are similar across different index. (2) Compared to tracker, DHT does not have better performance as expected. (3) Seeds contribute most of piece replicas. When they are of offline, the availability of pieces will decrease dramatically. (4) The replicas of all pieces are almost the same without very rare pieces and the distribution of pieces is equal and effective. (5) The variability of availability shows a typical life cycle pattern over time, which means it is difficult for users to obtain files in the latter half of stage. In summary, this paper advances our understanding of availability by comparing different index, exploring the distribution of pieces and analyzing the fluctuation of availability.
[Availability, Measurement errors, Fluctuations, peer-to-peer computing, Peer to peer computing, availability, Computer crime, measurement methods, measurement, Computer science, life cycle pattern over time, peer-to-peer systems, BitTorrent availability, piece, peer, Bandwidth, Streaming media, BitTorrent, Internet, Performance analysis]
Association Link Network: An Incremental Semantic Data Model on Organizing Web Resources
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Association link network (ALN) is used to establish associated relations among various resources, aiming at extending the hyperlink network World Wide Web to an association-rich network, for effectively supporting Web intelligence activities. Unfortunately, with the increase number of Web resources, the challenge of incremental building of ALN is on how to perform the association weight of the new coming Web resources efficiently and exactly. A naive way is to compare every pair of data in the existing ALN, thus bearing a O(n2) time complexity. Given the scale of the Web, it is unrealistic to compute the association weight between the new coming Web data and each data in the existing ALN, respectively. In this paper, a new method based on All-Pairs algorithm for incremental building of ALN is proposed. The experiments and evaluations show that our incremental building method performs a high accuracy. Moreover, the scale-independent property of our method make it more appropriate to be used on the Web.
[Buildings, association link network, time complexity, Resource description framework, Distributed computing, Organizing, Semantic Web, Concurrent computing, Web pages, hyperlink network World Wide Web, all-pairs algorithm, Data models, Computer networks, Internet, incremental semantic data model, Web sites, Web resources, computational complexity]
A Case of Parallel EEG Data Processing upon a Beowulf Cluster
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Electroencephalogram (EEG) data processing applications have become routine tasks in both bioscience and neuroscience research, which are usually highly compute and data intensive. In this paper, we present a parallel method to analyze the huge EEG data with a Beowulf cluster. Through an example of the synchronization measurement of multiple neuronal populations, the procedure of exploiting the parallelism of EEG data processing applications to achieve speed-up has been detailed. The experimental results indicate that the execution efficiency of EEG data processing can be improved dramatically using parallel and distributed computing techniques even with inexpensive computing platform.
[Beowulf cluster, MPI, Electroencephalography, Distributed computing, parallel processing, neuroscience research, Concurrent computing, Neuroscience, distributed computing techniques, Parallel processing, medical administrative data processing, Bioinformatics, Local area networks, electroencephalography, Data processing, parallel EEG data processing, parallel computing techniques, Beowulf Cluster, Application software, electroencephalogram, Parallel and Distributed Computing, Message passing, multiple neuronal populations, Neuroinformatics, bioscience research, medical computing]
A Fast and Intelligent Resource Allocation Service for Service-Oriented Grid
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Grid has evolved dramatically into the era of service-oriented grid, which facilitates building of large-scale systems in standard fashions, reusability of essential functions, and interoperability among components. However, grid resource allocation is still a challenging problem for which a grid scheduler has to be operating in a dynamic and uncertain environment. Conventional scheduling algorithms will fail due to the static rules used and much user intervention required. We suggest that learning with neural networks is promising to solve this problem. In this paper we propose a fast and intelligent resource selection algorithm based on neural networks. Extreme Learning Machine (ELM) is exploited as the learning paradigm due to its fast learning speed and satisfactory performance. Moreover, we present an architecture, which defines components of a proposed resource allocation service and also specifies interactions to the other service-oriented grid components. Experiments show that the proposed scheduling algorithm outperforms the conventional algorithm in terms of computing power utilization.
[grid scheduler, open systems, grid computing, intelligent resource selection algorithm, Intelligent networks, software architecture, resource allocation, Computer architecture, scheduling, Grid computing, Large-scale systems, learning (artificial intelligence), Dynamic scheduling, intelligent resource allocation service, interoperability, neural network learning, Intelligent structures, Scheduling algorithm, large-scale systems, service-oriented grid components, conventional scheduling algorithms, grid resource allocation, Neural networks, extreme learning machine, Machine learning, Resource management, neural nets]
A Symmetric Matchmaking Engine for Web Service Composition
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Matchmaking of Web services is a key step in Web service composition process. To enable automated Web service composition and achieve a better performance, matchmaking process needs formally representation of Web service capabilities, which should not be only machine readable, but also machine understandable. Based on our previous work on Web service composition prototype D-Composer, we bring out issues of Web service modeling by separating service type and instance on OWL-S. In this paper, we describe the design and implementation of a symmetric matchmaking engine (SME) based on symmetry of information exchange - both service provider and service consumer have to provide information to the matchmaking engine. We also provide the experimental comparison, which shows that our method has a better performance and scalability.
[D-Composer, Scalability, service provider, Ontologies, web service composition, service instance, formal specification, Web service capabilities, Manufacturing processes, service modeling, Web and internet services, Prototypes, Search engines, Web service composition, symmetric matchmaking, Process planning, OWL-S, Web service modeling, Sun, symmetric matchmaking engine, Web services, information exchange, ontologies (artificial intelligence), service type, Web service matchmaking, Artificial intelligence, service consumer]
Evaluation of a Performance Prediction Tool for Peer-to-Peer Distributed Computing Applications
2009 15th International Conference on Parallel and Distributed Systems
None
2009
In the search of new architecture for distributed computing, peer-to-peer is studied as a new way forward. But, its intrinsic properties like the absence of centralized topology or the dynamic reorganization of the network make it difficult to reach high performances. Furthermore, it is not trivial to execute applications on an existing testbed which brings together a sufficient number of nodes. That is why it is necessary to use a simulation environment which will be used to bypass bottlenecks and to correct the parts of the applications which slow down the execution time. In this context, we have proposed P2PPerf: a simulation tool which aims at predicting performance and the execution time of a distributed application before its finalization. This article presents some experiments conducted with P2PPerf to evaluate its accuracy. The emphasis is put on the use of real execution platforms with the Grid'5000 platform and with the Wrekavoc tool.
[simulation tool, Java, centralized topology, peer-to-peer computing, Peer to peer computing, Computational modeling, Wrekavoc tool, Application software, peer-to-peer distributed computing, distributed application, Distributed computing, Network topology, Computer architecture, Computer networks, Grid 5000, dynamic reorganization, performance prediction tool, Testing, Context modeling]
Comparing Methods for the Efficient Analysis of PEPA Models of Non-repudiation Protocols
2009 15th International Conference on Parallel and Distributed Systems
None
2009
In this paper we study the overhead introduced by secure functions in considering two models of non-repudiation protocols. The models are specified using the Markovian process algebra PEPA. The basic model suffers from the well known state space explosion problem when tackled using Markov chain analysis. Following previous study of performance modelling on security protocols, mean value analysis and fluid flow approximation based on ordinary differential equations (ODEs) have been chosen as efficient analysis techniques. Mean value analysis is an efficient exact method for deriving a limited set of metrics for large numbers of clients involved in the protocols. Fluid flow approximations can be adopted to solve the system with extremely large populations and potentially derive a wider range of metrics. The models are analyzed numerically and results derived from mean value analysis are compared with the ODE solution.
[Markov chain analysis, cryptographic protocols, PEPA, Stochastic processes, nonrepudiation protocols, Fluid flow, mean value analysis, Security, state space explosion problem, PEPA Models, differential equations, Algebra, public key cryptography, stochastic process algebra, non-repudiation, Performance analysis, Numerical models, fluid flow approximation, approximation theory, queueing theory, Explosions, State-space methods, Cryptographic protocols, security protocols, ordinary differential equations, Markovian process algebra, process algebra, Differential equations, Markov processes, fluid flow analysis]
Detection of Conjunctive Stable Predicates in Dynamic Systems
2009 15th International Conference on Parallel and Distributed Systems
None
2009
We present a distributed on-line algorithm for detecting conjunctive stable predicates in dynamic systems. The algorithm consists of a virtual network topology to maintain the causality relationships between distributed events and protocols to check the verification of the predicates over consistent global states. A lazy detection protocol has been developed to minimize the number of messages for the detection. Let M<sub>c</sub> be the number of detection messages, then the message complexity of the algorithm is M<sub>c</sub>  M + n<sub>0</sub>, where M is the number of computation messages and n<sub>0</sub> is the number of initial processes the computation starts with. Computation trees are maintained in the virtual network to reduce the detection delay. Suppose that h<sub>max</sub> is the maximum height of the computation trees. The detection delay is O(h<sub>max</sub> + n<sub>0</sub>). It has been proved that a solution is declared if and only if the global predicate is verified during the progress of the underlying distributed computation.
[Protocols, conjunctive stable predicate, distributed system, global predicate, Distributed computing, Delay, virtual network topology, Network topology, distributed events, Computer networks, protocols, Monitoring, detection messages, trees (mathematics), Debugging, distributed online algorithm, Synchronization, network topology, conjunctive stable predicates detection, dynamic system, global states, causality, causality relationships, Computer science, predicates verification, computation trees, distributed algorithms, dynamic systems, System recovery, lazy detection protocol]
iRank: Supporting Proximity Ranking for Peer-to-Peer Applications
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Proximity ranking according to end-to-end network distances (e.g., Round-Trip Time, RTT) can reveal detailed proximity information, which is important in network management and performance diagnosis in distributed systems. However, to the best of our knowledge, there has been no similar work on this subject in the P2P computing field. We present a distributed rating method iRank, that enables proximity rankings by providing discrete ratings in a distributed manner. It formulates the proximity ranking as a rating problem that faithfully captures the proximity based on noisy distance measurements scalably and practically. The primary challenge in inferring proximity rankings is enforcing distributed ratings with complex rating policies. Our solution is based on reconstructing ratings by decomposing a centralized rating method Maximum Margin Matrix Factorization (MMMF) into independent sub-problems, that can be efficiently solved in a decentralized manner. By relaxing the dependence on infrastructure nodes that are a single point of failure and limit scalability, iRank can gracefully handle network churns. Through real network latency data sets, we demonstrate that iRank can predict ratings with low distortion, which are smaller than 20 percentage worse than the centralized method, in the context of synthetic complex rating policies.
[Laboratories, maximum margin matrix factorization, detailed proximity information, Conference management, synthetic complex policies, matrix decomposition, Distributed computing, Concurrent computing, peer-to-peer applications, Network servers, Distributed processing, end-to-end network distances, distributed ratings, complex rating policies, Computer networks, network churns, iRank, proximity rankings, network management performance diagnosis, peer-to-peer computing, Peer to peer computing, discrete ratings, single point failure, network latency data sets, Application software, computer network management, proximity ranking, Computer network management, noisy distance measurements]
A Diffusive Load Balancing Scheme for Clustered Peer-to-Peer Systems
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Node clustering is an effective solution for achieving good performance and high reliability for peer-to-peer (P2P) systems. To improve the performance of a clustered P2P system, it is important to balance the service load among the clusters in the system. In this paper, we describe a diffusive load balancing scheme for clustered P2P systems, which dynamically adjusts the size of the clusters, by moving nodes among the clusters, based on their service demands and node resource capacities. Our simulations show that the proposed load balancing scheme significantly improves the performance of a P2P system in terms of balanced available capacity.
[Costs, peer-to-peer computing, Peer to peer computing, Service oriented architecture, server clusters, dynamic resource allocation, Reliability engineering, Information technology, diffusive load balancing scheme, diffusive load balancing, Delay, Web services, resource allocation, node resource capacities, peer-to-peer systems, distributed algorithms, clustered P2P system, Load management, clustered peer-to-peer systems, node clustering, Resource management, Load balancing, Distributed algorithms, performance management]
Cowtra: A COntribution Willingness-Based Two-phase Bandwidth Resource Allocation Algorithm in P2P Network
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Free-riding phenomenon is overwhelming in nowadays P2P network which causes researchers to investigate and develop many approaches to combat it. However, almost all the studies neglect the role of relative contribution of peers, namely willingness of contribution (WoC) in our paper. The ignorant to the ratio of peer's actually contribution to its physical capability would undoubtedly lead to unfairly resource allocation and discourage many peers, as well as loose their support in long term. Therefore, in this paper we present a novel and effective approach Cowtra to address such problem. Our algorithm guarantees that the bandwidth of a source node is distributed properly according to the absolute contribution of the competing peers. Then we adjust the amount of competing peers' received bandwidth by utilizing the WoC in the second phase, such that peers with higher WoC obtain more resource while peers with lower WoC otherwise. At last, simulation results demonstrate the superiority of our algorithm in terms of fairness and efficiency.
[fairness, peer-to-peer computing, Peer to peer computing, peer competition, Decision making, P2P network, Electronic mail, Game theory, Physics, two-phase bandwidth resource allocation algorithm, Information analysis, p2p, bandwidth allocation, Cowtra, resource allocation, free-rider, contribution willingness, Bandwidth, Channel allocation, peer contribution, free-riding phenomenon, source node bandwidth distribution, Resource management]
A Forwarding-Based Task Scheduling Algorithm for Distributed Web Crawling over DHTs
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Distributed Web crawling (DWC) over DHTs is proposed to solve the bottlenecks in the traditional Web crawling. The core of this kind of system is its fully distributed task scheduling mechanism in which the crawlers are treated as peers and the crawlees are treated as resources maintained by the peers. A system model based on the content addressable network (CAN) can further optimize the scheduling mechanism by exploiting the network proximity of the crawlers and the crawlees. In this paper, we propose a new method for CAN in order to achieve load balancing in the CAN-based DWC system. The method not only keeps the load balancing among peers but also keeps the distance between peers and resources very short in our simulations. The shortened peer-resource distance fulfills the need of shortening crawler-crawlee latencies.
[load balancing, Peer to peer computing, Scalability, network proximity, Crawlers, forwarding-based task scheduling algorithm, cryptography, Scheduling algorithm, Delay, Computer science, crawler-crawlee latencies, DHT, distributed Web crawling, Processor scheduling, resource allocation, distributed hash tables, content addressable network, scheduling, Load management, Computer networks, Robustness, Internet, Content Addressable Network, task scheduling]
A Steady Network Coordinate System for Network Distance Estimating
2009 15th International Conference on Parallel and Distributed Systems
None
2009
A lot of large distributed system can benefit from the implement of network coordinate system, which can estimate latencies among Internet hosts. In this paper, we focus on problems in building network coordinate system. Firstly, we analyze the disadvantages of some algorithms that based on fixed reference nodes and algorithms based on unfixed reference nodes. Then we propose a new architecture of network coordinate system, in which, network delay space is divided into several global clustering firstly, and a new way to select reference nodes and acquire coordinates is applied. According to the experiments on PlanetLab, our method proved to be accurate and steady.
[unfixed reference nodes, lantency estimation, Economic indicators, Peer to peer computing, large distributed system, Hydrogen, steady network coordinate system, global clustering, Topology, PlanetLab, Delay, Computer science, network distance estimation, network distance, Space technology, pattern clustering, network coordinates, Clustering algorithms, Bismuth, Internet, Internet hosts]
HyperSpring: Accurate and Stable Latency Estimation in the Hyperbolic Space
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Predicting network latencies between Internet hosts can efficiently support large-scale Internet applications, e.g., file sharing service and the overlay construction. Several study use the hyperbolic space to model the Internet dense-core and many-tendril structure. However, existing hyperbolic space based embedding approaches are not designed for accurate latency estimation in the distributed context. We present HyperSpring, which estimates latency by modelling a mass spring system in the hyperbolic similar with Vivaldi. HyperSpring adopts coordinate initialization to speed up the convergence of coordinate computation, uses multiple-round symmetric updates to escape from bad local minima, and stabilizes coordinates by compensating RTT measurements to reduce the coordinate drifts. Evaluation results based on a network trace of 226 PlanetLab nodes indicate that, compared to Euclidean-space based Vivaldi, hyperspring provides performance improvements for most nodes, and incurs slightly higher distortions for a small number of nodes.
[RTT measurements, Euclidean-space based Vivaldi, PlanetLab nodes, distributed processing, mass spring system, Delay, Convergence, Web and internet services, embedded systems, stable latency estimation, Large-scale systems, IP networks, Distortion measurement, Springs, Internet hosts, Internet dense-core, HyperSpring, Peer to peer computing, Extraterrestrial measurements, file sharing service, Coordinate measuring machines, multiple-round symmetric updates, Internet, many-tendril structure, hyperbolic space based embedding approach]
Using Grid Middleware to Query a Heterogeneous Distributed Version of the SDSS Database
2009 15th International Conference on Parallel and Distributed Systems
None
2009
This paper explores the use of Grid technologies for the manipulation of the Sloan Digital Sky Survey (SDSS) database. The paper first provides background information on the SDSS database, OGSA-DAI operations and OGSA-DQP operators. We then analyse the execution of OGSA-DQP for querying a heterogeneous distributed version of the SDSS database. In particular, we examine the different SOAP interactions between OGSA-DQP components and look at simple queries plans. After modifying the OGSA-DQP coordinator we successfully ran OGSA-DQP queries against the distributed SDSS database.
[grid computing, Relational databases, OGSA-DAI operation, Simple object access protocol, query processing, astronomy computing, Distributed databases, distributed databases, Database systems, OGSA-DQP operator, Mirrors, Photometry, OGSA-DQP, middleware, Sloan Digital Sky Survey (SDSS), Scientific Database, Spectroscopy, database query, OGSA-DAI, Grid middleware, heterogeneous distributed SDSS database, Middleware, Complex Database Schemas, Radio access networks, Computer science, Sloan Digital Sky Survey, grid middleware, scientific datasets]
A User-Level Remote Paging System for Grid Computing
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Most of grid applications require a large amount of memory resources. If there is not enough memory supplied during execution, the program performance will be significantly degraded by page swapping. To address this problem, we propose a remote paging system called Grid Remote Pager (GRP) for grid computing in this paper. Compared with other remote paging systems, GRP keeps high portability and transparency. The more important matter is that it actively adapts the limit of the amount of memory available for user programs according to the memory load of owner jobs. Consequently, not only the competition of memory between owner programs and user programs is effectively alleviated but also sufficient exploitation of free memory is achieved by user programs with the support of GRP.
[paged storage, Costs, Ethernet networks, grid remote pager, Weather forecasting, grid computing, user level remote paging system, page swapping, remote paging, Delay, Degradation, memory reservation, High-speed networks, Operating systems, Physics computing, memory-utilization adaptation, Grid computing, memory resources, memory competition, Bioinformatics]
Extending POP-C++ to Integrate Web Services into Parallel Objects
2009 15th International Conference on Parallel and Distributed Systems
None
2009
A grid environment is a collection of a large number of geographically distributed resources managed by various organizations. And it is considered as a powerful computing infrastructure to solve large and complex problems. To support the software development, until now, many grid programming languages have been investigated. Most of them use proprietary protocols or TCP-based protocols to communicate. Connectivity, especially the network firewall and the heterogeneous interoperability is one of the big issues for grid applications and development tools. POP-C++, which is a programming language adding a new type of parallel object to C++, also faces the same problem in its communication protocol which heavily based on TCP socket/XDR. This research aims at providing POP-C++ a novel mechanism for objects to choose a suitable protocol and message encoding to communicate among them. Web Services, which is a broadly accepted standard, is a good choice because it is widely used in structuring interactions among distributed software services. Its communications mostly use SOAP buffer over HTTP protocol which is normally allowed in all firewalls. In this paper, we discuss an extension of POP-C++ toward HTTP/SOAP to overcome the connectivity issue.
[open systems, SOAP buffer, grid computing, hypermedia, development tools, Programming, message encoding, communication protocol, HTTP protocol, network firewall, Simple object access protocol, Environmental management, parallel programming, POP-C++. Grid computing. parallel objects. Web Services. HTTP. SOAP, authorisation, Software standards, software development, heterogeneous interoperability, parallel objects, XDR, Encoding, C++ language, Application software, geographically distributed resource management, Computer languages, proprietary protocols, Web services, Sockets, transport protocols, TCP-based protocols, POP-C++, Resource management, grid programming languages]
Sequential Pattern Mining in Data Streams Using the Weighted Sliding Window Model
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Mining data streams for knowledge discovery is important to many applications, including Web click stream mining, network intrusion detection, and on-line transaction analysis. In this paper, by analyzing data characteristics, we propose an efficient algorithm SWSS (Sequential pattern mining with the weighted sliding window model in SPAM) to mine frequent sequential patterns based on the weighted sliding windows model. This algorithm provides more space for users to specify which sequences they are more interested in. Extensive experiments show that the proposed algorithm is feasible and efficient for mining all sequential patterns as users specified.
[Algorithm design and analysis, Data analysis, data streams mining, Unsolicited electronic mail, data mining, Data Mining, knowledge discovery, Data mining, Information analysis, Information science, Stream Mining, Itemsets, network intrusion detection, frequent sequential patterns, Intrusion detection, weighted sliding window model, Sliding Window, sequential pattern mining, Data models, Web click stream mining, Sequential Pattern Mining, Pattern analysis, online transaction analysis]
SPBCA: Semantic Pattern-Based Context-Aware Middleware
2009 15th International Conference on Parallel and Distributed Systems
None
2009
This paper proposes a semantic pattern-based context-aware (SPBCA) middleware for ubiquitous computing environments. It introduces two concepts, the ontology and the proactive conversion, to solve the heterogeneity issues existing in the interaction process in the ubiquitous computing environments. Furthermore, it can support user-centric context awareness not only by automatic reactive information processing, but also by service customization at run-time. To enable semantic pattern adaptation, we employ ontologies to define semantics of various concepts. The functions of proactive conversion and run-time service customization are implemented with a reactive event mechanism. User defined services are represented in SWRL language which is triggered by Pellet engine. The SPBCA middleware is applicable to an internal, local network environment. Hence, we exploit a tuple space based decentralized architecture as our middleware basis which supports asynchronous, context-sensitive communication, and privacy sensitivity.
[Context-aware services, automatic reactive information processing, user-centric context awareness, tuple space based decentralized architecture, Context awareness, Pulp manufacturing, Ontologies, Ubiquitous computing, context-sensitive communication, ubiquitous computing, Middleware, Information systems, run-time service customization, semantic pattern-based context-aware middleware, proactive conversion, Runtime, SWRL language, Information processing, Computer architecture, Pellet engine, ontologies (artificial intelligence), ontology, middleware, privacy sensitivity]
The Vulnerability Analysis Framework for Java Bytecode
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Since Java web applications are used widely in Internet today, the security of it becomes an outstanding problem. The attacks, including SQL injection attack, XSS attack, and etc, are great challenges for the Java application. This paper presents the vulnerability analysis framework to detect the security hole in the Java web applications. The framework combines the techniques of the static points-to dataflow analysis, the dynamical instrument and the fuzzing test. With the cooperation of these static and dynamical analysis techniques, it can improve the efficiency and accuracy of the analysis and lower the false positive rate.
[Java, Data analysis, false positive rate, Java bytecode, Data security, dynamical analysis, analysis framework, data flow analysis, static analysis, security hole, Application software, Programming profession, Information analysis, vulnerability analysis, fuzzing test, security of data, static points-to-dataflow analysis, Systems engineering and theory, Internet, Performance analysis, Testing]
Research on Technologies of Building Experimental Environment for Network Worm Simulation
2009 15th International Conference on Parallel and Distributed Systems
None
2009
The worm experimental environment is a pivotal foundation for the worm research. In this paper, with a comprehensive analysis of existing technologies for building worm experimental environment, including analytical model, packet-level simulation, network simulation, hybrid method, and so on, we present a novel virtual-real hybrid worm simulation model. This model integrates the advantages of network simulation and packet-level simulation, and thus achieves a preferable balance between the fidelity and scalability. Our model builds a promising foundation for constructing a flexible and extensible worm experimental environment.
[invasive software, packet-level simulation, Computer worms, Data analysis, virtual reality, virtual-real hybrid worm simulation model, hybrid method, Scalability, Computational modeling, network worm, worm simulation, Information retrieval, network worm simulation, Analytical models, Layout, worm experimental environment, worm model, experimental environment, Systems engineering and theory, Large-scale systems, Monitoring]
Adaptive Multi-versioning for OpenMP Parallelization via Machine Learning
2009 15th International Conference on Parallel and Distributed Systems
None
2009
The introduction of multi-core architectures generates a higher demand for parallelism in order to fully exploit the potential of modern computers. It is of vital importance that a compiler can allocate parallel workload in a cost-aware manner in order to achieve optimal performance on a multi-core architecture. This paper presents an adaptive OpenMP-based mechanism capable of generating a reasonable number of representative multi-threaded versions for a given loop, and selecting at runtime a suitable version to execute on a multi-core architecture. Preliminary experimental results show that, on average, it achieves 87% of the highest performance improvement across a whole spectrum of input sizes on two multi-core platforms.
[parallel workload, Costs, multiprocessing systems, multithreaded loop versions, application program interfaces, multi-threading, OpenMP, machine learning, parallelization, Yarn, performance improvement, Programming profession, Computer science, Concurrent computing, multicore architectures performance optimisation, OpenMP-based mechanism, Runtime, multi-versioning, Machine learning, Computer architecture, Parallel processing, Hardware, openMP parallelization multiversioning, learning (artificial intelligence)]
Parallel Lexicographic Names Construction with CUDA
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Suffix array is a simpler and compact alternative to the suffix tree, lexicographic name construction is the fundamental building block in suffix array construction process. This paper depicts the design issues of first data parallel implementation of the lexicographic name construction algorithm on a commodity multiprocessor GPU using the Compute Unified Device Architecture (CUDA) platform, both from NVIDIA Corporation. The full parallel version runs much faster than any serial implementation on CPU. The thread level parallel code block provides an efficient primitive for building a high performance suffix array construction program and many other applications.
[thread level parallel code block, coprocessors, Yarn, parallel processing, GPGPU, Concurrent computing, suffix array construction program, data parallel implementation, Computer architecture, Computer graphics, Parallel processing, Hardware, compute unified device architecture platform, Bioinformatics, multiprocessing systems, Buildings, trees (mathematics), parallel lexicographic names construction, commodity multiprocessor GPU, Sorting, suffix array construction process, CUDA, Lexicographic name construction, Parallel programming, NVIDIA Corporation, lexicographic name construction algorithm, suffix tree]
Count Sort for GPU Computing
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Counting sort is a simple, stable and efficient sort algorithm with linear running time, which is a fundamental building block for many applications. This paper depicts the design issues of a data parallel implementation of the count sort algorithm on a commodity multiprocessor GPU using the Compute Unified Device Architecture (CUDA) platform, both from NVIDIA Corporation. The full parallel version runs much faster than any serial implementation on CPU with the loss of stability due to the limitation of the massive threads parallel model. But the thread-level parallel implementation still provides an efficient parallel sort primitive for many applications, which do not require stable sort or can be adapted for unstable subroutines.
[Count sort, linear running time, Yarn, parallel processing, GPGPU, Concurrent computing, software architecture, data parallel implementation, Computer architecture, Computer graphics, sorting, Parallel processing, Hardware, Bioinformatics, sort algorithm, multiprocessing programs, counting sort, compute unified device architecture, Application software, commodity multiprocessor GPU, Programming profession, CUDA, Parallel programming, CUDA platform, GPU computing]
Low Overhead Object Communication Scheme in CMP Implementation of Object-Oriented Programming
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Object-oriented (OO) programming appears as a promising framework for parallel programming. Objects which are inherently concurrent communicate through message passing. But most of OO languages choose to avoid supporting message passing directly because the implementation is too complicated. As we enter the era of CMP with multiple cores on a single die, real message passing can be realized. Objects can be allocated to cores and communicate through the network on chip. In this paper, we propose a hardware object communication scheme based on CMP, which realizes the message passing with low overhead and high efficiency. The locality of messages in object-oriented programs is discussed particularly. We implement our scheme in SystemC and evaluated it using a SystemC and ISS co-simulation method. The results show the hit ratio of message mapping can reach 92% and the speedup of parallel OO programs executed in 9-core CMP is about 6 compared with sequential OO programs at best.
[network-on-chip, object-oriented programming, ISS cosimulation method, CMP, network on chip, Application software, Yarn, SystemC cosimulation method, parallel programming, message mapping, Computer science, chip multiprocessors, CMP implementation, Parallel programming, Message passing, Network-on-a-chip, Computer architecture, Hardware, low overhead object communication scheme, Object Communication, Acceleration, Object oriented programming, Object-oriented]
A Virtual Machine Server Aggregation Algorithm Based on Hierarchical Clustering
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Combining virtual machine technology and network computing technology will be able to effectively aggregate the widely distributed heterogeneous and autonomous resources in the Internet. This paper proposes a virtual machine server aggregation algorithm, called DVSA, based on hierarchical clustering method for virtual computing environment. According to network latencies, the algorithm clusters virtual machine servers into groups. If servers in the same group are scheduled, the latencies between virtual machines which host on these servers will be small, and the performance and stability of the distributed execution environment could be improved.
[Computers, DVSA, Clustering methods, virtual machine server aggregation algorithm, hierarchical clustering method, Virtual machining, Delay, Network servers, network computing technology, Processor scheduling, virtual machine, Aggregates, pattern clustering, Clustering algorithms, algorithm theory, virtual machines, hierarchical clustering, distributed execution environment stability, Internet, IP networks, Web server, resource aggregation]
Enhancing Reliability for Virtual Machines via Continual Migration
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Our approach is to design and implement a continual migration strategy for virtual machines to achieve automatic failure recovery. By continually and transparently propagating virtual machine's state to a backup host via live migration techniques, trivial applications encapsulated in the virtual machine can be recovered from hardware failures with minimal downtime while no modifications are required. Deployment agility is considered so that initiating continual migration can be done with no time consuming preparations. Experimental results show that virtual machine in a continual migration system can be recovered in less than one second after a failure is detected, while performance impact to the protected virtual machine can be reduced to 30%.
[virtual machines reliability, hardware failures, deployment agility, virtualization, fault-tolerance, Reliability engineering, continual migration system, Virtual machining, availability, system recovery, Computer science, Virtual machine monitors, automatic failure recovery, Fault tolerant systems, live migration techniques, Prototypes, virtual machines, Hardware, fault tolerant computing, Image storage, Protection, Frequency synchronization, continual migration strategy]
Creating Virtual Networks for Distributed Virtual Computing Environment
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Virtual network is an important approach to support multiple legacy applications running unmodified in distributed virtual computing environments. A virtual networking approach called VirNet is proposed in this paper. VirNet can build multiple customized and isolated virtual networks simultaneously on the same physical hosts in the network, and it enables existing distributed applications written for LANs to run seamlessly on Internet. VirNet is designed completely in user space and requires no change to the kernel of operating systems. Experimental evaluation performed by a reference implementation shows that VirNet is efficient. The latency overhead caused by VirNet is very small and the bandwidth of VirNet is more than 85% of the available physical network bandwidth in both the LAN and emulated WAN environments.
[Performance evaluation, Wide area networks, emulated WAN environments, virtual reality, VirNet, multiple legacy applications, local area networks, software maintenance, Distributed computing, distributed applications, Delay, virtual networks, Operating systems, Bandwidth, distributed virtual computing environment, LAN, Computer networks, Internet, IP networks, Kernel, Local area networks]
A Runtime Reputation Based Grid Resource Selection Algorithm on the Open Science Grid
2009 15th International Conference on Parallel and Distributed Systems
None
2009
The scheduling and execution for grid application is an important problem in the grid environment. To get the high reliability and efficiency, we propose a runtime reputation based grid resource selection algorithm. According to the accumulated raw score, the runtime reputation degree for a grid resource is quantified as an evaluating score in the runtime of an application. Instead of being dependent on the historical experiences, it is dynamically adaptive to the runtime availability, load, and performance of the grid resources. The execution framework on the grid is based on Globus Toolkit and Swift system. In a real production grid, Open Science Grid (OSG), a typical grid application with large scale independent jobs was experimented, which was based on BLAST application. The experimental results for the performance of different policies are presented, with a benchmarking workload size of 10,000 jobs. The runtime reputation and behavior statistics for the grid resources are also presented.
[grid computing, BLAST application, Swift system, runtime reputation, Grid computing, benchmarking workload, Blast application, Large-scale systems, grid resource selection algorithm, Availability, Runtime environment, globus toolkit, OSG, Job listing service, accumulated raw score, Resource selection, Application software, Grid scheduling, Scheduling algorithm, production grid, Processor scheduling, open science grid, High performance computing, Runtime reputation, Resource management]
GOS Security: Design and Implementation
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Grid technology has being widely accepted in distributed resources sharing and high performance computing cross multi administrative domains. In this paper, we analysis the security issues in grid computing environments, and propose a security framework for VegaGOS which is a service oriented architecture middleware developed for the China National Grid. We address mutual authentication using certificate with digital signature. We address authorization through combining VO level access control decision and resource level enforcement. Communication security is guaranteed by TLS/SSL at transport level and WS-security at message level. This security framework has been implemented in VegaGOS and deployed in China National Grid Environment.
[grid computing, certificate, VegaGOS, Security, grid technology, mutual authentication, authorization, software architecture, VO level access control decision, resource allocation, digital signature, WS-security, distributed resources sharing, authorisation, communication security, digital signatures, high performance computing, service oriented architecture, GOS security, middleware, resource level enforcement]
Solving 2D Nonlinear Unsteady Convection-Diffusion Equations on Heterogenous Platforms with Multiple GPUs
2009 15th International Conference on Parallel and Distributed Systems
None
2009
Solving complex convection-diffusion equations is very important to many practical mathematical and physical problems. After the finite difference discretization, most of the time for equations solution is spent on sparse linear equation solvers. In this paper, our goal is to solve 2D Nonlinear Unsteady Convection-Diffusion Equations by accelerating an iterative algorithm named Jacobi-preconditioned QMRCGSTAB on a heterogenous platform, which is composed of a multi-core processor and multiple GPUs. Firstly, a basic implementation and evaluation for adapting the problem to this kind of platform is given. Then, we propose two optimization methods to improve the performance: kernel merging method and matrix boundary data processing. Our experimental evaluation on an AMD Opteron(tm) quad-core processor 2380 linked to an NVIDIA Tesla S1070 platform with four GPUs delivers the peak performance of 33 GFLOPS (double precision), which is a speedup of close to a factor 32 compared to the same problem running on 4 cores of the same CPU.
[iterative methods, accelerating iterative algorithm, Merging, Optimization methods, finite difference methods, coprocessors, matrix boundary data processing, NUCDE solver, GPU, Jacobian matrices, Jacobi preconditioned QMRCGSTAB, kernel merging method, accelerate, multiple GPU, multicore processor, Kernel, finite difference discretization, Finite difference methods, sparse linear equation solvers, Nonlinear equations, Multicore processing, heterogenous, mathematical analysis, 2D nonlinear unsteady convection-diffusion equations, computer graphics, Difference equations, quad core processor, PQMRCGSTAB, Iterative algorithms, Acceleration]
A Process Fusion Approach for MPI Performance Enhancement on Multi-core Systems
2009 15th International Conference on Parallel and Distributed Systems
None
2009
With the growing popularity of multi-core technology, traditional MPI programs are facing great challenges in fully exploit the computing power of this kind of chipsets. In this paper, we change the run time orgnaization of a MPI program and use some newly designed communication methods to enhance the performance of MPI programs on a multicore system without changing the code. We have evaluated the test results and the potential "side effects'' of this technique is discussed in the paper as well. The remarks on the related and possible future research work is also given in the end of the paper.
[Costs, message passing, multiprocessing systems, Multicore processing, Runtime library, Scalability, Design methodology, process fusion, Yarn, multicore systems, MPI programs, Message passing, Parallel processing, Large-scale systems, Testing]
Domain-specific Pattern Matching Based Automatic Parallelization: Demonstrated by 2-D Prestack Migration
2009 15th International Conference on Parallel and Distributed Systems
None
2009
How to automatically parallel large amount of sequential legacy code has become one of the key issues, especially for domain specific scientific applications. It is more urgent as multi-core processors are increasingly popular. Taking the extremely compute and I/O intensive prestack migration application of the geophysics exploration as an example, this paper discusses its algorithms and implementations, analyzes the data distribution and computing steps, and then proposes an automatic parallel method based on source code pattern matching. We developed a software tool, which can automatically parallel ALL the 2-D prestack migration programs in the popular open source software package Seismic Unix (SU), from sequential C code to MPI code. The experimental results on a 64-node NUMA server show that under heavy workload conditions, the parallel efficiency is more than 95%. Our method can be applied to parallelize a series of programs with similar characteristics. We believe this work may serve as the solid step to motive further researches of domain specific automatic parallelization for the benefit of multi-core processors.
[Algorithm design and analysis, Unix, pattern matching, Geophysics computing, Distributed computing, parallel processing, multicore, Concurrent computing, automatic parallel, open source software package Seismic Unix, prestack migration, software packages, Pattern analysis, geophysics exploration, seismic unix, Data analysis, Multicore processing, domain-specific pattern matching, source code pattern matching, Application software, 2-D prestack migration programs, I/O intensive prestack migration, NUMA server, multi-core processors, Software tools, Pattern matching]
Message from the General and Program Co-chairs
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Presents the welcome message from the conference proceedings.
[]
Organizing Committee
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Provides a listing of current committee members.
[]
Program Committee
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Provides a listing of current committee members.
[]
Keynote abstracts
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Provides an abstract for each of the keynote presentations and a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.
[]
A Discrete Event Simulation Model for Understanding Kernel Lock Thrashing on Multi-core Architectures
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Multi-core architectures have become mainstream. Trends suggest that the number of cores integrated on a single chip will increase continuously. However, lock contention in operating systems can limit the parallel scalability on multi-cores so significantly that the speedup decreases with the increasing number of cores (thrashing). Although the phenomenon can be easily reproduced experimentally, most existing lock models are not able to do so. To overcome this challenge, this paper develops a discrete event simulation model which has the capability of capturing both the sequential execution in critical sections and the contention for shared hardware resources. The model is evaluated using a series of typical parameter configurations which can represent different degrees of lock contention. Experimental results suggest that the thrashing phenomenon can be observed when the model parameters are selected properly. To further understand this phenomenon, statistics such as the percentage of time spent waiting for locks and the number of cores waiting for a lock are exploited to characterize the lock thrashing. In addition, the model sensitivity to changes in memory latency and hardware architectures are also examined. Finally, we use this model to compare three methods which are proposed for preventing the lock thrashing.
[model sensitivity, multi-core, kernel lock thrashing, sequential execution, parallel architectures, lock contention, memory latency, discrete event simulation model, Discrete event simulation, Servers, parameter configurations, cores thrashing, thrashing phenomenon, Hardware, discrete event simulation, hardware architectures, multiprocessing systems, Multicore processing, thrashing, operating systems, Data structures, multicores, Synchronization, single chip, shared hardware resources, time spent waiting for locks, model parameters, multicore architectures, operating systems (computers), parallel scalability]
Privacy Protection in Participatory Sensing Applications Requiring Fine-Grained Locations
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
The emerging participatory sensing applications have brought a privacy risk where users expose their location information. Most of the existing solutions preserve location privacy by generalizing a precise user location to a coarse-grained location, and hence they cannot be applied in those applications requiring fine-grained location information. To address this issue, in this paper we propose a novel method to preserve location privacy by anonymizing coarse-grained locations and retaining fine-grained locations using Attribute Based Encryption (ABE). In addition, we do not assume the service provider is an trustworthy entity, making our solution more feasible to practical applications. We present and analyze our security model, and evaluate the performance and scalability of our system.
[ABE, kanonymity, participatory sensing, cryptography, Entropy, Encryption, privacy risk, Privacy, user location, fine-grained location information, Location privacy, Tiles, privacy protection, Public key, attribute based encryption, data privacy, fine-grained locations, Sensors, anonymizing coarse-grained locations, participatory sensing applications]
A Scheduling Method for Avoiding Kernel Lock Thrashing on Multi-cores
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Multi-core architectures have been adopted in various computing environments. Predictions based on Moore's Law state that thousands of cores can be integrated on a single chip within 10 years. To achieve better performance and scalability on multi-cores, applications should be multi-threaded, and therefore threads assigned on different cores can execute concurrently. However, lock contention in kernels can affect the scalability so significantly that the speedup decreases with the increasing number of cores (thrashing). Existing efforts to address this problem mainly focus on deferring lock thrashing, and therefore these techniques cannot prevent thrashing fundamentally. In this paper, we propose to use lock-aware scheduling to avoid thrashing. Our method detects thrashing on a per-thread basis and migrates contended threads to a smaller set of cores. The optimal number of cores is determined by maximizing the proposed normalized throughput model of migrated threads. The proposed method is implemented in Linux 2.6.29.4 and evaluated on a 32-core system. Experimental results on a series of lock-intensive micro- and macro-benchmarks show the effectiveness: for 3 of 5 workloads exhibiting thrashing behaviour, lock-aware scheduling can detect the speedup decrease accurately and sustain the maximal speedup, for the remaining 2 workloads, the performance can be improved greatly although the maximal speedup is not sustained, for 1 workload which does not suffer thrashing, the method introduces negligible runtime overhead.
[multi-core, kernel lock thrashing, Instruction sets, parallel architectures, Linux 2.6.29.4, Throughput, Complexity theory, processor scheduling, lock thrashing, Moore's law, Benchmark testing, scheduling, migrated threads, Kernel, thrashing behaviour, 32-core system, per-thread basis, operating system kernels, multiprocessing systems, computing environment, Multicore processing, multicore architecture, normalized throughput model, lockaware scheduling, lock-aware scheduling, Linux, negligible runtime overhead]
Towards a P2P Cloud: Reliable Resource Reservations in Unreliable P2P Systems
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
The peer-to-peer paradigm shows the potential to provide the same functionality and quality like client/server based systems, but with much lower costs. However, the resources, e.g. storage space, CPU power and online time, provided by the peers are unreliable due to churn. In order to enable churn resistant reliable services using the resources in p2p systems, we propose in this paper a distributed mechanism termed P3R3O.KOM. The mechanism allows to reserve, monitor and use resources provided by the unreliable p2p system and maintains long-term resource reservations through controlled redundant resource provision. Evaluation shows that using KAD measurements on the prediction of the lifetime of peers allows for 100% successful reservations under churn with very low traffic overhead. This approach marks a first step for the building of a reliable p2p-based SOA and future p2p-based clouds.
[Measurement, Protocols, P3R3OKOM, peer-to-peer computing, Peer to peer computing, reliable resource reservations, SOA, Redundancy, Service oriented architecture, Probability, unreliable P2P systems, quality of service, p2p, cloud, P2P cloud, client-server based systems, cloud computing, guarantees, Monitoring, service-oriented architecture, distributed services]
An Effective Framework of Light-Weight Handling for Three-Level Fine-Grained Recoverable Temporal Violations in Scientific Workflows
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Temporal violations may often take place and deteriorate the overall QoS of scientific workflows. To handle temporal violations in an automatic and cost-effective fashion, we need to resolve the following issues: 1) how to define fine-grained recoverable temporal violations, 2) which light-weight effective exception handling strategies to be facilitated. This paper proposes an effective exception handling framework. Based on a probability based temporal consistency model, the probability range for recoverable temporal violations is divided into three levels of fine-grained temporal violations. Afterwards, three corresponding light-weight exception handling strategies including TDA (Time Deficit Allocation), ACOWR (Ant Colony Optimisation based two-stage Workflow local Rescheduling) and TDA+ACOWR (the combined strategy of TDA and ACOWR) are presented. The experimental results demonstrate the excellent performance of our framework in reducing both local and global temporal violations.
[scientific workflow, Quality of service, Gaussian distribution, Optimization, Workflow Scheduling, TDA, optimisation, QoS, Scientific Workflows, natural sciences computing, ant colony optimisation, middleware, Workflow QoS, light-weight exception handling strategy, Redundancy, probability, Temporal Violations, quality of service, workflow local rescheduling, fine-grained recoverable temporal violation, probability based temporal consistency, Processor scheduling, time deficit allocation, Exception Handling, ACOWR, Artificial intelligence, light-weight handling]
Resilient Virtual Network Service Provision in Network Virtualization Environments
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Network Virtualization has recently emerged to provide scalable, customized and on-demand virtual network services over a shared substrate network. How to provide VN services with resiliency guarantees against network failures has become a critical issue, meanwhile the service resource usages should be minimized under the strict constraints such as link bandwidth capability and service resiliency guarantees etc. In this paper, we present a resource allocation algorithm to balance the tradeoff between service resource consumptions and service resiliency. By exploiting a heuristic VN mapping scheme and a restoration path selection scheme based on intelligent bandwidth sharing, the algorithm simultaneously makes cost-effective usage of network resources and protects VN services against network failures. We perform evaluations and find that the algorithm is near optimal in terms of network resource usage, especially the additional restoration bandwidth cost for resiliency protection.
[Algorithm design and analysis, bandwidth capability, shared substrate network, Switches, service resource usages, intelligent bandwidth sharing, virutal network mapping, network virtualization environments, resource allocation, network failures, resilient virtual network service provision, Bandwidth, Economics, on-demand virtual network services, computer networks, resiliency, network virtualization, Substrates, Equations, service resiliency guarantees, bandwidth allocation, heuristic VN mapping, network resources, Resource management, virtual network service]
Towards a Common Interface for Overlay Network Simulators
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Simulation has become an important evaluation method in the area of Peer-to-Peer (P2P) research due to the scalability limitations of evaluation test beds such as Planet Lab or G-Lab. Current simulators provide various abstraction levels for different underlay models, such that applications can be evaluated at different granularity. However, existing simulators suffer from a lack of interoperability and portability making the comparison of research results extremely difficult. To overcome this problem, we present an approach for a generic application interface for discrete-event P2P overlay network simulators. It enables porting of the same implementation of a targeted application once and then running it on various simulators as well as in a real network environment, thereby enabling a diverse and extensive evaluation. We established the feasibility of our approach and showed negligible memory and runtime overhead.
[Protocols, runtime overhead, open systems, Adaptation model, simulation, abstraction level, Engines, portability, peer-to-peer research, Runtime, Simulator-Design, scalability limitation, Prototypes, underlay model, Overlay, Testbed, discrete event P2P overlay network simulator, peer-to-peer computing, Peer to peer computing, interoperability, Peer-to-Peer, negligible memory overhead, Simulation, granularity, Common API, common interface, Data models, Interface]
VMGuard: An Integrity Monitoring System for Management Virtual Machines
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
A cloud computing provider can dynamically allocate virtual machines (VM) based on the needs of the customers, while maintaining the privileged access to the Management Virtual Machine that directly manages the hardware and supports the guest VMs. The customers must trust the cloud providers to protect the confidentiality and integrity of their applications and data. However, as the VMs from different customers are running on the same host, an attack to the management virtual machine will easily lead to the compromise of the guest VMs. Therefore, it is critical for a cloud computing system to ensure the trustworthiness of management VMs. To this end, we propose VMGuard, an integrity monitoring and detecting system for management virtual machines in a distributed environment. VMGuard utilizes a special VM, Guard Domain, which runs on each physical node to monitor the co-resident management VMs. The integrity measurements collected by the Guard Domains are sent to the VMGuard server for safe store and independent analysis. The experimental evaluation of a Xen-based prototype shows that VMGuard can quickly detect the root kit attacks while the performance overhead is low.
[Integrity Monitoring, cloud computing system, guard domain, detecting system, Area measurement, Remote I/O, Virtual machining, distributed environment, Servers, management virtual machine, Engines, VMGuard server, coresident management, virtual machines, independent analysis, system monitoring, integrity monitoring system, cloud providers, cloud computing, Kernel, Monitoring, cloud computing provider, Driver circuits, Virtual Machine]
Detection of Behavioral Contextual Properties in Asynchronous Pervasive Computing Environments
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Detection of contextual properties is one of the primary approaches to enabling context-awareness. In order to adapt to temporal evolution of the pervasive computing environment, context-aware applications often need to detect behavioral properties specified over the contexts. This problem is challenging mainly due to the intrinsic asynchrony of pervasive computing environments. However, existing schemes implicitly assume the availability of a global clock or synchronous coordination, thus not working in asynchronous environments. We argue that in pervasive computing environments, the concept of time needs to be reexamined. Toward this objective, we propose the Ordering Global Activity (OGA) algorithm, which detects behavioral contextual properties in asynchronous environments. The essence of our approach is to utilize the message causality and its on-the-fly coding as logical vector clocks. The OGA algorithm is implemented and evaluated based on the open-source context-aware middleware MIPA. The evaluation results show the impact of asynchrony on the detection of contextual properties, which justifies the primary motivation of our work. They also show that OGA can achieve accurate detection of contextual properties in dynamic pervasive computing environments.
[Context, Pervasive computing, global clock, Neodymium, open-source context-aware middleware MIPA, asynchronous pervasive computing environment, ubiquitous computing, message causality, Middleware, Concurrent computing, ordering global activity, logical vector clock, Local activities, OGA algorithm, on-the-fly coding, behavioral contextual property, Clocks, middleware]
Utilizing RF Interference to Enable Private Estimation in RFID Systems
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Counting or estimating the number of tags is crucial for RFID system. Researchers have proposed several fast cardinality estimation schemes to estimate the quantity of a batch of tags within a short time frame. Existing estimation schemes scarcely consider the privacy issue. Without effective protection, the adversary can utilize the responding signals to estimate the number of tags as accurate as the valid reader. To address this issue, we propose a novel privacy-preserving estimation scheme, termed as MEAS, which provides an active RF countermeasure against the estimation from invalid readers. MEAS comprises of two components, an Estimation Interference Device (EID) and two well-designed Interference Blanking Estimators (IBE). EID is deployed with the tags to actively generate interfering signals, which introduce sufficiently large estimation errors to invalid or malicious readers. Using a secret interference factor shared with EID, a valid reader can perform accurate estimation via two IBEs. Our theoretical analysis and simulation results show the effectiveness of MEAS. Meanwhile, MEAS can also maintain a high estimation accuracy using IBEs.
[Algorithm design and analysis, privacy issue, radiofrequency identification, interference factor, MIMICs, privacy preserving estimation, Entropy, malicious readers, Privacy, estimation interference device, Estimation error, interference suppression, interference blanking estimators, Interference, RF interference, RFID, accurate estimation, MEAS, Cardinality Estimation, RF countermeasure, estimation errors, RFID systems, fast cardinality estimation, private estimation, data privacy, Radiofrequency identification]
Automatic Thread Decomposition for Pipelined Multithreading
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
An appropriate automatic thread decomposition approach is critical for pipelined multithreading (PMT) to maximize pipeline performance with balanced thread size on target multi-core processor. This paper presents an automatic thread decomposition approach, which maps the pipeline thread decomposition problem onto a graph-theoretic framework to construct an optimized DAG with minimal bottleneck node size and balanced node size under constrained core number. In this approach, control dependence is treated as special data dependence and then an effective mechanism is proposed to remove redundant control dependences. A heuristic decomposition algorithm is given to generate an optimized pipeline. The algorithm has been evaluated on a commodity multi-core processor, and experimental results show that it has achieved speedup ranging from 113% to 174% on several SPEC CPU 2000 benchmark programs.
[multiprocessing systems, graph-theoretic framework, Multicore processing, multi-threading, Instruction sets, Heuristic algorithms, heuristic decomposition algorithm, Pipelines, graph theory, Pipelined multithreading, DAG, multi-core processor, automatic thread decomposition, Optimization, Pipeline processing, optimized pipeline, Multithreading, pipelined multithreading, optimized DAG, multi-core processors, pipeline processing]
PicFS: The Privacy-Enhancing Image-Based Collaborative File System
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Cloud computing makes available a vast amount of computation and storage resources in the pay-as-you-go manner. However, the users of cloud storage have to trust the providers to ensure the data privacy and confidentiality. In this paper, we present the Privacy-enhancing Image-based Collaborative File System (PicFS), a network file system that steganographically encodes itself into images and provides anonymous uploads and downloads from a media sharing website. PicFS provides plausible deniability by preventing traffic and image analysis by any third party from revealing the existence of PicFS or compromising its data. Because all accesses are anonymized, users of PicFS are dissociated from their data, which protects users against being compelled to release their keys. For further security and ease of use, we develop a method for automatically generating a large set of non-suspicious images to serve as input to the system. Our prototype leverages a number of existing technologies, including the F5 algorithm for steganography, Quick-Flickr for Flickr API access, Tor for anonymization, and FUSE-J for user-level file system calls. We show that the PicFS is indeed practical as the prototype demonstrates satisfactory performance in the real-world environment.
[Measurement, Flickr API access, Videos, Prototypes, FUSE-J, Quick-Flickr, cloud computing, network file system, media sharing Website, Video sequences, Media, plausible deniability, PicFS, privacy-enhancing image-based collaborative file system, File system, F5 algorithm, steganography, Tor, Image generation, cloud storage, data privacy, Resource management, Web sites, image coding, user-level file system calls]
M-cube: A Duty Cycle Based Multi-channel MAC Protocol with Multiple Channel Reservation for WSNs
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
In this paper, a duty cycle based multi-channel MAC protocol with multiple channel reservation, called M-cube, is proposed to tackle the triple hidden terminal problems. M-cube can make nodes to choose one actually idle channel from all the expected idle channels. Therefore, M-cube can avoid data packet collisions resulted by the triple hidden terminal problems. By minimizing the lower bound of the average number of times of channel switching in M-cube, the optimal duty cycle is obtained through theoretical analysis. To validate the effectiveness of multiple channel reservation and dynamic optimal duty cycling, extensive simulations and real test bed experiments were conducted. Both the simulation and experiment results show that when the number of channels is large or network loads are heavy, M-cube improves energy efficiency and throughput significantly compared with other works in the literature.
[wireless sensor networks, triple hidden terminal problem, multichannel MAC protocol, channel reservation, Receivers, Media, Throughput, access protocols, duty cycle, Synchronization, channel switching, telecommunication switching, Wireless sensor networks, WSN, Media Access Protocol, wireless channels, M-cube, dynamic optimal duty cycling]
Optimistic Localization: Avoiding Location Ambiguity in Wireless Ad-hoc Networks
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Accurate self-localization is a key enabling technique for many pervasive applications. Existing approaches, most of which are multilateration based, often suffer ambiguities, resulting in unbounded positioning errors. To address this problem, previous approaches discard those positioning results with possible flip ambiguities, trading the performance for robustness. However, the high false positive rate of flip prediction incorrectly rejects many reliable location estimates. By exploiting the characteristics of flip ambiguity, which causes either huge or zero error, we propose the concept of optimistic localization and design an algorithm, OFA, that employs a global consistency check and a location correction phase in the localization process. We evaluate this design through extensive simulations. The results show that OFA obtains robustness with extremely low performance cost, so as to reduce the requirement on average degree from 25 to 10 for robustly localizing a network.
[Algorithm design and analysis, flip ambiguity, wireless ad hoc networks, ubiquitous computing, Optimistic design, global consistency check, Wireless networks, optimistic localization, Robustness, Skeleton, wireless channels, multilateration, Localization, Flip ambiguity, flip prediction, Estimation, self localization, pervasive applications, location estimates, unbounded positioning errors, location correction phase, Position measurement, Distance measurement, Error correction, ad hoc networks]
A Self-Stabilizing Locality-Aware Peer-to-Peer Network Combining Random Networks, Search Trees, and DHTs
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
We present 3nuts, a self-stabilizing peer-to-peer (p2p) network supporting range queries and adapting the overlay structure to the underlying physical network. 3nuts combines concepts of structured and unstructured p2p networks to over-come their individual shortcomings while keeping their strengths. This is achieved by combining self maintaining random networks for robustness, a search tree to allow range queries, and DHTs for load balancing. Simple handshake operations with provable guarantees are used for maintenance and self-stabilization. Efficiency of load balancing, fast data access, and robustness are proven by rigorous analysis.
[TV, load balancing, self-stabilization, random graphs, table lookup, DHT, mobile computing, Network topology, resource allocation, Robustness, peer-to-peer computing, Peer to peer computing, self-stabilizing peer-to-peer network, robustness, locality, Routing, Topology, self maintaining random network, tree searching, range query, p2p, search tree, locality-aware peer-to-peer network, handshake operation, distributed hash table, data access, Load management, search trees, unstructured P2P network]
Mixed-Parallel Implementations of Extrapolation Methods with Reduced Synchronization Overhead for Large Shared-Memory Computers
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Extrapolation methods belong to the class of one-step methods for the solution of systems of ordinary differential equations (ODEs). In this paper, we present parallel implementation variants of extrapolation methods for large shared-memory computer systems which exploit pure data parallelism or mixed task and data parallelism and make use of different load balancing strategies and different loop structures. In addition to general implementation variants suitable for ODE systems with arbitrary access structure, we devise specialized implementation variants which exploit the specific access structure of a large class of ODE systems to reduce synchronization costs and to improve the locality of memory references. We analyze and compare the scalability and the locality behavior of the implementation variants on an SGI Altix 4700 using up to 500 threads.
[load balancing, Instruction sets, large shared-memory computers, ODE systems, Registers, POSIX Threads, scalability, differential equations, ODE, resource allocation, shared memory systems, extrapolation method, parallel algorithms, one-step method, shared memory computer systems, locality, Synchronization, parallel extrapolation methods, memory references, Pipeline processing, synchronisation, SGI Altix 4700, task parallelism, Extrapolation, ordinary differential equations, extrapolation, synchronization costs, data parallelism, mixed-parallel implementation, reduced synchronization overhead, loop structures, shared memory, NUMA]
DSpam: Defending Against Spam in Tagging Systems via Users' Reliability
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Resisting spam in tagging system is very challenging. This paper presents DSpam, a novel spam-resistant tagging system which can significantly diminish spam in tag search results with users' reliabilities. DSpam client groups other users into unfamiliar users and interacted users according to the fact whether the client has interacted with such users. For an unfamiliar user, the client computes his reliability by tagging behavior-based mechanism which reflects correlation of annotations between them. For an interacted user, the reliability includes two parts: feedback-based reliability, which indicates direct interactions between that user and the client, and recommendation reliability, which indicates the evaluation about that user from the client's friends. The client ranks search result with the average reliabilities of himself with respect to annotators of each result. Experimental results show DSpam can effectively resist tag spam and work better than existing tag search schemes.
[Computational modeling, Social network services, DSpam, unsolicited e-mail, tagging behavior-based mechanism, Equations, Videos, security of data, Tag spam, Tagging system, Tagging, tagging systems, Reliability, Mathematical model, spam-resistant tagging system, users' reliability]
VirtCFT: A Transparent VM-Level Fault-Tolerant System for Virtual Clusters
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
A virtual cluster consists of a multitude of virtual machines and software components that are doomed to fail eventually. In many environments, such failures can result in unanticipated, potentially devastating failure behavior and in service unavailability. The ability of failover is essential to the virtual cluster's availability, reliability, and manageability. Most of the existing methods have several common disadvantages: requiring modifications to the target processes or their OSes, which is usually error prone and sometimes impractical; only targeting at taking checkpoints of processes, not whole entire OS images, which limits the areas to be applied. In this paper we present VirtCFT, an innovative and practical system of fault tolerance for virtual cluster. VirtCFT is a system-level, coordinated distributed checkpointing fault tolerant system. It coordinates the distributed VMs to periodically reach the globally consistent state and take the checkpoint of the whole virtual cluster including states of CPU, memory, disk of each VM as well as the network communications. When faults occur, VirtCFT will automatically recover the entire virtual cluster to the correct state within a few seconds and keep it running. Superior to all the existing fault tolerance mechanisms, VirtCFT provides a simpler and totally transparent fault tolerant platform that allows existing, unmodified software and operating system (version unawareness) to be protected from the failure of the physical machine on which it runs. We have implemented this system based on the Xen virtualization platform. Our experiments with real-world benchmarks demonstrate the effectiveness and correctness of VirtCFT.
[Checkpointing, checkpointing, workstation clusters, Protocols, real-world benchmarks, memory, physical machine, distributed processing, virtual clusters, CPU, fault tolerance mechanisms, Coordinated Checkpointing, Fault tolerance, Fault tolerant systems, coordinated distributed checkpointing fault tolerant system, globally consistent state, transparent VM-level fault-tolerant system, VirtCFT, software components, Kernel, unmodified software, High Availability, object-oriented programming, distributed VM, practical system, operating system, Xen virtualization platform, checkpoints, Virtual machining, software fault tolerance, OS images, failure behavior, virtual machines, operating systems (computers), innovative system, network communications, service unavailability, Fault Tolerance, Virtual Machine]
Performance Data Extrapolation in Parallel Codes
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Measuring the performance of parallel codes is a compromise between lots of factors. The most important one is which data has to be analyzed. Current supercomputers are able to run applications in large number of processors as well as the analysis data that can be extracted is also large and varied. That implies a hard compromise between the potential problems one want to analyze and the information one is able to capture during the application execution. In this paper we present an extrapolation methodology to maximize the information extracted in a single application execution. It is based on a structural characterization of the applications, performed using clustering techniques, the ability to multiplex the read of performance hardware counters, plus a projection process. As a result, we obtain the approximated values of a large set of metrics for each phase of the application, with minimum error.
[Multiplexing, Measurement, codes, Radiation detectors, clustering technique, projection process, Data mining, Clustering, performance hardware counter, Extrapolation, Program processors, extrapolation, parallel code, Hardware, data extrapolation, Performance Analysis, Parallel Applications]
Approximate Data Collection for Wireless Sensor Networks
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Data collection is a fundamental issue in wireless sensor networks. In many application scenarios for sensor networks, approximate data collection is a wise choice due to the constraints in communication bandwidth and energy budget. In this paper, we focus on efficient approximate data collection with given error bounds in wireless sensor networks. The key idea of our data collection approach ADC (Approximate Data Collection) is to divide a sensor network into clusters, discover local data correlations on each cluster head, and perform a global approximate data collection on the sink according to model parameters uploaded by cluster heads. Specifically, we propose a local estimation model to approximate the readings of several subsets of sensor nodes, and prove rated error-bounds of data collection using this model. In the process of model-based data collection, we formulate the problem of selecting the minimum subset of sensor nodes into a minimum dominating set problem which is known to be NP-hard, and use a greedy heuristic algorithm to find an approximate solution. We also propose a monitoring algorithm to adjust these subsets according to the changes of sensor readings. Our trace-driving simulation results show that our data collection approach ADC can notably reduce the communication cost with given error bounds.
[Correlation, wireless sensor networks, greedy algorithms, Estimation, wireless sensor network, Approximation methods, monitoring algorithm, trace-driving simulation, approximate data collection, Wireless sensor networks, model-based data collection, NP-hard problem, minimum dominating set, Clustering algorithms, greedy heuristic algorithm, Approximation algorithms, Data models, data handling, computational complexity]
Scalable Authentication and Key Management in SCADA
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
In this paper we develop a SCADA key management system to provide better security, performance, and scalability. Conventional symmetric key based approaches have several problems. We adopt public key based approaches due to its flexibility in authentication and access control and efficiency in rekeying. However, existing public key based approaches are not scalable. Simple replication of CAs (certificate authorities) raises security concerns. We consider several novel designs to bridge the gaps in existing approaches. First, a master key based semi-autonomous key refreshing scheme has been developed to shift the rekeying burdens from CAs to individual SCADA node. Then, we design a CA-grid approach, which combines the threshold scheme and replication of CAs to achieve better protection of the master keys, improved availability, and enhanced performance by load sharing. Analyses show that our scheme has many advantages than the existing SCADA key management systems.
[Access control, symmetric key, threshold scheme, Protocols, load sharing, semiautonomous key refreshing scheme, SCADA key management systems, Materials, certificate authorities, key management, scalable authentication, public key cryptography, Public key, Authentication, SCADA systems, authorisation, SCADA system, access control, SCADA node, public key based approach, security concerns, CA grid approach, master keys, authentication]
Packet Content Matching with packetC Searchsets
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Increasing speeds and volumes push network packet applications to use parallel processing to boost performance. Examining the packet payload (message content) is a key aspect of packet processing. Applications search payloads to find strings that match a pattern described by regular expressions (regex). Searching for multiple strings that may start anywhere in the payload is a major obstacle to performance. Commercial systems often employ multiple network processors to provide parallel processing in general and use regex software engines or special regex processors to speed up searching performance via parallelism. Typically, regex rules are prepared separately from the application program and compiled into a binary image to be read by a regex processor or software engine. Our approach integrates specifying search rules with specifying network application code written in packet C, a C dialect that hides host-machine specifics, supports coarse-grain parallelism and supplies high-level data type and operator extensions for packet processing. packetC provides a search set data type, as well as match and find operations, to support payload searching. We show that our search set operator implementation, using associative memory and regex processors, lets users enjoy the performance benefits of parallel regex technology without learning hardware-specifics or using a separate regex toolchain's use.
[Protocols, packet payload, binary image, packet switching, packetC searchsets, searching performance, network processing, C language, regex software engines, programming languages, parallel processing, program compilers, associative memory, packet content matching, Program processors, application program, network application code, deep packet inspection, Parallel processing, regular expressions, Hardware, query formulation, network processors, regex processors, coarse-grain parallelism, Software algorithms, computer networks, network packet applications, packetpayload, commercial systems, Automata, search rules, host-machine specifics, Payloads, packet processing]
A Taxonomy of Autonomic Application Management in Grids
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
In this paper, we propose a taxonomy that characterizes and classifies different components of autonomic application management in Grids. We also survey several representative Grid systems developed by various projects world-wide to demonstrate the comprehensiveness of the taxonomy. The taxonomy not only highlights the similarities and differences of state-of-the-art technologies utilized in autonomic application management from the perspective of Grid computing, but also identifies the areas that require further research initiatives.
[Taxonomy, Decision making, grid computing, Dynamic scheduling, grid system, autonomic application management, Autonomic computing, Grids, Survey, Processor scheduling, fault tolerant computing, Resource management, Monitoring]
ILGT: Group Reputation Aggregation Method for Unstructured Peer-to-Peer Networks
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Reputation aggregation methods in unstructured Peer-to-Peer(P2P) networks are used to evaluate the trustworthiness of participating peers and to combat malicious peer's behaviors. In the reputation aggregation method, each peer calculates global scores by aggregating local scores which are obtained from each peer in P2P networks. In structured P2P networks, each peer exchanges local scores with other peers. Gossip Trust is proposed as a reputation aggregation method for the unstructured P2P networks. However, Gossip Trust has problem that high frequency of communications and long aggregation time increase as the number of peers in the network increase. In this paper, we propose a reputation aggregation method called &#x201D;ID-List based Group Trust(ILGT)&#x201D;. ILGT limits the number of peers that participate in reputation aggregation process and share the global scores by using list of ID. And ILGT improves communication frequency and aggregation time. Through computer simulations, we compare the proposed method ILGT with Gossip Trust.
[trust, peer-to-peer computing, Peer to peer computing, Computer simulation, malicious peer behaviors, ID-List based Group Trust, Receivers, Gossip Trust, Time frequency analysis, computer simulations, History, reputation aggregation, P2P, ILGT, Aggregates, computer crime, group reputation aggregation method, Internet, unstructured peer-to-peer networks, ID-List, group]
TRIP: Temporal Redundancy Integrated Performance Booster for Parity-Based RAID Storage Systems
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Parity redundancy is widely employed in RAID-structured storage systems to protect against disk failures. However, the small-write problem has been a persistent root cause of the performance bottleneck of such parity-based RAID systems, due to the additional parity update overhead upon each write operation. In this paper, we propose a novel RAID architecture, TRIP, based on the conventional parity-based RAID systems. TRIP alleviates the small-write problem by integrating and exploiting the temporal redundancy (i.e., snapshots and logs) that commonly exists in storage systems to protect data from soft errors while boosting write performance. During the write-intensive periods, TRIP can reduce the penalty of each small-write request to as few as one device IO operation, at a minimal cost of maintaining the temporal redundant information. Reliability analysis, in terms of Mean Time to Data Loss (MTTDL), shows that the reliability of TRIP is only marginally affected. On the other hand, our prototype implementation and performance evaluation demonstrate that TRIP significantly outperforms the conventional parity-based RAID systems in data transfer rate and user response time, especially in write-intensive environments.
[temporal redundancy integrated performance booster, TRIP, fault tolerance, parity-based RAID storage system, Redundancy, log, parity redundancy, Switches, small write, RAID, disk failure, performance booster, Delay, Reed-Solomon codes, MTTDL, reliability analysis, temporal redundancy, mean-time-to-data loss, Arrays, small-write problem, snapshot, disc drives]
Semantics-Aware, Timely Prefetching of Linked Data Structure
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Traversal through a Linked Data Structure (LDS) in applications encounters heavy cache misses and severe performance degradation. Due to tight load-load dependences in LDS traversal, the chance of overlapping the cache misses in exploiting the memory-level parallelism is slim. Furthermore, the irregularity of the missing block addresses makes it difficult for accurate data prefetching without recording a huge miss history. In this paper, we present a semantics-aware approach to dynamically identify pointer links in each node for traversal to the next node. Accurate LDS prefetching based on the node semantics can be accomplished with minimum history information. In addition, we evaluate three hardware-based leap prefetching methods to timely fetch the nodes further ahead in the traversal path for overcoming the lateness in LDS prefetching. Performance evaluations based on LDS intensive applications show that with an integrated stream/stride prefetcher, semantics-aware prefetcher improves performance over without prefetching by 45%. In comparison with the stream prefetcher, a dependence-based prefetcher, and a content-directed prefetcher, the average improvement is 20%, 16%, and 23% respectively.
[Out of order, dependence-based prefetcher, leap prefetching, prefetch, data prefetching, timely prefetching, cache storage, pointer chasing, Registers, History, stride prefetcher, stream prefetcher, storage management, program semantics, Semantics, memory-level parallelism, data structures, linked data structure, cache misses, Prefetching, performance degradation, content-directed prefetcher, semantics-aware prefetching, semantics-aware prefetcher, Arrays, hardware-based leap prefetching method]
Simulating Large Scale Parallel Applications Using Statistical Models for Sequential Execution Blocks
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Predicting sequential execution blocks of a large scale parallel application is an essential part of accurate prediction of the overall performance of the application. When simulating a future machine, or a prototype system only available at a small scale, it becomes a significant challenge. Using hardware simulators may not be feasible due to excessively slowed down execution times and insufficient resources. The difficulty of these challenges increases proportionally with the scale of the simulation. In this paper, we propose an approach based on statistical models to accurately predict the performance of the sequential execution blocks that comprise a parallel application. We deployed these techniques in a trace-driven simulation framework to capture both the detailed behavior of the application as well as the overall predicted performance. The technique is validated using both synthetic benchmarks and the NAMD application.
[trace-driven, large scale parallel application, Computational modeling, program diagnostics, Predictive models, statistical model, hardware simulators, machine learning, parallel processing, parallel simulator, performance prediction, insufficient resources, Emulation, Machine learning, statistical models, Benchmark testing, sequential execution blocks, trace-driven simulation framework, Data models, Hardware, statistical analysis]
Accelerating Spatial Data Processing with MapReduce
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Map Reduce is a key-value based programming model and an associated implementation for processing large data sets. It has been adopted in various scenarios and seems promising. However, when spatial computation is expressed straightforward by this key-value based model, difficulties arise due to unfit features and performance degradation. In this paper, we present methods as follows: 1) a splitting method for balancing workload, 2) pending file structure and redundant data partition dealing with relation between spatial objects, 3) a strip-based two-direction plane sweeping algorithm for computation accelerating. Based on these methods, ANN(All nearest neighbors) query and astronomical cross-certification are developed. Performance evaluation shows that the Map Reduce-based spatial applications outperform the traditional one on DBMS.
[key-value based programming model, cross-certification, visual databases, distributed parallel processing, parallel processing, workload balancing, MapReduce, query processing, resource allocation, very large databases, spatial computation, Distributed databases, splitting method, file structure, large data set processing, spatial data processing, data partition, spatial object, strip-based two-direction plane sweeping algorithm, astronomical cross-certification, Artificial neural networks, all nearest neighbor query, Spatial databases, Encoding, Spatial Applications, Nearest neighbor searches, Tiles, All Nearest Neighbor, file organisation, Data models]
Auto-tuning Dense Matrix Multiplication for GPGPU with Cache
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
In this paper we discuss about our experiences in improving the performance of GEMM (both single and double precision) on Fermi architecture using CUDA, and how the new features of Fermi such as cache affect performance. It is found that the addition of cache in GPU on one hand helps the processers take advantage of data locality occurred in runtime but on the other hand renders the dependency of performance on algorithmic parameters less predictable. Auto tuning then becomes a useful technique to address this issue. Our auto-tuned SGEMM and DGEMM reach 563 GFlops and 253 GFlops respectively on Tesla C2050. The design and implementation entirely use CUDA and C and have not benefited from tuning at the level of binary code.
[Instruction sets, auto-tuning dense matrix multiplication, cache affect performance, Fermi, autotuning, cache storage, Fermi architecture, Registers, coprocessors, GPU, Tuning, Optimization, GPGPU, Graphics processing unit, CUDA, matrix multiplication, Tesla C2050, data locality, Computer architecture, auto-tuned SGEMM, Kernel, DGEMM]
A New Structured Peer-to-Peer Architecture Based on Physical Distance
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Recently, structured P2P (Peer-to-Peer) system has become more and more popular. However, P2P systems' large scale and high dynamics have brought a great challenge to data availability and accessing performance. Redundancy techniques are used to solve these problems. However, in structured P2P systems, due to the consistent hash, the overlay network could not match underlying physical network well. The nodes close to each other in the overlay network may have long distances of physical network. In this paper, we put forward a new P2P architecture that constructs node identifiers and places redundant data according to physical location information. It can provide better load balance and access performance. The node's identifier is divided into four parts, representing the node's state, ISP, city and IP respectively, so the nodes having similar identifiers are close to each other in the physical network. Moreover, a query tree is used to help a node routing queries quickly in the physical network. In addition, we maintain an access list for each file. When a node becomes overloaded, replicas are placed on another node selected in the routing path according to the access list, so subsequent access requests could be met in advance.
[peer-to-peer computing, physical distance, load balancing, Peer to peer computing, Redundancy, trees (mathematics), overlay network, Routing, physical network, Delay, P2P, resource allocation, query tree, Computer architecture, Cities and towns, redundancy technique, structured P2P system, redundancy, IP networks, structured peer-to-peer architecture]
Data Vitalization: A New Paradigm for Large-Scale Dataset Analysis
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Nowadays, datasets grow enormously both in size and complexity. One of the key issues confronted by large-scale dataset analysis is how to adapt systems to new, unprecedented query loads. Existing systems nail down the data organization scheme once and for all at the beginning of the system design, thus inevitably will see the performance goes down when user requirements change. In this paper, we propose a new paradigm, Data Vitalization, for large-scale dataset analysis. Our goal is to enable high flexibility such that the system is adaptive to complex analytical applications. Specifically, data are organized into a group of vitalized cells, each of which is a collection of data coupled with computing power. As user requirements change over time, cells evolve spontaneously to meet the potential new query loads. Besides basic functionality of Data Vitalization, we also explore an envisioned architecture of Data Vitalization including possible approaches for query processing, data evolution, as well as its tight-coupled mechanism for data storage and computing.
[query load, Roads, data vitalization, Optimization, query processing, storage management, data organization, data storage, Databases, large-scale dataset analysis, Computer architecture, Hardware, data acquisition, user requirement, vitalized data cell, data analysis, system design, data evolution, high flexibility, data collection, complex analytical application, tight-coupled mechanism, Organizations, large-scale dataset, Data models, data computing]
Employing Multiple CUDA Devices to Accelerate LTL Model Checking
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Recently, the CUDA technology has been used to accelerate many computation demanding tasks. For example, in our previous work we have shown how CUDA technology can be employed to accelerate the process of Linear Temporal Logic (LTL) Model Checking. While the raw computing power of a CUDA enabled device is tremendous, the applicability of the technology is quite often limited to small or middle-sized instances of the problems being solved. This is because the memory that a single device is equipped with, is simply not large enough to cope with large or realistic instances of the problem, which is also the case of our CUDA-aware LTL Model Checking solution. In this paper we suggest how to overcome this limitations by employing multiple (two in our case) CUDA devices for acceleration of our fine-grained communication-intensive parallel algorithm for LTL Model Checking.
[parallel algorithms, linear temporal logic model checking, Computational modeling, employing multiple CUDA devices, Random access memory, temporal logic, Data structures, multiple CUDA device, CUDA technology, CUDA enabled device, Graphics processing unit, fine-grained communication-intensive parallel algorithm, formal verification, parallel LTL model checking, computation demanding task, Automata, CUDA-aware LTL model checking solution, computing power, Acceleration, Kernel]
Consensus Service to Solve Agreement Problems
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
This paper describes an extension of the Consensus Service proposed by Guerraoui and Schiper. The objective is to provide a standard way to implement agreement protocols resilient to Byzantine faults using an intrusion tolerant service built upon virtual machines technology. This is achieved through the implementation of a Generic Consensus Service (GCS). GCS separates specificities of different agreement problems from consensus in a clear way, using client server interaction, allowing total independence between consensus protocols used and problem specific specializations. Besides that, the framework provides a set of properties and guarantees. It will be shown how the GCS works, its general properties and how it may be used to solve some agreement problems, for instance, reliable and atomic broadcast.
[client-server systems, Protocols, fault tolerance, Dependable Systems, agreement protocol, client-server interaction, consensus protocol, Computer crashes, Virtual machining, Servers, Proposals, Synchronization, Consensus, Fault tolerance, virtual machine technology, generic consensus service, Intrusion tolerance, distributed algorithms, virtual machines, Distributed algorithm, intrusion tolerant service, agreement problem, Byzantine fault, problem specific specialization]
Enhanced Privilege Separation for Commodity Software on Virtualized Platform
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Conventional privilege separation can effectively reduce the TCB size by granting privilege to only the privileged compartments. However, since they this approach relies on process isolation to ensure security assurance, malware exploiting against kernel components can easily compromise. Meanwhile, the frequent inter-process communications between separated processes inevitably incur notable overhead. To ameliorate these problems, we propose to perform privilege separation without partitioning application into two processes. Instead, we leverage virtualization to enforce the isolation of sensitive portions from other untrusted code. The virtual machine monitor intercepts all the code context switches transparently without requiring the application to explicitly use IPC as privilege context transition. We have implemented a prototype of our system, named Coir, based on commodity hypervisor Xen. Evaluation of our prototype includes a real-world remote control application, which is partitioned and protected in Coir-enabled hypervisor on unmodified Windows XP. We discuss the isolation strength as well as the performance penalty of our system based on the practical case.
[Context, invasive software, malware, kernel components, Coir-enabled hypervisor, Security, commodity software, untrusted code, inter-process communications, Virtual machine monitors, Runtime, unmodified Windows XP, security, virtual machine, telecontrol, remote control application, Prototypes, virtual machines, privilege separation, Kernel, virtualized platform, Virtualization, Monitoring]
On Sweep Coverage with Minimum Mobile Sensors
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
For some sensor network applications, the problem of sweep coverage, which periodically covers POIs (Points of Interest) to sense events, is of importance. How to schedule minimum number of mobile sensors to achieve the sweep coverage within specified sweep period is a challenging problem, especially when the POIs to be scanned exceeds certain scale and the speed of mobile sensor is limited. Therefore, multiple mobile sensors are required to collaboratively complete the scanning task. When the mobile sensor is restricted to follow the same trajectory in different sweep periods, we design a centralized algorithm, MinExpand, to schedule the scan path. When the scan path of the existing mobile sensors has been exceeds the length constraint, MinExpand gradually deploys more mobile sensors and eventually achieves sweep coverage to all POIs. When the mobile sensors are not restricted to follow the same trajectory in different sweep periods, we design OSweep algorithm, where all the mobile sensors are scheduled to move along a TSP (Traveling Salesman Problem) ring consists of POIs. We conduct comprehensive simulations to study the performance of the proposed algorithms. The simulation results show that MinExpand and OSweep outperform CSWEEP in both effectiveness and efficiency.
[Algorithm design and analysis, MinExpand, mobile sensor, wireless sensor networks, mobile sensors, Heuristic algorithms, scan path scheduling, wireless sensor network, Mobile communication, points of interest, global t-sweep coverage, Classification algorithms, sweep coverage, travelling salesman problems, length constraint, scheduling, OSweep algorithm, Sensors, Monitoring, mobile radio, traveling salesman problem, POI, centralized algorithm, TSP, Approximation algorithms, sensor network applications]
Data-Aware Task Scheduling on Multi-accelerator Based Platforms
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
To fully tap into the potential of heterogeneous machines composed of multicore processors and multiple accelerators, simple offloading approaches in which the main trunk of the application runs on regular cores while only specific parts are offloaded on accelerators are not sufficient. The real challenge is to build systems where the application would permanently spread across the entire machine, that is, where parallel tasks would be dynamically scheduled over the full set of available processing units. To face this challenge, we previously proposed StarPU, a runtime system capable of scheduling tasks over multicore machines equipped with GPU accelerators. StarPU uses a software virtual shared memory (VSM) that provides a highlevel programming interface and automates data transfers between processing units so as to enable a dynamic scheduling of tasks. We now present how we have extended StarPU to minimize the cost of transfers between processing units in order to efficiently cope with multi-GPU hardware configurations. To this end, our runtime system implements data prefetching based on asynchronous data transfers, and uses data transfer cost prediction to influence the decisions taken by the task scheduler. We demonstrate the relevance of our approach by benchmarking two parallel numerical algorithms using our runtime system. We obtain significant speedups and high efficiency over multicore machines equipped with multiple accelerators. We also evaluate the behaviour of these applications over clusters featuring multiple GPUs per node, showing how our runtime system can combine with MPI.
[application program interfaces, highlevel programming interface, StarPU, MPI, Programming, coprocessors, data transfer automation, dynamic task scheduling, processor scheduling, Graphics processing unit, Runtime, Semantics, parallel numerical algorithms, heterogeneous machines, Libraries, parallel tasks, parallel algorithms, multicore machines, Prefetching, multi-accelerator based platforms, GPU accelerators, multiple accelerators, data-aware task scheduling, distributed shared memory systems, software virtual shared memory, multicore processors]
Broadcasting Algorithm Via Shortest Paths
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
In this paper, we present a new heuristic that generates broadcast schemes in arbitrary networks. The heuristic gives optimal broadcast time for ring, tree and grid if the originator is on the corner. Extensive simulations show that our new heuristic outperforms the best known broadcast algorithms for two different network models representing Internet and ATM networks. It also allows to generate broadcast time of networks of bigger size because its time complexity, O(|E|), is lower compared to the complexities of the other algorithms. The last advantage of the heuristic is that every node is informed via a shortest path from the originator.
[Heuristic algorithms, Computational modeling, graph theory, optimal broadcast time, time complexity, asynchronous transfer mode, heuristic, Complexity theory, Topology, broadcasting, network, shortest path, broadcast scheme, broadcasting algorithm, Network topology, shortest paths, Broadcasting, Hypercubes, broadcast algorithm, Internet, arbitrary networks, computational complexity, ATM networks, algorithm]
4Sensing -- Decentralized Processing for Participatory Sensing Data
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Participatory Sensing is an emerging application paradigm that leverages the growing ubiquity of sensor-capable smart phones to allow communities carry out wide-area sensing tasks, as a side-effect of people's everyday lives and movements. This paper proposes a decentralized infrastructure for supporting Participatory Sensing applications. It describes an architecture and a domain specific programming language for modeling, prototyping and developing the distributed processing of participatory sensing data with the goal of allowing faster and easier development of these applications. Moreover, a case-study application is also presented as the basis for an experimental evaluation.
[sensor-capable smart phones, data streaming, Roads, Peer to peer computing, Pipelines, domain specific programming language, participatory sensing, 4Sensing, Mobile communication, distributed processing development, distributed processing prototyping, Participatory Sensing, decentralized processing, distributed processing modeling, mobile computing, Aggregates, Computer architecture, Sensors, wide-area sensing tasks]
A Parallel XPath Engine Based on Concurrent NFA Execution
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
The importance of XPath in XML filtering systems has led to a significant body of research on improving the processing performance of XPath queries. Most of the work, however, has been in the context of a single processing core. Given the prevalence of multicore processors, we believe that a parallel approach can provide significant benefits for a number of application scenarios. In this paper we thus investigate the use of multiple threads to concurrently process XPath queries on a shared incoming XML document. Using an approach that builds on YFilter, we divide the NFA into several smaller ones for concurrent processing. We implement and test two strategies for load balancing: a static approach and a dynamic approach. We test our approach on an eight-core machine, and show that it provides reasonable speedup up to eight cores.
[Algorithm design and analysis, finite automata, load balancing, Instruction sets, Heuristic algorithms, multiple threads, YFilter, information filtering, parallel XPath engine, concurrent NFA execution, multicore, concurrent processing, xpath, resource allocation, parallel, xml, XPath query, single processing core, algorithm, multiprocessing systems, Multicore processing, multi-threading, data processing, Generators, XML filtering systems, processing performance, XML, concurrency control, XML document, eight-core machine, multicore processors, load balance]
Kumoi: A High-Level Scripting Environment for Collective Virtual Machines
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
We have designed and implemented a scripting environment called "Kumoi" for managing collective VMs in a large-scale data center. Kumoi is unlike other scripting environments because it exploits strong typing with type inference and high-level description. Kumoi introduces several advancements, including treating virtual machines as first-class objects and decoupling the scripting model and its execution for hiding as many details as possible. We implemented Kumoi as an embedded domain-specific language based on Scala along with distributed agents running on each physical machine. Evaluation using example scripts showed that an administrator can more concisely write the instructions for performing complex VM lifecycle management tasks. Use of this environment should improve management efficiency and agility.
[distributed processing, first-class objects, type theory, virtual machine lifecycle management task, Data Center, Cloud Computing, Virtual Machines, Semantics, embedded systems, Management Shells, Clusters, large-scale data center, high-level scripting environment, DSL, cloud computing, type inference, Java, high-level description, Object oriented modeling, Scala, Virtual machining, Kumoi, embedded domain-specific language, collective virtual machines, computer centres, software agents, distributed agent, XML, virtual machines, Software, scripting model decoupling, reasoning about programs]
Unifying Buffer Replacement and Prefetching with Data Migration for Heterogeneous Storage Devices
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
With the good properties of NAND flash memory such as small size, shock resistance, and low-power consumption, large capacity SSD (Solid State Disk) is anticipated to replace hard disk in high-end systems. However, the cost of NAND flash memory is still high to substitute for hard disk entirely. Using hard disk and NAND flash memory together as secondary storage is an alternative solution to provide relatively low response time, large capacity, and reasonable cost. In this paper, we present a new buffer cache management scheme with data migration that is optimized to use both NAND flash memory and hard disk together as secondary storage. The proposed scheme has three salient features. First, it detects I/O access patterns from each storage, and allocates the buffer cache space for each pattern by computing marginal gain adaptively considering the I/O cost of storage. Second, it prefetches data selectively according to their access pattern and storage devices. Third, it moves the evicted data from the buffer cache to hard disk or NAND flash memory considering the access patterns of block references on the reclamation. Trace-driven simulations show that the proposed scheme improves the I/O performance significantly. It enhances the buffer cache hit ratio by up to 29.9% and reduces the total I/O elapsed time by up to 49.5% compared to the well-acknowledged UBM scheme.
[Performance evaluation, heterogeneous storage devices, NAND flash memory, solid state disk, Prefetching, Buffer storage, buffer cache management scheme, Media, flash memory, cache storage, data migration, hard disk, buffer cache replacement algorithm, buffer replacement, mobile computer, buffer prefetching, flash memories, Flash memory, Memory management, Hard disks]
Detection of a Weak Conjunction of Unstable Predicates in Dynamic Systems
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
A weak conjunctive predicate is a conjunction of local predicates that is evaluated under the Possibly modality. In this study, we have proposed a distributed on-line algorithm for detecting weak conjunctions of unstable predicates in dynamic systems. In the algorithm, a virtual network of a logical ring combined with computation trees is dynamically maintained using local variables to keep track of causality relationships between distributed events. The differential technique is exploited to minimize the size of detection related information. During the execution of the distributed computation, each process maintains a vector containing potential solutions. Detection will be announced when the global predicate is verified. This algorithm does not require extra messages except those for process termination. The detection of a solution may not be announced until the termination of the underlying distributed computation in the case where a process never communicates with others. At each process, storage need is proportional to the number of processes in the system. This demand will increase in some extreme cases, although they are very rare in practice.
[Algorithm design and analysis, TV, Heuristic algorithms, computational linguistics, weak conjunction detection, differential technique, unstable predicates, distributed events, Possibly modality, Copper, Testing, trees (mathematics), causality relationship, conjunctive unstable predicate detection, Data structures, distributed online algorithm, distributed computation, logical ring, process termination, dynamic system, causality, weak conjunctive predicate, computation trees, distributed algorithms, local variables, Clocks]
Utility Accrual Object Distribution in Real-Time Systems
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
This paper considers object-based distributed real-time systems within which objects provide system services to the real-time tasks. Each task is subject to a time/utility function (TUF) which determines the accrued utility of the task according to its completion time. One major problem in such systems is to place the objects onto the computing nodes so as to maximize the total accrued utility. Thus, we propose a utility accrual object distribution (UAOD) algorithm which consists of two phases. In the first phase, through object placement and replication beside some types of deadline decomposition and adaptation, the computing nodes are reserved for the most beneficial tasks. As the second phase, UAOD follows a load-balancing algorithm for the placement of the remaining objects on the nodes to service the less beneficial tasks. Simulation results reveal that the total accrued utility is improved with the UAOD algorithm comparing to the traditional object placement methods.
[Real time systems, utility accrual object distribution, time-utility function, object placement, object-based distributed real-time systems, object replication, Educational institutions, deadline decomposition, Program processors, Processor scheduling, resource allocation, real-time systems, Load management, load-balancing algorithm, distributed systems, Time factors, Resource management, distributed object management, utility accrual scheduling]
Cost-Effective Congestion Management for Interconnection Networks Using Distributed Deterministic Routing
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
The Interconnection networks are essential elements in current computing systems. For this reason, achieving the best network performance, even in congestion situations, has been a primary goal in recent years. In that sense, there exist several techniques focused on eliminating the main negative effect of congestion: the Head of Line (HOL) blocking. One of the most successful HOL blocking elimination techniques is RECN, which can be applied in source routing networks. FBICM follows the same approach as RECN, but it has been developed for distributed deterministic routing networks. Although FBICM effectively eliminates HOL blocking, it requires too much resources to be implemented. In this paper we present a new FBICM version, based on a new organization of switch memory resources, that significantly reduces the required silicon area, complexity and cost. Moreover, we present new results about FBICM, in network topologies not yet analyzed. From the experiment results we can conclude that a far less complex and feasible FBICM implementation can be achieved by using the proposed improvements, while not losing efficiency.
[Head-of-Line Blocking, telecommunication congestion control, Computer aided manufacturing, HOL blocking elimination technique, Random access memory, multiprocessor interconnection networks, Switches, head-of-line blocking, telecommunication network topology, Routing, FBICM, switch memory resources, Congestion Management, Multiprocessor interconnection, distributed deterministic routing, interconnection network, Memory management, telecommunication network routing, Organizations, RECN, Deterministic Routing, Interconnection Networks, cost-effective congestion management, source routing network]
Smart: Service Model for Integrating Wireless Sensor Networks and the Internet
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Wireless sensor networks (WSNs) have received considerable attention in recent years as they have great potential for many distributed applications in different scenarios. Whatever the scenario, WSNs are actually connected to an external network, through which sensed information are passed to the Internet and control messages can reach the WSN. This paper presents Smart, a service model for integrating WSNs and the Internet at service level. Instead of integrating protocol stacks and/or mapping logical addresses, Smart allows the integration of Internet's and WSN's services by providing service interoperability. A communication infrastructure that implements the main components of Smart, along with a power consumption evaluation, is presented to validate the model.
[Protocols, open systems, wireless sensor networks, Unified modeling language, Quality of service, service interoperability, wireless sensor network, interoperability, distributed application, Middleware, service model, Wireless sensor networks, mapping logical address, integrating protocol stacks, power consumption evaluation, integration, Internet, Monitoring]
Effective Performance Measurement at Petascale Using IPM
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
As supercomputers are being built from an ever increasing number of processing elements, the effort required to achieve a substantial fraction of the system peak performance is continuously growing. Tools are needed that give developers and computing center staff holistic indicators about the resource consumption of applications and potential performance pitfalls at scale. To use the full potential of a supercomputer today, applications must incorporate multilevel parallelism (threading and message passing) and carefully orchestrate file I/O. As a consequence, performance tools must also be able to monitor these system components in an integrated way and at the full machine scales. We present IPM, a modularized monitoring approach for MPI, Open MP, file I/O, and other event sources. We describe its implementation design principles, which are targeted for efficiency and minimal application perturbation, and present an application study of using IPM at scale.
[performance tool, supercomputer, system component monitoring, message passing, multi-threading, Radiation detectors, Instruction sets, Instruments, MPI, Supercomputers, petascale, parallel machines, Runtime, file I/O, Open MP, modularized monitoring, IPM performance, threading, Timing, multilevel parallelism, Monitoring, resource consumption]
Achieving Predictable High Performance in Imbalanced Fat Trees
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
The fat-tree topology has become a popular choice for InfiniBand fabrics due to its inherent deadlock freedom, fault-tolerance and full bisection bandwidth. InfiniBand is used by more than 40% of the systems on the latest Top 500 list, and many of these systems are based on a fat-tree topology. However, the current InfiniBand fat-tree routing algorithm suffers from flaws that reduce its scalability and flexibility. Counter-intuitively, the achievable throughput per node deteriorates both when the number of nodes in a tree decreases or when the node distribution among leaves is nonuniform. In this paper, we identify the weaknesses of the current enhanced fat-tree routing algorithm in Open Fabrics Enterprise Distribution and we propose extensions to it that alleviate all performance problems related to node distribution. The new algorithm is implemented in OpenSM for real world evaluation and for future contribution to the Open Fabrics community. We demonstrate that our solution allows to achieve a predictable high throughput regardless of the number of nodes and their distribution. Furthermore, the simulations show that our extensions improve throughput up to 30% depending on topology size and node distribution.
[workstation clusters, InfiniBand, fault-tolerance, inherent deadlock freedom, open fabrics enterprise distribution, imbalanced fat trees, telecommunication network topology, Routing, Throughput, predictable high performance, fat-trees, Topology, OpenSM, interconnection networks, routing, fat-tree routing algorithm, Network topology, telecommunication network routing, Bandwidth, System recovery, InfiniBand fabrics, Approximation algorithms, full bisection bandwidth, fat-tree topology]
Coordinated Selective Rejuvenation for Distributed Services
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Service availability and QoS, in terms of customer affecting performance metrics, is crucial for service systems. However, the increasing complexity in distributed service systems introduce hidden space for software faults, which undermine system availability, leading to fault or even down time. In this paper, we introduce a composition technique, Coordinated Selective Rejuvenation, to automate the whole procession of fault component identification and rejuvenation arbitration, in order to guarantee distributed service system's customer-affecting metrics. We take evaluation with fault injection experiment on RUBiS, which simulates distributed eCommerce of eBay.com. The results indicate that our request path analysis approach and system model technique are effective for fault component's location, Bayesian network technique is feasible for fault pinpointing, in terms of request tracing context. Meanwhile, the arbitration scheme, can effectively guarantee system QoS, by identifying and rejuvenating most likely performance fault tier, before the degradation of customer affecting performance metric become severe.
[Measurement, Correlation, Heuristic algorithms, software fault, customer relationship management, Fault diagnosis, Analytical models, request tracing context, QoS, fault pinpointing, fault component identification, fault injection experiment, Bayesian network, software performance evaluation, electronic commerce, path analysis, coordinated selective rejuvenation, Biological system modeling, fault component location, distributed eCommerce, service availability, quality of service, rejuvenation arbitration, Web services, Bayesian methods, performance metrics, distributed service system, system availability, fault tolerant computing, customer-affecting metrics, eBay.com]
Enhancing MapReduce via Asynchronous Data Processing
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
The Map Reduce programming model simplifies large-scale data processing on commodity clusters by having users specify a map function that processes input key/value pairs to generate intermediate key/value pairs, and a reduce function that merges and converts intermediate key/value pairs into final results. Typical Map Reduce implementations such as Hadoop enforce barrier synchronization between the map and reduce phases, i.e., the reduce phase does not start until all map tasks are finished. In turn, this synchronization requirement can cause inefficient utilization of computing resources and can adversely impact performance. Thus, we present and evaluate two different approaches to cope with the synchronization drawback of existing Map Reduce implementations. The first approach, hierarchical reduction, starts a reduce task as soon as a predefined number of map tasks completes, it then aggregates the results of different reduce tasks following a tree structure. The second approach, incremental reduction, starts a predefined number of reduce tasks from the beginning and has each reduce task incrementally reduce records collected from map tasks. Together with our performance modeling, we evaluate different reducing approaches with two real applications on a 32-node cluster. The experimental results have shown that incremental reduction outperforms hierarchical reduction in general. Also, incremental reduction can speed-up the original Hadoop implementation by up to 35.33% for the word count application and 57.98% for the grep application. In addition, incremental reduction outperforms the original Hadoop in an emulated cloud environment with heterogeneous compute nodes.
[workstation clusters, Merging, tree structure, Hadoop enforce barrier synchronization, commodity cluster, distributed computing, MapReduce, Analytical models, resource allocation, Cloud Computing, performance modeling, emulated cloud environment, MapReduce programming model, grep application, Mathematical model, cloud computing, software performance evaluation, Asynchronous processing, Hadoop, Data processing, incremental reduction, Synchronization, Equations, Sorting, hierarchical reduction, asynchronous data processing, large-scale data processing, computing resource utilization, Distributed Computing]
Dynamic Graph Traversals for Concurrent Rewriting Using Work-Stealing Frameworks for Multicore Platforms
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Parallel programming/execution frameworks for multicore platforms should support as many applications as possible. In general, work-stealing frameworks provide efficient load balancing even for irregular parallel applications. Unfortunately, naive parallel programs for ``general computations'' governed by rewriting rules over configurations (system states) cause stack overflow or unacceptable load imbalance. In this study, we develop parallel programs to perform probabilistically balanced divide-and-conquer graph traversals. We propose a programming technique for accumulating overflowed calls for the next iteration of repeated parallel stages. In an emerging backtracking-based work-stealing framework called "Tascell," which avoids excessive concurrency, we propose a programming technique for long-term use of workspaces, leading a similar technique in the Cilk framework.
[work stealing, Protocols, divide and conquer methods, load balancing, unacceptable load imbalance, Instruction sets, graph theory, stack overflow, divide-and-conquer graph traversals, concurrent rewriting, Proposals, graph traversals, execution frameworks, parallel programming, parallel programs, naive parallel program, resource allocation, work-stealing framework, multicore platforms, dynamic graph traversal, irregular parallel application, rewriting systems, Multicore processing, multicores, Synchronization, Cilk framework, Parallel programming, spanning tree, general computations, Load management, rewriting rules, programming technique, Tascell]
Cloud Storage Design Based on Hybrid of Replication and Data Partitioning
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Most existing works on data partitioning techniques simply consider to partition data objects and distribute the shares to servers. Such pure data partitioning approaches may bring potential performance and scalability problems when used in widely distributed systems. First, it is difficult to apply lazy update. Second, the share consistency verification may incur costly communications among widely distributed servers. In this paper, we propose a two-level DHT (TDHT) approach for widely distributed cloud storage to address these problems. First, we analyze the tradeoffs on security and availability between TDHT and the conventional pure data partitioning approach (integrated with DHT and called GDHT (global DHT)). The results show that TDHT can provide better security than GDHT and almost the same level of availability as GDHT. To compare their performance, we design a two-level access (TLA) protocol for the TDHT approach and compare it with the distributed version server (DVS) protocol proposed in [Ye10] for the GDHT approach. The experimental results show that TLA can provide much better user perceived update response latency and the same level or even better read access latency compared to DVS.
[Protocols, distributed servers, two-level DHT, access protocols, cloud storage design, Servers, Security, History, Voltage control, two-level access protocol, storage management, security, short secret sharing, lazy update, user perceived update response latency, potential performance, distributed systems, Silicon, cloud computing, Availability, replication, pure data partitioning, global DHT, distributed version server protocol, distributed cloud storage, security of data, consistency verification, cloud storage, scalability problem, partition data objects, access latency]
LADPM: Latency-Aware Dual-Partition Multicast Routing for Mesh-Based Network-on-Chips
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Networks-on-Chips (NoCs) provides an efficient architectural paradigm as interconnect for state-of-the-art Chip Multi-processors (CMPs). With the increasing development of novel applications in NoCs, one-to-many (multicast) or one-to-all (broadcast) communications are becoming universal and indispensable. The performance constraint metrics, such as power consumption and network latency, are often stringent on NoC systems. Without multicast support, the performance of traditional NoCs will be significantly degraded by such communications. In this paper, we propose Latency-Aware Dual-Partition Multicast (LADPM) routing for mesh-based on-chip networks to reduce packet latency and balance network load. A detailed wormhole router design is also presented for the proposed LADPM scheme. LADPM scheme can adaptively make routing decision based on the distribution of the destination nodes of the multicast traffic. Experimental results, implemented under a cycle-accurate simulator, show that compared with the best known multicast scheme RPM, LADPM reduces Energy-Delay Product by 25.4% on average. More importantly, in heavy traffic load networks, LADPM is a scalable solution.
[NoC systems, multiprocessor interconnection networks, architectural paradigm, Load-balance, energy-delay product, Registers, power consumption, RPM, Multicast, Unicast, state-of-the-art chip multiprocessors, wormhole router design, multicast communication, multicast support, Network-on-Chips (NoCs), IP networks, System-on-a-chip, network latency, LADPM, mesh-based on-chip networks, multiprocessing systems, network-on-chip, Rectilinear Steiner Arborescence, network routing, CMP, destination nodes, cycle-accurate simulator, Routing, performance constraint metrics, latency-aware dual-partition multicast routing, interconnect, mesh-based network-on-chips, balance network load, Tiles, one-to-all communication, packet latency, System recovery, broadcast communications, multicast traffic, Latency-Aware, routing decision, heavy traffic load networks, one-to-many communication]
BLAST: Applying Streaming Ciphers Into Outsourced Cloud Storage
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Providing secure and efficient outsourced storage is a precursor to widespread cloud deployment and availability. For this purpose, existing designs mainly rely on block ciphers, although stream ciphers are more computationally-efficient than block ciphers. This paper presents a construction of secure storage, BLAST, enhanced with a stream cipher rather than a block cipher with a novel block accessible encryption mechanism based on streaming ciphers. In BLAST, a hierarchical tree generated in the form of an n-level quad tree is created for each user during the registration phase of the storage system. When a user wants to access the outsourced storage, the user can access their data after encrypting or decrypting it using a sequence of key stream frames derived from the hierarchical tree. The experimental results show that the proposed system achieves significant improved performance than normal streaming/block cipher-based secure storage system in terms of throughput and access latency.
[Cloud computing, stream cipher, Secure Storage, Throughput, cryptography, key stream frame, Encryption, cloud deployment, block accessible encryption, Secure storage, hierarchical tree, quad tree, storage management, BLAST secure storage, Block Cipher, Seals, outsourced cloud storage, Storage Outsourcing, quadtrees, IP networks, cloud computing, Streaming Cipher, decryption]
Multi-mapping Meshes: A New Communicating Fabric for Networks-on-Chip
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Traditional mesh is a popular interconnection architecture for Networks-on-Chip (NoCs). However, with the trend towards larger number of cores in chip multiprocessors, it is unable to stand the fast-growing diameter and average distance in meshes. Recently, concentration and express channels are two countermeasures for that. In this paper, a new scheme called multi-mapping is proposed, which allows one processing element (PE) to be connected to multiple routers, and vice versa. By properly establishing the mapping relationships between PE and router, both diameter and average distance of the network are lowered while the interconnections between routers are not altered. To provide efficient and in-order communication in NoCs, we develop the X-Y routing scheme with wormhole-switching technique used in multi-mapping meshes. The simulation results are presented to show the effectiveness of our method by comparing with traditional meshes and mesh-based express cubes under different traffic patterns.
[multiprocessor interconnection networks, Switches, processing element, networks-on-chip, Networks-on-Chip (NoCs), Complexity theory, Program processors, wormhole-switching technique, meshes, in-order communication, mesh-based express cubes, network-on-chip, X-Y routing scheme, Routing, Equations, Sorting, telecommunication switching, chip multiprocessors, wormhole-switching, Tiles, multi-mapping, telecommunication network routing, multi-mapping meshes, interconnection architecture, communicating fabric, telecommunication traffic]
Inter-Coding: An Interleaving and Erasure Coding Based Stable Routing Scheme in Multi-path DTN
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
The main challenge in DTNs is how to deal with path uncertainty in achieving a reliable routing scheme. All Erasure coding based routing algorithms make the assumption that the underlying path probabilities are known previously and remain constant, which is unpractical. On the other hand, the overall behavior of path probability tends to be stable with the increasing number of paths, which can be used to increase the stability of Erasure coding based schemes. Bearing this in mind, we present Inter-Coding: Inter-Coding is designed to fully combine the reliability of erasure coding, and the stability of interleaving to cope with uncertainties. We evaluate our approach in terms of delivery ratio under different level of uncertainty as well as different interleaving policy, and validate that Inter-Coding offers reliable and stable performance even the path uncertainty and dynamic is high.
[stable routing scheme, delay tolerant network, network coding, Uncertainty, probability, Interference, Routing, erasure coding, Inter-Coding, Encoding, Decoding, path probability, Delay, interleaving, intercoding, multipath DTN, Erasure Coding, telecommunication network routing, path uncertainty, interleaved codes, DTN]
Automatically Tuned Dynamic Programming with an Algorithm-by-Blocks
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
As the complexity of current computer architecture increases, domain-specific program generators are extensively used to implement performance portable libraries. Dynamic programming is a performance-critical kernel in many applications including engineering operations and bioinformatics. In this paper, we propose an Automatically Tuned Dynamic Programming (ATDP) to optimize performance of dynamic programming algorithm across various architectures. First, an algorithm-by-blocks for dynamic programming is designed to facilitate optimizing with well-known techniques including cache and register tiling. Further, the parameterized algorithm-by-blocks is cooperative with an auto-tuning framework and leverages a hill climbing algorithm to search the possible best program on a given platform. The experiments on two &#x00D7;86 processors demonstrate that (i) the generated scalar programs improve performance by over 10 times, (ii) the vector programs further speedup the scalar ones by a factor of 4 and 2 for single-precision and double-precision, respectively.
[ATDP, High Performance Computing, Heuristic algorithms, RNA, Auto-tuning, dynamic programming, Partitioning algorithms, SIMD, autotuning framework, hill climbing algorithm, Program processors, algorithm-by-blocks, domain-specific program generator, Tiles, Dynamic Programming, Computer architecture, Algorithm-by-Blocks, Dynamic programming, automatically tuned dynamic programming, performance-critical kernel]
Acceleration of a High Order Accurate Method for Compressible Flows on SDSM Based GPU Clusters
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
The recent advent of multicore processors, and especially the introduction of many-core GPUs, opens new horizons to large-scale, high-resolution, simulations for a broad range of scientific fields. Among them, the scientific area of CFD appears to be one of the candidates that could significantly benefit from the utilization of many-core GPUs. In o rder to investigate such a potential, we evaluate the performance of a high-order accurate method for the simulation of compressible flows. Current implementation is taking place on a GPU cluster. Nevertheless, a novel approach is followed concerning the utilization of GPU clusters that does not involve explicit message passing. Instead, the presented implementation resides on Software Distributed Shared Memory (SDSM) to propagate changes across the simulation phases. The first results prove to be emboldening and lay grounds for further research along the use of shared memory abstraction in order to utilize future GPU clusters.
[CFD, Solid modeling, flow simulation, computer graphic equipment, software distributed shared memory, GPU Clusters, Instruction sets, high-order accurate method, SDSM, Graphics processing unit, many-core GPU, Three dimensional displays, Numerical models, multicore processor, compressible flow simulation, message passing, Multicore processing, Computational modeling, compressible flow, computational fluid dynamics, OpenMP, WENO, CUDA, shared memory abstraction, distributed shared memory systems, SDSM based GPU clusters, large-scale high-resolution simulation]
Gossiping Differential Evolution: A Decentralized Heuristic for Function Optimization in P2P Networks
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
P2P-based optimization has recently gained interest among distributed function optimization scientists. Several well-known optimization heuristics have been recently re-designed to exploit the peculiarity of such a distributed environment. The final goal is to perform high quality function optimization by means of inexpensive, fully decentralized machines, which may either be purposely organized in a P2P network, or voluntarily join a running P2P optimization task. In this paper we present the GoDE algorithm (Gossip-based Differential Evolution), which obtains remarkable results on several test functions. We describe in detail the algorithm design and the epidemic mechanism that greatly improves the performance. Experimental results in a simulated environment show how GoDE adapts to network scale and how the epidemic communication protocol can make the algorithm achieve good results even in presence of a high churn rate.
[Algorithm design and analysis, gossiping, peer-to-peer computing, decentralized machines, epidemic communication protocol, Peer to peer computing, churn rate, churn, heuristic, Vectors, Topology, genetic algorithms, function optimization, Synchronization, parallel machines, Optimization, P2P networks, peer-to-peer, differential evolution, distributed function optimization, decentralized heuristic, protocols, Distributed algorithms, GoDE algorithm]
General vs. Interval Mappings for Streaming Applications
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
This paper deals with the problem of mapping pipelined applications on heterogeneous platforms whose processors are subject to failures. We address a difficult bi-criteria problem, namely deciding which stages to replicate, and on which resources, in order to optimize the reliability of the schedule, while guaranteeing a minimal throughput. Previous work had addressed the complexity of interval mappings, where the application is partitioned into intervals of consecutive stages (which are then replicated and assigned to processors). In this paper we investigate general mappings, where stages may be partitioned without any constraint, thereby allowing a better usage of processors and communication network capabilities. The price to pay for general mappings is a dramatic increase in the problem complexity. We show that computing the period of a given general mapping is an NP-complete problem, and we provide polynomial bounds to determine a (conservative) approximated value. The bi-criteria mapping problem itself becomes NP-complete on homogeneous platforms, while it is polynomial with interval mappings. We design a set of efficient heuristics, which we compare with interval mapping strategies through extensive simulations.
[complexity, Protocols, communication network capabilities, Pipelines, reliability, bi-criteria optimization, Complexity theory, polynomial bounds, general mapping, Program processors, bi-criteria mapping problem, heuristics, Bandwidth, pipelined application mapping, Silicon, pipelined applications, throughput, interval mapping, interval mappings, bi-criteria problem, NP-complete problem, heterogeneous platforms, streaming applications, pipeline processing, Reliability, computational complexity]
Multi-task Downloading for P2P-VoD: An Empirical Perspective
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
For current P2P-VoD systems, three fundamental problems exit in user experience: exceedingly large startup delay, long jump latency, and poor playback continuity. These problems primarily stem from lack of media data. In this paper, we propose Multi-Task Downloading with Bandwidth Control (MTD(BC)), an efficient and practical mechanism to prefetch media data. In MTD, a user can download multiple videos in parallel with its current viewing, which significantly decreases video switching delays. However, MTD brings a serious problem: downloading "other" tasks could impede the playback performance of the current viewing, especially in low-bandwidth network. This problem is solved through our design of bandwidth control. To our knowledge, we are the first to propose MTD with bandwidth control for P2P-VoD and conduct empirical evaluations in the real-world system. The running results show that MTD(BC) achieves better streaming quality than the traditional method. In particular, our mechanism reduces 75% of startup delay and 36% of jump latency in low-bandwidth network with high system scalability.
[streaming quality, startup delay, User Experience, P2P-VoD system, Servers, Delay, Videos, storage management, Video-on-Demand, playback continuity, media data prefetching, Bit rate, video on demand, Bandwidth, system scalability, video streaming, multitask downloading, video signal processing, video downloading, peer-to-peer computing, video switching delay, jump latency, Media, Peer-to-Peer, bandwidth control, Data Schedule, System Scalability, bandwidth allocation, Upper bound, low-bandwidth network]
SCAS: Sensing Channel ASsignment for Spectrum Sensing Using Dedicated Wireless Sensor Networks
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Spectrum sensing is essential for the success of the cognitive radio networks. In traditional spectrum sensing schemes, Secondary Users (SUs) are responsible for the spectrum sensing which could be very time and resource consuming. It leads to a great deal of inefficiency in spectrum usage and introduces many practical challenges. To tackle these challenges and leverage the spectrum opportunity more efficiently, we propose a new system that provides a spectrum sensing service for SUs using dedicated wireless spectrum sensor networks (WSSNs). In this paper we focus on the sensing channel assignment problem in WSSNs and formulate the problem as a Sensing Effectiveness Maximization Problem (SEMP). We prove that SEMP is NP-complete under the ideal case, and show that the more challenges arises in real environments. To address the issues, we systematically study the design tradeoff and critical factors when maximizing the sensing effectiveness. Based on these study results we propose a Sensing Channel Assignment algorithm (SCAS). We conduct test-bed empirical investigations as well as comprehensive simulations. Performance evaluation results show that for both the scenarios of given deployments and manual deployments, SCAS is able to sense more channels to improve the sensing effectiveness. The improvement is up to 300% and the average improvement is 150% compared with other simple alternatives.
[Algorithm design and analysis, radio spectrum management, wireless sensor networks, SCAS, cogntive radio networks, Scheduling, dedicated wireless sensor networks, NP-complete, cognitive radio networks, Wireless communication, Wireless sensor networks, Accuracy, optimisation, cognitive radio, sensing effectiveness maximization problem, sensing channel assignment, spectrum sensing, time consuming, Feature extraction, resource consuming, Sensors, secondary users, computational complexity]
PinComm: Characterizing Intra-application Communication for the Many-Core Era
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
As the number of cores in both embedded Multi-Processor Systems-on-Chip and general purpose processors keeps rising, on-chip communication becomes more and more important. In order to write efficient programs for these architectures it is therefore necessary to have a good idea of the communication behavior of an application. We present a communication profiler that extracts this behavior from compiled, sequential or parallel C/C++ programs, and constructs a dynamic data-flow graph at the level of major functional blocks. In contrast to existing methods of measuring inter-program communication, our tool automatically generates the program's data-flow graph and is less demanding for the developer. It can also be used to view differences between program phases (such as different video frames), which allows both input- and phase-specific optimizations to be made. We will also describe briefly how this information can subsequently be used to guide the effort of parallelizing the application, to co-design the software, memory hierarchy and communication hardware, and to provide new sources of communication-related runtime optimizations.
[multi-processor systems-on-chip, Profiling, Instruction sets, data flow graphs, general purpose processors, Optimization, Runtime, sequential C/C++ programs, System-on-a-chip, communication, communication-related runtime optimization, parallel C/C++ programs, on-chip communication, network-on-chip, multiprocessing programs, Data structures, inter-program communication, Decoding, C++ language, PinComm, many-core era, dynamic dataflow graph, Cameras, data-flow graph, intra-application communication, system-on-chip]
A Distributed Workflow Mapping Algorithm for Minimum End-to-End Delay under Fault-Tolerance Constraint
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Many large-scale scientific applications feature distributed computing workflows of complex structures that must be executed and transferred in shared wide-area networks consisting of unreliable nodes and links. Mapping these computing workflows in such faulty network environments for optimal latency while ensuring certain fault tolerance is crucial to the success of eScience that requires both performance and reliability. We construct analytical cost models and formulate workflow mapping as an optimization problem under failure rate constraint. We propose a distributed heuristic mapping solution based on recursive critical path to achieve minimum end-to-end delay and satisfy a pre-specified overall failure rate for a guaranteed level of fault tolerance. The performance superiority of the proposed mapping solution is illustrated by extensive simulation-based comparisons with existing mapping algorithms.
[Computers, optimization problem, scientific workflow, complex structures, software reliability, distributed processing, Complexity theory, Delay, minimum end-to-end delay, Fault tolerance, optimisation, end-to-end delay, Fault tolerant systems, faulty network environments, Computer networks, distributed computing workflows, eScience, fault tolerance, Computational modeling, distributed heuristic mapping solution, large-scale scientific applications, shared wide-area networks, distributed algorithm, fault-tolerance constraint, fault tolerant computing, distributed workflow mapping algorithm]
Bandwidth Constrained Tree Construction for Live Streaming Systems in P2P Networks
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
The traditional client-server architecture widely adopted on the Internet is not adequate to meet the increasing user loads and bandwidth demands in live streaming systems especially for multimedia content delivery. Peer-to-peer P2P) overlay networks provide excellent system scalability and high resource utilization, which make it an attractive solution to this problem. This paper considers a hybrid hierarchical P2P overlay network structure that consists of both super and normal peers. The media streaming architecture is built upon a tree structured network of super peers and the tree construction process has a significant impact on the overall system performance. We construct network cost models and formulate a Bandwidth Constrained Tree (BCT) construction problem, which aims at maximizing the number of peers that satisfy a specified bandwidth constraint. We prove that BCT is NP-complete and propose optimal algorithms in two special cases and a heuristic approach in a general case. The performance superiority of the proposed method is illustrated by an extensive set of experiments on simulated networks of various sizes in comparison with existing greedy and degree constrained algorithms.
[Algorithm design and analysis, hybrid hierarchical P2P overlay network structure, Heuristic algorithms, media streaming architecture, Throughput, tree structured network, communication complexity, cost model, bandwidth constrained tree construction, Network topology, resource allocation, multimedia content delivery, Bandwidth, media streaming, system scalability, user load, peer-to-peer overlay network, resource utilization, client-server architecture, client-server systems, peer-to-peer computing, super peers, Peer to peer computing, trees (mathematics), live streaming, telecommunication network topology, Topology, bandwidth demand, NP-complete problem, P2P, bandwidth allocation, live streaming system, overlay networks, spanning tree, Internet]
Hybrid Checkpointing for MPI Jobs in HPC Environments
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
As the core count in high-performance computing systems keeps increasing, faults are becoming common place. Check pointing addresses such faults but captures full process images even though only a subset of the process image changes between checkpoints. We have designed a hybrid check pointing technique for MPI tasks of high-performance applications. This technique alternates between full and incremental checkpoints: At incremental checkpoints, only data changed since the last checkpoint is captured. Our implementation integrates new BLCR and LAM/MPI features that complement traditional full checkpoints. This results in significantly reduced checkpoint sizes and overheads with only moderate increases in restart overhead. After accounting for cost and savings, benefits due to incremental checkpoints are an order of magnitude larger than overheads on restarts. We further derive qualitative results indicating an optimal balance between full/incremental checkpoints of our novel approach at a ratio of 1:9, which outperforms both always-full and always-incremental check pointing.
[Checkpointing, checkpointing, application program interfaces, MPI jobs, Instruction sets, BLCR, LAM-MPI, high performance computing systems, High-Performance Computing, Image restoration, Registers, hybrid checkpointing, Checkpoint/Restart, Benchmark testing, Kernel, Message systems, Fault Tolerance]
Optimizing Energy-Latency Trade-off in Wireless Sensor Networks with Mobile Element
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Recent research shows that significant energy saving can be achieved in wireless sensor networks by using mobile elements (MEs) capable of carrying data mechanically. Though ME can reduce the energy consumption at each sensor node, it increases the latency from the time the data is generated at a node to the time the base station receives it. To address this issue, we proposed a collection-based approach in which a subset of nodes served as the data collection points (CPs) that buffer data originated from sources and transfer to ME when it arrives. CPs shorten the trajectory of ME which can gather a large volume of data at a time, however, they also increase energy consumption of nodes to transmit data to them. There is a trade-off between the energy consumption of network and the length of trajectory of ME which is determined by the number of CPs. In this paper, we introduce a Probabilistic Path Selection (PPS) algorithm to reduce the data collection delay for stochastic event detection scenario. Furthermore, we develop a heuristic algorithm and extend to the more general case of combined ME and CPs selection to enable a flexible trade-off between energy consumption and data delivery latency. Our implementation and simulation results show nearly monotonic decrease of data delivery latency for greater limits on the energy consumption, and which is same on the contrary.
[Energy consumption, Base stations, wireless sensor networks, Mobile communication, linear programming, Delay, mobile element, Wireless sensor networks, mobile communication, probabilistic path selection algorithm, energy-latency trade-off, Sensors, energy consumption]
Fault Tolerant Network Routing through Software Overlays for Intelligent Power Grids
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Control decisions of intelligent devices in critical infrastructure can have a significant impact on human life and the environment. Ensuring that the appropriate data is available is crucial for making informed decisions. Such considerations are becoming increasingly important in today's cyber-physical systems that combine computational decision making on the cyber side with physical control on the device side. The job of ensuring the timely arrival of data falls onto the network that connects these intelligent devices. This network needs to be fault tolerant. When nodes, devices or communication links fail along a default route of a message from A to B, the underlying hardware and software layers should ensure that this message will actually be delivered as long as alternative routes exist. Existence and discovery of multi-route pathways is essential in ensuring delivery of critical data. In this work, we present methods of developing network topologies of smart devices that will enable multi-route discovery in an intelligent power grid. This will be accomplished through the utilization of software overlays that (1) maintain a digital structure for the physical network and (2) identify new routes in the case of faults.
[control decision, communication link, critical infrastructure, smart power grids, digital structure, Power grids, default route, intelligent device, multiroute discovery, Fault tolerance, Network topology, intelligent power grid, Fault tolerant systems, physical control, fault tolerance, intelligent control, Distributed Fault Tolerance, fault tolerant network routing, software overlay, telecommunication network topology, multiroute pathway, Routing, cyber-physical system, network topology, power engineering computing, computational decision making, critical data, energy management systems, SCADA systems, telecommunication network routing, Distributed Networking, cyber side, smart device, Software]
Approximate Algorithms for Sensor Deployment with k-coverage in Constrained 3D Space
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Sensor deployment is one fundamental task in sensor network implementation. We generalize and investigate the problem of deploying a minimum set of wireless sensors at candidate locations in constrained 3D space of interest to achieve k-coverage of given target areas such that each point in the target areas is covered by at least k sensors. Based on different constraints on sensor locations and target areas, we formulate four sensor deployment problems: Discrete / Continuous sensor Locations (D/CL) with Discrete / Continuous Target areas (D/CT). We propose an approximate algorithm for DLDT and reduce DLCT and CLDT to DLDT by discretizing continuous sensor locations or target areas into a number of divisions without loss of sensing precision. We further consider the connected version of these four sensor deployment problems where deployed sensors must form a connected network, and propose an approximate algorithm for each of these connected deployment problems.
[Greedy algorithms, approximation theory, wireless sensor networks, continuous sensor location, Optimized production technology, wireless sensor network, k-coverage, sensor deployment, Complexity theory, discrete target area, sensing precision, Approximation methods, discrete sensor location, Wireless sensor networks, approximate algorithm, Approximation algorithms, Three dimensional displays, Sensors, continuous target area, constrained 3D space]
Distributed Spatial Analysis in Wireless Sensor Networks
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Environmental monitoring is an important application area for wireless sensor networks (WSNs). An important problem for environmental WSNs is the characterization of the dynamic behaviour of transient physical phenomena over space. In the case of mote-level WSNs, a solution that is computed inside the WSN is essential for energy efficiency. In this context, the main contributions of this paper to the literature on in network processing in WSNs are threefold. The paper further develops an algebraic framework with which one can express and evaluate complex topological relationships over geometrical representations of permanent features (e.g., buildings, or geographical features such as lakes and rivers) and of transient phenomena (e.g., areas of mist over a cultivated field). The paper then describes distributed implementations of spatial-algebraic operations over the regions represented by that framework, thereby enabling identification of topological relationships between regions. Finally, the paper presents experimental evidence that the techniques described lead to efficient runtime behaviour. Taken together, these contributions constitute a further step towards enabling the high-level specification of expressive spatial analyses for efficient execution inside a WSN.
[environmental monitoring, Event detection, wireless sensor networks, permanent features, geographic information systems, Table lookup, network processing, building, Distributed Algorithms, Algebra, lakes, Spatial Algebra, Transient analysis, rivers, telecommunication network topology, distributed spatial analysis, geometrical representations, Wireless Sensor Networks, complex topological relationships, spatial-algebraic operations, Geometry, Spatial Analysis, environmental monitoring (geophysics), buildings, Logic gates, Distributed Computing, geographical features, Topological Relationships]
Multi-consistency Data Replication
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Replication is a technique widely used in parallel and distributed systems to provide qualities such as performance, scalability, reliability and availability to their clients. These qualities comprise the non-functional requirements of the system. But the functional requirement consistency may also get affected as a side-effect of replication. Different replica control protocols provide different levels of consistency from the system. In this paper we present the middleware based McRep replication protocol that supports multiple consistency model in a distributed system with replicated data. Both correctness criteria and divergence aspects of a consistency model can be specified in the McRep configuration. Supported correctness criteria include linearizability, sequential consistency, serializability, snapshot isolation and causal consistency. Bounds on divergence can be specified in either version metric or delay metric. Our approach allows the same middleware to be used for applications requiring different consistency guarantees, eliminating the need for mastering a new replication middleware or framework for every application. We carried out experiments to compare the performance of various consistency requirements in terms of response time, concurrency conflict and bandwidth overhead. We demonstrate that in McRep workloads only pay for the consistency guarantees they actually need.
[Real time systems, Protocols, causal consistency, multiple consistency model, distributed system, corectness criteria, consistency, Delay, linearizability, snapshot isolation, serializability, multiconsistency data replication, middleware, McRep replication protocol, serializability correctness criteria, Computational modeling, replication middleware, Middleware, sequencial consistency, parallel systems, divergence, linearizability correctness criteria, sequential consistency correctness criteria, causal consistency correctness criteria, session guarantee, Replication, Data models, data handling, snapshot isolation correctness criteria]
Downlink Resource Auction in a Tree Topology Structured Wireless Mesh Network
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
We analyze the problem of downlink resource allocation in a non-cooperative multi-level tree topology structured wireless mesh network in which a selfish mesh router (MR) may refuse to relay other MRs' traffic so as to improve its own performance at the cost of overall system performance. Based on game theory, we propose an auction framework, where the parent MR serves as the auctioneer while its children MRs act as bidders and compete for time-slots. We derive a payment function from radio resource used for relaying traffic instead of money, so as to simplify the implementation and avoid the possible security problems from monetary payment. We prove the existence and uniqueness of Nash Equilibrium and propose a stochastic best response updating algorithm to allow the bids to iteratively converge to NE in a practical distributed fashion. Simulation results show the proposed auction algorithm greatly outperforms traditional algorithms in non-cooperative environments.
[game theory, telecommunication network topology, Downlink, Nash equilibrium, selfish mesh router, Relays, wireless mesh networks, Cost accounting, downlink resource auction, Wireless communication, resource allocation, Wireless mesh networks, auction algorithms, Silicon, Resource management, tree topology structured wireless mesh network, non-cooperative multi-level tree topology]
2F: A Special Cache for Mapping Table of Page-Level Flash Translation Layer
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
The development of flash memory drives flash based SSDs to enter into enterprise-scale storage systems. As the kernel of SSD, flash translation layer (FTL) attracts many attentions. Generally, there are two types of FTLs according to the granularity of address mapping: block-level and page-level mapping FTLs. We focus on the latter one. Typically, page-level mapping scheme must employ a cache to alleviate the memory pressure introduced by the big mapping table. We argue that classic cache replacement policies aren't competent for the page table cache of FTLs. The major contribution of this work is to design a dedicated cache replacement policy called Two Filters (abbreviated as 2F) for page-level mapping FTLs. 2F aims at two goals. The first is higher hit ratio as all the replacement policies pursue. As 2F not only protects frequently accessed pages, but also protects sequentially accessed pages at little cost, it does achieve a higher hit ratio. The second goal is to distinguish hot pages from the cold. This goal is special for page table of FTLs. If hot and cold pages are directed to separate blocks, garbage collection will be more efficient. In order to achieve this goal, 2F employs two filters. One is used for containing sequentially accessed pages. Another is used for selecting hot pages. Trace driven simulations present that 2F outperforms classic replacement policies in both hit ratio and data classification.
[two filters, cache, Prefetching, flash based SSD, SSD, Containers, cache storage, flash memory drives, page-level mapping, Indexes, enterprise scale storage, page level flash translation layer, flash memories, Flash memory, mapping table, data classification, Hard disks, Information filters, Kernel, flash, FTL]
Making Human Connectome Faster: GPU Acceleration of Brain Network Analysis
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
The research on complex Brain Networks plays a vital role in understanding the connectivity patterns of the human brain and disease-related alterations. Recent studies have suggested a noninvasive way to model and analyze human brain networks by using multi-modal imaging and graph theoretical approaches. Both the construction and analysis of the Brain Networks require tremendous computation. As a result, most current studies of the Brain Networks are focused on a coarse scale based on Brain Regions. Networks on this scale usually consist around 100 nodes. The more accurate and meticulous voxel-base Brain Networks, on the other hand, may consist 20K to 100K nodes. In response to the difficulties of analyzing large-scale networks, we propose an acceleration framework for voxel-base Brain Network Analysis based on Graphics Processing Unit (GPU). Our GPU implementations of Brain Network construction and modularity achieve 24x and 80x speedup respectively, compared with single-core CPU. Our work makes the processing time affordable to analyze multiple large-scale Brain Networks.
[Algorithm design and analysis, brain regions, large-scale networks, Correlation, complex brain networks, Instruction sets, graph theory, brain models, hardware computing, coprocessors, Sparse matrices, GPU, Graphics processing unit, GPU acceleration, acceleration framework, Human Connectome, connectivity patterns, Eigenvalues and eigenfunctions, Hardware, Voxel based Brain Network, multiple large-scale brain networks, multimodal imaging, voxel-base brain network analysis, human brain networks, brain network construction, single-core CPU, graphics processing unit, meticulous voxel-base brain networks, processing time, graph theoretical approaches, neurophysiology, human connectome, neural nets, disease-related alterations]
Impacts of Asynchrony on Epidemic-Style Aggregation Protocols
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
The large scale and dynamic nature of a distributed system makes it difficult to collect the attributes of the individual nodes in the network. In these systems, often an aggregate (e.g. AVG, COUNT, MIN, MAX, SUM etc) of these attributes is adequate. Epidemic-style protocols are one of the popular approaches to estimate aggregates in such systems. In existing epidemic-style aggregation protocols the accuracy of the estimated aggregate at local nodes heavily depends upon synchronization of aggregation rounds. To enforce synchronization in these protocols, length of aggregation round should be long enough so that all the nodes in the system complete their aggregation information exchange. In this paper, we study the impacts of asynchrony in epidemic-style aggregation protocols. We present a simple asynchronous technique to estimate system aggregates in a distributed system. Based upon this technique, we analyze two popular existing epidemic-style aggregation protocols, Push-Pull and Push-Sum. Through detailed simulations, we evaluate accuracy and cost of asynchronous version of these protocols. We found that to obtain an estimate of the true system aggregate, aggregation protocols do not need to be synchronized and hence an efficient estimate can be obtained in lesser time.
[Protocols, Peer to peer computing, distributed processing, distributed system, epidemic-style aggregation protocol, Synchronization, Delay, push-pull protocol, Accuracy, Aggregates, asynchrony, protocols, push-sum protocol, Message systems]
Adaptive Audio Synchronization Scheme Based on Feedback Loop with Local Clock in Wireless Audio Sensor Networks
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Wireless Audio Sensor Networks (WASNs) can provide event detection, object tracking and emergency response through cooperative audio sensor nodes. Effective collaboration of audio sensors requires precise synchronization among audio streams. Some researches have been done on the timestamp mechanism based on time synchronization ignoring propagation delay and many other researches have focused on the synchronization of simple gunshot or scream. However, for the synchronization of intermittent and fluctuating audio stream, there still exists many challenges. In this paper, we propose an effective audio synchronization scheme which can synchronize the intermittent audio streams adaptively while maintain low energy cost. On one hand, we obtain audio synchronization without global clock which save energy tremendously. On the other hand, by introducing a feedback loop mechanism, we can keep a high audio synchronization fidelity even when the audio source moves around and the sound strength varies with time. Furthermore, we discuss the extension for flexibility and scalability of this scheme when there exist several sound sources simultaneously or the audio source moves among clusters. Through experiments on a WASNs platform and simulations, we show that the proposed scheme is desirable to guarantee the accuracy of audio synchronization in practical environment with low energy cost.
[signal detection, wireless audio sensor networks, emergency response, wireless sensor networks, time synchronization, propagation delay, WASNs, local clock, adaptive audio synchronization, Magnetic heads, Feedback control, Synchronization, synchronisation, feedback loop, Feedback loop, audio signal processing, event detection, cooperative audio sensor nodes, object tracking, audio synchronization, Propagation delay, Clocks, Signal to noise ratio]
A Game-Theoretic Approach to Anti-jamming in Sensor Networks
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Jamming is a serious security threat to a wireless sensor network since the network relies on open wireless radio channel. A jamming attacker launches jamming attacks easily by transmitting high-power signals and all legitimate sensor nodes interfered by jamming signals suffer corrupted packet transmissions. More importantly, the jammer is typically strategic and chooses its jamming strategy in response to the possible defense strategy taken by the sensor network. In this paper we model the interaction between the sensor network and the attacker as a non-cooperative non-zero-sum static game. In such a game, the sensor network has a set of strategies of controlling its probability of accessing the wireless channel and the attacker manipulates its jamming by controlling its jamming probability after sensing a transmission activity. We propose an efficient algorithm for computing the optimal strategies for jamming attack and network defense. A critical issue is that there may exist a number of possible strategy profiles of Nash equilibria. To address this issue, we further propose to choose realistic Nash equilibria by applying Pareto-dominance and risk dominance. Our numerical results demonstrate that the strategies chosen by Pareto-dominance and risk dominance achieve the expected performance. Our results presented in the paper provides valuable defense guidance for wireless sensor networks against jamming attacks.
[Algorithm design and analysis, wireless sensor networks, risk dominance, Pareto-dominance, jamming, game theory, wireless sensor network, Communication system security, Jamming, game otheoretic, anti-jamming, Wireless communication, defense strategy, Wireless sensor networks, game-theoretic approach, non-cooperative non-zero-sum static game, Games, wireless channel, wireless channels, Pareto analysis, Monitoring]
A Pervasive Simplified Method for Human Movement Pattern Assessing
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Human movement pattern can be a valuable information for rehabilitation therapy, sport medicine and elderly people monitoring, but acquisition of them through multi-cite accelerormeters would result in uncomfortable wearing and complex data processing. In this paper, method of using a single waist-fixed accelerometer to detect human movement pattern was investigated and evaluated. 10 subjects were asked to run or walk on a treadmill in a regular way. A 5th order Butterworth low pass filter with cutoff frequency 20Hz was designed to filter the acceleration data and denoise the sample. By collecting the velocity from treadmill as label data and the individual's waist acceleration data, training data set was established. A Bayesian network classifier trained by EM learning algorithm was developed for human movement pattern assessing. Experiment showed that the method could predict the human walking and running state with a considerable accuracy more than 90%. Such accuracy could also be achieved even with a single superior-inferior acceleration feature. The classification of fast speed walking and normal speed one also achieved satisfying result. This indicated that in some application in which walking and running state were only needed to classify could employ the low power, low computational complexity uniaxial accelerometer as the human movement detector.
[pervasive simplified method, Humans, single waist-fixed accelerometer, ubiquitous computing, Accuracy, elderly people monitoring, low-pass filters, belief networks, learning (artificial intelligence), rehabilitation therapy, sport medicine, pattern recognition, Legged locomotion, Accelerometers, EM learning algorithm, human movement pattern assessment, acceleration feature, human movement detector, low computational complexity uniaxial accelerometer, Bayesian methods, Butterworth low pass filter, human movement, accelerometers, triaxial accelerometer, human movement pattern detection, low power accelerometer, Acceleration, Biomedical monitoring, Bayesian network classifier]
Power Aware Scheduling for Parallel Tasks via Task Clustering
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
It has been widely known that various benefits can be achieved by reducing energy consumption for high end computing. This paper aims to develop power aware scheduling heuristics for parallel tasks in a cluster with the DVFS technique. In this paper, formal models are presented for precedence-constrained parallel tasks, DVFS enabled clusters, and energy consumption. This paper studies the slack time for non-critical jobs, extends their execution time and reduces the energy consumption without increasing the task's execution time as a whole. This paper develops a power aware task clustering algorithm for parallel task scheduling Simulation results justify the design and implementation of proposed energy aware scheduling heuristics in the paper.
[Schedules, Energy consumption, Power demand, simulation, power aware scheduling, Scheduling, task analysis, parallel processing, Program processors, power aware computing, Processor scheduling, Clustering algorithms, scheduling, task clustering, high end computing, energy consumption, parallel task scheduling]
An Energy Efficient Clustering Scheme with Self-Organized ID Assignment for Wireless Sensor Networks
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
In wireless sensor networks, how to efficiently use the energy of the nodes while assigning global unique ID to each node is a challenging problem. By analyzing the communication cost of the clustering and topological features of a sensor network, we present a distributed scheme of Energy Efficient Clustering with Self-organized ID Assignment (EECSIA). In the context of EECSIA, a network first selects the nodes in the high-density areas as cluster heads, and then assigns an unique ID to each node based on local information. In addition, EECSIA periodically updates cluster heads according to the nodes' residual energy and density. The method is independent of time synchronization, and it does not rely on the nodes' geographic locations either. Simulation results show that the scheme performs well in terms of cluster scale, and number of nodes alive over rounds.
[Energy consumption, EECSIA, wireless sensor networks, topological features, node geographic locations, energy efficient clustering, Sensors, Base stations, residual energy, time synchronization, Biological system modeling, clustering scheme, telecommunication network topology, cluster heads, local information, Ad hoc networks, communication cost, synchronisation, Wireless sensor networks, pattern clustering, self-organized ID assignment, residual density, ID assignment, Mobile computing, network lifetime]
Quantitative Evaluation of TBC Systems of Gas Turbine Blades Using TFEC
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
In mechanical engineering and aerospace, gas turbine blades are taken as the crucial components, and need special treatment for protection and long-turn service. Therefore, ceramic Thermal Barrier Coating (TBC), acting as the insulation between gas and the alloying bodies of blades, is utilized to guarantee that the gas turbine blades are able to work in the high-temperature and high-stress environment. It is imperative to non-destructively evaluate TBC systems of blades, since the TBC with thin thickness or delimitation will lead to catastrophic accidents of gas turbines. In this paper, Tri-frequency Eddy Current inspection (TFEC) is proposed for quantitative evaluation of TBC systems, which involves assessment of: (1) TBC thickness to identify possible thinning, and (2) thickness and conductivity of bond coating to detect delimitation and degradation. The Levenberg-Marquardt Algorithm is adopted in the inversion for the parameters, which need to be quantified. In the inverse process, the closed-form expressions of the Jacobian Matrix, which is conventionally obtained by using interpolation functions, are derived based on an analytical modeling. The inversion method is verified by a hybrid numerical modeling, which indicates that the proposed TFEC for quantitative evaluation of TBC is valid and applicable.
[Coils, eddy current testing, blades, hybrid numerical modeling, Blades, blade alloying body, Coatings, interpolation functions, thermal barrier coatings, quantitative evaluation, Jacobian matrices, Jacobian matrix, analytical modelling, analytical modeling, inversion, inverse problems, high-temperature environment, Levenberg-Marquardt algorithm, delimitation detection, ceramic thermal barrier coating, TBC systems, mechanical engineering, high-stress environment, ceramic insulation, gas turbines, Conductivity, trifrequency eddy current inspection, closed-form expressions, TBC thickness, electromagnetic nondestructive evaluation, aerospace engineering, tri-frequency eddy current inspection, inverse process, interpolation, TFEC, Alloying, bond coating conductivity, Impedance, long-turn service, gas turbine blades, degradation deetction]
Velocity Field Based Modelling and Simulation of Crowd in Confrontation Operations
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
In peacekeeping, domestic or combat operations, unanticipated crowd confrontations can occur. As a highly dynamic social group, human crowd in confrontation is a fascinating phenomenon. This study proposes a novel method based on the concept of vector field to formulate the way in which external stimuli may affect the behaviors of individuals in a crowd. Our approach represents each individual as an autonomous agent whose actions are guided by the vector field model. Furthermore, the concept of information entropy has been adopted to describe the connection between individuals' behaviors and the potential of disorder of the whole crowd. A quantitative analysis on intangible dynamics of a crowd in confrontation is then enabled, which is significant in designing crowd control tactics.
[domestic operation, Force, Entropy, Information entropy, crowd control tactic, Analytical models, autonomous agent, Modelling &amp; Simulation, entropy, social sciences computing, Mathematical model, Vector field, information entropy, Computational modeling, Buildings, human crowd, crowd behavior, combat operation, dynamic social group, Crowd, software agents, vector field model, crowd simulation, crowd confrontation, behavioural sciences computing, Agent-based Simulation, solid modelling, confrontation operation]
Motion Tracking Based on Boolean Compressive Infrared Sampling
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
This paper concerns the issue of motion tracking with Bayesian filtering driven by boolean compressive infrared sampling. In particular, this paper proposes an implementation of boolean compressive infrared sampling modality, and a measurement transformation method which maps the presence state recovered from boolean compressive infrared sampling into functional measurement for Bayesian filtering with a functional vector quantizer. Simulation studies are reported to validate the proposed transformation method in motion tracking.
[functional quantizer, Tracking, measurement transformation method, functional vector quantizer, Motion tracking, filtering theory, Bayesian filtering, image motion analysis, vector quantisation, motion tracking, Bayesian methods, Detectors, boolean compressive infrared sampling, vector quantization, Mathematical model, Boolean compressive infrared sampling, Lenses]
A Qualitative Analysis of Uncertainty and Correlation Computing for the Business Processes of Enterprise Interoperability
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
In the domain of enterprise interoperability many uncertain factors affect the performance of the whole cross-organizational business process, e.g. uncertain business process executing time, uncertain business logic in a process, et al. Uncertain factors couldn't be avoided but can be analyzed. In this paper a model about Enterprise Interoperability Domain (EID) is given and the main uncertain factors during enterprise interoperability are analyzed. To analyze the correlation between the business processes in an EID, an updated grey correlation analysis method is given to help calculating the grey relational degree between the elements with uncertainty in an EID. The simulation shows that the result of grey correlation degree can be very helpful for the further optimization of enterprise interoperability business process.
[correlation computing, Uncertainty, Correlation, Barium, grey correlation analysis method, grey relational degree, uncertainty handling, Enterprise Interoperability, grey correlation analysis, cross-organizational business process, Analytical models, optimisation, Collaboration, uncertainty computing, Business Process, grey systems, enterprise interoperability domain, virtual enterprises, Internet, organisational aspects, Business, enterprise interoperability business process optimization]
A Novel Chaining Approach for Direct Control Transfer Instructions
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Software-based code cache systems are the key element in the dynamic translation system or optimization system to store the translated or optimized code for reuse. Translated code is organized in terms of code blocks in the code cache which transfers execution to the next code block through a control transfer instruction. As the target address of the control transfer instruction is in the form of its source program counter, the code cache system has to check the address mapping table for the translated program counter of the target address before entering the required code block. This will cause the performance degradation. As the target address of the direct control transfer instruction is fixed during the execution of a program, its source target address can be replaced with the translated target address. A direct control transfer chaining approach which occupies specific software assists is proposed in this paper. Evaluation of DCTC is conducted on a code cache simulator. The experiment results show the dramatic performance improvement brought by DCTC.
[codes, chain, direct control transfer instructions, simulator, Radiation detectors, cache storage, Table lookup, dynamic translation system, direct control transfer, Optimization, Degradation, optimisation, direct control transfer chaining approach, performance, software based code cache systems, optimization system, code cache, Benchmark testing, Software]
Vapor: Virtual Machine Based Parallel Program Profiling Framework
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
It is hard to execute parallel program efficiently on man-core platform because we could not divide program into appropriate granularity executed simultaneously. Based on virtual machine and binary translation technologies the article proposes the vapor profiling framework that uses SBIRP instruction in-place replacement method to collect program's run-time control flow and data flow information precisely. Moreover, it explains how to create control flow and data flow dependency graphs. Experiment results prove that vapor has better performance than traditional methods.
[Context, vapor, program profiling, data flow information, parallel program profiling framework, vapor profiling framework, data flow graphs, Virtual machining, man-core platform, Flow graphs, run-time control flow, parallel programming, Virtual machine monitors, virtual machine, Operating systems, data flow dependency graph, data flow computing, virtual machines, binary translation technology, binary translation, Time factors, instruction in-place replacement method, Monitoring, super-block-based in-place replacement]
Cooperative Work Systems for the Security of Digital Computing Infrastructure
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
On open digital computing infrastructure, various large-scale and complicated malicious behaviors are increasingly threatening the security of digital computing infrastructure. In this paper, a Cooperative Work Model (CRM) is presented by extending the conceptions of the Universal Turing Machine to deal with the threats. Then the Cooperative Work System Framework (CWSF) is derived from the model. Based on the framework, two practical Cooperative Work Systems (CWSs) are developed to track and analyze the Botnet and DDoS on digital computing infrastructure respectively. The systems collectively use and coordinate various monitoring systems distributed in the back-bone network of the infrastructure. The experimental results of analyzing typical security events show that the framework and systems are efficient and effective to collaboratively use diverse related network systems for monitoring and analyzing the large-scale network events. Currently, the systems are running steadily in the monitoring environment of a large-scale back-bone network.
[Protocols, universal turing machine, Computational modeling, Botnet, bonets, Security, Servers, security of data, complicated malicious behavior, groupware, digital computing infrastructure, cooperative work system, Collaborative work, large-scale complicated malicious behavior, backbone network, DDoS, IP networks, Univerasl Turing Machine, Monitoring, cooperative work system framework]
Space Speedup and Its Relationship with Time Speedup
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
In the optimizing work of parallel program, especially in the realm of massively parallel computing, the parallel computing time and space must be concurrently carefully considered to cut down the computing time as much as possible, because lots of poor parallel space strategies would impact negative effects on computing time. Although, sometimes we have no choice but to sacrifice the space for the time's further diminishing. What relationship should the computing time and space to keep and how are they going on are two problems, which deciding our optimizing direction directly and must be clear in parallel optimizing. This paper proposes a space theory, named as space speedup, to denote the scalability of memory requirement, and discusses the relationship of time speedup and space speedup, through which the speedups' guidance capacity in optimizing parallel codes are given.
[Computers, space theory, Multicore processing, Computational modeling, Scalability, parallel optimizing, parallel space strategies, parallel processing, time speedup, massively parallel computing, Program processors, Memory management, optimizing direction, Parallel processing, space speedup, parallel program, parallel computing]
Building Similar Link Network in Large-Scale Web Resources
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Similar Link Network (SiLN) is a semantic over layer on Web resources with similar relations instead of hyperlinks, which aims at providing plentiful semantics for intelligent Web activities. However, SiLN is difficult to be built based on cosine computation in large-scale Web resources due to its high building time complexity and weak connectivity. Herein, three strategies are proposed to address those issues. First, dividing and conquering strategy is applied to divide the large-scale Web resources into amounts of rough similar communities, which reduces SiLN's building time complexity significantly. After that, a multi-level structure network is designed to effectively manage the large-scale Web resources to guarantee SiLN's connectivity. Finally, two-level feedback with isolated resources strategy is developed to improve the accuracy of the building of SiLN. Experimental results have proved that our proposed method of building SiLN is feasible and efficient, with the merits of low complexity, good connectivity and high precision.
[large-scale Web resources, similar link network, intelligent Web activities, weak connectivity, multilevel structure network, two-level feedback, divide and conquer methods, semantic Web, Communities, Buildings, time complexity, dividing and conquering, multi-level structure network, Partitioning algorithms, Complexity theory, isolated resources strategy, Sparse matrices, Accuracy, Semantics, Web resource management, dividing and conquering strategy, computational complexity]
A Policy-Based Framework for Automated SLA Negotiation for Internet-Based Virtual Computing Environment
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Service Level Agreements (SLAs) play an important role in guaranteeing successful collaborations among autonomous entities in Internet-based Virtual Computing Environment (iVCE). However, traditional static and predefined SLAs are not suitable for the open and dynamic iVCE. This paper proposes a policy-based framework for supporting dynamic and automated SLA negotiations between autonomous entities in iVCE. Requirements or constraints on SLA attributes as well as negotiation strategies of negotiating parties are both specified in policy specifications using a simple but expressive policy language that extends the WS-Policy framework. According to these policies, a negotiation agent will be dynamically created for each negotiation party, which is responsible for SLA negotiations on behalf of its owner party. We have implemented a prototype of our framework and demonstrated our approach through a case study.
[Availability, Policy, Negotiation, Protocols, Radiation detectors, Quality of service, SLA, Proposals, negotiation agent, Engines, service level agreement, policy specification, policy language, Prototypes, automated SLA negotiation, groupware, policy-based framework, entity collaboration, Internet-based virtual computing environment, Internet, iVCE, negotiation strategy, WS-Policy framework]
A Complex Network Based Virtual Computing Environment Topology Generating Method
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
The topologies of Internet and Internet-based information systems have complex network properties. Designing Internet-based virtual computing environment topology with appropriate properties is significant for both the resource sharing and system performance. We analyses the topology properties of the typical P2P systems, and proposes a new topology generating method, which includes three phases, birth, growth and maturity, and supports multi-node concurrent joining in. The iVCE topology generation method can produce stable structure, with load balancing capability. Analysis of the generated topologies shows that the degree of their super-node obeys normal distribution law, the average path length between nodes shows small-world properties.
[power-law distribution, peer-to-peer computing, Peer to peer computing, Gaussian distribution, telecommunication network topology, complex network, Topology, virtualisation, virtual computing environment topology generating method, network topology, Internet-based information system topology, Radio access networks, normal distribution law, small world, resource allocation, P2P system, resource sharing, Virtual Computing Environment, Complex networks, Internet, information systems, load balancing capability, Internet topology]
A VMM-Based System Call Interposition Framework for Program Monitoring
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
System call interposition is a powerful method for regulating and monitoring program behavior. A wide variety of security tools have been developed which use this technique. However, traditional system call interposition techniques are vulnerable to kernel attacks and have some limitations on effectiveness and transparency. In this paper, we propose a novel approach named VSyscall, which leverages virtualization technology to enable system call interposition outside the operating system. A system call correlating method is proposed to identify the coherent system calls belonging to the same process from the system call sequence. We have developed a prototype of VSyscall and implemented it in two mainstream virtual machine monitors, Qemu and KVM, respectively. We also evaluate the effectiveness and performance overhead of our approach by comprehensive experiments. The results show that VSyscall achieves effectiveness with a small overhead, and our experiments with six real-world applications indicate its practicality.
[operating system kernels, program monitoring, virtualization, operating system, system call interposition, program behavior monitoring, Registers, system call sequence, kernel attack, VSyscall, security of data, Linux, Loading, virtualization technology, virtual machines, security tool, VMM, VMM-based system call interposition, system monitoring, Malware, Kernel, system call correlating method, virtual machine monitor, Monitoring]
A Prefetching Framework for the Streaming Loading of Virtual Software
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
In recent years, the Software as a Service, largely enabled by the Internet, has become an innovative software delivery model. During the streaming execution of virtualization software, the execution will wait until the missing data was downloaded, which greatly influences the user experience. In this paper, we present a block-level prefetching framework for streaming delivery of software based on N-Gram prediction model and an incremental data mining algorithm. The prefetching framework uses the historical block access logs for data mining, then dynamically updates and polishes the prefetching rules. The experimental results show that this prefetching framework achieves a launch time reduced by 10% to 50%, as well as hit rate between 81% and 97%.
[software as a service, Prefetching, Software algorithms, Streaming Deliverying, data mining, SaaS, virtualisation, Data mining, Servers, prefetching, log mining, prefetching framework, data mining algorithm, Databases, virtual software streaming loading, software virtualization, Loading, innovative software, Internet, cloud computing, N-Gram prediction model, streaming execution]
PGOS: An Architecture of a Personal Net Computing Platform
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
The development of web has brought rich applications and services, giving users convenience, but also causing that the user's information is locked and isolated, the users' resource are disperse, and the operation granularity is not uniform, which ultimately harm the end-users. This paper presents PGOS,Personal Grid Operation System, a general-purpose software for controlled sharing of cross-domain resources in personal net computing [1]. It accesses disperse resources uniformly from web client in order to connect the information islands formed by the companies, and it provides uniform fine-grained sharing mechanism, in addition, we can build new applications by combining the integrated resources in PGOS. The article proposes PGOS Core to complete decentralized user authentication, authorization and access control, Funnel is used to abstract the resource and make decentralized resource discovery, simultaneously PGSML, the Personal Grid Service Markup Language, is put forward to construct PGOS applications.
[Access control, Funnel, peer-to-peer computing, Blogs, grid computing, World Wide Web, personal grid service markup language, Browsers, decentralized resource discovery, Engines, FSDF, authorization, personal net computing platform, PGSML, personal grid operation system, Authentication, message authentication, Computer architecture, Organizations, authorisation, decentralized user authentication, access control, PGOS]
PMSAI: A Novel Peer-to-Peer Multimedia Streaming Architecture over IMS
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Deploying multimedia streaming over IP Multimedia Subsystem (IMS) can enable all kinds of user equipments to enjoy unified streaming service, without considering the differences of heterogeneous accessing methods. The combination of IMS and peer-to-peer (P2P) may help operators (especially telecommunication operators) to provide more scalable multimedia streaming. In this paper, we propose PMSAI, a novel P2P multimedia streaming architecture over IMS, in order to expand the capability of IMS multimedia streaming and reduce server side resource consumption, while restricting P2P traffic within local areas and trying best to keep standard IMS unchanged. To study how PMSAI performs, we also build a experimental system of live multimedia streaming. Carefully measurement study demonstrates the feasibility and effectiveness of PMSAI. To the best of our knowledge, PMSAI is the first P2P multimedia streaming architecture over IMS.
[Protocols, peer-to-peer computing, Peer to peer computing, Media, Multimedia communication, IMS, IP multimedia subsystem, PMSAI, Bandwidth, Streaming media, media streaming, peer-to-peer multimedia streaming architecture, heterogeneous accessing methods, IP networks]
A General Distributed Object Locating Architecture in the Internet of Things
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
This paper proposes a novel platform for object locating application in the Internet of Things environment. In this platform, objects and inquirers access and query locations using uniform service entry interfaces in heterogeneous services. To build a virtual storage system, services entries integrate enterprise database clusters and a DHT peer-to-peer network built with inquirers' devices. The DHT network is originally designed for accurate object locating, to enable fuzzy object locating we construct a hierarchical storage overlay network based on the DHT network. This LBS platform simplifies the object locating operation for ordinary inquirers greatly, moreover it provides huge virtual computing and storage resources for small companies and individual developers.
[DHT peer-to-peer network, peer-to-peer computing, uniform service entry interfaces, Peer to peer computing, hierarchical structure, Companies, object locating, user interfaces, Servers, Internet of Things, Internet-of-Things environment, distributed object locating architecture, Databases, virtual storage system, distributed hash table, Computer architecture, Internet, enterprise database clusters, hierarchical storage overlay network, peer-to-peer network]
Towards the Efficiency of Ontology-Based Context Reasoning in Virtual Computing Environment
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Context-aware computing is one of the key issues in virtual computing environment. This paper aims to compare the ontology-based context modeling and reasoning approach with the traditional non-semantic one. A general scenario of virtual computing is given about the dynamical service aggregation and evolution. And the context of the environment is modeled respectively using an OWL based way and a Markup scheme. Then two sets of experiments are designed to analyze these two approaches from different perspectives like reasoning efficiency, modeling capability and convenience.
[Context, Context Reasoning, Object oriented modeling, Computational modeling, Ontology, OWL, virtual computing environment, Cognition, dynamical service aggregation, markup scheme, ubiquitous computing, knowledge representation languages, context aware computing, Efficiency, XML, ontology based context reasoning, Virtual Computing Environment, ontologies (artificial intelligence), Context modeling]
Personalized Reputation Model in Cooperative Distributed Systems
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Reputation systems provide a promising way to build trust relationships between users in distributed cooperation systems, such as file sharing, streaming, distributed computing and social network, through which a user can distinguish good services or users from malicious ones and cooperate with them. However, most reputation models mainly focus on evaluating the qualities of different services in one dimension, but care less about the preferences of different users. This paper proposes a personalized reputation model which provides each user a personalized trust view on others according to his preference. In our approach, we aggregate the users' preferences with collaborative filtering method and qualify it with user similarity which is integrated into the computing of reputation values. The experimental results suggest that our model can resist possible kinds of malicious behaviors efficiently.
[Measurement, personalized reputation model, distributed processing, Electronic mail, reputation value, distributed computing, social network, malicious behavior, similarity, groupware, cooperative distributed system, user preference, user similarity, file sharing, personalized trust view, distributed cooperation system, Computational modeling, Peer to peer computing, eigentrust, Indexes, security of data, Aggregates, Resists, reputation system, collaborative filtering method]
Design and Practice on iVCE for Memory System
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
As one of the components in iVCE software platform, iVCE/M devotes to the performance improvement of the I/O-intensive and memory-intensive applications with efficient aggregation of distributed memory resources. To facilitate the deployment of iVCE/M, the data locating algorithm with balanced time and space cost, as well as the transparent interface for the legacy applications without code modification, are both significant in the implementation of iVCE/M. We propose the logarithmic search tree based client-side metadata structure to accelerate the data locating using moderate memory consumption, the implicit I/O redirection mechanism, and the implementation of iVCE/M based disk cache system. The experiments with cross domain emulation prove that the scheme is applicable to exploit the distributed memory resources for applications with small granularity I/O accesses.
[meta data, memory intensive applications, input-output programs, Servers, memory system, distributed memory resources, client side metadata structure, Memory management, Prototypes, iVCE software platform, I/O-intensive applications, code modification, I/O redirection mechanism, Internet, transparent interface, Kernel, Driver circuits]
Efficient Virtual Machine Deployment in Large Scale Resource Environment
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Combining virtual machine technology, virtual computing is able to effectively aggregate the widely distributed resources to provide users services. We view the federation of multiple data centers and voluntary resources on the Internet as a very large scale resource pool. Based on the tree structure of the pool, this paper proposes a virtual machine deployment algorithm, called iVDA, considers users' requests and the capabilities of the physical resources as well as the dynamic load, implements an adaptive mechanism to scheduling servers to host virtual machines forming virtual execution environments for various applications, and supports on-demand computing.
[very large scale resource pool, Heuristic algorithms, tree structure, Cloning, on-demand computing, multiple data centers, Dynamic scheduling, virtual computing, Virtual machining, dynamic load, Servers, virtual machine deployment, computer centres, Processor scheduling, virtual execution environments, virtual machines, large scale resource environment, distributed resources, Internet, tree data structures, iVDA]
CANSE: A Churn Adaptive Approach to Network Size Estimation
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Network size is one of the fundamental information of distributed applications. The approach to estimate network size must feature both high accuracy and robustness in order to adapt to the dynamic environment in different topologies. However, existing approaches fail to guarantee accuracy and robustness simultaneously in dynamic topologies due to the randomness of nodes sampled. In this paper, we propose a churn adaptive approach to network size estimation - CANSE, which collects closest nodes in identification to each node's identification by sampling nodes periodically. Each node collects closest identifications by two schemes. One scheme is sampling random nodes from random walks along the topology. The other one is exchanging the closest identifications with other nodes. Finally, each node calculates the average spacing of the closest identifications collected to estimate network size. Compared with existing approaches, extensive experiments show that CANSE provides accurate estimation values quickly in various dynamic topologies.
[dynamic topologies, sampling methods, Peer to peer computing, scale-free, Estimation, random network, distributed processing, telecommunication network topology, network size estimation, Topology, size estimation, CANSE, network size, Convergence, Accuracy, small world, Network topology, Robustness, churn adaptive approach]
Nexus: Speculative Execution for Event-Driven Networking Programs
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
The efficiency of communication is a key factor to the performance of networking applications, and concurrent communication is an important approach to the efficiency of communication. However, many concurrency opportunities are very difficult to exploit because they depend on some undeterministic conditions. If these conditions are highly predictable, speculative execution can be a very effective approach to cope with the uncertainties. Existing researches on speculation seldom target at networking systems, and none of them can handle the event-driven model that is very popular in such systems. In this paper, we propose Nexus, a novel speculation scheme that supports event-driven networking applications. Nexus analyzes the dependence relationship of events, and performs speculation according to the duality of events and threads. Evaluation on a prototype implementation of nexus shows that this approach can significantly reduces the time needed to complete an event-driven program.
[speculation, Protocols, Instruction sets, Nexus, concurrent communication, Containers, Programming, distributed processing, event-driven, networking, event-driven networking program, Prototypes, concurrency control, Benchmark testing, distributed systems, Internet, speculative execution]
Small-World Social Relationship Awareness in Unstructured Peer-to-Peer Networks
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Unstructured peer-to-peer (P2P) file-sharing networks are popular in the mass market. As the peers participating in unstructured networks interconnect randomly, they rely on flooding query messages to discover objects of interest. Empirical measurement studies indicate that the peers in P2P networks have similar preferences, and recently proposed unstructured P2P networks intend to organize the participating peers in a small-world (SW) fashion by exploiting the knowledge of contents stored in peers. As existing algorithms for constructing SW-based unstructured P2P networks may not precisely reveal the object sharing patterns, the resultant networks thus may not perform searches efficiently and effectively by exploiting the common interests among peers. In this paper, we suggest a novel P2P network formation algorithm to construct SW-based unstructured networks. We validate our proposal in simulations with an empirical data set, and the simulation results prove that our proposal greatly outperforms existing algorithms in terms of search efficiency and effectiveness.
[Measurement, object sharing patterns, mass market, P2P file-sharing networks, Search problems, Proposals, SW-based unstructured networks, unstructured overlay networks, query processing, Peer-to-peer systems, empirical measurement study, small-world social relationship awareness, unstructured peer-to-peer networks, social aspects of automation, small-world fashion, message passing, peer-to-peer computing, Peer to peer computing, YouTube, Simulation, social relations, flooding query messages, unstructured P2P networks, P2P network formation algorithm, Joining processes]
Decentralized Search in Scale-Free P2P Networks
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Search in peer-to-peer networks is a challenging problem due to the absence of any centralized control &amp; the limited information available at each node. When information is available about the overall structure of the network, use of this information can significantly improve the efficiency of decentralized search algorithms. Many peer-to-peer networks have been shown to exhibit power-law degree distribution. We propose two new decentralized search algorithms that can be used for efficient search in networks exhibiting scale-free design. Unlike previous work, our algorithms perform efficient search for a large range of power-law coefficients. Our algorithms are also unique in that they complete decentralized searches efficiently even when the network has disconnected components. As a corollary of this, our algorithms are also more resilient to network failure.
[Algorithm design and analysis, Measurement, scale-free P2P network, power-law distribution, peer-to-peer computing, Peer to peer computing, Heuristic algorithms, decentralized search algorithm, Peer-to-peer networks, Routing, power-law degree distribution, Analytical models, scale-free networks, Approximation algorithms, centralized control, decentralized search, scale-free design, peer-to-peer network]
Characteristics of Random Walk Search on Embedded Tree Structure for Unstructured P2Ps
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
We propose a random-walk-based file search for unstructured P2P networks. In the proposal, each node keeps two pieces of information, one is on the hop-limited shortest path tree rooted at itself and the other is on the indexes of files owned by neighbor nodes, referred to as the file list. A random-walk search is conducted along the concatenation of hop limited shortest path trees. To find a file, a node first checks its file list. If the requested file is found in the list, the node sends the file request message to the file owner, otherwise, it sends a file-search message to a randomly-selected leaf node on the hop-limited shortest path trees. Numerical examples show that our proposal is much more efficient than the normal random-walk search, while it waists much less network bandwidth than the flooding (network-broadcast) based search.
[Barium, random walk search, file request message, peer-to-peer computing, Peer to peer computing, graph theory, random processes, Routing, hop-limited shortest path tree, Proposals, Indexes, file list, P2P, shortest path tree, random walk, Network topology, embedded tree structure, Bandwidth, unstructured, file search, tree data structures, unstructured P2P network]
A Hierarchical DHT for Fault Tolerant Management in P2P-SIP Networks
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
This paper focuses on fault tolerance of super-nodes in P2P-SIP systems. The large-scale environments such as P2P-SIP networks are characterized by high volatility (i.e. a high frequency of failures of super-nodes). Most fault-tolerant proposed solutions are only for physical defects. They do not take into account the timing faults that are very important for multimedia applications such as telephony. We propose HP2P-SIP which is a timing and physical fault tolerant approach based on a hierarchical approach for P2P-SIP systems. Using the Oversim simulator, we demonstrate the feasibility and the efficiency of HP2P-SIP. The obtained results show that our proposition reduces significantly the localization time of nodes, and increases the probability to find the called nodes. This optimization allows to improve the efficiency of applications that have a strong time constraints such as VoIP systems in dynamic P2P networks.
[Electric breakdown, peer-to-peer computing, Peer to peer computing, fault tolerant management, multimedia applications, VoIP systems, VoIP, P2P-SIP networks, Servers, Oversim simulator, Delay, P2P, Fault tolerance, DHT, Fault tolerant systems, hierarchical DHT, physical defects, fault tolerant computing, SIP, Internet telephony, large scale environments]
Cooperative User Centric Information Dissemination in Human Content-Based Networks
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Powerful wireless devices carried by humans can form human contact-based networks. Such networks often suffer from intermittent connectivity. Thus, providing an effective information dissemination feature in such networks is very important. In this paper, we explore a cooperative user centric information dissemination scheme which allows published data items to be delivered to interested nodes efficiently. Our scheme uses fewer relays and allows each node to operate distributedly using locally gathered information. Our scheme is more effective than the epidemic scheme since it achieves comparable success ratio with a 45-60% reduction in storage requirement and 47-53% reduction in transmissions. We also compare our scheme with an ideal scheme which assumes one can analyze contact traces apriori to determine their dominating sets, and show that our scheme can be more efficient than this ideal scheme.
[peer-to-peer computing, Peer to peer computing, Computational modeling, Communities, human content-based network, Humans, performance evaluation, opportunistic networks, data dissemination, Ad hoc networks, intermittent connectivity, cooperative, Wireless communication, mobile computing, cooperative user centric information dissemination, user-centric, Data models]
Cloud Assisted P2P Media Streaming for Bandwidth Constrained Mobile Subscribers
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Multimedia streaming applications have disruptively occupied bandwidth in wire line Internet, yet today's fledging mobile media streaming still poses many challenges in efficient content distribution due to the form of mobile devices. At the same time, cloud computing is gaining power as a promising technology to transform IT industry and many eminent enterprises are developing their own cloud infrastructures. However, the lack of applications hinders clouds' large-scale implementation. In this paper, we envision a cloud-assisted power-efficient mobile P2P media streaming architecture that addresses the weakness of today's wireless access technologies. Clouds are responsible for storage and computing demanding tasks, and mobile devices colocating with each other share bandwidth and cooperatively stream media content to distribute the load. We first model interactions among mobile devices as a coalition game, and then discuss the optimal chunk retrieval scheduling. Finally, we draw on realistic mobile phone data and utilize an ARIMA model for colocation duration prediction among mobile devices.
[IT industry, wireless access technologies, Mobile communication, Mobile handsets, mobile media streaming, ARIMA model, bandwidth constrained mobile subscribers, cooperative mobile media streaming, cloud, coalition game, Bandwidth, media streaming, cloud infrastructures, cloud computing, mobile phone data, Power demand, mobile radio, multimedia streaming applications, peer-to-peer computing, Peer to peer computing, Media, cloud assisted P2P media streaming, p2p, ARIMA, Games, mobile devices, Internet]
Discovering Free-Riders Before Trading: A Simple Approach
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Free-riding is one of the most serious problems encountered in Peer-to-peer (P2P) systems like Bit Torrent. Incentive mechanisms, including those based on reputation have been proposed to deal with this problem, but are still not effective in preventing free-riders from completing a download. This is because they discover the free-riders' behavior during or after the process of trading, giving free-riders the opportunity to download from others. In this paper, we propose PreDiscover, a novel approach to prevent free-riding behavior in Bit Torrent. In PreDiscover, regular peers and free-riders can be recognized before trading. So free-riders have little opportunity to download blocks from others. Our simulation results indicate that this new mechanism is very effective in discouraging free-riders and foster fairness.
[Reputation System, PreDiscover, peer-to-peer computing, Peer to peer computing, Scalability, incentive mechanism, Thin film transistors, Classification algorithms, Inductors, P2P, System performance, free-riding, peer-to-peer system, BitTorrent, Robustness, Free-riding]
Multi-party Videoconferencing Based on Hybrid Multicast with Peer-Forwarding
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Multi-party videoconferencing is one many-to-many group communication application in which multicast could be utilized to save bandwidth. For native multicast is still not available everywhere, we need provide scalable and efficient way for unicast users to communicate with multicast-capable users in videoconferencing. This paper proposes one scalable hybrid multicast scheme with peer-forwarding for multi-party videoconferencing. The multicast-capable users are designed as peers to forward data as reflectors do. The group communication mechanisms of hybrid multicast with reflector-based and peer-based data forwarding are introduced. Session management, connection building between peers and unicast users, and terminal functions are introduced. We implement the videoconferencing system based on hybrid multicast and applied it on CERNET backbone. The applications and experiments show that the scheme is feasible and valid.
[session management, videoconferecing, Buildings, multicast-capable users, multi-party videoconferencing, many-to-many group communication application, terminal functions, teleconferencing, hybird multcast, Delay, Teleconferencing, Unicast, Bandwidth, multicast communication, Streaming media, hybrid multicast, peer-based data forwarding, IP networks, video communication, peer-forwarding]
Optimal Data Scheduling for P2P VoD Streaming Systems
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Media streaming services have been much more popular nowadays, and these services consume lots of network bandwidth. Peer-to-Peer(P2P) technology has been employed in the streaming systems to save the server bandwidth consumption and enhance the system scalability. In a P2P streaming system, the quality of the data scheduling scheme will largely affect the server stress that being saved. In this paper, we present our optimal data scheduling scheme to achieve both server stress minimization and fairness among users when the playback continuities of the users are guaranteed. The data scheduling scheme we proposed is based on the maximum network flow problem, and it is proved to be polynomial. Simulations also show that the time consumption for the computation of the optimal scheduling is acceptable, and the result of the scheduling also achieves small node degree bound which makes it practical.
[peer-to-peer technology, peer-to-peer computing, Peer to peer computing, media streaming services, Optimal scheduling, Scheduling, Servers, Stress, Processor scheduling, P2P VoD streaming systems, video on demand, optimal data scheduling, Data models, video streaming, P2P Streaming, Maximum Flow, Data Scheduling]
A Two-Key Agreement Based Supervising Mechanism for Cluster-Based Peer-to-Peer Applications
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Peer-to-Peer (P2P) technologies are developing rapidly and have gained popularity. The application of P2P to areas such as file sharing, collaborative business environment, and distributed computing requires secure communication among the nodes. Cluster-based P2P structure provides an efficient way to do file sharing and distributed computing. A key agreement protocol is a set of communication rules whereby two users establish a shared common key. The shared key is used by users in future secure communications. Supervising services for governing communications between two nodes is an important topic, especially in the area of government affairs. The proposed paper provides a framework for supporting a supervising mechanism in cluster-based P2P networks based on the concept of two-key agreement protocol. This mechanism uses the idea of hash-based two-key agreement protocol to help the nodes in higher level supervise the nodes in lower level for cluster-based P2P communication environment. In the proposed paper, a global cluster head supervises the whole network, cluster heads in each cluster supervise their own clusters' communications. Security analyses show that the proposed mechanism is secure enough for P2P. Any two nodes within the same cluster generate their common session key by themselves. In the same cluster, no nodes gain this session key except the cluster head. Moreover, there are only two kinds of operations, hash operation and XOR operation, in the proposed mechanism. Hence, the proposed mechanism provides an efficient way to supervise the P2P network.
[workstation clusters, peer-to-peer technology, Protocols, communication rule, cryptographic protocols, P2P network, collaborative business environment, cluster-based P2P structure, distributed computing, P2P technology, cluster-based P2P, file sharing, global cluster head, security analysis, cluster-based P2P communication environment, Peer-to-Peer technologies, peer-to-peer computing, Peer to peer computing, XOR operation, supervising mechanism, shared key, hash operation, computer network security, secure communication, two-key agreement protocol, Public key, hash-based two-key agreement protocol, Nickel]
Bandwidth- and Latency-Aware Peer-to-Peer Instant Friendcast for Online Social Networks
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Online Social Networks (OSNs) are more and more popular recently, people may through them interact with each other for the purpose of social intercourse. The client/server OSN architecture brings about the bottleneck of bandwidth and computation. It leads to the scalability problem and the communication latency increases as users grows. This paper proposes a bandwidth- and latency-aware peer-to-peer (P2P) instant friend cast scheme for every user (or peer) in OSNs to construct a friend cast tree (FCT) to send instant messages to all of its friends. A lightweight server is responsible for only easy tasks, such as logining and maintaining peer information, to facilitate the tree construction. A peer logins to the server to obtain the list of friends and their Vivaldi coordinates, which are computed by every peer in a distributed way to estimate the latency between peers. The proposed scheme also uses Available Out-Degree Estimation (AODE) to evaluate the proper out-degree of a peer, and then uses Degree-Adapted Greedy Tree Algorithm (DATGA) to construct FCT. The scheme is simulated and compared with other relevant ones to show its advantages.
[Algorithm design and analysis, client-server systems, peer-to-peer instant friendcast, multicast tree, peer-to-peer computing, degree adapted greedy tree algorithm, Peer to peer computing, online social networks, Servers, friendcast, peer-to-peer, Vivaldi coordinates, client-server OSN architecture, network coordinate, Bandwidth, Computer architecture, social networking (online), Internet, friendcast tree, available out degree estimation, Facebook, online social network]
Dual-Path Peer-to-Peer Anonymous Approach
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
There are many approaches proposed to provide anonymous peer-to-peer communications. Data sent via peer-to-peer communications is vulnerable to traffic analysis. Traffic analysis is the process of intercepting and analysing messages in order to compromise information from patterns in communications. An intruder can get information about the content of the data, the requester's and provider's identities. Anonymous approaches are designed with the following three goals: to protect the identity of provider, to protect the identity of requester and to protect the contents of transferred data between them. This article presents a new peer-to-peer approach to achieve anonymity between a requester and a provider in a hybrid peer-to-peer information-sharing environment with trusted servers called supper node so that the provider will not be able to identify the requester and no other peers can identify the two communicating parties with certainty. This article shows that the proposed approaches improved reliability and has more security. This approach, based on onion routing and randomization, protects transferring data against traffic analysis attack. The ultimate goal of this anonymous communications approach is to allow a requester to communicate with a provider in such a manner that nobody can determine the requester's identity and the content of transferred data.
[message analysis, peer-to-peer computing, Peer to peer computing, peer-to-peer information-sharing environment, message interception, Anonymity, Dual-Path, Routing, Onion Routing, Servers, computer network security, dual-path peer-to-peer anonymous approach, Peer-to-peer Networks, traffic analysis attack, Public key, Suppernode, Traffic Analysis, Reliability, anonymous peer-to-peer communications]
Load-Balancing Properties of 3D Voronoi Diagrams in Peer-to-Peer Virtual Environments
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Balancing communication workloads is a perennial performance issue in the area of Distributed Virtual Environments (DVE). The stringent time constraints of Multiplayer Online Games (MOG) complicate efforts to effectively distribute the networking load amongst servers. This issue becomes ever more exacting, when we move towards a fully Peer-to-peer virtual world (P2P-VE). We are consequently forced to factor in the limited capabilities of ordinary peers in the network. Traditional MOGs have been built on the client-server (CS) paradigm and the industry brute-force approach of over-provisioning resources is both inelegant and non-resilient in the face of failures. Moving such systems onto P2P architectures mitigates these drawbacks significantly. Our recent application of three-dimensional Voronoi Diagrams (3D-VD) onto P2P-VEs has further introduced desirable load-balancing properties in such systems. This is due to the novel use network capacity as the metric for the 3rd dimension and the subsequent use of the 3D-VD to intelligently appoint dynamic game-play arbitrators from amongst the peer population. This short paper is a preliminary report on the load-balancing properties seen in our extensive simulations. It is shown how this approach is able to appropriately distribute load in a variety of network configurations and peer populations. Thus, the performance of the collaborating peers is enhanced, ultimately leading to better game-play experience.
[Industries, Measurement, peer-to-peer virtual environments, gameplay arbitrators, P2P-VE, computational geometry, Applied Voronoi Diagrams, 3D voronoi diagrams, Servers, peer population, distributed virtual environments, resource allocation, multiplayer online games, computer games, Peer-to-peer, Three dimensional displays, load balancing properties, Multiplayer Online Games, Virtual environment, peer-to-peer computing, Peer to peer computing, MOG, client server, CS, DVE, communication workloads, Games, Distributed Virtual Environments, Internet]
Avatar Path Clustering in Networked Virtual Environments
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
With the increase of network bandwidth and the advance of 3D graphics technology, networked virtual environments (NVEs) have become popular recently. Early SIMNET and currently booming massively multiplayer online games (MMOGs), such as Second Life (SE) and World of War craft (WoW), are examples of NVEs. Because NVE users, interests or habits may be similar, avatars, or the representative of NVE users, may have similar behavior patterns, which leads to similar motion paths in the NVE. This paper proposes two NVE avatar path clustering algorithms, namely, Average Distance of Corresponding Points-Density Clustering (ADCP-DC) and Longest Common Subsequence-Density Clustering (LCSS-DC). Given avatar paths, both algorithms will produce a collection of path clusters and their representative paths (RPs), which can be used to analyze avatar behaviors for improving NVE design. We take SE user trace data as input of the algorithms to demonstrate their applicability. We also show how to adjust algorithm parameters to obtain high-quality path clustering in terms of silhouette coefficient and cluster coverage.
[Algorithm design and analysis, Measurement, Avatars, Clustering methods, Noise, ADCP-DC, 3D graphics technology, LCSS-DC, density clustering, Clustering algorithms, computer games, average distance, corresponding points-density clustering, World of War craft, networked virtual environment, MMOGs, Second Life, WoW, avatars, longest common subsequence-density clustering, Virtual environment, massively multiplayer online game, avatar path clustering, massively multiplyer online games, SIMNET, silhouette coefficient, computer graphics, pattern clustering, networked virtual environments, cluster coverage, path clustering, NVE]
Cross-Layer Design to Merge Structured P2P Networks over MANET
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Peer-to-peer (P2P) network is an alternative of client/server system for sharing resources, e.g. files. P2P network is a robust, distributed and fault tolerant architecture. There are basic two types of P2P networks, structured P2P network and unstructured P2P network. Each of them has its own applications and advantages. Due recent advances in wireless and mobile technology, the P2P network can be deployed over mobile ad hoc network (MANET). We consider the scenarios of P2P network over MANET where all nodes are not the members of P2P network. Due to limited radio range and the mobility of nodes in MANET, there can occur network partition and merging of networks in the physical network. This can also lead to P2P network partition and merging at overlay layer. When two physical networks merge by coming into communication range of each other then their P2P networks would not be connected at overlay layer. Because P2P network operates at application layer as an overlay network. That is their P2P networks are connected in physical network but these P2P networks are disconnected at overlay layer. To detect this situation and merge these P2P networks at overlay layer, we extend the ODACP, an address auto-configuration protocol. Then we propose an approach to efficiently merge P2P networks such that routing traffic is minimized. Considering limited radio range and mobility of nodes, the simulation results shows that CAN over MANET performs better as compared to Chord over MANET in term of routing traffic and false-negative ratio.
[ODACP, Merging, Mobile ad hoc networks, address auto-configuration protocol, mobile computing, Network topology, cross-layer design, mobile ad hoc networks, Peer-to-peer network, IP networks, protocols, overlay layer, structured P2P network, mobile ad hoc network, merging, peer-to-peer computing, Peer to peer computing, Routing, traffic routing, Bridges, address auto-configuration, MANET, CAN over MANET, telecommunication network routing, false negative ratio, structured P2P networks]
Achieving Lower Delay with Energy Efficiency in Extremely Low-Duty-Cycle and Unreliable WSN
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
In extremely low-duty-cycle wireless sensor networks, a sender has to wait for a certain period of time to forward a packet until its receiver becomes active, which will result in longer end-to-end delay than ever. Many works have been done to improve delivery ratio but lack of the consideration on energy efficient delivery delay. In addition, unreliable links is another challenge in wireless sensor networks. Redundancy and multiple paths can be used to cope with unreliability, but neither of them is energy efficient. Even worse, both of them have poor performance on delivery delay. In this work, we introduce a novel way of allocating erasure coded blocks over multiple paths to improve energy efficient delivery delay while achieving comparably high delivery ratio. We evaluate our algorithm with extensive simulations. Evaluations show that our design decreases delivery delay greatly with slight decrease in delivery ratio.
[Schedules, wireless sensor networks, receiver, delivery delay, Receivers, low-duty-cycle, Encoding, delivery ratio, allocation strategy, radio receivers, Delay, Wireless sensor networks, coding, end-to-end delay, packet forwarding, block codes, multiple path, unreliable WSN, redundancy, Resource management, Reliability, extremely low duty cycle, erasure coded blocks]
Distributed Real-Time Data Traffic Statistics Assisted Routing Protocol for Vehicular Networks
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
A Vehicular Ad Hoc Network (VANET) not only experiences highly mobile and frequently disconnected, but may also have to deal with rapid changes of network topologies, especially when accidents and road traffic jams happen. In this paper, we propose a novel approach to address this issue. Due to the intermittent connectivity in VANET, we adopt the idea of carry and forward, where a moving vehicle carries the message until forwarding the message to a new vehicle. Different from existing carry and forward solutions in VANET, we make use of the distributed real-time evaluations of data traffic statistics of all roads for each vehicle. Based on the evaluation of current message delivery delay along each road, each vehicle can find the routing path to forward the message to reduce the delay. We propose a distributed real-time data traffic statistics assisted routing protocol (DRTAR) to forward the message to the appropriate road. Experimental results show that the proposed DRTAR protocol outperforms other solutions.
[Real time systems, vehicular ad hoc networks, road traffic, Protocols, message passing, Roads, road traffic jams, distributed real-time data traffic statistics, telecommunication network topology, VANET, routing protocol, Routing, Ad hoc networks, message forwarding, network topology, accidents, intermittent connectivity, Delay, Vehicles, Vehicle Ad Hoc network, real-time systems, routing protocols, vehicular ad hoc network, Delay-Tolerant Network]
Load Aware Routing with Delay Threshold for Vehicular Ad Hoc Networks
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
It is being envisaged that a wide variety of applications can be running on vehicular ad hoc networks (VANETs) in the near future. The coexistence of these applications suggests that they will be competing for the use of the wireless medium. That easily leads to severe congestion and even &#x201C;hotspot&#x201D; problem in high traffic density urban areas, and results in the failure of time-critical applications. To address this issue, we propose two carry-and-forward schemes called LARD-Greedy and LARD-Optimal respectively that attempt to deliver packets along the path with optimal performance. The proposed algorithms leverage local or global knowledge of traffic statistics to choose the path with optimal performance, and rationally alternate between the Carrying and Multihop Forwarding strategies according to the current load status in order to avoid congestion. Experimental results based on a real city map show the proposed solutions reach very good performance in terms of packet delivery ratio and delay.
[traffic density, telecommunication congestion control, Roads, multihop forwarding strategy, Delay, Vehicles, LARD-optimal scheme, packet delivery, traffic statistics, vehicular ad hoc network, Routing protocols, hotspot problem, carry-and-forward scheme, vehicular ad hoc networks, carrying strategy, network congestion, VANET, routing protocol, Routing, Ad hoc networks, load aware routing, telecommunication network routing, load aware, LARD-greedy scheme, delay threshold, statistical analysis, telecommunication traffic]
A Primal Dual Approach for Dynamic Bid Optimization
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
We study the dynamic bid optimization problem via a primal dual approach. In the case we have no information about the distribution of queries, we reconstruct the ln(U/L) + 1 competitive algorithm proposed in [ZCL08] through a systematic way and showed the intuition behind this algorithm. In the case of random permutation model, we showed that the learning technique used in [DH09] can give us a (1 - O(&#x03F5;)) competitive algorithm for any small constant &#x03F5; &gt;; 0 as long as the optimum is large enough.
[Algorithm design and analysis, approximation theory, bid optimization, Smoothing methods, Heuristic algorithms, Optimized production technology, primal dual approach, dynamic bid optimization, Systematics, optimisation, approximation algorithm, Approximation algorithms, query distribution, random permutation model, competitive algorithm, computational complexity]
[Publisher information]
2010 IEEE 16th International Conference on Parallel and Distributed Systems
None
2010
Provides a listing of current committee members and society officers.
[]
Message from General and Program Co-chairs
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Presents the welcome message from the conference proceedings.
[]
Organizing Committee
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Provides a listing of current committee members.
[]
Program Committee
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Provides a listing of current committee members and society officers.
[]
Extending Lifetime and Reducing Garbage Collection Overhead of Solid State Disks with Virtual Machine Aware Journaling
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Virtualization is becoming widely deployed in commercial servers. In our previous study, we proposed Virtual Machine Aware journaling (VMA journaling), a file system journaling approach for virtual server environments. With reliable VMM and hardware subsystems, VMA journaling eliminates journal writes while ensuring file system consistency and data integrity in virtual server environments, allowing it to be an effective alternative to traditional journaling approaches in these environments. In recent years, solid-state disks (SSDs) have shown their potential as a replacement of traditional magnetic disks in commercial servers. In this paper, we demonstrate the benefits of VMA journaling on SSDs. We compare the performance of VMA journaling and three traditional journaling approaches (i.e., the three journaling modes of the ext3 journaling file system) in terms of lifetime and garbage collection overhead, which are two key performance metrics for SSDs. Since a Flash Translation Layer (FTL) is used in an SSD to emulate traditional disk interface and the SSD performance highly depends on the FTL being used, three state-of-the-art FTLs (i.e., FAST, Super Block FTL and DFTL) are implemented for performance evaluation. The performance results show that, traditional full data journaling could reduce the lifetime of the SSD significantly. VMA journaling extends the SSD lifetime under full data journaling by up to 86.5%. Moreover, GC overhead is reduced by up to 80.6% when compared to the full data journaling approach of ext3. Finally, VMA journaling is effective under all the FTLs. These results demonstrate that VMA journaling is effective in SSD-based virtual server environments.
[lifetime extension, commercial server, virtual server environment, virtualisation, Servers, disc storage, storage management, File systems, flash memories, journaling file system, Operating systems, garbage collection overhead reduction, Benchmark testing, Hardware, disk interface emulation, solid state disks, virtualization, performance evaluation, Virtual machining, data integrity, file system journaling approach, file system consistency, flash translation layer, virtual machine, Flash memory, virtual machine aware journaling, virtual machines]
Scheduling Mixed Real-Time and Non-real-Time Applications in MapReduce Environment
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
MapReduce scheduling is becoming a hot topic as MapReduce attracts more and more attention from both industry and academia. In this paper, we focus on the scheduling of mixed real-time and non-real-time applications in MapReduce environment, which is a challenging problem but receives only limited attention. To solve this problem, we present a two-level MapReduce scheduler built on previous techniques and make two key contributions. First, to meet the performance goal of real-time applications, we propose a deadline scheduler which adopts (1) a sampling based approach-Tasks Forward Scheduling (TFS) to predict map/reduce task execution time(unlike prior work that requires users to input an estimated value). (2) a resource allocation model-Approximately Uniform Minimum Degree of parallelism (AUMD) to dynamically control each realtime job to execute with minimum tasks assignment in any time so as to maximize the number of concurrent real-time jobs. Second, through integrating this deadline scheduler into existing MapReduce scheduler, we develop a two-level scheduler with resource preemption supported, and it could schedule mixed real-time and non-real-time jobs according to their respective performance demands. We implement our scheduler in Hadoop system and experiments running on a real, small-scale cluster demonstrate that it could schedule mixed real-time and nonreal-time jobs to meet their different quality-of-service (QoS) demands.
[Real time systems, deadline scheduler, Hadoop system, Schedules, tasks forward scheduling, Real-time, Quality of service, distributed processing, Two-level Scheduler, MapReduce scheduling, MapReduce, resource allocation, Non-real-time, scheduling, application scheduling, Deadline Scheduler, sampling methods, sampling based approach, nonrealtime application, Hadoop, mixed realtime application, quality of service, resource preemption support, resource allocation model, Processor scheduling, MapReduce task execution time, quality-of-service demand, Resource management, approximately uniform minimum degree of parallelism model, Manganese]
An In-Memory Framework for Extended MapReduce
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
The MapReduce programming model simplifies the design and implementation of certain parallel algorithms. Recently, several work-groups have extended MapReduce's application domain to iterative and on-line data processing. Despite having different data access characteristics, these extensions rely on the same storage facility as the original model, but propagate data updates using additional techniques. In order to benefit from large main memories, fast data access and stronger data consistency, we propose to employ in-memory storage for extended MapReduce. In this paper, we describe the design and implementation of EMR, an in-memory framework for extended MapReduce. To illustrate the usage and performance of our framework, we present measurements of typical MapReduce applications.
[iterative methods, parallel algorithm, Programming, iterative processing, distributed applications, parallel programming, scalability, in-memory framework, MapReduce, Fault tolerance, storage management, data storage, Fault tolerant systems, experimentation, MapReduce programming model, design, Google, parallel algorithms, extended MapReduce, Computational modeling, data integrity, on-line data processing, Synchronization, data consistency, Data models, in-memory storage]
Physical Machine State Migration
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
A powerful functionality enabled by modern virtualization technologies is the ability to move a virtual machine (VM) from one physical machine to another, which enables unprecedented flexibility for system fault tolerance and load balancing. However, no similar capability exists for physical machines. This paper describes the first known successful implementation of migrating a physical machine's state from one physical Linux machine to another. This physical machine state migration (PMSM) capability greatly decreases the amount of disruption due to scheduled shut-down for non-virtualized physical machines, and is more challenging than VM migration because it cannot rely on a separate piece of software to perform the state transfer, e.g., the hyper visor in the case of VM migration. The PMSM prototype described in this paper is adapted from Linux's hibernation facility. The current PMSM prototype can migrate a physical machine running the MySQL DBMS server under 7 seconds.
[Structured Query Language, MySQL DBMS server, load balancing, data centers, Suspend, virtualisation, Servers, Data Center, resource allocation, virtualization technology, Benchmark testing, Hardware, Kernel, physical machine state migration, powerful functionality, database management system, fault tolerance, Linux hibernation facility, State Migration, Image restoration, relational databases, computer centres, SQL, Virtual machine monitors, Processor scheduling, virtual machine, Linux, virtual machines, Linux machine, nonvirtualized physical machines, fault tolerant computing, VM migration, Resume, Fault Tolerance]
Hypervisor Support for Efficient Memory De-duplication
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Memory de-duplication removes the memory state redundancy among virtual machines that run on the same physical machine by identifying common memory pages shared by these virtual machines and storing only one copy for each of common memory pages. A standard approach to identifying common memory pages is to hash the content of each memory page, and compare the resulting hash values. In a virtualized server, only the hyper visor is in a position to compute the hash value of every physical memory page on the server, but the memory de-duplication engine is best implemented outside the hyper visor for flexibility and simplicity reasons. A key design issue in the memory de-duplication engine is to minimize the performance impact of these hashing computations on the running VMs. To reduce this impact, memory page hashing should be performed with low overhead and when the CPU is idle. This paper describes why existing hyper visors do not provide adequate support for our memory de-duplication engine, how a new primitive called deferrable aggregate hyper call (DAH) fills the need, and what the resulting performance improvement is.
[Context, memory deduplication, memory state redundancy, Schedules, hashing computations, virtualization, defferable function, deferrable aggregate hyper call, Electronic mail, Engines, storage management, Virtual machine monitors, Memory management, virtual machines, memory de-duplication, Kernel, hypervisor support]
A Hierarchical Memory Service Mechanism in Server Consolidation Environment
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Increasing Internet business and computing footprint motivate server consolidation in data centers. Through virtualization technology, server consolidation can reduce physical hosts and provide scalable services. However, the ineffective memory usage among multiple virtual machines (VMs) becomes the bottleneck in server consolidation environment. Because of inaccurate memory usage estimate and the lack of memory resource managements, there is much service performance degradation in data centers, even though they have occupied a large amount of memory. In order to improve this scenario, we first introduce VM's memory division view and VM's free memory division view. Based on them, we propose a hierarchal memory service mechanism. We have designed and implemented the corresponding memory scheduling algorithm to enhance memory efficiency and achieve service level agreement. The benchmark test results show that our implementation can save 30% physical memory with 1% to 5% performance degradation. Based on Xen virtualization platform and balloon driver technology, our works actually bring dramatic benefits to commercial cloud computing center which is providing more than 2,000 VMs' services to cloud computing users.
[Cloud computing, Server Consolidation, data centers, Iaas, virtualisation, Servers, footprint motivate server consolidation computing, storage management, resource allocation, Cloud Computing, System performance, commercial cloud computing center, virtualization technology, Benchmark testing, cloud computing, balloon driver technology, hierarchal memory service mechanism, Xen virtualization platform, Hierarchical Service, multiple virtual machines, memory usage, Memory Scheduling, computer centres, Scheduling algorithm, server consolidation environment, service level agreement, Internet business, Memory management, hierarchical memory service mechanism, virtual machines, VM, memory resource managements, Resource management]
A Method for Improving Concurrent Write Performance by Dynamic Mapping Virtual Storage System Combined with Cache Management
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
The paper presents a new dynamic mapping virtual storage system, which is characterized by: (a) distribute storage resources according to need, thus improving storage resource utilization, (b) more fundamentally, through write-anywhere pattern to sequentialize concurrent write requests by dynamic address mapping mechanism, thus improving concurrent write performance for applications. Meanwhile, taking into account imperfection small size write requests handling and the negative impact on read performance, we combine the system with CBD system which is responsible for optimizing cache management. We have implemented our virtual storage system in Linux kernel 2.6.18 as a pseudo device driver. The experiment results show that concurrent write bandwidth approaches raw disk write performance. And in 64-stream concurrent write case, E-ASD can outperform concurrent write bandwidth of LVM by up to 160%, and its max single-stream read loss is less than 25% compared to LVM, with concur-rent read performance close to each other. Read performance still has space to be improved given larger cache and proper prefetching.
[Performance evaluation, dynamic mapping, virtual storage, concurrent write performance improvement, cache storage, E-ASD, storage resource utilization, dynamic mapping virtual storage system, Linux kernel 2.6.18, concurrent write performance, mapping mechanism, cache management, cache management optimisation, pseudo device driver, Aggregates, Linux, Layout, Bandwidth, Variable speed drives, distribute storage resources, Resource management]
Source Code Partitioning in Program Optimization
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Program analysis and program optimization seek to improve program performance. There are optimization techniques which are applied to various scopes such as a source file, function or basic block. Inter-procedural program optimization techniques have the scope of source file and analyze the interaction and relationship between different program functions. The techniques analyze the entire translation unit (typically a source file) and optimize the whole translation unit globally instead of just optimizing inside a function. Analyzing and optimizing an entire translation unit increases compilation time drastically because many factors need to be considered during analysis and optimization. The translation unit size can be quite large, containing many functions. Another issue is that functions in different translation units can be more closely related to each other than to the functions within their translation unit. The main goal of this research is grouping or partitioning of closely related program functions into the same translation unit. Our method profiles an application, determines relationship information between program functions and groups closely related functions together. The source code partitioner method improves the processing time of inter-procedural optimization techniques by applying it to a subset of program functions. Partitioning of program functions by analyzing profiling output shows dramatic decrease in compilation time of programs. Our results show we can improve the compiling time in all tested real world benchmarks.
[divisive, interprocedural program optimization technique, Communities, source code partitioning, program compilers, program performance, Optimization, USA Councils, optimization, Clustering algorithms, program analysis, source file, instrumentation, compiler, program diagnostics, Software algorithms, Partitioning algorithms, program interpreters, translation unit, program compilation time, partitioning, program function, agglomerative, betweenness, Mutual information]
Fair and Efficient Online Adaptive Scheduling for Multiple Sets of Parallel Applications
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Both fairness and efficiency are crucial measures for the performance of parallel applications on multiprocessor systems. In this paper, we study online adaptive scheduling for multiple sets of such applications, where each set may contain one or more jobs with time-varying parallelism profile. This scenario arises naturally when dealing with several applications submitted simultaneously by different users in a large parallel system, where both user-level fairness and system-wide efficiency are important concerns. To achieve fairness, we use the equipartitioning algorithm, which evenly splits the available processors among the active job sets at any time. For efficiency, we apply a feedback-driven adaptive scheduler, which periodically adjusts the processor allocations within each set by consciously exploiting the jobs' execution history. We show that our algorithm is competitive for the objective of minimizing the set response time. For sufficiently large jobs, this theoretical result improves upon an existing algorithm that provides only fairness but lacks efficiency. Furthermore, we conduct simulations to empirically evaluate our algorithm, and the results confirm its improved performance using malleable workloads consisting of a wide range of parallelism variation structures.
[Algorithm design and analysis, Schedules, parallelism variation structure, processor allocation, parallel processing, processor scheduling, system-wide efficiency, Set response time, Program processors, Efficiency, parallel application, Multiprocessor systems, feedback-driven adaptive scheduler, multiprocessing systems, equipartitioning algorithm, Feedback-driven scheduling, set response time minimisation, Parallel applications, job execution history, Fairness, user-level fairness, online adaptive scheduling, malleable workloads, Adaptive scheduling, Online algorithms, Processor scheduling, multiprocessor system, Time factors, Resource management, time-varying parallelism profile]
Combining Multiple Metrics to Control BSP Process Rescheduling in Response to Resource and Application Dynamics
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
This article discusses MigBSP: a rescheduling model that acts on Bulk Synchronous Parallel applications running over computational Grids. It combines the metrics Computation, Communication and Memory to make migration decisions. MigBSP also offers efficient adaptations to reduce its overhead. Additionally, MigBSP is infrastructure and application independent and tries to handle dynamicity on both levels. MigBSP's results show application performance improvements of up to 16% on dynamic environments while maintaining a small overhead when migrations do not take place.
[Measurement, Algorithm design and analysis, rescheduling model, Bulk Synchronous Parallel, Heuristic algorithms, Computational modeling, multiple metrics, rescheduling, grid computing, MigBSP, heuristic, adaptation, parallel processing, bulk synchronous parallel application, computational grids, self-organizing, Program processors, Processor scheduling, scheduling, Load management, BSP process rescheduling, application dynamics, software metrics]
Roystonea: A Cloud Computing System with Pluggable Component Architecture
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
A Cloud computing system provides infrastructure layer services to users by managing virtualized infrastructure resources. The infrastructure resources include CPU, hyper visor, storage, and networking. Each category of infrastructure resources is a subsystem in a cloud computing system. The cloud computing system coordinates infrastructure subsystems to provide services to users. Most current cloud computing systems lacks pluggability in their infrastructure subsystems and decision algorithms, which restricts the development of infrastructure subsystems and decision algorithms in cloud computing system. A cloud computing system should have the flexibility to switch from one infrastructure subsystem to another, and one decision algorithm to another with ease. This paper describes Roystonea, a hierarchical distributed cloud computing system with plug gable component architecture. The component pluggability ability gives administrators the flexibility to use the most appropriate subsystem as they wish. The component pluggability of Roystonea is based on a specifically designed interfaces among Roystonea controlling system and infrastructure subsystems components. The component pluggability also encourages the development of infrastructure subsystems in cloud computing. Roystonea provides a test bed for designing decision algorithms used in cloud computing system. The decision algorithms are totally isolated from other components in Roystonea architecture, so the designers of the decision algorithms can focus on algorithm design without worrying about how his algorithm will interact with other Roystonea components. We believed that component pluggability will be one of the most important issues in the research of cloud computing system.
[Algorithm design and analysis, Cloud computing, hierarchical distributed cloud computing, Switches, distributed processing, CPU, Virtual machining, storage, Roystonea, pluggable component architecture, virtualized infrastructure resources, software architecture, Virtual machine monitors, cloud middleware, hyper visor, decision algorithm, cloud computing, Monitoring]
Optimizing Dynamic Programming on Graphics Processing Units via Adaptive Thread-Level Parallelism
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Dynamic programming (DP) is an important computational method for solving a wide variety of discrete optimization problems such as scheduling, string editing, packaging, and inventory management. In general, DP is classified into four categories based on the characteristics of the optimization equation. Because applications that are classified in the same category of DP have similar program behavior, the research community has sought to propose general solutions for parallelizing each category of DP. However, most existing studies focus on running DP on CPU-based parallel systems rather than on accelerating DP algorithms on the graphics processing unit (GPU). This paper presents the GPU acceleration of an important category of DP problems called nonserial polyadic dynamic programming (NPDP). In NPDP applications, the degree of parallelism varies significantly in different stages of computation, making it difficult to fully utilize the compute power of hundreds of processing cores in a GPU. To address this challenge, we propose a methodology that can adaptively adjust the thread-level parallelism in mapping a NPDP problem onto the GPU, thus providing sufficient and steady degrees of parallelism across different compute stages. We realize our approach in a real-world NPDP application -- the optimal matrix parenthesization problem. Experimental results demonstrate our method can achieve a speedup of 13.40 over the previously published GPU algorithm.
[discrete optimization problems, Instruction sets, Heuristic algorithms, adaptive thread-level parallelism, optimization equation, CPU-based parallel systems, parallelism, dynamic programming, graphics processing units, parallel processing, GPU, Niobium, Graphics processing unit, optimization, Parallel processing, nonserial polyadic dynamic programming, Dynamic programming, Kernel, optimal matrix parenthesization problem, parallel computing]
SAW: Java Synchronization Selection from Lock or Software Transactional Memory
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
To rewrite a sequential program into a concurrent one, the programmer has to enforce atomic execution of a sequence of accesses to shared memory to avoid unexpected inconsistency. There are two means of enforcing this atomicity: one is the use of lock-based synchronization and the other is the use of software transactional memory (STM). However, it is difficult to predict which one is more suitable for an application than the other without trying both mechanisms because their performance heavily depends on the application. We have developed a system named SAW that decouples the synchronization mechanism from the application logic of a Java program and enables the programmer to statically select a suitable synchronization mechanism from a lock or an STM. We introduce annotations to specify critical sections and shared objects. In accordance with the annotated source program and the programmer's choice of a synchronization mechanism, SAW generates aspects representing the synchronization processing. By comparing the rewriting cost using SAW and that using individual synchronization mechanism directly, we show that SAW relieves the programmer's burden. Through several benchmarks, we demonstrate that SAW is an effective way of switching synchronization mechanisms according to the characteristics of each application.
[transaction processing, Java, sequential program rewriting, Cloning, Java synchronization selection, Synchronization, synchronisation, synchronization aspect weaving, software transactional memory, lock-based synchronization, unexpected inconsistency avoidance, Surface acoustic waves, aspect, shared memory systems, atomic execution, synchronization, lock, Libraries, Software, Weaving, shared memory]
Mercury: A Negotiation-Based Resource Management System for Grids
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
In this paper, we present the design and implementation of a negotiation-based resource management system called Mercury. As the number of resources in Grids is increasing rapidly, selecting appropriate resources to execute tasks has become a crucial issue. The proposed system implements the Extended Contract Net Protocol (ECNP), which improves the standard bidding model by integrating a matchmaking technique. Our model addresses the issues of matchmaker overload and the lack of up-to-date resource state information in the original matchmaking model. To ensure that the system is user-friendly, we provide a web interface. By using the provided job templates, users can describe their jobs more easily, and service providers can deploy their services in the system in a simple manner. Furthermore, to improve the interoperability of the system, we adopt open standards, such as the WS-Agreement and JSDL protocols. Our experimental results demonstrate the scalability and efficiency of the system.
[Web interface, resource management, Protocols, open systems, matchmaking technique, grid computing, Registers, user interfaces, grids, WS-agreement, extended contract net protocol, resource allocation, system interoperability, standard bidding model, protocols, Advertising, Monitoring, Dynamic scheduling, matchmaking, open standards, JSDL protocols, negotiation based resource management system, WS-Agreement, JSDL, Information services, job templates, Internet, Resource management, Mercury]
Catwalk-ROMIO: A Cost-Effective MPI-IO
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
The nature of highly parallelized parallel file access which often consists of lots of fine grain, non-contiguous I/O requests, can degrade the I/O performance severely. To tackle this problem, a novel technique to maximize the bandwidth of the MPI-IO is proposed. This proposed technique is utilize a ring communication topology. This technique is implemented as an ADIO device of ROMIO, named Catwalk-ROMIO, and evaluated. The evaluation shows that Catwalk-ROMIO utilizing only one disk can exhibit comparable performance with parallel files systems, PVFS2 and Lustre, utilizing several file servers and disks. The evaluation also shows that Catwalk-ROMIO performance is almost independent from file access patterns, in contrast to the performance of parallel file systems performing only well with collective I/O. Catwalk-ROMIO only requires TCP/IP network for the ring communication topology and one file server which are common in HPC clusters without any additional cost. Thus, Catwalk-ROMIO is considered to be a very cost-effective MPI-IO implementation.
[ADIO device, Servers, parallel processing, ring communication topology, file servers, Bandwidth, Tunneling, file server, IP networks, parallel files systems, noncontiguous IO requests, message passing, multiprocessing systems, HPC clusters, MPI-IO, highly parallelized parallel file access, file access patterns, telecommunication network topology, Parallel File System, Supercomputers, Lustre, Topology, TCP/IP network, PVFS2, Catwalk-ROMIO, Sockets, COTS, parallel file systems]
An Efficient Video Adaptation Scheme for SVC Transport over LTE Networks
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Third Generation Partnership Project (3GPP) Long Term Evolution (LTE) offers high data rate capabilities to mobile users, and, operators are trying to deliver a true mobile broadband experience over LTE networks. Mobile TV and Video on Demand (VoD) are expected to be the main revenue generators in the near future and efficient video streaming over wireless is the key to enabling this. In this paper, we are proposing an efficient adaptation scheme for Scalable Video Coding (SVC) transport over LTE networks and investigate the benefits of this scheme for video streaming over LTE networks. Video streaming over LTE networks is analyzed using a 3GPP compliant LTE simulator using H.264 and SVC video traces. Analysis is done using real time use cases of mobile video streaming. Different parameters like throughput, packet loss ratio, delay, and jitter are compared with H.264 single layer video for unicast and multicast scenarios using different kinds of scalabilities. Results show that considerable packet loss reduction and throughput savings (18 to 30%) with acceptable video quality are achieved with proposed scheme based on SVC compared to H.264. Advantages of proposed scheme for LTE networks are evident from the simulation results.
[SVC, packet loss reduction, Scalability, 3G mobile communication, mobile broadband, H.264, Mobile communication, Throughput, VoD, scalable video coding, 3GPP compliant LTE simulator, Delay, SVC Transport, Multicast, Unicast, video on demand, Static VAr compensators, video adaptation scheme, video-on-demand, video streaming, throughput savings, Long Term Evolution, Adaptation, mobile television, LTE networks, H.264 single-layer video, video quality, video coding, high-data rate capabilities, mobile video streaming, mobile TV, Video, Streaming media, Third Generation Partnership Project, LTE]
Optimization of an Instrumentation Tool for Stripped Win32/X86 Binaries
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Many software security, instruction set architecture virtualization and performance enhancement techniques require instrumentation of application program binaries either to add run-time checks or to perform dynamic analysis and transformation. Unfortunately, commercially distributed application binaries on the Win32 platform are often stripped of their symbol table, and therefore cannot be easily disassembled, let alone correctly instrumented. BIRD is an instrumentation tool that applies an IA-32 disassembler both statically and dynamically, and successfully guarantees that no instruction in an input binary can be executed without being examined first. Unfortunately, the first version of BIRD has several performance problems. This paper describes our experiences of optimizing the first BIRD prototype to remove these problems. In particular, we develop a novel speculative disassembly technique that successfully reaps most of the performance benefits of static disassembly while ensuring the same level of correctness as dynamic disassembly, a bitmap-based target address check algorithm that reduces the fixed performance overhead associated with every instrumentation, and a comprehensive in-place instrumentation technique that relies mostly on instruction substitution and drastically cuts down the number of debug exceptions (int 3) invoked at run time. Together these performance optimizations reduce the average performance overhead of a set of batch Win32 programs from 23.6% to 8.8%.
[application program interfaces, Heuristic algorithms, instruction substitution, x86 platform, Optimization, bitmap-based target address check algorithm, Accuracy, computer debugging, debug exception, BIRD instrumentation tool, instruction set architecture virtualization, performance enhancement technique, Assembly, speculative disassembly technique, Instruments, program assemblers, Birds, Generators, dynamic disassembly, distributed application binary, application program binary, operating systems (computers), static disassembly, software security, performance optimization, Win32 platform, IA-32 disassembler]
A Semantic Decentralized Chord-Based Resource Discovery Model for Grid Computing
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Grid computing aggregates distributed computing resources to execute computationally complex jobs. The selection of resources in a Grid system involves finding and locating resources based on users' requirements. Identifying an appropriate resource selection mechanism for Grid jobs is a major concern because a suitable mechanism helps to schedule and allocate resources better. The resource discovery techniques proposed over the years can be categorized into three major types-centralized, hierarchical and decentralized. The performance of each type can be measured in terms of scalability, load balancing and fault tolerance. The resource discovery based on centralized and hierarchical techniques is recommended for small and medium size Grids. Due to limitations such as poor scalability and single point of failure in centralized and hierarchical approaches, a decentralized resource discovery is usually preferred for large size Grids. Lack of coordination between users and providers in a highly decentralized heterogeneous Grid environment often results in user jobs failing in finding relevant resources. One of the reasons for rejection of jobs is the usage of fixed schema between users request and providers availability which causes high possibility of missing relevant resources. We present a semantic decentralized resource discovery model by using P2P Chord protocol to improve job success probability and to enhance utilization of resources. Preliminary simulations carried out using GridSim and PlanetSim simulators show improved success probability for complex jobs and also show promising results with enhanced utilization of resources.
[Protocols, load balancing, Scalability, grid computing, Semantic, Ontologies, GridSim, Grid Computing, semantic decentralized resource discovery model, decentralized heterogeneous grid environment, semantic decentralized chord-based resource discovery model, resource allocation, Semantics, Computer architecture, distributed computing resources, Chord Protocol, fault tolerance, P2P chord protocol, Peer to peer computing, Computational modeling, PlanetSim, grid system, Decentralized, resource selection mechanism, Resource Discovery, fault tolerant computing]
Sorting Large Multifield Records on a GPU
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
We extend the fastest comparison based (sample sort) and non-comparison based (radix sort) number sorting algorithms on a GPU to sort large multifield records. Two extensions - direct (the entire record is moved whenever its key is to be moved) and indirect ((key, index) pairs are sorted using the direct extension and then records are ordered according to the obtained index permutation) are discussed. Our results show that for the By Field layout, the direct extension of the radix sort algorithm GRS[1] is the fastest for 32-bit keys when records have at least 12 fields, otherwise, the direct extension of the radix sort algorithm SRTS[14] is the fastest. For the Hybrid layout, the indirect extension of SRTS is the fastest for records with 2 or more keys.
[Field layout, Instruction sets, direct extension, Graphics Processing Units, sorting large multifield records, sorting multifield records, radix sort algorithm, coprocessors, GPU, Sorting, Graphics processing unit, Histograms, index permutation, Tiles, Layout, digital arithmetic, Hybrid layout, sorting, Arrays, sample sort, radix sort, number sorting algorithms, multifield records]
Strassen's Matrix Multiplication on GPUs
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
We provide efficient single-precision and integer GPU implementations of Strassen's algorithm as well as of Winograd's variant. On an NVIDIA C1060 GPU, a speedup of 32% (35%) is obtained for Strassen's 4-level implementation and 33% (36%) for Winograd's variant relative to the sgemm (integer version of sgemm) code in CUBLAS 3.0 when multiplying 16384&#x00D7;16384 matrices. The maximum numerical error for the single-precision implementations is about 2 orders of magnitude higher than those for sgemm when n = 16384 and is zero for the integer implementations.
[NVIDIA C1060 GPU, computer graphic equipment, Strassen's algorithm, mathematics computing, CUBLAS 3.0, Winograd's variant, accuracy, Vectors, Complexity theory, coprocessors, Matrix decomposition, integer GPU, GPU, Graphics processing unit, CUDA, matrix multiplication, Memory management, sgemm, Strassen matrix multiplication, Winograd variant, Kernel]
Optimization of Sparse Matrix-Vector Multiplication with Variant CSR on GPUs
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Sparse Matrix-Vector multiplication (SpMV) is one of the most significant yet challenging issues in computational science area. It is a memory-bound application whose performance mostly depends on the input matrix and the underlying architecture. Many researchers have paid more attentions on exploring a variety of optimization techniques to SpMV. One of the most promising respects is how to adapt the storage format to satisfy the underlying architecture. Alterative storage formats can largely lessen memory pressure, however, the computational resources are often underutilized. Therefore, a new storage format, which is called Compressed Sparse Row with Segmented Interleave Combination (SIC), is proposed. Stemming from Compressed Sparse Row format (CSR), SIC format employs an interleave combination pattern that combines certain amount of CSR rows to form a new SIC row. In order to further improve performance, segmented processing is also brought in. According to the empirical data, we also develop an automatic SIC-based SpMV suitable for all the matrices. Experimental results show that our approach outperforms the NVIDIA CSR vector kernel, achieving up to 12.6 &#x00D7; speedup. It also demonstrates a comparable performance with the Hybrid format, even with the highest 2.89 &#x00D7; speedup.
[Sparse Matrix-Vector Multiplication, Instruction sets, mathematics computing, storage format, Sparse matrices, GPU, Graphics processing unit, Interleaved Row Combination, optimisation, Segmented Processing, optimization, Computer architecture, Kernel, segmented interleave combination, Compress Sparse Row, Vectors, graphics processing units, matrix multiplication, variant CSR, sparse matrix-vector multiplication, computational science, Silicon carbide, segmented processing, memory-bound application, compressed sparse row, sparse matrices]
Broadcasting on Large Scale Heterogeneous Platforms with Connectivity Artifacts under the Bounded Multi-port Model
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
We consider the classical problem of broadcasting a large message at an optimal rate in a large scale distributed network. The main novelty of our approach is that we consider that the set of participating nodes can be split into two parts: "green" nodes that stay in the open-Internet and "red" nodes that lie behind firewalls or NATs. Two red nodes cannot communicate directly, but rather need to use a green node as a gateway for transmitting a message. In this context, we are interested in both maximizing the throughput (i.e. the rate at which nodes receive the message) and minimizing the degree at the participating nodes, i.e. the number of TCP connections they must handle simultaneously. We consider both cyclic and a cyclic solutions for the flow graph. In the cyclic case, our main contributions are a closed form formula for the optimal cyclic throughput and the proof that the optimal solution may require arbitrarily large degrees. In the a cyclic case, we propose an algorithm to achieve the optimal throughput with low degree. Then, we prove a worst case ratio between the optimal a cyclic and cyclic throughput and show through simulations that this ratio is on average very close to 1, which makes a cyclic solutions efficient both in terms of throughput and of number of connections.
[Communication modeling, message transmission, internetworking, Quality of service, red node, Throughput, large scale distributed network, Broadcast, Bandwidth, flow graph, optimal cyclic throughput, large scale heterogeneous platform, Resource Augmentation, Context, green node, Peer to peer computing, worst case ratio, connectivity artifact, Scheduling, Approximation Algorithms, broadcasting, bounded multiport model, Firewall, TCP connection, transport protocols, open-Internet, Internet, Feeds, gateway]
Node-to-Node Disjoint Paths in k-ary n-cubes with Faulty Edges
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Let u and v be any two given nodes in a k-ary n-cube Q<sub>n</sub>k with at most 2n-2 faulty edges. Suppose that the number of healthy links incident with u is no more than that of v, and denote this number by m. In this paper, we show that there are m mutually node-disjoint paths between u and v.
[fault tolerance, Conferences, graph theory, Parallel machines, node-to-node disjoint path, Routing, Educational institutions, Indexes, interconnection networks, healthy links incident, Program processors, Parallel processing, mutually node-disjoint path, disjoint paths, k-ary n-cube, k-ary n-cubes, faulty edge]
Building High-Performance Application Protocol Parsers on Multi-core Architectures
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Parsing packet payloads according to the syntax and semantics of an application protocol is a key step in analyzing network traffic. However, it is still a challenge to fulfill this task with high speed(10Gbps+) because parsing packets through deep-content analysis to build a corresponding syntax tree requires tremendous computing resources. Multi-core architectures provide a viable solution for building high-performance parsers for application protocols. Existing sequential application protocol parsers are hard to be reused, and building a new protocol parser from scratch is error-prone and time-consuming. This paper proposes a general and efficient approach to building high-performance parallel application protocol parsers on multi-core platforms. First, the open-source lexical analyzer FLEX is used to describe a protocol and generate a sequential parser. Then a source-to-source translation is performed to transform the sequential parser into a parallel one. Finally, an efficient parallel run-time system is built by employing lock-free design principles from top to bottom to support multi-threaded execution on multi-core processors. Experimental results show that our parsers achieve nearly 20Gbps for average HTTP packets and 5Gbps for the challenging smaller FIX packets.
[Protocols, Pipelines, Highperformance Network Processing, parallel run-time system, Servers, program compilers, packet payload parsing, source-to-source translation, Protocol Parser, Multi-core, FLEX open-source lexical analyzer, protocols, application protocol parser, Flexible printed circuits, multiprocessing systems, Multicore processing, multi-threading, Buildings, syntax tree, multicore architecture, network traffic analysis, program interpreters, multithreaded execution, deep-content analysis, lock-free design principle]
Design and Implementation of MapReduce Using the PGAS Programming Model with UPC
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
MapReduce is a powerful tool for processing large data sets used by many applications running in distributed environments. However, despite the increasing number of computationally intensive problems that require low-latency communications, the adoption of MapReduce in High Performance Computing (HPC) is still emerging. Here languages based on the Partitioned Global Address Space (PGAS) programming model have shown to be a good choice for implementing parallel applications, in order to take advantage of the increasing number of cores per node and the programmability benefits achieved by their global memory view, such as the transparent access to remote data. This paper presents the first PGAS-based MapReduce implementation that uses the Unified Parallel C (UPC) language, which (1) obtains programmability benefits in parallel programming, (2) offers advanced configuration options to define a customized load distribution for different codes, and (3) overcomes performance penalties and bottlenecks that have traditionally prevented the deployment of MapReduce applications in HPC. The performance evaluation of representative applications on shared and distributed memory environments assesses the scalability of the presented MapReduce framework, confirming its suitability.
[Java, shared memory environments, Multicore processing, Instruction sets, Programming, performance evaluation, UPC, distributed environments, C language, partitioned global address space programming model, programmability, parallel programming, MapReduce, Unified Parallel C language, customized load distribution, distributed shared memory systems, Libraries, high performance computing, HPC, distributed memory environments, parallel languages, software performance evaluation, collective primitives, Electronics packaging]
Reflex Barrier: A Scalable Network-Based Synchronization Barrier
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
High-performance computing is witnessing the proliferation of multi-core processors in parallel architectures, and the trend is expected to increase further with the emerging many-core technology, leading to hundreds of processing cores within each compute node in the near future. Along side with this trend, it is also clear that total number of cores within the whole system is increasing. To be able to harvest the fruits of this massive parallelism, inter-process synchronization and communication should be as lightweight as they can be, and should be relying on as limited involvement as possible of the participating processors/cores. The synchronization algorithms that target shared memory processors are expected not to be able to scale on many-cores as they rely on atomics, locks, and/or cache coherence protocols, which all should be very costly operations on many-cores. In the same time, some many core architectures provide user space networks on chip (NoCs) that operate similar to regular networks. In this paper, we are introducing the Reflex barrier, a new synchronization barrier algorithm that relies on fundamental networking concepts. As the barrier relies on the characteristics of the network, it requires very little intervention from the participating processors/cores. The algorithm can also be implemented as split phase, which furnish an opportunity to reduce the synchronization cost. We implemented the algorithm using Unified Parallel C (UPC), MPI and pThreads. We tested our implementation on TILE64, a 64-core processor. The performance of the Reflex barrier is also analyzed and compared to other algorithms using performance models.
[networks on chip, application program interfaces, parallel architectures, synchronization cost reduction, MPI, cache storage, Distributed memory barrier, shared memory processor, Program processors, reflex barrier, TILE64, split phase, Computer architecture, shared memory systems, Many-core clusters, Unified Parallel C, multicore processor, inter-process synchronization, high-performance computing, Message systems, Synchronization barrier, message passing, network-on-chip, scalable network-based synchronization barrier, Manycores, pThreads, Routing, parallel architecture, Synchronization, synchronisation, Tiles, many core architecture, Reflex barrier, Coherence, many-core technology, cache coherence protocol]
Is It Time to Rethink Distributed Shared Memory Systems?
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
We present core elements of Samhita, a new user level software distributed shared memory (DSM) system. Our work is motivated by two observations. First, the rise of many-core architectures is producing a growing emphasis on threaded codes to achieve performance. Second, architectural trends, especially in high performance interconnects, suggest a new look at overcoming the bottlenecks that have hindered DSM performance. Samhita leverages the capabilities of remote direct memory access (RDMA) interconnects, and views the problem of providing a shared global address space as a cache management problem. Performance results on two 256 processor clusters demonstrate scalability on micro benchmarks and two real applications. The results are the largest scale tests and achieve the highest performance of any DSM system reported to date.
[Protocols, distributed shared memory, Instruction sets, Computational modeling, threaded codes, many-core architecture, cache storage, Servers, Synchronization, remote direct memory access interconnects, storage management, Samhita, high performance interconnects, Memory management, distributed shared memory systems, user level software distributed shared memory system, shared global address space, distributed systems, distributed run-time systems, cache management problem]
Automatic Handling of Global Variables for Multi-threaded MPI Programs
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Conventional implementations of the MPI standard tend to associate one MPI process per processor, which limits their support for modern multi-core platforms. An increasingly popular approach is to combine MPI with threads where MPI "processes" are light-weight threads. Global variables in legacy MPI applications, however, present a challenge because they may be accessed by multiple MPI threads simultaneously. Thus, transforming legacy MPI applications to become thread-safe in such MPI execution environments requires proper handling of global variables. In this paper, we present three approaches to automatically eliminate global variables to ensure thread-safety for an MPI program. These approaches include: (a) a compiler-based refactoring technique, using a Photran-based tool as an example, which automates the source-to-source transformation for programs written in Fortran, (b) a technique based on a global offset table (GOT); and (c) a technique based on thread local storage (TLS). The second and third methods automatically detect global variables and privatize them for each thread at runtime. We discuss the advantages and disadvantages of these approaches and compare their performance using both synthetic benchmarks, such as the NAS Benchmarks, and a real scientific application, the FLASH code.
[Instruction sets, adaptive MPI, Fortran, Reactive power, Runtime, source-to-source transformation, global variables, Libraries, Message systems, multi-threaded MPI, Context, message passing, thread safety, multi-threading, software maintenance, scientific application, NAS Benchmarks, automatic global variable handling, thread local storage, Privatization, privatization, legacy MPI applications, FORTRAN, global offset table, FLASH code, Photran based tool, multithreaded MPI programs]
Automatic FFT Performance Tuning on OpenCL GPUs
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Many fields of science and engineering, such as astronomy, medical imaging, seismology and spectroscopy, have been revolutionized by Fourier methods. The fast Fourier transform (FFT) is an efficient algorithm to compute the discrete Fourier transform (DFT) and its inverse. The emerging class of high performance computing architectures, such as GPU, seeks to achieve much higher performance and efficiency by exposing a hierarchy of distinct memories to programmers. However, the complexity of GPU programming poses a significant challenge for programmers. In this paper, based on the Kronecker product form multi-dimensional FFTs, we propose an automatic performance tuning framework for various OpenCL GPUs. Several key techniques of GPU programming on AMD and NVIDIA GPUs are also identified. Our OpenCL FFT library achieves up to 1.5 to 4 times, 1.5 to 40 times and 1.4 times the performance of clAmdFft 1.0 for 1D, 2D and 3D FFT respectively on an AMD GPU, and the overall performance is within 90% of CUFFT 4.0 on two NVIDIA GPUs.
[Instruction sets, seismology, Programming, GPU, OpenCL GPU, Graphics processing unit, OpenCL FFT library, DFT, spectroscopy, FFT, NVIDIA GPU, Kernel, automatic FFT performance tuning, discrete Fourier transforms, GPU programming, Auto-tuning, performance evaluation, Kronecker product, Vectors, astronomy, AMD GPU, graphics processing units, fast Fourier transform, discrete Fourier transform, Memory management, graphics processing unit, multidimensional FFT, OpenCL, medical imaging]
Mapping of BLASTP Algorithm onto GPU Clusters
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Searching protein sequence database is a fundamental and often repeated task in computational biology and bioinformatics. However, the high computational cost and long runtime of many database scanning algorithms on sequential architectures heavily restrict their applications for large-scale protein databases, such as GenBank. The continuing exponential growth of sequence databases and the high rate of newly generated queries further deteriorate the situation and establish a strong requirement for time-efficient scalable database searching algorithms. In this paper, we demonstrate how GPU clusters, powered by the Compute Unified Device Architecture (CUDA), OpenMP, and MPI parallel programming models can be used as an efficient computational platform to accelerate the popular BLASTP algorithm. Compared to GPU-BLAST 1.0-2.2.24, our implementation achieves speedups up to 1.6 on a single GPU and up to 6.6 on the 6 GPUs of a Tesla S1060 quad-GPU computing system. The source code is available at: http://sites.google.com/site/liuweiguohome/mpicuda-blastp.
[computational biology, parallel architectures, MPI, protein sequence database, database management systems, parallel programming, Graphics processing unit, Runtime, Databases, Clustering algorithms, proteins, GPU clusters, Kernel, MPI parallel programming, BLAST, Message systems, message passing, OpenMP, compute unified device architecture, BLASTP algorithm, graphics processing units, CUDA, GPU Cluster, bioinformatics, Arrays]
Hybrid CPU-GPU Solver for Gradient Domain Processing of Massive Images
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Gradient domain processing is a computationally expensive image processing technique. Its use for processing massive images, giga or terapixels in size, can take several hours with serial techniques. To address this challenge, parallel algorithms are being developed to make this class of techniques applicable to the largest images available with running times that are more acceptable to the users. To this end we target the most ubiquitous form of computing power available today, which is small or medium scale clusters of commodity hardware. Such clusters are continuously increasing in scale, not only in the number of nodes, but also in the amount of parallelism available within each node in the form of multicore CPUs and GPUs. In this paper we present a hybrid parallel implementation of gradient domain processing for seamless stitching of gigapixel panoramas that utilizes MPI, threading and a CUDA based GPU component. We demonstrate the performance and scalability of our implementation by presenting results from two GPU clusters processing two large data sets.
[cluster, CUDA based GPU component, Instruction sets, Scalability, parallel architectures, serial techniques, Random access memory, gradient domain processing, MPI, hybrid programming, seamless stitching, Graphics processing unit, Image color analysis, image segmentation, hybrid CPU-GPU solver, Kernel, gradient methods, image processing, parallel algorithms, message passing, gigapixel panoramas, graphics processing units, gradient domain, Tiles, massive images, computationally expensive image processing technique, gpgpu]
Fast Snippet Generation Based on CPU-GPU Hybrid System
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
As an important part of searching result presentation, query-biased document snippet generation has become a popular method of search engines that makes the result list more informative to users. Generating a single snippet is a lightweight task. However, it will be a heavy workload to generate multiple snippets of multiple documents as the search engines need to process large amount of queries per second, and each result list usually contains several snippets. To deal with this heavy workload, we propose a new high-performance snippet generation approach based on CPU-GPU hybrid system. Our main contribution of this paper is to present a parallel processing stream for large-scale snippet generation tasks using GPU. We adopt a sliding document segmentation method in our approach which costs more computing resources but can avoid the common defect that the high relevant fragment may be cut off. The experimental results show that our approach gains a speedup of nearly 6 times in average process time compared with the baseline approach-Highlighter.
[document handling, search engines, large-scale snippet generation, sliding document segmentation, Throughput, Vectors, graphics processing units, parallel processing, Graphics processing unit, Sorting, CPU-GPU hybrid system, query-biased document snippet generation, Search engines, Parallel processing, high-performance snippet generation approach, graphics processing unit, fast snippet generation, parallel processing stream, query-biased snippet generation, Time factors]
A Failure Detection Service for Internet-Based Multi-AS Distributed Systems
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Failure detectors are one of the basic building blocks of fault-tolerant distributed systems. A failure detector is a distributed oracle that provides information about the state of processes of a distributed system. This work presents a failure detector service for Internet-based distributed systems that span multiple autonomous systems. The service is based on monitors which are capable of providing global process state information through a SNMP interface. A monitor executes on each network where processes are monitored. Monitors at different networks communicate across the Internet using Web Services. The system was implemented and evaluated for monitored processes running both at a single LAN and distributed throughout the world in Planet Lab. Experimental results are presented, showing CPU usage, failure detection latency, and mistake rate.
[autonomous systems, Multi-AS Internet Systems, failure detector service, SNMP interface, Process Management, local area networks, system recovery, Failure Detectors, Heart beat, Detectors, Planet Lab, Monitoring, Local area networks, Internet-based distributed systems, global process state information, Computer crashes, Internet-based multiAS distributed systems, Distributed Systems Management, Web services, CPU usage, LAN, fault tolerant computing, fault-tolerant distributed systems, failure detection latency, Biomedical monitoring, failure detection service]
Classifier Grouping to Enhance Data Locality for a Multi-threaded Object Detection Algorithm
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Object detection has become an enabling function for modern smart embedded devices to perform intelligent applications and interact with the environment appropriately and promptly. However, the limited computation resource of embedded devices has become a barrier to execute the computation intensive object detection algorithm. Leveraging the multi-threading scheme on embedded multi-core systems provides an opportunity to boost the performance. However, the memory bottleneck limits the performance scalability. Improving data locality of applications and maximizing the data reuse for on-chip caches have therefore become critical design concerns. This paper comprehensively analyzes the memory behavior and data locality of a multi-threaded object detection algorithm. A novel Classifier-Grouping scheme is proposed to significantly enhance the data reuse for on-chip caches of embedded multicore systems. By executing a multi-threaded object detection algorithm on a cycle-accurate multi-core simulator, the proposed approach can achieve up to 62% better performance when compared with the original parallel program.
[Algorithm design and analysis, multi-core, embedded device, multiprocessing systems, multi-threading, Instruction sets, cache storage, object detection, Classification algorithms, embedded multicore systems, parallel processing, data locality, embedded systems, knowledge based systems, memory behavior, Object detection, multithreaded object detection algorithm, Parallel processing, Feature extraction, Hardware, classifier grouping, on-chip caches]
PQEMU: A Parallel System Emulator Based on QEMU
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
A full system emulator, such as QEMU, can provide a versatile virtual platform for software development. However, most current system simulators do not have sufficient support for multi-processor emulations to effectively utilize the underlying parallelism presented by today's multi-core processors. In this paper, we focus on parallelizing a system emulator and implement a prototype parallel emulator based on the widely used QEMU. Using this parallel QEMU, emulating an ARM11MPCore platform on a quad-core Intel i7 machine with the SPLASH-2 benchmarks, we have achieved 3.8x speedup over the original QEMU design. We have also evaluated and compared the performance impact of two different parallelization strategies, one with minimum sharing among emulated CPU, and one with maximum sharing.
[Sychronization, multiprocessing systems, multiprocessor emulation, Multicore processing, software development, ARM11MPCore platform, quad-core Intel i7 machine, Instruction sets, parallel system emulator, SPLASH-2 benchmark, Emulator, Parallel, parallelization strategy, parallel processing, PQEMU emulator, Engines, Emulation, Multi-core, Parallel processing, QEMU, Hardware, multicore processor]
Set Utilization Based Dynamic Shared Cache Partitioning
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
As the number of processors sharing a cache increases, conflict misses due to interference amongst competing processes have an increasing impact on the individual performance of processes. Cache partitioning is a method of allocating a cache between concurrently executing processes in order to counteract the effects of inter-process conflicts. However, cache partitioning methods commonly divide a shared cache into private partitions dedicated to a single processor, which can lead to underutilized portions of the cache when set accesses are non-uniform. Our proposed method compliments these cache partitioning algorithms by creating an additional shared partition able to be shared amongst all processors. Underutilized areas of the cache are identified by a monitoring circuit and used for the shared partition. Detection of underutilization is based on the number of unique set accesses for a given allocated way. For a 16-way set associative cache, the implementation of our method requires 64 bytes of storage overhead per core in addition to that needed for the method that determines the sizes of the private partitions. For the tested system, our method is able to improve performance over the traditional LRU policy for a number of selected benchmark sets by an average of 1.4% and up to 13.3% for a two core system and an average of 1.4% and up to 7.8% for a four core system, and is able to improve the performance of a conventional cache partitioning method (Utility-Based Cache Partitioning) by an average of 0.1% and up to 0.5% for both a two and four core systems.
[cache partitioning, utility-based cache partitioning, Multicore processing, shared cache, set utilization, cache storage, Partitioning algorithms, Classification algorithms, storage management, Program processors, inter-process conflicts, Benchmark testing, Cost function, dynamic shared cache partitioning, 16-way set associative cache, Monitoring, chip multi-processor]
TrC-MC: Decentralized Software Transactional Memory for Multi-multicore Computers
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
To achieve single-lock atomicity in software transactional memory systems, the commit procedure often goes through a common clock variable. When there are frequent transactional commits, clock sharing becomes inefficient. Tremendous cache contention takes place between the processors and the computing throughput no longer scales with processor count. Therefore, traditional transactional memories are unable to accelerate applications with frequent commits regardless of thread count. While systems with decentralized data structures have better performance on these applications, we argue they are incomplete as they create much more aborts than traditional transactional systems. In this paper we apply two design changes, namely zone partitioning and timestamp extension, to optimize an existing decentralized algorithm. We prove the correctness and evaluate some benchmark programs with frequent transactional commits. We find it as much as several times faster than the state-of-the-art software transactional memory system. We have also reduced the abort rate of the system to an acceptable level.
[multiprocessing systems, TrC-MC, Multicore processing, storage management chips, Instruction sets, clock variable, Programming, decentralized software transactional memory, Algorithm, clocks, cache contention, Multicore, decentralized data structures, clock sharing, benchmark programs, data structures, Arrays, Cache Contention, Clocks, multimulticore computers, Software Transactional Memory]
CU2CL: A CUDA-to-OpenCL Translator for Multi- and Many-Core Architectures
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
The use of graphics processing units (GPUs) in high-performance parallel computing continues to become more prevalent, often as part of a heterogeneous system. For years, CUDA has been the de facto programming environment for nearly all general-purpose GPU (GPGPU) applications. In spite of this, the framework is available only on NVIDIA GPUs, traditionally requiring reimplementation in other frameworks in order to utilize additional multi- or many-core devices. On the other hand, OpenCL provides an open and vendor-neutral programming environment and runtime system. With implementations available for CPUs, GPUs, and other types of accelerators, OpenCL therefore holds the promise of a "write once, run anywhere" ecosystem for heterogeneous computing. Given the many similarities between CUDA and OpenCL, manually porting a CUDA application to OpenCL is typically straightforward, albeit tedious and error-prone. In response to this issue, we created CU2CL, an automated CUDA-to-OpenCL source-to-source translator that possesses a novel design and clever reuse of the Clang compiler framework. Currently, the CU2CL translator covers the primary constructs found in CUDA runtime API, and we have successfully translated many applications from the CUDA SDK and Rodinia benchmark suite. The performance of the automatically translated applications via CU2CL is on par with their manually ported counterparts.
[general-purpose GPU application, application program interfaces, Instruction sets, parallel architectures, Programming, CPU, coprocessors, program compilers, source-to-source translation, Graphics processing unit, heterogeneous computing, Runtime, Clang, NVIDIA GPU, Libraries, vendor-neutral programming environment, compiler, Kernel, parallel computing, multiprocessing systems, CUDA SDK, abstract syntax tree, multicore architecture, many-core architecture, graphics processing units, runtime system, program interpreters, CUDA, de facto programming environment, Memory management, heterogeneous system, automated CUDA-to-OpenCL source-to-source translator, API, OpenCL, Clang compiler framework reusing, programming environments, CU2CL, Rodinia benchmark suite]
PADS: A Pattern-Driven Stencil Compiler-Based Tool for Reuse of Optimizations on GPGPUs
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Stencil computations are core of wide range of scientific and engineering applications. A lot of efforts have been put into improving efficiency of stencil calculations on different platforms, but unfortunately it is not easy to reuse. In this paper we present a PAttern-Driven Stencil compiler-based tool and a simple tuning system to reuse those well optimized methods and codes. We also suggest extensions to OpenMP, depicting high-level data structures in order to facilitate recognition of various stencil computation patterns. The PADS allows programmers to rewrite kernel of stencils or reuse source-to-source translator outputs as optimized stencil template codes with related tuning parameters, In addition, PADS consists of a OpenMP to CUDA translator and code generator using optimized template codes. It also obtains architecture-specific parameters to tune stencils across different GPU platforms. To demonstrate our system flexibility and performance portability, we illustrate four different stencil computations, Laplacian operator with Jacobi iterative method, divergence operator, 3 dimension 25 point stencil and a 2D heat equation using ADI method with periodic boundary conditions. PADS succeeds in generating all these four stencil codes using different optimization strategies and delivers a promising performance improvement.
[iterative methods, pattern matching, heat equation, data structure, divergence operator, program compilers, Optimization, GPGPU, optimisation, multiprocessing system, Libraries, Kernel, optimization reuse, multiprocessing systems, OpenMP, stencil template code, stencil tuning, Generators, graphics processing units, general-purpose graphics processing unit, Tuning, pattern-driven stencil compiler, stencil computation, Laplacian operator, program interpreters, Jacobi iterative method, periodic boundary condition, source-to-source translator, optimization strategy, Pattern matching]
Architecture-Aware Mapping and Optimization on a 1600-Core GPU
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
The graphics processing unit (GPU) continues to make in-roads as a computational accelerator for high-performance computing (HPC). However, despite its increasing popularity, mapping and optimizing GPU code remains a difficult task, it is a multi-dimensional problem that requires deep technical knowledge of GPU architecture. Although substantial literature exists on how to map and optimize GPU performance on the more mature NVIDIA CUDA architecture, the converse is true for OpenCL on an AMD GPU, such as the 1600-core AMD Radeon HD 5870 GPU. Consequently, we present and evaluate architecture-aware mapping and optimizations for the AMD GPU. The most prominent of which include (i) explicit use of registers, (ii) use of vector types, (iii) removal of branches, and (iv) use of image memory for global data. We demonstrate the efficacy of our AMD GPU mapping and optimizations by applying each in isolation as well as in concert to a large-scale, molecular modeling application called GEM. Via these AMD-specific GPU optimizations, our optimized OpenCL implementation on an AMD Radeon HD 5870 delivers more than a four-fold improvement in performance over the basic OpenCL implementation. In addition, it outperforms our optimized CUDA version on an NVIDIA GTX280 by 12%. Overall, we achieve a speedup of 371-fold over a serial but hand-tuned SSE version of our molecular modeling application, and in turn, a 46-fold speedup over an ideal scaling on an 8-core CPU.
[GPU code optimizing, NVIDIA, Instruction sets, parallel architectures, NVIDIA CUDA architecture, Registers, GPU, Optimization, Graphics processing unit, optimisation, optimization, Computer architecture, AMD, architecture aware mapping, Kernel, performance evaluation, Vectors, AMD specific GPU optimizations, graphics processing units, GEM, CUDA, GPU architecture, graphics processing unit, high performance computing, OpenCL, 1600 core AMD Radeon HD 5870 GPU, NVIDIA GTX280]
An Optimal and Flexible TCAM Software Simulation Algorithm
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
This paper presents an optimal and flexible algorithm for matching many search keys against many entries with arbitrary wildcard (i.e., "don't care") bits to simulate TCAM lookups in software. The algorithm is also enhanced by using more memory to match search keys containing arbitrary wildcards. To speed up the lookup operation in the functional TCAM model, the algorithm stores intermediary results in a two-dimensional array based on the TCAM entries. It is proved that the algorithm uses the optimal number of bit wise-AND operations in finding the matches. In a theoretical analysis, the proposed algorithm achieves a speedup factor of more than 16 compared to sequential search using reasonable memory footprints, and the best speedup reaches a factor of 27 in our experiments. Finally, the empirical evaluation of the algorithm identifies a sweet spot in the trade-off between space and time which can be practically configurable in simulation.
[Algorithm design and analysis, modeling and simulation, sequential search, search algorithm, Software algorithms, Buildings, two-dimensional array, bit wise-AND operation, Complexity theory, Indexes, ternary content addressable memory, ternary CAM, memory footprint, Software, flexible TCAM software simulation, Arrays, optimal TCAM software simulation, search problems, content-addressable storage]
Trace Spectral Analysis toward Dynamic Levels of Detail
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
The emergence of Petascale systems has raised new challenges to performance analysis tools. Understanding every single detail of an execution is important to bridge the gap between the theoretical peak and the actual performance achieved. Tracing tools are the best option when it comes to providing detailed information about the application behavior, but not without liabilities. The amount of information that a single execution can generate grows so fast that it easily becomes unmanageable. An effective analysis in such scenarios necessitates the intelligent selection of information. In this paper we present an on-line performance tool based on spectral analysis of signals that automatically identifies the different computing phases of the application as it runs, selects a few representative periods and decides the granularity of the information gathered for these regions. As a result, the execution is completely characterized at different levels of detail, reducing the amount of data collected while maximizing the amount of useful information presented for the analysis.
[Measurement, Radiation detectors, trace spectral analysis, performance evaluation, spectral analysis, Spectral analysis, scalability, petascale systems, Program processors, performance analysis tools, Benchmark testing, Hardware, Tracing, online performance tool, Periodic structures, performance tools]
Simulation-Based Performance Analysis and Tuning for a Two-Level Directly Connected System
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Hardware and software co-design is becoming increasingly important due to complexities in supercomputing architectures. Simulating applications before there is access to the real hardware can assist machine architects in making better design decisions that can optimize application performance. At the same time, the application and runtime can be optimized and tuned beforehand. BigSim is a simulation-based performance prediction framework designed for these purposes. It can be used to perform packet-level network simulations of parallel applications using existing parallel machines. In this paper, we demonstrate the utility of BigSim in analyzing and optimizing parallel application performance for future systems based on the PERCS network. We present simulation studies using benchmarks and real applications expected to run on future supercomputers. Future petascale systems will have more than 100,000 cores, and we present simulations at that scale.
[BigSim, Noise, hardware-software codesign, simulation, collective communication, system noise, digital simulation, parallel machines, supercomputing architectures, Network topology, Three dimensional displays, two level directly connected system, machine architecture, mapping, packet level network simulations, hardware and software codesign, performance evaluation, Routing, Supercomputers, parallel applications, Topology, Tuning, simulation based performance tuning, performance prediction, simulation based performance analysis]
Power-Performance Comparison of Single-Task Driven Many-Cores
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Many-cores, processors with 100s of cores, are becoming increasingly popular in general-purpose computing, yet power is a limiting factor in their performance. In this paper, we compare the power and performance of two design points in the many-core processor domain. The XMT general-purpose processor provides significant runtime advantage on irregular parallel programs (e.g., graph algorithms). This was previously demonstrated and tied to its architecture choices and ease-of-programming. In contrast, current commercial GPUs excel at regular parallel programs that require high processing capability. In this work, we set the power envelope as a constraint and evaluate an envisioned 1024-core XMT processor against an NVIDIA GTX280 GPU considering various scenarios for estimating the power of the XMT chip. Even under worst-case assumptions and scenarios, simulations show that the XMT processor sustains its advantage over the GPU on irregular parallel programs, while not falling significantly behind on regular programs. The total energy spent per benchmark fits a similar pattern. Given that the two architectures target different types of parallelism, a future system can potentially utilize an XMT chip and a GPU chip in complementary roles.
[power envelope, Random access memory, parallelism, many-core, PRAM, GPU, parallel programming, many-core processor design point, Graphics processing unit, power aware computing, Computer architecture, Benchmark testing, power-performance comparison, multiprocessing systems, XMT chip, XMT general-purpose processor, performance evaluation, single-task driven many-core, power and performance comparison, graphics processing units, Temperature measurement, NVIDIA GTX280 GPU, general-purpose computing, graphics processing unit, parallel program, XMT, Clocks]
Is Buffer Cache Still Effective for High Speed PCM (Phase Change Memory) Storage?
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Recently, PCM (phase change memory) emerges as a new storage media and there is a bright prospect that PCM will be used as a storage device in the near future. Since the optimistic access time of PCM is expected to be almost identical to that of DRAM, we can make a question that the traditional buffer cache will be still effective for high speed secondary storage such as PCM. This paper answers it by showing that the buffer cache is still effective in such environments due to the software overhead and the bimodal block reference characteristics. Based on this observation, we present a new buffer cache management scheme appropriately for the system where the speed gap between the cache and storage is small. To this end, we analyze the condition that caching gains and find the characteristics of I/O traces that can be exploited in managing buffer cache for PCM-storage.
[storage media, Buffer storage, bimodal block reference characteristics, Random access memory, Phase Change Memory, buffer cache management scheme, cache storage, high speed PCM, History, Servers, Buffer Cache, software overhead, Phase change materials, storage management, Memory management, high speed secondary storage, phase change memory storage, Hard disks, DRAM chips, DRAM, File Systmes]
StreamMR: An Optimized MapReduce Framework for AMD GPUs
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
MapReduce is a programming model from Google that facilitates parallel processing on a cluster of thousands of commodity computers. The success of MapReduce in cluster environments has motivated several studies of implementing MapReduce on a graphics processing unit (GPU), but generally focusing on the NVIDIA GPU. Our investigation reveals that the design and mapping of the MapReduce framework needs to be revisited for AMD GPUs due to their notable architectural differences from NVIDIA GPUs. For instance, current state-of-the-art MapReduce implementations employ atomic operations to coordinate the execution of different threads. However, atomic operations can implicitly cause inefficient memory access, and in turn, severely impact performance. In this paper, we propose Streamer, an OpenCL MapReduce framework optimized for AMD GPUs. With efficient atomic-free algorithms for output handling and intermediate result shuffling, Stream MR is superior to atomic-based MapReduce designs and can outperform existing atomic-free MapReduce implementations by nearly five-fold on an AMD Radeon HD 5870.
[workstation clusters, OpenCL MapReduce framework, Instruction sets, commodity computers, Programming, atomic-free algorithm, MapCG, parallel processing, Optimization, GPGPU, Graphics processing unit, MapReduce, Mars, programming model, atomics, NVIDIA GPU, StreamMR, Kernel, parallel computing, cluster environments, AMD GPU, graphics processing units, graphics processing unit, OpenCL, High definition video, optimized MapReduce framework]
ANEPROF: Energy Profiling for Android Java Virtual Machine and Applications
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Battery energy is one of the most critical resources in a handheld device. Modern designs for handheld devices thus call for optimized use of system power. To develop power-efficient systems, it is essential to understand how power is consumed throughout the system. A promising approach is to measure the power consumption of the system and then match the measurements with the profiled system events. The latter then provides information about how the system consumes power. However, existing tools mostly profile only at the process level due to problems such as profiling overhead and event synchronization. Modern handheld systems, such as Android, complicate the problem further because of the extra layers of software such as Java runtime environment and libraries. To address the above challenges, this paper presents ANEPROF, Android Energy Profiler - a profiling tool for Android that allows energy profiling down to the function level. The design issues and considerations are discussed and its implementation is described. The performance of the tool is evaluated by comparing with other profiling methods.
[Java runtime environment, Humanoid robots, power consumption measurement, software libraries, energy profiling, power consumption, ANEPROF, Android energy profiler, mobile computing, power aware computing, handheld system, handheld device, battery energy, Android Java virtual machine, Performance measurement, Monitoring, profiling overhead, Java, Power demand, libraries, power-efficient system, Synchronization, virtual machine, profiling, virtual machines, Androids, programming environments, event synchronization, Clocks]
Lonestar: An Energy-Aware Disk Based Long-Term Archival Storage System
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
We present the architecture for an disk based archival storage system and propose a new RAID scheme that is designed for "write once, read sometimes" workloads. By intertwining parity groups into a multi-dimensional RAID and improving the single disk reliability with intra-disk redundancy, the system achieves an elastic fault tolerance that can at least recover from all 3-disk failures. We do not stripe data to multiple disks and store related information to the same disks. Typically, all disks of the RAID are powered off, and for a read request, only a single disk has to be spun up, while write and rebuild processes require multiple disks. We analyzed our RAID scheme and showed that it provides a MTTDL that is orders of magnitudes higher than a set of RAID6 systems with a similar amount of disk drives. We also present and evaluate our prototype and show that it is well suited for archival scenarios.
[disk failure recovery, Energy consumption, elastic fault tolerance, reliability, intra-disk redundancy, energy-aware, Lonestar, storage, Servers, system recovery, power aware computing, Fault tolerant systems, RAID scheme, multidimensional RAID, archival, RAID6 systems, fault tolerance, Redundancy, energy-aware disk based long-term archival storage system, disk based archival storage system, RAID, records management, disk, information retrieval systems, disk drives, Arrays, single disk reliability, disc drives]
A Versatile Nodal Energy Consumption Monitoring Method for Wireless Sensor Network Testbed
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Energy efficiency is a critical criterion in wireless sensor networks (WSN). Given the energy consumption of a node, or even the whole network, is precisely measured. Great improvement can be expected in the WSN system optimization. In this paper, we propose a versatile nodal energy consumption monitoring schema, which precisely measures the energy consumption of each node at any moment. In addition, our schema can be integrated with existing test bed technologies to measure the energy consumption of the overall network. Results show that our method can fulfill various challenges in energy consumption measurement in wireless sensor network. We believe the design and implementation of this monitoring schema is an important move towards accurate and flexible energy efficiency analysis.
[Resistors, Energy consumption, wireless sensor networks, versatile nodal energy consumption monitoring method, wireless sensor network, Wireless sensor networks, Measurement units, wireless sensor network testbed, energy efficiency, energy conservation, testbed, Universal Serial Bus, Energy efficiency, nodal energy consumption monitoring, energy efficiency analysis, energy consumption, Monitoring]
Heterogeneity-Aware Peak Power Management for Accelerator-Based Systems
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Power management has become one of the first-order considerations in high performance computing field. Many recent studies focus on optimizing the performance of a computer system within a given power budget. However, most existing solutions adopt fixed period control mechanism and are transparent to the running applications. Although the application-transparent control mechanism has relatively good portability, it exhibits low efficiency in accelerator-based heterogeneous parallel systems. In typical accelerator-based parallel systems, different processing units have largely different processing speeds and power consumption. Under a given power constraint, how to choose the processor to be slowed down and how to schedule a parallel task onto different processors for the maximum performance are different from those in homogeneous systems and have not been well studied. From the motivating example in this paper, we could find that in order to efficiently harness the heterogeneous parallel processing, one should not only perform dynamic voltage/frequency scaling (DVFS) to meet the power budget, but also tune the parallel task scheduling to adapt to the changes. In this paper, we propose a heterogeneity-aware peak power management, which extends existing application-transparent power controller with an application-aware power controller. Firstly, we theoretically analyze the conditions for the maximum performance given a power budget for heterogeneous systems. Based on this result, we provide a power-constrained parallel task partition algorithm, which coordinates parallel task partition and voltage scaling for heterogeneous processing units to achieve the optimal performance given a system power budget. Finally, we evaluate the proposed method on a typical CPU-GPU heterogeneous system, and validate the superiority of application-aware power controller over the existing method.
[Schedules, heterogeneity-aware peak power management, dynamic voltage frequency scaling, parallel processing, GPU, power consumption, Graphics processing unit, power constraint, application-transparent control mechanism, power aware computing, Benchmark testing, scheduling, Peak Power Management, Kernel, Monitoring, parallel task partition algorithm, fixed period control mechanism, CPU-GPU heterogeneous system, Power demand, application-transparent power controller, heterogeneous processing unit, Partitioning algorithms, Accelerator-based Systems, graphics processing units, power budget, heterogeneous parallel processing, accelerator-based heterogeneous parallel system, graphics processing unit, high performance computing, parallel task scheduling, application-aware power controller, processing speed]
Using a Pheromone Mechanism to Estimate the Size of Unstructured Networks
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Accurately estimating network size is essential in unstructured networks. In previous studies, proposed sampling mechanisms for estimating network sizes assumed the probability that a peer is sampled is proportional to the number of its neighbors. This assumption leads to a sampling bias in favor of peers with many neighbors - something that commonly occurs in power law networks. To reduce this sampling bias, we propose a pheromone mechanism, that calibrates sampling probability by the amount of pheromone. This mechanism can be adapted to existing size-estimation techniques. Our empirical studies show that by adapting the pheromone mechanism, most size-estimation techniques can be significantly improved (in some cases, by more than 100%).
[sampling methods, peer-to-peer computing, Peer to peer computing, Social network services, unstructured networks, Estimation, network size estimation, size-estimation techniques, Environmental factors, Topology, power law networks, pheromone mechanism, Accuracy, Network topology, sampling probability, sampling bias, Unstructured networks, Pheromone mechanism, Size estimation]
Building the Knowledge Base through Bayesian Network for Cognitive Wireless Networks
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Tactical communication networking faces complexity, heterogeneity, and reliability requirements. The emerging research area of cognitive networks offers a potential for dealing with these problems. A key feature of cognitive networks is the knowledge base, which is produced during the process of learning and responsible for the decision making. We propose a cognitive network model integrated with the knowledge base, which is a primary part of cognitive networks. And then we focus on the construction of the knowledge base and the expression form of the knowledge in the model. In this paper, we use the Bayesian Network (BN) to construct the knowledge base, which is a unique tool for creating a representation of the dependence relationships among network protocol parameters. The data structure of the dependence relationships of the BN is translated into the knowledge which is expressed by the probability. In the simulation experiments, we create the BN through the sampling data to construct the knowledge base using the mathematical tool MATLAB and prove the efficiency of our cognitive network model for optimizing network performance in the OPENT simulation platform.
[Knowledge engineering, knowledge base, network protocol parameters, knowledge engineering, tactical communication networking, Protocols, learning process, Matlab mathematical tool, sampling data, network optimization, telecommunication computing, Delay, cognitive radio, telecommunication network reliability, complexity requirement, cognitive wireless networks, Mathematical model, belief networks, protocols, Bayesian network, military computing, heterogeneity requirement, cognitive network model, Knowledge based systems, Decision making, probability, Bayesian methods, reliability requirement, OPENT simulation platform, decision making, military communication, Bayes methods]
Joint Bandwidth-Aware Relay Placement and Routing in Heterogeneous Wireless Networks
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
The relay placement problem has been studied extensively in wireless networks. Existing work typically focuses on connectivity to prolong the network time or to achieve fault-tolerance. In contrast, we tackle the problem with the goal of achieving bandwidth sufficiency when real-time multimedia streams need to be sent to the sink. We consider the critical condition of heterogeneous link capacity and transmission range. Besides, we consider the relay placement and routing jointly because routing decides the path on which a stream traverses; and the bandwidth sufficiency depends on both supply (the link capacity) and demand (which streams use the link given the routing paths). We formulate the problem as a new variant of the Steiner tree problem called the heterogeneous bandwidth Steiner routing problem. Extensive simulations show that our scheme reduces the number of relays by an average of 44% compared to the widely used minimum spanning tree based approximation algorithm for relay placement. We also found that considering heterogeneous range and rate is beneficial in relay placement. Compared to the uniform range and rate placement algorithm, our scheme reduces the number of relays by 25%-39%. Besides, our scheme notably improves the movement efficiency when applied to a real-time multi-robot exploration strategy.
[Steiner trees, radio networks, bandwidth sufficiency, fault-tolerance, minimum spanning tree, relays, heterogeneous wireless network, Relays, Bandwidth, telecommunication network reliability, joint bandwidth-aware relay placement, approximation algorithm, heterogeneous range and rate., multimedia communication, real-time multimedia stream, approximation theory, fault tolerance, bandwidth, trees (mathematics), real-time multirobot exploration strategy, Routing, Vectors, Steiner tree problem, quality of service, Indexes, heterogeneous link capacity, Relay placement, Steiner tree, telecommunication network routing, Tin, heterogeneous bandwidth Steiner routing problem]
A Distributed Routing Protocol and Handover Schemes in Hybrid Vehicular Ad Hoc Networks
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Vehicular Ad Hoc Networks (VANETs) have received considerable attention in recent years. VANETs provide many services and applications such as Internet access Voice over Internet Protocol (VoIP) and information dissemination. Due to dynamic changes in the network topologies, various routing protocols have been studied in the vehicular environments. However, the communications between source and destination vehicles involve many intermediate vehicles, and due to the high mobility of vehicles, these communication links become disconnected. In this paper, we propose a distributed routing protocol in VANETs with the help of roadside units (RSUs). The proposed scheme includes vehicle registration, finding the location of destination vehicle and the handover maintenance. The simulation results show that our proposed protocol is suitable for vehicles communications in VANETs.
[vehicular ad hoc networks, wireless communications, handover maintenance, distributed routing protocol, handover, Receivers, VANET, Routing, Ad hoc networks, mobility management (mobile radio), distributed protocol, vehicle registration, Vehicles, destination vehicle, roadside units, routing protocols, VANETs, Routing protocols, hybrid vehicular ad hoc networks, Mobile computing, handover schemes]
A Bandwidth Aggregation Scheme for Member-Based Cooperative Networking over the Hybrid VANET
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
In this paper, we proposed a K-hop bandwidth aggregation scheme for member-based cooperative video streaming over the hybrid vehicular networks consisting of DSRC ad hoc network and 3G/3.5G cellular network. Each member in the same group is known to each other, e.g., they may be a family or a group of friends driving their sedans and they set out from the same position along the same path to the same destination during the journey. When a member in a vehicle wants to watch video during traveling, he will request video through 3G/3.5G network. However, the limited bandwidth of the 3G/3.5G network is not enough to support higher resolution or higher quality of downloading video. In order to enhance the quality of video playback, the member, who is called requester, in one vehicle can ask members, who are called helpers, of the same group in other vehicles to cooperatively download video through their 3G/3.5G networks and then forward their downloaded video to the requester through the DSRC ad hoc network. In order to select suitable members as helpers to download and forward the video and make video streaming more effectively to the requester over the DSRC Ad hoc vehicular network, how to select suitable helpers to forward the downloaded video hop-by-hop back to the requester needs to be resolved. In this paper, we proposed a K-hop bandwidth aggregation scheme to tackle the aforementioned problem. The performance results show that our proposed scheme can get better quality video than cooperative video streaming schemes without the deliberately designed K-hop bandwidth aggregation mechanism.
[vehicular ad hoc networks, 3G mobile communication, Bandwidth Aggregation, VANET, Cooperative Network, Ad hoc networks, member-based cooperative video streaming, Equations, Vehicles, Wireless communication, dedicated short-range communications, hybrid VANET, DSRC ad hoc vehicular network, Vehicular Network, Bandwidth, Streaming media, hybrid vehicular networks, video playback quality, K-hop bandwidth aggregation scheme, video streaming, member-based cooperative networking, 3G-3.5G cellular network, Mathematical model, video downloading]
A Study of Video Frame Sharing in Sparse Vehicular Networks
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Due to the limited budget and high maintenance cost, it is difficult to deploy a large amount of roadside units (such as 802.11 base stations) to cover all road sections. Through a sparse network infrastructure, vehicles experience long download delay for large-size videos. In this paper, we proposed a Video Frame Sharing Mechanism (VFSM) to improve the performance of download sessions. We also investigate how to prevent selfish nodes in VFSM. Through VFSM, users can download video frames from RSUs (Road Side Units) or share video frames with neighboring vehicles by V2V communications in a fair manner. As a result, the video downloading can be speeded up and the video quality can be substantially improved. The extensive simulation results show that the our proposed protocol is practical.
[vehicular ad hoc networks, MPEG, PSNR, peer-to-peer computing, Peer to peer computing, VANET, Ad hoc networks, Indexes, selfish node, Vehicles, sparse vehicular networks, V2V communications, video frame sharing, roadside units, RSU, Transform coding, Streaming media, file sharing]
Minimizing Ceased Areas with Power Control for Spatial Reuse in IEEE 802.11 Ad Hoc Networks
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
In this paper, based on the concept of ceased areas, the spatial reuse in IEEE 802.11 ad hoc networks is enhanced by transmission power control (TPC). A ceased area is created by a transmitter-receiver pair where all other nodes should cease their transmissions for a while. The size of a ceased area varies with the transmission power. A new TPC scheme that can minimize the induced ceased area is proposed. It is practical without needs to know the transmitter-receiver distance. And its performance in network throughput is superior to previous TPC schemes.
[IEEE 802.11, spatial reuse, IEEE 802.11 Standards, telecommunication control, Radio transmitters, ad hoc network, Receivers, transmission power control, Throughput, network throughput, Ad hoc networks, TPC scheme, ceased area minimization, Media Access Protocol, power control, throughput, ad hoc networks, wireless LAN, IEEE 802.11 ad hoc networks, transmitter-receiver pair]
Efficient VANET Unicast Routing Using Historical and Real-Time Traffic Information
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
In this paper, we propose an intersection graph-based vehicular ad hoc network (VANET) architecture. Using the available electronic MAP and historical traffic statistics from public traffic databases, we create an intersection graph (IG) consisting of all connected road segments, which have shorter average inter-vehicle distances than the wireless transmission range, as its edges and intersections of these road segments as its vertices. We then calculate the least cost routing path in the IG. Hence, the source vehicle leverages the proposed IG and IG bypass routing protocols to greedily forward unicast packets to the destination vehicle via each intermediate intersection on the least cost IG path. Further, we also propose the IG routing path recovery process to handle the broken IG path in real-time. Finally, we execute NS2 simulations to exhibit that the IG and IG bypass routing protocols significantly outperform four well-known VANET ones in terms of the average packet delivery ratio, end-to-end delay and hop count.
[Real time systems, IG bypass routing protocol, Roads, real-time traffic information, Vehicle dynamics, Vehicles, average intervehicle distance, unicast routing protocol, IG routing path recovery process, end-to-end delay, Unicast, hop count, road vehicles, intersection graph-based vehicular ad hoc network architecture, electronic MAP, wireless transmission range, Traffic control, vehicular ad hoc networks, NS2 simulation, historical traffic statistic, average packet delivery ratio, VANET, Routing, road segment, greedily forward unicast packet, VANET unicast routing, source vehicle leverage, routing protocols, historical traffic information, public traffic database, IG-based VANET architecture, historical traffic statistics, telecommunication traffic, least cost routing path, intersection graph]
Gossip-Based Cooperative Caching for Mobile Phone Games in IMANETs
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Cooperative caching is an efficient way to improve the performance of data access in mobile wireless networks, by cache nodes cooperatively caching copies of data items in their limited storage. With more demands on sharing a video or other data, especially for mobile phone games in an Internet-based Mobile Ad Hoc Network, the relations among data items become much more important than before. However, most of the existing works do not consider inherent relations among these data items. In this paper, we present a novel solution, Gossip-based Cooperative Caching (GosCC), to address the cache placement problem, considering the sequential relation among data items. Each mobile node stores the IDs of data items cached locally and the ID of the data item in use into its progress report. Each mobile node also makes use of these progress reports to determine whether a data item should be cached locally. These progress reports are propagated within the network in a gossip-based way. To improve user experience, GosCC aims to provide users with an uninterrupted data access service. Simulation results show that GosCC achieves better performance than the best one of the existing solutions, Benefit-based Data Caching, in terms of average interruption intervals and average interruption times, while sacrificing message cost to a certain degree.
[Cooperative caching, Internet-based mobile ad hoc network, gossip-based, data relation, cache placement, Mobile communication, Mobile handsets, cache storage, mobile wireless networks, cache placement problem, mobile computing, computer games, mobile ad hoc networks, GosCC, Peer to peer computing, video sharing, IMANET, Games, data access, benefit-based data caching, cooperatie caching, Internet, gossip-based cooperative caching, cache nodes, Reliability, mobile phone games, Mobile computing]
MR-DBSCAN: An Efficient Parallel Density-Based Clustering Algorithm Using MapReduce
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Data clustering is an important data mining technology that plays a crucial role in numerous scientific applications. However, it is challenging due to the size of datasets has been growing rapidly to extra-large scale in the real world. Meanwhile, MapReduce is a desirable parallel programming platform that is widely applied in kinds of data process fields. In this paper, we propose an efficient parallel density-based clustering algorithm and implement it by a 4-stages MapReduce paradigm. Furthermore, we adopt a quick partitioning strategy for large scale non-indexed data. We study the metric of merge among bordering partitions and make optimizations on it. At last, we evaluate our work on real large scale datasets using Hadoop platform. Results reveal that the speedup and scale up of our work are very efficient.
[Algorithm design and analysis, parallel system, MR-DBSCAN, Hadoop platform, Merging, data mining, partitioning strategy, parallel density-based clustering algorithm MapReduce, data clustering, DBSCAN, Spatial databases, Partitioning algorithms, Indexes, large scale nonindexed data, parallel programming, MapReduce, pattern clustering, Clustering algorithms, Silicon]
RDTS: A Reliable Erasure-Coding Based Data Transfer Scheme for Wireless Sensor Networks
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Information redundancy using erasure coding is an efficient way to increase the reliability of data transmission in communication systems. In Wireless Sensor Networks (WSNs), erasure encoding and decoding are performed on the source node and sink node, respectively, and a large amount of redundant data is generated according to the quality of the whole path and transmitted through multiple hops. In this paper, we propose a reliable data transfer scheme, RDTS, where erasure coding is performed in a hop-by-hop manner, which means that each intermediate node is able to perform erasure coding and adaptively calculates the number of redundant packets for the next hop. Usually, only a small amount of redundant data is needed for reliable transmission over a single hop. Therefore, using RDTS, the network load caused by redundant data is significantly reduced and also well balanced, leading to a longer network lifetime. In addition, hop-by-hop coding has also the advantage of low coding overhead. We further reduce the coding time by proposing a partial coding scheme. Our experimental results show that RDTS achieves up to 69.7% less network load and 153.8% longer lifetime, and meanwhile, the coding overhead is reduced by up to 78.1%, compared with a state-of-the-art erasure-coding based approach.
[reliable erasure-coding, wireless sensor networks, Redundancy, Receivers, Encoding, Decoding, encoding, Wireless Sensor Networks, hop-by-hop manner, Information Redundancy, Reed-Solomon codes, Wireless sensor networks, information redundancy, Erasure Coding, data transfer scheme, Reliability, Reed-Solomon Codes]
Data Collection with Multiple Controlled Mobile Nodes in Wireless Sensor Networks
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Data collection is a fundamental and important issue in wireless sensor networks. Recent research has shown that using mobile nodes to collect and carry data in WSNs has many advantages over static multi-hop routing. In this paper, we focus on the problem of reducing the number of mobile nodes require by data collection in WSNs. Each mobile node visits the sensor nodes, that assigned to it, every t seconds to pick up the cached data. The data gathered by mobile nodes far away from the base station are transferred to the base station through mobile nodes closer to the base station. We formally prove that the problem of data collection in WSNs with the minimum mobile nodes is NP hard. In order to reduce the number of mobile nodes, We propose two algorithms. One tries to reduce the number of mobile nodes and optimize their travel paths, and the other one carefully schedules the movement of all mobile nodes to make sure that they can cooperate with each other to transfer all gathered data to the base station. Our simulation results show that our approach can notably reduce the number of required mobile nodes as much as 46%.
[Base stations, mobile radio, wireless sensor networks, wireless sensor network, Mobile communication, Routing, WSNs, cached data, data collection, multiple controlled mobile node, Wireless sensor networks, optimisation, telecommunication equipment, telecommunication network routing, NP hard problem, mobile node, scheduling, static multihop routing, Sensors, base station, Mobile computing, Monitoring, computational complexity]
Rendezvous Enhancement in Arbitrary-Duty-Cycled Wireless Sensor Networks
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Prolonging network lifetime is always a critical issue in wireless sensor networks (WSNs). To reduce energy consumption, utilizing duty cycling, i.e., sensor nodes switching to sleeping mode for most of the time, is a common strategy in WSNs. However, without properly scheduled, sensor nodes may not be able to stay awake simultaneously to communicate with each other. That is, nodes may not have a rendezvous to each other. To solve this problem, in this paper, we propose a new scheduling mechanism, Staggered Scheduling (SS). By adjusting each node's instantaneous duty cycle in a staggered way, the SS scheme can i) ensure each pair of nodes a rendezvous in a limited time span, ii) produce more rendezvous when compared with existing mechanisms, and iii) allow each sensor node to choose its target duty cycle independently. Extensive simulation results verify the superiority and feasibility of the proposed SS scheme.
[Schedules, Energy consumption, Protocols, wireless sensor networks, Receivers, staggered scheduling, Wireless Sensor Networks, Rendezvous Problem, Equations, arbitrary-duty-cycled wireless sensor networks, Wireless sensor networks, rendezvous enhancement, Low Duty Cycle Networks, Mathematical model, protocols]
Dynamic Data Forwarding in Low-Duty-Cycle Sensor Networks
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
In wireless sensor networks (WSNs), asynchronous duty-cycle technique can significantly reduce energy consumption. However, packets in low-duty-cycle networks suffer high end-to-end (E2E) delay. Besides, recent experimental studies have also shown that links in WSNs are highly unreliable and radio irregularity has adverse impact on routing protocols. In this work, we introduce a dynamic data forwarding (DDF) scheme which combines a realistic link model with asynchronous duty cycle. Different from most of other routing protocols, each node in our solution first finds out a set of candidate nodes and then forwards packet to the first waking up node in this set. Our solution can reduce E2E delay, guarantee delivery ratio and improve network lifetime. We evaluate this dynamic data forwarding scheme with extensive simulations and the simulation results demonstrate the efficiency of our solution.
[Schedules, Asynchronous, wireless sensor networks, network lifetime improvement, asynchronous duty-cycle technique, duty cycle, Relays, Delay, end-to-end delay, DDF scheme, telecommunication network reliability, Routing protocols, link model, realistic link model, dynamic data forwarding scheme, Routing, radio irregularity, Wireless sensor networks, low-duty cycle sensor networks, delivery ratio guarantee, WSN, routing protocols, dynamic data forwarding, energy consumption reduction, E2E delay, waking-up node]
A Cooperative Approach to Cache Consistency Maintenance in Wireless Mesh Networks
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Cooperative caching is especially desirable for multi-hop wireless networks to achieve efficient data access. Existing cooperative caching algorithms for wireless networks mostly focus on cache placement. Another key issue, cache consistency maintenance has not been adequately addressed. In this paper, we propose the first cooperative approach to maintain cache consistency for wireless mesh networks. It basically combines push and pull by making use of the hierarchical architecture of mesh networks. More precisely, we propose two techniques introducing cooperation among network nodes in delivering Invalidation Reports (IR) so as to reduce communication cost and tolerate message losses: IR integration buffers and integrates IRs at the gateway nodes and periodically broadcasts them, Cooperative IR re-sending lets the intermediate nodes resend missed IR messages upon request. The most challenging issue in our design is the determination of the optimal IR broadcast period in order to achieve the optimal tradeoff between push and pull. We conduct numerical analysis to get optimal values for different scenarios. Simulation results confirm our analysis well and comparisons with existing approaches show that our approach can save message cost significantly (50%-70%).
[Cooperative caching, cache placement, message losses, Mobile communication, cache storage, IR integration, multihop wireless networks, Servers, cache consistency maintenance, wireless mesh networks, network nodes, communication cost reduction, Wireless networks, telecommunication network reliability, numerical analysis, Broadcasting, cache consistency, invalidation report, wireless mesh network, invalidation report delivery, gateway nodes, Maintenance engineering, optimal IR broadcast period, consistency maintenance, cooperative communication, cooperative caching algorithm, Logic gates, Internet, cooperative IR resending]
Attachment Learning for Multi-channel Allocation in Distributed OFDMA Networks
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Wireless technologies have gained tremendous popularity in recent years, resulting in a dense deployment of wireless devices. Therefore, it is desired to provide multiple concurrent transmissions by dividing a broadband channel into separate sub channels. This fine-grained channel access calls for efficient channel allocation mechanisms, especially in distributed networks. However, most of the current multichannel access methods rely on costy coordination, which significantly degrade their performance. Motivated by this, we propose a cross layer design called Attachment Learning (AT-learning) in distributed OFDMA (Orthogonal Frequency Division Multiple Access) based networks. AT-learning utilizes jamming technique to attach identifier signals on data traffic, where the identifier signals can help mobile stations to learn allocation strategy by themselves. After the learning stage, mobile stations can achieve a TDMA-like performance, where stations can know when exactly to transmit on which channel without further collisions. We conduct comprehensive simulations and the experimental results show that AT-learning can improve the throughput by up to 300% compared with traditional multichannel access method which asks mobile stations to randomly choose channels without learning.
[attachment learning design, wireless device dense deployment, distributed OFDMA networks, Protocols, OFDM, fine-grained channel access, signal identifier, Jamming, data traffic, mobile stations, jamming technique, OFDM modulation, channel allocation, wireless technologies, broadband channel, mobile radio, frequency division multiple access, multichannel access methods, orthogonal frequency division multiple access, Receivers, TDMA-like performance, multichannel allocation, Frequency domain analysis, time division multiple access, AT-learning design, Games, distributed networks, concurrent transmissions, Resource management]
Dynamic Resource Allocation of IEEE 802.16j Networks with Directional Antenna
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
The IEEE 802.16j standard supports multihop relay function for WiMAX network. The relay technique could enhance the signal strength and help the devices to compete the shadow fading effect from buildings. How to allocate resource becomes an important issue in recent research. In this paper, we discuss the resource allocation with directional antenna in IEEE 802.16j networks. In prior study, the frequency is equally divided according to the number of sectors in a cell. However, the prior allocation schemes often result in waste resource or insufficient bandwidth if the user distribution is not even. The proposed dynamic resource allocation (DRA) scheme considers the user distribution and allocates resource blocks from users' bandwidth request. The simulation results show the improvements from the DRA scheme in terms of utilization of frequency bands and duration of transmission time.
[WiMAX network, frequency allocation, Frequency conversion, Relays, resource allocation, directive antennas, transmission time duration, Bandwidth, directional antenna, frequency band utilization, frequency reuse, user distribution, Base stations, IEEE 802.16j networks, dynamic resource allocation, WiMAX, WiMax, 802.16j, relay technique, user bandwidth request, multihop relay function, DRA scheme, signal strength, shadow fading effect, IEEE 802.16 Standards, Resource management]
Optimal Mobility-Aware Handoff in Mobile Environments
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Mobile terminals with multi-radio devices have become increasingly prevalent. This makes it possible for Internet applications being supported by heterogeneous wireless networks while the terminal is on move. As the user is constantly moving, it is highly desirable that the terminal connects to the best network and retains high performance of Internet connection. Handoff can be made within the same type of network or different types of networks. A vertical handoff may interrupt the ongoing activity but may also bring higher performance. Handoff in such a heterogeneous mobile environment faces several challenges, such as user mobility randomness, handoff overhead and optimality requirement. Existing work often focuses only on the current network condition when making handoff decisions, ignoring user mobility patterns. As a result, a handoff decision good for the present may soon become poor when the user moves to another place. Importantly, the current handoff decision may influence the future performance of the user's connection. This paper proposes an optimal handoff algorithm that explicitly exploits user mobility patterns, with which we can effectively predict further locations of a user. This algorithm can produce optimal handoff decisions in the long run. Employing a comprehensive framework for preference customization, the algorithm supports user customization caring for different user preferences. We conduct extensive real trace driven simulation experiments and comparative study shows our algorithm is better than the typical algorithms.
[optimal mobility-aware handoff algorithm, user mobility patterns, Performance gain, Mobility-aware, Mobile communication, heterogeneous mobile environment, preference customization, Entropy, multiradio devices, mobility management (mobile radio), mobile computing, optimisation, entropy, Wireless networks, vertical handoff, Sensors, handoff overhead, optimal handoff decisions, Base stations, mobile terminals, optimality requirement, Internet connection, handoff, user customization, Markov processes, Markov decision process, heterogeneous wireless networks, Internet, user mobility randomness]
Towards Service-Oriented Cognitive Networks over IP Multimedia Subsystems
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Cognitive Radio (CR) is an innovative wireless sensing technology which can improve efficiency of spectrum usage. Based on CR, Cognitive network (CN) further focus on end-to-end communications for the global optimization and policy enforcement. However, there is a lack of global control channel or middleware for cognitive optimization in CN environment. In this paper, we proposed a Cognitive IP Multimedia Subsystem framework called Cog IMS, which consists of benefits from CN and IP multimedia subsystem (IMS). In this framework, the cognitive optimization is modeled as a service over IMS architecture. By adopting the proposed service-oriented framework, the global optimization is simple to achieve under standardized architecture and the computation requirement is reduced from user to core network. Finally, we simulate wireless users who using services to compare Cog IMS with CN and IMS. The simulation results show that the proposed Cog IMS can achieve the optimization of wireless resource allocation with different network situations.
[policy enforcement, radio networks, wireless sensor networks, Cognitive Radio, Multimedia communication, Servers, Cog IMS, Optimization, Multi-Objective Optimization, cognitive optimization, Wireless communication, optimisation, IP multimedia subsystem, cognitive radio, resource allocation, Computer architecture, IP networks, Cognitive Networks, multimedia communication, service-oriented architecture, middleware, cognitive IP multimedia subsystem framework, end-to-end communication, wireless resource allocation optimization, service-oriented cognitive radio network, CN, global optimization, CR, Wireless sensor networks, Next generation networks, global control channel, wireless sensing technology]
Efficient SINR Estimating with Accuracy Control in Large Scale Cognitive Radio Networks
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Recently, the SINR-model has been widely utilized in link scheduling, spectrum allocation and other applications. The SINR model requires the receiving power information of all potential link peers, which is usually assumed to be known as priori or following a uniform propagation model. We have performed experiments to illustrate how real power data could improve the performance of the SINR-based applications with considerable margin. Thus, obtaining the real power data through measurements is promising. However, this method faces many challenges. We propose a pathloss model based solution, including a representative link selection method to cut down the measurement pairs; accuracy control to determine the sample size; and a measurement distribution method to shorten the measurement duration. Our experiments show that our solution significantly improves the SINR-based scheduling's performance.
[Fading, link scheduling, power information, telecommunication control, Estimation, Interference, measurement distribution method, centralized algorithm, pathloss model, large scale cognitive radio networks, Accuracy, Power measurement, cognitive radio, accuracy control, uniform propagation model, SINR estimation, spectrum allocation, scheduling, throughput optimization, Mathematical model, representative link selection method, spectrum assignment, Signal to noise ratio]
A Tabu Based Cache to Improve Latency and Load Balancing on Prefix Trees
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Distributed Hash Tables (DHTs) provide the substrate to build large scale distributed applications over Peer-to-Peer networks. A major limitation of DHTs is that they only support exact-match queries. In order to offer range queries over a DHT it is necessary to build additional indexing structures. Prefix-based indexes, such as Prefix Hash Tree (PHT), are interesting approaches for building distributed indexes on top of DHTs. Nevertheless, the lookup operation of these indexes usually generates a high amount of unnecessary traffic overhead which degrades system performance by increasing response time. In this paper, we propose a novel distributed cache system called Tabu Prefix Table Cache (TPT-C), aiming at improving the performance of the Prefix-trees. We have implemented our solution over PHT, and the results confirm that our searching approach reduces up to a 70% the search latency and traffic overhead.
[distributed Hash tables, load balancing, Peer-to-Peer networks, cache storage, TPT-C, DHT, resource allocation, Distributed Cache, Search methods, PHT, search problems, distributed indexes, latency improvement, lookup operation, Peer to peer computing, trees (mathematics), Data structures, prefix Hash tree, Peer-to-Peer, tabu based cache, Computer science, prefix based indexes, tabu prefix table cache, Load management, Complex Queries, Information Retrieval, Indexing]
Optimal P2P Cache Sizing: A Monetary Cost Perspective on Capacity Design of Caches to Reduce P2P Traffic
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Peer-to-Peer (P2P) systems are generating a large portion of the total Internet traffic and imposing a heavy burden on Internet Services Providers (ISPs). Proxy caching for P2P traffic is an effective means of reducing network usage, thereby reducing operation costs for ISPs. Proxy cache storage design has a significant impact on ISPs. While there are several works on how to optimally design cache locations and capacity allocation to each location given a total capacity, few works tell ISPs what is the optimal total P2P cache storage capacity. In this paper, we propose an analysis method to the problem of optimally determining P2P cache size. An analysis methodology is proposed to determine the optimal cache size by considering the monetary costs of cache storage and bandwidth. Guided by our model, a close-form expression is developed to guide an ISP in the cache capacity design. Numerical evaluation results show that ISPs can achieve significant cost saving by deploying P2P cache and by allocating the cache capacity optimally.
[peer-to-peer computing, Peer to peer computing, cache locations, capacity allocation, Cache storage, cache storage, Internet traffic, Equations, Storage area networks, P2P traffic, optimal P2P cache sizing, peer-to-peer systems, Internet services providers, monetary cost, proxy cache storage design, Bandwidth, proxy cache, Internet, monetary cost perspective, Arrays, Mathematical model, capacity design, peer-to-peer network, cache capacity design]
A New Auction Based Approach to Efficient P2P Live Streaming
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
P2P live media streaming systems have proliferated and become indispensable vehicles for Internet based entertainment applications. However, it is also well known that scalability of such systems is limited by the lack of proper incentive mechanisms. Specifically, it is notoriously hard to efficiently allocate upload bandwidth at each peer so as to maximize overall system performance. In this paper, we propose a new auction based mechanism for optimizing the allocation of upload bandwidth at each peer. One of the distinctive features in our approach is that peers use real"goods" (i.e., their own bandwidth resources) for payments, instead of relying on some fictitious currency. Essentially, peers use a barter mechanism in the payment step in the auction. Simulation results indicate that our proposed auction approach consistently outperforms existing practical approaches (e.g., tit for tat) in terms of average incoming stream rate, average playback delay, and control packets ratio.
[upload bandwidth, fairness, peer-to-peer computing, Peer to peer computing, entertainment, Internet based entertainment application, incentive mechanism, Media, barter, incentive schemes, Servers, Delay, payment step, P2P, auction, Bandwidth, P2P live media streaming system, media streaming, barter mechanism, upload bandwidth allocation, Internet, Resource management, auction based approach]
Scalable and Reliable Live Streaming Service through Coordinating CDN and P2P
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
In order to fully utilize CDN network edge nearby transmission capability and P2P scalable end-to-end transmission capability, while overcoming CDN's limited service capacity and P2P's dynamics, we need to combine the CDN and P2P technologies together. In recent years, some researchers have begun to focus on CDN-P2P-hybrid architecture and ISP-friendly P2P content delivery technology. In this paper, we firstly make an analysis on main problems of CDN-P2P-hybrid technology, and we compare some current existing solutions. And then we propose a novel scalable and reliable live streaming service scheme through coordinating CDN and P2P. In this scheme, different overlay hybrid methods, we design a smoother CDN and P2P overlay hybrid approach. After that we design a new peer node buffer structure for consuming media block from both CDN and P2P sources. And then we propose a novel algorithm for identifying and serving super node. The simulation experiment results show that our CDN-P2P-hybrid based live streaming scheme has better performance and reliability than pure P2P. Finally, we make a conclusion and analyze future research directions.
[peer-to-peer computing, Peer to peer computing, P2P content delivery, CDN network, peer node buffer structure, Media, live streaming service, overlay hybrid method, Servers, CDN, Live Streaming, P2P, CDN-P2P-hybrid Architecture, Bandwidth, VoD Streaming, Tin, Streaming media, media streaming, CDN-P2P-hybrid architecture, Reliability, P2P Streaming, P2P scalable end-to-end transmission capability]
Robust Fault-Tolerant Majority-Based Key-Value Store Supporting Multiple Consistency Levels
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
The wide spread of Web 2.0 applications with rapidly growing amounts of user generated data, such as, wikis, social networks, and media sharing, have posed new challenges on the supporting infrastructure, in particular, on storage systems. In order to meet these challenges, Web 2.0 applications have to tradeoff between the high availability and the consistency of their data. Another important issue is the privacy of user generated data that might be caused by organizations that own and control data centers where user data are stored. We propose a large-scale, robust and fault-tolerant key-value object store that is based on a peer-to-peer network owned and controlled by a community of users. To meet the demands of Web 2.0 applications, the store supports an API consisting of different read and write operations with various data consistency guarantees from which a wide range of web applications would be able to choose the operations according to their data consistency, performance and availability requirements. For evaluation, simulation has been carried out to test the system availability, scalability and fault-tolerance in a dynamic, Internet wide environment.
[application program interfaces, Scalability, fault-tolerance, fault-tolerant key-value object store, Web applications, Fault tolerance, consistency models, Distributed databases, multiple consistency level support, dynamic environment, control data centers, Availability, large-scale key-value object store, peer-to-peer computing, Peer to peer computing, key-value store, social networks, Generators, computer centres, peer-to-peer, Web 2.0 applications, user generated data, majority-based quorum technique, distributed hash table, storage systems, file organisation, media sharing, Data models, fault tolerant computing, API, Internet, wikis, robust fault-tolerant majority-based key-value store, robust key-value object store, peer-to-peer network, data consistency guarantees, Internet wide environment]
Scalable Distributed Processing of Spatial Point Data
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
The increasing availability of cheap location tracking devices is causing a steadily increasing demand for location based services. Such services usually utilize spatial data structures that need to scale with increasing request load. While static data allows for scaling by simple service replication, dynamic data such as moving users requires administration in a single coherent system to provide consistent and up-to-date processing results. In this paper, we propose a distributed system based on a P2P architecture to store and process spatial data, in particular with window- and k-nearest-neighbors queries. Our system is very simple in that it solely manages a range-partitioned linear data space defined by a Hilbert Curve mapping and neither requires explicit hashing, clustering or the maintenance of a dedicated distributed spatial structure at all. Our main focus is on the inherent quad-tree structure of the 2d Hilbert Curve and how it suffices to efficiently evaluate nearest-neighbor queries in a distributed manner. We verify our approach using real-world data from Open Street Map and demonstrate that the throughput of our system scales asymptotically linear with the network size.
[scalable distributed processing, distributed processing, Spatial indexes, query processing, hilbert curve, spatial data structures, static data, Distributed databases, location tracking devices, tree data structures, range partitioned linear data space, nearest neighbor queries, peer-to-peer computing, Peer to peer computing, spatial queries, open street map, Maintenance engineering, P2P architecture, Spatial databases, Partitioning algorithms, window queries, distributed spatial index, p2p, Hilbert curve mapping, quad tree structure, pattern clustering, k-nearest neighbors queries, explicit hashing, single coherent system, quadtrees, clustering, distributed spatial structure, spatial point data]
New RFID Authentication Protocol with DOS-attack Resistance
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Forward security and resistance to denial-of-service (DOS) attack are two critical properties for low-cost Radio Frequency Identification (RFID) systems because low-cost tags are prone to be compromised once they are captured and wireless communications are relatively easier to be manipulated. In 2011, Duc et al. proposed two RFID authentication protocols called O-FRAP+ and O-RAP+ to enhance the forward security and resistance to DOS attacks. Unfortunately, we find that even if only a single tag is compromised in the O-FRAP+ and the O-RAP+, then all the other tags would be vulnerable to DOS attacks. In this paper, we report the weaknesses, analyze the design flaws and propose a new scheme to improve the security. Security analysis and performance evaluation show that the proposed scheme out-performs its counterparts.
[security analysis, Protocols, cryptographic protocols, radiofrequency identification, RFID, Denialof-service attack, Servers, Computer crime, Privacy, DOS-attack resistance, Authentication, RFID authentication protocol, forward security, Radiofrequency identification, denial-of-service attack, authentication]
BlockTapping: An Online Transparent Integrity Checker for Virtual Storage
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
The integrity of virtual storage has become a very important issue in the virtual computing environment (like Xen-based computing platform). Current integrity detection systems have some disadvantages, for example, they cannot protect themselves well or the dependence between the detection results and the target system is high. We refer to the problem as lack of transparency. This paper presents a novel online integrity checker, Block Tapping, which ensures its security benefiting from the isolation property of virtual machine. Block tapping monitors the block-level data streams transparently through block-to-file semantic-translation at the virtual block device layer. Based on the self-described information of virtual storages, Block Tapping detects the file-level malicious behaviors independent of the internal state of the compromised virtual machine. Experiments show that the prototype system successfully captures 13 typical user-mode root kit attacks against virtual storage, and the performance overhead is acceptable.
[block-to-file semantic-translation, block-level data stream, Transparent Detection, Data Integrity, virtual block device layer, virtual storage, virtual computing environment, blocktapping, Virtual machining, data integrity, Virtual Storage, Synchronization, Xen-based computing platform, Databases, virtual machine, Semantics, Prototypes, virtual machines, online transparent integrity checker, file organisation, data privacy, Kernel, file-level malicious behavior, Monitoring]
A Trustworthy Computing of ADAPT Principle Guaranteeing Genuine Medical Image
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Cancer is usually treated with surgery and probably with chemotherapy or radiation. A medical malpractice of breast cancer has become an urgent need to clear the mess based on technical, workable strategy. The paper tries to: (i) understand a general diagnosis, (ii) pay attentions on how to improve the right judgments of possible misinterpretation, (iii) focus on breast cancer malpractice combined with digital medical image, (iv) present a trustworthy image based upon ADAPT principle, (v) utilize steganography concepts to assure data confidentiality, integrity, and authenticity. The physician's failure to diagnose medical images can result a patient's death, or cause enormous medical bills. A workable strategy of trustworthy computing in APAPT principle is proposed to improve the information security issue of mammography image, and establish the trustworthy computing of image diagnoses.
[medical image diagnosis, chemotherapy, Forensic Analysis, ADAPT principle guaranteeing genuine medical image, Medical Malpractice, Trustworthy Computing, mammography image, medical image processing, steganography concepts, information security, breast cancer, Breast cancer, Mammography, ADAPT Principle, Hospitals, radiation, trustworthy image, medical malpractice, patients death, cancer, trustworthy computing, image coding, trusted computing, image diagnoses, Medical diagnostic imaging]
A Study on Information Security Management with Personal Data Protection
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
In the process of standardization, whether the announcement of a standard represents a cause or an outcome, it is opportunity of the trend of standardization or achievement. The process of standardization is to understand "why" and "how" to explore the detailed outline of a time flow. From a long-term perspective, a standard is the milestone of the standardization process. On May 26th 2010, with the announcement of the Personal Data Protection Act in Taiwan, information security management (ISM) of the Personal Data Protection Act has received much attention from the public. This study is centered on the working items of standards announced by the International Organization for Standardization (ISO) and the ongoing information security management system (ISMS) standards and standardization in order to propose standards which comply with the ISMS of the Personal Data Protection Act and methods which increase implementation control measures.
[Access control, International Organization for Standardization, information sharing, ISO, Personal Data Protection Act, ISMS standards, ISO standards, standardization, personally identifiable information, ISMS, IEC standards, security of data, information security management system, Information security, Taiwan, Information exchange]
A System Call Analysis Method with MapReduce for Malware Detection
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
System calls have long been used to profile a program as a malware. As previous system call based malware detection approaches are often process-oriented, which determines a process as a malware only by its invoking system calls, they often miss the module-based malware such as DLL-based malware and the co-working malware that splits itself into several programs and co-works to complete their functions. To deal with this problem, the system calls should be collected and analyzed as richly as before. However, analyzing rich system calls will cause a significant performance impact on the clients. Fortunately, with the evolution of distributable computing techniques such as MapReduce, we can overcome this tradeoff by analyzing the system calls for malware detection on the servers and then reduce the performance impact on the clients. In this paper, we revise the previous malware persistent model to cover the module-based and co-working malware. We also propose a MapReduce-based system call analysis method to realize the new model. This method is implemented on a Hadoop platform and uses 50 read-world malware for effective and efficient tests. The experimental results show that the detection rate can improve by 28% and performance can improve by more than 30% in comparison to previous research.
[Computers, invasive software, module-based malware, Hadoop platform, behavior-based, distributed processing, malware detection, Servers, Sparse matrices, distributable computing technique, MapReduce, malware persistent model, mapreduce, systems analysis, system call analysis method, Software, system calls, Spyware, co-working malware, Monitoring]
Hybrid Provable Data Possession at Untrusted Stores in Cloud Computing
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
In recent years, cloud computing has gradually become the mainstream of Internet services. When cloud computing environments become more perfect, the business and user will be an enormous amount of data stored in the remote cloud storage devices, hoping to achieve random access, data collection, reduce costs, facilitate the sharing of other services. However, when the data is stored in the cloud storage device, a long time, enterprises and users inevitably will have security concerns, fearing that the information is actually stored in the cloud is still in the storage device or too long without access to, has long been the cloud server removed or destroyed, resulting in businesses and users in the future can't access or restore the data files. Therefore, this scheme goal to research and design for data storage cloud computing environments that are proved. Stored in the cloud for data storage, research and develop a security and efficient storage of proof protocol, also can delegate or authorize others to public verifiability whether the data actually stored in the cloud storage devices.
[Internet service, archival storage, Cloud computing, Protocols, cloud server, data security, Memory, hybrid cryptosystem, information security, Encryption, storage, Servers, storage management, data storage, security of data, public verifiability, hybrid provable data possession, remote cloud storage device, proof protocol, provable data possession, cloud computing, PDP]
A Modified Hopfield Neural Network for Diagnosing Comparison-Based Multiprocessor Systems Using Partial Syndromes
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
A modified Hop field neural network is introduced to solve the comparison-based system-level fault diagnosis problem when only partial syndromes are available. We use the generalized comparison model, where a set of tasks is assigned to pairs of nodes and their outcomes are compared by neighboring nodes. To identify the set of permanently faulty nodes, the collections of all agreements and disagreements, i.e., the comparison outcomes, are used. First, we show that the new diagnosis approach works correctly when t-diagnosable systems are considered. Then, we show the main contribution of this new diagnosis approach which is its capability of correctly identifying the set of faulty nodes when not all the comparison outcomes are available to the diagnosis algorithm at the beginning of the diagnosis phase, i.e., partial syndromes. The simulation results indicate that the modified Hop field neural network-based fault identification algorithm provides an effective solution to the system-level fault diagnosis problem even when partial syndromes are available.
[Backpropagation, Hopfield neural nets, multiprocessing systems, Neurons, modified Hopfield neural network, comparison-based system-level fault diagnosis problem, diagnosable systems, Biological neural networks, Equations, Fault diagnosis, neighboring nodes, Fault tolerance, neural network-based fault identification algorithm, systems analysis, comparison-based multiprocessor systems diagnosis, Hopfield neural networks, diagnosis algorithm, Comparison-based system-level diagnosis, partial syndromes, Silicon, fault tolerant computing, Mathematical model, Partial syndromes, faulty nodes]
Secure Communication Scheme of VANET with Privacy Preserving
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
This paper proposes a secure communication and privacy preserving scheme of Vehicular ad-hoc network (VANET). VANET improves road safety and traffic conditions via the vehicle exchange the traffic information with other vehicles and some infrastructures immediately. Ensuring that exchange messages are secure, trustworthy and protect user privacy are important issues. The transmission message must be well protected to ensure the integrity, confidentiality, anonymity and unlink ability. The transmission message must also satisfy trace ability and non-repudiation to guarantee the trusted third party can reveal misbehaving users. This proposed scheme is an efficient self-generated pseudonym mechanism based on Identity-Based Encryption (IBE) to provide privacy preservation. It also provides integrity, confidentiality, anonymity, trace ability and non-repudiation for VANET communications.
[telecommunication security, privacy preservation, Network Security, misbehaving user, Servers, road safety, vehicle exchange message, IBE, Vehicles, Authorization, Privacy, privacy preserving scheme, road traffic information, road vehicles, Broadcasting, Privacy Preserving, secure communication scheme, user privacy protection, vehicular ad hoc networks, road traffic, transmission message, Receivers, VANET, cryptography, vehicular ad-hoc network, identity-based encryption, data privacy, self-generated pseudonym mechanism, telecommunication traffic]
Detecting Chaff Perturbation on Stepping-Stone Connection
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Cyber criminals often use a sequence of intermediate "stepping-stone" hosts to attack a target machine in order to maintain anonymity. This type of attack of using a connection chain is called stepping-stone attack. Most existing algorithms to detect such attack is to use timing-based correlation on the connections. However, these timing-based approaches are vulnerable if the intruders add chaff packets to evade the detection. The stepping-stone detection rate decreases as the chaff rate increases. We developed a novel anomaly detection algorithm to detect the presence of chaff in a connection by monitoring the packet inter-arrival times. Our study shows the probability distribution of the inter-arrival time of a chaffed connection differs from that of one without chaff. Our experiments show the detection rate as a function of the chaff rate under a variety of complex circumstances. The new algorithm complements the existing correlation-based stepping-stone detection algorithms in providing a more robust solution to stepping-stone detection.
[stepping-stone detection, Correlation, Classification algorithms, statistical distributions, distributed computing, packet inter-arrival time model, probability distribution, chaff evasion technique, chaff packet, Mathematical model, Testing, stepping-stone attack, anomaly detection algorithm, network security, chaff perturbation detection, connection chain, cyber criminals, security of data, stepping-stone host, Stepping-stone intrusion detection, Feature extraction, Data models, packet inter-arrival time, Detection algorithms, timing-based correlation]
Security Analysis of an eSeal Used in Taiwan Customs Officials
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Customs officials have introduced the eSeal with EPC Gen 2 chips to prevent smuggling, and replace manual escorts with an automated system to improve security and efficiency. This paper, however, showed some vulnerabilities of the eSeal system. The pointed out vulnerabilities can be applied in some possible forgery attack scenarios.
[security analysis, Supply chains, ISO standards, Transportation, Containers, smuggling, RFID, Security, Taiwan customs officials, forgery attack, C-TPAT, ISO 17712, security, EPCglobal C1 Gen2, eSeal, Seals, automated system, EPC Gen 2 chips, Radiofrequency identification, tariffs]
Robust e-Voting Composition
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
This paper is concerned with the presentation of a perspective on robustness in e-voting systems. It is argued that the effective design of an e-voting system and its viability can be enhanced by a two-pronged approach to robustness. First, it requires a clear distinction between two forms of robustness: robustness at protocol level and robustness at system level. Second, selected technologies should be integrated into an appropriate architecture in order to address robustness simultaneously at the two levels. The proposed approach is illustrated by the design and implementation of an e-voting system based on the FOO92 protocol. A service-oriented architecture supported by onion routing forms the basis of the system. It facilitates the distribution of tasks and state, the dynamic path configuration and the just-in-time (JIT) composition of the election system. The system conforms to most e-voting requirements.
[onion routing, Protocols, Servers, election system, system level, Robustness, FOO92 protocol, Web service composition, protocols, Electronic voting, service-oriented architecture, robust e-voting composition, JIT composition, just-in-time composition, Radiation detectors, Nominations and elections, just-in-time, protocol level, dynamic path configuration, JIT approach, Web services, blind signatures, FOO protocol, government data processing, e-voting system]
On Least Idle Slot First Co-scheduling of Update and Control Tasks in Real-Time Sensing and Control Systems
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Typical real-time sensing and control systems consist of a set of update tasks for installing sensor measurements from the operation environment and a set of control tasks to access to these measurements for making control decisions. Although configuring the sensors with higher sampling rates could improve the accuracy of the measurements and control quality in general, scheduling high frequent update jobs may seriously affect the schedulability of the control tasks. Missing or delaying the control tasks may severely degrade the overall control performance of the system. In this paper, instead of using the traditional periodic update model, we adopt the a periodic update model in generating update jobs for maintaining data validity. We propose an adaptive co-scheduling algorithm called Least Idle Slot First (LISF) to schedule the update tasks and control tasks with the purposes to meet the deadlines of the control tasks and maximize the quality of control (QoC) offered by the control tasks. LISF schedules the jobs in the ascending order of the number of available idle slots before their deadlines and defers the release times of update jobs as long as the corresponding data objects are maintained within the required quality. The experiment results show that LISF can effectively improve the system schedulability and the control performance in the real-time sensing and control systems.
[Real time systems, aperiodic update model, Real-time Co-scheduling, Schedules, Real-time Database, Control systems, update tasks, sensor measurements, Databases, Update Generation and data validity, control systems, scheduling, Sensors, control engineering computing, real time sensing, sampling rates, data validity, Cyber-Physical Systems, control tasks, quality of control, Wireless sensor networks, least idle slot first coscheduling, sensors, Data models, data handling]
Charge Scheduling of Electric Vehicles in Highways through Mobile Computing
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Today, charging stations (CSs) for electric vehicles (EVs) are much less popular than gas stations. Therefore, searching and selecting CSs is an important issue for the drivers of EVs. This paper investigates the EV charging problem. We propose two types of CS-selection algorithms. The first type only utilizes local information of an EV. The second type utilizes the global information obtained from mobile computing. Specifically, the EVs interact with a Global CS-selection (GCS) server through the mobile telecommunications network. Our study indicates that by using the global information (specifically the workload status of each CS), the EVs can be effectively charged with short waiting times at the CSs.
[Computational modeling, electric vehicles, global charging station-selection server, charge scheduling, Servers, Noise measurement, power engineering computing, mobile telecommunications network, wireless communication, Road transportation, Flowcharts, mobile computing, road vehicles, electric vehicle, scheduling, highway, Mobile computing, charging station selection, Cascading style sheets]
Automatic Extraction of Pipeline Parallelism for Embedded Software Using Linear Programming
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
The complexity and performance requirements of embedded software are continuously increasing, making Multiprocessor System-on-Chip (MPSoC) architectures more and more important in the domain of embedded and cyber-physical systems. Using multiple cores in a single system reduces problems concerning energy consumption, heat dissipation, and increases performance. Nevertheless, these benefits do not come for free. Porting existing, mostly sequential, applications to MPSoCs requires extracting efficient parallelism to utilize all available cores. Many embedded applications, like network services and multimedia tasks for voice-, image- and video processing, are operating on data streams and thus have a streaming-based structure. Despite the abundance of parallelism in streaming applications, it is a non-trivial task to split and efficiently map sequential applications to MPSoCs. Therefore, we present an algorithm which automatically extracts pipeline parallelism from sequential ANSI-C applications. The presented tool employs an integer linear programming (ILP) based approach enriched with an adequate cost model to automatically control the granularity of the parallelization. By applying our tool to real-life applications, it can be shown that our approach is able to speed up applications by a factor of up to 3.9x on a four-core MPSoC architecture, compared to a sequential execution.
[integer linear programming, Pipelines, data streams, embedded software performance requirements, linear programming, pipeline parallelism automatic extraction, video processing, Embedded Software, Pipeline Parallelism, embedded software complexity, sequential ANSI-C applications, embedded systems, voice processing, Mathematical model, energy consumption, heat dissipation, streaming-based structure, image processing, MPSoC, network services, ILP based approach, Program Dependence Graph, MPSoC architectures, Cyber-Physical Systems, Indexes, Equations, multiple cores, Pipeline processing, cyber-physical systems, multimedia tasks, multiprocessor system-on-chip architectures, Automatic Parallelization, Streaming media, streaming applications, pipeline processing, system-on-chip]
Distributed Multi-agent Schemes for Predictable QoS on Heterogenous Wireless Networks
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
In realizing cyber-physical systems, research on developing network supported with predictable Quality of Service (QoS) is extremely important. This study exploits heterogeneous networking as communication medium of cyber physical application using a distributed multi-agent scheme (DMAS). The DMAS is based on the concept of cooperation and the awareness algorithm. The proposed scheme is developed for supporting QoS management which consists of a collection of problem-solving agents with three modules: the knowledge source, the in-cloud blackboard system, and the control engine built into the scheme. A set of problem-solving agents autonomously process local tasks and cooperatively interoperate via an in-cloud blackboard system to guarantee QoS. An awareness algorithm, called the Q-learning algorithm, calculates the exceptive rewards of a handoff to all access networks. These rewards are then used by these problem solving agents to determine what to do. Through operations and cooperation among the active agents, a policy is selected and a user-accepted schedule that meets the specified QoS is generated. In addition, to realize actual results, an integrated test-bed emulation network based on NetFPGA-OpenFlow platform is purposed in this paper.
[Heterogeneous Wireless Networks, Wireless LAN, NetFPGA-OpenFlow platform, multi-agent systems, NetFPGA, predictable QoS, 3G mobile communication, Quality of service, WiMAX, quality of service, Cooperative Networking, Quality of Service, cooperative communication, Q-learning Algorithm, Distributed Multi-Agent Scheme, Databases, heterogenous wireless networks, Cost function, distributed multiagent schemes, Problem-solving, IP networks, Q-learning algorithm]
ERWF: Embedded Real-Time Workflow Engine for User-Centric Cyber-Physical Systems
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Many modern embedded systems, such as cyber-physical systems, feature close integration of computation and physical components. Configurability, efficiency, adaptability, reliability, and usability are essential features for such systems. A workflow engine is a software application that manages workflows. It helps developers separate control flows from activities of the system, and thus is able to enhance configurability and development efficiency. This work aims at the design and implementation of a workflow engine for cyber-physical systems so as to configure workflows with less efforts. To meet timing requirements, the engine is designed to schedule activities to meet their timing requirements, and provides admission control so that these requirements of a set of workflows are guaranteed as long as it is admitted. A humanoid robot is used as the test-bed for the workflow engine. We model robot applications as workflows, and show that how the workflow engine provides real time guarantee and enhances its configurability.
[Real time systems, efficiency feature, Instruction sets, admission control, Cognition, reliability feature, Engines, robot, embedded systems, workflow engine, embedded system, Robot sensing systems, humanoid robots, control engineering computing, workflow management software, real time, adaptability feature, workflow, user-centric cyber-physical system, humanoid robot, embedded realtime workflow engine, timing requirement, configurability feature, usability feature, Timing, software application]
Browsing Architecture with Presentation Metadata for the Internet of Things
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
To realize the Internet of Things (IoT), an important step would be to allow things and information about them to be accessible in an easy way and a platform-independent manner. As World Wide Web (WWW, Web) shows explosive growth over the last decade, the web is the most popular and familiar user interface to acquire knowledge about everyday life. In this respect, web browsing through standard protocols like HTTP over TCP could be a promising approach to access things and to collect things' information. In future, IP-based wireless sensor networks (IP-WSNs) are expected to be integrated into the IoT. However, due to the characteristics of IP-WSN such as resource limited and low data-rate, the challenges that we confronted are 1) how to make clients access IP-WSNs as if they use regular web browsers with rich presentation and acceptable response time and 2) how to embed the standard web in such resource-constrained sensor nodes. In this paper, to address these challenges, we introduce a new browsing architecture, which enables the exploration of the IP-WSNs through a conventional web browser and the navigation with rich user interfaces. We also design lightweight web protocols, lwHTTP and lwTCP, with header compression in order for HTTP and TCP to run on the resource-constrained IP-WSNs. To show the feasibility, we implement the proposed architecture in SNAIL (Sensor Networks for an All-IP world), our IP-WSN platform, and we evaluate the performance of proposed browsing architecture.
[Protocols, IP-WSN, wireless sensor networks, World Wide Web, Servers, IoT, Web Browsing, Computer architecture, Web browsing, 6LoWPAN, IP networks, IP-based wireless sensor network, resource-constrained sensor node, SNAIL, presentation metadata, lightweight Web protocol, Wireless Sensor networks, Service oriented architecture, lwHTTP, Browsers, Internet of Things, user interface, browsing architecture, sensor networks for an All-IP world, Wireless sensor networks, transport protocols, online front-ends, Internet, lwTCP]
An Optimal Topology for a Static P2P Live Streaming Network with Limited Resources
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
In this paper we propose a P2P live streaming topology, prove its optimality under common constraints, and match the analytical research with results from a running commercial network. We assume two types of nodes: viewers that consume the entire media, and amplifiers which are non-viewing nodes utilized for their upstream bandwidth. We analytically derive the minimum needed server upload capacity, for any topology, under the following assumptions: the amount of amplifiers and buffer time are limited, dynamics are low, and the total bandwidth required by the viewers exceeds the total upstream bandwidth of all peers. Then, we present a two-level topology and prove that it achieves the minimum possible server upload, up to a small fraction. Finally, the assumptions and derivation are supported by performing several experiments on RayV's real-world commercial system, with varying network parameters. Namely, we show our predictions are valid while varying the viewers to amplifiers ratio, the stream bit-rate, and the country of the peers. These results not only verify the analytical static predictions, but also evaluate the dynamic costs during the `flash crowd', the initial time when peers are joining the system.
[peer-to-peer computing, Peer to peer computing, static P2P live streaming network, flash crowd, telecommunication network topology, amplifiers node, RayV commercial system, Topology, Servers, network topology, Equations, Delay, network resource, server upload capacity, Network topology, USA Councils, viewer node, peer-to-peer network]
Efficient Hybrid Push-Pull Based P2P Media Streaming System
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Peer-to-Peer (P2P) communication is a popular protocol that has significant impacted and also changed the way for files being distributed over the large networks. Variants of P2P protocols are also applied for other media distribution such as audio and video streaming. The P2P protocol is widely adapted by researchers as a method to handle larger group of users. Cool streaming, the first large scale P2P streaming experiment till today, applied a mesh-based streaming system which has slowly evolved from a pure pull system to a hybrid push-pull system. In this paper, we present our proposed push-pull scheduling for P2P streaming that can heuristically select the most optimal frame to be pushed based on the rules that we designed. Our proposed solution incorporates the fast content distribution characteristic of both Push and Pull approaches. For performance evaluation, we compare our scheduling algorithm with the pure pull Cool streaming scheduling and Random push-pull scheduling where both scheduling serve as a benchmark in three main criteria - end-to-end delay, frame miss-ratio and frame redundancy. Simulation results showed that our proposed heuristic push-pull overall outperformed the other scheduling schemes, where our proposed scheduling algorithm demonstrates as a better solution towards reducing mesh delay in P2P streaming.
[Protocols, Push-Pull Scheduling, audio streaming, scheduling algorithm, media distribution, Delay, processor scheduling, end-to-end delay, pure pull cool streaming scheduling, video streaming, peer-to-peer communication, CoolStreaming, peer-to-peer computing, Peer to peer computing, Redundancy, hybrid push-pull based P2P media streaming system, large scale P2P streaming experiment, performance evaluation, random push-pull scheduling, Scheduling, frame redundancy, large networks, Scheduling algorithm, mesh-based streaming system, benchmark, mesh delay, frame miss-ratio, Streaming media, P2P protocols, P2P Streaming, Mesh-based Pull, hybrid push-pull system]
Service Availability for P2P On-Demand Streaming with Dynamic Buffering
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
In general, on-demand video services enable clients to watch videos from beginning to end. As long as clients are able to buffer the initial part of the video they are watching, on-demand service can provide access to the video to the next clients who request to watch it. In our previous research work, we proposed a novel caching scheme for peer-to-peer on-demand streaming, called Dynamic Buffering. The dynamic buffering relies on the feature of Multiple Description Coding to gradually reduce the number of cached descriptions held in a peer's buffers once the buffer is full. In this paper, we proposed three description dropping policies for dynamic buffering, called sequence dropping, m-dropping, and binary dropping. Mathematical formulas of the reduced number of buffer adjustments of descriptions and the reduction of the average number of selectable descriptions for m-dropping and binary dropping by factors of n and m were established in the paper. Experimental results showed that the m-dropping, m=[n/2] generally outperformed m-dropping, m=2 and binary dropping in terms of service availability. Even though the accumulated reduction of buffer adjustments for m-dropping polices was less than that for binary dropping, the average number of selectable descriptions for m-dropping was much greater than that for binary dropping.
[cache storage, description dropping policy, Servers, Relays, dynamic buffering, m-dropping, streaming, binary dropping, video on demand, Bandwidth, video streaming, multimedia communication, Availability, multiple description coding, peer-to-peer computing, Peer to peer computing, P2P on demand streaming, service availability, on-demand video services, caching scheme, Encoding, encoding, peer-to-peer on-demand streaming, peer-to-peer, sequence dropping, Streaming media]
RELookup: Providing Resilient and Efficient Lookup Service for P2P-VoD Streaming
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
For P2P-VoD streaming, an effective lookup algorithm for appropriate data suppliers is required to support the user's operation of random jump on the video. Existing lookup algorithms mainly adopt a centralized, flooding based, or DHT-based method. Facing the highly dynamic Internet environments, the centralized method incurs a single point of failure, the flooding-based method lacks scalability, and the DHT-based method is not resilient. Motivated by these problems, we propose a novel lookup algorithm, named "RELookup\
[Algorithm design and analysis, dynamic Internet environments, P2P (peer-to-peer), flooding-based method, Servers, coordination costs, Accuracy, Heart beat, video on demand, video streaming, super node, VoD (video-on-demand), peer-to-peer computing, RELookup algorithm, play point distance, Peer to peer computing, P2P-VoD streaming, DHT-based method, Indexes, centralized method, trace-driven simulations, data suppliers, lookup algorithm, resilient-efficient lookup service, Streaming media, Internet]
Adaptive and Efficient Peer Selection in Peer-to-Peer Streaming Networks
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Nowadays Peer-to-Peer technologies for content distribution such as file-sharing and streaming media delivery are quite popular with Internet users. For P2P streaming applications, an appropriate peer selection scheme is critical to reduce startup delay and to provide media delivery of high quality. In this work, we propose a novel peer selection scheme AEPS that is both efficient and adaptive to dynamic P2P network. The AEPS combines the advantages of round-trip time based (RTT-based) schemes and available bandwidth based (ABW-based) schemes, to perform efficient and accurate selection. Experiments under three distinct scenarios are realized to quantitatively compare the AEPS with other selection schemes. The experiment results exhibit that the AEPS is more accurate than RTT-based schemes and more efficient than ABW-based schemes, achieving impressive performance in general.
[peer selection, peer-to-peer computing, Peer to peer computing, P2P network, Estimation, peer-to-peer technologies, available bandwidth, Servers, peer-to-peer streaming, round-trip time, Equations, Delay, adaptive peer selection, content distribution, RTT, peer-to-peer streaming networks, ABW, Bandwidth, Streaming media, media streaming, Internet, peer-to-peer network, file sharing, round trip time]
Peer-to-Peer Immersive Voice Communication for Massively Multiplayer Online Games
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
In this paper, we propose a peer-to-peer voice communication scheme, called immersive voice communication (IVC), to provide MMOG players with the immersive experience to hear the voice of neighbor players within the area of interest (AOI). IVC uses a relation model to classify neighbors of a player into the listener and the over hearer, in which the latter consumes much less bandwidth than the former to reduce the traffic load. IVC also adopts the network coordinate system (NCS) to reduce the voice transmission latency and construct an adaptive k-ary tree (AK-tree) to utilize the bandwidth completely. Furthermore, IVC prioritizes voice data and uses voice masking mechanism to reduce the network traffic. As shown by the simulation results, the proposed scheme outperforms other related schemes.
[peer-to-peer computing, Peer to peer computing, Scalability, voice masking mechanism, Servers, network bandwidth, massively multiplayer online games, voice data, peer-to-peer, network coordinate system, computer games, adaptive k-ary tree, network traffic reduction, Bandwidth, Games, Vegetation, peer-to-peer immersive voice communication, voice transmission latency, voice communication, immersive voice communication, Load modeling]
Using P2P Networks to Repair Packet Losses in Digital Video Broadcasting Systems
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Digital multimedia broadcasting services require resiliency against signal fading due to signal encounters with bad weather, terrain obstructions, such as tree, building, etc. There are several solutions that use peer-to-peer network to repair packet losses. But they have some shortcomings such as long lookup latency, control message overhead, flash crowd problem and low chunk availability. In this paper we propose a novel solution that considers load distribution in structured peer-to-peer networks and repairing packet losses in short lookup latency. Our proposed solution doesn't need to modify the codec technology for multimedia stream. Our proposed solution can achieve load evenly distributed, high chunk availability, short lookup latency and low control message overhead by using peer-to-peer network to repair packet losses in digital multimedia broadcasting systems.
[digital multimedia broadcasting service, RELOAD, P2P network, lookup latency, codec technology, token passing, Multimedia communication, flash crowd problem, DVB-T, digital video broadcasting system, digital multimedia broadcasting, computer network reliability, Chord, Message systems, control message overhead, signal fading, peer-to-peer computing, Peer to peer computing, Receivers, Media, terrain obstruction, load distribution, DVB, digital video broadcasting, distributed hash table, Streaming media, packet loss repair, low chunk availability, peer-to-peer network, Digital video broadcasting]
Spatial Queries Processing in Autonomous Mobile System Environment
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
In this paper, authors discuss a scheme for distributed processing of spatial index tree[1], and its application to spatial queries, Shortest Path Search(SPS), Distance Range Query(DRQ) and k-Nearest Neighbor search(kNN) for spatial networks. Spatial index tree is generated from a given road network using Network Voronoi Diagrams, and possesses layered structure. We herein assume autonomous mobile system environment where spatial index tree servers and GPS-equipped mobile devices, such as ANDROID embedded smart phones, communicate with each other to process these spatial queries in a distributed manner. Queries are processed in two separate steps, rough search in servers and fine search in mobile devices, respectively. Structured data of spatial index tree are extracted for each generator region, and stored in servers. Some numerical results are presented for real map data issued by Geographical Survey Institute.
[shortest path search, distance range query, trees (mathematics), computational geometry, Search problems, spatial index tree, Generators, Servers, Spatial indexes, distributed applications, k-nearest neighbor search, query processing, mobile computing, index tree servers, Geographical Survey Institute, autonomous mobile system environment, GPS-equipped mobile devices, spatial queries processing, network Voronoi diagrams, Smart phones, search problems]
Peer Content Groups for Reliable and Transparent Content Access in P2P Networks
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
This work presents a strategy based on the group communication for transparent and robust content access in P2P networks. Instead of accessing a single peer for obtaining the desired content, a user request is received and processed by a group of peers. A Peer Content Group (PCG) guarantees continuous access to content even if members crash or leave the system provided at least one member remains fault-free. Each PCG member is capable of independently serving the request. A PCG is transparent to the user, as the group interface is identical to the interface provided by a single peer. A group member is elected to serve each request. A fault monitoring component allows the detection of member crashes. If the peer that is serving a request crashes, another group member is elected to continue providing the service. Peer election and replacement are transparent to client peers. The PCG and a P2P file sharing application were implemented in the JXTA platform. Experimental results are presented showing the latency of group operations and system components.
[Protocols, Servers, P2P networks, P2P file sharing application, Heart beat, JXTA platform, fault monitoring component, JXTA, robust content access, Group Communication, peer content groups, Java, peer-to-peer computing, Peer to peer computing, Nominations and elections, Computer crashes, group communication, reliable content access, peer election, system components, P2P systems, fault tolerant computing, Heart rate variability, PCG guarantees, transparent content access]
SSNG: A Self-Similar Super-Peer Overlay Construction Scheme for Super Large-Scale P2P Systems
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Unstructured peer-to-peer (P2P) systems with two-layer hierarchy, comprising an upper layer of super-peers and an underlying layer of ordinary peers, are used to improve the performance of large-scale P2P systems. In order to deal with continuous growth of participating peers, a scalable super-peer overlay topology with a lower diameter is essential. However, there is relatively little research conducted on constructing a scalable super-peer overlay topology. In the existing solutions, the number of connections that super-peers need to maintain is in direct proportion to the total number of super-peers which makes the solutions not scalable as well as not practical. Therefore, in this paper, we propose a scalable hierarchical unstructured P2P system which using a self-similar square network graph (SSNG) to construct and maintain the super-peer overlay topology dynamically. Moreover, a forwarding mechanism over SSNG is presented to enable each super-peer to receive just one flooding message. The analytical results show that the proposed SSNG-based overlay is more scalable and efficient than the perfect difference graph (PDG)-based overlay proposed in the literature.
[Algorithm design and analysis, peer-to-peer computing, Peer to peer computing, super large-scale P2P systems, graph theory, topology, forwarding mechanism, scalable super peer overlay topology, flooding message, Topology, Servers, Indexes, self similar square network graph, self-similar, Delay, Network topology, overlay networks, forwarding algorithm, square network graph, selfsimilar super peer overlay construction scheme, unstructured P2P system, super-peer overlay, SSNG]
On the Power Law Property of Latency-Reducing Relays
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Application relays can be used to reduce latency for real-time media communication. In this paper, we utilize the triangle inequality violations (TIV) in delay space to model the relays and analyze their properties in several data sets. We find that both the demand and utility for relays approximate the power law distribution, which inspires us to deduce guidelines to further improve the efficiency of relay discovery and selection algorithms.
[Algorithm design and analysis, peer-to-peer computing, Peer to peer computing, real-time media communication, relay, Relays, Delay, application relay, Guidelines, selection algorithm, power-law, power law distribution, power law property, relay discovery, triangle inequality violation, overlay routing, Internet, multimedia communication, latency-reducing relay, delay space]
SYMA: A Synchronous Multihop Architecture for Wireless Ad Hoc Multiplayer Games
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
In this paper, we propose a synchronous multihop architecture (SYMA) for wireless ad hoc multiplayer games. In SYMA, devices are assumed to have one-hop neighbor device information, as well as timers synchronized with good accuracy. A player, called game initiator, broadcasts an invitation message to invite other players to join a new game. A joining player issues a message along with its neighborhood information, including the expected transmission time (ETT) for each neighbor node, to reply to the invitation so that the initiator can derive the topology of the participating players. The initiator applies Floyd-Warshall shortest path algorithm, taking the topology and ETT values as input to construct the shortest path spanning tree rooted at each player (or node). It then calculates the expected broadcast time (EBT) for each tree, which is useful to estimate the time for the root node of a tree to broadcast a game state to all tree nodes. Finally, the tree with the smallest EBT is selected as the communication tree and its root is designated as the coordinator, which collects time stamped actions of each player, executes the game logic, and then broadcasts the new game state. By exploiting synchronized timers, the game state consistency is guaranteed, and each node can be scheduled to send/receive game messages without causing collisions.
[Schedules, game logic, Floyd-Warshall shortest path algorithm, expected broadcast time, Wireless communication, optimisation, one-hop neighbor device information, computer games, game initiator, shortest path spanning tree, multiplayer, joining player, trees (mathematics), telecommunication network topology, ad hoc, Ad hoc networks, Topology, wireless ad hoc multiplayer games, Indexes, expected transmission time, game message, multihop games, Games, Vegetation, synchronous multihop architecture, ad hoc networks, SYMA architecture, player topology, game state consistency]
How Helpful Can Social Network Friends Be in Peer-to-Peer Video Distribution?
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Today two keywords more and more frequently recur over the Internet: Peer-to-Peer (P2P) and social networks. P2P, in all of its different declinations, represents a widely-adopted approach for content distribution, particularly for video diffusion. In parallel, the proliferation of social networks is an analogously stunning phenomenon, of unprecedented popularity and scope. In the present work we examine a mesh-based P2P overlay, specifically designed for video streaming, and put forth some modifications to the neighborhood creation and chunk scheduling algorithm the platform adopts, with the goal of favoring peers belonging to a social network and granting them better performance. The improvements that such modifications attain are measured in terms of delivery ratio (throughput) and playback delay. We find that it is possible to guarantee a clear service differentiation, so that social network peers experience an improved viewing experience at the expense of ordinary overlay members, and that the scheduling mechanism modifications warrant the more consistent gains, we also show the role that different percentages of peers belonging to the social network have on the considered metrics. We finally suggest that the attained differentiated service level can be leveraged as an incentive to convince peers of the video overlay to join the social network.
[differentiated service level, peer-to-peer computing, playback delay, Peer to peer computing, Social network services, Communities, neighborhood creation algorithm, chunk scheduling algorithm, Online Social Networks, delivery ratio, Peer-to-Peer, Proposals, Delay, Scheduling algorithm, Differentiated Service Level, social network, content distribution, Video Streaming, mesh-based P2P overlay, Streaming media, peer-to-peer video distribution, social networking (online), video streaming, Internet]
Participation and Departure Processes of Nodes in Connection Graph
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Since the departure of users has a large effect on the integrity of shared files in a P2P system, there is a strong need for methods that can mitigate the effects of user departure. In this paper, introducing a graph to represent the file-sharing in a P2P system, we model the participation and departure processes of users with a renewal process to determine the node departure time. The fact that the departure interval of nodes in the introduced connection graph can be expressed in terms of residual lifetimes is used to derive the expected values of node departure time for several lifetime distributions of nodes. It is also shown that the Laplace transform of the number of departure nodes can be expressed in terms of the node lifetime distribution function.
[peer-to-peer computing, nodes participation process, P2P system, Conferences, graph theory, graph connection, node participation and departure, renewal process, Laplace transform, connection graph, file sharing, residual lifetime]
Social Trust and Reputation in Online Social Networks
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Online social networking systems are rapidly becoming popular on the Internet for users to share, organize and locate interesting content. However, these systems have increasingly been employed as ideal platforms to spread spam and irrelevant content, abusing the valuable human attention and service resource. We propose a social reputation model to guide users to browse the desirable content. First, we compute the statistical correlation between different users to distinguish various user interests, then, since a user's friends are usually trustworthy and share the similar interest, we further exploit the inherent friend relationships to perform the reliable social enhancements of vote history extension and efficient reputation estimation. In addition to providing a strong incentive for user cooperation, our model can handle the practical problems of inactive users, unpopular content and Sybil attacks effectively and efficiently. Our evaluation on a large-scale realistic network validates our analysis, and shows that our social reputation model can help users find the desirable content in various scenarios with a precision of around 94%.
[Correlation, Computational modeling, Trust and Reputation, irrelevant content, Estimation, online social networks, unsolicited e-mail, statistical correlation, Online Social Networks, History, YouTube, social trust, social reputation, Databases, interactive programming, social networking (online), Internet, statistical analysis, spam]
Towards a Common Architecture to Interconnect Heterogeneous Overlay Networks
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
This paper presents a novel overlay architecture to allow the design and development of distributed applications based on multiple interconnected overlay networks. Message routing between overlays is achieved via co-located nodes, i.e. nodes that are part of multiple overlay networks at the same time. Co-located nodes, playing the role of distributed gateways, allow a message to reach a wider set of peers while overlay maintenance remains localized to individual overlays of smaller size. To increase robustness, gateway nodes route messages in an unstructured fashion, and can discover each other by analyzing the overlay traffic. The approach is able to work in both "collaborative" scenarios, where overlay protocol messages can be modified to include additional inter-routing information, or non-collaborative ones. This allows for the interaction with existing overlay protocols already deployed.
[distributed gateway, overlay maintenance, heterogeneous overlay networks, co-located nodes, peer-to-peer computing, Peer to peer computing, Merging, Routing, interconnected overlay network, message routing, distributed application, peer-to-peer, inter-routing information, overlay protocol message, overlay interconnection, distributed hash tables, Collaboration, overlay networks, telecommunication network routing, overlay traffic, Logic gates, Routing protocols, protocols]
A Batch Join Scheme for Flash Crowd Reduction in IPTV Systems
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Peer-to-peer (P2P) streaming is an effective and popular approach for large scale television multicasting over Internet. However, it is always a major challenge when thousands of peers join a popular P2P IPTV channel in a short time, so called the problem of flash crowd. When the problem occurs, a large number of users suffer from large latency in joining a channel and even get blocked. In the past, rare studies are focused on the flash crowd issue. In this paper, we identify several key factors that may cause the flash crowd problem: request congestion, stream congestion, control message, and maintenance overhead. Based on our analysis, we present a batch join scheme instead of processing new users one by one. Our batch join process also solves the problem of current join processes that only use existing active peers to serve new users. We generate a virtual sub tee based purely on a batch of new peers which is then connected to an active peer server. Simulation results demonstrate that our batch join scheme significantly mitigate the flash crowd situation. Moreover, both of the peer blocking rate and re-join times are significant reduced.
[Algorithm design and analysis, PPLive, batch join scheme, IPTV, Servers, peer-to-peer streaming, stream congestion, contribution, Bandwidth, IPTV systems, peer-to-peer computing, Peer to peer computing, bandwidth, maintenance overhead, live streaming, request congestion, Topology, large scale television multicasting over Internet, control message, P2P, flash crowd reduction, Streaming media, P2P IPTV channel, Internet, location aware]
Catching Preference Drift with Initiators in Social Network
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
User's preference drift over time gets it difficult to make accurate recommendation. A recommender system ignoring the fact always recommends similar items which were loved previously by users while users' preference have changed and new items appear. It has been proven empirically that traditional algorithms handling the concept drift problem which simply takes time into account is not appropriate, so a model considering both the static and dynamic preference well is the key to catch users' preference drift. We put forward an original model which explores the behavior of influential people, the initiators who initiate trends in social network, to handle this problem. Compared to traditional collaborative filtering approaches and time weighted approaches, empirical study on lastfm dataset has shown that our model improves the accuracy of the recommendation.
[collaborative filtering, preference drift, Social network services, users preference drift, initiator, social network, Accuracy, recommender systems, recommender system, Collaboration, Markov processes, Lead, Prediction algorithms, social networking (online), catching preference drift, Recommender systems]
Wisdom of the Crowd: Incorporating Social Influence in Recommendation Models
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Recommendation systems have received considerable attention recently. However, most research has been focused on improving the performance of collaborative filtering (CF) techniques. Social networks, indispensably, provide us extra information on people's preferences, and should be considered and deployed to improve the quality of recommendations. In this paper, we propose two recommendation models, for individuals and for groups respectively, based on social contagion and social influence network theory. In the recommendation model for individuals, we improve the result of collaborative filtering prediction with social contagion outcome, which simulates the result of information cascade in the decision-making process. In the recommendation model for groups, we apply social influence network theory to take interpersonal influence into account to form a settled pattern of disagreement, and then aggregate opinions of group members. By introducing the concept of susceptibility and interpersonal influence, the settled rating results are flexible, and inclined to members whose ratings are "essential".
[collaborative filtering, Social network services, network theory, Vectors, information cascade, interpersonal influence, Equations, recommendation system, social network, recommender systems, Collaboration, recommendation model, Games, decision making, decision-making process, social contagion, Motion pictures, social sciences computing, Mathematical model, social influence]
Stochastic Load Rebalancing in Distributed Hash Tables
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
With the notion of virtual servers, peers participating in a distributed hash table (DHT) may host different numbers of virtual servers and are enabled to balance their loads in the reallocation of virtual servers. While most decentralized load balance algorithms designed for the DHTs based on virtual servers offer no performance guarantees, we present in this paper a novel distributed load balancing algorithm for DHTs with stochastic performance bounds. In addition to rigorous performance analysis, we compare our proposal with competitive algorithms through computer simulations. The simulation results indicate that our proposal clearly outperforms the previous algorithms.
[Algorithm design and analysis, Peer to peer computing, stochastic performance bounds, cryptography, decentralized load balance algorithms, Probability distribution, stochastic load rebalancing, computer simulations, Servers, Proposals, virtual servers, DHT, resource allocation, Simulation, distributed hash tables, Load management, stochastic processes]
Data Selection for User Topic Model in Twitter-Like Service
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Twitter-like services are now a popular kind of online social networking services, in which user can express themselves, share contents, and follow others they are interested in. User modeling, building a model for user's interests, is a key problem in many social networking applications, such as recommendation, advertisement, etc. This paper focuses on data selection for user modeling in Twitter-like services. That is, we study the problem of how to select useful data to model a user's interests. Using different data, three user modeling methods are proposed and experiments on a real Twitter-like service are conducted to verify the effectiveness of proposed approaches. Experimental results shows that modeling user's interests with what he/she wrote and selectively what he read performs the best among the three methods we proposed.
[data selection, user topic model, user modeling, LDA, Twitter, Vectors, Probability distribution, Twitter-like service, Data selection, user interfaces, User modeling, Online services, user interest, social networking (online), Data models, data handling, online social networking service, Testing]
Internet of Things Architecture Based on Integrated PLC and 3G Communication Networks
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Internet of Things (IoT) applications are developed and implemented through enlightened communication networks. The phenomena is proving a fact that research in IoT architecture becomes important domain in realizing IoT framework. Therefore, in this paper a new IoT architecture is purposed combining two sophisticated communication networks: power line communication (PLC) and 3G networks. This combination is motivated by scalability advantages of (PLC) and 3G networks. Further, the 3G-PLC networks test bed is constructed and analyzed to show its feasibility in supporting IoT applications.
[enlightened communication networks, Internet of Things (IoT), 3G mobile communication, scalability advantages, Mobile communication, integrated PLC, Broadband communication, power line communication, carrier transmission on power lines, Internet of Things architecture, 3G communication networks, 3G Networks, Computer architecture, Internet, Mobile computing, Radiofrequency identification, Power Line Communications (PLC)]
On the Disruptive Potentials in Internet of Things
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Internet of things (IoT for short) has won great concerns in recent years. There will be massive changes that IoT would bring to current markets and industries. Not only some existing technologies need to be modified to support the complexity, but also some disruptive changes would happen to create new values. The potentials of disruptive innovations are analyzed and exampled from the view of technology, service, business, science and society, which features the future of IoT.
[Industries, innovation, Technological innovation, social aspects of automation, Biological system modeling, Humans, disruptive innovation potential, Internet of Things, IoT, Information science, disruptive, integration, innovation management, Internet, Business]
A RESTful Architecture for Integrating Decomposable Delayed Services within the Web of Things
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
The Web of Things research activities consist essentially in developing concepts, tools and systems for creating and operating global networks of devices associated with embedded resources - RFID tags, sensors, actuators and even complex computing facilities, which are accessed by services. In that context, one of the most accepted standardization technique in order to seamlessly integrate this potentially huge set of heterogeneous services consists in RESTifying them. This operation is rather straightforward for "quickly computed" services such as Flickr photo service, Google Maps or Twitter. But what about cleanly integrating delayed, possibly decomposable services, such as a parcel delivery service computing first the partitions of parcels to various tours and then, for each tour, the optimal routing respecting additional constraints? We claim that this question represents an important challenge if one desires to enrich the potentialities of the web of things. In order to best contribute to its solution, this paper first tackles the problematic of RESTifying decomposable delayed services in a generic way. Secondly, it proposes a general architecture and validates it with a case study including a prototypical implementation.
[Google, radiofrequency identification, Web of Things, Routing, Twitter, Browsers, Servers, Internet of Things, decomposable delayed service, RFID tags, standardization technique, Vehicles, WebSocket, heterogeneous service, Google Maps, Web services, sensors, actuators, Computer architecture, RESTful architecture, Internet, Flickr photo service, Decomposable delayed Web-Services]
Adaptive Traffic-Aware Power-Saving Protocol for IEEE 802.11 Ad Hoc Networks
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Due to rapid advances in wireless network technologies, and the maturity of portable communication devices, ubiquitous wireless computing has become increasingly popular. One of the most important issues in ubiquitous wireless computing is the reduction of power consumption of the portable devices. In this study, we propose a traffic-aware power-saving protocol for IEEE 802.11 multi-hop ad hoc networks. In the proposed protocol, the mobile stations determine their sleep-awake patterns based on a quorum approach. In general, the length of the quorum cycle is an important factor in the sleep-awake ratio, and will determine the power consumption ratio. In light of this observation, the proposed protocol dynamically adjusts the length of the quorum cycle by considering the network utilization during its previous quorum cycle. Simulation results show that our proposed protocol can effectively reduce power consumption in mobile Ad Hoc networks.
[Protocols, IEEE 802.11 multihop ad hoc networks, network utilization, Telecommunication traffic, sleep-awake ratio, power consumption ratio, Mobile communication, power-saving, mobile stations, Switching circuits, mobile computing, mobile ad hoc networks, adaptive traffic-aware power-saving protocol, traffic-awareness, protocols, ubiquitous wireless computing, quorum cycle, Power demand, IEEE 802.11 Standards, quorum, quorum approach, Ad hoc networks, portable communication devices, power consumption reduction, sleep-awake patterns, wireless LAN, ad hoc networks]
Passive Tag for Multi-carrier RFID Systems
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
A passive RFID tag derives its power from RF signal emitted by a RFID reader and responds its modulated backscatter signals to the reader. Because of the large propagation loss, the accessible range of a passive RFID tag is hence limited. In addition, the readability of a passive RFID tag is often influenced by multipath fading problems. In order to mitigate the problems, a multi-carrier UHF passive RFID system utilizes isolated power sources which provide additional power to passive RFID tags via a carrier frequency different with the RFID reader operating frequencies. This approach however affects reader command demodulation circuit of a passive RFID tag. This work proposes a new passive tag demodulation circuit design which is more suitable for multi-carrier RFID systems.
[Charge pumps, radiofrequency identification, modulation, multicarrier UHF passive RFID system, RFID, Multi-carrier, Passive RFID tags, multicarrier RFID systems, Radio frequency, passive tag, multipath fading, Filter, Receiving antennas, passive RFID tag, Tag, Demodulation, Backscatter]
A Study of Comfort Measuring System Using Taxi Trajectories
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
The comfort of rides has been identified as one of the top criteria that affects passengers' satisfaction with public transportation systems. Conventional comfort measurement approaches rely on professional measure tools or interviews from passengers, which are costly, time-consuming, and not yet feasible. The concept of Internet of Things (IoT) is a new solution to answer this problem. The idea of IoT is to interconnect state-of-the-art digital products in physical world to provide more powerful applications. Vehicles equipped with GPS devices and wireless access technologies are parts of the IoT elements in traffic networks. We use the GPS data to measure the comfort level of vehicle rides, and provide a detailed comfort statistics as a value added service. Using real data collected from one of the Taipei taxi service providers, we show that over 95% taxi trajectories are viewed as comfortable. In addition, rides without passengers get higher comfort scores than with passengers. We also give the rankings of all taxi drivers according to a number of criteria, such as the comfort score and the number of loads. With the ranking results, we can track back to the trajectories and infer drives' driving behaviors, road conditions, and traffic conditions. We believe that the proposed solution has the potential to provide a representative comfort measurement service for taxi services and additional value-added services for public transportation systems.
[Roads, ride comfort, Comfort Measurement, road condition, GPS, Participatory Sensing, Vehicles, physical world, traffic network, comfort measuring system, professional measure tool, passenger satisfaction, GPS device, Trajectory, wireless access technology, road traffic, IoT element, Taxi Trajectory, Internet of Things, Sun, public transportation system, Global Positioning System, transportation, digital product, traffic condition, taxi trajectories, GPS data, Acceleration, Taipei taxi service provider]
Efficient Identity-Based Key Management for Configurable Hierarchical Cloud Computing Environment
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
The security of cloud computing data centers is an important issue. In recent years, some schemes of encryption and authentication based on hierarchical identity-based key management systems have been developed. However, these schemes did not consider the case when PKG (Private Key Generator) went down. In this paper, we proposed an identity-based key management scheme for configurable hierarchical cloud computing environment. The proposed scheme requires fewer computations on encryption, and authentication, and it also provides efficient key reconstruction in case of PKG failures. As a result, the scheme proposed in this paper can reduce the key reconstructing cost efficiently on cloud computing data center.
[Cloud computing, Identity-based encryption, Computational modeling, Identity-Based Authentication, cryptography, configurable hierarchical cloud computing environment, Encryption, Servers, Identity-Based Encryption, hierarchical identity-based key management systems, private key generator, cloud computing data centers, security, Cloud Computing, encryption, Authentication, cloud computing, efficient identity-based key management, authentication]
Simulation of Anti-malware User Support System Using Queuing Network Model
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
With the increasing number of new malware programs, traditional malware detection approaches depending on signature files have been less effective, as it is quite difficult for anti-virus vendors to keep up with the frequent appearance of new malware programs. To address this problem, a system called Anti-malware User Support System, which detects malware files using dynamic analysis and remove them from user PCs, is developed. In this paper, we model this system as a queuing network model. Then, using this model, we evaluate the performance of Anti-malware User Support System against the large scale malware epidemics.
[signature files, invasive software, malware programs, queueing theory, detection, large scale malware epidemics, malware files, malware detection, digital simulation, malwaware, Servers, Analytical models, antimalware user support system simulation, Detectors, antivirus vendors, Malware, Internet, queuing network model, Mathematical model, Monitoring]
The Low-Cost Secure Sessions of Access Control Model for Distributed Applications by Public Personal Smart Cards
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
The secure requirement of modern information systems is increasing significantly nowadays, especially in cloud computing with distributed applications. Among secure mechanisms of an organization, the access control (AC) is a foundation for modern information systems. In order to reach an effective and flexible approach of access control, the Role-based Access Control (RBAC) models are widely deployed in organizations. Comparing with traditional discretionary access control (DAC) and mandatory access control (MAC), the user-role-subject mapping of a RBAC model provides effective permissions assignments for access control of an organization. However, the RBAC sessions could be stretched over multiple distributed servers in cloud computing. The traversed sessions between servers could be modified, monitored and attacked by modern hacker techniques, and become secure leaks in RBAC models. In order to ensure secure sessions in cloud computing, various encryption approaches are used. Among these encryption approaches, the use of digital certificates by asymmetric encryption methods could be an appropriate solution to ensure the security of RBAC sessions. However, the cost of public/private keys management and issuing an appropriate certificate tokens for each member in organizations could be very expensive. The high cost might block the deployment of secure RBAC sessions, and then reduce the secure level of organizations. In order to improve this issue, a low-cost approach of secure sessions for RBAC models is proposed in this paper. The personal smart cards can be used as a certificate tokens in RBAC models to reach effective user authentications. Moreover, each session of RBAC models, including user-role-subject assignments and content-based accesses, can be protected by digital certificates which is generated by user own smart cards. Thus the security of RBAC sessions can be improved significantly. It is worth noting that personal smart cards are issued by public departments, thus the expense of tokens issuing and key management could be minimized. Therefore, the session security of a RBAC model could be ensured with user own smart cards without additional cost.
[Access control, Smart cards, discretionary access control, smart cards, secure requirement, Encryption, user interfaces, Servers, distributed applications, role-based access control, authorisation, information systems, Access Control, cloud computing, user authentications, DAC, Smart Cards, secure sessions, RBAC models, MAC, Authentication, Organizations, hacker techniques, mandatory access control, public personal smart cards, Authentications]
Trust Issues that Create Threats for Cyber Attacks in Cloud Computing
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
The research contribution in this paper is twofold. First, an investigative survey on cloud computing is conducted with the main focus on gaps that is slowing down cloud adoption as well as reviewing the threat remediation challenges. Next, some thoughts are constructed on novel approaches to address some of the widely discussed attack types using machine learning techniques. Such thoughts captured through a series of experiments are expected to give researchers, cloud providers and their customers' additional insight and tools to proactively protect themselves from known or perhaps even unknown security issues that follow the same patterns.
[trust, Cloud computing, Security, threat remediation, machine learning, Support vector machines, Accuracy, security of data, Machine learning, threats, Polynomials, trust Issue, cyber attack, cloud computing, learning (artificial intelligence), Kernel, trusted computing, security issue]
Reactor Containment Dependability Analysis in Safety Critical Nuclear Power Plants: Design, Implementation and Experience
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
The use of nuclear energy to generate electric power is crucial in meeting the high energy demand of modern economy. The dependability analysis of nuclear power plants has been a critical issue and the reactor containment is the most important safety structure acting as a barrier against the release of radioactive material to the environment. In this paper, we analyze the dependability of the reactor containment. We also propose a tool for design, implementation, and V&amp;V to enhance the dependability of reactor containment through an integrated leakage rate test. Our practical experiences in the on-site tests are also discussed.
[fission reactor containment, Instruments, leakage rate, ILRT, electrical faults, nuclear power stations, Inductors, fission reactor design, on-site test, electric power, Temperature sensors, VLRT, containment, reactor containment dependability analysis, integrated leakage rate test, Atmosphere, V, radioactive material, safety structure, Software, Safety, V&#x0026;amp, safety critical nuclear power plant, Power generation]
Malware Virtualization-Resistant Behavior Detection
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Many researchers monitor malicious software (malware) behavior using Virtual Machines (VM) to protect the underlying operating system. For virtual machines, the malware monitor process exists at the same layer as the real system so the monitor can get detailed behavior information without being discovered. There are some Anti-VM techniques employed by malware authors to ward off collection, analysis and reverse engineering of their malicious programs. Therefore, malware researchers may obtain inaccurate analysis from VM aware programs. This paper presents a solution to detect Anti-VM techniques. We collect behavioral information from malware and use an enhanced behavior distance algorithm to calculate the difference between real and virtual environments to distinguish if the malware has Anti-VM capability. Our experiments show this algorithm works well. This idea can improve malware analysis results and reduce malware misdetection.
[Algorithm design and analysis, invasive software, operating system, Virtual environments, reverse engineering, Virtual machining, virtualisation, malicious programs, malicious software, malware analysis, behavior distance algorithm, malware misdetection, virtual machines, malware virtualization-resistant behavior detection, Malware, Software, Biomedical monitoring, Monitoring]
A Revised Ant Colony Optimization Scheme for Discovering Attack Paths of Botnet
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
IP trace back technique is an effective method to find either the attack origin or command-and-control (C&amp;C) server on the Internet. The traditional ACO (ant colony optimization) constantly converged to a local minimum solution easily such that the global most portable of the final solution might be missed. Accordingly, the present study proposes a modified ACS (ant colony system) scheme designated as ACS-IPTBK to solve the IP trace back problem, predict both the most probable attack path and the computational resources needed in botnets. The ability of the ants to search all feasible attack paths is enhanced by means of a global heuristics. A series of ns2 simulations are performed to investigate the minimum resources required to successfully reconstruct the attack path. The convergence time for attack paths of different routing distances were investigated using a random graph generator based on Waxman's scheme. Overall, the results confirm that the proposed method provides an effective means of reconstructing the path between the attacker and the victim based on the incomplete routing information from the related ISPs.
[graph theory, revised ant colony optimization scheme, command-and-control server, convergence time, Servers, incomplete routing information, IP traceback, routing distance, Convergence, Ant colony system, global heuristics, Network topology, Waxman's scheme, IP networks, ant colony optimisation, search problems, Waxman scheme, Attack path, ns2 simulations, random graph generator, Botnet, ISP, Routing, path reconstruction, Generators, Topology, software agents, computer network security, attack paths discovery, telecommunication network routing, IP trace back technique, Internet, botnet]
Secure Mechanism for Mobile Web Browsing
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
The prevailing of mobile network provides increasing web applications and services available on mobile devices. However, most of the websites are developed with limited security consideration, let alone web services for mobile users. Hackers take advantage of the web-based vulnerabilities to inject malicious JavaScript into compromised web pages, and a mobile victim visits the site will be compromised. Current anti-virus software for mobile devices is not adequate to protect mobile users from malicious web pages. Based on our observation, mobile malicious web pages have unusual behaviors for evading detection which makes them different form normal ones. Therefore, we propose a client-side malicious web page detection method based on anomaly behaviors observed from web page analysis. Based on the experimental results, our method can identify malicious web page efficiently and warn the website visitors before browsing.
[Java, antivirus software, Mobile communication, Mobile handsets, Encoding, Security, mobile victim, anomaly behavior, Malicious web page, mobile computing, security of data, Web services, mobile Web browsing, Semantics, Web pages, online front-ends, JavaScript, secure mechanism, mobile devices, mobile users, Web sites, Mobile computing, mobile network]
Visualization System for Log Analysis with Probabilities of Incorrect Operation
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
As advancement of information society, information leakages grow into a serious problem. It is important for security managers to analysis log-files for finding out cause of leakages promptly. Existing methods of presenting log-files take the method of ordering them in time. It makes easy to understand a flow of operations. However, if a log recording an incorrect operation is included in the back of log-file, finding out it may drop back. To address this problem, this paper presents visualization system for log analysis with probabilities of incorrect operation. Incorrect operations are operations that may cause a security incident. Probabilities of incorrect operation are set up by rate of number of incorrect operations in past log-files. Security analysts set order of priority, and logs are sorted. Also, we introduce Visualize Part to help security analysts understand a flow of operations in spite of not ordering logs in time. We aim to contribute speedy security analyses by combine visualizing log-file with probabilities of incorrect operation. To evaluate our proposal, accuracy and efficiency are measured by user experiment. Our proposal tool was compared with the tool without probabilities of incorrect operation. As the result, in terms of accuracy, there are no significant difference between. However, our proposal demonstrate a 39.5% improved efficiency.
[security managers, Human Interface, Visualization, information society, incorrect operation, Probability, Information Security, Security, Proposals, Servers, visualization system, Sorting, security of data, Data visualization, data visualisation, Data Analysis, system monitoring, Data Visualization, log analysis, information leakages]
Transparent Communications for Applications behind NAT/Firewall over any Transport Protocol
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
The massive deployment of NAT/firewall devices in the Internet has greatly affected its end-to-end connectivity. Several applications, in particular Grid computing systems which span several Autonomous Domains require the communication among hosts behind NAT/firewall. Despite the existence of successful techniques for the establishment of UDP flows between hosts behind NAT/firewall, the same does not hold for TCP. Furthermore, existing techniques must be implemented individually by each application, possibly causing code duplication, or depend on relay servers, making it prone to performance problems. This work proposes a strategy that allows application processes behind NAT/firewall to communicate transparently, on top of any transport protocol. The system works by establishing IPv6-over-UDP tunnels between hosts, in which IPv6 packets are encapsulated within UDP data grams and are sent through a UDP hole punching session. A detailed description of the proposed system, case studies and experimental results are presented.
[Transport protocols, Punching, NAT/firewall traversal, grid computing, Throughput, transparent communication, protocol tunneling, Servers, Relays, NAT/firewall devices, Delay, transport protocol, transport protocols, autonomous domains, end-to-end connectivity, Internet, IPv6-over-UDP tunnels, UDP hole punching]
Characterizing Fine-Grain Parallelism on Modern Multicore Platform
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Since chip multiprocessors have dominated the processor market, developing a parallel programming model with proper trade-off between productivity and efficiency become increasingly important. As a typical fine-grain parallelism model, Intel Threading Building Blocks (TBB) simplifies parallel programming by runtime schedule. Despite its simplicity, it costs non-trivial runtime overhead which may increase as the thread counts increase. In this work, we conduct an experiment on real commodity hardware to evaluate performance scalability of TBB using PARSEC benchmark suite. We first compare TBB with Pthreads to show that TBB applications can achieve comparable performance as Pthreads applications. To find the performance bottleneck of TBB applications, we measure the runtime overhead of TBB focused on 3 basic TBB runtime activities. The result provides valuable implications which can be used to develop scalable runtime libraries and architectural support for alleviating performance bottlenecks.
[multicore platform, runtime overhead, Instruction sets, Scalability, Pthreads, runtime schedule, parallel programming, Runtime, productivity, Intel threading building block, TBB, fine-grain parallelism, Bandwidth, Benchmark testing, Parallel processing, Intel TBB, chip multiprocessor, Hardware, software performance evaluation, scalable runtime libraries, multiprocessing systems, efficiency, PARSEC benchmark, performance scalability evaluation, microprocessor chips, C++ language, real commodity hardware, fine-grain parallelism model, parallel programming model]
A Time-Series Based Precopy Approach for Live Migration of Virtual Machines
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
As an important technique for load balancing in cloud computing, live migration of virtual machines has received increasing attention in recent years. One key issue of live migration of virtual machine is how to quickly and transparently migrate virtual machines to meet Service Level Agreements (SLAs). In this paper, we evaluate traditional precopy approach, and propose an improved time-series based precopy approach for virtual machine migration. With the time-series prediction technique, we identify frequently updated dirty pages (high dirty pages) in the past and future period more precisely, and transmit them in the last round of iteration, in order to reduce unnecessary, repeated transmission of dirty pages. Doing so can significantly reduce the total migration time. Simulation results show that compared with the traditional approach of Xen, our approach can effectively improve the performance of virtual machine migration, including less number of iterations, less down time and migration time, and fewer pages transferred.
[Algorithm design and analysis, precopy approach, Cloud computing, service level agreements, load balancing, time-series, time series, SLA, Virtual machining, virtual machine migration, Equations, time series based precopy, resource allocation, virtual machine, live migration, virtual machines, Arrays, Acceleration, Mathematical model, cloud computing]
Energy-Aware High Performance Computing: A Taxonomy Study
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
To reduce the energy consumption and build a sustainable computer infrastructure now becomes a major goal of the high performance community. A number of research projects have been carried out in the field of energy-aware high performance computing. This paper is devoted to categorize energy-aware computing methods for the high-end computing infrastructures, such as servers, clusters, data centers, and Grids/Clouds. Based on a taxonomy of methods and system scales, this paper reviews the current status of energy-aware HPC research and summarizes open questions and research directions of software architecture for future energy-aware HPC studies.
[high performance community, Power demand, energy aware high performance computing, High Performance Computing, Computational modeling, computer infrastructure, data centers, Green Computing, Servers, software architecture, Energy-aware Computing, Program processors, power aware computing, servers, USA Councils, Green products, taxonomy study, energy consumption, Monitoring, clusters, Grids/Clouds]
Energy-Aware Depth Map Generation for 3D Portrait on Android Systems
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
The most important information in transforming a 2D image into a 3D image is the depth of each pixel in the image. However, a normal 2D image usually does not contain such information, which makes the transformation impossible. On the other hand, for certain types of pictures, such as personal portraits, it is possible to infer crude depth information from their known contexts and properties. Unfortunately, depth map generation is very involving and, if executed on a mobile phone, will consume a lot of energy. This is undesirable, particularly when the mobile device is running out of battery. The application must be aware of the energy status of the system, make appropriate tradeoffs, and then adapt accordingly. This paper presents such an energy-aware, 2D-to-3D image transformation tool for personal portraits on mobile phones. The tool will choose a suitable depth-map generation algorithm based on the remaining energy of the device. We will discuss how to make the tradeoffs and evaluate the idea on real machines.
[image processing, 2D image transformation, image pixel, android systems, Mobile handsets, smart phones, Batteries, energy status, 3D image transformation, mobile device, mobile computing, power aware computing, Image color analysis, personal portraits, 3D image, mobile phone, embedded systems, depth map, Three dimensional displays, energy aware depth map generation, energy-awareness, Neck, Face, 3D portrait]
Towards Providing Cloud Functionalities for Grid Users
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Grid computing uses a job submission model that requires the users to perform a set of interactive operations for executing an application on the Grid. Grid users are therefore burdened with the tasks of understanding the basic concept of Grid computing and the details of job management. Cloud computing, on the other hand, applies a utility model that allows the user to access the underlying platform via Web services. This work brings the Cloud concept to the Grid with a result of replacing the job submission model with a service infrastructure. In this case, Grid applications are presented as Web services that can be executed on the Grid automatically without user interactions. All issues related to job management are performed by the system. The service infrastructure significantly simplifies the users' task in accessing the Grid.
[Cloud computing, Computational modeling, grid computing, Servers, Web services, cloud functionalities, job management, User interface, Production, interactive operations, Grid computing, cloud computing, grid users, Software as a Service, Portals]
A Hybrid Simulation of Large Crowd Evacuation
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
A Grid simulation infrastructure can facilitate a simulation comprising models of different grains and/or models of various types in nature to investigate large and complicated problems. On top of the infrastructure, a simulation of evacuating thousands of pedestrians in a large urban area has been constructed. A number of agent-based and computational models residing at two administrative domains operate together, which successfully presents the dynamics of the complex scenario at scales of both individual and crowd levels. Experimental results indicate that the proposed approach can effectively cope with the size and complexity of a scenario involving a large crowd.
[computational models, pedestrian evacuation, Computational modeling, Roads, Biological system modeling, Crowd Modelling &#x0026;amp, grid computing, Complex Systems, Educational institutions, grid simulation infrastructure, digital simulation, traffic engineering computing, large crowd evacuation, Vehicles, Grid Computing, Simulation, agent-based models, Data models, hybrid simulation, behavioural sciences computing, Assembly]
Task Scheduling of Massive Spatial Data Processing across Distributed Data Centers: What's New?
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Spatial data processing in general involves large-scale distributed computing resources across multiple data centers and distributed massive data sets. Task scheduling of spatial data processing across distributed data centers brings new research issues and calls for novel scheduling mechanisms and frameworks. This paper analyzes the features of spatial data processing, identifies the research issues of task scheduling in massive spatial data processing, and proposes task model &amp; scheduling mechanisms for task scheduling in spatial data processing.
[massive spatial data processing, Massive data processing, Computational modeling, spatial data infrastructure, visual databases, Data processing, Spatial databases, Scheduling, large-scale distributed computing resource, distributed massive data set, computer centres, scheduling mechanism, Processor scheduling, Distributed databases, scheduling framework, scheduling, Data models, task scheduling, distributed data center]
Volunteer Sensing: The New Paradigm of Social Sensing
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Mobile platform has been widely used in social computing and networking thanks to its powerful computing and sensing ability. Various mobile sensing frameworks have been developed to facilitate the human participation in social environment. However, problems exist in current solutions such as how to maintain large number of participants to run long-term real social projects. We propose a new sensing paradigm called Volunteer Sensing to provide a suitable framework, which reduces the participants duty and enlarge the usage of resources.
[Social sensing, Humans, Microcomputers, Mobile communication, Data processing, Mobile handsets, social networking, social sensing, mobile platform, social computing, Distributed computing, mobile computing, Volunteer sensing, volunteer sensing, social networking (online), Sensors, Mobile computing]
The Implementation of Multilayer Virtual Network Management System on NetFPGA
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
With the rapid development of network technology and popularity of virtualization, the importance of developing an adaptive management system for such physical and virtual network environment is obvious increasing. Not simply do the demand systems keep the convenience of virtualization, but conform to the hierarchy of system administration. This article presents the Multilayer Virtual Network Management System, integrated the architecture of network virtualization and the features of multilayer network management. The system is implemented by Net FPGA network development platform and provides a solution of network management for a large scale network. Experiments demonstrate the exact execution of Virtual Network through the policies of network flows and the multilayer management of the proposed system.
[Protocols, NetFPGA, field programmable gate arrays, Virtual Network, Switches, Nonhomogeneous media, adaptive management system, virtualisation, network virtualization, virtual network environment, large scale network, network technology, physical environment, Computer architecture, Bandwidth, virtual machines, multilayer virtual network management system, Monitoring, multilayer network management]
Creating Future Networks: Designing, Implementing and Operating Advanced Experimental Network Research Testbeds
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
For many years, experimental network research test beds have been designed and implemented to explore theoretical concepts at scale, progressing beyond ideas that initially may have been explored through lab simulation and modeling. However, traditionally, such test beds almost always were created for specific, highly defined projects, and they were designed to last for fairly short periods of time. More recently, network research communities have been designing, implementing, and operating new types of large scale experimental network test beds, including those global in scope, which are intended to be long term persistent resources and can support multiple types of experiments. These test beds reflect recent innovations in architecture and in core technologies and also the declining cost curves of basic components, a trend which is reducing the economic barriers to developing test beds. This paper presents several key ideas motivating the creation of next generation test beds and it describes design concepts, core technologies, and operational issues. This paper also describes a number of recent and current large scale test beds that have been created to advance the state of communications, especially those designed to assist in providing a transition to the future Internet.
[Optical fibers, Technological innovation, long term persistent resources, modeling, Optical switches, programmable network, Communities, experimental networks, Optical fiber networks, digital simulation, network research testbeds, experimental control planes, network research communities, advanced experimental network research testbeds, next generation test beds, large scale experimental network test beds, lab simulation, Distributed databases, Computer architecture, future network creation, Internet]
Network Virtualization with Cloud Virtual Switch
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Recently, Cloud Computing is getting considerable attentions not only as the technology trend but also the new business model. Virtualization plays an extremely important role in Cloud Computing. Server virtualization has been achieved using virtualization hypervisor software such as VMware, Xen, and KVM,etc. Virtual Machines (VMs) created by hypervisors share the same physical environment with each other. Network virtualization is made by virtually connecting each virtual machine to a port on the virtual switch, which can be implemented by software or hardware. In this paper, we review both software and hardware network virtualization techniques and present the integration of open-source hypervisor with software virtual switch, which provides cloud users a more elastic and secure network environment.
[Cloud computing, Network virtualization, public domain software, Switches, server virtualization, Virtual Switch, virtualisation, Servers, Network interfaces, cloud virtual switch, virtualization hypervisor software, software virtual switch, cloud computing, KVM software, Virtual machining, network virtualization, Xen software, Bridges, Virtual machine monitors, open-source hypervisor, Open vSwtich, VMware software, virtual machines, Software, OpenNebula]
Improving Speculative Execution Performance with Coworker for Cloud Computing
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
MapReduce is an important programming model for large-scale parallel applications. It divides a job into several parallel tasks and completes the job by sequential phases, i.e. map phase and reduce phase. The job completion time will be delayed when a task, called straggler, consumes more time than others. The main reason that a straggler occurs is the imbalance resource distribution among computing nodes in the cloud. Speculative execution is a solution for dealing with stragglers. Duplicate tasks are launched on other nodes to process the same data as the straggler does. Any completion of these tasks implies that this task is finished and other duplicate tasks can be aborted. However, aborting tasks misspends resources. In this paper, we propose an idea of using coworkers to help a straggler. According to the processing rate of the straggler and the coworker, the amount of data parceled out from the straggler to the coworker should be determined. Different from speculative execution, coworkers finish tasks with stragglers and do not misspend computing resources. Experimental results show that coworkers can reduce the task completion time by 37% and the network traffic by 64% when comparing with speculative execution.
[Computers, Cloud computing, Computational modeling, Programming, Straggler, Virtual machining, task analysis, speculative execution performance, parallel programming, Coworker, MapReduce, large-scale parallel applications, network traffic, Fault tolerance, task completion, Cloud Computing, Speculative execution, programming model, Bandwidth, imbalance resource distribution, cloud computing, telecommunication traffic]
FPGA Design of an Automatic Target Generation Process for Hyperspectral Image Analysis
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Onboard processing of remotely sensed hyper spectral data is a highly desirable goal in many applications. For this purpose, compact reconfigurable hardware modules such as field programmable gate arrays (FPGAs) are widely used. In this paper, we develop a new implementation of an automatic target generation process (ATGP) for hyper spectral images. Our implementation is based on a design methodology that starts from a high-level description in Matlab (or alternative C/C++) and obtains a register transfer level (RTL) description that can be ported to FPGAs. In order to validate our new implementation, we develop a quantitative and comparative study using two different FPGA architectures: Xilinx Virtex-5 and Altera Stratix-III Altera. Experimental results have been obtained in the context of a real application focused on the detection of mineral components over the Cup rite mining district (Nevada), using hyper spectral data collected by NASA's Airborne Visible Infra-Red Imaging Spectrometer (AVIRIS). Our experimental results indicate that the proposed implementation can achieve peak frequency designs above 200MHz in the considered FPGAs, in addition to satisfactory results in terms of target detection accuracy and parallel performance. This represents a step forward towards the design of real-time onboard implementations of hyper spectral image analysis algorithms.
[Algorithm design and analysis, Cup rite mining district, spectra, Hyperspectral image analysis, field programmable gate arrays, field programmable gate arrays (FPGAs), hyperspectral image analysis, object detection, reconfigurable hardware module, field programmable gate array, Xilinx Virtex-5 FPGA, Hardware, Nevada, automatic target generation process, Vectors, Altera Stratix-III FPGA, mineral component detection, FPGA design, mineral processing industry, Image analysis, target detection, register transfer level, airborne visible infra-red imaging spectrometer, Field programmable gate arrays, Hyperspectral imaging, Matlab]
Accelerating the Kalman Filter on a GPU
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
For linear dynamic systems with hidden states, the Kalman filter can estimate the system state and its error covariance considering the uncertainties in transition and observation models. In each iteration of applying the Kalman filter, the two phases of predict and update contain a total of 18 matrix operations which include addition, subtraction, multiplication and inversion. As recent graphic processor units (GPU) have shown to provide high speedup in matrix operations, we implemented a GPU accelerated Kalman filter in this work. For general reference purposes, we tested the filter on typical large-scale over-determined systems with thousands of components in states and measurements. For the various combinations of configurations in our test, the GPU accelerated filter shows a scalable speedup as either the state or the measurement dimension increases. The obtained 2 to 3 orders of magnitude speedup over its single-threaded CPU counterpart shows a promising direction of using the GPU-based Kalman filter in large-scale time-critical applications.
[Symmetric matrices, linear dynamic system, Instruction sets, Noise, Kalman filter, GPU accelerated filter, error covariance, Covariance matrix, graphics processing units, GPU, Graphics processing unit, matrix algebra, CUDA, graphic processor unit, matrix operation, large-scale time-critical application, Kalman filters, Acceleration, parallel computing]
Parallel Processing with MPI for Inter-band Registration in Remote Sensing
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Inter-band registration is an essential processing method that is used to generate satellite imagery products with raw data obtained from a high-resolution satellite. The processing method requires considerable time owing to its high computational cost as well as the large-scale data to be processed. In this paper, we present a parallel inter-band registration method on windows-based clusters as part of an effort to reduce the total execution time during product generation. We use a blade system as clusters and the MPICH2 library for the parallel programming tool. We logically divide roles of the nodes into one root node, three sub-root nodes, and several compute nodes. We divide the inter-band registration into sequential and parallel processing areas to design the nodes suitable for inter-band registration. The root node and sub-root nodes control the sequential processing area and exchange data with compute nodes in the parallel processing area. In addition, we introduce an object-oriented pseudo code for easy parallelization using a Message Passing Interface (MPI). We apply our system on the KOMPSAT-2 product generation process. The experimental result shows that our system reduces the total execution time from 330 seconds to 79 seconds.
[Frequency modulation, application program interfaces, high-resolution satellite, Blades, image registration, inter-band registration, MPI, satellite imagery product generation, parallel processing, parallel programming, software libraries, parallel inter-band registration method, Parallel processing, windows-based clusters, Libraries, parallel programming tool, software tools, image resolution, Image registration, message passing, object-oriented programming, parallel processing areas, object-oriented pseudo code, message passing interface, KOMPSAT-2 product generation process, geophysical image processing, remote sensing, blade system, Satellites, Feature extraction, KOMPSAT-2, HPC, sequential processing areas, MPICH2 library]
Volume Data Numerical Integration and Differentiation Using CUDA
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Earthquake simulations generate large-scale ground-motion velocity wave-field data sets. One way to look at the data is to make it volume rendered so that researchers can examine the data efficiently. However, the volume data is usually quite large and a commodity graphics device can store one set of volume data. When the researcher need to look at the acceleration or displacement wave-field. The velocity data has to be integrated into displacement wave-field. Sometimes, the velocity data has to be differentiated into displacement wave-field. In this study, we used CUDA to compute the ground acceleration and displacement from velocity data. Numerical quadrature and centred difference method were implemented using CUDA for on-the-fly data exploration. We used CUDA to speed up ray casting method, trapezoidal quadrature, and centered difference method. We achieved a speed up of 100 times faster than using a CPU.
[data analysis, volume rendering, Computational modeling, parallel architectures, CPU, volume data numerical integration, parallel programming, Graphics processing unit, CUDA, Casting, Earthquakes, wave-field data sets, Rendering (computer graphics), large-scale ground-motion velocity, Three dimensional displays, volume data numerical differentiation, Acceleration, earthquake engineering, earthquake simulations]
Parallel Computation of the Weather Research and Forecast (WRF) WDM5 Cloud Microphysics on a Many-Core GPU
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
The Weather Research and Forecast (WRF) Double Moment 5-class (WDM5) mixed ice microphysics scheme predicts mixing ratio of hydrometeors and their number concentrations for warm rain species including clouds and rain. WDM5 can be computed in parallel in the horizontal domain using a many-core GPU. In order to obtain better GPU performance, we manually rewrote the original WDM5 Fortran module into a highly parallel CUDA C program. The GPU-based implementation of WDM5 microphysics scheme on 1 GTX590 GPU achieves a significant speedup of 147&#x00D7; over its CPU-based single-threaded counterpart when we use asynchronous data transfer and non-coalesced memory access. More importantly, the speedup excluding the host-device data transfer time is 206&#x00D7; when using coalesced memory access. Since the WDM5 microphysics scheme is only an intermediate module of the entire WRF model, its input data should be already available in the GPU global memory from previous modules and its output data should reside at the GPU global memory for later usage by other modules.
[parallel computation, asynchronous data transfer, Clouds, Snow, Instruction sets, parallel architectures, WRF WDM5 cloud microphysics, highly parallel CUDA C program, WRF, clouds, GPU, WDM5 microphysics scheme, Graphics processing unit, GTX590 GPU, Rain, weather forecasting, Weather Research and Forecast, many-core GPU, warm rain species, hydrometeors, horizontal domain, double moment 5-class, mixed ice microphysics scheme, multiprocessing systems, WDM5 Fortran module, GPU-based implementation, GPU global memory, geophysics computing, Wavelength division multiplexing, Ice, CPU-based single-threaded counterpart, graphics processing units, physics computing, CUDA, noncoalesced memory access]
Commodity Cluster-Based Parallel Implementation of an Automatic Target Generation Process for Hyperspectral Image Analysis
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
The development of parallel implementations is an important task for hyper spectral data exploitation. In most cases, real-time or nearly real-time processing of hyper spectral images is required for swift decisions which depend upon high computing performance of algorithm analysis. A popular algorithm in hyper spectral image interpretation is the automatic target generation process (ATGP). In this paper, we develop a new parallel version of this algorithm, which is routinely applied in many application domains, including defence and intelligence, precision agriculture, geology, or forestry. We improve considerably the computational cost of this algorithm, and also improve its detection accuracy by incorporating a new method for calculating the orthogonal projection process in which the algorithm is based using the Gram-Schmidt method. Our proposed strategy reduces the computational cost over the a previous implementation of the same algorithm which uses the pseudoinverse operation to compute the orthogonal projection. Our parallel algorithm is implemented on a multi-core cluster system made up of of sixteen nodes, with two CPUs of four cores per node, and quantitatively evaluated using hyper spectral data collected by NASA's Airborne Visible Infra-Red Imaging Spectrometer (AVIRIS) over the World Trade Center (WTC) in New York and over the Cup rite mining district, Nevada, United States.
[Algorithm design and analysis, forestry application, Cup rite mining district, multicore cluster system, geology application, hyperspectral image analysis, United States, parallel algorithm, hyperspectral data exploitation, object detection, cluster computing, Parallel algorithms, Clustering algorithms, commodity cluster-based parallel implementation, Nevada, parallel algorithms, multiprocessing systems, Gram-Schmidt method, automatic target generation process (ATGP), automatic target generation process, Gram-Schmidt orthogonalization, world trade center, Vectors, precision agriculture application, airborne visible infrared imaging spectrometer, defence application, orthogonal projection process, intelligence application, pseudoinverse operation, Object detection, Hyperspectral imaging]
GPU Implementation of Orthogonal Matching Pursuit for Compressive Sensing
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Recovery algorithms play a key role in compressive sampling (CS). Currently, a popular recovery algorithm for CS is the orthogonal matching pursuit (OMP), which possesses the merits of low complexity and good recovery quality. Considering that the OMP involves massive matrix/vector operations, it is very suited to being implemented in parallel on graphics processing unit (GPU). In this paper, we first analyze the complexity of each module in the OMP and point out the bottlenecks of the OMP lie in the projection module and the least-squares module. To speedup the projection module, Fujimoto's matrix-vector multiplication algorithm is adopted. To speedup the least-squares module, the matrix-inverse-update algorithm is adopted. Experimental results show that +40x speedup is achieved by our implementation of OMP on GTX480 GPU over on Intel(R) Core(TM) i7 CPU. Since the projection module occupies more than 2/3 of the total run time, we are looking for a faster matrix-vector multiplication algorithm.
[iterative methods, least mean squares methods, recovery algorithm, compressed sensing, Instruction sets, Matching pursuit algorithms, least-squares module, compressive sampling, Vectors, Complexity theory, Registers, graphics processing units, orthogonal matching pursuit, Graphics processing unit, matrix-inverse-update algorithm, matrix-vector multiplication algorithm, matrix multiplication, vectors, OMP, graphics processing unit, GPU implementation, Kernel, compressive sensing, projection module]
Fast Band Selection for Hyperspectral Imagery
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Band selection is a common technique for dimensionality reduction of hyperspectral imagery. When the desired object information is unknown, an unsupervised band selection approach is employed to select the most distinctive and informative bands. However, it may be time-consuming for unsupervised band selection methods that need to take all pixels into consideration. Here, we propose an approach to select several pixels for unsupervised band selection and the number of pixels required can be equal to the number of bands to be selected minus 1. With whitened pixel signatures (not the original pixels), band selection performance can be comparable to or even better than that from using all the pixels. For this approach, graphics processing unit (GPU)-based parallel computing is implemented for pixel selection only to further expedite the process, since computational complexity in band selection has been greatly reduced.
[Algorithm design and analysis, image processing, Noise, Band selection, object information, hyperspectral imagery, Vectors, graphics processing units, GPU, parallel programming, Graphics processing unit, Graphics, dimensionality reduction, graphics processing unit, graphics computing units (GPUs)., fast band selection, unsupervised band selection, parallel computing, whitened pixel signatures, Hyperspectral imaging, computational complexity]
Parallel Implementation of Edge-Directed Image Interpolation on a Graphics Processing Unit
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
The edge-directed interpolation scheme is a non-iterative, orientation-adaptive method to enhance image resolution with better visual effect than conventional interpolation methods. It interpolates the missing pixels based on the covariance of a high-resolution image estimated from the covariance of the low-resolution image. In spite of the impressive performance, the computational complexity of covariance-based adaptation is significantly higher than that of the conventional linear interpolation algorithms. In this paper, we propose a GPU-based massively parallel version of the edge-directed interpolation scheme. A speedup of 61.7x can be achieved with respect to its single-threaded CPU counterpart in the host computer.
[missing pixel, edge-directed interpolation, computer graphic equipment, Instruction sets, Registers, noniterative orientation-adaptive method, Graphics processing unit, edge-directed image interpolation, visual effect, single-threaded CPU, Kernel, parallel computing, image resolution, covariance-based adaptation, Image edge detection, parallel implementation, GPU-based massively parallel version, high-resolution image, CUDA, Interpolation, interpolation, Graphics Processing Unit (GPU), Streaming media, graphics processing unit, covariance analysis, low-resolution image, computational complexity]
Single-Phase Wireless LAN Based Multi-floor Indoor Location Determination System
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Current indoor location determination systems require an offline training phase in order to build a radio map that contains a list of radio fingerprints at known locations. The process of building this radio map is usually a time-consuming and requires an intensive work where the accuracy of these systems depends on various factors such as the number of collected radio fingerprints and the time in which they were collected. This paper presents an indoor location determination system that does not require a time-consuming offline training phase in order to build the radio map. Instead, the system uses the online phase for both reading the current Received Signal Strength (RSS) and inferring the location values for each random variable using Bayesian Graphical Model (BGM). The proposed system is based on Markov Chain Monte Carlo (MCMC) sampling techniques in order to draw samples from the posterior distribution. We will also introduce the Feed and Infer algorithm to be used in conjunction with the proposed graphical model. The proposed system will be compared with other single-phase systems.
[Wireless LAN, current received signal strength, MCMC, Mobile communication, Accuracy, Graphical models, Monte Carlo methods, Markov chain Monte Carlo sampling techniques, single-phase systems, Bayesian graphical model, MCMC sampling techniques, Indoor positioning, indoor radio, radio map, BGM, WLAN, radio fingerprints, multifloor indoor location determination system, RSS, Bayesian methods, time-consuming offline training phase, graphical models, Markov processes, Random variables, Bayes methods, Bayesian networks, Feeds, wireless LAN, single-phase wireless LAN]
[Publishers information]
2011 IEEE 17th International Conference on Parallel and Distributed Systems
None
2011
Provides a listing of current committee members and society officers.
[]
Message from the General Co-Chairs
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
We welcome you to the 18th IEEE International Conference on Parallel and Distributed Systems (ICPADS 2012), held this year in the heart of South-East Asia, Singapore. This is the first time that ICPADS takes place in Singapore, a vibrant and cosmopolitan city that offers brimming culture diversity, multiplicity of arts and architecture; and unique experience of tropical nature, shopping and dining. With so many things to see and do, we are sure all of you will have a memorable stay during this conference. ICPADS 2012 takes place at the Nanyang Executive Centre, located on the Yunnan Garden campus of Nanyang Technological University. This year's ICPADS received a large number of submissions, further strengthening its status as a leading conference in the areas of parallel and distributed systems. The three day conference consists of three keynote talks, a session of best paper candidates, a poster session, and parallel tracks of paper presentations from the main conference and five workshops. We are delighted that Prof. Ivan Stojmenovic (University of Ottawa, Canada), Dr. Sunil D. Sherlekar (Intel Labs, Bangalore, India), and Prof. Jiannong Cao (Hong Kong Polytechnic University, Hong Kong, China) had accepted our invitation and will give keynote talks at the conference. We hope that you will enjoy this exciting program that we have arranged for you.
[]
Message from the Program Co-Chairs
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
It is our great pleasure to welcome all of you to the 18th IEEE International Conference on Parallel and Distributed Systems (ICPADS 2012) and to Singapore. Established in 1992, ICPADS has been a major international conference in the fields of parallel and distributed systems. We thank you for participating and contributing to ICPADS 2012 and hope that you find this year's program interesting and inspiring. We received a total of 294 paper submissions in eight research tracks, including Parallel/Distributed Algorithms and Applications; Multicore Computing and Parallel/Distributed Architectures; Cloud Computing and Services; Cyber- Physical Systems; P2P and Web-based Computing; Mobile, Sensor and Ubiquitous Computing; Security and Trustworthy Computing; and Massive Data and Storage Systems. The program committee worked very hard to thoroughly review all the submitted papers. The selection of papers to the conference was very competitive. We finally accepted 87 regular papers to be presented at the conference, leading to an acceptance rate of less than 30%.
[]
ICPADS 2012 Organizing Committee
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Provides a listing of current committee members.
[]
Program Committee Members
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Provides a listing of current committee members and society officers.
[]
Workshop Organizers
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Provides a listing of current committee members and society officers.
[]
Keynote 1: Mobile cloud and crowd computing and sensing
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Summary form only given, as follows. Mobile devices (smart phones, tablets, laptops, embedded boards, robots) can serve as terminals for cloud computing services over intelligent network. Mobile cloud has emerged as a new cloud computing platform that 'puts cloud into a pocket.' Important issues include optimizing the scheduling and transport schemes, access management, and application optimization, for mobile devices to achieve energy saving. This talk will first introduce the development of mobile cloud computing and describe some applications involving multimedia, vision/recognition, graphics, gaming, text processing. It will present the transmission, computation (e.g. task outsourcing), and sensing (e.g. location based services) challenges and solution approaches of green computing in mobile cloud. 'Crowd computing' combines mobile devices and social interactions to achieve large-scale distributed computation. Examples include task farming and social network creation and cooperation. Mobile devices are being equipped with various sensors to provide input for participatory and opportunistic crowd-sourced sensing. Particular emerging concepts are 'vehicular cloud' and 'vehicular crowd,' with applications such as cloud server, vehicular data center, and congestion mitigation.
[]
Keynote 2: Exascale computing and the democratisation of HPC
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Summary form only given, as follows. Development of Exascale Computers before the end of this decade is an ambitious target taken up by the international HPC community. The challenges are numerous; the important ones being power consumption & dissipation, reliability, application scaling, display technology and ubiquitous access. These challenges also occur at all levels of abstraction: materials & device physics, circuit design, logic design, architecture, the software stack, algorithms and high-speed networking (the last one for ubiquitous access). We need to meet these challenges not only at every level of abstraction but also need to look at the interaction between levels. Meeting the target of building Exascale systems will of course help us solve some "grand challenge" problems in science and engineering. More importantly, perhaps, it will help us "democratise" the use of HPC because of the affordability in terms of both capital and operating costs. However, such democratisation will require meeting yet another overriding challenge: education. A large workforce will be needed to design algorithms, optimise application software and intelligently use (high-end) simulation in their design flow. The talk will cover all the above aspects. Specific examples will be used for concrete illustration.
[]
Keynote 3: Distributed processing in wireless ad hoc and sensor networks
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Summary form only given, as follows. Wireless ad hoc and sensor networks (WASNs) raise new problems and/or new challenging issues in distributed processing. Furthermore, WASNs are mostly application specific. Design of distributed algorithms needs to explicitly account for mobility, resource constraints, dynamic changes and other characteristics of WASNs, as well as special application requirements. In this talk, I will overview distributed processing in WASNs and discuss the important issues and principles of designing effective solutions. I will also report our research in designing efficient and reliable distributed algorithms for applications of wireless sensor networks in structural health monitoring
[]
Reuse of GSM White Space Spectrum for Cognitive Femtocell Access
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Nowadays, cyber-physical system (CPS) relies on wireless networks for devices control and information backhaul. But the mass deployment CPS devices make operators' spectrum scarce situations even more worse. Hence, cellular network operators anticipate the Dynamic Spectrum Access (DSA) technology to solve the spectrum shortage problem in the context of cognitive radio (CR). Femtocells, acting as gateways in CPS, integrate CPS devices into cellular networks in a seamless manner. The concept of cognitive femtocell can solve the spectrum congestion problem even within a massive network on the CPS scale. However, in practical systems, cellular white space spectrum should be quantitatively measured to guide cognitive femtocell access algorithms design. We are the first to conduct a comprehensive measurement study for the purpose of measurement, discovery and model features of GSM white space spectrum. We evaluate availabilities of extra 21.4 MHz capacity in GSM white space spectrum as a reason of artificial GSM network spectrum planning. In our study, we find out that perfect results can hardly be obtained because of inherent measurement trade-offs, even when extremely high sweep speed of 16 GHz/s is applied at the receiver. Based on statistical analysis of real-scene traces, we propose an Efficient Duty Cycle (EDC) model to accurately characterize the white space in GSM network by considering miss-detection probabilities. Cross-validating evaluation results show that the EDC model can well decrease interference probabilities at high time-granularity measurement periodicity scenarios. Our results confirm the feasibility of cognitive femtocells access in an intra-operator scenario and can be applied to future wireless networks.
[interference probability, GSM white space, Frequency measurement, cellular network operator, radiofrequency interference, efficient duty cycle, Efficient Duty Cycle model, cognitive radio, Spectrum measurement, information backhaul, embedded systems, GSM white space spectrum, cognitive femtocell access algorithm, Sensors, Femtocells, EDC model, GSM, radio spectrum management, telecommunication network planning, dynamic spectrum access, spectrum shortage problem, Receivers, devices control, DSA technology, wireless networks, Time measurement, cyber-physical system, artificial GSM network spectrum planning, femtocellular radio, cellular white space spectrum, Dynamic spectrum access, time-granularity measurement periodicity scenario, White spaces, statistical analysis, CPS device]
A General Framework for Broadcasting in Static to Highly Mobile Wireless Ad hoc, Sensor, Robot and Vehicular Networks
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
In a broadcasting task, source node wants to send the same message to all the other nodes in the network. Existing solutions address specific mobility scenarios, e.g. connected dominating set (CDS) based for static networks, blind flooding for moderate mobility, hyper flooding for highly mobile and frequently partitioned networks. We are interested in designing a unique protocol that will seamlessly (without using any parameter) adjust itself to any mobility scenario, and with capability to address various model assumptions and optimality criteria. Existing approaches for all scenarios are based on some threshold parameters (e.g. speed) to locally select among different algorithms, and therefore different nodes may run different algorithms. Here we describe a novel general BSM (Broadcasting from Static to Mobile) framework, built over several recent algorithms handling special cases. It aims at high delivery rate with low message cost, and addresses intermittent connectivity and delay minimization. Each node activates with respect to broadcast message whenever it identifies one or more neighbors in need of the message. It selects, upon activation, waiting time (dynamically adjusted with reception of any message) depending on the number of such neighbors, distance to their centroid, and its CDS membership. It competes (at MAC layer) to retransmit at timeout expiry if it still believes that a neighbor needs message. We map this algorithm to variety of multi-hop wireless networks scenarios, with and without: positional information, acknowledgments, and time criticality goal. Some existing solutions are derived as special cases, and we also show how to timely deliver warnings in vehicular networks with arbitrary road structure, without using road maps.
[CDS membership, Protocols, mobility, highly mobile wireless ad hoc networks, wireless ad hoc and sensor networks, Mobile communication, sensor networks, threshold parameters, access protocols, general framework, Floods, broadcast communication, Vehicles, hyperflooding, blind flooding, delay minimization, mobile ad hoc networks, connected dominating set, Broadcasting, vehicular networks, BSM, MAC layer, robot networks, broadcast message, broadcasting task, static networks, road structure, Reliability, Mobile computing, timely deliver warnings]
Achieving Private, Scalable, and Precise Data Collection in Wireless Sensor Networks
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Wireless Sensor Networks (WSN) become increasingly popular to collect data over a large area. Given the collected data set, the network manager can extract various kinds of aggregate statistics from the set to characterize the physical space. On the collection of the data, three requirements should be imposed: (1) Privacy: as sensor nodes are source limited and often deployed in an open environment, the sensed data suffer from privacy vulnerabilities. Secure mechanism should be provided to protect data privacy, (2) Communication efficiency: collecting data from large-scale sensor networks often involves large-volume data generation and transmission, which may quickly consume the energy of the WSN. To prolong the lifetimes of the sensor nodes, the sensed data should be transmitted in lightweight manner, (3) Accuracy: the sensed data should be recovered accurately at the base station (BS) so that the manager can manipulate them freely to achieve any precise aggregate statistic he prefers. To satisfy these requirements, we propose two novel privacy-preserving data collection schemes based on compressive sensing techniques. Our schemes address the privacy, communication efficiency and accuracy issues simultaneously. Detailed theoretical analysis and simulation results confirm the high performance of the proposed schemes.
[Data privacy, compressed sensing, wireless sensor networks, large-scale sensor networks, Encryption, privacy vulnerability, sensor nodes, BS, Privacy, Accuracy, network manager, data privacy protection, energy consumption, base station, privacy-preserving data collection schemes, Wireless Sensor Networks, Wireless sensor networks, WSN, Aggregates, compressive sensing technique, data transmission, Data collection, secure mechanism, data communication, data privacy, communication efficiency, large-volume data generation]
Hybrid Mode Radio Link Control for Efficient Video Transmission over 4GLTE Network
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Accessing video content on mobile devices has become widespread in recent years. The wireless network has to become content aware in order to offer enhanced quality of video service through efficient utilisation of the wireless spectrum. This paper proposes a scheme for end-to-end video transmission over Long Term Evolution (LTE) network. The network elements are made aware of the critical video frames that are transmitted through the network with the help of a field in Internet Protocol (IP) header which reduces the processing time. A new mode of operation called Hybrid Mode (HM) for Radio Link Control (RLC) of evolved NodeB (eNodeB) has been defined to ensure the reliable delivery of critical video frames which in turn increase the video quality. The simulation results with the proposed scheme show a 7% increase in received video quality in comparison to the legacy Unacknowledged Mode (UM) of RLC. The proposed scheme also removes the major jitter and provides a reduction in end-to-end delay by 99% when compared with the Acknowledged Mode (AM).
[Protocols, H. 264, eNodeB, 4G-LTE network, PD control, 4G/LTE Networks, Delay, video content, Wireless communication, Video Streaming, end-to-end delay, quality of video service, internet protocol, IP networks, video communication, Long Term Evolution, radio link control, hybrid mode radio link control, evolved NodeB, AVC, Content Aware, 4G mobile communication, unacknowledged mode, Streaming media, Tin, mobile devices, Radio Link Control, efficient video transmission]
Auto-Tuning GEMV on Many-Core GPU
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
GPUs provide powerful computing ability especially for data parallel algorithms. However, the complexity of the GPU system makes the optimization of even a simple algorithm difficult. Different parallel algorithms or optimization methods on a GPU often lead to very different performances. The matrix-vector multiplication routine for general dense matrices (GEMV) is a building block for many scientific and engineering computations. We find that the implementations of GEMV in CUBLAS 4.0 or MAGMA are not efficient, especially for small matrix or fat matrix (a matrix with small number of rows and large number of columns). In this paper, we propose two new algorithms to optimize GEMV on Fermi GPU. Instead of using only one thread, we use a warp to compute an element of vector y. We also propose a novel register blocking method to accelerate GEMV on GPU further. The proposed optimization methods for GEMV are comprehensively evaluated on the matrices with different sizes. Experiment results show that the new methods can achieve over 10x speedup for small square matrices and fat matrices compared to CUBLAS 4.0 or MAGMA, and the new register blocking method can also perform better than CUBLAS 4.0 or MAGMA for large square matrices. We also propose a performance-tuning framework on how to choose an optimal algorithm of GEMV for an arbitrary input matrix on GPU.
[Algorithm design and analysis, Instruction sets, mathematics computing, Graphics processing units, small square matrices, fat matrices, Registers, data parallel algorithms, Performance Tuning, GPU, Fermi GPU, optimisation, many-core GPU, general dense matrices, register blocking method, Computer architecture, Kernel, software performance evaluation, scientific computations, parallel algorithms, multiprocessing systems, GEMV, Vectors, autotuning GEMV, graphics processing units, arbitrary input matrix, matrix multiplication, vectors, matrix-vector multiplication routine, engineering computations]
Pipelined Parallel LZSS for Streaming Data Compression on GPGPUs
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
In this paper, we present an algorithm and provide design improvements needed to port the serial Lempel-Ziv-Storer-Szymanski (LZSS), lossless data compression algorithm, to a parallelized version suitable for general purpose graphic processor units (GPGPU), specifically for NVIDIA's CUDA Framework. The two main stages of the algorithm, substring matching and encoding, are studied in detail to fit into the GPU architecture. We conducted detailed analysis of our performance results and compared them to serial and parallel CPU implementations of LZSS algorithm. We also benchmarked our algorithm in comparison with well known, widely used programs, GZIP and ZLIB. We achieved up to 34x better throughput than the serial CPU implementation of LZSS algorithm and up to 2.21x better than the parallelized version.
[Algorithm design and analysis, Instruction sets, parallel architectures, Graphics processing units, Data compression, streaming data compression, serial Lempel-Ziv-Storer-Szymanski, GZIP program, pipelined parallel LZSS algorithm benchmarking, History, ZLIB program, LZSS, GPU, lossless data compression algorithm, NVIDIA CUDA framework, parallel CPU implementations, throughput, data compression, substring encoding, Software algorithms, performance evaluation, Encoding, serial CPU implementations, Lossless data compression, graphics processing units, CUDA, GPGPU architecture, substring matching, general purpose graphic processor units, pipeline processing, string matching]
Performance Evaluation of Concurrent Lock-free Data Structures on GPUs
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Graphics processing units (GPUs) have emerged as a strong candidate for high-performance computing. While regular data-parallel computations with little or no synchronization are easy to map on the GPU architectures, it is a challenge to scale up computations on dynamically changing pointer-linked data structures. The traditional lock-based implementations are known to offer poor scalability due to high lock contention in the presence of thousands of active threads, which is common in GPU architectures. In this paper, we present a performance evaluation of concurrent lock-free implementations of four popular data structures on GPUs. We implement a set using lock-free linked list, hash table, skip list, and priority queue. On the first three data structures, we evaluate the performance of different mixes of addition, deletion, and search operations. The priority queue is designed to support retrieval and deletion of the minimum element and addition operations to the set. We evaluate the performance of these lock-free data structures on a Tesla C2070 Fermi GPU and compare it with the performance of multi-threaded lock-free implementations for CPU running on a 24-core Intel Xeon server. The linked list, hash table, skip list, and priority queue implementations achieve speedup of up to 7.4, 11.3, 30.7, and 30.8, respectively on the GPU compared to the Xeon server.
[skip list, Instruction sets, lock-free linked list, priority queue, lock-free, Graphics processing units, addition operations, concurrent, concurrent lock-free data structures, linked list, search operations, GPU, deletion operations, pointer-linked data structures, lock-based implementations, data-parallel computations, data structures, Kernel, high-performance computing, Intel Xeon server, Tesla C2070 Fermi GPU, hash table, multi-threading, performance evaluation, Synchronization, multithreaded lock-free implementations, graphics processing units, CUDA, Arrays]
An Optimal Fully Distributed Algorithm to Minimize the Resource Consumption of Cloud Applications
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
According to the pay-per-use model adopted in clouds, the more the resources consumed by an application running in a cloud computing environment, the greater the amount of money the owner of the corresponding application will be charged. Therefore, applying intelligent solutions to minimize the resource consumption is of great importance. Because centralized solutions are deemed unsuitable for large-distributed systems or large-scale applications, we propose a fully distributed algorithm (called DRA) to overcome the scalability issues. The aforementioned problem can be solved by identifying an assignment scheme between the interacting components of an application, such as processes and virtual machines, and the computing nodes of a cloud system, such that the total amount of resources consumed by the respective application is minimized. The decisions for the transition from one assignment scheme to another one are made in a dynamic way and based only on local information. It should be stressed that DRA achieves convergence and always results in the optimal solution. We also show, through an experimental evaluation, that DRA achieves up to 55% network cost reduction when compared to the most recent algorithm in the literature.
[Algorithm design and analysis, Measurement, Computational modeling, Minimization, Virtual machining, cloud computing environment, distributed algorithms, cloud applications, virtual machines, network flow, placement, cloud computing, minimisation, Distributed algorithms, optimal fully distributed algorithm, Gravity, resource consumption]
Multi-tier Service Differentiation: Coordinated Resource Provisioning and Admission Control
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Multiple Internet applications are often hosted in one datacenter and share underlying virtualized server resources. It is important but challenging to provide differentiated treatment to co-hosted applications and improve overall performance with efficient use of limited resources. We propose a coordinated self-adaptive resource management and admission control for multi-tier Internet service differentiation and performance improvement in a shared virtualized platform. We develop reinforcement learning based approaches for virtual machine (VM) auto-configuration and session based admission control. VM auto-configuration simultaneously provisions proportional service differentiation between co-located applications and improves application response time. Admission control improves session throughput of the applications, minimizing resource wastage due to aborted sessions. A shared reward actualizes coordination between the two learning modules. For system agility and scalability, we integrate reinforcement learning with cascade neural networks. We implement the integrated approach in a virtualized blade server system hosting multi-tier RUBiS applications. Experimental results demonstrate that the approach accurately meets differentiation targets and achieves performance improvement of applications. Our approach reacts to dynamic bursty workloads in agile and scalable manner.
[multiple Internet applications, Resource Provisioning, virtualized server resources, Throughput, multitier service differentiation, admission control, Servers, performance improvement, Admission Control, reinforcement learning, Learning, Virtualized Environment, RUBiS applications, learning (artificial intelligence), coordinated resource provisioning, cascade neural networks, Service Differentiation, datacenter, computer centres, multi-tier Internet applications, virtual machine, Admission control, Neural networks, virtual machines, VM, virtualized blade server system, Internet, Time factors, neural nets]
Reconciling Dynamic System Sizing and Content Locality through Hierarchical Workload Forecasting
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
The cloud has recently surged as a promising paradigm for hosting scalable Web systems serving a large number of users with large workload variations. It makes possible to dynamically add and remove resources to horizontally scalable architectures in order to save costs, while maintaining the quality of service. However, in order to achieve these goals the resource management of a platform must include policies and mechanisms for dynamically resizing the system, redistributing content and redirecting user requests. In this work, we address the problem of reconciling dynamic system sizing and content locality. There are three main contributions of our study. First, we address the problem of determining the system size by employing a hierarchical prediction framework that proactively provisions resources based on statistical models of the incoming workload. Second, we show how to employ the hierarchical prediction framework for designing a dispatching mechanism which can be used with any request distribution policy. Third, we propose two novel prediction-based locality-aware request distribution policies: Oblivious Locality-Aware Request Distribution (OLARD) and Affinity-Based Locality-Aware Request Distribution (ABLARD). We demonstrate the advantages of using our hierarchical prediction framework and how our approach achieves a high content locality, while adapting to unexpected workload changes.
[resource management, Adaptation models, Web systems, data centers, reconciling dynamic system sizing locality, Predictive models, workload variations, OLARD, Servers, affinity based locality aware request distribution, dispatching mechanism, statistical models, content locality, cloud computing, Monitoring, reconciling dynamic system content locality, forecasting, hierarchical workload forecasting, quality of service, Forecasting, computer centres, oblivious locality aware request distribution, resource provisioning, ABLARD, Dispatching, Internet]
Free Elasticity and Free CPU Power for Scientific Workloads on IaaS Clouds
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Recent Infrastructure as a Service (IaaS) solutions, such as Amazon's EC2 cloud, provide virtualized on-demand computing resources on a pay-per-use model. From the user point of view, the cloud provides an inexhaustible supply of resources, which can be dynamically claimed and released. In the context of independent tasks, the main pricing model of EC2 promises two exciting features that drastically change the problem of resource provisioning and job scheduling. We call them free elasticity and free CPU power. Indeed, the price of CPU cycles is constant whatever the type of CPU and the amount of resources leased. Consequently, as soon as a user is able to keep its resources busy, the cost of one computation is the same using a lot of powerful resources or few slow ones. In this article, we study if these features can be exploited to execute bags of tasks, and what efforts are required to reach this goal. Efforts might be put on implementation, with complex provisioning and scheduling strategies, and in terms of performance, with the acceptance of execution delays. Using real workloads, we show that: (1) Most of the users can benefit from free elasticity with few efforts; (2) Free CPU power is difficult to achieve; (3) Using adapted provisioning and scheduling strategies can improve the results for a significant number of users; And (4) the outcomes of these efforts is difficult to predict.
[free elasticity, resource management, IaaS clouds, Computational modeling, execution delays, Elasticity, virtualized on-demand computing resources, Virtual machining, Scheduling, CPU cycles, scientific workloads, Amazon EC2 cloud, infrastructure as a service solutions, cloud, free CPU power, Runtime, Processor scheduling, resource provisioning, Pricing, pay-per-use model, provisioning, cloud computing, main pricing model, job scheduling]
Dictionary Attack on TrueCrypt with RIVYERA S3-5000
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
The popular free encryption software True Crypt uses whole device or partition encryption as well as encrypted container files to protect sensible data from unauthorized access. Several combinations of encryption algorithms and hash functions used for the key derivation can be chosen by the user. This paper regards the combination with SERPENT as encryption algorithm and WHIRLPOOL as hash function for the key derivation. A dictionary attack has been implemented for this combination using the FPGA-based high-performance computer RIVYERA S3-5000. The achieved performance reaches more than 200,000 passwords per second. Compared to 820 passwords per second, achieved by a fully threaded Intel Core i7-970 system at 3.2GHz using the Crypto++ library, this leads to a speedup of more than 247 with energy savings of about 99%.
[free encryption software True Crypt, Ciphers, dictionary attack, Dictionaries, field programmable gate arrays, fully threaded Intel Core i7-970 system, frequency 3.2 GHz, reconfigurable high-performance computing, FPGA, Random access memory, FPGA-based high-performance computer RIVYERA S3-5000, key derivation, Containers, Crypto++ library, cryptography, Encryption, hash functions, WHIRLPOOL, SERPENT, file organisation, data protection, known-plaintext dictionary attack, Field programmable gate arrays, PBKDF2]
A Lightweight Secure Provenance Scheme for Wireless Sensor Networks
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Large-scale sensor networks are being deployed in numerous application domains, and often the data they collect are used in decision-making for critical infrastructures. Data are streamed from multiple sources through intermediate processing nodes that aggregate information. A malicious adversary may tamper with the data by introducing additional nodes in the network, or by compromising existing ones. Therefore, assuring high data trustworthiness in such a context is crucial for correct decision-making. Data provenance represents a key factor in evaluating the trustworthiness of sensor data. Provenance management for sensor networks introduces several challenging requirements, such as low energy and bandwidth consumption, efficient storage and secure transmission. In this paper, we propose a novel light-weight scheme to securely transmit provenance for sensor data. The proposed technique relies on in-packet Bloom filters to encode provenance. In addition, we introduce efficient mechanisms for provenance verification and reconstruction at the base station. We evaluate the proposed technique both analytically and empirically, and the results prove its effectiveness and efficiency for secure provenance encoding and decoding.
[telecommunication security, sensor data, data trustworthiness, Provenance, wireless sensor networks, large-scale sensor networks, Security, data provenance management, data structures, Cryptography, bandwidth consumption, Base stations, Sensor Networks, Encoding, Decoding, encoding, decoding, malicious adversary, lightweight secure provenance scheme, in-packet Bloom filters, Aggregates, decision-making, decision making, Data models, intermediate processing nodes]
Accountable Anonymity: A Proxy Re-Encryption Based Anonymous Communication System
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
While several well-developed anonymous services have been made available on-line to protect Internet user's privacy, we have seen that due to the lack of accountability, they have often been exploited by criminals to conduct various illegal activities (e.g., sharing child pornography materials, sending threatening emails, etc.) against innocent people in both cyberspace and physical world. There is an immediate need for building accountability within anonymous systems. In this paper, we show that although accountability appears to be contradictory to and may impair anonymity, it is technically feasible to combine anonymity and accountability into one framework, namely Accountable Anonymity. To our knowledge, this is one of the first attempts trying to achieve both accountability and anonymity. We propose an efficient proxy re-encryption based design that allows normal activities to be done in anonymous manner, while keeping malicious/criminal operations accountable at the same time. Formal security analysis and prototype implementation demonstrate that our scheme is not only immune to well-known attacks against anonymity and accountability, but also able to provide non-frameability and confidentiality along with Accountable Anonymity.
[Art, Protocols, proxy reencryption based anonymous communication system, Forensics, Internet user privacy protection, Incentive-compatible, Anonymity, cryptography, illegal activities, Proxy Reencryption, Encryption, Electronic mail, malicious operations, formal security analysis, physical world, criminal operations, prototype implementation, Public key, Accountability, accountable anonymity, Internet, cyberspace world, Payloads]
Maximizing Availability of Consistent Data in Unreliable Networks
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
We address the issue of maximization of the availability of replicated data that are distributed in a wide area network. We consider a system that uses majority voting, which is a common mechanism for providing consistency of replicated data in the presence of failures. The data availability provided by this mechanism critically depends on the vote assignment to the replicas. In this paper we formulate the problem of finding the optimal vote assignment into a specific form of a combinatorial optimization problem, namely the MAX-SMT problem. This formulation allows us to use a modern, fast MAX-SMT solver to solve the vote assignment problem. To evaluate the effectiveness of this approach, we build a failure repair model of underlying networks and estimate the data availability using that model. The results of the estimation show that data availability can be significantly improved using the optimal vote assignment in the presence of failures.
[Availability, Wide area networks, network partition, wide area networks, MAX-SMT problem, distributed processing, majority voting, Probabilistic logic, maximizing availability, Topology, availability, Servers, consistent data, Optimization, combinatorial optimization, optimisation, Network topology, data availability, unreliable networks, data handling, SMT, wide area network]
Mobile Edutainment with Interactive Augmented Reality Using Adaptive Marker Tracking
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Augmented Reality (AR) is a great partner of Edutainment in motivating students and enriching a class. Students can check out additional digital information of physical items such as books, samples, exhibits or even sites. The information looks just like existing in the reality. In addition to a show of multimedia content, we opine that if the virtual objects augmented to the reality can act and react like real objects, the learning experience can be greatly promoted. We developed an AR system in that the virtual objects can interact with each others. We implemented several applications using the system to demonstrate the use of interactive AR in edutainment. We also attempted to resolve the limitation of image angle coming with vision-based AR by building a multi-marker mechanism using a cube structure with surface area approximation. We also discussed some challenges and issues that researchers or developers should take note of in pursuing AR development.
[interactive augmented reality, edutainment, Mobile communication, augmented reality, multimedia content, Pattern recognition, Approximation methods, multimedia computing, Engines, Augmented reality, AR, mobile computing, digital information, Education, marker tracking, Cameras, adaptive marker tracking, computer aided instruction, virtual objects, mobile edutainment]
A Novel Multi-Channel Data Broadcast Scheme for Multimedia Database Systems
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
MultiMedia DataBase Management System (MMDBMS) becomes more popular in recent years, which supports complex and large multimedia data like images, audios, and videos etc. Data Broadcasting is an attractive approach for data dissemination to improve the limitations in mobile environment, such as narrow bandwidth, unreliable connections, and battery limitation. However, existing data broadcast schemes are inefficient for MMDBMS. In this paper, we present four novel multimedia data broadcast schemes (namely, SDAA, MDAA, AEA, and COA) specifically for wireless multichannel communications. The major strategies are scalable coding to generate data segments to different qualities, indexing and channel assignment to minimize the expected waiting time for clients. We prove theoretically that SDAA is a 2-approximation. COA performs best when we release the constraints and it can be judged as an theoretical lower bound, while AEA outputs local optimal solution with quality allocation constraints. Finally, SDAA+AEA form a best scheduling for practical applications. We also provide numerical experiments to evaluate the system performance, proving the efficiency of our schemes.
[MMDBMS, Multimedia databases, Mobile communication, local optimal solution, Multimedia communication, Servers, quality allocation constraints, Wireless communication, mobile computing, MDAA, COA, SDAA, Broadcasting, scheduling, digital multimedia broadcasting, data scheduling, MIMO communication, wireless multichannel communications, multimedia databases, data segments, mobile environment, novel multichannel multimedia data broadcast scheme, data dissemination, 2-approximation, Multimedia Data Broadcast, Streaming media, multimedia database management system, scalable coding, Data Scheduling, SDAA+AEA]
MobUser: Publish-subscribe Communication for Mobile Nodes
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Mobile devices with wireless communication capabilities are a constant in our daily lives. This offers the possibility for such devices to exchange information for their users to assist in their daily lives. Consider for instance the exchange of information about points of interest in the locations that the user usually frequents. One should expect those devices to support such an application in a decentralized fashion, through pair wise interactions, without the interaction of the user, and avoiding to disclose information that might be sensitive to a central entity. Current solutions are not suitable for supporting such applications since most rely on some kind of centralized architecture (either brokers or rendezvous nodes), other solutions that don't rely on centralized solutions present a high overhead. To address this challenge, in this paper we propose MobUser, a novel topic-based publish-subscribe service specially tailored for supporting location-aware operation for mobile devices in a decentralized fashion. Extensive experimental work shows that MobUser offers not only better performance but also superior delivery rates when compared with a state-of-the-art mobile publish subscribe solution. We also show through a prototype implementation deployed on Android 4.0 devices, that the overhead and energy consumption of MobUser is acceptably low.
[Performance evaluation, Protocols, Subscriptions, wireless networks, Ad hoc networks, wireless communication, network, Wireless communication, mobile computing, Publish-subscribe, mobile devices, publish-subscribe, gossip protocols, Reliability, MobUser, location aware operation, publish subscribe communication, mobile nodes, decentralized fashion, location aware]
ESAMR: An Enhanced Self-Adaptive MapReduce Scheduling Algorithm
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
MapReduce is a programming model and an associated implementation for processing and generating large data sets. Hadoop is an open-source implementation of Map Reduce, enjoying wide adoption, and is used not only for batch jobs but also for short jobs where low response time is critical. However, Hadoop's performance is currently limited by its default task scheduler, which implicitly assumes that cluster nodes are homogeneous and tasks make progress linearly, and uses these assumptions to decide when to speculatively re-execute tasks that appear to be stragglers. In practice, the homogeneity assumption does not always hold. Longest Approximate Time to End (LATE) is a Map Reduce scheduling algorithm that takes heterogeneous environments into consideration. It, however, adopts a static method to compute the progress of tasks. As a result neither Hadoop default nor LATE schedulers perform well in a heterogeneous environment. Self-adaptive Map Reduce Scheduling Algorithm (SAMR) uses historical information to adjust stage weights of map and reduce tasks when estimating task execution times. However, SAMR does not consider the fact that for different types of jobs their map and reduce stage weights may be different. Even for the same type of jobs, different datasets may lead to different weights. To this end, we propose ESAMR: an Enhanced Self-Adaptive Map Reduce scheduling algorithm to improve the speculative re-execution of slow tasks in Map Reduce. In ESAMR, in order to identify slow tasks accurately, we differentiate historical stage weights information on each node and divide them into k clusters using a k-means clustering algorithm and when executing a job's tasks on a node, ESAMR classifies the tasks into one of the clusters and uses the cluster's weights to estimate the execution time of the job's tasks on the node. Experimental results show that among the aforementioned algorithms, ESAMR leads to the smallest error in task execution time estimation and identifies slow tasks most accurately.
[Algorithm design and analysis, approximation theory, enhanced self adaptive MapReduce scheduling algorithm, Computational modeling, LATE, Programming, distributed processing, longest approximate time to end, k-means clustering algorithm, ESAMR, History, speculative task re-execution, distributed computing, heterogeneity, static method, MapReduce, Scheduling algorithms, pattern clustering, programming model, Hadoops performance, Clustering algorithms, scheduling, open source implementation, Hardware]
Workload Characteristic Oriented Scheduler for MapReduce
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Applications in many areas are increasingly developed and ported using the Map Reduce framework (more specifically, Hadoop) to exploit (data) parallelism. The application scope of Map Reduce has been extended beyond the original design goal which was large-scale data processing. This extension inherently makes a need for scheduler to explicitly take into account characteristics of job for two main goals of efficient resource use and performance improvement. In this paper, we study Map Reduce scheduling strategies to effectively deal with different workload characteristics CPU intensive and I/O intensive. We present the Workload Characteristic Oriented Scheduler (WCO), which strives for co-locating tasks of possibly different Map Reduce jobs with complementing resource usage characteristics. WCO is characterized by its essentially dynamic and adaptive scheduling decisions using information obtained from its characteristic estimator. Workload characteristics of tasks are primarily estimated by sampling with the help of some static task selection strategies, e.g., Java byte code analysis. Results obtained from extensive experiments using 11 benchmarks in a 4-node local cluster and a 51-node Amazon EC2 cluster show 17% performance improvement on average in terms of throughput in the situation of co-existing diverse workloads.
[WCO, adaptive scheduling, Estimation, Static Program Analysis, Hadoop, distributed processing, Throughput, Dynamic scheduling, Map Reduce framework, dynamic scheduling, performance improvement, distributed computing, Heart beat, Workload Co-location, large-scale data processing, Benchmark testing, scheduling, I/O intensive, Resource management, workload characteristic oriented scheduler, MapReduce Scheduling]
Parallel Processing of Massive EEG Data with MapReduce
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Analysis of neural signals like electroencephalogram (EEG) is one of the key technologies in detecting and diagnosing various brain disorders. As neural signals are non-stationary and non-linear in nature, it is almost impossible to understand their true physical dynamics until the recent advent of the Ensemble Empirical Mode Decomposition (EEMD) algorithm. The neural signal processing with EEMD is highly compute-intensive due to the high complexity of the EEMD algorithm. It is also data intensive because 1) EEG signals contain massive data sets 2) EEMD has to introduce a large number of trials in processing to ensure precision. The Map Reduce programming mode is a promising parallel computing paradigm for data intensive computing. To increase the efficiency and performance of the neural signal analysis, this research develops parallel EEMD neural signal processing with Map Reduce. In this paper, we implement the parallel EEMD with Hadoop in a modern cyber infrastructure. Test results and performance evaluation show that parallel EEMD can significantly improve the performance of neural signal processing.
[Electroencephalography, parallel processing, Data-intensive computing, MapReduce, brain disorders, Parallel processing, massive EEG data, electroencephalography, EEMD algorithm, Time series analysis, data intensive computing, parallel EEMD neural signal processing, Hadoop, Educational institutions, brain, electroencephalogram, Map Reduce programming mode, parallel computing paradigm, high complexity, medical signal processing, Signal processing algorithms, ensemble empirical mode decomposition algorithm, Signal processing, cyber infrastructure, Brain modeling]
Smart Replication for In-memory Computations
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Cloud computing enables diverse new application areas for distributed computing. Many upcoming cloud applications do not fit to simple programming models such as "embarrassing parallelism" but have complex data dependencies and require atomic operations spanning multiple objects. Some large-scale storage systems already implement atomic multiobject operations, but they do not address the complementary problem of efficiently propagating replica updates. In this paper, we present the design and implementation of a smart replication protocol in the ECRAM in-memory storage, which supports atomic multi-object operations. The performance analysis shows that the adaptive mechanism requires much less bandwidth, less memory, and results in improved application performance and responsiveness.
[inmemory computations, Protocols, atomic multiobject operations, random-access storage, Peer to peer computing, Radiation detectors, smart replication, embarrassing parallelism, Consistency, distributed processing, Distributed transactional memory, Synchronization, ECRAM, distributed computing, In-memory data management, Bandwidth, Aging, Replication, cloud computing, Monitoring]
ESB: Ext2 Split Block Device
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Solid State Disks (SSDs) start to replace rotating media (hard disks, HDD) in many areas, but are still not as cost efficient concerning capacity to completely replace them. One approach to use their superior performance properties is to use them as a cache for magnetic disks to speed up overall storage operations. In this paper, we present and evaluate a file system level optimization based on ext2. We split metadata and data and store the metadata on a SDD while the data remains on a common HDD. We evaluate our system with filebench under a file server, web server, and web proxy scenario and compare the results with flashcache. We find that many of the scenarios do not contain enough metadata operations to reasonably speed up IO performance.
[metadata, Stacking, Random access memory, Throughput, HDD, file system, Servers, hard discs, storage management, optimisation, IO performance, ESB, magnetic disks, file server, rotating media, Kernel, Ext2 split block device, Web server, meta data, solid state disks, SSD, Web proxy scenario, EXT2, Linux, Hard disks, file system level optimization, disc drives]
AREN: A Popularity Aware Replication Scheme for Cloud Storage
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Delivering on-demand web content to end-users in order to carry out strict QoS metrics is not a trivial task for globally distributed network providers. This task becomes still harder when content popularity varies over the time and the SLA definitions have to include both transfer rate and latency metrics. Current worldwide content delivery approaches and datacenter infrastructures rely on cumbersome replication schemes that are agnostic to edge-network resources, and damage content provision. In this work we present AREN, an novel replication scheme for cloud storage on edge networks. AREN relies on a collaborative cache strategy and bandwidth reservation to adapt the replication degree according to strict SLA contracts and content popularity growth. We have evaluated the performances of replication schemes on edge networks using Caju, a content distribution system for edge networks. Compared to a non-collaborative caching, evaluations show that AREN prevents nearly 99.8% of all SLA violations when the storage system is heavily loaded. We also show that AREN provides a sevenfold decrease in the amount of storage usage for replicas, and it increases by roughly 20% the aggregate bandwidth, hence accelerating content delivery.
[Cloud computing, AREN, collaborative cache strategy, data centers, SLA, bandwidth reservation, Multimedia communication, Web content, content data networks, distributed network providers, Bandwidth, cloud computing, online services, replication, Peer to peer computing, quality of service, computer centres, edge network resources, Aggregates, popularity aware replication scheme, cloud storage, Datacenter, QoS metrics, Resource management, popularity growth]
Ad-hoc Anonymity: Privacy Preservation for Location-based Services in Mobile Networks
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Location-based Service (LBS) becomes increasingly important for wireless and mobile networks. In current LBS schemes, Service Providers (SPs) require users report their accurate locations, which can be illegally used by adversaries to infer sensitive information of users. Privacy disclosure raises serious concerns and limits the application of LBS. Previous solutions either rely on pre-installed centralized intermediary or assume cooperative users. Other distributed solutions, lacking of user obligation, only provide relatively poor anonymity. In this study, we propose a new solution of pseudonym change, which works for non-cooperative users and in the absence of intermediary. The basic idea behind our solution is ad-hoc anonymity. The proposed solution allows users to decide whether or not participating according to their own wills. In addition, artificially generated dummies mix up all the users who have participated in pseudonym changes at different times. Theoretical analysis demonstrates that asynchronous pseudonym change and dummy participation notably enhance privacy protection. We implement our solution on a real-world dataset of mobile phone users, which is collected at Boston, MA, and by the Reality Mining, MIT. The simulation results show that our approach significantly outperforms existing solutions.
[location based services, ad hoc anonymity, privacy preservation, privacy disclosure, Quality of service, Entropy, Mobile radio mobility management, dummy participation, Privacy, mobile ad hoc networks, Mobile Networks, Location Privacy, LBS, mobile phone users, noncooperative users, asynchronous pseudonym change, user obligation, Anonymity, Educational institutions, Ad hoc networks, mobile networks, privacy protection, service providers, data privacy, mobile handsets]
Complete Bipartite Anonymity: Confusing Anonymous Mobility Traces for Location Privacy
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Using mobile devices, people can easily obtain their location information, and access a wide range of location based services (LBSs). Many existing LBSs rely in accurate, continuous, and real-time streams of location information to provide quality of service guarantees. In this case, even if an user accesses LBSs anonymously, the identity of the user can still be revealed by analyzing the mobility trace. To protect user privacy, existing work sacrifice the quality of LBSs by degrading spatial and temporal accuracy. To achieve a better tradeoff between user privacy and the quality of service, we present a novel approach, Complete Bipartite Anonymity (CBA), to confuse the paths of nearby users by connecting different users' real traces with fake ones. CBA protects user privacy as users become indistinguishable after their paths are confused, the quality of service of LBSs is also guaranteed since users are able to report their accurate locations. We evaluate CBA by comparing the system and privacy performance with existing techniques such as Path Confusion or Query Obfuscation using a real-world data set, the results show that our scheme increases the chance for a user joining an anonymity group by 10 times in low user density areas, and reduces the resources consumed by about 90% for achieving the same anonymity degree.
[telecommunication security, CBA, Query Obfuscation, location information, Quality of service, Delay, query obfuscation, Privacy, Accuracy, location privacy, real-time streams, Real-time systems, location-based services, quality of service guarantees, Bipartite graph, Location Privacy, LBS, anonymous mobility traces, Complete Bipartite Anonymity, quality of service, low-user density area, temporal accuracy, Path Confusion, path confusion, user privacy, mobility trace, anonymity group, mobile devices, complete bipartite anonymity, spatial accuracy, data privacy, Joining processes, mobile handsets]
Privacy Preserving for Continuous Query in Location Based Services
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Location-based services (LBSs) have become a popular and important way to provide real-time information and guidance. The abuse of mobile users' location data, which may violate their sensitive and private personal information, is one of the major challenges faced by LBS. On the other hand, the query launched by mobile users should not be linked to them even if they are required to expose their location information to attain some services. However, many location based systems (e.g., mobile social networks, store finders) are lacking of users' private preserving consideration. In this paper, we focuse-focus on the issues related to query linking privacy. Particularly, we aim to preserve mobile users' privacy in location based mobile systems where their location information may be available, furthermore, while facing attacks, the sensitive data of a specific mobile user launching the query should not be disclosed to an adversary. We present a new query linking privacy preserving algorithm (V-DCA) for continuous LBS by taking the user's velocity and acceleration similarity into consideration. The consecutive generated cloaked sets are used to create the new cloaked region, which decreases the complexity of the algorithm while fulfilling the privacy requirement. The simulation results show that V-DCA can preserve mobile user's privacy as well as provide better Quality of Service (QoS).
[Algorithm design and analysis, velocity and acceleration similarity, continuous query, location based services, Quality of service, privacy preserving, Mobile communication, query linking privacy, quality of service(QoS), Quality of Service, query processing, Privacy, mobile computing, QoS, location-based services (LBSs), sensitive data, Silicon, data privacy, Acceleration, LBS, mobile user location data, Joining processes]
Two-Level Result Caching for Web Search Queries on Structured P2P Networks
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
This paper proposes a two-level caching strategy for Web search queries which is devised to operate on P2P networks. The aim is to significantly reduce query traffic going from a large community of users to commercial search engines by placing between them a P2P caching service capable of storing and efficiently distributing frequent queries among users. The proposed design takes into consideration the highly dynamic nature of user queries both in traffic intensity and drastic shifts in user interest which are both usually driven by unpredictable world-wide events. Each peer maintains a LRU result cache (RCache) used to keep the answers for queries originated in the peer itself and queries for which the peer is responsible for by contacting on-demand a Web search engine to get the query answers. When query traffic is predominantly routed to a few responsible peers our strategy replicates the role of ``being responsible for" to neighboring peers so that they can absorb part of the traffic to restore load balance. This is a fairly slow and adaptive process that we call mid-term load balancing. To achieve a short-term fair distribution of queries we introduce in each peer a location cache (LCache) which keeps pointers to peers that have already requested the same queries in the very recent past. This lets these peers share their query answers with newly requesting peers. This process is fast as these popular queries are usually cached in the first DHT hop of a requesting peer which quickly tends to redistribute load among more and more peers. A comparative study shows that the proposed strategy achieves better load balance, significantly smaller communication volume among peers, and larger cache hit ratios than previous strategies.
[two-level result caching, user queries, search engines, Caching Services, Communities, cache storage, frequent query storage, Load Balancing, DHT hop, Engines, P2P networks, query processing, Distributed Hash Tables, resource allocation, Web search engine, load redistribution, distributed hash tables, short-term fair distribution, Search engines, LCache, query traffic reduction, RCache, peer-to-peer computing, Peer to peer computing, Web search queries, Routing, query answering, traffic intensity, LRU result cache, Web Search Engines, two-level caching strategy, P2P caching service, location cache, Load management, mid-term load balancing, Internet, Web search, telecommunication traffic, structured P2P networks]
FIMD: Fine-grained Device-free Motion Detection
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Device-free passive (Dfp) motion detection seeks to monitor the position change of entities without actively carrying any physical devices. Recently, WLAN with a rich set of installed wireless infrastructures enables motion detection in the area of interest. WLAN-enabled DfP motion detection rely on received signal strength (RSS) is verified to be able to provide acceptable high accuracy. Although RSS can be easily measured with commercial equipments, it is suspectable to measurement itself due to multipath effect in indoor environment. In this paper, we present an Indoor device-free Motion Detection system (FIMD) to overcome the preceding RSS-based limitation. FIMD explores properties of Channel State Information (CSI) from PHY layer in OFDM system. FIMD is designed based on the insight that CSI maintains temporal stability in static environment, while exhibits burst patterns when motion takes place. Motivated by this observation, FIMD uses a novel feature extracted from CSI to leverage its temporal stability and frequency diversity. The motion detection is conducted with outliers identification from normal features in continuous monitoring using density-based DBSCAN algorithm. Moreover, we leverage two schemes including false alert filter and data fusion to enhance the detection accuracy. We implement FIMD system with commercial IEEE 802.11n NICs and evaluate its performance in two typical indoor scenarios. Experiment results show that FIMD can achieve high detection rate. Moreover, comparing with RSSI, the feature extracted from CSI enables better detection performance in accuracy and robustness to narrowband interference.
[device-free passive motion detection, data fusion, Wireless LAN, outlier identification, temporal stability, detection performance, channel state information, Motion Detection, sensor fusion, static environment, IEEE 802.11n NIC, indoor environment, frequency diversity, Accuracy, FIMD design, feature extraction, Clustering algorithms, false alert filter, fine-grained device-free motion detection, received signal strength, indoor device-free motion detection system, CSI, burst patterns, Motion detection, Eigenvalues and eigenfunctions, OFDM modulation, WLAN-enabled DfP motion detection, indoor radio, OFDM system, position measurement, Monitoring, PHY, filtering theory, performance evaluation, entity position change monitoring, WLAN, computer network performance evaluation, PHY layer, multipath effect, RSS, density-based DBSCAN algorithm, Feature extraction, wireless infrastructures, wireless LAN, detection accuracy enhancement]
Asymptotically Optimal Load Balancing for Hierarchical Multi-Core Systems
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Current multi-core machines feature a complex and hierarchical core topology, multiple levels of cache and memory subsystem with NUMA design. Although this design provides high processing power to parallel machines, it comes with the cost of asymmetric memory access latencies. Depending on the parallel application communication patterns, this asymmetry may reduce the overall performance of the system. Therefore, to achieve scalable performance in this environment, it becomes crucial to exploit the machine architecture while taking into account the application communication patterns. In this paper, we introduce a topology-aware load balancing algorithm named HWTOPOLB. It combines the machine topology characteristics with the communication patterns of the application to equalize the application load on the available cores while reducing latencies. We also present the proof that the algorithm is asymptotically optimal (Theorem 1). We have implemented our load balancing algorithm using the CHARM++ Parallel System and analyzed its performance using three different benchmarks. Our experimental results show that the HWTOPOLB can achieve average performance improvements of 24% when compared to existing load balancing strategies on three different multi-core machines.
[Algorithm design and analysis, multi-core, load balancing, hierarchical architecture, parallel machines, Runtime, resource allocation, CHARM++ Parallel System, machine topology characteristics, Benchmark testing, cache subsystem, Libraries, high processing power, asymptotically optimal load balancing, algorithm, NUMA design, multicore machines, machine architecture, multiprocessing systems, Multicore processing, memory subsystem, topology, performance evaluation, topology aware load balancing algorithm, parallel application communication patterns, Topology, hierarchical core topology, Load management, hierarchical multicore systems]
Efficient Multi-Keyword Ranked Query on Encrypted Data in the Cloud
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Cloud computing is becoming increasingly prevalent in recent years. It introduces an efficient way to achieve management flexibility and economic savings for distributed applications. To take advantage of computing and storage resources offered by cloud service providers, data owners must outsource their data onto public cloud servers which are not within their trusted domains. Therefore, the data security and privacy become a big concern. To prevent information disclosure, sensitive data has to be encrypted before uploading onto the cloud servers. This makes plain text keyword queries impossible. As the total amount of data stored in public clouds accumulates exponentially, it is very challenging to support efficient keyword based queries and rank the matching results on encrypted data. Most current works only consider single keyword queries without appropriate ranking schemes. The multi-keyword query problem was being considered only recently. MRSE [1] is one of the first research works to define and address the problem of effective yet secure ranked multi-keyword search over encrypted cloud data. However, the keyword dictionary used in MRSE is static and must be rebuilt when the number of keywords in the dictionary increases. It also has severe out-of-order problems in the matching results and does not take the keyword access frequencies into account, which greatly affects its usability. In this paper, we propose a novel approach, called MKQE, to address these issues. Only minor changes in the dictionary structure have to be done when extra keywords are introduced. We also introduce new trapdoor generation and scoring algorithms to make in-order query results. Furthermore, the keyword access frequency is considered so as to select an adequate matching file set. We conduct extensive simulations and the results prove that our approach performs much better than previous solutions.
[Out of order, Cloud computing, Dictionaries, multikeyword ranked query, multi-keyword query, Servers, distributed applications, trapdoor generation, ranked query, matching file set, MRSE, MKQE, Cryptography, cloud computing, storage resources, cloud service providers, data encryption, data security, cryptography, Vectors, Indexes, economic savings, management flexibility, heavy tail, encrypted data, data owners, data privacy, public cloud servers, scoring algorithms, keyword dictionary]
Privacy-Aware Multi-Keyword Top-k Search over Untrust Data Cloud
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
In this paper, we focus on data privacy of searchable symmetric encryption (SSE) in cloud computing. For the first time, we formulate the privacy issue from the aspect of similarity relevance and scheme robustness and then prove server-side ranking based on order-preserving encryption (OPE) inevitably leaks data privacy. In order to solve this problem, we propose a two round searchable encryption (TRSE) scheme, supporting top-k multi-keyword search, in which novel technologies, i.e., homomorphic encryption and vector space model, are employed. Vector space model helps to provide sufficient search accuracy, and homomorphic encryption enables users involve in the ranking while majority of computing work is still done on server-side by operations only on ciphertext. In this way, information leakage can be eliminated and data security is ensured. Thorough security analysis and performance analysis show that the proposed scheme guarantees high security and practical efficiency.
[privacy-aware multikeyword top-k search, searchable symmetric encryption, privacy issue, Conferences, OPE, two round searchable encryption scheme, query processing, cloud, similarity relevance, untrusted data cloud, cloud computing, search problems, ciphertext, SSE, server-side ranking, vector space model, cryptography, TRSE scheme, homomorphic encryption, homo- morphic, information leakage, data security analysis, data privacy, order-preserving encryption, performance analysis]
An Efficient Geometry Data Allocation Algorithm in Cloud Computing Environments
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
The number of location-based services is growing and developing. Usually, these services put a huge amount of effort into geometry data computation. Thus, their workload is generally high. By exploring cloud computing techniques, one could utilize a number of computing nodes to distribute the workload of the systems. However, the workload is usually not equally balanced across computing nodes, if data is not well distributed. To make the best use of computing nodes, we propose a sophisticated data distribution technology for geometry computation processing. Intuitively, one can simply divide geometry data into tiles so that the geometry data in each tile can be stored on one computing node. Unfortunately, since data in a tile shares spatial-proximity, processing a geometry computation on spatial proximity data still incurs a huge workload. To address this issue, we propose a new data distribution approach, Reversed K-means, to distribute geometry data that shares spatial-proximity across different computing nodes. In this way, we can use more computing nodes to process geometry computation and get better performance. To evaluate the performance of our proposed algorithm, we evaluate the utility of computing nodes and the response time when performing geometry computations. The experimental results show that the utility of the computing nodes is higher than existing methods, and the response time is the fastest of all methods.
[computing nodes, Cloud computing, Roads, Scalability, geometry data computation, performance evaluation, reversed K-means approach, Data allocation, Geometry computation, Geometry, cloud computing environments, geometry data allocation algorithm, Tiles, spatial data structures, Distributed databases, system workload distribution, geometry computation processing, location-based services, data handling, data distribution technology, Resource management, cloud computing, software performance evaluation, spatial-proximity data]
Elasticity Controller for Cloud-Based Key-Value Stores
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Clouds provide an illusion of an infinite amount of resources and enable elastic services and applications that are capable to scale up and down (grow and shrink by requesting and releasing resources) in response to changes in its environment, workload, and Quality of Service (QoS) requirements. Elasticity allows to achieve required QoS at a minimal cost in a Cloud environment with its pay-as-you-go pricing model. In this paper, we present our experience in designing a feedback elastically controller for a key-value store. The goal of our research is to investigate the feasibility of the control theoretic approach to the automation of elasticity of Cloud-based key-value stores. We describe design steps necessary to build a feedback controller for a real system, namely Voldemort, which we use as a case study in this work. The design steps include defining touchpoints (sensors and actuators), system identification, and controller design. We have designed, developed, and implemented a prototype of the feedback elasticity controller for Voldemort. Our initial evaluation results show the feasibility of using feedback control to automate elasticity of distributed key-value stores.
[Actuators, Cloud computing, Elasticity, feedback elasticity controller design, feedback, actuators, identification, Cloud Computing, QoS, control theoretic approach, cloud environment, Benchmark testing, Sensors, quality of service requirements, Mathematical model, cloud computing, system identification, control engineering computing, Monitoring, pay-as-you-go pricing model, Feedback Control, distributed key-value store, control system synthesis, touchpoints, quality of service, elastic services, sensors, Key-Value Store, cloud-based key-value store, Voldemort]
A Parallel H.264 Encoder with CUDA: Mapping and Evaluation
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Efficient mapping of a real-time HD video application to graphics hardware is challenging. Developers face the challenges of choosing the right parallelism model, balancing thread's process granularity between massive computing resources on the GPU, and partitioning tasks between the CPU and GPU. The paper illustrated the mapping approaches by a case of HD H.264 encoder based on X264 reference code and then evaluating it on state-of-the-art CPU and GPUs in depth. In the paper, we first split most of the computing task into Single-Instruction Multiple-Thread (SIMT) kernels, which are then chained intocertaininput/output data stream. Then we implementeda completed H.264 encoding on the computer unified device architecture (CUDA) platform. Finally, we present methods for exploiting multi-level parallelism and memory efficiency when mapping H.264 code, which we use to increase the efficiency of the execution on GPUs. Our experimental results show that computation efficiency of GPU and then real-time encoding performance are achieved with CUDA.
[real-time HD video application, memory efficiency, Instruction sets, parallel architectures, Graphics processing units, CPU, thread process, parallel processing, GPU, Parallel Algorithm, Real Time Encode, High-performance Media Computing, Parallel processing, graphics hardware, SIMT, Kernel, X264 reference code, Filtering, parallel H.264 encoder, computer unified device architecture, Encoding, graphics processing units, CUDA, single instruction multiple thread kernels, Video Processing, GPU Programming, Streaming media, multilevel parallelism]
CUDA Acceleration of 3D Dynamic Scene Reconstruction and 3D Motion Estimation for Motion Capture
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Tracking of 3D human body movement from multiple camera video streams is an important problem in the domain of computer vision. In this paper we perform body pose tracking in 3D space using 3D data reconstructed at every frame. We present an efficient GPU-based method for 3D reconstruction of the real world dynamic scenes. Besides volumetric reconstruction, we propose to compute view-independent 3D optical flow (i.e., scene flow) in combination with volumetric reconstruction, and have attained efficient scene flow estimation using GPU acceleration. Body pose estimation starts from a deterministic prediction based on scene flow, and then uses a multi-layer search algorithm involving stochastic search and local optimization. We design and parallelize the PSO-based (particle swarm optimization) stochastic search algorithm and 3D DT (distance transform) computation of the pose estimation method on GPU. To the end, our system can reach efficient and robust body pose tracking.
[view-independent 3D optical flow, Tracking, particle swarm optimization stochastic search algorithm, Instruction sets, parallel architectures, particle swarm optimisation, Graphics processing units, transforms, Scene Flow, multi-layer search algorithm, GPU, Image reconstruction, scene flow estimation, 3D DT computation, real world dynamic scenes, GPU acceleration, 3D human body movement, volumetric reconstruction, GPU-based method, body pose estimation method, pose estimation, motion estimation, PSO-based stochastic search algorithm, camera video streams, image sequences, search problems, 3D distance transform computation, Estimation, body pose tracking, Optical imaging, image reconstruction, local optimization, deterministic prediction, graphics processing units, 3D motion estimation, Volumetric Reconstruction, 3D space, CUDA, computer vision, Cameras, 3D dynamic scene reconstruction, Markerless Motion Capture, CUDA acceleration]
DirectedPush - A High Performance Peer-to-Peer Live Streaming System Using Network Coding
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Network coding technique can improve the efficiency of P2P live streaming systems, while it may also bring some extra delays and computational overhead. In this paper, we present Directed Push, a high performance P2P live streaming system based on network coding. In Directed Push, the original video is encoded into a number of coded sub-streams with improved content diversity. Peers are organized into a two-layer overlay and use an efficient push algorithm to deliver the coded sub-streams. The proposed directed push algorithm can reduce the packet delay as well as the coding overhead. Meanwhile, the adoption of network coding technique makes the system robust to peer dynamics. Experimental results show that our system has demonstrated superior performance compared with previous approaches, especially under the challenging flash crowd scenarios.
[network coding, two-layer overlay, peer-to-peer computing, overlay multicast, Peer to peer computing, video encoding, Encoding, content diversity, video coding, Delay, network coding technique, computational overhead, delay, high performance peer-to-peer live streaming system, delays, packet delay reduction, Bandwidth, Streaming media, Network coding, substream coding, Peer-to-peer network, Robustness, video streaming, directed push algorithm]
CPDID: A Novel CDN-P2P Dynamic Interactive Delivery Scheme for Live Streaming
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Although many streaming application providers have relied on CDN services, there are several barriers to making CDN a more common service: expensive construction cost and fixed service mode. The rise of cloud computing requires "CDN as a Service" with a more open service mode, which requires content services available on-demand and be utilized in an open and loosely-coupled fashion. In most of the current streaming systems, CDN provides serve requesters with a passive and static state, there is no dynamic interaction with P2P systems, so the total CDN-P2P-Hybrid efficiency is not high. In this paper, we present CPDID: a CDN-P2P Dynamic Interactive Delivery scheme for Live Streaming. We explore CPDID architecture based on REST interface and JSON message format. And then we propose an identifying and selecting 'upload amplification nodes' algorithm to more efficiently utilize CDN resource. Our experimental results show that CPDID achieves at least 10-25% performance improvement compared with the existing native CDN-P2P-Hybrid schemes. At last, we analyze the prospective research direction and propose our future work.
[novel CDN-P2P dynamic interactive delivery scheme, fixed service mode, construction cost mode, peer-to-peer computing, Conferences, CPDID, live streaming, CDN, Live Streaming, CDN services, Cloud Computing, VoD Streaming, media streaming, CDN-P2P-Hybrid Architecture, cloud computing, P2P Streaming]
EODS: An Energy-efficient Online Decision Scheme in Delay-sensitive Sensor Networks for Rare-event Detection
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
In many applications of WSNs, the events occur infrequently but once they occur, the corresponding information needs to be sent to the sink node in a short period of time for the necessary reactions. Detection of rare events in a fast and energy-efficient manner is an important issue in WSNs. In this paper, based on the optimal solution for the Best-choice Problem with Bounded Random Observation Number, we propose an Energy-efficient Online Decision Scheme (EODS) to handle this problem. Combining with the design of nodes' duty cycle, the EODS avoids the redundant transmissions and achieves the tradeoff between delay and energy efficiency. Simulation results reveal that the EODS achieves a good balance between delay and energy efficiency.
[duty-cycled WSNs, Energy consumption, wireless sensor networks, best-choice problem, energy-efficient manner, Educational institutions, Probability distribution, optimal solution, Delay, redundant transmissions, Wireless sensor networks, energy-efficient, WSN, Surveillance, delays, energy-efficient online decision scheme, delay-sensitive sensor networks, energy conservation, EODS, Sensors, bounded random observation number, rare-event detection, online decision]
LogA: Concurrent Medium Access Control through Time Log Analysis in Sensor Networks
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
This paper focuses on the design of high-throughput MAC for data-intensive sensor networks with unpredictable traffic. We propose Log A, a reactive concurrent MAC protocol that increases transmission concurrency based on time log analysis. Each node reactively and passively learns the interference relationship by analyzing the start transmission time and the end transmission time of packet blocks. Then, the learned interference relationship is exploited to improve the probability of beneficial concurrent transmissions of nodes that are within the interference range of each other. Log A has two salient features. First, it is passive and does not need network downtime to build interference relationship. Second, it is reactive and works only when traffic are generated. Log A has been implemented in Tinyos-2.1 and extensively evaluated in TOSSIM, the simulator of sensor networks. Experimental result shows that Log A outperforms the traditional CSMA protocol and an existing reactive concurrent MAC in terms of throughput, delivery latency, and energy consumption.
[wireless sensor networks, interference relationship, Throughput, reactive concurrent MAC protocol, sensor networks, transmission time, access protocols, transmission concurrency, throughput, CSMA protocol, interference range, Interference, Receivers, medium access control, packet blocks, TOSSIM, Vectors, time log analysis, interference (signal), delivery latency, Wireless sensor networks, Media Access Protocol, LogA, concurrent transmissions, interference, Tinyos-2.1, telecommunication traffic, carrier sense multiple access]
Efficient Cache Discovery for Cooperative Caching in Wireless Ad Hoc Networks
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Cooperative caching is especially desirable for wireless ad hoc networks to achieve efficient data access. Although studies have been conducted on cooperative cache placement and cache consistency, the cache discovery problem has been largely ignored. In this paper, we propose a Voronoi Diagram (VD) based cooperative cache discovery approach to reduce data access cost by limiting the cache information update and query within a single Voronoi Region (VR). By addressing several challenging issues caused by the lack of geometrical coordinate in ad hoc networks, we design two topological VD algorithms for different network environments. To our knowledge, this is the first work for the construction of topological VD. Simulation results show that our proposed approach can get cache copies faster while incurring much lower message cost compared with existing ones.
[Algorithm design and analysis, topological VD algorithms, radio networks, cache information updating, Cooperative caching, Cooperative cache, wireless ad hoc networks, computational geometry, data access reduction, cache storage, geometrical coordinate, cooperative cache placement, cache information query, Wireless communication, query processing, Voronoi Diagram based cooperative cache discovery approach, Voronoi diagram, mobile computing, network environments, mobile ad hoc networks, Broadcasting, cache consistency, Voronoi region, message cost, ad hoc network, Educational institutions, Ad hoc networks, cooperative communication, cache discovery, cooperative caching]
SIEVE: A Distributed, Accurate, and Robust Technique to Identify Malicious Nodes in Data Dissemination on MANET
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
In this paper we consider the following problem: nodes in a MANET must disseminate data chunks using rateless codes but some nodes are assumed to be malicious, i.e., before transmitting a coded packet they may modify its payload. Nodes receiving corrupted coded packets are prevented from correctly decoding the original chunk. We propose SIEVE, a fully distributed technique to identify malicious nodes. SIEVE is based on special messages called checks that nodes periodically transmit. A check contains the list of nodes identifiers that provided coded packets of a chunk as well as a flag to signal if the chunk has been corrupted. SIEVE operates on top of an otherwise reliable architecture and it is based on the construction of a factor graph obtained from the collected checks on which an incremental belief propagation algorithm is run to compute the probability of a node being malicious. Analysis is carried out by detailed simulations using ns-3. We show that SIEVE is very accurate and discuss how nodes speed impacts on its accuracy. We also show SIEVE robustness under several attack scenarios and deceiving actions.
[telecommunication security, factor graph, belief propagation algorithm, Peer to peer computing, data dissemination, Decoding, distributed accurate robust technique, malicious node iden- tification, SIEVE, Equations, malicious nodes, decoding, Mobile ad hoc networks, Accuracy, MANET, mobile ad hoc networks, ns-3, data chunks, Robustness, statistical inference, coded packet, Payloads]
A Memory Access Model for Highly-threaded Many-core Architectures
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Many-core architectures are excellent in hiding memory-access latency by low-overhead context switching among a large number of threads. The speedup of algorithms carried out on these machines depends on how well the latency is hidden. If the number of threads were infinite, then theoretically these machines should provide the performance predicted by the PRAM analysis of the programs. However, the number of allowable threads per processor is not infinite. In this paper, we introduce the Threaded Many-core Memory (TMM) model which is meant to capture the important characteristics of these highly-threaded, many-core machines. Since we model some important machine parameters of these machines, we expect analysis under this model to give more fine-grained performance prediction than the PRAM analysis. We analyze 4 algorithms for the classic all pairs shortest paths problem under this model. We find that even when two algorithms have the same PRAM performance, our model predicts different performance for some settings of machine parameters. For example, for dense graphs, the Floyd-Warshall algorithm and Johnson's algorithms have the same performance in the PRAM model. However, our model predicts different performance for large enough memory-access latency and validates the intuition that the Floyd-Warshall algorithm performs better on these machines.
[Algorithm design and analysis, multiprocessing systems, random-access storage, Instruction sets, Computational modeling, memory access latency, Phase change random access memory, PRAM, machine parameters, memory access model, Analytical models, PRAM analysis, All Pairs Shortest Paths (APSP), TMM, Hidden Markov models, computer architecture, Prediction algorithms, highly threaded manycore architectures]
An Improvement of OpenMP Pipeline Parallelism with the BatchQueue Algorithm
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
In the context of multicore programming, pipeline parallelism is a solution to easily transform a sequential program into a parallel one without requiring a whole rewriting of the code. The OpenMP stream-computing extension presented by Pop and Cohen proposes an extension of OpenMP to handle pipeline parallelism. However, their communication algorithm relies on Multiple-producer-Multiple-Consumer queues, while pipelined applications mostly deal with linear chains of communication, i.e., with only a single producer and a single consumer. To improve the performance of the OpenMP stream-extension, we propose to add a more specialized Single-Producer-Single-Consumer communication algorithm called Batch Queue and to select it for one-to-one communication. Our evaluation shows that Batch Queue is then able to improve the throughput up to a factor 2 on an 8-core machine both for example application and real applications. Our study shows therefore that using specialized and efficient communication algorithms can have a significant impact on the overall performance of pipelined applications.
[message passing, inter-core communication, Multicore processing, multiprocessing programs, Scalability, Pipelines, OpenMP, single producer single consumer communication algorithm, Throughput, multi-core systems, Synchronization, multicore programming, multiple producer multiple consumer queues, OpenMP stream-computing extension, OpenMP pipeline parallelism, Parallel processing, Benchmark testing, cache coherency, pipeline processing, sequential program, BatchQueue algorithm, Pipeline parallelism]
Reconfiguration Algorithms for Degradable VLSI Arrays with Switch Faults
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
The problem of reconfiguring two-dimensional VLSI arrays with faults is to find a maximum logical array without faults. The existing algorithms only consider faults associated with processing elements, and all switches and links are assumed to be fault-free. But switch faults may often occur in the network-on-chips with high density. In this paper, two novel approaches are proposed to tackle the reconfiguration problem of degradable VLSI arrays with switch faults. The first approach extends the well-known existing algorithm with simple pre-processing and row bypass scheme. The second one employs a novel row and column rerouting scheme to maximize the size of the logical array. Simulation results show that the proposed two approaches can effectively generate the logical arrays on the given host array with switch faults, and the second algorithm performs more favorably with the increasing number of the switch faults.
[logical array, network-on-chip, fault tolerance, reconfiguration algorithms, network routing, VLSI, 2D VLSI arrays, switch faults, Switches, Very large scale integration, degradable VLSI arrays, Routing, NP-complete, Circuit faults, Indexes, column rerouting scheme, Reconfiguration algorithms, Fault-tolerance, Arrays, row bypass scheme, Degradable VLSI array, Logic arrays]
Non-Backtracking Reconfiguration Algorithm for Three-dimensional VLSI Arrays
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Fast reconfiguration is one of the main challenges in fault tolerant VLSI arrays. In these arrays, there are some invalid processing elements (PEs) that are fault-free but cannot be used to form a target array. These invalid PEs lead to backtracking in reconfiguration. This paper proposes a non-backtracking reconfiguration (NBR) algorithm for three-dimensional degradable VLSI array with faults. The proposed algorithm accelerates the reconfiguration without loss of harvest, by eliminating the backtracking operation that frequently occurs in the existing algorithm (named as BGPR) cited in this paper. Initially, the invalid PEs are identified in the preprocessing for the host array. Then NBR algorithm constructs each logical plane from bottom to top in the host array, and updates the set of the invalid PEs in the host array after a logical plane is constructed. Experimental results show that the NBR algorithm is more scalable than the BGPR algorithm, and thus it can reconfigure large host arrays much faster. In addition, the runtime of NBR algorithm tends to decrease, rather than increase as did in BGPR algorithm, with the increasing fault density.
[nonbacktracking reconfiguration algorithm, fault tolerance, VLSI, Switches, processing element, Very large scale integration, reconfiguration, Indexes, fault tolerant VLSI array, fault density, Fault tolerance, Runtime, Fault tolerant systems, PE, NBR algorithm, backtracking operation elimination, fault tolerant, three-dimensional integrated circuits, 3D VLSI array, backtracking, three-dimensional degradable VLSI array, non-backtracking algorithm, Logic arrays, BGPR algorithm]
LVMCI: Efficient and Effective VM Live Migration Selection Scheme in Virtualized Data Centers
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Virtualization can provide significant benefits in virtualized data centers by enabling efficient and effective live migration to ensure service level agreement(SLA). Most of existing studies make decision on which bad virtual machines (VMs) should be migrated to which appropriate physical machines (PMs) in terms of resource utilizations. However, migration actions may degrade migrated application performance due to extra CPU and bandwidth consumptions. Furthermore, negative performance interferences amongst applications scheduled to the same PM may arise given the poor performance isolations of VMs on a PM. We design and implement a VM migration selection system with less migration costs and application performance interferences, called LVMCI (Live Virtual machine Migration with less Costs and application Interference). We propose a migration cost evaluation model to analyze quantitatively the aspects (i.e. throughput and response latency) of application performance degradation. Dirty rate and frequent dirty rate are two key factors that affect iteration time and downtime. We implement a tool that measures these parameters before VMs are migrated. We distinguish the performance degradation of migrated applications caused by memory iteration phase and stop-and-copy phase, which helps to select VM migrated. Besides that, we propose a performance interference model which helps to select the destination PM. The experimental results show that our system can estimate memory iteration time and downtime with high accuracy, and ensures a high level of SLAs by minimizing performance degradation during migration process and performance interference among co-located VMs at the destination PM.
[SLA, contracts, Degradation, Live Migration, Bandwidth, efficient VM live migration selection scheme, Monitoring, virtualized data centers, effective VM live migration selection scheme, live virtual machine migration, Interference, performance evaluation, Vectors, Virtual machining, computer centres, service level agreement, bandwidth consumptions, Performance Interference, Virtualized Data Center, physical machines, virtual machines, Resource management, PM, LVMCI, Migration Cost, Virtual Machine]
iROW: An Efficient Live Snapshot System for Virtual Machine Disk
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
The high-availiablity of mission-critical data and services hosted in a virtual machine (VM) is one of the top concerns in a cloud computing environment. The live disk snapshot is an emerging technology to save the whole state and the data of a VM at a specific point of time, and be used for quick disaster recovery. However, the existing VM disk snapshot systems suffer from long operation time and I/O performance degradation problems during snapshots creating and managing, and thereby affecting the performance of the VM and its services. To address such issues, we designed an efficient VM disk snapshot system, named iROW (improved Redirect-on-Write). In iROW, a bitmap based light-weight index scheme is adopted to replace the existing multi-level index tree structure to reduce query cost. Additionally, through a combination of Redirect-on-Write (ROW) and Copy-on-Demand (COD) schema to avoid extra copy operation on the first write after snapshot with Copy-on-Write (COW) schema, and the file fragmentation problem caused by ROW snapshot after long-term using. Finally, iROW gives a unified disk space allocation function by the host machine's file system. We have implemented iROW in qemu-kvm 0.12.5 and conducted some experiments. The implementation of iROW completely obey the interfaces of the block device driver in QEMU, so it is transparent to the upper system or applications and original disk image formats can be also supported. The experimental results show that iROW has obvious performance advantages in snapshot creating and management operations. Compared with the existing qcow2 disk image in KVM, when the VM disk size is 50GB, and the cluster size is 64KB (the default cluster size of qcow2), the snapshot creation and rollback time is only about 6% and 3% of original qcow2's. With the increasing of the VM disk size, iROW has more performance advantages on snapshot creation and rollback operations. In addition, the I/O performance of iROW is better than qcow2. When the cluster size is 64 KB, typically the iROW's performance loss is 10% less than qcow2's, and its first write performance after snapshot creation is about 250% of qcow2's.
[Performance evaluation, Computers, iROW, VM disk size, quick disaster recovery, disc storage, Virtual Disk Image, host machine file system, improved redirect-on-write schema, bitmap based light-weight index scheme, Degradation, unified disk space allocation function, File systems, Cloud Computing, ROW schema, memory size 64 KByte, VM disk snapshot system, snapshot creation, query cost reduction, QEMU, cloud computing, block device driver, I/O performance degradation problems, software performance evaluation, disk image formats, COD schema, file fragmentation problem, I/O performance, Virtual machining, Indexes, copy-on-demand schema, cloud computing environment, Snapshot, mission-critical services, mission-critical data, rollback operations, virtual machine disk, virtual machines, memory size 50 GByte, file organisation, live disk snapshot system, Reliability, cluster size, Virtual Machine]
A Scalable Signalling Mechanism for VM Migration with SR-IOV over Infiniband
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Single Root I/O Virtualization (SR-IOV) is a promising I/O virtualization approach for achieving high performance in the virtualization over InfiniBand (IB) network. One challenge is related to the hardware address assignment for each virtual IB device. There are two schemes for the hardware address assignment, static assignment and dynamic assignment. Static assignment always preserves the hardware address of a virtual IB device that is attached to a VM, but the dynamic assignment does not. A drawback, however, using static assignment is that its communication will be disconnected after VM migration. In this paper, we point out the problem related to SR-IOV over IB that breaks the network connections after VM migration when the static assignment is deployed. Then, we propose a signalling mechanism that can maintain the network connectivity after VM migration. The performance evaluation using an experimental test bed shows that the proposed signalling mechanism does not increase the service downtime during hot migration. We also optimize the signalling method, where the same event can only be forwarded to a physical server once regardless of the hosted VMs, to reduce the management message overhead from O(n*m) to O(n).
[Performance evaluation, Xen, static assignment, Scalability, Ports (Computers), single root I/O virtualization, VM Migration, virtualisation, scalable signalling mechanism, Servers, service downtime, Infiniband, physical server, network connections, network connectivity, Subnet Manager, Hardware, virtual IB device, message passing, virtualization, Architecture, InfiniBand network, performance evaluation, LAN interconnection, IPoIB, Indexes, computer network performance evaluation, hardware address assignment, peripheral interfaces, dynamic assignment, virtual machines, SR-IOV, IB network, management message overhead reduction, Virtualization, VM migration]
AutoTunium: An Evolutionary Tuner for General-Purpose Multicore Applications
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Today's increasing diversity in multicore hardware challenges programmers when it comes to software performance optimization and portability. As multicore processors are in almost every PC and server, programmers now have to parallelize a larger spectrum of applications, many of which are non-numerical. To obtain good performance, programmers typically try out different software tuning parameter configurations on each platform. However, this manual approach to finding good configurations in the search space is impractical due to combinatorial explosion, but yet it is common practice due to lack of alternatives for general programs. This paper presents a smarter way to tackle this problem algorithmically for a variety of multicore applications, including non-numerical ones. Our work introduces Auto Tunium, a novel feedback-directed optimizer that automates the application tuning process with evolutionary search strategies. The software infrastructure is easy to use and integrated in the popular Eclipse environment. It collects run-time information to predict parameter configurations that are likely to lead to good performance in future runs, and configures programs for production runs in the best possible way. We quantify the effectiveness of various tuning strategies on a diverse set of real applications and multicore platforms. The evaluation shows that Auto Tedium's evolutionary strategies work well despite the broad scope of applications and perform better in this context than other simplex-based search algorithms. Our insights are derived from model-based analyses as well as from performance analyses with real programs in the PARSEC benchmark suite.
[AutoTunium, general-purpose multicore applications, software infrastructure, multicore hardware, software performance optimization, combinatorial explosion, feedback-directed optimizer, Optimization, parallel programming, portability, Analytical models, evolutionary tuner, Sociology, application tuning process, search space, search problems, software performance evaluation, automatic programming, Eclipse environment, multiprocessing systems, Multicore processing, performance tuning, software tuning parameter configurations, run-time information, Vectors, parallel applications, model-based analysis, Particle swarm optimization, Tuning, Multicore, evolutionary computation, software portability, PARSEC benchmark suite, evolutionary search strategies, performance analysis]
Hardware-aware Thread Scheduling: The Case of Asymmetric Multicore Processors
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Modern processor architectures are increasingly complex and heterogeneous, often requiring solutions tailored to the specific characteristics of each processor model. In this paper we address this problem by targeting the AMD Bulldozer processor as case study for specific hardware-oriented performance optimizations. The Bulldozer architecture features an asymmetric simultaneous multithreading implementation with shared floating point units (FPUs) and per-core arithmetic logic units (ALUs). Bulld Over, presented in this paper, improves thread scheduling by exploiting this hardware characteristic to increase performance of floating point-intensive workloads on Linux-based operating systems. Bulld Over is a user-space monitoring tool that automatically identifies FPU-intensive threads and schedules them in a more efficient way without requiring any patches or modifications at the kernel level. Our measurements using standard benchmark suites show that speedups of up to 10% can be achieved by simply allowing Bulld Over to monitor applications, without any modification of the workload.
[floating point-intensive workloads, Instruction sets, parallel architectures, asymmetric processors, asymmetric simultaneous multithreading implementation, hardware-oriented performance optimization, ALU, asymmetric multicore processors, floating point arithmetic, processor scheduling, multicore, hardware characteristics, shared floating point units, Benchmark testing, workload characterization, Hardware, Monitoring, per-core arithmetic logic units, Bulldozer processor architecture, multiprocessing systems, multi-threading, Radiation detectors, user-space monitoring tool, performance evaluation, Linux-based operating systems, FPU-intensive threads, hardware-aware thread scheduling, performance, Linux, AMD Bulldozer processor, Land vehicles]
Topology Virtualization for Throughput Maximization on Many-Core Platforms
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
As transistor's feature size continues to scale down into the deep sub-micron domain, IC chip performance variation caused by manufacturing process becomes un-negligible and can cause significant discrepancies between an application's nominal design and its actual realization on individual manycore platforms. In this paper, we study the problem on how to reduce the total schedule length of a task graph when realizing its nominal design on individual Network-on-Chip(NoC) based many-core platform with faulty cores. Different from traditional approaches to re-define the mapping/scheduling decisions in the nominal design, our methods judiciously mirror the physical architecture of each individual platform to the logical platform, based on which the nominal design is conducted. To facilitate the phyical/logic architecture virtualization, we develop a performance metric based on the opportunity cost, a concept borrowed from the economics field. Three virtualization heuristics are presented in this paper. Our experimental results show that the proposed approach can achieve up to 30% with an average 15% performance improvement by taking advantage of the heterogeneity of each individual platform.
[Measurement, performance yield, virtualisation, physical logic architecture virtualization, topology virtualization, performance metric, Program processors, optimisation, integrated circuit design, manycore platforms, logic architecture virtualization, task graph, multiprocessing systems, network-on-chip, logical platform, Multicore processing, throughput maximization, virtualization, topology, IC chip performance variation, Network-on-Chip, performance evaluation, Topology, multi-core/many-core, NoC design, process variations, manufacturing process, nominal design, Transistors, Virtualization]
Capturing Tag Dynamics by Prediction for Pervasive Internet-of-Things Applications
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Efficient detection of RFID-tagged physical objects is one of the key enabling technologies to build pervasive Internet-of-Things applications. However, the detection of tagged-objects is faced with the critical challenge of tag dynamics, which mainly arises from the movement of tagged physical objects. To capture tag dynamics, the application needs to detect the presence/absence of tags in an accurate, timely and cost-effective way. To address these challenges, we propose the Prediction of Tag Dynamics (PTD) algorithm. PTD achieves runtime detection of tagged-objects by i) streaming of the temporally-correlated tag readings obtained from persistent tracking of the tagged-object, and ii) runtime prediction of tag dynamics based on the streaming of tag readings. The performance of PTD is investigated based on real implementation and experimental evaluation, where PTD processes tag readings gathered with high fidelity from persistent tracking of real activities of tagged objects. The evaluation results demonstrate the accuracy, timeliness and cost-effectiveness of PTD.
[pervasive Internet-of-things applications, PTD, Tracking, radiofrequency identification, Heuristic algorithms, information retrieval, Jitter, RFID, Noise measurement, Internet of Things, tagged-objects, Delay, RFID-tagged physical objects, Accuracy, tag readings, prediction, prediction of tag dynamics algorithm, tag dynamics, Radiofrequency identification]
BEST: A Bidirectional Efficiency-Privacy Transferable Authentication Protocol for RFID-Enabled Supply Chain
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Radio Frequency Identification (RFID) technique is gaining increasing popularity in supply chain for the product management. By attaching a tag to each product, a reader can employ an authentication protocol to interrogate the tag's information for verification, which facilitates the automatic processing and monitoring of products in many applications. However, most current solutions cannot be directly used as they cannot balance the tradeoff between the privacy and efficiency for individual parties. In this paper, we design a bidirectional efficiency-privacy transferable (BEST) authentication protocol to address this issue. In a relatively secure domain, BEST works in an efficient manner to authenticate batches of tags with less privacy guarantee. Once the tags flow into open environment, BEST can migrate to provide stronger privacy protection to the tags with moderate efficiency degradation. The analytic result shows that BEST can well adapt to the RFID-enabled supply chain.
[Protocols, cryptographic protocols, radiofrequency identification, supply chains, Supply chains, radio frequency identification technique, automatic processing, RFID, RFID-enabled supply chain, Communication efficiency, Supply chain, Privacy, products monitoring, privacy protection, Authentication, Bismuth, authentication protocol, bidirectional efficiency-privacy transferable authentication protocol, BEST authentication protocol, Radiofrequency identification]
Placement of Multiple RFID Reader Antennas to Alleviate the Negative Effect of Tag Orientation
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
One primary issue which hinders large-scale Radio Frequency Identification (RFID) applications is the imperfect read rate. According to Friis equation and experiments, we find that the dipole tag's orientation plays a significant role in read performance both in theory and practice. In this paper, we consider how to deploy multiple reader antennas to alleviate the negative effect caused by uncontrollable tag orientations in item-level applications. Different from previous work, we expand the candidate antenna positions from the portal to real three dimensional space and establish two estimation functions based on sphere coverage models under different polarized environment. Meanwhile, we provide an improved enumeration scheme with leaping and layering search strategies based on sphere quadtree (SQT) model to find optimized deployment.
[Polarization, radiofrequency identification, sphere coverage models, multiple reader antennas, sphere coverage, dipole antenna arrays, leaping search, reader antenna, layering search strategy, polarization, Mathematical model, enumeration, tag orientation, deployment, antenna positions, RFID, Equations, dipole tag orientation, sphere quadtree model, Directive antennas, estimation functions, Belts, quadtrees, Friis equation, Radiofrequency identification]
Leveraging Cloud Infrastructure for Troubleshooting Edge Computing Systems
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Modern cloud-based applications (e.g., Face book, Dropbox) serve a wide range of edge clients (e.g., laptops, smart phones). The clients' characteristics vary significantly in terms of hardware (e.g., high end desktop vs. resource constrained smart phones), operating systems (e.g., Linux, Android, Mac OS, Windows), network connections (e.g., wireless vs. wired, 3G vs. 2G), and software versions (e.g., Firefox 12 vs. Firefox 13), just to name a few. Unfortunately, due to misconfiguration, outdated software, faulty hardware, or other reasons, many edge systems operate at suboptimal performance. Poor performance and root cause identification is extremely challenging for the client of the cloud system. To address this challenge, the troubleshooting service presented in this paper leverages such heterogeneity to identify and debug performance problems on edge devices. First, by looking at many runs across many different clients, the service groups clients in different clusters based on performance. Next, the service enables logging on remote clients to collect run time traces, and subsequently identifies the root cause by analyzing logs automatically. We leverage high level features such as machine/OS type along with more low level kernel level statistics such as I/O rate and system calls. To demonstrate our system we first introduce a configuration bug that was artificially injected in a recently built cluster by changing the TCP buffer size. Next, we present two real-life bugs, one I/O inefficiency bug relating to network transfers on Android, and another misconfiguration bug in VirtualBox, that were identified using our tool.
[Performance evaluation, Windows, Servers, edge clients, Engines, distributed computing, network connections, system calls, cloud computing, Dropbox, Mac OS, Facebook, Monitoring, laptops, middleware, TCP buffer size, leveraging cloud infrastructure, cloud based applications, automated debugging, operating systems, smart phones, Android, Linux, Computer bugs, cloud service, Software, troubleshooting edge computing systems, Smart phones]
NOHAA: A NOvel Framework for HPC Analytics over Windows Azure
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
HPC analytics has become increasingly vital to analyze the large volumes of data produced by sophisticated computing instruments. Meanwhile, with the successful development of cloud computing, more and more scientists are devoted to deploy HPC analytics in the ever-popular clouds, which poses new challenges mainly caused by different storage architectures, resource management mechanisms and programming APIs. Firstly, there exists a ``data semantics" gap between the way data are stored by Cloud platform and the way data will be accessed by the HPC Analytics. Secondly, data are mostly distributed across data nodes for in-house data-intensive clusters to achieve co-located computation and storage, however, it is challenging for the public clouds to mimic because their data are stored centrally. In this paper, we develop a new HPC analytics framework called NOHAA, to provide 1) a semantics-aware intelligent data upload interface and 2) a locality-aware hierarchical storage system in support of co-located computation and storage on Windows Azure. Our extensive real world experiments show that NOHAA significantly reduces the average data access time by up to 85% and accelerates the HPC analytics execution time by a factor of 2 to 7.
[Co-located Computation and Storage, Cloud computing, application program interfaces, Data preprocessing, data semantic gap, parallel processing, MapReduce, mobile computing, colocated storage, Semantics, Distributed databases, cloud computing, HPC analytics, resource management mechanisms, in-house data-intensive clusters, storage architectures, Azure, Computational modeling, Hadoop, HDFS, programming API, cloud platform, colocated computation, memory architecture, HPC analytic execution time, NOHAA, Windows Azure, Data transfer, Data models, locality-aware hierarchical storage system, data nodes, semantic-aware intelligent data upload interface, Data-intensive]
WS-TaaS: A Testing as a Service Platform for Web Service Load Testing
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Web services are widely known as the building blocks of typical service oriented applications. The performance of such an application system is mainly dependent on that of component web services. Thus the effective load testing of web services is of great importance to understand and improve the performance of a service oriented system. However, existing Web Service load testing tools ignore the real characteristics of the practical running environment of a web service, which leads to inaccurate test results. In this work, we present WS-TaaS, a load testing platform for web services, which enables load testing process to be as close as possible to the real running scenarios. In this way, we aim at providing testers with more accurate performance testing results than existing tools. WS-TaaS is developed on the basis of our existing Cloud PaaS platform: Service4All. First, we briefly introduce the functionalities and main components of Service4All. Second, we provide detailed analysis of the requirements of Web Service load testing and present the conceptual architecture and design of key components. Third, we present the implementation details of WS-TaaS on the basis of Service4All. Finally, we perform a set of experiments based on the testing of real web services, and the experiments illustrate that WS-TaaS can efficiently facilitate the whole process of Web Service load testing. Especially, comparing with existing testing tools, WS-TaaS can obtain more effective and accurate test results.
[Cloud computing, testing as a service, program testing, component Web services, web services, performance evaluation, Web service load testing tools, Service4All, load testing, Runtime, WS-TaaS, Web services, service oriented system performance, cloud PaaS platform, testing as a service platform, building blocks, service oriented applications, cloud computing, Monitoring, service-oriented architecture, Testing]
An Efficient MPI Message Queue Mechanism for Large-scale Jobs
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
The Message Passing Interface (MPI) message queues have been shown to grow proportionately to the job size for many applications. With such a behaviour and knowing that message queues are used very frequently, ensuring fast queue operations at large scales is of paramount importance in the current and the upcoming exascale computing eras. Scalability, however, is two-fold. With the growing processor core density per node, and the expected smaller memory density per core at larger scales, a queue mechanism that is blind on memory requirements poses another scalability issue even if it solves the speed of operation problem. In this work we propose a multidimensional queue traversal mechanism whose operation time and memory overhead grow sub-linearly with the job size. We compare our proposal with a linked list-based approach which is not scalable in terms of speed of operation, and with an array-based method which is not scalable in terms of memory consumption. Our proposed multidimensional approach yields queue operation time speedups that translate to up to 4-fold execution time improvement over the linked list design for the applications studied in this work. It also shows a consistent lower memory footprint compared to the array-based design.
[job size, MPI message queue mechanism, application program interfaces, Scalability, memory consumption, memory requirements, MPI, Complexity theory, parallel programming, storage management, Runtime, memory overhead, large-scale jobs, message passing, queueing theory, message passing interface, memory density, 4-fold execution time improvement, Multidimensional Searches, scalability issue, processor core density, multidimensional queue traversal mechanism, array-based method, Memory management, operation time, Exascale, Arrays, Portals, exascale computing, Message Queues]
Accelerating Cost Aggregation for Real-Time Stereo Matching
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Real-time stereo matching, which is important in many applications like self-driving cars and 3-D scene reconstruction, requires large computation capability and high memory bandwidth. The most time-consuming part of stereo-matching algorithms is the aggregation of information (i.e. costs) over local image regions. In this paper, we present a generic representation and suitable implementations for three commonly used cost aggregators on many-core processors. We perform typical optimizations on the kernels, which leads to significant performance improvement (up to two orders of magnitude). Finally, we present a performance model for the three aggregators to predict the aggregation speed for a given pair of input images on a given architecture. Experimental results validate our model with an acceptable error margin (an average of 10.4%). We conclude that GPU-like many-cores are excellent platforms for accelerating stereo matching.
[Graphics processing units, GPUs, performance improvement, Optimization, self-driving cars, Image color analysis, generic representation, Cost Aggregation, real-time stereo matching, local image regions, Kernel, computation capability, Performance Optimization, graphics processing units, image matching, memory bandwidth, acceptable error margin, Performance Modeling, Memory management, Venus, stereo image processing, 3D scene reconstruction, Stereo Matching, cost aggregation acceleration, OpenCL, Acceleration, GPU-like many-cores, computational complexity]
A Non-Intrusive Read-Copy-Update for UTS
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Read-Copy-Update (RCU) is a mechanism designed to increase the level of concurrency in readers-writer synchronization scenarios, vastly improving scalability of software running on multiprocessor machines. Most existing RCU variants have been developed for and studied within the Linux kernel. Due to strong dependency on the Linux internals, they cannot be easily transferred to other operating system kernels. This paper presents a novel non-intrusive variant of the RCU mechanism (AP-RCU), which depends only on basic kernel-level concepts while maintaining the scalability benefits. We have implemented AP-RCU in the Solaris kernel (UTS) and experimentally confirmed the expected benefits over traditional forms of synchronization, comparable with previous RCU implementations.
[Context, operating system kernels, Radiation detectors, Instruction sets, nonintrusive read copy update, RCU, Solaris, Linux kernel, mechanism design, multiprocessor machines, Synchronization, scalability, read-copy-update, security of data, Solaris kernel, Linux, UTS, SMP, Linux internals, synchronization, Kernel]
Neighbor Discovery in Low-Duty-Cycle Wireless Sensor Networks with Multipacket Reception
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Neighbor Discovery (ND) plays an important role in the initialization phase of wireless sensor networks. In real deployments, sensor nodes may not always be awake due to limited power supply, which forms low-duty-cycle networks. Existing researches on the problem of ND in low-duty-cycle networks are all based on the assumption that a receiver can receive only one packet successfully at a time. k-Multipacket Reception (MPR) techniques (i.e., k (k &#x2265; 2) packets can be successfully received at a time.) have shown their significance in improving packet transmission. However, how can MPR benefit the problem of ND is still unknown. In this paper, we the first to discuss the problem of ND in low-duty-cycle networks with MPR. Specifically, we first present an ALOHA-like protocol, and show that the expected time to discover all n-1 neighbors is O(n log n log log n/k) by reducing the problem to a generalized form of the classic K Coupon Collector's Problem. Second, we show that when there is a feedback mechanism to inform a node whether its transmission is successful or not, ND can be finished in time O(n log log n/k). Third, we point out that lacking of knowledge of n results in a factor of two slowdown in two protocols above. Finally, we evaluate the ND protocols introduced in this paper, and compare their performance with the analysis results.
[Protocols, wireless sensor networks, multipacket reception, low-duty-cycle wireless sensor networks, MPR technique, ND protocol, access protocols, Multiaccess communication, coupon collector problem, feedback mechanism, sensor nodes, Neighbor Discovery, Mathematical model, packet transmission, power supply, initialization phase, receiver, neighbor discovery, Receivers, radio receivers, Wireless Sensor Networks, Low-Duty-Cycle, Equations, Wireless sensor networks, ALOHA-like protocol, Time complexity, Multipakcet Reception]
A Reconfigurable Hardware Accelerated Platform for Clustered Wireless Sensor Networks
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
With the advent of the FPGA technology, a reconfigurable platform becomes possible to enhance the capability and adaptability of wireless sensor nodes while reducing the energy consumption. In this paper, we investigate the application of reconfigurable platform in wireless sensor networks. We focus on using hardware accelerated lossless compression to reduce the energy consumption of the data aggregation in clustered networks. Our study is based on real-world measurement on our integrated reconfigurable sensor node and the measured data is then used in the network simulation with a proper channel model. Our experiments show that the use of hardware acceleration on our platform can reduce the energy consumption by up to 35%. In addition, with proper parameters of the network, the run-time reconfiguration becomes beneficial and this opens a lot of opportunities for the optimizations according to the field situations.
[Energy consumption, wireless sensor nodes, Compression, wireless sensor networks, field programmable gate arrays, FPGA, clustered wireless sensor networks, data aggregation, Image coding, Program processors, integrated reconfigurable sensor node, reconfigurable architectures, Energy, Hardware, data compression, Head, run-time reconfiguration, Cluster, hardware accelerated lossless compression, Wireless Sensor Networks, Wireless sensor networks, FPGA technology, reconfigurable hardware accelerated platform, Field programmable gate arrays, Reconfiguration]
Self-Stabilizing Micro Controller for Large-Scale Sensor Networks in Spite of Program Counter Corruptions Due to Soft Errors
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
For large installations of networked embedded systems it is important that each entity is self-stabilizing, because usually there is nobody to restart nodes that have hung up. Self-stabilization means to recover from temporary failures (soft errors) and adapt to a change of network topology caused by permanent failures. On the software side self-stabilizing algorithms must assume that the hardware is executing the software correctly. In this paper we discuss cases in which soft errors invalidate this assumption, especially in cases where CPU registers or the watchdog timer are affected by the fault. Based on the observation that a guaranteed self-stabilization is only possible as long as the watchdog-timer is working properly after temporary failures, we propose and compare three different approaches that meet the requirements of sensor networks, to solve this problem with a combination of hardware- and software-modifications: 1) A run-time verification of every watchdog access 2) A completely hardware-based approach, without any software modifications 3) A 2X byte code alignment, to realign a corrupted program counter Furthermore we determine the average code-size increase and evaluate necessary hardware-modifications that come along with each approach.
[watchdog timer, software side self-stabilizing algorithms, wireless sensor networks, run-time verification, Self-Stabilization, Random access memory, program counter corruptions, large-scale sensor networks, Registers, network topology, CPU registers, Soft Errors, Read only memory, Static Code Analysis, Embedded systems, hardware-modifications, Microprocessors, Hardware, self-stabilizing microcontroller, Embedded Systems, soft errors, software-modifications, microcontrollers]
Behavior Aware Data Locality for Caches
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Optimizing cache performance through improving data locality has been receiving a lot of attention. However, none of the existing approaches can combine each task's behavior to optimize data locality for caches. We present a behavior aware data locality (BADL) to optimize cache performance in this paper. The key idea is to add each task's behavior when allocating memory, which can take advantage of each task's different locality to optimize cache performance. There are five main contributions: 1. to our best knowledge, this is the first attempt to improve cache performance through combining task behavior, 2. BADL detailed analyzes low performance derived from internal of the cache line, which is more fine-grained than the current state-of-the-art fine-grained in hardware angle, 3. BADL optimizes the cache performance through improving internal of cache line efficiency, 4. we implement BADL both in single-threaded application and multi-threaded applications scenarios, 5. BADL can be combined to most of the cache optimizing researches. The experiment results show our proposed BADL can improve 18.6% performance on average in single-threaded application situation and improve 20.8% performance on average in multi-threaded application situation.
[storage allocation, memory allocation, Instruction sets, cache performance optimization, cache storage, behavior aware data locality, multithreaded application situation, internal cache line efficiency improvement, cache line, data locality, Distributed databases, single-threaded application situation, Benchmark testing, hardware angle, BADL, Hardware, multi-threaded, software performance evaluation, multi-threading, Cache performance, Time frequency analysis, fine-grained, single-threaded, task behavior, data handling, Electronics packaging]
SeTM: Efficient Execution of Speculative Threads with Hardware Transactional Memory
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Thread-Level Speculation (TLS) was researched to automatically parallelize portions of serial programs for execution, and transactional memory (TM) was studied as a promising alternative of lock for parallel programming due to its simplicity. Both TLS and TM require similar underlying support. In the paper, we present SeTM (Sequential Transactional Memory), a hardware enhanced TM system which supports TLS at minor extra cost. Signature is an effective way to buffer speculative states in TM and TLS. But it cripples TM and TLS performance due to its false-positive in terms of conflict detection, especially for conflict-intensive TLS. SeTM adopts R/W bits and signature concurrently to ameliorate this bad influence. Additionally, SeTM introduces fast rollback mechanism, which provides fast abort recovery for eager log-based HTM and TLS. The most important contribution of SeTM is conflict-tolerant mechanism, which tolerates some ambiguous data conflicts in TLS. Six representative benchmarks have been adopted to evaluate our model. Our experimental results show that our scheme improves the execution performance of most tested codes at a modest hardware cost. For a set of important scientific loops, we report the highest speedup of 6.5 with 15 cores. Besides, experimental results also show good scalability of SeTM system.
[Conflict-Tolerant, Protocols, efficient execution, Instruction sets, speculative threads, hardware transactional memory, SeTM, parallel programming, sequential transactional memory, serial programs, Coherence, Computer architecture, shared memory systems, Hardware Transactional Memory, Hardware, TLS, Fast Rollback, Thread-Level Speculation, Message systems]
Optimizing Scheduling in Embedded CMP Systems with Phase Change Memory
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Phase Change Memory (PCM) is emerging as one of the most promising alternative technology to the Dynamic RAM (DRAM) when building large-scale main memory systems. Even though the PCM is easy to scale, it encounters serious endurance problems. Writes are the primary wear mechanism in the PCM. The PCM can perform 108 to 109 times of writes before it cannot be programmed reliably. In addition, the PCM has high write latency. To prolong the lifetime of the PCM as the main memory and enhance the performance, we propose a Scratch Pad Memory (SPM) based memory mechanism and an Integer Linear Programming (ILP) memory activities scheduling algorithm to reduce the redundant write operations in the PCM. The idea of our approach is to share the data copies among the SPMs, instead of writing back to the PCM main memory each time a modify occurs. Our experimental results show that the ILP scheduling can generate the optimal schedule of memory activities with minimum write operations, reducing the number of write by up to 61%.
[Schedules, integer programming, data copy sharing, Random access memory, linear programming, high write latency, ILP scheduling, Optimization, Phase change materials, SPM, scratch pad memory based memory mechanism, embedded CMP systems, DRAM chips, DRAM, large-scale main memory systems, redundant write operations reduction, integer linear programming memory activities scheduling algorithm, dynamic RAM, multiprocessing systems, performance enhancement, performance evaluation, scheduling optimization, microprocessor chips, phase change memories, primary wear mechanism, Memory management, Heating, PCM, endurance problems, Clocks, phase change memory]
A Synchronization Aware Memory Race Recorder
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Memory race recording has been proved to be a hard problem in multithreaded deterministic record-replay. It is important to develop an efficient memory race recording algorithm. However, most of the prior work tries to record all memory conflicts, whether they affect deterministic replay or not, resulting a relatively large memory race log. This paper proposes an innovative synchronization aware point-to-point memory race recorder, called SAMR. SAMR analyzes memory conflicts introduced by synchronization operations and classifies them into harmful synchronization conflicts and harmless synchronization conflicts. Harmless synchronization conflicts are filtered out by identifying synchronization operations when recording, and a reduced memory race log is achieved. At the same time, SAMR reduces hardware overhead by using signatures instead of cache memory. Simulations with splash-2 workloads on 8-core CMP system show that SAMR can achieve small memory race size (~2 bytes per thousand memory instructions), good scalability in log size and low bandwidth overhead (&lt;; 5%), while not needing too much hardware state (~1129 bytes).
[point-to-point memory race recorder, multi-threading, synchronization operations, Instruction sets, shared memory multithreaded programming, Cache memory, CMP, microprocessor chips, memory race recording, Registers, Synchronization, innovative synchronization, cache memory, chip multiprocessors, 8-core CMP system, Memory management, synchronization aware memory race recorder, shared memory systems, synchronization, Hardware, multiprocessor, deterministic replay, Spinning, memory race log]
MSSM: An Efficient Scheduling Mechanism for CUDA Basing on Task Partition
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
This paper presents a multiple stream scheduling mechanism to enable parallel execution of kernels, data sending from host to device and data receiving from device to host with multiple streams in CUDA. Our mechanism can divide the kernels and bi-directional data transmission into small subtasks, and allow to easily and efficiently overlap them on the CUDA compatible graphic processing unit(GPU). To set the optimal subtask size, we have built one compute bound model for computing intensive application and one data bound model for bi-directional data transmission intensive application. Basing on the two models, we also provided three scheduling algorithms for data dependent and data independent applications to maximize the efficiency of the overlap. We have applied the mechanism to a set of benchmarks to understand the performance. The results show that our work can successfully hide the latency to achieve high performance which is very close to the optimal.
[multiple stream scheduling mechanism, subtask overlap, parallel architectures, MSSM, Graphics processing units, graphic processing unit, parallelism, Dynamic scheduling, graphics processing units, GPU, Equations, processor scheduling, CUDA, kernel data transmission, efficient scheduling mechanism, Bidirectional control, bidirectional data transmission, Stream scheduling, Data transfer, Mathematical model, Kernel, task partition]
Comparing the Performance of Blue Gene/Q with Leading Cray XE6 and InfiniBand Systems
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Three types of systems dominate the current High Performance Computing landscape: the Cray XE6, the IBM Blue Gene, and commodity clusters using InfiniBand. These systems have quite different characteristics making the choice for a particular deployment difficult. The XE6 uses Cray's proprietary Gemini 3-D torus interconnect with two nodes at each network endpoint. The latest IBM Blue Gene/Q uses a single socket integrating processor and communication in a 5-D torus network. InfiniBand provides the flexibility of using nodes from many vendors connected in many possible topologies. The performance characteristics of each vary vastly along with their utilization model. In this work we compare the performance of these three systems using a combination of micro-benchmarks and a set of production applications. We also discuss the causes of variability in performance across the systems and quantify where performance is lost using a combination of measurements and models. Our results show that significant performance can be lost in normal production operation of the Cray XE6 and InfiniBand Clusters in comparison to Blue Gene/Q.
[High Performance Computing, Switches, high performance computing landscape, supercomputers, parallel machines, Application Analysis, InfiniBand systems, Program processors, Network topology, 5D torus network, Bandwidth, Production, commodity clusters, performance characteristics, IBM blue gene-Q, proprietary Gemini 3D torus, mainframes, production applications, Cray XE6 systems, Topology, single socket integrating processor, Performance Evaluation, Performance Modeling, utilization model, microbenchmarks, Resource management]
Energy-Aware Communication and Remapping of Tasks for Reliable Multimedia Multiprocessor Systems
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Shrinking transistor geometries, aggressive voltage scaling and higher operating frequencies have negatively impacted the dependability of embedded multiprocessor systems-on-chip (MPSoCs). Fault-tolerance and energy efficiency are the two most desired features of modern-day MPSoCs. For most of the multimedia applications, task communication energy constitutes more than 40% of the overall application energy. In this paper, an integer linear programming (ILP) based approach is proposed to reduce the communication energy and fault-tolerant migration overhead of throughput-constrained multimedia applications modeled using synchronous data flow graphs (SDFGs). The ILP is solved at compile-time for all fault-scenarios to generate task-core mappings satisfying an application throughput requirement. These mappings are stored in a table which is looked up at run-time as and when faults occur. Experiments conducted with real and synthetic applications demonstrate that the proposed technique reduces communication energy by an average 40% and migration overhead by 33% as compared to the existing fault-tolerant techniques.
[integer programming, integer linear programming based approach, throughput-constrained multimedia applications, computational geometry, data flow graphs, Throughput, Fault-Tolerance, linear programming, energy-aware communication, Multimedia communication, ILP, reliable multimedia multiprocessor systems, task communication energy, Communication Energy, Fault tolerance, power aware computing, task remapping, Fault tolerant systems, Synchronous Data Flow Graph, embedded systems, energy efficiency, transistor circuits, communication energy reduction, Transient analysis, multimedia communication, Linear Programming, multiprocessing systems, higher operating frequencies, MPSoC, task-core mappings, Task Mapping, Minimization, shrinking transistor geometries, Tiles, aggressive voltage scaling, synchronous data flow graphs, SDFG, fault tolerant computing, fault-tolerant migration overhead, system-on-chip, embedded multiprocessor systems-on-chip, application throughput requirement]
Gargamel: Boosting DBMS Performance by Parallelising Write Transactions
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Parallel transactions in distributed DBs incur high overhead for concurrency control and aborts. We propose an alternative approach by pre-serializing possibly conflicting transactions, and parallelizing non-conflicting update transactions to different replicas. Our system provides strong transactional guarantees. In effect, Gargamel partitions the database dynamically according to the update workload. Each database replica runs sequentially, at full bandwidth, mutual synchronisation between replicas remains minimal. Our simulations show that Gargamel improves both response time and load by an order of magnitude when contention is high (highly loaded system with bounded resources), and that otherwise slow-down is negligible.
[write transaction parallelization, Cloud computing, response time improvement, load improvement, Throughput, Databases, DBMS performance boosting, scheduing alghorithms, Benchmark testing, scheduling, database replica, distributed DBMSes, Numerical models, mutual synchronisation, cloud computing, aborts, parallel transactions, Load modeling, replicated databases, parallel databases, conflicting transaction pre serialization, distributed DBMS, nonconflicting update transaction parallelization, concurrency control, Gargamel partitions, scheduling alghorithms, Time factors, Resource management]
Effective Caching Schemes for Minimizing Inter-ISP Traffic in Named Data Networking
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Internet has evolved to be content-oriented and its key usage focuses on content dissemination and retrieval, while Internet architecture is designed for host-oriented services. To address the challenge, Named Data Networking (NDN) has been proposed, where in-network caching becomes a new research topic due to its dominant position in NDN architecture. This work develops efficient caching schemes for Internet Service Providers (ISPs) so as to maximize the inter-ISP traffic savings. With the special goal, we design caching system according to the NDN network model and present coordinated caching algorithms which can dynamically determine cache placement along the forwarding path. Comprehensive simulation results show that our schemes outperform the widely used Leaving Copies Everywhere (LCE) both in inter-ISP traffic savings and the average number of access hops by up to 20%. In addition, we demonstrate good feasibility of the proposed caching algorithms in a set of simulations spanning a wide range of parameter values.
[Algorithm design and analysis, Innetwork caching, Popularity-based, Heuristic algorithms, Coordinated caching, Internet architecture, coordinated caching algorithms, content dissemination, parameter values, Sociology, Content-centric, NDN architecture, content retrieval, Decision making, Named Data Networking, Routing, inter-ISP traffic minimization, Internet service providers, Statistics, LCE, host-oriented services, Internet, named data networking, telecommunication traffic, leaving copies everywhere]
The Freshman Handbook: A Hint for Server Placement in Online Social Network Services
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
For new social service providers (freshmen), it is critical to determine where to deploy the computational resources to best accommodate future client requests. Existing proposals on server placement rely on collecting and analyzing request history from servers that are already running, which are not so useful to those starting new online social network services (OSNs). In this work, we aim at helping new OSN providers with intelligent server placement by exploring available public information from existing social network communities. We explore the commonality between the selected set of server locations from multiple OSNs and utilize such similarity for future OSNs to select their server locations. The similarity is ultimately due to the fact that the underlying human relationship is relatively consistent across OSNs.
[Algorithm design and analysis, server locations, intelligent server placement, online social network services, Twitter, Partitioning algorithms, Servers, OSN providers, client requests, Social Network, server placement, computational resources, Clustering algorithms, social network communities, social networking (online), Joints, human relationship, public information, freshman handbook]
An Enhanced Genetic Algorithm for Server Placement in Distributed Interactive Applications
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Recent years have witnessed the enormous popularity of distributed interactive applications (DIAs), which allow participants that are distributed in the network to interact with each other concurrently. The rapid growth of DIAs has raised stringent requirements on providing realistic sense of interaction between participants, whose quality is heavily influenced by network latencies. Although network latencies cannot be eliminated due to geographical spreads of participants, it is possible to reduce them by a smart selection of the locations where the servers of the DIAs are placed. The locations of servers affect not only the inter-server latencies but also the latencies from participants to servers, both of which are involved in the interactions among participants. Thus, the placement of servers is an important factor to the interactivity performance of DIAs. We formulate the server placement problem, and propose to solve it by an enhanced genetic algorithm, whose genetic operators are specially designed based on the nature of the problem. Experimental results using various datasets show that our algorithm leads to appreciable improvement of the interaction quality in DIAs.
[network servers, genetic algorithms, quality of service, Servers, Approximation methods, participant interaction, Biological cells, Delay, Genetic algorithms, distributed interactive applications, distributed interactive application, genetic algorithm, server placement, network latencies, DIA interactivity performance, optimization, Games, enhanced genetic algorithm, participant geographical spreads, network latency, interserver latencies]
Quality of Experience-Aware Event Synchronization for Distributed Virtual Worlds
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Distributed virtual worlds encompass a number of different possible user interactions. These are realized as interaction events that need to be reflected in all user's views in a timely and synchronous fashion. This is difficult because of two aspects: A) user requirements vary for different events and due to B) restricted system resources, mainly network delay. In this paper we present a framework that allows the processing of events using different synchronization protocols depending on the user requirements. We integrate existing protocols into the framework allowing different handling of events.
[Context, Protocols, virtual reality, experience-aware event synchronization, Avatars, Virtual environments, distributed processing, Synchronization, Delay, synchronisation, quality of experience, synchronization protocols, restricted system resources, network delay, events handling, resource allocation, distributed virtual worlds, user requirements, user interactions, protocols, event synchronization]
Page-Based Memory Allocation Policies of Local and Remote Memory in Cluster Computers
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Main memory latencies have a strong impact on the overall execution time of the applications. The need of efficiently scheduling the costly DRAM memory resources in the different motherboards is a major concern in cluster computers. Most of these systems implement remote access capabilities which allow the OS to access to remote memory. In this context, efficient scheduling becomes even more critical since remote memory accesses may be several orders of magnitude higher than local accesses. These systems typically support interleaved memory at cache-block granularity. In contrast, in this paper we explore the impact on the system performance when allocating memory at OS page granularity. Experimental results show that simply supporting interleaved memory at OS page granularity is a feasible solution that does not impact on the performance of most of the benchmarks. Based on this observation we investigated the reasons of performance drops in those benchmarks showing unacceptable performance when working at page granularity. The results of this analysis lead us to propose two memory allocation policies, namely on-demand (OD) and Most-accessed in-local (Mail). The OD policy first places the requested pages in local memory, once this memory region is full, the subsequent memory pages are placed in remote memory. This policy shows good performance when the most accessed pages are requested and allocated before than the least accessed ones, which as proven in this work, is the most common case. This simple policy reaches performance improvements by 25% in some benchmarks with respect to a typical block interleaving memory system. Nevertheless, this strategy has poor performance when a noticeable amount of the least accessed pages are requested before than the most accessed ones. This performance drawback is solved by the Mail allocation policy by using profile information to guide the allocation of new pages. This scheme always outperforms the baseline block interleaving policy and, in some cases, improves the performance of the OD policy by 25%.
[Computers, workstation clusters, memory allocation, Mail allocation policy, interleaved memory, block interleaving memory system, Postal services, Degradation, page-based memory allocation policies, remote memory, memory management, OS page granularity, remote access capabilities, Benchmark testing, Bismuth, DRAM chips, workload characterization, DRAM memory resources, profile information, baseline block interleaving policy, most-accessed in-local policy, local memory, Memory management, cache-block granularity, on-demand policy, Cluster computers, Resource management, OD policy]
OPAMP: Evaluation Framework for Optimal Page Allocation of Hybrid Main Memory Architecture
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Main memory as a hybrid between DRAM and nonvolatile memory is rapidly considered as a basic building block of computing systems. Despite widely-performed researches no one can confirm whether hybrid memory is at its full performance in terms of energy consumption, time delay or both. The main problem is that evaluating their performance in comparison with the optimal performance is challenging since deriving the optimal value is NP-complete. In this paper, we design and implement an evaluation framework termed OPAMP, which calculates optimal performance of the hybrid memory environment. This system gathers workload, specification of DRAM and PRAM, and environmental parameters of the hybrid main memory. After that, it calculates the maximum performance under the corresponding conditions. We suggest the way of deriving the optimal value by profiling instead of page migration which is the mainstream of recent researches on hybrid main memory system. Also, proportion of DRAM's size to PRAM's and proportion of DRAM's usage space to PRAM's are impactive factors. While designing hybrid main memory, those two variables must be determined carefully and OPAMP gives the guideline to the researchers.
[Energy consumption, hybrid main memory architecture, Phase change random access memory, optimal page allocation, time delay, NP-complete, PRAM, DRAM memory, Energy Efficiency, Equations, Memory management, DRAM chips, Resource management, Mathematical model, OPAMP, energy consumption, dynamic random access memory, computational complexity, evaluation framework, nonvolatile memory]
CAR: Securing PCM Main Memory System with Cache Address Remapping
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Phase Change Memory (PCM) has emerged as a promising alternative of DRAM to provide energy-efficient and high-capacity memory for high performance servers. A new DRAM + PCM hybrid memory architecture has been proposed to leverage PCM's high density and DRAM's robustness and performance. One of the big challenges of PCM is its limited write endurance (107 ~ 108 times per cell). By knowing the association between DRAM and PCM, malicious software can easily force DRAM cache to be flushed continuously, which produces writes to certain PCM cells repeatedly (known as selective attack) and wears out PCM. Although existing wear-leveling approaches could evenly distribute writes under selective attack, the overall endurance of PCM is still severely impacted, and therefore it is suboptimal. In this paper, we propose Cache Address Remapping (CAR), that can adaptively remap DRAM cache address, to hide the association between DRAM and PCM. Moreover, CAR can minimize the write-back traffic to PCM under selective attack by uniformly distributing the writes to a single cache set into different cache sets. We propose a practical and low overhead implementation of CAR, called RanCAR. Experimental results show that CAR could reduce DRAM cache miss rate by ~4600x under selective attack, and prolong PCM lifetime from several minutes to 13.8 years on average.
[PCM main memory system security, Random access memory, Phase Change Memory, Servers, write-back traffic minimization, Phase change materials, Malicious Attack, Microprocessors, low overhead implementation, Computer architecture, energy efficiency, DRAM chips, selective attack, DRAM cache address, Hardware, cache address remapping, Endurance, Hybrid Memory System, Address Remapping, cache miss rate reduction, phase change memories, Indexes, CAR, high performance servers, high-capacity memory, wear-leveling approaches, phase change memory]
An Efficient and Scalable Memory Allocator for Multithreaded Tabled Evaluation of Logic Programs
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Despite the availability of both multithreading and tabling in some Prolog systems, the implementation of these two features, such that they work together, implies complex ties to one another and to the underlying engine. In recent work, we have proposed an approach to combine multithreading with tabling, implemented on top of the Yap Prolog system, whose primary goal was to reduce memory usage for the table space. Regarding the execution times, we observed some problems related to Yap's memory allocator, which is based on the operating system's default memory allocator, when running programs that allocate a higher number of data structures in the table space. In this paper, we propose a more efficient and scalable memory allocator for multithreaded tabled evaluation of logic programs. Our goal is to minimize the performance degradation that the system suffers when it is exposed to simultaneous memory requests made by multiple threads. For that, we propose a memory allocator based on local and global pages, to split memory among specific data structures and different threads, together with a strategy where data structures of the same type are pre-allocated within a page. Experimental results show that our new memory allocator can effectively reduce the execution time and scale better, when increasing the number of threads, than the original allocator.
[logic programs, Tabling, Instruction sets, Scalability, execution time reduction, multithreaded tabled evaluation, memory allocator scalability, logic programming, data structures, PROLOG, memory usage reduction, global pages, multi-threading, Yap Prolog system, performance degradation minimization, Logic Programming, Memory Allocation, memory requests, Synchronization, operating system default memory allocator, Multithreading, Organizations, operating systems (computers), Arrays, Resource management, Performance, local pages]
Design of Energy-Efficient Video Sharing Servers with Delay Constraints
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
With the continually growing popularity of online video sharing, energy consumption in video sharing servers has become an pivotal issue. Energy savings in video servers can be achieved by utilizing low power modes in disks, but this causes enhanced delay which is not desirable for video sharing services. In this paper, we present a new energy management scheme that jointly optimizes energy and delay for the video storage system. There are two salient characteristics of the proposed scheme. First, it exploits the unique workload characteristics of video sharing services. Second, it includes a new model that efficiently facilitates the selection of power modes for disks. Experiments show that our scheme achieves considerably more energy savings under the same delay level compared to the traditional threshold-based energy management scheme.
[Energy consumption, delay constraints, threshold-based energy management scheme, energy savings, Servers, video servers, Delay, Optimization, energy management scheme, energy management, parallel video servers, Mathematical model, video signal processing, energy consumption, video storage system, disk power modes, delay level, video sharing services, Equations, low power modes, energy management systems, energy-efficient video sharing servers design, Streaming media, energy conservation, green computing]
Fine-Grained Energy-Efficient Sorting on a Many-Core Processor Array
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Data centers require significant and growing amounts of power to operate, and with increasing numbers of data centers worldwide, power consumption for enterprise workloads is a significant concern. Sorting is a key computational kernel in large database systems, and the development of energy efficient sorting capabilities would therefore significantly reduce data center power usage. We propose highly parallel sorting algorithms and mappings using a modular design for a fine-grained many-core system that greatly decreases the amount of energy consumed to perform sorts of arbitrarily large data sets. The memory, computational, and nearest-neighbor inter-processor communication hardware of the many-core processor array require relatively small die area. We present the design and implementation of several sorting variants that perform the first phase of an external sort. They are built using program kernels operating on independent processors in a many-core array with 256 bytes of data memory and fewer than 128 instructions per processor. The algorithms employed are simple and the vast majority of processors contain identical programs. Compared to a quicksort implementation on an Intel Core 2 Duo T9600 the highest throughput design achieves up to 27&#x00D7; higher throughput per chip area, and the most energy efficient sort yields a 330&#x00D7; reduction in energy dissipated per sorted block. Compared to a radix sort implementation on a GPU, the highest throughput design achieves up to 22&#x00D7; higher throughput per chip area, and the most energy efficient sort yields a 750&#x00D7; reduction in energy dissipated per sorted block.
[processor array, streaming sorting, Throughput, external sorting, large database systems, parallel processing, parallel programming, parallel sorting algorithms, computational hardware, storage management, power aware computing, enterprise workloads, modular programing, sorting, Database systems, Kernel, modular design, parallel algorithms, multiprocessing systems, computational kernel, data center power usage reduction, computer centres, nearest-neighbor interprocessor communication hardware, Sorting, many-core processor array, fine-grained energy-efficient sorting, program kernels, throughput design, Memory management, fine-grained many-core, power consumption reduction, parallel mappings, memory hardware, Arrays, Clocks, data memory]
A Runtime Framework for Energy Efficient HPC Systems without a Priori Knowledge of Applications
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
The rising computing demands of scientific endeavors often require the creation and management of High Performance Computing (HPC) systems for running experiments and processing vast amounts of data. These HPC systems generally operate at peak performance, consuming a large quantity of electricity, even though their workload varies over time. Understanding the behavioral patterns (i.e., phases) of HPC systems during their use is key to adjust performance to resource demand and hence improve the energy efficiency. In this paper, we describe (i) a method to detect phases of an HPC system based on its workload, and (ii) a partial phase recognition technique that works cooperatively with on-the-fly dynamic management. We implement a prototype that guides the use of energy saving capabilities to demonstrate the benefits of our approach. Experimental results reveal the effectiveness of the phase detection method under real-life workload and benchmarks. A comparison with baseline unmanaged execution shows that the partial phase recognition technique saves up to 15% of energy with less than 1% performance degradation.
[partial phase recognition technique, Radiation detectors, green leverage, Switches, high performance computing systems, Vectors, parallel processing, Phase detection, phase recognition, Runtime, phase detection method, Green products, energy saving capabilities, energy efficient HPC systems, energy efficiency, energy conservation, execution vector, Hardware, on-the-fly dynamic management, runtime framework]
Convexity in Non-convex Optimizations of Streaming Applications
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Streaming data applications are frequently pipelined and deployed on application-specific systems to meet performance requirements and resource constraints. Typically, there are several design parameters in the algorithms and architectures used that impact the application performance as well as resource utilization. Efficient exploration of this design space is the goal of this research. When using architecturally diverse systems to accelerate streaming applications, the design search space is often complex. The search complexity can be reduced by recognizing and exploiting convex variables to perform convex decomposition, preserving optimality even in the context of a non-convex optimization problem. This paper presents a formal treatment of convex variables and convex decomposition, including a proof that the technique preserves optimality. It also quantifies the reduction in the search space that is realized, at minimum equal to the number of distinct values of the convex variable and potentially much higher.
[performance requirements, Computational modeling, resource constraints, convex decomposition, Linear programming, convex programming, Vectors, nonconvex optimizations, Topology, Optimization, Reactive power, Network topology, convex variables, media streaming, streaming data applications, search complexity, nonconvex optimization problem, design-space exploration, search space, decomposition of queueing networks, domain-specific branch and bound, design space, search problems]
Performance Optimization of a Parallel, Two Stage Stochastic Linear Program
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Stochastic optimization is used in several high impact contexts to provide optimal solutions in the face of uncertainties. This paper explores the parallelization of two-stage stochastic resource allocation problems that seek an optimal solution in the first stage, while accounting for sudden changes in resource requirements by evaluating multiple possible scenarios in the second stage. Unlike typical scientific computing algorithms, linear programs (which are the individual grains of computation in our parallel design) have unpredictable and long execution times. This confounds both a priori load distribution as well as persistence-based dynamic load balancing techniques. We present a master-worker decomposition coupled with a pull-based work assignment scheme for load balance. We discuss some of the challenges encountered in optimizing both the master and the worker portions of the computations, and techniques to address them. Of note are cut retirement schemes for balancing memory requirements with duplicated worker computation, and scenario clustering for accelerating the evaluation of similar scenarios. We base our work in the context of a real application: the optimization of US military aircraft allocation to various cargo and personnel movement missions in the face of uncertain demands. We demonstrate scaling up to 122 cores of an intel 64 cluster, even for very small, but representative datasets. Our decision to eschew problem-specific decompositions has resulted in a parallel infrastructure that should be easily adapted to other similar problems. Similarly, we believe the techniques developed in this paper will be generally applicable to other contexts that require quick solutions to stochastic optimization problems.
[US military aircraft allocation optimization, Stochastic processes, stochastic programming, a priori load distribution, linear programming, duplicated worker computation, Optimization, parallel programming, two-stage stochastic linear program, stochastic optimization, large scale optimization, resource allocation, problem-specific decompositions, scientific computing algorithm, parallel infrastructure, Military aircraft, memory requirement balancing, parallel computing, military computing, parallel design, persistence-based dynamic load balancing, military aircraft, Atmospheric modeling, Computational modeling, master-worker decomposition, cargo movement missions, airfleet management, personnel movement missions, two-stage stochastic resource allocation, pull-based work assignment scheme, execution time, personnel, performance optimization, intel 64 cluster, Resource management, Aircraft]
Model-driven Level 3 BLAS Performance Optimization on Loongson 3A Processor
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Every mainstream processor vendor provides an optimized BLAS implementation for its CPU, as BLAS is a fundamental math library in scientific computing. The Loongson 3A CPU is a general-purpose 64-bit MIPS64 quad-core processor, developed by the Institute of Computing Technology, Chinese Academy of Sciences. To date, there has not been a sufficiently optimized BLAS on the Loongson 3A CPU. The purpose of this research is to optimize level 3 BLAS performance on the Loongson 3A CPU. We analyzed the Loongson 3A architecture and built a performance model to highlight the key point, L1 data cache misses, which is different from level 3 BLAS optimization on the mainstream x86 CPU. Therefore, we employed a variety of methods to avoid L1 cache misses in single thread optimization, including cache and register blocking, the Loongson 3A 128-bit memory accessing extension instructions, software prefetching, and single precision floating-point SIMD instructions. Furthermore, we improved parallel performance by reducing bank conflicts among multiple threads in the shared L2 cache. We created an open source BLAS project, OpenBLAS, to demonstrate the performance improvement on the Loongson 3A quad-core processor.
[model-driven level 3 BLAS performance optimization, parallel architectures, public domain software, Pipelines, mathematics computing, L1 data cache misses, mainstream processor vendor, multiple threads, basic linear algebra subprograms, performance model, cache storage, Registers, floating point arithmetic, Optimization, word length 64 bit, optimisation, MIPS64, parallel performance improvement, Loongson 3A 128-bit memory, Multi-core, shared memory systems, fundamental math library, OpenBLAS, Loongson 3A quadcore processor, Kernel, linear algebra, software performance evaluation, word length 128 bit, Chinese Academy of Sciences, multiprocessing systems, multi-threading, single precision floating point SIMD instructions, bank conflicts, BLAS, Prefetching, scientific computing, extension instructions, Loongson 3A architecture, open source BLAS project, register blocking, Loongson 3A, mainstream x86 CPU, single thread optimization, general-purpose 64-bit MIPS64 quadcore processor, Institute of Computing Technology, software prefetching]
Obfuscated Counting in Single-Hop Radio Network
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
In this paper we consider the problem of listing all active stations in a single hop radio network in such a way that the outer adversary observing communication could not gain any significant information about the real number of stations. We also consider a counterpart of this problem such that only a good approximation of the number of activated stations is needed. This problem is motivated mainly by military applications of sensors networks, however we present how our approach can be extended to other natural problems and similar models. In our paper we present two algorithms for secure listing and size approximation of the set of activated stations. Both of them are fairly practical (in terms of volume of communication, time of execution and computational complexity) and provably secure for the assumed adversarial model.
[Algorithm design and analysis, telecommunication security, radio networks, Protocols, adversary observing communication, secure listing, ad hoc network, Approximation methods, ad hoc radio networks, size approximation, active stations, single-hop radio network, passive adversary, indistinguishability, obfuscated counting, sensors networks, Approximation algorithms, initialization, Radio networks, Random variables, Sensors, ad hoc networks, computational complexity]
GPU Accelerated Molecular Docking with Parallel Genetic Algorithm
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Molecular docking is a widely used tool in Computer-aided Drug Design and Discovery. Due to the complexity of simulating the chemical events when two molecules interact, highly accelerated molecular docking programs are of great interest and importance for practical use. In this paper, we present a GPU accelerated docking program implemented with CUDA. The hardware-enabled texture interpolation is employed for fast energy evaluation. Two types of parallel genetic algorithms are mapped to the CUDA computing architecture and used for the search of optimal docking result. Comparing to the CPU implementation, the GPU accelerated docking program achieved significant speedup while producing comparable results to the CPU version. The source code is made public at http://code.google.com/p/cudock/.
[Drugs, cuda, parallel architectures, Graphics processing units, GPU accelerated molecular docking, parallel genetic algorithms, chemical engineering computing, hardware-enabled texture interpolation, GPU, parallel programming, Genetic algorithms, parallel genetic algorithm, genetic algorithm, Sociology, Genetics, computer-aided drug design and discovery, parallel algorithms, CPU version, energy evaluation, CUDA computing architecture, drugs, genetic algorithms, Statistics, graphics processing units, optimal docking result, highly accelerated molecular docking programs, GPU accelerated docking program, interpolation, molecular docking, Acceleration, CPU implementation]
Supporting User-directed Fault Tolerance over Standard MPI
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
User-directed means the process of carrying out fault tolerance is dynamic and the fault tolerance mode is chosen by users based on application requirements. In this paper, we introduce a general scheme based on standard MPI to provide the user directed support for application level algorithmic fault tolerance. The user-directed fault tolerance plays the role as a connection between applications and algorithmic fault tolerance. As a case study, our scheme has been incorporated to HPL combined with a non-blocking ABFT technique. We have tested the functional availability of our scheme for fault tolerance in real circumstance. We also evaluated that when there is no failure occurring, our support only brings 2.5 percent overhead. When failure occurs, with our scheme, the scalability of algorithmic fault tolerance maintains well.
[Algorithm design and analysis, message passing, algorithmic fault tolerance, application program interfaces, Scalability, Conferences, HPL, user-directed fault tolerance, functional availability, application-level, Standards, nonblocking ABFT technique, Fault tolerance, application level algorithmic fault tolerance, Fault tolerant systems, standard MPI, Detectors, fault tolerant computing, user-directed fault tolerance mode]
Automatic Refactoring of Legacy Fortran Code to the Array Slicing Notation
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
There are many legacy Fortran programs still in use today, especially scientific codes which were written decades ago. Many of these codes use explicit DO-loops in programs that tend to clutter the code and make it harder to understand and maintain. Modern features of the Fortran language, such as the array slicing notation and introduction of commonly used intrinsic functions, go a long way in helping programmers write code that is easier to read and maintain. We introduce a refactoring tool that can help to transform code to make use of the array slicing notation and related intrinsic functions.
[intrinsic functions, Java, DO-loops, scientific codes, Transforms, parallelism, Educational institutions, loops, software maintenance, Clutter, Optimization, legacy Fortran programs, Fortran, Jacobian matrices, vectorization, automatic refactoring tool, refactoring, FORTRAN, Arrays, program slicing, array slicing notation]
Simplifying Install-time Auto-Tuning for Cross-Compilation Environments by Program Execution Forwarding
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
This paper proposes a tool called the TLDT (Transparent Load Distribution Tool), which runs two types of executables, one for a local host and one for a remote host on a cross-compilation environment, by detecting the differences of them automatically. We confirmed the effectiveness of this tool by installing software with an install-time auto-tuning facility like ATLAS, which does not fully support cross compilation.
[transparent load distribution tool, Auto tuning, Buildings, Force, cross-compilation environment, Educational institutions, load distribution, Supercomputers, Cross compile, program compilers, Tuning, TLDT, ATLAS, remote host, Phantoms, program execution forwarding, Software, install-time auto-tuning, Execution forwarding]
IODET: A HoL-blocking-aware Deterministic Routing Algorithm for Direct Topologies
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
In large parallel computers routing is a key design point to obtain the maximum possible performance out of the interconnection network. Routing can be classified into two categories depending on the number of routing options that a packet can use to go from its source to its destination. If the packet can only use a single predetermined path then the routing is deterministic, whereas if several paths are possible it is adaptive. It is a well-known fact that adaptive routing usually outperforms deterministic routing; but in this paper we take the challenge of developing a HOL-blocking-aware deterministic routing algorithm that can obtain a similar or even better performance than adaptive routing, while decreasing its implementation complexity and providing some inherent advantages to deterministic routing such as in-order delivery of packets. In this large computers regular direct topologies are widely-used, so in this paper we focus on meshes and tori.
[Algorithm design and analysis, Adaptive systems, IODET, adaptive routing, topology, Switches, telecommunication network topology, Routing, implementation complexity reduction, HoL Blocking, LAN interconnection, Topology, direct topologies, computer network performance evaluation, Network topology, Multiprocessor interconnection, large parallel computers, interconnection network, routing algorithm, telecommunication network routing, HoL-blocking-aware deterministic routing algorithm, maximum possible performance]
Measuring Distributed Applications through MapReduce and Traffic Analysis
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Monitoring and evaluating distributed systems at runtime is a difficult effort. Network traffic analysis may be used to analyse distributed systems, but the capacity to analyse large amount of traffic is a challenge. This work proposes the use of Map Reduce for deeply inspecting distributed systems network traffic, evaluating the effectiveness and the processing capacity of Map Reduce for this inspection.
[Peer to peer computing, Scalability, network traffic analysis, Inspection, distributed processing, Distributed Systems, MapReduce, Runtime, Deep Packet Inspection, MapReduce and, Network Traffic Analysis, Sockets, processing capacity, Hardware, distributed systems evaluation, Monitoring, telecommunication traffic, inspection]
Time-Stamped Equal Size Segmentation and Chunk Scheduling Algorithms for SVC Based P2P Streaming Systems
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
We propose a novel segmentation algorithm TSESS and a novel chunk scheduling algorithm TSESCS for SVC based P2P streaming systems. TSESS splits each layer into equal sized chunks and tags each chunk with a time-stamp. TSESCS is based on TSESS and adopts two scheduling windows with adaptive strategies. Compared with traditional segmentation algorithm and chunk scheduling algorithms, our TSESS and TSESCS algorithms can achieve better video quality on clients in heterogeneous network environments.
[TSESS algorithm, SVC, peer-to-peer computing, Segmentation, TSESCS algorithm, video quality, Chunk Scheduling, Scheduling, chunk scheduling algorithm, Decoding, Video recording, P2P, time-stamped equal size segmentation, Scheduling algorithms, heterogeneous network environments, image segmentation, Static VAr compensators, SVC based P2P streaming systems, Streaming media, scheduling, video streaming, Quality assessment]
A Novel Cache Replacement Policy for ISP Merged CDN
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
The cache replacement policy is the key factor affecting the performance of the ISP merged Content Delivery Networks. Current cache replacement schemes only consider the frequency and locality as the basis of replacement. However, we argue the access interval change rate is more valuable in predicting the new objects arrival through analyzing the real network logs. Considering this new metric, we propose a novel cache replacement algorithm based on access density. Using this novel method, the cache can achieve higher hit rate. Experiments with real network data show that our method improves 3% to 5% hit rate than the typical cache schemes and ISP can reduce 3% to 7% network traffic.
[real network data, ISP merged content delivery networks, real network logs, Conferences, Telecommunication traffic, content delivery, cache storage, Electronic mail, cache replacement, content management, access interval, access interval change rate, Content distribution networks, cache replacement policy, cache replacement schemes, Information security, Bandwidth, Logic gates, ISP merged CDN, Internet, access density, performacne optimization, hit rate]
Modeling and Analysis of Large Scale Interconnected Unstructured P2P Networks
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Interconnection of multiple P2P networks has recently emerged as a viable solution to increase system reliability and fault-tolerance as well as to increase resource availability. In this paper we consider interconnection of large scale unstructured P2P networks by means of special nodes (called synapses) that are co-located in more than one overlay. Synapses act as trait d'union by sending/forwarding a query to all the P2P networks they belong to. Modeling and analysis of the resulting interconnected system is crucial to design efficient and effective search algorithms and to control the cost of interconnection. Yet, simulation and/or prototype deployment based analysis can be very difficult - if not impossible - due to the size of each component (we consider large scale systems that can be composed of millions of nodes) and to the complexity arising from the interconnection of several such complex systems. To overcome this strong limitation, we developed a generalized random graph based model that is validated against simulations and it is used to investigate the performance of search algorithms for different interconnection costs and to provide some insight in the characteristics of the interconnection of a large number of P2P networks.
[peer-to-peer computing, modeling, Peer to peer computing, Computational modeling, fault-tolerance, interconnection, Interconnected systems, large scale interconnected unstructured P2P networks, system reliability, resource availability, search algorithms, Probability distribution, random graphs, Electronic mail, peer-to-peer, Analytical models, prototype deployment based analysis, generalized random graph based model, interconnected system, fault tolerant computing, Random variables, interconnected systems, special nodes, search problems]
Balancing Precision and Battery Drain in Activity Recognition on Mobile Phone
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Many achievements have been announced with real time running capability for activity recognition (AR) using mobile accelerometer. However, they also have weak points including low accuracies especially in multiple-subject activity recognition and lacking of evidences about power consumption. In this paper, we contribute a novel method for extracting features on time domain and frequency domain. These different features were then respectively applied to Support Vector Machine (SVM) classifier and Dynamic Time Warping (DTW) method in order to find out the most effective combinations. Our own data and SCUTT-NAA dataset were used in our experiment. Accuracy rates of 95% and 97% in multiple-subject AR were achieved by respectively using SVM and DTW from time domain features (TF). These approaches were then implemented on a mobile phone to measure the power consumptions. SVM using time feature method was found as the most effective method for balancing accuracy and energy consumption.
[Battery Drain, dynamic time warping, DTW, Mobile handsets, Batteries, SVM, balancing precision, power consumption, Accuracy, power aware computing, DTW method, Accelerometers, time domain features, support vector machines, mobile accelerometer, Mobile AR, smart phones, battery drain, Frequency domain analysis, Support vector machines, AR, TF, support vector machine, mobile phone, Feature extraction, activity recognition]
Inner-Distance-Based Shape Recognition of Target Object Using Binary Sensors
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
We propose an algorithm for shape recognition of the target object using the notion of the inner distance. The inner distance of the object going across the field, where binary sensors are densely deployed, are obtained from the distance between two binary sensors concurrently detecting the object. To estimate the position of each binary sensor, we apply a range-free localization using a moving landmark, which wanders around the field of watch. We numerically study the accuracy of the shape recognition based on the inner distance when it is combinedly used with the range-free sensor localization based on the moving landmark.
[Shape, Target recognition, binary sensors, range-free sensor localization, Watches, Educational institutions, inner-distance-based shape recognition, object detection, target object, Accuracy, sensors, moving landmark, range-free localization, inner distance, Silicon, shape recognition, sensor, Sensors, landmark]
An Evaluation of Vehicular Networks with Real Traces
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
We have developed an NS-2 simulation framework incorporating a large dataset of real GPS taxi traces collected from around 4000 taxis in Shanghai, China. Extensive simulations have been performed to explore the achievable performance of real vehicular networks. Simulation results show that a real vehicular network has surprisingly poor data delivery performance under wide network configurations, which strongly suggests that the challenging characteristics of vehicular networks, such as unique node mobility, constraints of road topology, need further exploration.
[Evaluation, mobile radio, wide network configurations, vehicular networks evaluation, Shanghai, Telecommunication traffic, Routing, road topology constraints, data delivery performance, Delay, Vehicles, Global Positioning System, NS-2 simulation framework, node mobility, GPS taxi traces, China, routing protocols, Routing protocols, vehicular networks, real traces, Load modeling]
Software Aging in Virtualized Environments: Detection and Prediction
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Software aging has been cited in many scenarios including Operating System, Web Servers, Real-time Systems. However, few studies have been conducted in long running virtualized environments where more and more software is being delivered as a service. Furthermore, state-of-the-art methods lack the ability to deal with miscellaneous upper applications and underlying systems transparently in virtualized scenarios. In this paper, we detect aging phenomenon by conducting experiments in physical and virtual machines and identify the differences between the two, and propose a feature code-based methodology for failure prediction through system call, then implement a prototype in virtual machine manager layer to predict failure time and rejuvenate transparently, which is suitable in virtualized scenarios. The evaluation shows the prediction deviation against reality is less than 10%.
[aging phenomenon detection, feature code, virtualisation, availability, system call, Prototypes, Aging, Market research, rejuvenation, software performance evaluation, virtualized environments, feature code-based methodology, virtualization, operating system, Educational institutions, Virtual machining, software aging, Web servers, software fault tolerance, virtual machine manager layer, failure time prediction, Memory management, real-time systems, physical machines, virtual machines, Software]
An Erasure Coded Archival Storage System
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
There is an ever increasing need of storage capacity for storage of digital archives and historical data-digital preservation, because of regulatory and compliance requirements. There is an increasing interest in disk based archival system. Major technical challenges in creating large disk based storage archive are - providing large capacity at low costs, large read and write throughput, data integrity and sustaining hardware and operating system refresh. In this paper we present the architecture and working principle of an archival storage system that uses an erasure-coded redundancy scheme. We present the design of a Quality of Service (QoS) framework that tries to achieve an optimum balance between file availability, performance and system availability. The design includes a file encoding and placement scheme that allows files to be read from the archive without the need to access any metadata. Finally, we present the results obtained from running an experimental setup on Amazon Web Services.
[archival storage, metadata, regeneration, Quality of service, Throughput, erasure coding, erasure coded archival storage system, Servers, disc storage, Operating systems, QoS, Bandwidth, long term storage, Availability, file encoding, operating system, hardware system, Encoding, data integrity, quality of service, digital archives, disk based archival system, records management, information retrieval systems, storage capacity, historical data-digital preservation, compliance requirements]
R-Barrier: Rapid Barrier for Software RAID Cache Using Hints from Journaling Filesystem
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
While adopting cache in software RAID brings performance benefit, it can cause data loss at power failure which results in the broken filesystem consistency. Though I/O Barrier can be used to remove the consistency issue, it sacrifices the write performance of software RAID. To mitigate this tradeoff, we suggest R-Barrier for software RAID. The basic idea of R-Barrier is to avoid the synchronization of the entire cache space. Instead, based on given hints from the filesystem layer, R-Barrier makes software RAID cache select one of the following methods when the cache writes a certain write request into the disk: (1)Strictly ordered destaging for the write requests affecting the filesystem consistency, (2)Reordered destaging for the rest of write requests. We show that R-Barrier guarantees same filesystem consistency with I/O Barrier while the performance degradation of R-Barrier is less than that of I/O Barrier.
[R-Barrier, filesystem layer, write request, data loss, Random access memory, strictly ordered destaging, cache space, rapid barrier, Throughput, cache storage, Barrier, power failure, software RAID cache, Filesystem, software performance evaluation, Electrical engineering, Software algorithms, performance degradation, Synchronization, RAID, Caching, Writing, Software, I/O barrier, write performance, broken filesystem consistency, reordered destaging]
Automated Malware Analysis Framework with Honeynet Technology in Taiwan Campuses
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
In this short paper, an automatic malware analysis framework is introduced to facilitate the security community to keep the pace of rapidly changing malwares. In our framework, the honeynet technology and Taiwan Malware Analysis Net (TWMAN) can simultaneously collect and analyze the latest malicious software. The well-organized malware database and sharing platform can assist security experts in searching malware patterns. Owing to the prevalence of Bonnet, the number of malware increases quickly. Our automatic malware analysis framework is an excellent solution to deal with the Bonnet problem.
[security community, invasive software, Botnet, and Virtual Machine, malware database, Virtual machining, malware pattern search, Servers, database management systems, automated malware analysis framework, computer network security, malicious software, sharing platform, Databases, Operating systems, Taiwan campuses, honeynet technology, Malware, Taiwan malware analysis net, TWMAN, Honeypot]
Multicast Congestion Control Using Communicating Realtime State Machines and Dynamical Systems for Distributed Iptv Scenarios
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
In this poster, the authors describe original work of a developed real time multicast approach for distributed multimedia systems. The poster focuses especially ongoing research work on several aspects of distributed multimedia and dynamical systems oriented to IPTV scenarios.
[Context, Protocols, telecommunication congestion control, multimedia systems, distributed IPTV, IPTV, realtime state machines, Multimedia Systems, Multimedia communication, multicast congestion control, Distributed Systems, Dynamical systems, Multicast, dynamical systems, Multicast congestion control, transport protocols, Real-time transport protocols, multicast communication, Real-time systems, Formal methods, distributed multimedia systems]
A Distributed Sensor Data Stream Delivery System with Communication Loads Balancing for Heterogeneous Collection Cycle Requests
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Due to the prevalence of sensors such as live cameras or environmental sensors, sensor data stream delivery, which requires continuous and cyclic data delivery attracts great attention. For sensor data stream delivery, various communication loads balancing techniques have been studied since the load of the sensor data source become high to accommodate large number of clients. However, these studies assume only the requests that have the same collection cycle, which is not enough for the actual applications. In this paper, we propose a sensor data stream delivery system with communication loads balancing for heterogeneous collection cycle requests. The proposed system distributes the loads by re-delivering the sensor data that are requested by other clients with different collection cycles but have common cycles.
[wireless sensor networks, streaming data, Peer to peer computing, Conferences, senior networks, communication loads balancing techniques, sensor networks, heterogeneous collection cycle requests, data collection, cameras, environmental sensors, sensor data source, sensor data stream delivery system, Prototypes, Streaming media, Cameras, Load management, data communication, distributed sensor data stream delivery system, communication loads balancing, collection cycles, Smart phones]
Cyber Attacks Prediction Model Based on Bayesian Network
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Cyber attacks prediction is an important part of risk management. Existing cyber attacks prediction methods did not fully consider the specific environment factors of the target network, which may make the results deviate from the true situation. In this paper, we propose a cyber attacks prediction model based on Bayesian network. We use attack graphs to represent all the vulnerabilities and possible attack paths. Then we capture the using environment factors using Bayesian network model. Cyber attacks predictions are performed on the constructed Bayesian network. Experimental analysis shows that our method gets more accurate results.
[risk management, environment factors, Computational modeling, network security, graph theory, attack graph, Predictive models, Web servers, Security, History, cyber attacks prediction model, attack graphs, security of data, Bayesian methods, target network, quantitative assessment, cyber attacks prediction, belief networks, Bayesian network]
iCirrus Wop: Workload Analysis for Virtual Machine Placements
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
True essence of the technology of virtualization is the ability to allow one or more workloads to share the underlying physical resources, thereby bringing about significant cost saving. However, in order to maximize the cost savings from this disruptive technology, it is essential to adopt optimal resource management techniques. These techniques broadly encompass approaches to virtual machine (VM) sizing and placement in a manner that maximizes the physical infrastructure utilization, alongside ensuring that the desired service-level objectives of the candidate workloads are met. In this paper, we propose a novel workload analysis approach for VM placement, which relies on examining the time varying processing demands and variability of the workloads to determine the most optimal placement. Such a solution will result in maximizing infrastructure utilization and ensure that the SLAs of the candidate workloads are met after placement. The technique has been effectively applied to real-life workloads that pertain to SaaS based business platforms offered to clients spread across different geographical locations. A paper based assessment reported over 25% improvement in the overall infrastructure utilization by using the proposed algorithm as compared to other well-known approaches.
[Measurement, Algorithm design and analysis, Heuristic algorithms, optimal resource management techniques, SLA, virtualisation, CoV, contracts, variability, Phase change materials, resource allocation, total capacity metric, geographical location, cloud computing, percentage compatibility metric, virtual machine placement, virtualization, workload analysis, Time series analysis, service-level objective, SaaS, workload, Virtual machining, time varying processing, service level agreement, cost maximisation, resource sharing, workload analysis approach, virtual machines, virtual machine (VM), VM sizing, Resource management, workload percentile, Virtualization, VM placement]
Parallel Secondo: Boosting Database Engines with Hadoop
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Hadoop is an efficient and simple parallel framework following the Map Reduce paradigm, and making the parallel processing recently become a hot issue in data-intensive applications. Since Hadoop can be easily deployed on large-scale clusters including up to thousands of computers, various studies intend to process common relational database operations also on this new platform and expect to achieve a remarkable performance. However, these works have to prepare customized programs according to different input format, making the communication between co-workers difficult. Additionally, all intermediate data have to be transformed to key-value pairs and then transferred through the underlying HDFS, making the data processable by Map and Reduce tasks and keeping a balanced workload on the cluster. During this period, unnecessary overhead decreases both the speed-up and scale-up of these systems. Therefore, this paper attempts to propose a light and efficient coupling structure thus to combine Hadoop with single-computer databases on the engine level. On one hand, it uses a well-designed parallel data model to make end-users represent parallel queries like common queries. All current and future data types and algorithms can be used directly, having no need to be specifically changed for the parallel platform. On the other hand, it provides a simple and independent distributed file system to transfer data among database engines directly, without passing through HDFS, hence to remove as much as possible unnecessary transform and transfer overhead. For purpose of demonstration, a prototype Parallel Secondo is introduced in this paper. It has been fully evaluated in both small and large scale clusters, achieving satisfactory performances for different database operations.
[Computers, data-intensive applications, public domain software, small-scale clusters, distributed file system, parallel framework, Servers, parallel processing, Engines, parallel programming, Hybrid system, query processing, Parallel Secondo, key-value pairs, customized programs, network operating systems, Distributed databases, MapReduce paradigm, parallel queries, data types, Trajectory, Distribution functions, parallel data model, parallel databases, Hadoop, large-scale clusters, HDFS, common relational database operations, relational databases, data models, moving objects database, end-users, common queries, data transfer, database engines, single-computer databases]
A Remote USB Architecture for Virtual Machine Oriented Device Sharing and Transparent Mgration
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
IaaS cloud environments which are driven by virtualization technologies enable managing applications and resources in a cost-efficient way and become the main operating environments of modern data centers. While, in such environments, how to use remote peripheral devices in a virtual machine (VM) becomes a key research problem, and the problem is aggravated when facing VM migration. The state of the art migration technologies lack for the consideration of peripheral devices, which can result in data loss. To address these two problems, the paper presents a remote USB architecture, which consists of two parts: the virtual machine oriented USB device sharing (VMDS) and transparent virtual USB device migration (TVDM). VMDS is used to share locally attached USB devices to remote virtual machines, and TVDM supports continuous accessing to remote devices during virtual machine live migration. A system based on Linux and KVM is implemented to demonstrate the ideas. Experimental evaluations illustrate the system's excellent usability and performance.
[Performance evaluation, USB device, remote share, data loss, data centers, key research problem, Throughput, Servers, IaaS cloud environments, live migration, virtualization technologies, Universal Serial Bus, virtual machine oriented device sharing, Hardware, IP networks, cloud computing, operating environments, remote peripheral devices, Virtual machining, electronic engineering computing, TVDM, computer centres, VMDS, transparent migration, virtual machine, Linux, peripheral devices, peripheral interfaces, virtual machines, remote USB architecture, transparent virtual USB device migration]
Towards a Common Benchmark Framework for Cloud Brokers
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Cloud computing has attracted a lot of interest from both academia and industry because of the promise of cost-effectiveness. As a result, increasingly large numbers of cloud services have become available. This implies challenges in brokering between the cloud consumers and providers. To meet this demand, several cloud brokers have been introduced. Unfortunately, most of the cloud brokers cannot solve real life scenarios and cannot be evaluated in an unbiased manner because their evaluations are often academic and independent. This paper proposes a new benchmark to evaluate and compare cloud brokers. Our benchmark called Cloud Broker Challenge (CBC) describes the cloud providers, cloud consumers, and goals with five variety levels of complexities. CBC Benchmark is useful to the Cloud community for an unbiased evaluation and comparison of brokers and can potentially enable development of real-life cloud brokers.
[Cloud service selection, Performance evaluation, Cloud computing, cost-effectiveness, cloud services, Conferences, cloud consumers, unbiased cloud broker evaluation, cloud community, Complexity theory, Cloud discovery, common benchmark framework, Cloud broker, CBC benchmark, Linux, Semantics, cloud broker challenge, Bandwidth, Benchmark testing, cloud providers, cloud computing, software performance evaluation]
Enabling On-Demand Mashups of Open Data with Semantic Services
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Analogous to software-as-a-service (SaaS), platform-as-a-service (PaaS) and infrastructure-as-a-service (IaaS), data-as-a-service (DaaS) is used to provide data on demand to users over the Internet and is gaining popularity in the current cloud computing era. In particular, several Open Data initiatives have led to a number of data services in various formats becoming available online, and it remains a challenge to make full use of the data by transferring between and answering queries from different sources in a automatic, dynamic and meaningful manner. In this paper we propose a service-oriented, composition-based approach towards tackling the integration of open data services. We provide a formal, semantic-based modeling of data services and convert the integration problem into the service composition problem. Then the composition graph can be used to create the executable queries to the various data services. We illustrate our idea by using a case study in the real estate domain.
[software-as-a-service, Mashups, PaaS, Ontologies, open data on-demand mashup enabling, DaaS, query processing, formal data service modeling, Semantics, composition, Data integration, cloud computing, SaaS, IaaS, data-as-a-service, Educational institutions, semantic services, cloud computing era, composition-based approach, infrastructure-as-a-service, Global Positioning System, data models, query answering, service composition problem, service-oriented approach, open data service integration, platform-as-a-service, Data models, semantic-based data service modeling, semantic service, Internet, open data initiatives]
Online Optimization of VM Deployment in IaaS Cloud
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Infrastructure-as-a-Service (IaaS) clouds provide on-demand virtual machines (VMs) to users. How to improve the quality of IaaS cloud services is important for service providers. Currently, the VMs in an IaaS cloud are usually deployed with respect to the maximum utilization of resources. In this paper, we propose an online VM optimization method for IaaS clouds. Our method mainly optimizes the VM deployment in IaaS clouds according to the traffics among VMs. VMs are allocated with respect to cabinet capacities at the beginning. At runtime, we monitor the traffics among VMs to get the traffic topology, based on which related VMs are migrated to neighbors to improve performance and reduce the traffics across cabinets. Preliminary simulation experiments are conducted on a well-know simulator, and the experimental results indicate that our method is effective and promising.
[Cloud computing, IaaS cloud services, infrastructure-as-a-service clouds, IaaS cloud, Optimization, Runtime, optimisation, cabinet capacities, Clustering algorithms, Bandwidth, cloud computing, online VM optimization method, Monitoring, deployment, topology, Topology, VM deployment, traffic topology, virtual machine, on-demand virtual machines, maximum utilization, online optimization, virtual machines, VM, telecommunication traffic]
Accelerating the Cloud Backup Using GPU Based Data Deduplication
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Data deduplication is an efficient technique for data reduction. It has been adopted and widely used in cloud backup service. However deduplication requires high computing capability since it has to calculate the fingerprint of data many times using a hash algorithm. This can be a performance bottleneck for the system. In this paper, we propose to use Graphics Processing Unit (GPU), a high computing architecture, to accelerate the fingerprint generating process in deduplication. Our experiment shows that GPU can significantly improve the performance of deduplication especially when running on large data, where we achieve an acceleration ratio up to 53 times greater compared to with CPU.
[Cloud computing, cloud backup service, data deduplication, Instruction sets, Graphics processing units, cloud backup, Fingerprint recognition, cryptography, hash algorithm, deduplication, graphics processing units, GPU, data reduction, component, Computer architecture, graphics processing unit, Central Processing Unit, Acceleration, cloud computing, performance bottleneck]
Improving Data Processing Time with Access Sequence Prediction
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Genomic research nowadays often faces the problem of big data. The data size from genome sequencing process can grow very quickly and continuously creating the problem with storage and processing. BGI, one of the renowned genomic research institutes in China also faces the similar problem. The research at BGI depends on several sequencing machines. One machine pipeline may generate temporary data of around 1.4 terabytes. In addition, multiple read and write operations occur continuously during processing time. The I/O bottleneck thus degrades research throughput tremendously. Using a high performance computing system alone is not sufficiently effective in experimental results processing. In order to hide the I/O latency, an effective big data management framework is needed at BGI. In this paper, we proposed the hybrid prediction model for data access pattern. The goal is to predict the next pieces of data needed in the processor and preload them into the memory in order to improve the overall processing time. From the results obtained from the initial experiments, the proposed model can deliver high prediction accuracy in linear-time. Moreover, the error rate is low at 1.85%, which is better than the common methods used, such as Prediction Graph, ANN and ARMA. We believe that with some further fine-tuning, the model can be used as a part of the big data management framework deployed at BGI in the near future.
[I/O Bottleneck, Paired t-Test, data management framework, Predictive models, I/O latency, Complexity theory, data access sequence prediction, data storage, read and write operation, sequencing machine, data access pattern, Prediction algorithms, genomics, Mathematical model, Computational modeling, BGI, information retrieval, Big Data, medical information systems, hybrid prediction model, Hybrid ARMA Model, Data models, pipeline processing, data processing time improvement, genome sequencing process, I/O bottleneck, Autoregressive processes]
Modeling Dwarfs for Workload Characterization
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Workload characterization is an important concept in performance tuning and efficiency improvement of high performance computing systems. Not only does it allow a system to dynamically adjust resources, it also helps improve energy efficiency resulting in lower cost for computation. Nevertheless, lack of quantitative methods to describe workload characteristics prevents the system to work at its full potential. In this work, we proposed two discrete-time Markov chains, which can describe workload characteristics of a set of computational kernels, called Berkeley's "dwarfs". These discrete-time models were derived from the relationship between stalling in processor pipeline and resource usage behaviors of the workload. The models can quantitatively describe behaviors of each dwarf. Moreover, the model enables the concept of combining each dwarf to extend performance analysis over any arbitrary processes.
[Measurement, Computers, Pipelines, dwarf modeling, discrete-time Markov chains, performance tuning analysis, resource allocation, Discrete-time Markov Chain, high-performance computing systems, dynamic resource adjustment, computational kernels, Computer architecture, processor pipeline stalling, workload characterization, Mathematical model, Workload Characterization, software performance evaluation, MICA, energy efficiency improvement, Computational modeling, Tuning, Markov processes, Berkeley's Dwarfs, pipeline processing, benchmark testing, workload resource usage behaviors]
A Distributed Fine-Grained Flow Control System for Scalable Aircraft Spares Management and Optimization in Clouds
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
In this paper, we presented the design, implementation, and evaluation of a distributed system to manage the parallelized analytics for Aircraft Spare parts Management and Optimizations (SMO), which is a well-known problem in logistics industry. Our proposed solution is able to solve the resource-intensive SMO problem using distributed computing infrastructures (e.g., private or public clouds) in a scalable manner. We designed and fine-tuned a parallel meta-heuristics based on a fine-grained flow control workflow model which enables flow controls of running parallel meta-heuristics in multiple processors and achieved significant performance gains. Together with priority based scheduling, the proposed system effectively dispatches submitted SMO jobs over the set of distributed resources to accommodate different classes of users. Extensive experimental studies were conducted to analyze the performance of parallelized SMO job executions in term of execution time, computation and data transmission time, waiting time, memory usage, etc. Insightful lessons have been drawn from the obtained results, and potential areas for further improvements have also been identified.
[resource-intensive SMO problem, Cloud computing, Torque, logistics industry, data transmission time, logistics, distributed computing infrastructures, parallel processing, Optimization, Program processors, resource allocation, priority-based scheduling, computation time, distributed systems, distributed system implementation, cloud computing, parallelized SMO job executions, aircraft maintenance, job scheduling, aircraft SMO, distributed system evaluation, waiting time, Computational modeling, scalable aircraft spare parts management and optimizations, private clouds, SMO job dispatching, memory usage, fine-grained flow control workflow model, multiple processors, Processor scheduling, distributed system design, distributed fine-grained flow control system, Memory management, execution time, distributed resources, public clouds, parallel meta-heuristics, Resource management, workflow control, job shop scheduling, performance analysis]
A VM-based Resource Management Method Using Statistics
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Virtualization technology is one of the key technologies in cloud computing. Proper allocation of the virtual machines onto available hosts plays an important role on the performance optimization of cloud computing. At present, many resource management methods are based on the load model to predict the resource requirements of the application. However, a variety of heterogeneous hardware environments and virtualization technologies make it hard to efficiently allocate the resources based on existing methods. To address the problem, we proposed a virtual machine allocation strategy based on statistics. And The contribution of this paper include:(1) A load-resource model for estimating the resource demand of each virtual machine, (2) An algorithm for assigning virtual machines onto the hosts among resource pool according to the resources requirement of virtual machine. Experiments show that our load-resource model can accurately estimate the resource requirements of virtual machines for physical host, and our virtual machine assigning algorithm can achieve better load balancing compared with the first-fit and best-fit algorithm.
[VM based resource management method, Computational modeling, Virtual machine performance modeling, load resource model, Predictive models, Virtual machining, hardware environments, optimisation, Cloud Computing, virtualization technology, virtual machines, Prediction algorithms, performance optimization, Virtual Machine Allocation Strategy, Resource management, cloud computing, statistical analysis, Load balancing, Virtualization, Load modeling, statistics]
Failure Prediction of Data Centers Using Time Series and Fault Tree Analysis
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
This paper proposes a framework for online failure prediction of data centers. A data center often has a high failure rate as it features a number of servers and components. Moreover, long running applications and intensive workloads are common in such facilities. Performance of the system depends on the availability of the machines, which can be easily compromised if failure cannot be handled gracefully. The main idea of this paper is to create an effective prediction model focusing on hardware failure. Accurate prediction may enhance the overall system performance. In this work, we employ two methods, namely, ARMA (Auto Regressive Moving Average) and Fault Tree Analysis. Experiments were then performed on a simulated cluster built based on Simi's platform. The results show prediction accuracy of 97%, which is very high. We thus believe that our framework is practical and can be adapted to use in data centers in the future.
[Time Series Prediction, hardware failure prediction model, autoregressive moving average processes, data centers, Fault Management, fault tree analysis, Predictive models, Accuracy, Fault Tree Analysis, failure rate, Mathematical model, prediction accuracy, Fault trees, Availability, simulated cluster, intensive workloads, performance evaluation, fault trees, time series, Simics platform, computer centres, online failure prediction, long running applications, Performance Enhancement, system performance, Data models, ARMA method, Autoregressive processes, autoregressive moving average method]
Enhancing Cloud Object Storage Performance Using Dynamic Replication Approach
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
The cloud storage is a new paradigm of data storage. Because of the internet connection speed increasing, everyone can store data on the cloud storage. Some data is shared for many user accesses, some data is private data. It has different characteristic of access pattern. In this work, the adaptive dynamic replication is presented. The model of dynamic replication is discussed. We propose the object-based storage as our cloud storage architecture. The storage object life time is used for handle our dynamic replication mechanism. The experimental results show the distribution of storage object workload that has some characteristic that relate with storage object life time and other factors.
[Cloud computing, cloud storage architecture, Aerospace electronics, storage object workload distribution, Servers, storage management, cloud object storage performance evaluation, data storage, data access pattern, Computer architecture, Market research, cloud computing, dynamic replication approach, replication, user access, object placement, object-based storage, replicated databases, information retrieval, Educational institutions, Internet connection, data shared, cloud storage, data privacy, Internet]
Data Value Chain as a Service Framework: For Enabling Data Handling, Data Security and Data Analysis in the Cloud
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
The concept of Data Value Chain (DVC) involves the chain of activities to collect, manage, share, integrate, harmonize and analyze data for scientific or enterprise insight. For some applications, it also entails the leverage of visualization and simulation. However, the curse of big data (volume, velocity, variety) makes it difficult to efficiently handle and understand the data in near real-time. To address these challenges, this paper proposed the Data Value Chain as a Service (DVCaaS) framework, a data-oriented approach for data handling, data security and analytics in the cloud environment.
[Data privacy, data value chain as a service framework, data management, scientific insight, data value chain, Analytical models, Distributed databases, data visualisation, DVCaaS framework, cloud environment, enterprise insight, data visualization, cloud computing, service framework, Data analysis, data analysis, data analytics, Computational modeling, data security, data collection, security of data, Data visualization, real-time data, Data models, data handling, data integration, data-oriented approach, data sharing]
Privacy Preservation in Streaming Data Collection
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Big data management and analysis has become a hot topic in academic and industrial research. In fact, a large portion of big data in service today are initially streaming data. To preserve the privacy of such data that are collected from data streams, the most efficient way is to control the process of data collection according to corresponding privacy polices. In this paper, we design a framework to support data stream management with privacy-preserving capabilities. In particular, we focus on two premier principles of data privacy, limited disclosure and limited collection. With these two principles guaranteed, the archived data will not necessarily be checked for privacy protection, before analysis and other operations can be done.
[Access control, policy enforcement, Data privacy, data analysis, streaming data, big data management, privacy-preserving capabilities, privacy preservation, data stream management, streaming data collection, Information management, Privacy, Databases, Vegetation, limited collection, data privacy, limited disclosure]
Hidden Markov Models for Abnormal Event Processing in Transportation Data Streams
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Making sense of big data and big metadata remains a challenge as more and more data are churned out every day. The problem of adding value to unstructured data requires the application of computationally intensive algorithms to discover useful patterns in the data. In terms of data streams from public transport such as buses, we address the problem of performing time-consuming algorithms to model the data while still being able to process abnormal events in real-time. We propose using Hidden Markov Models (HMMs) for identifying conditions for an abnormal event in bus journeys and methods for isolating HMM computations from real-time event processing. Results show that training HMMs with even noisy metadata can generate models that can recognize an abnormal event in a parallel and distributed manner in the cloud.
[Computers, metadata, Computational modeling, time-consuming algorithms, event processing, Big Data, HMM, Noise measurement, transportation data streams, Engines, Training, cloud, hidden Markov models, traffic information systems, computationally intensive algorithms, Hidden Markov models, public transport, abnormal event processing, Real-time systems, data handling, Hidden Markov Models, event-driven architecture, cloud computing]
Smart Traffic Cloud: An Infrastructure for Traffic Applications
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
With rapid development of sensor technologies and wireless network infrastructure, research and development of traffic related applications, such as real time traffic map and on-demand travel route recommendation have attracted much more attentions than ever before. Both archived and real-time data involved in these applications could potentially be very big, depending on the number of deployed sensors. Emerging Cloud infrastructure can elastically handle such big data and conveniently providing nearly unlimited computing and storage resources to hosted applications, to carry out analysis not only for long-term planning and decision making, but also analytics for near real-time decision support. In this paper, we propose Smart Traffic Cloud, a software infrastructure to enable traffic data acquisition, and manage, analyze and present the results in a flexible, scalable and secure manner using a Cloud platform. The proposed infrastructure handles distributed and parallel data management and analysis using ontology database and the popular Map-Reduce framework. We have prototyped the infrastructure in a commercial Cloud platform and we developed a real-time traffic condition map using data collected from commuters' mobile phones.
[wireless network infrastructure, data management, Roads, ontology database, mobile phones, Ontologies, real time traffic map, traffic related applications, parallel processing, hosted applications, Vehicles, software architecture, Distributed databases, smart traffic cloud, traffic applications infrastructure, Real-time systems, cloud computing, storage resources, Participatory sensing, parallel data analysis, sensor technologies, Global Positioning System, parallel data management, Map-Reduce framework, ontologies (artificial intelligence), data handling, Smart phones]
Automatic VM Allocation for Scientific Application
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Cloud has been the main technology utilized as a high performance computing (HPC) platform. The characteristics of cloud can satisfy a large scale processing required by scientific applications, which are mostly compute-intensive with big data. Cloud can also reduce the computing cost through sharing and virtualizing of resources. In the cloud, a large number of virtual machines (VM) can be generated on demands. In order to obtain the optimal cost and high efficiency in the task execution on the public cloud, the suitable amount of virtual machines should be properly determined prior to the start of the computation. Moreover, the application should be effectively partitioned and distributed onto the virtual machines. In this paper, we propose an automatic mechanism to allocate the optimal numbers of resources in the cloud. The novel resource estimation model and scheduling algorithm are presented. We select an analytic application with high level of computations in the field of epidemic forecast to demonstrate the use of the designed mechanism. Experimental studies have been conducted to examine the resource prediction accuracy and the scalability of running the application on the cloud.
[Cloud computing, Parameter estimation, resource estimation, Predictive models, HPC platform, virtualisation, large-scale processing, scheduling algorithm, parallel processing, task execution, processor scheduling, resource estimation model, public cloud characteristics, resource allocation, resource virtualization, automatic VM allocation, computing cost reduction, Mathematical model, cloud computing, analytic application distribution, automatic optimal resource number allocation mechanism, Computational modeling, resource prediction accuracy, analytic application partitioning, epidemic forecasting, Virtual machining, scientific application, high-performance computing platform, resource sharing, regression, virtual machines, Resource management]
ProDFA: Accelerating Domain Applications with a Coarse-Grained Runtime Reconfigurable Architecture
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
With the on-chip resources largely increased, modern architectures are much different from traditional ones. The relation between temporal computing and spatial computing is getting more and more intricate. In this paper, we first analyzed and compared the abstract computing models of modern architectures. Then, a runtime reconfigurable architecture called programmable dataflow computing architecture (ProDFA) is proposed. The architecture of ProDFA is sumarized, then the process of how applications are mapped and execute are simply introduced. As a case study, a specific reconfigurable structure for symmetric ciphers is implemented. Performance of several typical symmetric ciphers are evaluated. The experimental results show high performance and efficiency of ProDFA.
[Context, Ciphers, run-time reconfigurable architecture, computing paradigm, spatial computing, coarse grained runtime reconfigurable architecture, cryptography, coarse grain architecture, Synchronization, spatiotemporal phenomena, temporal computing, symmetric cipher, accelerating domain application, ProDFA, reconfigurable architectures, on-chip resource, data flow computing, reconfigurable computing, Arrays, System-on-a-chip, programmable dataflow computing architecture]
Demonstration of Damson: Differential Privacy for Analysis of Large Data
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
We demonstrate Damson, a novel and powerful tool for publishing the results of biomedical research with strong privacy guarantees. Damson is developed based on the theory of differential privacy, which ensures that the adversary cannot infer the presence or absence of any individual from the published results, even with substantial background knowledge. Damson supports a variety of analysis tasks that are common in biomedical studies, including histograms, marginals, data cubes, classification, regression, clustering, and ad-hoc selection-counts. Additionally, Damson contains an effective query optimization engine, which obtains high accuracy for analysis results, while minimizing the privacy costs of performing such analysis.
[Data privacy, pattern classification, data analysis, marginals, histograms, Noise, regression analysis, medical analysis, Remuneration, substantial background knowledge, biomedical research, data cubes, query processing, Privacy, Histograms, Accuracy, large data, pattern clustering, large data analysis, ad-hoc selection-counts, data privacy, Damson demonstration, differential privacy, Logistics]
SWAT: Social Web Application for Team Recommendation
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Team recommendation aids decision support, by not only identifying individuals who are experts for various aspects of a complex task, but also determining various properties of the team as a group. Several aspects such as cohesion and repetition of teams have been identified as important indicators, besides individuals' expertise, in determining how well a team performs. While such information often do not exist explicitly, digital footprint of users' activities can be harnessed to retrieve the same from diverse sources. In this work, we lay out a proof-of-concept on how to do so in the case of scientific knowledge workers, as well as demonstrate some necessary visualization, manipulation and communication tools to determine and manage multi-disciplinary teams. While the focus of our presentation is the specific application 'SWAT' for team recommendation, it also serves as a vehicle demonstrating how, in general, apparently disparate data sources can be harnessed to provide decision support guided by suitable analytics.
[Electronic publishing, visualization, Navigation, Social network services, team recommendation, scientific knowledge workers, digital footprint, social networks, Encyclopedias, Cleaning, decision support, team, expertise, social Web application, multidisciplinary teams, decision support systems, recommendation system, SWAT, analytics, recommender systems, Collaboration, data visualisation, social networking (online)]
A Demo Paper: An Analytic Workflow Framework for Green Campus
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
This paper proposes a multi-tenant workflow framework that allows users to create data analytic workflows whose tasks are efficiently scheduled and distributed in cloud computing environment. We provide a demo of an event room assignment (ERA) as a test application of the framework. The ERA dynamically and automatically assigns registered events (e.g., meetings, classes, conferences, etc.) to available rooms meeting the user requirements such as the event size, purpose, reservation period, etc. The assignment will lead to the energy efficiency with respect to the power usage (e.g., lighting, ventilation, devices, etc.), and the energy savings can be achieved without affecting people's comfort. We run the ERA with power consumption data (whose size is approximately 50GB) collected from each of over 200 rooms in a building at Dept. of Engineering, Tokyo University. Through the demonstration, we will show that the proposed framework accelerates the speed of data analysis by providing user-friendly workflow composition and parallel processing features utilizing cloud computing technologies.
[Cloud computing, Big Data Analytics, parallel processing, event room assignment, Department of Engineering, green campus, Tokyo University, power aware computing, power usage, Cloud Computing, distributed tasks, user requirements, Computer architecture, analytic multitenant workflow framework, energy efficiency, Real-time systems, user-friendly workflow composition, cloud computing, workflow management software, Power demand, data analysis, Buildings, data analytic workflow, Educational institutions, cloud computing environment, power consumption data, ERA, XML, human computer interaction, green computing, task scheduling, cloud computing technologies, Workflow Composition]
PlugCloud - Scaling by Plugging a Personal Cloud Infrastructure
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
We present a novel elastic system architecture called Plug Cloud, which aims to increase the power of low-compute (and possibly mobile) devices, such as tablets, through distributing high-compute tasks, such as rendering, data analysis and visualization, to a set of Plug Computers [1, 2] that can be added or removed from the system incrementally. These devices are connected through a wired/wireless connection and the network is formed seamlessly with zero configurations. Plug Cloud allows users of low-compute devices to acquire more processing power externally on demand by plugging one or more plug-computers as needed. Furthermore, it allows a user to remove the plug-computer safely at any time without bringing down the whole system. This makes an elastic network where it can expand and shrink automatically with one or more plug-computers being added or removed from the system. This innovative architecture forms a personal cloud infrastructure on demand to support users' computational needs. We have implemented a computer graphics rendering application using Plug Cloud architecture. We will demo our architecture and prototype system using a tablet and several plug-computers.
[Computers, personal cloud infrastructure, Cloud computing, Distribution strategy, data analysis, Tablet Computer, Processing power on-demand, PlugCloud, elastic system architecture, plug computers, mobile computing, power aware computing, Plug Computing, data visualisation, Computer architecture, zero configurations, computer graphics rendering, Rendering (computer graphics), plug computer safely, cloud computing, Personal Cloud, Plugs]
Design and Development of an Adaptive Workflow-Enabled Spatial-Temporal Analytics Framework
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Cloud computing is a suitable platform for execution of complex computational tasks and scientific simulations that are described in the form of workflows. Such applications are managed by Workflow Management System (WfMS). Because existing WfMSs are not able to autonomically provision resources to real-time applications and schedule them while supporting fault tolerance and data privacy, we present a highly-scalable workflow-enabled analytics system that manages inter-dependable analytics tasks adaptively with varying operational requirements on a common platform and enables visualization of multidimensional datasets of real world phenomena. In this paper, we present the architecture of such a WfMS and evaluate it in terms of performance for execution of workflows in Clouds. A real world application of climate-associated dengue fever prediction was evaluated on public, private, and hybrid Clouds and experienced effective speedup in all the environments.
[Cloud computing, Data privacy, multidimensional datasets, scientific simulations, fault tolerance, workflow, adaptive workflow enabled spatial temporal analytics framework, Security, real-time applications, Data visualization, real-time systems, Computer architecture, Real-time systems, data privacy, Resource management, cloud computing, WfMS, dynamic resource, workflow management software]
GPU-accelerated Computation of 3D Laser Radar Range Imaging of Arbitrary Coarse Targets
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
We report here a backscattering model of average signal power function (SPF) for laser radar 3D range imagery obtained by arrays of detectors for arbitrary coarse targets. The model relates the average power seen by the receiver with laser pulse, target shape, optical scattering properties of surface material, incidence angle and other factors. The optical scattering property of the material is characterized by bidirectional reflectance distribution function (BRDF). The model can be used for demonstration of 3D laser radar system and can also be used to generate library of model data sets for automatic target recognition (ATR). Finally, Compute Unified Device Architecture (CUDA) is introduced for parallelizing these algorithms. The acceleration reaches 56 times speedup on single Fermi-generation NVIDIA GTX 480 as compared to the traditional CPU version of code on Intel i7 930 CPU.
[parallel architectures, GPU-accelerated computation, Graphics processing units, bidirectional reflectance distribution function, Backscattering, SPF, BRDF, Power lasers, Lasers, Detectors, Radar imaging, surface material, Laser modes, radar imaging, parallel computing, optical radar, CPU version, laser pulse, automatic target recognition, Intel i7 930 CPU, compute unified device architecture (CUDA), compute unified device architecture, 3D laser radar range imaging, graphics processing units, incidence angle, CUDA, single Fermi-generation NVIDIA GTX 480, Laser radar, Image analysis, backscattering model, model data sets, arbitrary coarse targets, optical scattering properties, image recognition, average signal power function, target shape, ATR]
Parallel and Distributed Processing of Remote Sensing Data on Large Displays
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
A typical LIDAR (Light Detection and Ranging) scan contains hundreds of millions of points. As such, the visualization of LIDAR point clouds poses a significant challenge in data analysis. One solution is to display LIDAR point clouds on a large display wall with an array of LCD monitors. This provides researchers with a high-resolution display environment for looking at and studying large datasets. In this paper, we present a case study that visualizes LIDAR point clouds on a tiled display wall termed HIPerDisplay (Highly Interactive Parallelized Display). It has twenty 24-inch LCDs with a total resolution of 46 megapixels. Interaction between the user and the display wall is achieved by using a video camera system that is able to track the position of a hand-held light ball device. A user holds it to manipulate point clouds on HIPerDisplay. Case studies are conducted to study the LIDAR scans of slopes in the Houshanyue mountain areas in Taiwan. Experiments were conducted to examine the advantages of using the HIPerDisplay for point clouds in data post-processing. The experiments assess two tasks for manipulating point cloud data designed to evaluate the efficiency of the interactive devices. To evaluate the efficiency of the system, a group of thirty graduate students participated in the experiment. User surveys were performed to evaluate the efficiency of the system and to discover the users' opinions about using the interactive device in a large display environment. The results showed that the participants preferred to perform LIDAR data operation tasks on a high-resolution large display environment rather than on a single monitor. The results also showed that HIPerDisplay offered superior performance for the processing of large LIDAR datasets.
[Performance evaluation, HIPerDisplay, video camera system, Noise, light detection and ranging, distributed processing, highly interactive parallelized display, parallel processing, data visualisation, radar imaging, LIDAR point clouds visualization, Monitoring, optical radar, LCD monitors, liquid crystal displays, tiled display wall, video cameras, geophysical image processing, remote sensing, point cloud, Surface topography, large displays, Laser radar, Data visualization, remote sensing data, LIDAR scan, Arrays, hand-held light ball device]
A GPU-based Implementation of WRF PBL/MYNN Surface Layer Scheme
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Nakanishi and Niino proposed an improved Mellor-Yamada (M-Y) Level-3 model (MYNN) surface for three-dimensional simulation of advection fog. The model is on the basis of large-eddy simulation and is numerically stable. The model predicts vertical profiles of mean quantities such as temperature that are in good agreement with those obtained from large-eddy simulation of a radiation fog. In this paper, we accelerate Nakanishi and Niino PBL's Surface Layer scheme in a highly parallel environment, using NVIDIA Graphics Processing Units (GPU). This GPU implementation efficiently utilizes the fine grained parallelism exhibited by the MYNN PBL scheme. The algorithm is accelerated on a low-cost personal supercomputer with over 500 CUDA cores running on a GPU. This implementation achieves a high speedup of 160&#x00D7;.
[CUDA core, eddy simulation, NVIDIA, Instruction sets, parallel architectures, Graphics Processing Units (GPU), Graphics processing units, Predictive models, parallel machines, GPU, numerical stability, fine grained parallelism, weather forecasting, surface layer scheme, radiation fog, Numerical models, Meteorology, advection fog, MYNN PBL/Surface Layer Model, Atmospheric modeling, Computational modeling, WRF MYNN PBL, geophysics computing, personal supercomputer, graphics processing units, CUDA, 3D simulation, graphics processing unit, fog, Weather Research and Forecasting (WRF)]
GPU-based Calculation for Scattering Characteristics of Complex Targets from Background Radiance in Infrared Spectrum
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Scattering characteristic of complex targets from sky and ground background radiance plays an important role in engineering fields. Firstly, a 5-parameter BRDF (Bidirectional Reflectance Distributional Function) model is introduced. Then MODTRAN is used to calculate the background radiance in infrared spectrum of 3-5 um and 8-12um bands. Considering the background radiance comes from all directions of space in large numbers of different bands, there will be multiple loops in the computation thus it's quite time-consuming. Thanks to the NVIDIA CUDA (Compute Unified Device Architecture), programing GPU does not require as much knowledge about graphics card and complex programing interfaces as before. On the basis of CUDA, a parallel implementation is presented and to get a higher speedup the code is optimized to reduce the access latency as much as possible by using the shared and constant memory on GPU. The implementation is test on an NVIDIA GTX GeForce 480 and 2.79 GHz Intel i7 CPU. Compared to the CPU implementation, we achieve a peak speedup of 308 and results showing the efficiency of the parallelism and optimization.
[Instruction sets, parallel architectures, Graphics processing units, Random access memory, 5-parameter BRDF model, GPU programing, complex target scattering characteristics, BRDF, GPU, parallel programming, moderate resolution atmospheric transmission, bidirectional reflectance distributional function model, optimization, NVIDIA CUDA, Computer architecture, GPU-based calculation, Atmospheric modeling, Scattering, background radiance, parallel implementation, geophysics computing, scattering characteristic, Educational institutions, compute unified device architecture, NVIDIA GTX GeForce 480, graphics processing units, CUDA, brightness, scattering, infrared spectrum, MODTRAN, Intel i7 CPU, code optimization]
GPU Parallel Computing of Spherical Panorama Video Stitching
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
This paper presents a GPU-based spherical coordinate conversion system for panorama video image stitching. Modern programmable GPU makes it possible to process multiple images in an interactive frame rates. To perform image stitching to form a panorama view, we use OpenCL to stitch multiple images and then texture map it to a spherical object. This allows us to compose an immersive environment. In the case study presented in this paper, we achieve a speedup factor of 76x.
[spherical panorama, Pipelines, spherical panorama video stitching, graphics processing units, parallel processing, GPU, image texture, texture map, Graphics, Webcams, image segmentation, panorama video image stitching, GPU-based spherical coordinate conversion system, Parallel processing, Streaming media, Real-time systems, OpenCL, programmable GPU, Kernel, video signal processing, GPU parallel computing]
Accelerating Volkov's Hybrid Implementation of Cholesky Factorization on a Fermi GPU
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
In linear algebra, Cholesky factorization is useful in solving a system of equations with a symmetric positive definite coefficient matrix. Cholesky factorization is roughly twice as fast relative to LU factorization which applies to general matrices. In recent years, with advances in technology, a Fermi GPU card can accommodate hundreds of cores compared to the small number of 8 or 16 cores on CPU. Therefore a trend is seen to use the graphics card as a general purpose graphics processing unit (GPGPU) for parallel computation. In this work, Volkov's hybrid implementation of Cholesky factorization is evaluated on the new Fermi GPU with others and then some improvement strategies were proposed. After experiments, compared to the CPU version using Intel Math Kernel Library (MKL), our proposed GPU improvement strategy can achieve a speedup of 3.85x on Cholesky factorization of a square matrix of dimension 10,000.
[general matrices, square matrix, parallel computation, Symmetric matrices, Instruction sets, Graphics processing units, Educational institutions, general purpose graphics processing unit, graphics processing units, parallel processing, GPGPU, Intel Math Kernel Library, matrix algebra, MKL, accelerating Volkov hybrid implementation, Fermi GPU, Linear algebra, symmetric positive definite coefficient matrix, Libraries, Kernel, parallel computing, Cholesky factorization]
Geographic Routing with Minimal Local Geometry
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Geographic routing based on virtual coordinates has been studied extensively, especially in environments expensive localization techniques are infeasible. Even though, the construction of virtual coordinate system is theoretically understood, their practical deployment is questionable due to computational requirements. An alternative approach is to use raw range measures from a special set of nodes called "anchors" as virtual coordinates, which only preserve partial geographic knowledge. In this paper we follow a similar approach, but focus on answering the question "what are the minimal geometric primitives required to perform geometric routing?". We take the first step towards answering this question, based on a node centric local geometric view of localized nodes. We define local geometric primitives and show that geographic face routing can be performed with those primitives.
[partial geographic knowledge, node centric local geometric view, Geometric Primitives, Routing, Ad hoc networks, Geographic Routing, Topology, virtual coordinate construction, Geometry, minimal local geometry, telecommunication network routing, computational requirement, anchor node, minimal geometric primitive, geometry, Lo- calization, geographic face routing, Face, environments expensive localization technique, Wireless ad-hoc routing, Clocks, Virtual Coordinates]
Visualization for Anomaly Detection and Data Management by Leveraging Network, Sensor and GIS Techniques
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
This paper studies the importance of visualization for discerning and interpreting patterns of data and its application for solving real problems, such as anomaly detection and data management. There are various ways to realize visualization to cater to the needs of numerous real life applications. Depending on needs, a combination of some of these ways may be required for presenting an effective visualization. The authors present visualization schemes for anomaly detection/condition monitoring and data management by leveraging network techniques and combining them with modern techniques such as sensor, database, mobile communication, GPS and GIS techniques. Two case studies are presented and analyzed. By stepping through the design and implementation processes of these projects, this paper aims to serve as a guide for other designers or researchers to create visual analysis tools or implement projects requiring such visualization.
[Visualization, visualization, infrared detectors, data pattern visualization, Mobile communication, geographic information systems, anomaly detection, GPS, Servers, database management systems, visual analysis tools, network, sensor techniques, Databases, data visualisation, GIS techniques, Real-time systems, data management visualization, network techniques, Global Positioning System, GIS, mobile communication, Data visualization, computerised monitoring, condition monitoring, sensor, anomaly detection visualization, programing design]
Power Control Algorithm Based on SNR Cost Function in Cognitive Radio System
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
The power control problem in the cognitive radio system is studied in this paper with game theory. A cost function based on fairness is designed, a power control algorithm based on SNR cost function is proposed, and a Non-cooperative Power control Game via Fairness Pricing(NPGFP) algorithm is developed. The algorithm proposed in this paper-NPGFP algorithm uses the complete information game method to make each cognitive user dynamically adjust its power parameters by detecting the power and location parameters of other cognitive users and the primary users. The algorithm ensures the system convergence stability and the benefit maximization of each cognitive user by making the transmit power to reach a steady state with iteration. By simulation in three different scenes it is shown that NPGFP algorithm is better than NPGP(Non-cooperative Power control Game model based on Pricing) algorithm in throughput, SNR, cost function of the cognitive user and the stability of the algorithm convergence.
[Algorithm design and analysis, iterative methods, Throughput, cognitive radio, cognitive user, location parameter detection, SNR, cost function, Cost function, SNR cost function, NPGP algorithm, system convergence stability, signal detection, noncooperative power control game via fairness pricing algorithm, primary users, telecommunication control, game theory, Cognitive radio, iterative method, power parameter detection, cognitive radio system, NPGFP algorithm, Power control, Games, benefit maximization, power control, noncooperative power control game model-based-on-pricing algorithm, power control algorithm, Signal to noise ratio]
A Remote View System to Allow Real-Time Walkthrough
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
A remote view system is a system for the delivery and presentation of information such as video and audio to users in remote locations. Methods for the construction of virtual environments based on video information have been applied to remote view technology. When a system that supports remote viewing is applied to situations such as viewing scenery or engaging in collaborative work, it is useful to provide users with a function that enables them to walk through a virtual environment. Although walkthroughs are possible in remote view systems that use pre-prepared images to construct virtual environments, these systems are lacking in terms of their real-time performance. In this study, we built a virtual walk-through environment that allows remote viewing based on images sent in real time from an omni directional camera. By using an improved map projection method, the system produces less image distortion than the conventional projection method. We have confirmed the walk-through capabilities of this system, evaluated the performance of improved map projection method, and subjected it to user trials.
[remote view, multi-users, remote view system, Servers, image distortion, remote locations, real time walkthrough, groupware, Real-time systems, video signal processing, collaborative work, pre prepared images, virtual walk-through environment, image sensing, panoramic image, Virtual environments, Educational institutions, omnidirectional camera, remote view technology, omni directional camera, Surveillance, improved map projection method, Keyboards, video information, Distributed network application, Cameras, walkthrough]
Multicore Computing for SIFT Algorithm in MATLAB Parallel Environment
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
An important processing stage in computer vision such as recognizes an object is feature extraction. Among those feature extraction algorithms, Lowe proposed the scale-invariant feature transform (SIFT) algorithm has been considered as one of the robust approaches. But due to the implementation of the convolution operation, the computing of SIFT algorithm is highly time-consuming. There are such problems as consuming too much power, sacrifice accuracy and lacking scalability for hardware-based acceleration scheme, it is still necessary to find the software-based acceleration approaches, especially for some applications and researches which need high-precision matched. Matlab is a general algorithm development environment with powerful image processing and other supporting toolboxes. With the rapid development of multicore CPU technology, using multicore computer and Matlab is an intuitive and simple way to speed up the computing for SIFT algorithm. In this paper, we try to have a view for using the Matlab parallel toolbox to accelerate the SIFT algorithm by two schemes of task-parallelism and data-parallelism modal. The results show that the parallel versions of former sequential algorithm with simple modifications achieve the speedup up to 6.6 times.
[Computers, object recognition, convolution search operation, mathematics computing, multicore CPU technology, transforms, hardware-based acceleration scheme, MATLAB, parallel processing, software-based acceleration approaches, Histograms, multicore computing, feature extraction, Matlab parallel toolbox component, Parallel processing, convolution, Feature Extraction Scale-invariant Feature Transform(SIFT), image processing, scale-invariant feature transform, sequential algorithm, data-parallelism, multiprocessing systems, Multicore processing, task-parallelism, feature extraction algorithms, Matlab parallel toolbox, multicore computer, Multicore, SIFT algorithm, MATLAB parallel environment, computer vision, Feature extraction, Acceleration, toolboxes]
User Satisfaction-Based Scheduling Algorithm for Uplink Transmission in Long Term Evolution System
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
With low Peak-to-Average Power Ratio (PAPR), Single Carrier &#x00A1;V Frequency Division Multiple Access (SCFDMA) is adopted by 3GPP to be the uplink access method of the LTE system. SC-FDMA allocates continuous Resource Blocks (RBs) to each subscriber to reduce PAPR. However, the continuous RB allocation degrades the flexibility in resource allocation. How to efficiently allocate radio resource in the SCFDMA scheme has been a critical issue. This paper proposes a scheduling algorithm concerning user satisfaction in LTE uplink radio resource allocation. The proposed algorithm is a generalization and an improvement of the well-known Mean Enhanced Greedy (MEG) scheduling algorithm. The simulation results show that the proposed algorithm achieves not only higher user satisfaction degree but also better fairness between subscribers than the MEG scheduling algorithm and a related algorithm, called the Recursive Maximum Expansion (RME) algorithm, no matter the desired uplink transmission rates among subscribers are the same or not.
[MEG scheduling algorithm, mean enhanced greedy scheduling algorithm, 3G mobile communication, Peak to average power ratio, Throughput, LTE uplink radio resource allocation, scheduling algorithm, uplink transmission, single carrier-frequency division multiple access, RB allocation, User Satisfaction, resource blocks, RME algorithm, Scheduling algorithms, resource allocation, PAPR, scheduling, Universal Serial Bus, Uplink, Long Term Evolution, user satisfaction-based scheduling algorithm, frequency division multiple access, recursive maximum expansion, greedy algorithms, peak-to-average power ratio, Scheduling, Indexes, Resource Block, long term evolution system, SC-FDMA, Resource management, LTE]
A Design of Middleware Layer for QoS in Wireless Sensor Networks
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
Based on the characteristics of the wireless sensor networks, a middleware layer for quality of service support was proposed. For kinds of user's QoS requirements, tasks of network were scheduled by collecting and updating local information continuously, and the reliability of network was enhanced by redundancy. Results of simulation show that the middleware mechanism can not only meet the user QoS requirements, but also reduce the energy consumption of network.
[Energy consumption, wireless sensor networks, Delay effects, Quality of service, wireless sensor network, reliability, quality of service, Middleware, Delay, Wireless sensor networks, QoS, telecommunication network reliability, redundancy, middleware layer mechanism, Monitoring, middleware]
A Smart Home System Based on ZigBee and IOS Software
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
A smart home system based on ZigBee and iOS is proposed. ZigBee is a low-power personal area network. This technology is combined with the mainstream IOS operational system, which established a wireless smart home system. The system uses CC2430 chip and ZigBee2006 protocol stack from TI as the core of the main system with the aid of iOS which enables the realization of remote control and monitoring. Specific designs including light intensity detection module, temperature sensitive module, light control module and curtain control module are demonstrated in this paper. The whole system demonstrates characteristics of low cost, low power dissipation, user-friendliness and scalability.
[Protocols, Zigbee, light control module, Smart homes, Servers, iOS, Network topology, light intensity detection module, personal area network, TCP/IP, remote monitoring, IP networks, protocols, ZigBee2006 protocol stack, temperature sensitive module, Smart home system, UDP, ZigBee, CC2430, remote control, iOS software, HTTP, Topology, CC2430 chip, wireless smart home system, energy-saving, home automation, curtain control module, telecontrol, TI, low power dissipation, computerised monitoring, operating systems (computers), system-on-chip]
GPGPU for real-time data analytics
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
The demand for real-time data analytics (RTDA) has been on the rise in the past decades and is ever-growing with the proliferation of different data collection devices.GPGPU (General-Purpose computation on Graphics Processing Units) is an emerging research area in HPC (high performance computing). With the massive computation power and high memory bandwidth, GPUs have become a sharp weapon to address the performance requirement of RTDA. Designed as co-processors, GPUs pose a number of technical challenges for RTDA in terms of efficiency and programmability. On the one hand, while new generation GPUs can have over an order of magnitude higher memory bandwidth and higher computation power (in terms of GFLOPS) than CPUs, novel GPGPU algorithmic design and implementation are a must to unleash the hardware power. On the other hand, writing a correct and efficient GPU program is still challenging in general, and even more difficult for RTDA with streaming updates and real-time multi-tasking.
[real-time data analytics, data analysis, computation power, GFLOPS, Graphics processing units, Tutorials, CPU, data collection devices, Helium, graphics processing units, co-processors, programmability, GPGPU, Graphics, memory bandwidth, GPGPU algorithmic design, GPGPU implementation, real-time multitasking, USA Councils, data visualisation, RTDA, general-purpose computation on graphics processing units, Real-time systems, streaming updates]
Tutorial: Intel many integrated core (MIC) architecture
2012 IEEE 18th International Conference on Parallel and Distributed Systems
None
2012
The document was not made available for publication as part of the conference proceedings.
[Microwave integrated circuits, Conferences, Tutorials]
Message from the General Chair
2013 International Conference on Parallel and Distributed Systems
None
2013
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from the Program Chairs
2013 International Conference on Parallel and Distributed Systems
None
2013
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Organizing Committee
2013 International Conference on Parallel and Distributed Systems
None
2013
Provides a listing of current committee members and society officers.
[]
Program Committee
2013 International Conference on Parallel and Distributed Systems
None
2013
Provides a listing of current committee members and society officers.
[]
Workshop Organizers
2013 International Conference on Parallel and Distributed Systems
None
2013
Provides a listing of current committee members and society officers.
[]
Keynotes [2 abstracts]
2013 International Conference on Parallel and Distributed Systems
None
2013
Provides an abstract for each of the two keynote presentations and a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.
[]
Detecting Correlation Violations and Data Races by Inferring Non-deterministic Reads
2013 International Conference on Parallel and Distributed Systems
None
2013
With the introduction of multicore systems and parallel programs concurrency bugs have become more common. A notorious class of these bugs are data races that violate correlations between variables. This happens, for example, when the programmer does not update correlated variables atomically, which is needed to maintain their semantic relationship. The detection of such races is challenging because correlations among variables usually escape traditional race detectors which are oblivious of semantic relationships. In this paper, we present an effective method for dynamically identifying correlated variables together with a race detector based on the notion of non-deterministic reads that identifies malicious data races on correlated variables. In eight programs and 190 micro benchmarks, we found more than 100 races that were overlooked by other race detectors. Furthermore, we identified about 300 variable correlations which were violated by these races.
[Algorithm design and analysis, data race detection, program debugging, Correlation, multiprocessing systems, Heuristic algorithms, race detector, Synchronization, parallel programming, parallel programs, multicore systems, Computer bugs, Detectors, program analysis, concurrency bugs, Debugging and testing, correlated variables, Detection algorithms, correlation violation detection, data races, nondeterministic read inference]
Efficient Barrier Synchronization for OpenMP-Like Parallelism on the Intel SCC
2013 International Conference on Parallel and Distributed Systems
None
2013
The continuous increase of the number of processing cores on die poses a new set of challenges to HPC applications programming including how to model, write, and verify software that has to use the full power of NoC-based manycore processors. Therefore, to simplify program development for the Single-chip Cloud Computer (SCC), it is desirable to have high-level, shared memory-based parallel programming abstractions (e.g., an OpenMP-like programming model). One of the key components of any similar programming model are barrier synchronization primitives, coordinating the work of parallel threads. To allow high-level barrier constructs to deliver good performance, we need an efficient implementation of the underlying synchronization algorithm. In this paper, we propose effective barrier synchronization implementations for shared-memory programming on non-cache-coherent cluster-on-chip represented by the Intel SCC. In particular, we present an extensive evaluation of the overhead associated with integrating barrier algorithms required for OpenMP runtime libraries on such a machine, validating several implementation variants that efficiently exploit the network topology and leveraging SCC-specific hardware. We provide a detailed evaluation of the performance achieved by different approaches by using micro-benchmarks.
[single-chip cloud computer, Instruction sets, processing cores, System-on-Chip, Programming, Table lookup, parallel thread, parallel programming, OpenMP runtime library, NoC-based manycore processor, SCC-specific hardware, shared memory systems, Barrier synchronization, synchronization algorithm, cloud computing, barrier synchronization primitives, shared-memory programming, HPC applications programming, Message systems, shared memory-based parallel programming abstractions, network-on-chip, Radiation detectors, integrating barrier algorithms, OpenMP, high-level barrier constructs, microprocessor chips, Synchronization, barrier synchronization implementation, network topology, synchronisation, Many-cores, program development, OpenMP-like parallelism, OpenMP-like programming model, Performance Evaluation, noncache-coherent cluster-on-chip, Delays, Intel SCC]
Mark-Sharing: A Parallel Garbage Collection Algorithm for Low Synchronization Overhead
2013 International Conference on Parallel and Distributed Systems
None
2013
Two main problems prevent a parallel garbage collection (GC) scheme with lock-based synchronization from providing a high level of scalability: the load imbalance and the runtime overhead of thread synchronization operations. These problems become even more serious as the number of available threads increases. We propose the Mark-Sharing algorithm to improve the performance of parallel GC using transactional memory (TM) systems. The Mark-Sharing algorithm guarantees that all threads access the shared resource by using both the task-stealing and task-releasing mechanisms appropriately. In addition, we introduce a selection manager that minimizes the contention and idle time of garbage collectors by maintaining task information. The proposed algorithm outperforms the prior pool-sharing algorithm of GC in the HTM, providing more than 90% performance improvement on average.
[Algorithm design and analysis, HTM, thread synchronization operations, Instruction sets, Heuristic algorithms, Scalability, parallel garbage collection algorithm, pool-sharing algorithm, Load Balancing, hardware transactional memory, parallel processing, parallel GC, load imbalance, storage management, task-stealing mechanisms, Runtime, Garbage Collection, Thread Synchronization, resource allocation, Low Contention, mark-sharing algorithm, Radiation detectors, lock-based synchronization overhead, task-releasing mechanisms, Synchronization, synchronisation, transactional memory systems, Hardware Transactional Memory]
Adaptive Packet Resizing by Spatial Locality and Data Sharing for Energy-Efficient NOC
2013 International Conference on Parallel and Distributed Systems
None
2013
Single-processor chips have given way to multicore chips to enable a cost-effective implementation of computer systems. Toward continuous performance scaling, Network-On-Chip (NOC) is the communication architecture supporting the core count increase to hundreds or thousands in multicore chips. Low-power, low-latency, and high-bandwidth support in the NOC design is critical for meeting performance and energy targets of the overall system. Much of previous work has focused on improving the NOC design but without more fully taking into consideration the communication characteristics and the interplay with cache memory that can be exploited in the NOC design. In this paper, low spatial locality within cache blocks is exploited in reducing memory traffic toward energy savings in the NOC. We present a spatial locality predictor that separately manages different degrees of spatial locality across shared and private blocks for better prediction accuracy. To further optimize performance and power in the NOC, we present the adaptive control of the predictor and packet data resizing techniques. Evaluations for the 16-core system running PARSEC benchmarks reveal that our spatial-locality based packet resizing improves NOC power consumption on average by 21% (up to 33%).
[Conferences, energy savings, cache storage, communication architecture, power aware computing, packet data resizing techniques, multicore chips, spatial-locality based packet resizing, Spatial Locality, network-on-chip, NOC design, Network-on-Chip, PARSEC benchmarks, performance evaluation, adaptive control, cache blocks, single-processor chips, NOC power consumption, Energy Efficiency, Multicore, low spatial locality, memory traffic reduction, continuous performance scaling, energy-efficient NOC, computer systems, spatial locality predictor, 16-core system evaluation, performance optimization, data sharing, adaptive packet resizing]
File-Level, Host-Side Flash Caching with Loris
2013 International Conference on Parallel and Distributed Systems
None
2013
As enterprises shift from using direct-attached storage to network-based storage for housing primary data, flash-based, host-side caching has gained momentum as the primary latency reduction technique. In this paper, we make the case for integration of flash caching algorithms at the file level, as opposed to the conventional block-level integration. In doing so, we will show how our extensions to Loris, a reliable, file-oriented storage stack, transform it into a framework for designing layout-independent, file-level caching systems. Using our Loris prototype, we demonstrate the effectiveness of Loris-based, file-level flash caching systems over their block-level counterparts, and investigate the effect of various write and allocation policies on the overall performance.
[Algorithm design and analysis, Performance evaluation, host-side flash caching, NAS, direct-attached storage, SSD, Physical layer, cache storage, network-based storage, Indexes, Servers, file-oriented storage stack, primary latency reduction technique, primary caching, allocation policies, file-level flash caching, Caching, File System Architecture, Layout, Ash, flash caching algorithm integration, Loris, Flash, write policies]
AGIOS: Application-Guided I/O Scheduling for Parallel File Systems
2013 International Conference on Parallel and Distributed Systems
None
2013
In this paper, we improve the performance of server-side I/O scheduling on parallel file systems by transparently including information about the applications' access patterns. Server-side I/O scheduling is a valuable tool on multiapplication scenarios, where the applications' spatial locality suffers from interference caused by concurrent accesses to the file system. We present AGIOS, an I/O scheduling library for parallel file systems. We guide scheduler's decisions by including information about the applications' future requests. This information is obtained from traces generated by the scheduler itself, without changes in application or file system. Our approach shows performance improvements under different workloads of 46.3% on average when compared to a scenario without an I/O scheduler, and of 25.1% when compared to a scheduler which does not use information about future accesses.
[Interference, application-guided I-O scheduling, I/O, Disk Access, Scheduling, Servers, Synchronization, parallel processing, application access patterns, application spatial locality, AGIOS, Scheduling algorithms, File systems, server-side I-O scheduling, scheduling, I/O Scheduling, file organisation, Libraries, parallel file systems, Parallel File Systems]
A Distributed Cache Framework for Metadata Service of Distributed File Systems
2013 International Conference on Parallel and Distributed Systems
None
2013
Most recent distributed file systems have adopted architecture with an independent metadata server cluster. However, potential multiple hotspots and flash crowds access patterns often cause a metadata service that violates performance Service Level Objectives. To maximize the throughput of the metadata service, an adaptive request load balancing framework is critical. We present a distributed cache framework above the distributed metadata management schemes to manage hotspots rather than managing all metadata to achieve request load balancing. This benefits the metadata hierarchical locality and the system scalability. Compared with data, metadata has its own distinct characteristics, such as small size and large quantity. The cost of useless metadata prefetching is much less than data prefetching. In light of this, we devise a time period-based prefetching strategy and a perfecting-based adaptive replacement cache algorithm to improve the performance of the distributed caching layer to adapt constantly changing workloads. Finally, we evaluate our approach with a hadoop distributed file system cluster.
[adaptive request load balancing framework, metadata server cluster, load balancing, time period-based prefetching strategy, distributed file system, data prefetching, cache storage, Servers, caching, distributed cache framework, distributed metadata management schemes, Accuracy, resource allocation, distributed databases, flash crowds, system scalability, Monitoring, Load modeling, Hadoop distributed file system cluster, metadata hierarchical locality, meta data, perfecting-based adaptive replacement cache algorithm, Smoothing methods, Prefetching, independent metadata server cluster, distributed caching layer performance, prefetching, metadata prefetching, metadata service, Load management]
Achieving TeraCUPS on Longest Common Subsequence Problem Using GPGPUs
2013 International Conference on Parallel and Distributed Systems
None
2013
In this paper, we describe a novel technique to optimize longest common subsequence (LCS) algorithm for one-to-many matching problem on GPUs by transforming the computation into bit-wise operations and a post-processing step. The former can be highly optimized and achieves more than a trillion operations (cell updates) per second (CUPS)-a first for LCS algorithms. The latter is more efficiently done on CPUs, in a fraction of the bit-wise computation time. The bit-wise step promises to be a foundational step and a fundamentally new approach to developing algorithms for increasingly popular heterogeneous environments that could dramatically increase the applicability of hybrid CPU-GPU environments.
[longest common subsequence algorithm optimization, bit-wise operations, TeraCUPS, Instruction sets, Graphics processing units, GPU, parallel programming, GPGPU, Longest Common Subsequence, LCS algorithm optimization, Computer architecture, Parallel processing, Dynamic programming, Kernel, hybrid CPU-GPU environments, heterogeneous environments, semi-regular algorithms, postprocessing step, graphics processing units, Equations, CUDA, cell updates, string matching, one-to-many matching problem, bit-wise computation time]
On the GPU-CPU Performance Portability of OpenCL for 3D Stencil Computations
2013 International Conference on Parallel and Distributed Systems
None
2013
Although OpenCL programming provides full code portability between different hardware platforms, performance portability can be far from satisfactory. In this work, we use a set of representative 3D stencil computations to study OpenCL's performance portability between GPUs and CPUs. For each stencil computation, we have devised different implementations of the computational kernel function, all being 100% code-portable between the two architectures. The most straightforward and compact implementation gives satisfactory CPU performance but performs poorly on GPUs, because such an implementation hampers effective use of the GPU hardware. By injecting code complexity into the involved loop nests, we can create kernel functions that still have full code portability but with increased performance portability. It is found that spatial data blocking and register reuse can be beneficial for performance on both GPUs and CPUs, whereas use of OpenCL's local memory (and subsequent temporal blocking) may only have positive effects on GPUs.
[spatial data blocking, Graphics processing units, GPU-CPU, Programming, Registers, graphics processing units, representative 3D stencil computations, code complexity, software portability, GPU-CPU performance portability, Three-dimensional displays, performance portability, hardware platforms, Computer architecture, full code portability, 3D stencil computation, Hardware, OpenCL programming, OpenCL, Kernel]
Wideband Channelization for Software-Defined Radio via Mobile Graphics Processors
2013 International Conference on Parallel and Distributed Systems
None
2013
Wideband channelization is a computationally intensive task within software-defined radio (SDR). To support this task, the underlying hardware should provide high performance and allow flexible implementations. Traditional solutions use field-programmable gate arrays (FPGAs) to satisfy these requirements. While FPGAs allow for flexible implementations, realizing a FPGA implementation is a difficult and time-consuming process. On the other hand, multicore processors while more programmable, fail to satisfy performance requirements. Graphics processing units (GPUs) overcome the above limitations. However, traditional GPUs are power-hungry and can consume as much as 350 watts, making them ill-suited for many SDR environments, particularly those that are battery-powered. Here we explore the viability of low-power mobile graphics processors to simultaneously overcome the limitations of performance, flexibility, and power. Via execution profiling and performance analysis, we identify major bottlenecks in mapping the wideband channelization algorithm onto these devices and adopt several optimization techniques to achieve multiplicative speed-up over a multithreaded implementation. Overall, our approach delivers a speedup of up to 43-fold on the discrete AMD Radeon HD 6470M GPU and 27-fold on the integrated AMD Radeon HD 6480G GPU, when compared to a vectorized and multithreaded version running on the AMD A4-3300M CPU.
[Instruction sets, FPGA, Graphics processing units, execution profiling, Mobile communication, AMD A4-3300M CPU, Optimization, discrete AMD Radeon HD 6470M GPU, integrated AMD Radeon HD 6480G GPU, optimisation, Finite impulse response filters, software radio, channel bank filters, SDR, multithreaded implementation, field-programmable gate arrays, wideband channelization, graphics processing units, polyphase filter banks, wideband channelization algorithm, mobile GPU, optimization techniques, software-defined radio, multicore processors, Acceleration, low-power mobile graphics processors, Wideband, multiplicative speed-up]
Bi-direction Adjust Heuristic for Workflow Scheduling in Clouds
2013 International Conference on Parallel and Distributed Systems
None
2013
This paper considers the workflow scheduling problem in Clouds with the hourly charging model and data transfer times. It deals with the allocation of tasks to suitable VM instances while maintaining the precedence constraints on one hand and meeting the workflow deadline on the other. A bi-direction adjust heuristic (BDA) is proposed for the considered problem. Matching of tasks and the VM types is modeled as Mixed Integer Linear programming (MILP) problem and solved using CPLEX at the first stage of BDA. In the second stage, forward and backward scheduling procedures are applied to allocate tasks to VM instances according to the result of the first stage. In the backward scheduling procedure, a priority rule considering the finish time, wasted time fractions and added hours is developed to make appropriate matches of tasks and free time slots. Extensive experimental results show that the proposed BDA heuristic outperforms the existing state-of-the-art heuristic ICPCP in all cases. Further, compared with ICPCP, about 80% percentage of VM renting cost is saved for instances with 900 tasks at most.
[Schedules, workflow scheduling, backward scheduling procedure, TV, Job shop scheduling, mixed integer linear programming, integer programming, CPLEX, linear programming, bi-direction adjust, task allocation, virtual machine scheduling, Processor scheduling, virtual machine, VM instances, Bidirectional control, virtual machines, MILP problem, Data transfer, cloud computing, bidirection adjust heuristic, BDA, time cost tradeoff]
Particle Swarm Optimization for Energy-Aware Virtual Machine Placement Optimization in Virtualized Data Centers
2013 International Conference on Parallel and Distributed Systems
None
2013
A critical research issue is to lower the energy consumption of a virtualized data center by means of virtual machine placement optimization while satisfying the resource requirements of the cloud services. In this paper, we focus on different existing schemes and on the energy-aware virtual machine placement optimization problem of a heterogeneous virtualized data center. We attempt to explore a better alternative approach to minimizing the energy consumption, and we observe that particle swarm optimization (PSO) has considerable potential. However, the PSO must be improved to solve an optimization problem. The improvement includes redefining the parameters and operators of the PSO, adopting an energy-aware local fitness first strategy and designing a novel coding scheme. Using the improved PSO, an optimal virtual machine replacement scheme with the lowest energy consumption can be found. Experimental results indicate that our approach significantly outperforms other approaches, and can lessen 13%-23% energy consumption in the context of this paper.
[Energy consumption, virtualized data centers, cloud services, particle swarm optimisation, Virtual machining, Vectors, Encoding, particle swarm optimization, Servers, computer centres, PSO, Optimization, virtual machine replacement, virtual machines, Data models, cloud computing, energy-aware virtual machine placement optimization, energy consumption, virtualized data center]
An Efficient Power-Aware Resource Scheduling Strategy in Virtualized Datacenters
2013 International Conference on Parallel and Distributed Systems
None
2013
In the era of cloud computing, data centers are well-known to be bounded by the power wall issue. This issue lowers the profit of service providers and obstructs the expansions of data center's scale. As virtual machine's behavior was not explored sufficiently in classic data center's power-saving strategies, in this paper we address the power consumption issue in the setting of a virtualized data center. We propose an efficient power-aware resource scheduling strategy that reduces data center's power consumption effectively based on VM live migration which is a key technical feature of cloud computing. Our scheduling algorithm leverages the Xen platform and consolidates VM workloads periodically to reduce the number of running servers. To satisfy each VM's service level agreements, our strategy keeps adjusting VM placements between scheduling rounds. We developed a power-aware data center simulator to test our algorithm. The simulator runs in time domain and includes server's segmented linear power model. We validated our simulator using measured server power trace. Our simulation shows that compared with event-driven schedulers, our strategy improves data center power budget by 35% for random workloads resembling web-requests, and improve data center power budget by 22.7% for workloads exhibiting stable resource requirements like ScaLAPACK.
[Cloud computing, event-driven schedulers, power wall issue, datacenter power consumption, datacenter simulator, virtualisation, Servers, contracts, Time-domain analysis, VM service level agreements, power consumption, VM live migration, power-aware data center simulator, power aware computing, Scheduling algorithms, resource allocation, power-aware resource scheduling strategy, power consumption issue, ScaLAPACK, Xen platform, scheduling, VM placements, cloud computing, Power demand, Computational modeling, server power model, VM workloads, virtualized datacenters, computer centres, virtual machine, resource provisioning, server power trace, virtual machines, server segmented linear power model]
Dynamic Virtual Resource Renting Method for Maximizing the Profits of a Cloud Service Provider in a Dynamic Pricing Model
2013 International Conference on Parallel and Distributed Systems
None
2013
With an increasing number of cloud service providers (CSP) delivering services to customers from the cloud, maximizing the profits of CSPs becomes a critical problem. Existing methods are difficult to solve the problem because they do not make full use of temporal price differences. This paper introduces a dynamic virtual resource renting method that attempts to dynamically adjust the virtual resource rental strategy according to price distribution and task urgency. We first pretreat the historical price series and adopt the outlier detection technique to filter the extreme price. Then, considering task urgency and price distribution, we design a weak equilibrium operator to calculate the acceptable price for each type of virtual resource. All types of virtual resources that are at an acceptable price are inserted into a set. Finally, we design a novel rental decision-making algorithm to select the most profitable resource from the set. We provide an extensive evaluation of our method using Amazon EC2 spot price dataset and normally distributed price dataset. The results demonstrate the effectiveness of our method.
[Algorithm design and analysis, Cloud computing, virtual reality, profit maximization, virtual resource renting, temporal price difference, profitable resource, dynamic virtual resource renting method, Optimization, outlier detection technique, historical price series, Pricing, Mathematical model, cloud computing, Amazon EC2 spot price dataset, CSP delivering services, normally distributed price dataset, virtual resource rental strategy, non-uniform mutation operator, Dynamic scheduling, Virtual machining, rental, task urgency, price distribution, dynamic pricing model, decision making, rental decision-making algorithm, cloud service provider, pricing]
A Data-Aware Partitioning and Optimization Method for Large-Scale Workflows in Hybrid Computing Environments
2013 International Conference on Parallel and Distributed Systems
None
2013
While hybrid computing environments provide good potential for achieving high performance and low economic cost, it also introduces a broad set of unpredictable overheads especially for running data-intensive applications. This paper describes a novel approach which refines workflow structures and optimizes intermediate data transfers for large-scale scientific workflows containing thousands (or even millions) of tasks. The proposed method includes pre- and post-partitioning of workflows and data-flow optimization. Firstly, it partitions a workflow by identifying the critical path of the task graph. Secondly, it controls the granularity of partitions to reduce the complexity of task graph in order to process large-scale workflows. Thirdly, it optimizes the data-flow based on the scheduling to minimize its communication overheads. Our proposed approach is able to handle complex data flows and significantly reduce data transfer by replacing individual tasks according to data dependencies. We conducted experiments using real applications such as Montage and Broadband, and the results demonstrated the effectiveness of our methods in achieving low execution time with low communication overhead in a hybrid computing environments.
[Algorithm design and analysis, task graph, large-scale scientific workflow, graph theory, grid computing, data-aware partitioning, Partitioning algorithms, Complexity theory, optimization method, hybrid computing environment, Optimization, Physics, optimisation, data-flow optimization, Parallel processing, Data transfer]
PipeFlow Engine: Pipeline Scheduling with Distributed Workflow Made Simple
2013 International Conference on Parallel and Distributed Systems
None
2013
Distributed computing system is considered as a fundamental architecture to extend resources such as computation speed, storage capacity, and network bandwidth, which are limited for a single processor. Emerging big data processing techniques like Hadoop take advantages of distributed servers to accomplish scalable parallel computations. Large-scale processing jobs can run on different servers or even different clusters interdependently and be combined together as a workflow to provide meaningful outputs. In this paper, we analyze the common demands of big-data processing and distributed big-data workflow processing. According to that, we design Pipe Flow Engine that has the matching features to meet each of these demands. It orchestrates all involved jobs and schedules them in a batched pipeline mode. We also present two online ranking algorithms that make use of the Pipe Flow, sharing the experience and best practice of using Pipe Flow.
[Measurement, distributed servers, Data handling, Pipelines, parallel computations, PipeFlow, online ranking algorithms, Information management, Servers, parallel processing, Engines, processor scheduling, distributed computing system, pipeline scheduling, workflow, Hadoop, Big Data, big data processing techniques, fundamental architecture, Data storage systems, distributed big-data workflow processing, pipeline, performance, distributed workflow, pipeflow engine, pipeline processing, large-scale processing jobs]
UserScope: A Fine-Grained Framework for Collecting Energy-Related Smartphone User Contexts
2013 International Conference on Parallel and Distributed Systems
None
2013
To prolong the battery lifetime of modern mobile devices, the energy management policy should be developed in a personalized way, adequately reflecting user context or the energy behavior of the user. The first step toward this personalization is to collect the relevant information, accurately and efficiently, from the device. This paper presents a fine-grained and low-overhead framework, called UserScope, which is designed to collect energy-related user contexts in Android smartphones. We classified energy-related smart phone usage and designed an appropriate set of monitoring parameters to collect from the system. The UserScope core is then implemented as a kernel module to collect all the necessary information in an event-driven manner. This kernel-level implementation ensures monitoring accuracy and low system overhead. UserScope also provides a data-sharing mechanism with which other software components in the system can easily interface. Our experiments show that User Scope accurately extracts energy related system information with 0.8% CPU overhead. The practicality of UserScope is also validated with real deployment and subsequent analysis of the collected data.
[Energy consumption, energy-related smart phone usage, Batteries, low-overhead framework, data-sharing mechanism, Android (operating system), power aware computing, system monitoring techniques, UserScope, smartphones, software components, Kernel, Monitoring, operating system kernels, user energy behavior, Android smartphones, smart phones, energy behavior, fine-grained framework, energy-related smartphone user context collection, energy management policy, monitoring parameters, CPU overhead, battery lifetime, mobile devices, OS kernel, Androids, Energy management, kernel module]
Peer-to-Peer Group k-Nearest Neighbours in Mobile Ad-Hoc Networks
2013 International Conference on Parallel and Distributed Systems
None
2013
The increasing use of location-based services has raised many issues of decision support and resource allocation. A crucial problem is how to solve queries of Group k-Nearest Neighbour (GkNN). A typical example of a GkNN query is finding one or many nearest meeting places for a group of people. Existing methods mostly rely on a centralised base station. However, mobile P2P systems offer many benefits, including self-organization, fault-tolerance and load-balancing. In this study, we propose and evaluate a novel P2P algorithm focusing on GkNN queries, in which mobile query objects and static objects of interest are of two different categories. The algorithm is evaluated in the MiXiM simulation framework with both real and synthetic datasets. The results show the practical feasibility of the P2P approach for solving GkNN queries for mobile networks.
[static objects, peer-to-peer computing, location based services, Location-Based Services, Artificial neural networks, Mobile communication, Mobile handsets, decision support, mobile P2P systems, resource allocation, Query processing, Spatial Database, Mobile P2P Networks, Collaboration, mobile ad hoc networks, peer to peer group k-nearest neighbours, mobile query objects, Group Nearest Neighbour, Peer-to-peer computing, Mobile computing]
Continuous Possible K-Nearest Skyline Query in Euclidean Spaces
2013 International Conference on Parallel and Distributed Systems
None
2013
Continuous K-nearest skyline query (CKNSQ) is an important type of the spatio-temporal queries. Given a query time interval [ts, te] and a moving query object q, a CKNSQ is to retrieve the K-nearest skyline points of q at each time instant within [ts, te]. Different from the previous works, our work devotes to overcoming the past assumption that each object is static with certain dimensional values and located in road networks. In this paper, we focus on processing the CKNSQ over moving objects with uncertain dimensional values in Euclidean space and the velocity of each object (including the query object) varies within a known range. Such a query is called the continuous possible K-nearest skyline query (CPKNSQ). We first discuss the difficulties raised by the uncertainty of object and then propose the CPKNSQ algorithm operated with a data partitioning index, called the uncertain TPR-tree (UTPR-tree), to efficiently answer the CPKNSQ.
[Algorithm design and analysis, data partitioning index, pattern classification, Uncertainty, moving query object, Conferences, uncertain dimensional values, UTPR-tree, Educational institutions, Vectors, Electronic mail, Indexes, query time interval, query processing, continuous possible K-nearest skyline query, CPKNSQ, Euclidean space, Continuous K-nearest skyline query, uncertain TPR-tree, spatio-temporal queries]
ConsumSense: A Framework for Physical Consuming Behavior Prediction on Smartphones
2013 International Conference on Parallel and Distributed Systems
None
2013
Automatic track and prediction of consumer behaviors involve huge commercial interests and have been studied extensively in the past. We have seen very successful application of such techniques on online consuming behaviors. However, it is still very challenging to predict consumer behaviors in physical stores, because many operations in the middle are not digitized. As smartphones are becoming indispensible in our daily life, we propose to build a framework, called ConsumSense, into the smartphones that observes in close-up the physical consuming behavior of the user and predicts his/her future purchases. Our framework addresses two difficult issues: (1) how to use the limited number of sensors on a smart phone to observe and predict the consuming behavior of its user? (2) how to verify and correlate the purchases? To demonstrate the feasibility of the proposed framework, we have developed the framework on Android and evaluated it by asking 14 participants to conduct a 3-week experiment by using Easy card during their daily life. The results show that time and location are the most important contexts for predicting consuming activities and our framework can achieve a 76% prediction accuracy.
[Context, smartphone system, physical consuming behavior prediction, online consuming behavior, consuming activity, Mobile communication, consumer behaviour, smart phones, Android, Android (operating system), mobile computing, Easy card, Semantics, ConsumSense, smartphones, context awareness, Timing, Sensors, Consuming behavior, Smart phones, learning and prediction, Context modeling]
NR-MPI: A Non-stop and Fault Resilient MPI
2013 International Conference on Parallel and Distributed Systems
None
2013
Fault resilience has became a major issue for HPC systems, in particular in the perspective of future E-scale systems, which will consist of millions of CPU cores and other components. Fault tolerant MPI was proposed to offer support of software level fault tolerance approaches. However, the widely used MPI implementations, such as MPICH and Mvapich2, provide limited support for fault tolerance. This paper proposes NR-MPI, a Non-stop and Fault Resilient MPI. NR-MPI implements the semantics of FT-MPI based on MPICH. Specifically, this paper focuses on failure detection in MPI library, online failure recovery of communicators for multiple failures, friendly programming interface extending for NR-MPI. Furthermore, to support failure recovery of applications, NR-MPI implements data backup and restore interfaces based on double in-memory checkpoint/restart. We conduct experiments with NPB benchmarks on TH-1A supercomputer. Experimental results show that NR-MPI based fault tolerant programs can recover from failures online without restarting, and the overhead is small even for applications with tens of thousands of cores.
[Context, Application-level Checkpoint/Restart, message passing, application program interfaces, fault tolerance, message passing interface, Programming, software level fault tolerance, fault tolerant MPI, parallel processing, NR-MPI, E-scale systems, Fault tolerance, fault resilient MPI, Fault tolerant systems, Semantics, nonstop MPI, HPC systems, Libraries, Resource management]
MPI-Interoperable Generalized Active Messages
2013 International Conference on Parallel and Distributed Systems
None
2013
Data-intensive applications have become increasingly important in recent years, yet traditional data movement approaches for scientific computation are not well suited for such applications. The Active Message (AM) model is an alternative communication paradigm that is better suited for such applications by allowing computation to be dynamically moved closer to data. Given the wide usage of MPI in scientific computing, enabling an MPI-interoperable AM paradigm would allow traditional applications to incrementally start utilizing AMs in portions of their applications, thus eliminating the programming effort of rewriting entire applications. In our previous work, we extended the MPI ACCUMULATE and MPI GET ACCUMULATE operations in the MPI standard to support AMs. However, the semantics of accumulate-style AMs are fundamentally restricted by the semantics of MPI ACCUMULATE and MPI GET ACCUMULATE, which were not designed to support the AM model. In this paper, we present a new generalized framework for MPI-interoperable AMs that can alleviate those restrictions, thus providing a richer semantics to accommodate a wide variety of application computational patterns. Together with a new API, we present a detailed description of the correctness semantics of this functionality and a reference implementation that demonstrates how various API choices affect the flexibility provided to the MPI implementation and consequently its performance.
[message passing, application program interfaces, open systems, Computational modeling, scientific computing, data movement approach, AM model, Concurrent computing, Semantics, Layout, MPI ACCUMULATE operations, MPI-interoperable generalized active messages, Data models, Libraries, API, Message systems, MPI GET ACCUMULATE operations]
Real Asynchronous MPI Communication in Hybrid Codes through OpenMP Communication Tasks
2013 International Conference on Parallel and Distributed Systems
None
2013
With the number of cores growing faster than memory per node, hybrid programming models (mixing message passing with shared memory paradigms) become a requirement for efficient use of HPC systems. For this scenario, achieving efficient communication is challenging. This is true even when using asynchronous communication, as most MPI implementations can only advance communication inside library calls. In this paper we propose to move communication into a new type of OpenMP task, which gets scheduled as part of the regular OpenMP work-pool. We show for compute intensive iterative stencil algorithms, that this provides real asynchronous communication. Without complicating the programming interface, our results show an excellent performance independent of the communication to computation ratio.
[application program interfaces, OpenMP communication tasks, MPI, asynchronous communication, Programming, parallel programming, software libraries, regular OpenMP workpool, Jacobian matrices, Asynchronous communication, Three-dimensional displays, library calls, Benchmark testing, shared memory systems, Message systems, message passing, hybrid codes, shared memory paradigms, OpenMP, Synchronization, hybrid programming models, compute intensive iterative stencil algorithms, hybrid, comm-task, HPC systems, real asynchronous MPI communication, communication-to-computation ratio]
OutFlank Routing: Increasing Throughput in Toroidal Interconnection Networks
2013 International Conference on Parallel and Distributed Systems
None
2013
We present a new, deadlock-free, routing scheme for toroidal interconnection networks, called OutFlank Routing (OFR). OFR is an adaptive strategy which exploits non-minimal links, both in the source and in the destination nodes. When minimal links are congested, OFR deroutes packets to carefully chosen intermediate destinations, in order to obtain travel paths which are only an additive constant longer than the shortest ones. Since routing performance is very sensitive to changes in the traffic model or in the router parameters, an accurate discrete-event simulator of the toroidal network has been developed to empirically validate OFR, by comparing it against other relevant routing strategies, over a range of typical real-world traffic patterns. On the 16&#x00D7;16&#x00D7;16 (4096 nodes) simulated network OFR exhibits improvements of the maximum sustained throughput between 14% and 114%, with respect to Adaptive Bubble Routing.
[routing performance, Adaptive systems, Adaptive routing, multiprocessor interconnection networks, packet derouting, real-world traffic patterns, Throughput, Multiprocessor interconnection, Network topology, deadlock-free routing scheme, routing strategies, Bandwidth, toroidal network discrete-event simulator, traffic model, discrete event simulation, minimal link congestion, network routing, Routing, 16&#x00D7;16&#x00D7;16 simulated network, OutFlank routing, High performance computing, adaptive bubble routing, OFR validation, System recovery, toroidal interconnection networks, Interconnection networks, Toroidal networks, nonminimal link exploitation]
On the Programmability and Performance of Heterogeneous Platforms
2013 International Conference on Parallel and Distributed Systems
None
2013
General-purpose computing on an ever-broadening array of parallel devices has led to an increasingly complex and multi-dimensional landscape with respect to programmability and performance optimization. The growing diversity of parallel architectures presents many challenges to the domain scientist, including device selection, programming model, and level of investment in optimization. All of these choices influence the balance between programmability and performance. In this paper, we characterize the performance achievable across a range of optimizations, along with their programmability, for multi- and many-core platforms - specifically, an Intel Sandy Bridge CPU, Intel Xeon Phi co-processor, and NVIDIA Kepler K20 GPU - in the context of an n-body, molecular-modeling application called GEM. Our systematic approach to optimization delivers implementations with speed-ups of 194.98&#x00D7;, 885.18&#x00D7;, and 1020.88&#x00D7; on the CPU, Xeon Phi, and GPU, respectively, over the naive serial version. Beyond the speed-ups, we characterize the incremental optimization of the code from naive serial to fully hand-tuned on each platform through four distinct phases of increasing complexity to expose the strengths and weaknesses of the programming models offered by each platform.
[Performance evaluation, multicore platform, AVX, parallel architectures, parallel devices, Graphics processing units, Programming, optimization investment level, coprocessors, programmability, GPU, Optimization, Intel Xeon Phi coprocessor, n-body molecular-modeling application, programming models, Intel MIC, optimisation, optimization, many-core platforms, Computer architecture, Xeon Phi, device selection, Intel Sandy Bridge CPU, NVIDIA Kepler K20 GPU, OpenACC, Mathematical model, multiprocessing systems, performance evaluation, naive serial, Vectors, incremental optimization, GEM, general purpose computers, CUDA, NVIDIA Kepler K20, heterogeneous platform performance, performance, general-purpose computing, performance optimization]
OpenCL-Based Remote Offloading Framework for Trusted Mobile Cloud Computing
2013 International Conference on Parallel and Distributed Systems
None
2013
OpenCL has emerged as the open standard for parallel programming for heterogeneous platforms enabling a uniform framework to discover, program, and distribute parallel workloads to the diverse set of compute units in the hardware. For that reason, there have been efforts exploring the advantages of parallelism from the OpenCL framework by offloading GPGPU workloads within an HPC cluster environment. In this paper, we present an OpenCL-based remote offloading framework designed for mobile platforms by shifting the motivation and advantages of using the OpenCL framework for the HPC cluster environment into mobile cloud computing where OpenCL workloads can be exported from a mobile node to the cloud. Furthermore, our offloading framework handles service discovery, access control, and data privacy by building the framework on top of a social peer-to-peer virtual private network, Social VPN. We developed a prototype implementation and deployed it into local- and wide-area environments to evaluate the performance improvement and energy implications of the proposed offloading framework. Our results show that, depending on the complexity of the workload and the amount of data transfer, the proposed architecture can achieve more energy efficient performance by offloading than executing locally.
[trusted mobile cloud computing, open systems, energy efficient performance, parallelism, Mobile communication, Mobile handsets, parallel programming, mobile platforms, parallel workload distribution, mobile computing, power aware computing, Bandwidth, authorisation, Hardware, GPGPU workload offloading, OpenCL workloads, access control, IP networks, cloud computing, energy consumption, OpenCL-based remote offloading framework, peer-to-peer computing, local-area environments, virtual private networks, graphics processing units, heterogeneity, heterogeneous platforms, wide-area environments, SocialVPN, Mobile device, social networking (online), data privacy, Peer-to-peer computing, Virtual private networks, social peer-to-peer virtual private network, OpenCL, HPC cluster environment, trusted computing]
Adaptive Accurate Indoor-Localization Using Passive RFID
2013 International Conference on Parallel and Distributed Systems
None
2013
In many pervasive applications like the intelligent bookshelves in libraries, it is essential to accurately locate the items to provide the location-based service, e.g., the average localization error should be smaller than 50 cm and the localization delay should be within several seconds. Conventional indoor-localization schemes cannot provide such accurate localization results. In this paper, we design an adaptive, accurate indoor-localization scheme using passive RFID systems. We propose two adaptive solutions, i.e., the adaptive power stepping and the adaptive calibration, which can adaptively adjust the critical parameters and leverage the feedbacks to improve the localization accuracy. The realistic experiment results indicate that, our adaptive localization scheme can achieve an accuracy of 31 cm within 2.6 seconds on average.
[Antenna measurements, radiofrequency identification, power stepping, accurate, adaptive calibration, pervasive applications, indoor localization, Calibration, ubiquitous computing, Passive RFID tags, location-based service, adaptive accurate indoor-localization scheme, adaptive, Power measurement, Accuracy, passive RFID system, adaptive power stepping, passive RFID tag, calibration, Antennas]
A Better Understanding of Event-Triggered Control from a CPS Perspective
2013 International Conference on Parallel and Distributed Systems
None
2013
In cybernetics filed, the benefit of event-triggered control reaches a consensus, but in practice the event-triggered control is hampered by the lack of a system theory. This paper provides a Cyber-Physical System (CPS) viewpoint of the relation between the controller and the physical plant to explain the event detection logic of the event-triggered control. The relation between the controller and the physical plant is considered as communication, and based on this communication process the controller and the physical plant exchange the information entropy and thermodynamic entropy. The physical plant generates the thermodynamic entropy and the controller provides the negative thermodynamic entropy based on the predictability of the mathematical model for the physical plant, where there is an entropy balance process between the thermodynamic entropy and information entropy. The adjustment of the controller cycle period is the process to adjust the entropy balance process, which implies the event detection logic.
[Uncertainty, cybernetics, negative thermodynamic entropy, Predictive models, Control systems, Entropy, Information entropy, entropy, Mathematical model, information entropy, entropy balance process, Cyber-Physical System, Process control, fault trees, controller cycle period, predictability, event detection logic, system theory, event-triggered control, CPS perspective, Information Entropy, cyber physical system, mathematical model, CPS viewpoint, Event Driven Control]
Cost Minimization for Scheduling Parallel, Single-Threaded, Heterogeneous, Speed-Scalable Processors
2013 International Conference on Parallel and Distributed Systems
None
2013
We introduce an online scheduling algorithm to optimally assign a set of arriving heterogeneous tasks to heterogeneous speed-scalable processors. The goal of our algorithm is to minimize the total cost of response time and energy consumption (TCRTEC) of the tasks. We have three contributions that constitute the algorithm. First, we propose a novel task dispatching strategy for assigning the tasks to the processors. Second, we propose a novel preemptive service discipline called Smallest remaining Computation Volume Per unit Price of response Time (SCVPPT) to schedule the tasks on the assigned processor. Third, we propose a dynamic speed-scaling function that explicitly determines the optimum processing rate of each task. In our work, the processors are heterogeneous in that they may differ in their hardware specifications with respect to maximum processing rate and power functions. Tasks are heterogeneous in terms of computation volume and processing requirements. We also consider that the unit price of response time for each task is heterogeneous. Each task's unit price of response time is allowed to differ because the user may be willing to pay higher/lower unit prices for certain tasks, thereby increasing/decreasing their optimum processing rates. In our SCVPPT discipline, a task's scheduling priority is influenced by its remaining computation volume as well as its unit price of response time. Our simulation results show that SCVPPT outperforms the two known service disciplines, Shortest Remaining Processing Time (SRPT) and the First Come First Serve (FCFS), in terms of minimizing the TCRTEC performance metric. The results also show that the algorithm's dispatcher outperforms the well known Round Robin dispatcher when the processors are heterogeneous. We focus on multi-buffer, single-threading where a set of tasks is allocated to a given processor, but only one task is processed at a time until completion unless preemption is dictated by the service discipline.
[Energy consumption, Optimal scheduling, task scheduling priority, power functions, formal specification, parallel processing, processor scheduling, smallest remaining computation volume per unit price of response time, TCRTEC minimisation, multi-processor scheduling, dynamic speed-scaling function, Program processors, power aware computing, single-threading, task dispatching strategy, hardware specifications, algorithm dispatcher, Cost function, energy and response time cost, cost minimization, parallel computing, processing requirements, heterogeneous tasks, heterogeneous speed-scalable processors, cost reduction, speed scaling, TCRTEC performance metric minimization, performance evaluation, mobile, heterogeneous processor scheduling, maximum processing rate, single-threaded, single-threaded processor scheduling, task allocation, heterogeneous processors, parallel processor scheduling, Processor scheduling, SCVPPT, speed-scalable processor scheduling, online scheduling algorithm, total cost of response time and energy consumption minimisation, Time factors, task assigning, preemptive service discipline]
Data Transfer Matters for GPU Computing
2013 International Conference on Parallel and Distributed Systems
None
2013
Graphics processing units (GPUs) embrace many-core compute devices where massively parallel compute threads are offloaded from CPUs. This heterogeneous nature of GPU computing raises non-trivial data transfer problems especially against latency-critical real-time systems. However even the basic characteristics of data transfers associated with GPU computing are not well studied in the literature. In this paper, we investigate and characterize currently-achievable data transfer methods of cutting-edge GPU technology. We implement these methods using open-source software to compare their performance and latency for real-world systems. Our experimental results show that the hardware-assisted direct memory access (DMA) and the I/O read-and-write access methods are usually the most effective, while on-chip micro controllers inside the GPU are useful in terms of reducing the data transfer latency for concurrent multiple data streams. We also disclose that CPU priorities can protect the performance of GPU data transfers.
[Performance evaluation, public domain software, real-world systems, Graphics processing units, input-output programs, parallel processing, Engines, many-core compute devices, GPGPU, data transfer latency, data transfer methods, hardware-assisted direct memory access, DMA, Real-time systems, Hardware, on-chip microcontrollers, I/O read-and-write access methods, microcontrollers, multiprocessing systems, Microcontrollers, OS, graphics processing units, Latency, open-source software, Data Transfer, real-time systems, parallel compute threads, Data transfer, file organisation, Performance, GPU computing]
Online Performance Projection for Clusters with Heterogeneous GPUs
2013 International Conference on Parallel and Distributed Systems
None
2013
We present a fully automated approach to project the relative performance of an OpenCL program over different GPUs. Performance projections can be made within a small amount of time, and the projection overhead stays relatively constant with the input data size. As a result, the technique can help runtime tools make dynamic decisions about which GPU would run faster for a given kernel. Usage cases of this technique include scheduling or migrating GPU workloads over a heterogeneous cluster with different types of GPUs.
[Performance evaluation, Computational modeling, Graphics processing units, online performance projection, Performance modeling, Throughput, OpenCL program, Performance projection, heterogeneous GPU, graphics processing units, GPU, parallel programming, Runtime, Hardware, Kernel]
Leveraging Hybrid Hardware in New Ways - The GPU Paging Cache
2013 International Conference on Parallel and Distributed Systems
None
2013
Modern server and desktop systems combine multiple computational cores and accelerator devices into a hybrid architecture. GPUs as one class of such devices provide dedicated processing power and memory capacities for data parallel computation of 2D and 3D graphics. Although these cards have demonstrated their applicability in a variety of areas, they are almost exclusively used by special purpose software. If such software is not running, the accelerator resources of the hybrid system remain unused. In this paper, we present an operating system extension that allows leveraging the GPU accelerator memory for operating system purposes. Our approach utilizes graphics card memory as cache for virtual memory pages, which can improve the overall system responsiveness, especially under heavy load. Our prototypical implementation for Windows proves the potential of such an approach, but identifies also significant preconditions for a widespread adoption in desktop systems.
[hybrid architecture, Graphics processing units, Random access memory, cache storage, data parallel computation, GPU accelerator memory, overall system responsiveness, special purpose software, graphics card memory, GPU Compute Devices, virtual memory pages, Hardware, Operating System, Kernel, operating system extension, processing power, 2D graphics, GPU paging cache, Paging, accelerator devices, graphics processing units, memory architecture, computational cores, hybrid hardware, desktop systems, Memory management, memory capacity, operating systems (computers), 3D graphics, operating system purpose, Cache]
Towards Improving MapReduce Task Scheduling Using Online Simulation Based Predictions
2013 International Conference on Parallel and Distributed Systems
None
2013
MapReduce is the model of choice for processing emerging big-data applications, and is facing an ever increasing demand for higher efficiency. In this context, we propose a novel task scheduling scheme that uses current task and system state information to drive online simulations concurrently within Hadoop, and predict with high accuracy future events, e.g., when a job would complete, or when task-specific data-local nodes would be available. These predictions can then be used to make more efficient resource scheduling decisions. Our framework consists of two components: (i) Task Predictor that predicts task-level execution times based on historical data of the same type of tasks, and (ii) Job Simulator that instantiates the real task scheduler in a simulated environment, and predicts expected scheduling decisions for all the tasks comprising a MapReduce job. Evaluation shows that our framework can achieve high prediction accuracy - 95% of the predicted task execution times are within 10% of the actual times - with negligible overhead (1.29%). Finally, we also present two realistic use cases, job data prefetching and a multi-strategy dynamic scheduler, which can benefit from integration of our prediction framework in Hadoop.
[system state information, MapReduce task scheduling, task-level execution times, online simulation based prediction, real task scheduler, Predictive models, job simulator, digital simulation, job data prefetching, Engines, scheduling decisions, storage management, Accuracy, Heart beat, scheduling, resource scheduling decision, simulated environment, Job shop scheduling, task scheduling scheme, Linear regression, Hadoop, Big Data, high prediction accuracy, historical data, multistrategy dynamic scheduler, big-data application, Data models, task predictor, task-specific data-local nodes]
Towards Multi-way Join Evaluating with Indexing Partition Support in Map-Reduce
2013 International Conference on Parallel and Distributed Systems
None
2013
In the era of "big data\
[one-pass multiway join algorithm, dominated table partitioning, data locality improvement, OLAP workloads, RDBMS context, clustered partitions, data mining, DBMS indexing techniques, parallel processing, Optimization, MapReduce, database indexing, Map-Reduce, Relational Partition, high-performance data warehouse analyses, Context, multiway join evaluation, Cascade reference, Big Data, optimization model, join index, relational databases, decision support systems, DSS load, initial tuple dispersion avoidance, indexing partition support, Layout, Organizations, TPCH benchmark, data organization model, Data models, online analysis processing workloads, DBMS, DBMS prepartitioned techniques, data warehouses, cascade reference constraints, Indexing]
OPTAS: Optimal Data Placement in MapReduce
2013 International Conference on Parallel and Distributed Systems
None
2013
The data placement strategy greatly affects the efficiency of MapReduce. The current strategy only takes the map phase into account to optimize the map time. But the ignored shuffle phase may increase the total running time significantly in many jobs. We propose a new data placement strategy, named OPTAS, which optimizes both the map and shuffle phases to reduce their total time. However, the huge search space makes it difficult to find out an optimal data placement instance (DPI) rapidly. To address this problem, an algorithm is proposed which can prune most of the search space and find out an optimal result quickly. The search space firstly is segmented in ascending order according to the potential map time. Within each segment, we propose an efficient method to construct a local optimal DPI with the minimal total time of both the map and shuffle phases. To find the global optimal DPI, we scan the local optimal DPIs in order. We have proven that the global optimal DPI can be found as the first local optimal DPI whose total time stops decreasing, thus further pruning the search space. In practice, we find that at most fourteen local optimal DPIs are scanned in tens of thousands of segments with the pruning strategy. Extensive experiments with real trace data verify not only the theoretic analysis of our pruning strategy and construction method but also the optimality of OPTAS. The best improvements obtained in our experiments can be over 40% compared with the existing strategy used by MapReduce.
[Algorithm design and analysis, Conferences, Optimized production technology, Educational institutions, MapReduce, Analytical models, optimisation, data placement, Distributed databases, optimal data placement instance, pruning strategy, global optimal DPI, Data models, data handling, OPTAS, cloud computing]
Strong Dynamic Consensus in Byzantine Faulty Systems with Churn
2013 International Conference on Parallel and Distributed Systems
None
2013
Dynamic distributed systems allow processes to join and leave the system, so the number of processes participating in a computation varies over time. Examples of dynamic distributed systems include peer-to-peer networks, sensor networks, mobile ad-hoc networks, and many more. A fundamental problem in any distributed system is to find consensus among the processes on a common input value. For dynamic distributed systems, it is not entirely clear how the problem should be formulated, as processes can join and leave before consensus is reached. We formulate and solve a strong version of the consensus problem in dynamic distributed systems in the presence of Byzantine faulty processes. We show that one cannot improve upon our algorithm in terms of the bound on the number of processes. For stochastic dynamic distributed systems, we determine the probability that a set of processes can reach strong consensus.
[Algorithm design and analysis, Strong Dynamic Consensus, Heuristic algorithms, Byzantine faulty systems, Strong Consensus, Stochastic processes, distributed processing, churn, Analytical models, stochastic dynamic distributed systems, Message passing, Vegetation, Markov Process, Peer-to-peer computing, Dynamic Distributed Systems, stochastic processes, dynamic consensus]
SimNUMA: Simulating NUMA-Architecture Multiprocessor Systems Efficiently
2013 International Conference on Parallel and Distributed Systems
None
2013
Non-uniform memory access (NUMA) architecture is widely used in high-end servers and computing systems due to its scalability. In recent years, the number of processor cores in NUMA systems increases rapidly with the development of multi-core processors. Along with the growing of system scales, simulation of NUMA systems becomes a challenge to traditional general-purpose simulators by reason of their low simulation performance. This paper presents SimNUMA, an execution-driven full-system simulator dedicated for NUMA systems. In the design of SimNUMA, to improve simulation performance significantly, the same type of processor with the target machine is used in the host system, and a new method to capture remote-memory accesses efficiently is proposed, in addition, parallel simulation is used to achieve scalability and improve performance. The modeling and simulation of interconnection networks are also supported. The simulator is tested in accuracy, scalability and performance, results show that the simulation slowdown is rather satisfying. Finally, the paper gives simulation experiments for different scales of target NUMA systems.
[Scalability, parallel architectures, multiprocessor interconnection networks, simulation, interconnection networks, nonuniform memory access, Analytical models, Multiprocessor interconnection, Operating systems, NUMA architecture, parallel simulation, multiprocessing systems, general-purpose simulators, simulator, Computational modeling, SimNUMA, Integrated circuit interconnections, multi-core processor, general purpose computers, computing systems, NUMA-architecture multiprocessor systems, multicore processors, execution-driven, remote-memory access, high-end servers]
Application Aware DRAM Bank Partitioning in CMP
2013 International Conference on Parallel and Distributed Systems
None
2013
Main memory is a shared resource among cores in a chip and the speed gap between cores and main memory limits the total system performance. Thus, main memory should be effectively accessed by each core. Exploiting both parallelism and locality of main memory is the key to realize the efficient memory access. The parallelism between memory banks can hide the latency by pipelining memory accesses. The locality of memory accesses improves hit ratio of the row buffer in DRAM chips. The state-of-the-art method called bpart is proposed to improve memory access efficiency. In bpart one bank is monopolized by one thread and this monopolization improves row buffer locality because of alleviating inter-thread interference. However, bpart is not effective for the thread which has poor locality. Moreover, the bank level parallelism is not exploited. We propose the new bank partitioning method which exploits parallelism in addition to locality. Our method applies the two types of bank usage. One usage is that low locality threads share banks to improve parallelism, and the other usage is that each high locality thread monopolizes each bank to improve row buffer locality. We evaluate our proposed method by our in-house software simulator with SPEC CPU 2006 benchmark. On Average, system throughput is increased by 1.0% and minimum speedup (fairness metrics) is increased by 7.9% relative to bpart. This result shows that our porposed method has better performance and fairness than bpart.
[Instruction sets, Random access memory, bank level parallelism, bpart, Memory system, memory access, memory banks, Bank partitioning, high locality thread, in-house software simulator, Main memory, resource allocation, Parallel processing, main memory, DRAM chips, shared memory systems, Hardware, DRAM, bank partitioning method, Radiation detectors, CMP, SPEC CPU 2006 benchmark, Multicore, inter-thread interference, shared resource, pipeline processing, Arrays, Resource management, application aware DRAM bank partitioning]
A Distributed Approach to Constructing k-Hop Connected Dominating Set in Ad Hoc Networks
2013 International Conference on Parallel and Distributed Systems
None
2013
Since ad hoc networks do not have fixed or predefined infrastructures, nodes need to frequently flood control messages to discover and maintain routes, which causes performance problems in terms of unnecessary traffic and energy consumption, contention, and collision. A general solution is to construct a virtual backbone as the basis of routing and broadcasting, and the Connected Dominating Set (CDS) has been widely used. This paper gives a distributed approach to constructing k-Hop CDS with three unique characteristics: (1) the limitation on the range of k has been removed, (2) a token-based conflict avoidance mechanism has been introduced which can make the construction process faster and more effective, (3) a tree-type CDS can be constructed in bottom-up processing. Simulation experiments have been conducted to demonstrate the effectiveness of the proposed approach.
[k-hop CDS, Conferences, network routing, ad hoc network, k-hop connected dominating set, Ad hoc networks, Topology, energy collision, broadcasting, virtual backbone, distributed approach, Network topology, CDS, Wireless networks, tree-type CDS, telecommunication network routing, energy contention, distributed construction, System recovery, token-based conflict avoidance mechanism, ad hoc networks, bottom-up processing, Joining processes, energy consumption]
Development of Efficient Role-Based Sensor Network Applications with Excel Spreadsheets
2013 International Conference on Parallel and Distributed Systems
None
2013
Natural scientists use large scale sensor networks for gathering and analyzing environmental data. However, the implementation work requires expert programmers. The problem is complicated by limited battery lifetime, processing power and memory capacity of the nodes, because this requires a low-level programming language. Since scientists are used to analyzing data with spreadsheets, researchers have studied the possibility of applying spreadsheet-based programming to sensor networks. The approaches so far either require a central server to execute the spreadsheet, or they execute a spreadsheet run-time on each node. The first approach causes higher communication cost since all data has to be routed to the central server and the second one causes computational overhead, because evaluating a spreadsheet is slower than executing handcrafted NesC-code. Hence, we present a spreadsheet driven tool-chain that can create efficient NesC-code and allows for simulation in the spreadsheet itself. The nodes have to recompute the spreadsheet formulas upon new data. However, we can avoid a large fraction of this recomputation by applying several optimization strategies during code generation. In our example scenario, sensor nodes compute the variance across a series of sensor readings. We can show that the optimizations save 65% CPU cycles and the code size decreases by 12% when compared to non-optimized execution of the spreadsheet. Thus, our approach can deliver an easy way of developing sensor network programs while yielding very efficient code.
[wireless sensor networks, Conferences, optimization strategies, Shift registers, spreadsheet programs, Servers, programming languages, program compilers, Optimization, large scale sensor networks, optimisation, Designing Sensor Networks, Monitoring, excel spreadsheets, handcrafted NesC-code, data analysis, Redundancy, environmental data analysis, low-level programming language, Generators, environmental data gathering, code generation, role-based sensor network, Spreadsheet Programming, natural scientists, Computation Optimization]
Trust-Based Multi-objective Optimization for Node-to-Task Assignment in Coalition Networks
2013 International Conference on Parallel and Distributed Systems
None
2013
A temporary coalition is often formed to pursue a common goal based on the collaboration of multiple partners who may have their own objectives. The coalition network must attain multiple objectives, under resource constraints and time deadlines. We propose a task assignment algorithm for a scenario where tasks are dynamic, with different arrival times and deadlines. We propose a heuristic coalition formation technique that uses multiple dimensions of trust (i.e., integrity, competence, social connectedness, and reciprocity) to assess trust of each entity. The proposed scheme enables task leaders to make critical assignment decisions based on assessed trustworthiness of entities. We consider three different objectives, namely, maximizing resilience and resource utilization while minimizing delay to task completion. We devise a ranking-based heuristic with linear runtime complexity to select members based on risk derived from trust assessment of nodes. We validate the performance of our proposed scheme by comparing our scheme with a non-trust-based baseline scheme as well as a global optimal solution implemented with the Integer Linear Programming technique.
[trust, Protocols, integer programming, trust-based multiobjective optimization, ranking-based heuristic, linear programming, heuristic coalition formation technique, global optimal solution, Optimization, trustworthiness, resource allocation, task assignment algorithm, integer linear programming technique, task assignment, Mathematical model, resource utilization, Computational modeling, trust assessment, linear runtime complexity, node-to-task assignment, coalition networks, risk, Delays, multi-objective optimization, Resource management, trusted computing]
SAIL: A Strategy-Proof Auction Mechanism for Cooperative Communication
2013 International Conference on Parallel and Distributed Systems
None
2013
Cooperative communication is a new fashion to alleviate the low channel utilization and signal fading problems in today's wireless network. The success of cooperative communication heavily depends on the efficient assignment of relay resource. Auction theory has been applied successfully to allocate limited resources in wireless network for decades. However, most of the existing auction mechanisms restricted buyers to use simple bidding language, which greatly lowers the social welfare and relay assignment efficiency. In this paper, we model the relay assignment as a combinatorial auction with flexible bidding language and propose SAIL, which is a Strategy-proof and Approximately effIcient combinatoriaL auction for relay assignment in cooperative communication. We show analytically that SAIL is strategy-proof and achieves approximate efficient social welfare. Furthermore, we present evaluation results to show that SAIL achieves a good system performance in terms of social welfare, buyer satisfaction and relay utilization.
[combinatorial mathematics, relay resource, Conferences, Cooperative Communication, strategy-proof auction mechanism, auction theory, Relays, Cost accounting, Combinatorial Auction, relay assignment efficiency, resource allocation, social welfare, Wireless networks, approximately effIcient combinatorial auction, SAIL, buyer satisfaction, wireless network, relay networks (telecommunication), relay utilization, bidding language, Vectors, Relay Selection, cooperative communication, channel utilization, signal fading problems, Resource management]
Dual-Region Location Management for Mobile Ad Hoc Networks
2013 International Conference on Parallel and Distributed Systems
None
2013
We propose and analyze a novel location management scheme for mobile ad hoc networks (MANETs) called Dual-region Mobility Management (DrMoM). The basic design concept of DrMoM is to use local regions to complement existing location services in MANETs that assign home regions to mobile nodes and have mobile nodes in the home region of a mobile node serve as location servers for that node. DrMoM is based on the design notion of integrated mobility and service management for network cost minimization. Specifically, unlike existing location services that define the home region size statically at design time for all mobile users, DrMoM dynamically determines the optimal home region size and local region size per mobile user based on mobility and service characteristics of individual mobile nodes to minimize the overall network cost incurred by location management and data packet delivery. We develop a performance model to derive the optimal values of these two key design parameters under which the overall network cost incurred by DrMoM is minimized. Through a comparative performance study, we show that DrMoM outperforms a well-known scheme called SLURP based on static home regions as well as a region-based location management scheme called RUDLS which claims to outperform contemporary region-based location management schemes.
[dual-region mobility management, location management, data packet delivery, Ad hoc networks, Mobile nodes, mobility management (mobile radio), dual-region location management, Servers, DrMoM, SLURP, Wireless communication, MANET, mobile ad hoc networks, Mobile computing, mobile nodes, performance analysis]
Application Layer Multicast in P2P Distributed Interactive Applications
2013 International Conference on Parallel and Distributed Systems
None
2013
By sharing resources among peers in peer-to-peer network, application layer multicast (ALM) has been shown an efficient way to improve the scalability and reduce the latency of communication. To deploy ALM in peer-to-peer distributed interactive applications (DIAs), the property of many-to-many communication of DIA demands multiple multicast trees to be constructed in the overlay. Therefore, how to efficiently allocate resources among multiple trees to maximize the benefit is an important and challenging issue. In this paper, we study the problem of building ALM trees with minimum total end-to-end delay to receivers in peer-to-peer DIAs with resource constraints on network bandwidth. The end-to-end delay from a sender to receivers in a multicast tree consists of the link delay as well as the packet queuing delay at intermediate peers, while the latter is often ignored or not well studied in the previous work. In this paper, we explicitly establish the relationship between the queuing delay at peers and the topology of multicast trees in peer-to-peer DIAs. Based on the relationship, the above mentioned problem is defined and we prove that it is NP-complete. We present a centralized heuristic algorithm to obtain an approximate solution. Moreover, distributed algorithms are also investigated to refine the topology in practical systems with dynamic changes. Extensive experiments were conducted by simulations to evaluate the proposed algorithms and results are reported in the paper.
[Heuristic algorithms, link delay, NP-complete, Optimization, Application Layer Multicast, optimisation, end-to-end delay, packet queuing delay, resource allocation, Bandwidth, ALM trees, application layer multicast, queueing theory, peer-to-peer computing, resource constraints, Buildings, trees (mathematics), Receivers, Minimum Latency, network bandwidth, peer-to-peer distributed interactive application, centralized heuristic algorithm, P2P, distributed algorithm, distributed algorithms, multicast protocols, delays, Vegetation, P2P distributed interactive applications, multiple multicast trees, many-to-many communication, Peer-to-peer computing, Delays, peer-to-peer DIA, Distributed Interactive Applications, peer-to-peer network, multiple trees]
Policies for Efficient Data Replication in P2P Systems
2013 International Conference on Parallel and Distributed Systems
None
2013
This paper addresses the problem of maintaining replicated data in large scale P2P systems. Although this topic has been extensively studied in the literature, to maintain replicated data in this setting, in an efficient manner, still remains a significant challenge. This paper proposes novel policies to address this problem and evaluates their performance against different criteria, such as monitoring costs, data transfer costs, and load unbalance costs. We show that one of these new policies significantly outperforms previous work. Interestingly, this policy is based on a somehow counter-intuitive approach, that uses less reliable nodes to store the most accessed data items. The insights to derive this policy were obtained from an in depth analysis of existing solutions, that is also captured in the paper.
[Measurement, peer-to-peer computing, counter-intuitive approach, data transfer costs, data replication policies, Servers, P2P, Group-based DHTs, Data replication, monitoring costs, large scale P2P systems, Bandwidth, Load management, Data transfer, Peer-to-peer computing, data handling, Fault-tolerance, Reliability, load unbalance costs, Load balancing, Monitoring, replicated data]
Predicting Popularity and Adapting Replication of Internet Videos for High-Quality Delivery
2013 International Conference on Parallel and Distributed Systems
None
2013
Content availability has become increasingly important for the Internet video delivery chain. To deliver videos with an outstanding availability and meet the increasing user expectations, content delivery networks (CDNs) must enforce strict QoS metrics, like bitrate and latency, through SLA contracts. Adaptive content replication has been seen as a promising way to achieve this goal. However, it remains unclear how to avoid waste of resources when strict SLA contracts must be enforced. In this work, we introduce Hermes, an adaptive replication scheme based on accurate predictions about the popularity of Internet videos. Simulations using popularity growth curves from YouTube traces suggest that our approach meets user expectations efficiently. Compared to a non-collaborative caching, Hermes reduces storage usage for replication by two orders of magnitude, and under heavy load conditions, it increases the average bitrate provision by roughly 90%. Moreover, it prevents SLA violations through an application-level deadline-aware mechanism.
[content availability, high-quality delivery, YouTube traces, latency, storage usage reduction, SLA, Popularity Growth, contracts, Internet video delivery chain, CDN, Videos, adaptive content replication, application-level deadline-aware mechanism, Video Quality, Bit rate, Bandwidth, popularity prediction, Availability, popularity growth curves, SLA violation prevention, user expectations, Hermes, Prediction, quality of service, Peer-to-Peer, content delivery networks, SLA contracts, YouTube, heavy-load conditions, social networking (online), average bitrate provision, Replication, QoS metrics, Internet, Peer-to-peer computing, Hybrid CDN]
Nuclear Fusion Simulation Code Optimization on GPU Clusters
2013 International Conference on Parallel and Distributed Systems
None
2013
GT5D is a nuclear fusion simulation program which aims to analyze the turbulence phenomena in tokamak plasma. In this research, we optimize it for GPU clusters with multiple GPUs on a node. Based on the profile result of GT5D on a CPU node, we decide to offload the whole of the time development part of the program to GPUs except MPI communication. We achieved 3.37 times faster performance in maximum in function level evaluation, and 2.03 times faster performance in total than the case of CPU-only execution, both in the measurement on high density GPU cluster HA-PACS where each computation node consists of four NVIDIA M2090 GPUs and two Intel Xeon E5-2670 (Sandy Bridge) to provide 16 cores in total. These performance improvements on single GPU corresponds to four CPU cores, not compared with a single CPU core. It includes 53% performance gain with overlapping the communication between MPI processes with GPU calculation.
[workstation clusters, Conferences, parallel architectures, Graphics processing units, Nuclear Fusion, GPU, MPI processes, nuclear fusion simulation code optimization, high density GPU cluster HA-PACS, GPU clusters, multiple GPU, Plasmas, Kernel, Computational modeling, nuclear fusion simulation program, CPU-only execution, NVIDIA M2090 GPU, Fusion reactors, Educational institutions, Tokamak devices, turbulence, graphics processing units, physics computing, CUDA, tokamak plasma, turbulence phenomena, nuclear fusion, GT5D, MPI communication]
Subpopulation Diversity Based Accepting Immigrant in Distributed Evolutionary Algorithms
2013 International Conference on Parallel and Distributed Systems
None
2013
As a popular type of parallel evolutionary algorithms, distributed evolutionary algorithms (DEAs) are widely used in a variety of fields. To get better solutions of concrete problems, a scheme of subpopulation diversity based accepting immigrant in DEAs is proposed in this paper. In migration with this scheme, an immigrant will be put into its target subpopulation only if its current diversity is lower than a threshold value. Algorithm analysis shows that the extra cost of time for this scheme is acceptable in many DEAs. Experiments are conducted on instances of the Traveling Salesman Problem from the TSPLIB. Results show that the DEA based on the proposed scheme can get better solutions than the one without it.
[parallel algorithms, subpopulation diversity, parallel evolutionary algorithms, traveling salesman problem, Conferences, Geology, Evolutionary computation, Traveling salesman problems, Educational institutions, accepting immigrant, DEAs, Standards, Genetic algorithms, travelling salesman problems, evolutionary computation, distributed evolutionary algorithms, subpopulation diversity-based accepting immigrant, threshold value, distributed evolutionary algorithm, migration occasion, TSPLIB, target subpopulation]
Queue Reorganization for Subscription Congestion Avoidance in Publish/Subscribe Systems
2013 International Conference on Parallel and Distributed Systems
None
2013
This paper introduces the queue reorganization algorithm to alleviate congestion problem in pub/sub systems. Experimental studies show that our solution can greatly reduce the number of messages congested in the input queue, which in turn alleviates system pressure from congested queues.
[pub-sub systems, message passing, queueing theory, Conferences, Computational modeling, subscription congestion avoidance, Subscriptions, Routing, Indexes, Optimization, Jacobian matrices, queue reorganization algorithm, publish-subscribe systems, middleware]
Accelerating De Bruijn Graph-Based Genome Assembly for High-Throughput Short Read Data
2013 International Conference on Parallel and Distributed Systems
None
2013
Emerging next-generation sequencing technologies have opened up exciting new opportunities for genome sequencing by generating read data with a massive throughput. However, the generated reads are significantly shorter compared to the traditional Sanger shotgun sequencing method. This poses challenges for de novo assembly algorithms in terms of both accuracy and efficiency. And due to the continuing explosive growth of short read databases, there is a high demand to accelerate the often repeated long-runtime assembly task. In this paper, we present a scalable parallel algorithm to accelerate the de Bruijn graph-based genome assembly for high-throughput short read data.
[Genome Assembly, Conferences, de Bruijn graph-based genome assembly, Pipelines, graph theory, Genomics, parallel algorithm, MPI, database management systems, sequences, Short Read Data, Sequential analysis, biology computing, de Bruijn Graph, Sanger shotgun sequencing method, genomics, Bioinformatics, Assembly, Multi-threading, parallel algorithms, de novo assembly algorithms, program assemblers, short read databases, long-runtime assembly task, genome sequencing, next-generation sequencing technologies, Acceleration, high-throughput short read data]
Workload Estimation Algorithm in Parallel Traffic Simulation
2013 International Conference on Parallel and Distributed Systems
None
2013
Parallel traffic simulation is a critical component in large-scale traffic simulations and real-time traffic simulations. Dividing workloads evenly to multi-cores and multi-machines is a challenge in parallel traffic simulations. Current researches focus on map decomposition algorithms. However, without effective workload estimation algorithms, map decomposition algorithms tend to output imbalanced partitions. This paper proposes an elliptical-shaped workload estimation algorithm. The main idea of the algorithm is to assign the computational cost of a vehicle to links in an ellipse, whose centers (or foci points) are the vehicle's origin node and the destination node. The algorithm is evaluated on a test-bed using a mesoscopic traffic simulator on the Lower Westchester County network. Case studies show that the new algorithm reduces 36% of the estimation errors in current length of links based workload estimation algorithms.
[Algorithm design and analysis, parallel traffic simulation, multimachines, Parallel traffic simulation, Shape, estimation theory, digital simulation, destination node, map decomposition algorithm, parallel processing, real-time traffic simulation, Vehicles, elliptical-shaped workload estimation algorithm, road vehicles, Lower Westchester County network, Traffic control, workload estimation algorithm, Load balancing, estimation error, mesoscopic traffic simulator, Estimation error, computational cost, Workload estimation, vehicle origin node, multicores, Partitioning algorithms, traffic engineering computing, large-scale traffic simulation, Algorithm, output imbalanced partition, real-time systems]
A Heuristic Strategy for Performance Optimisation of Stream Programs
2013 International Conference on Parallel and Distributed Systems
None
2013
In this paper we present a design of a stream scheduler aiming at optimising throughput and latency of streaming programs with dynamic program structures. The scheduler uses heuristics based on the demand of data in communications streams. As we address dynamic structures of streaming programs, the particular challenge is that static scheduling based on formal constraints or probabilities is not applicable.
[Availability, Conferences, heuristic strategy, stream program performance optimisation, Programming, Throughput, Dynamic scheduling, stream scheduler design, parallel programming, communication streams, Parallel processing, scheduling, stream program throughput optimisation, Silicon, static scheduling, stream program latency optimisation, dynamic streaming program structures]
On the Portability of the OpenCL Dwarfs on Fixed and Reconfigurable Parallel Platforms
2013 International Conference on Parallel and Distributed Systems
None
2013
The proliferation of heterogeneous computing systems presents the parallel computing community with the challenge of porting legacy and emerging applications to multiple processors with diverse programming abstractions. OpenCL is a vendor-agnostic and industry-supported programming model that offers code portability on heterogeneous platforms, allowing applications to be developed once and deployed "anywhere." In this paper, we use the OpenCL implementation of the Open Dwarfs, a benchmark suite that captures patterns of computation and communication common to classes of important applications, as delineated by Berkeley's Dwarfs. We evaluate portability across multicore CPU, GPU, APU (CPUs+GPUs on a die), the Intel Xeon Phi co-processor, and the FPGA. To realize FPGA portability, we exploit SOpenCL (Silicon OpenCL), a CAD tool that automatically converts OpenCL kernels to customizable hardware accelerators. We show that a single, unmodified OpenCL code base, i.e., Open Dwarfs, can be effectively used to target multiple, architecturally diverse platforms.
[FPGA portability, heterogeneous computing systems, FPGA, Graphics processing units, APU, Programming, CPU, parallel processing, GPU, Silicon OpenCL, Intel Xeon Phi coprocessor, dwarfs, porting legacy, portability, OpenCL kernels, parallel computing community, Parallel processing, Xeon Phi, Hardware, Kernel, multicore CPU, OpenCL Dwarfs, industry-supported programming model, OpenCL code base, diverse programming abstractions, SOpenCL, reconfigurable parallel platforms, heterogeneous platforms, CAD tool, code portability, OpenCL, Hardware design languages, Field programmable gate arrays]
Pangaea: A Single Key Space, Inter-datacenter Key-Value Store
2013 International Conference on Parallel and Distributed Systems
None
2013
This paper presents Pangaea, an inter-data center key-value store that keeps reasonable expandability of storage capacity, data lookup latency and data transfer speed. Pangaea uses two techniques called multi-layered DHT (ML-DHT) and local-first data rebuilding (LDR). ML-DHT provides a global and consistent index of key-value pairs with efficient routings in inter-data center environments. LDR reduces inter-data center data transfer by using erasure coding techniques.
[Computers, Cloud computing, geo-distributed system, inter-datacenter key-value store, Conferences, Pangaea, local-first data rebuilding, ML-DHT, table lookup, data lookup latency, storage management, multilayered DHT, LDR, key-value store, interdata center data transfer reduction, single key space, data transfer speed, Generators, Encoding, datacenter, Indexes, computer centres, distributed hash table, Data transfer, storage capacity, erasure coding techniques]
Adaptive Peer Selection Strategy in P2P-VoD Systems Based on Dynamic Metaheuristic
2013 International Conference on Parallel and Distributed Systems
None
2013
During the past decade, Peer-to-Peer Video-on Demand (VoD) systems have proved their efficiency for large deployments. They raise new challenges such as peers resource allocation. Most literature on resource allocation tackle the problem with optimal static rules found at offline study of the system. In this paper, we use a dynamic metaheuristic, called Multiple Local-Search Algorithm for Dynamic Optimization (MLSDO) to optimize the problem at hand. The obtained results show that using a dynamic resource allocation reduces the rejection rate while enhancing the entropy of the system, in the face of a dynamically changing title demand.
[Measurement, Networks, Heuristic algorithms, Evolution strategy, Entropy, P2P-VoD system, Modeling, Optimization, adaptive peer selection, optimal static rule, Video-on-Demand, peer-to-peer video-on demand, video on demand, Resource Allocation, search problems, multiple local-search algorithm, Distributed application, peer-to-peer computing, dynamic resource allocation, Bayes, Dynamic scheduling, dynamic metaheuristic, Multiple Local Search, dynamic optimization, Peer-to-Peer, Peer-to-peer computing, Dynamic Optimization, Resource management, MLSDO]
A Rule Verification and Resolution Framework in Smart Building System
2013 International Conference on Parallel and Distributed Systems
None
2013
In smart building, services are stored as rules and achieved by rule analyzing and executing. However, the irrational contents of rules and conflicts between rules may bring confusion and maloperation in the rule system. In this paper, we propose a lightweight rule verification and resolution framework to solve the problem which provides content anomaly detection and rule conflict detection. We also provide a quick resolution strategy for rule conflicts based on conflict-scenario-analysis so as to guarantee the rule system performing appropriately.
[Charge coupled devices, Smart buildings, rule system performance analysis, Conferences, Knowledge based systems, knowledge-based, service storage, rule content anomaly detection, Educational institutions, conflict-scenario-analysis, anomaly detection, rule execution, rule conflict, home automation, rule verification framework, rule resolution framework, rule verification, smart building system, Usability, building management systems, wireless sensor-actuator networks, Erbium, rule conflict detection]
System-Level Scheduling of Mixed-Criticality Traffics in Avionics Networks
2013 International Conference on Parallel and Distributed Systems
None
2013
System-level mixed-criticality design towards low product cost and high resource efficiency. This paper studies the integration technology of mixed-criticality avionics traffics for Avionics Full-Duplex Switched Ethernet (AFDX) network, which can transmit both critical traffics and non-critical traffics. In the architecture, critical traffics use by Bandwidth Allocation Gap (BAG) based scheduler and non-critical traffics are scheduled by Round Robin manner. In order to estimate delay bound meeting requirements of applications, End-to-End delay for both critical traffics and non-critical traffics are analyzed exerting the approach of Network Calculus. Finally, a True Time based simulation of AFDX networks is conducted to show the effectiveness of proposed approach.
[delay estimation, Avionics Full-Duplex Switched Ethernet (AFDX), Conferences, Switches, Aerospace electronics, Length measurement, local area networks, noncritical traffics, end-to-end delay, mixed-criticality traffics, Network Calculus, End-To-End Delay, scheduling, system-level scheduling, Round robin, delay bound estimation, system-level mixed-criticality design, network calculus approach, Mixed-Criticality, Estimation, avionics, air traffic, BAG based scheduler, bandwidth allocation, avionics full-duplex switched Ethernet network, AFDX network, Delays, bandwidth allocation gap, true time based simulation]
The Future of Accelerator Programming: Abstraction, Performance or Can We Have Both?
2013 International Conference on Parallel and Distributed Systems
None
2013
In a perfect world, code would only be written once and would run on different devices with high efficiency. A programmer's time would primarily be spent on thinking about the algorithms and data structures, not on implementing them. To a degree, that used to be the case in the era of frequency scaling on a single core. However, due to power limitations, parallel programming has become necessary to obtain performance gains. But parallel architectures differ substantially from each other, often require specialized knowledge, and typically necessitate reimplementation and fine tuning of application code. These slow tasks frequently result in situations where most of the time is spent reimplementing old rather than writing new code. The goal of our research is to find new programming techniques that increase productivity, maintain high performance, and provide abstraction to free the programmer from these unnecessary and time-consuming tasks. However, such techniques usually come at the cost of substantial performance degradation. This paper investigates current approaches to portable accelerator programming, seeking to answer whether they make it possible to combine high efficiency with sufficient algorithm abstraction. It discusses OpenCL as a potential solution and presents three approaches of writing portable code: GPU-centric, CPU-centric and combined. By applying the three approaches to a real-world program, we show that it is at least sometimes possible to run exactly the same code on many different devices with minimal performance degradation using parameterization. The main contributions of this paper are an extensive review of the current state-of-the-art regarding the stated problem and our original approach of addressing this problem with a generalized excessive-parallelism approach.
[Performance evaluation, Instruction sets, parallel architectures, programmer time, Programming, CPU, Parallel, GPU, parallel programming, generalized excessive-parallelism approach, productivity, Parallel processing, Hardware, data structures, power limitations, high performance, CPU-centric, Manycore, software performance evaluation, frequency scaling, time-consuming tasks, accelerator programming, program diagnostics, abstraction, graphics processing units, application code, GPU-centric, Writing, Central Processing Unit, OpenCL, minimal performance degradation, algorithm abstraction]
Semantic-Aware Hot Data Selection Policy for Flash File System in Android-Based Smartphones
2013 International Conference on Parallel and Distributed Systems
None
2013
Flash memory has different characteristics from traditional hard disk drives. Therefore, the traditional file systems such as EXT4 are not well-optimized for flash memory storage. Recently, a flash memory-aware file system, called F2FS, is announced, which is based on the log-structured file system considering the poor random write performance of flash memory. Although F2FS uses a heuristic for separating hot and cold data, the heuristic is not aware of file type. In this paper, we observe the lifetime of different types of files in Android platform and propose a semantic-aware hot data selection policy for F2FS. Experimental results show that the proposed technique reduces the garbage collection cost by up to 31%.
[Android System, Humanoid robots, Hot-Cold Data Separation, flash memory-aware file system, Flash memories, garbage collection cost, Android (operating system), Garbage Collection, File systems, flash memories, hard disk drives, F2FS random write performance, Log-Structured File System, log-structured file system, flash file system, Educational institutions, smart phones, Browsers, EXT4, Android-based smartphones, semantic-aware hot data selection policy, Flash Memory, Mobile Platform, Androids, Smart phones]
SHOE: A SPARQL Query Engine Using MapReduce
2013 International Conference on Parallel and Distributed Systems
None
2013
As the reasoning aspects and the knowledge based processing capabilities of RDF (Resource Description Framework) have been widely adopted in W3C Recommendation, the ontology layer and query languages of the Semantic Web stack achieve a certain level of maturity. There exists an increasing need for high performance, read-only semantic analysis for the massive RDF data. In this demo we will present a Map-Reduce based SPARQL processing engine, called SHOE (SPARQL on Hadoop with Optimization Encoding), to handle billions of RDF triples. SHOE consists of three major components (1) the RDF data loader, (2) the partition generator and (3) the query processor. While this demonstration mainly focuses on enhancing SPARQL processing in the Hadoop platform, the underlying encoding and partitioning optimization strategies can be utilized by the common Map-Reduce frameworks in the share-nothing environment.
[Computers, SPARQL, search engines, partition generator, Conferences, RDF knowledge based processing capabilities, W3C recommendation, query languages, Resource description framework, SHOE, Engines, Optimization, MapReduce, query processing, share-nothing environment, encoding optimization strategies, RDF, semantic Web stack, read-only semantic analysis, SPARQL on Hadoop with optimization encoding, partitioning optimization strategies, Footwear, Educational institutions, RDF data loader, MapReduce based SPARQL processing engine, ontology layer, resource description framework, query processor, Semantic knowledge analysis, SPARQL query engine, ontologies (artificial intelligence), data handling]
MAP: Mobile Assistance Platform with a VM Type Selection Ability
2013 International Conference on Parallel and Distributed Systems
None
2013
The usage of remote compute and storage resources is becoming popular to assist mobile devices. However, existing frameworks that support mobile client/server applications do not consider the provision of resources in a large scale. Although a cloud-based infrastructure may provide a large number of resources on demand, most cloud offerings do not provide the right level of abstraction to assist mobile devices. Infrastructure-as-a-Service (IaaS) offerings are too complex to set up on demand and Platform-as-a-Service (PaaS) offerings are not flexible enough regarding Quality-of-Service (QoS) adjustment. To overcome these issues, we propose a middleware named Mobile Assistance Platform (MAP), which serves as an extended PaaS layer. It provides an on-demand execution platform for program code, but with the additional ability to select the underlying VM quality for execution. Furthermore, each mobile request is assigned to a single VM instance for processing. MAP provides configurable compute resources for mobile users in a large scale. For this demo, we have set up MAP on the Amazon EC2 infrastructure and we present two mobile applications that can benefit from the on-demand resource quality selection.
[Performance evaluation, Conferences, Mobile communication, Amazon EC2 infrastructure, Mobile handsets, remote compute, Servers, mobile computing, resource allocation, QoS, cloud computing, quality-of-service, middleware, infrastructure-as-a-service offering, Java, storage resources, platform-as-a-service offerings, Mobile Cloud Computing, Code Offloading, quality of service, Platform as a Service, Middleware, program code, IaaS offerings, mobile client-server applications, virtual machines, mobile devices, cloud-based infrastructure, PaaS offerings, resource quality selection, MAP, mobile assistance platform, VM type selection ability]
Using GPUs to Crack Android Pattern-Based Passwords
2013 International Conference on Parallel and Distributed Systems
None
2013
We investigate the strength of patterns as secret signatures in Android's pattern based authentication mechanism. Parallelism of GPU is exploited to exhaustively search for the secret pattern. Typically, searching for a pattern, composed of a number of nodes and edges, requires an exhaustive search for the pattern. In this work, we show that the use of GPU can speed up the graph search, hence the pattern password, through parallelization. Preliminary results on cracking the Android pattern based passwords shows that the technique can be used as the basis to implement a tool that can check the strength of a pattern based password and thereby recommend strong patterns to the user.
[Android pattern-based password cracking, Instruction sets, Conferences, Graphics processing units, Random access memory, GPUs, Indexes, Security, graphics processing units, parallel processing, GPU, Optimization, Android, Android (operating system), mobile computing, digital signatures, Pattern-password, Android pattern based authentication mechanism, Smart phones]
Towards Model Checking of Simulation Models for Embedded System Development
2013 International Conference on Parallel and Distributed Systems
None
2013
Modeling and simulation has been widely used to develop embedded systems and cyber-physical systems, especially in safety-critical domains. While models for such systems are required to be rigidly verified, simulation offers only partial observations on them with a limited number of test cases. Model checking techniques of timed automata would not be directly applied to formal verification of simulation models due to the differences in execution semantics and model composition. In order to enable model checking of simulation models, this paper presents an algorithm to obtain region transition systems from models written in an M&amp;S formalism. Such region transition systems could be exhaustively checked by the model checking techniques without any modification.
[simulation models, modeling and simulation, Schedules, Computational modeling, Conferences, automata theory, safety-critical software, execution semantics, Partitioning algorithms, model composition, cyber-physical systems, Embedded systems, formal verification, model checking, Automata, embedded system development, embedded systems, real-time systems, Model checking, safety critical domains, cyber physical systems, timed automata]
Privacy Vulnerability Analysis on Routing in Mobile Social Networks
2013 International Conference on Parallel and Distributed Systems
None
2013
Mobile social networks (MSNs) are a kind of delay tolerant network that consists of lots of mobile nodes with social characteristics. Recently, many social-aware algorithms have been proposed to address routing problems in MSNs. Because of the social properties introduced to routing, this results in node privacy disclosure. In this paper, analyzing social-based routing strategies, we propose a privacy attack tree model taking all the possible attack on social-based routing into account. This model describes all of the possibilities and approaches of privacy disclosure, and quantifies their the occurrence probability.
[telecommunication security, delay tolerant network, privacy attack tree model, social characteristics, mobile radio, Social network services, privacy disclosure, privacy vulnerability analysis, Routing, Mobile communication, delay tolerant networks, MSN, Privacy, Analytical models, routing problems, telecommunication network routing, social networking (online), mobile social networks, Delays, social-based routing, Mobile computing, mobile nodes, social-based routing strategies]
Lightweight Management of Authorization Update on Cloud Data
2013 International Conference on Parallel and Distributed Systems
None
2013
While outsourcing data to cloud, security and efficiency issues should be taken into account. However, it is very challenging to design a secure and efficient mechanism supporting authorization updates. In this paper, we aim to provide a mechanism supporting authorization updates which only incurs a lightweight cost of authorization updates and meanwhile supports a high level of security. This mechanism is consisted of two encryption schemes performed in different layers. The inner-layer encryption scheme is performed on the original plaintext and the generated cipher text is called inner-layer cipher text, while a part of the inner-layer cipher text is encrypted by the outer-layer encryption scheme to generate cipher text, called out-layer cipher text. These two encryption schemes are both performed by data owner. The inner-layer encryption realizes the initial authorization policy, while the outer-layer encryption reflects the updated authorization policy. We implement the proposed mechanism and conduct extensive experiments. The experimental results demonstrate that the proposed mechanism outperforms previous existing approaches, e.g. single-layer encryption and double-layer encryption.
[authorization policy, Educational institutions, security issues, plaintext, Encryption, Servers, efficiency issues, authorization updates, lightweight management, Authorization, Handheld computers, outsourcing, authorisation, cloud data outsourcing, private key cryptography, access control, inner-layer ciphertext encryption scheme, cloud computing, outer-layer ciphertext encryption scheme, authorization update]
Cloud Governance - The Relevance of Cloud Brokers
2013 International Conference on Parallel and Distributed Systems
None
2013
This contribution provides an overview of cloud governance aspects based on a representative survey of trends in the perception, assessment and adoption of Cloud Computing within the enterprise. The survey was conducted through a two-fold research approach and combined a meta-study of existing and published empirical studies with an own empirical study within a representative group of European and International enterprises. We consider governance through the prism of adoption drivers and focus on the role of cloud brokers and cloud marketplaces more specifically.
[Context, Cloud computing, European enterprise, International enterprise, Computational modeling, Security, cloud governance, cloud marketplaces, Organizations, cloud computing, adoption drivers, Contracts, cloud brokers]
Cloud Services Aided E-Tourism: In the Case of Low-Cost Airlines for Backpacking
2013 International Conference on Parallel and Distributed Systems
None
2013
The emergence of Cloud Services and Mobile Internet has influenced the society a lot, including the tourism industry. This paper proposes a design of backpacking service, not only aimed at the travelling routines, but also focus on the low-cost airlines. This kind of service aids backpackers with an effective travelling and satisfy the price requirement. With this low-cost airlines system, the backpackers can experience real e-Tourism and enjoy a better travel aided by the real-time information. According to the basic principles of low-cost airlines, this paper provides the backpackers with a definitely efficient way to check out a suitable flight itinerary under an acceptable price.
[Industries, Cloud computing, tourism industry, Backpacking, cloud services, Cloud Services, Transportation, mobile Internet, real e-tourism, Mobile communication, mobile computing, e-Tourism, travel industry, price requirement, Mobile Internet, Cities and towns, Real-time systems, cloud computing, low-cost airlines, backpacking service]
Reputation Measurement of Cloud Services Based on Unstable Feedback Ratings
2013 International Conference on Parallel and Distributed Systems
None
2013
With the rapid development of Cloud computing, more and more service providers could provide cloud services (applications) to users. Faced with mass Cloud services, trust and reputation mechanisms offer a promising way to solve the trust evaluation of Cloud services. Hence, trust and reputation play an important role in evaluating of Cloud services. In this paper, we propose a lightweight reputation measurement approach for Cloud services based on (user) feedback ratings. The proposed approach first adopts cloud model to obtain the trust vector of each cloud service by exploiting feedback ratings. The trust vector consists of Expected value, Entropy value and Hyper-Entropy value. Then we use fuzzy set theory to calculate the reputation scores of Cloud services. Simulation results show that the proposed approach is significantly effective for unstable feedback ratings.
[trust, Cloud computing, hyper-entropy value, TV, lightweight reputation measurement approach, reputation mechanism, user feedback ratings, trust vector, fuzzy set theory, reputation, Entropy, trust evaluation, feedback rating, Accuracy, entropy, cloud computing, expected value, trust mechanism, unstable feedback ratings, Vectors, Pragmatics, Fuzzy logic, Cloud service, vectors, mass cloud services, trusted computing, reputation scores]
Efficient Bulk Loading to Accelerate Spatial Keyword Queries
2013 International Conference on Parallel and Distributed Systems
None
2013
With the fast development of location-based services and geo-tagging, spatial keyword queries that retrieve objects satisfying both spatial and keyword conditions are gaining in prevalence. A hybrid index that integrates a spatial index (e.g., the R-tree or its variations) with a keyword filter offers a promising approach for processing such queries efficiently. However, it is still an open problem on how a hybrid index can be effectively constructed from scratch. The state-of-the-art bulk loading algorithms for the R-tree consider only spatial relationship, and cannot be employed for the hybrid index. In this paper, we propose a new bulk loading algorithm, named TPA, which constructs a hybrid index top-down. TPA utilizes a two-phase method to construct the children of nodes at each level of the hybrid index, taking both spatial and keyword information into consideration, and thus optimizes the hybrid index for spatial keyword queries. We analyze and evaluate its performance using both real and synthetic datasets. Comprehensive experiments show that TPA can achieve good performance and space utilization, reducing the construction time, the query latency and the index size remarkably.
[Algorithm design and analysis, index space utilization, bulk loading algorithm, index construction time reduction, Spatial indexes, keyword filter, location-based service, query processing, mobile computing, database indexing, spatial data structures, Loading, location-based services, spatial keyword query, index query latency, R-tree, hybrid index top-down, indexing, geotagging, spatial index, Educational institutions, Partitioning algorithms, two-phase partition algorithm, TPA index size, Vegetation, spatial keyword queries, object retrieval, bulk loading]
Spatial Data Mining in the Context of Big Data
2013 International Conference on Parallel and Distributed Systems
None
2013
In this paper, spatial data mining is presented in the context of big data. Spatial data plays a primary role in big data, attracting human interests. Spatial data mining confronts much difficulty when attracting the value hidden in spatial big data. The techniques to discover knowledge from spatial big data may help data to become intelligent.
[Spatial data mining, Data handling, data mining, Spatial databases, knowledge discovery, Information management, Data mining, Data storage systems, Earth, Satellites, spatial big data, human interests, Big data, spatial data mining, Data intelligence]
SNN Input Parameters: How Are They Related?
2013 International Conference on Parallel and Distributed Systems
None
2013
Nowadays, organizations are facing several challenges when they try to analyze generated data with the aim of extracting useful information. This analytical capacity needs to be enhanced with tools capable of dealing with big data sets without making the analytical process a difficult task. Clustering is usually used, as this technique does not require any prior knowledge about the data. However, clustering algorithms usually require one or more input parameters that influence the clustering process and the results that can be obtained. This work analyses the relation between the three input parameters of the SNN (Shared Nearest Neighbor) algorithm and proposes specific guidelines for the identification of the appropriate input parameters that optimizes the processing time.
[Algorithm design and analysis, Correlation, data analysis, density-based clustering, Noise, Big Data, Data mining, SNN input parameters, shared nearest neighbor algorithm, Guidelines, big data sets, Graphical models, information extraction, pattern clustering, clustering algorithms, shared nearest neighbor, Clustering algorithms, clustering, input parameters tuning]
Linear Function Based Transformation Scheme for Preserving Database Privacy in Cloud Computing
2013 International Conference on Parallel and Distributed Systems
None
2013
Because much interest in spatial database in cloud computing has been attracted, studies on preserving location data privacy in cloud computing have been actively done. However, since the existing spatial transformation schemes are weak to proximity attack, they cannot preserve the privacy of users who enjoy location-based services from the cloud computing. Therefore, a transformation scheme for providing a safe service to users is required. So, we, in this paper, propose a new transformation scheme based on a line symmetric transformation (LST). The proposed scheme performs both LST-based data distribution and error injection transformation for preventing proximity attack effectively. Finally, we show from our performance analysis that the proposed scheme greatly reduces the success rate of the proximity attack while performing the spatial transformation in an efficient way.
[Cloud computing, Data privacy, proximity attack prevention, spatial transformation schemes, visual databases, LST-based data distribution, line symmetric transformation, mobile computing, database privacy preservation, Spatial transformation scheme, data protection, proximity attack success rate reduction, location-based services, cloud computing, Estimation error, Linear Function, location data privacy preservation, Location data protection, Spatial databases, spatial database, Query processing, error injection transformation, Data models, linear function-based transformation scheme, performance analysis]
Quantitative Trust Management to Support QoS-Aware Service Selection in Service-Oriented Environments
2013 International Conference on Parallel and Distributed Systems
None
2013
Owing to the black-box nature of services, selecting a trustworthy service that best fits users' requirements is greatly critical in service-oriented computing. Once a set of services fulfilling users' functional requirements are founded, one of these services invoked by the users depends mostly on the Quality of Services (QoS), particularly security, trust, and reputation. This paper proposes a trust management model to support service discovery and selection based on QoS. We define a quantitative trust evaluating method for dynamic service discovery and selection. The proposed model makes service consumers get trustworthy services possible. Our mechanism uses consumers' feedback to describe the trust degree of services and service providers. The service selection using the quantitative measurement rather than consumers' intuition allows selecting a highly reliable service accomplishing their quality requirements well. Finally, we give experimental results.
[Availability, Context, Measurement, quantitative trust evaluating method, web services, Quality of service, dynamic service discovery, quality of service, Equations, trust management model, consumer intuition, service-oriented computing, Web services, trustworthy service selection, QoS, dynamic service selection, trust management, Service-oriented architecture, Mathematical model, service selection, trusted computing, service-oriented architecture, quality of services, consumer feedback]
Estimating Web Service Reputation from Integrated Social Service Network Model
2013 International Conference on Parallel and Distributed Systems
None
2013
Social networks facilitate information sharing and communication among people who have common interests. Although social networks have been widely used to compute the reputations of Web services, they are limited in the ability to support the sophisticated estimation of service reputations. This paper proposes a sophisticated method of computing service reputations, which considers service requesters, raters and providers participating in the service ecosystem. In order to represent the interactions among the service participants, we also propose an integrated model which combines social and service network models. Based on the model, the proposed method calculates the credibility and expertise of the raters and providers of a service and applies them to the reputation of the service. Experimental results with real-world Web services show that the proposed method computes service reputations elaborately.
[Web service reputation, service reputation, information sharing, Social network services, Computational modeling, Conferences, real-world Web services, computing service reputation, social service network model, Equations, service discovery, service requester, ecology, Web services, service participants, integrated social service network model, Organizations, sophisticated estimation, social network model, social networking (online), Mathematical model, Reliability, service ecosystem, information communication]
A Service Path Selection and Adaptation Algorithm in Service-Oriented Network Virtualization Architecture
2013 International Conference on Parallel and Distributed Systems
None
2013
With the advances of networking and cloud computing technologies, many web services are often implemented as cloud services by different service providers, and some SPs compose those services to create an end-to-end network service. In this service-oriented network virtualization environment, two important but somehow contradictory technical challenges are 1) how to select an optimal service path in order to meet the QoS requirements by users, and 2) how to balance the loads to fully utilize the whole systems. In this paper, we present an adaptive service path selection algorithm in service-oriented network virtualization environment, which assures QoS and balances the load at the same time. The proposed algorithm selects appropriate component services from multiple candidate service instances running on virtual machines and creates an optimal service path satisfying the users' QoS requirements. Our algorithm also dynamically adapts to the optimal service path based on various changes. Since the proposed algorithm handles a variation of the multi-constrained path selection problem known as NP-complete, we formulate the problem using ant colony optimization algorithm. The experimental results show that our algorithm guarantees the maximal QoS to the users while balancing the load at the same time, and adapts to the optimal service path based on the situational changes.
[load balancing, Quality of service, adaptation, ant colony optimization algorithm, Overlay networks, resource allocation, QoS, service overlay network, cloud computing technology, end-to-end network service, Mathematical model, cloud computing, ant colony optimisation, service-oriented architecture, service path selection, service path adaptation algorithm, cloud services, service-oriented network virtualization, load balances, quality of service, service-oriented network virtualization architecture, Indexes, NP-complete problem, multiconstrained path selection problem, Equations, Web services, service path selection algorithm, service providers, virtual machines, Load management, Time factors, computational complexity]
Towards a Scalable PaaS for Service Oriented Software
2013 International Conference on Parallel and Distributed Systems
None
2013
Software developers with service oriented technologies usually put a lot of efforts to deploy and manage supporting middleware and tools. Meanwhile PaaS in cloud computing aims at provide efficient support for software developers. In the light of this consideration, we have designed and implemented Service4All, a service cloud platform targeting at improve productivity of service oriented software developers. In this paper, we describe the design of SAE, a key component in Service4All, in terms of scalability. First, we present the key technical issues and architecture design of SAE. Second, we describe a software appliance based mechanism for elastic middleware management. Third, we describe a micro-kernel based AppEngine core for efficient coordination of various components. Finally, through a real application deployed on Service4All, we demonstrate the effectiveness of our solution.
[Cloud computing, Scalability, microkernel based AppEngine core, PaaS, Elasticity, Service4All, Service oriented computing, Home appliances, service cloud platform, software appliance based mechanism, SAE, service oriented technologies, service oriented software developers, Software appliance, elastic middleware management, cloud computing, service-oriented architecture, Testing, middleware]
Tag Co-occurrence Relationship Prediction in Heterogeneous Information Networks
2013 International Conference on Parallel and Distributed Systems
None
2013
In this work, we address a novel problem about tag co-occurrence relationship prediction across heterogeneous networks. Although tag co-occurrence has recently become a hot research topic, many studies mainly focus on how to produce the personalized recommendation leveraging the tag co-occurrence relationship and most of them are considered in a homogeneous network. So far, few studies pay attention to how to predict tag co-occurrence relationship across heterogeneous networks. In order to solve the aforementioned problem, we propose a novel two-step prediction approach. First, weight path-based topological features are systematically extracted from the network. Then, a supervised model is used to learn the best weights associated with different topological features in deciding the co-occurrence relationships. Experiments are performed on real-world dataset, the Flickr network, with comprehensive measurements. Experimental results demonstrate that weight path-based heterogeneous topological features have substantial advantages over commonly used link prediction approaches in predicting co-occurrence relations in information networks.
[Weight measurement, topology, Predictive models, heterogeneous information networks, weight path, Tag Co-occurrence, Training, Flickr, Semantics, feature extraction, topological feature extraction, heterogeneous network, link prediction, Tagging, image retrieval, Feature extraction, social networking (online), Flickr network, Internet, link prediction approaches, tag co-occurrence relationship prediction]
SoMed: A Hybrid DHT Framework towards Scalable Decentralized Microblogging Services
2013 International Conference on Parallel and Distributed Systems
None
2013
Microblogging services have experienced unprecedented growth. They serve more as news media outlets than online social networks, which can predict the H1N1 virus emergency, detect earthquakes, and live major events. These kinds of burst events will cause peak access. Servers will suffer from large amount of concurrent access and lead to single point of failures. Furthermore, divergent behaviors of two categories of users, social users and media users, will lead to asymmetric workloads. For some media users, such as Lady Gaga, who accounts for over 20 million followers, will generate a huge amount of traffic and make the centralized mechanism hard to scale. To address these problems together, we present SoMed, a hybrid DHT framework towards scalable, decentralized microblogging services. We also conduct a simulation with real traces of Twitter. The results show significantly bandwidth saving and response time reducing with high reliability and availability.
[media users, online social networks, Media, distributed processing, Twitter, Routing, centralized mechanism, Servers, decentralized microblogging services, DHT, SoMed, distributed hash table, online social networking, social users, Bandwidth, hybrid DHT framework, social networking (online), Peer-to-peer computing]
Ontology-Based TV Program Contents Retrieval and Recommendation
2013 International Conference on Parallel and Distributed Systems
None
2013
In this paper we propose a searching and recommendation method of TV program contents in order to reduce the information overload problem. Most of previous recommendation approaches are dependent on simple ratings of users to determine preference similarity among the users. But our approach is closely related to the combination of ontology-based TV program searching and content-based filtering based on TV ontology and usage history. We search TV programs by computing the similarity between contents ontologies, filter the candidates with preferences of users, and return the ranked list of TV programs. A subjective experiments show that our proposed method is effective in semantic-based searching and recommendation.
[TV, Filtering, Computational modeling, information overload problem, television, searching, Ontologies, semantic-based searching, recommendation method, recommendation, Vectors, information filtering, History, content-based retrieval, recommender systems, preference similarity, ontology-based TV program searching, Broadcasting, ontology-based TV program contents retrieval, ontologies (artificial intelligence), content-based filtering, TV program ontology, usage history]
Indoor Place Name Annotations with Mobile Crowd
2013 International Conference on Parallel and Distributed Systems
None
2013
With the popularity of mobile devices, numerous mobile applications have been and will continue to be developed for various interesting usage scenarios. Riding this trend, recent research community envisions a novel information retrieving and information-sharing platform, which views the users with mobile devices and being willing to accept crowd sourcing tasks as crowd sensors. With the neat idea, a set of crowd sensors applications have emerged. Among the applications, the geospatial information systems based on crowd sensors show significant potentials beyond traditional ones by providing real time geospatial information. In the applications, user positioning is of great importance. However, existing positioning techniques have their own disadvantages. In this paper, we study using pervasive Wi-Fi access point as a position indicator. The major challenge for using Wi-Fi access point is that there is no mechanism for mapping observed Wi-Fi signals to human-defined places. To this end, our idea is to employ crowd sourcing model to perform place name annotations by mobile participants to bridge the gap between signals and human-defined places. In this paper, we propose schemes for effectively enabling based-based place name annotation, and conduct real trials with recruited participants to study the effectiveness of the proposed schemes. The experiment results demonstrate the effectiveness of the proposed schemes over existing solutions.
[information retrieving platform, real time geospatial information, Mobile communication, geographic information systems, Mobile handsets, Geospatial analysis, crowd sourcing model, crowd sourcing task, information-sharing platform, mobile computing, Clustering algorithms, Geospatial Information System, indoor place name annotation, Sensors, user positioning, position indicator, Crowdsourcing, IEEE 802.11 Standards, wi-fi signals, recent research community, information retrieval, human-defined places, geospatial information system, positioning techniques, Global Positioning System, Mobile Application, mobile devices, crowd sensors, pervasive wi-fi access point, wireless LAN, mobile crowd]
On Deploying IoT Indoor: An Incremental Approach
2013 International Conference on Parallel and Distributed Systems
None
2013
Deploying radio-connected Internet-of-Things in an indoor environment is challenging. Most deployment methods are based on the radio range to determine the density or distance to place the nodes. Unfortunately, the surrounding obstacles such as walls and furniture make the communication pattern unpredictable. It follows that a well-planned IoT deployment plan that guarantees full connectivity on the drawing board may result in disconnected system when deployed on-site. A practical IoT deployment tool must cope with the difference in offline planning and on-site deployment. In this paper, a practical tool is introduced that deploys a network of IoT incrementally for indoor environments. The tool ensures node connectivity in the field despite environmental obstacles. The deployment process is divided into three main phases: offline deployment, online deployment and optimization. Each phase of the tool aims to place as few relay nodes as possible but always ensure full connectivity. The effectiveness of the tool is evaluated through real deployment.
[Internet-of-Things, online deployment, Buildings, Indoor environments, offline deployment, Loss measurement, indoor environment, Internet of Things, Relays, Optimization, tool, incremental approach, Bridges, Mesh networks, optimisation, IoT indoor, optimization, radio-connected Internet-of-Things, indoor environments, practical IoT deployment tool, radio propagation, deployment]
A Feasibility Study on Developing IoT/M2M Applications over ETSI M2M Architecture
2013 International Conference on Parallel and Distributed Systems
None
2013
A common service platform is considered as the key enabler to catalyze the development of IoT/M2M applications for a large variety of vertical markets such as smart energy, smart transportation, home and industry automation, eHealth and connected vehicles. This paper reports an initial effort at National Chiao Tung University in Taiwan with such an experimental approach. We first train our graduate students with an ETSI M2M architecture-compliant service platform OpenMTC from FOKUS, and then charter them with the tasks of developing diverse M2M applications. The effort is used as a feasibility study to investigate how useful the notion of a common service platform for IoT/M2M is and how urgent an international standard is required in defining such a platform. We intend to use the result of the study to create a suitable IoT/M2M curriculum for our students.
[ETSI M2M architecture, common service platform, IoT-M2M curriculum, connected vehicles, ETSI Standards, Containers, machine-to-machine architecture, eHealth, National Chiao Tung University, smart transportation, vertical markets, IoT/M2M Applications, key enabler, graduate students training, cloud computing, smart energy, OpenMTC compliant service platform, international standard, Horizontal Service Platforms, computer science education, Telecommunication standards, home and industry automation, Internet of Things, Intelligent sensors, standards, IoT-M2M application development, Taiwan, Logic gates, Smart phones, FOKUS, OpenMTC]
Challenges of Using Edge Devices in IoT Computation Grids
2013 International Conference on Parallel and Distributed Systems
None
2013
Internet of Things (IoT) has the potential to become a technology revolution with a vision of creating very large scale network, comprising of unprecedented number of connected devices. These devices, often referred to as smart items or intelligent things can be home appliances, healthcare devices, vehicles, buildings, factories and almost anything networked and fitted with sensors, actuators, embedded computers. There has been sustained research work and standardization effort from different IoT perspectives like integration of sensor and RFID devices to the Internet. With the increasing trend of gathering business insights from unstructured data, the high volume of data generated by such devices is also of interest. Cloud based data mining platforms are suitable for analyses of such data and researchers have proposed architectures where personal mobile phones can act as Edge Gateway between the sensor network and cloud analytics platform. It seems that the surge in the volume of data generated by huge number of Smart Items can only be matched if a large percentage of mobile users start sharing the computation capability of their personal devices and work together towards true Participatory Computing in the IoT systems. In this work we try to understand the challenges associated with running computation jobs on the mobile devices using different types of workload often observed in IoT applications. Based on the insights gained from experiments performed by us, we propose a scheme where mobile phones, residential gateways and other edge devices offer free slots to servers in a cloud based data analytics system. Based on the free time slots offered by the mobile phones, if commensurately sized computational jobs can be scheduled, the unpredictability associated with using mobile phones as grid resources can be solved.
[IoT computation grids, data mining, mobile phones, smart items, Mobile communication, vehicles, grid resources, factories, cloud based data mining platforms, cloud analytics platform, Servers, intelligent things, technology revolution, iot, analytics, Internet of things, mobile computing, actuators, participatory computing, embedded systems, business insights, residential gateways, Sensors, mobile users, cloud computing, Availability, RFID devices, home appliances, personal mobile phones, Internet of Things, edge devices, ubiquitous systems, healthcare devices, mobile grid, sensors, sensor network, internet of things, buildings, prediction, embedded computers, edge gateway, unstructured data, Internet, cyber-physical, Smart phones, mobile handsets]
Social Web of Things: A Survey
2013 International Conference on Parallel and Distributed Systems
None
2013
Recently, Wireless Sensor Networks (WSNs) are spread all over the world, and are commonly used to collect physical information from the surrounding world. WSNs play a central role in the Internet of Things (IoT) vision. IoT is a new paradigm that aims to integrate and connect anything at anytime, anyplace, with anything and by anyone. However, addressing objects is the main challenge for IoT. IPv6 over Low power Wireless Personal Area Networks (6LoWPAN) has been introduced to cope with this issue. The emerging paradigm Social web of thing (SWoT) enables users to manage, access, share, and integrate smart objects with Social Network Site (SNS). This paper investigates different SWoT platforms and architectures, provides an overview of the current state-of-the-art on WSN communication standards.
[Protocols, Bluetooth, wireless sensor networks, Zigbee, smart object access, Social Web of Things, social network site, IEEE 802.15 Standards, smart object management, smart object integration, social Web of things, Social Network Site, 6LoWPAN, IP networks, WSN communication standards, ZigBee, Social network services, Web of Things, IPv6 over Low-power Personal Area Networks, Internet of Things, telecommunication standards, Wireless sensor networks, personal area networks, IPv6 over low power wireless personal area networks, Wireless Sensor Network, SWoT architectures, social networking (online), smart object sharing, Home and Building Automation, Internet Of Things, Internet]
Coexistence of Heterogeneous and Homogeneous Wireless Technologies in Smart Grid-Home Area Network
2013 International Conference on Parallel and Distributed Systems
None
2013
In a home environment, a Smart Grid Home Area Network (SG-HAN) platform facilitates collection and delivery of power consumption information for load profiling and informed decisions on energy management. However, one of the main challenges in HAN is the overcrowded unlicensed 2.4-GHz ISM frequency band, occupied by several types of radio technologies such as ZigBee, Bluetooth, and WiFi. It is crucial that those technologies coexist peacefully to allow each user of the radio technology to fulfill their communication goals. In this paper, we present a potential coexistence scenario in SG-HAN for homogeneous and heterogeneous wireless technologies. The coexistence impact on SG-HAN performance is then modeled and analyzed. The numerical results show significant performance degradation due to the interference for devices in close proximity with the interfering sources in a spectrum sharing environment where in the worst case scenario, SG-HAN communication is almost impossible.
[heterogeneous wireless technology, 802.15.4, radio networks, Bluetooth, spectrum sharing environment, smart power grids, Throughput, Coexistence, Communication system security, overcrowded unlicensed ISM frequency band, Wireless communication, IEEE 802.15 Standards, frequency 2.4 GHz, home networks, energy management, Smart grids, Smart Grid, ZigBee, Interference, homogeneous wireless technology, load profiling, radio technology, Home Area Network, energy management systems, power consumption information, WiFi, smart grid-home area network, SG-HAN communication, Payloads]
Overhead Reduction for Duplicate Address Detection in VANET
2013 International Conference on Parallel and Distributed Systems
None
2013
In VANET, nodes move at high speed and change connecting network frequently. As a consequence, IP duplication may occur and packets would not be sent to the correct destination. This paper aims at reducing the size of location information in duplicate address detection (DAD) scheme. We propose a grid-based duplicate address detection (GDAD) scheme and separate network into several grids, where the location information in the packet is replaced with the grid number. In addition, we model and calculate the probability of occurrence of IP duplication. Finally, we simulate and estimate the probability of IP duplication.
[vehicular ad hoc networks, IP auto-configuration, GDAD, vehicular ad hoc networks (VANET), Conferences, location information, VANET, Routing, duplicate address detection (DAD), Global Positioning System, IP duplication, Simulation, grid based duplicate address detection, Vehicular ad hoc networks, overhead reduction, Internet, IP networks, grid number]
LocalSense: An Infrastructure-Mediated Sensing Method for Locating Appliance Usage Events in Homes
2013 International Conference on Parallel and Distributed Systems
None
2013
In this paper, we introduce a novel technique called Local Sense for detecting appliance usage events in a household by leveraging existing home power infrastructure. Specifically, the Local Sense technique works by purely analyzing the records taken from the main power meter. The main power meter is installed in a household to measure the overall real time aggregated power consumption in terms of wattage and voltage for the whole load of appliances in a household. The Local Sense technique requires no additional apparatuses being installed in existing household power infrastructure, which significantly reduces the cost for enabling the location-aware power usage information over existing techniques. The idea behind the Local Sense system is to reflect the fact that different outlets are with different power line impedances, which causes different power dissipation even for the same appliance usage. By identifying the additional power dissipation, detecting the appliance usage events at outlet levels is therefore possible. The proposed technique is validated in a research lab and preliminary results demonstrate the effectiveness of the proposed system.
[power line impedances, domestic appliances, power consumption, Home appliances, power aware computing, main power meter, Databases, Electricity, power cables, appliance usage events, Sensors, infrastructure-mediated sensing method, power meters, power utilisation, Power Consumption Awareness, power dissipation, Hair, Power demand, Green Computing, location-aware power usage information, localsense, Non-intrusive Load Monitoring System, real time aggregated power consumption, sensors, home power infrastructure, Impedance]
Virtual Coordinate Based Co-moving Detection in Wireless Sensor Networks
2013 International Conference on Parallel and Distributed Systems
None
2013
In mobile wireless sensor networks, it is important to understand the moving pattern of sensor nodes. There are a lot of application that need sensors nearby to co-work, such as data centric applications. Thus, knowing the co-moving nodes will be helpful to these applications. In this paper, we propose a co-moving sensor nodes detection algorithm based on virtual coordinate system. We use the geometric property of independent set to build our virtual coordinate system. The computation cost of building a virtual coordinate in this way is low and the result is relative accurate - just a coordinate rotation will it matches true coordinates. With the aids of virtual coordinate system, we can find out co-moving sensor node pairs efficiently and no precise localization scheme needed (for each sensor) in our works.
[wireless sensor networks, virtual coordinate system, comoving detection, Mobile nodes, Object tracking, Co-moving detection, Wireless sensor networks, sensor nodes, Animals, mobile wireless sensor networks, Virtual coordinate, object tracking, Detection algorithms]
Energy Saving Data Abstraction and Reformation Algorithms for Human Movement Monitoring
2013 International Conference on Parallel and Distributed Systems
None
2013
Detecting human movement is an important issue in monitoring and studying human activities especially for elderly and patients. The availability of wireless sensor network eases the monitoring work. Generally, a wireless sensor node for movement detection is embedded with an accelerometer and powered by batteries. The sensor node needs to transmit the sensed data from accelerometer wirelessly to other nodes or directly to a base station. If there are more data to be transmitted, it consumes more energy and thus the batteries drain out more quickly. Thus, an energy saving scheme, called Data Abstraction and Reformation (DAR), is proposed in this paper to reduce data transmission. Through data abstraction, sensor nodes filter out insignificant sensed data but only report those significant to a base station, and with data reformation, the complete data will be reconstructed at the base station with only the received data. We show that with a good selection of data abstraction and data reformation criteria, a movement detecting sensor node will only need to report 10%<sub>~</sub>30% of the sensed data in order to provide full human movement monitoring at a base station.
[Accelerometers, Base stations, wireless sensor networks, wireless sensor network, reformation algorithm, Vectors, human movement monitoring, data reformation, Wireless sensor networks, mobile computing, energy saving data abstraction, DAR, accelerometer, data transmission, accelerometers, Data communication, Biomedical monitoring, Monitoring, Data Abstraction and Reformation]
Design and Implementation of Media Content Sharing Services in Home-Based IoT Networks
2013 International Conference on Parallel and Distributed Systems
None
2013
The penetration ratio of broadband networks into residential areas increases rapidly by the wide distribution of Internet service providers and networks. People are able to distribute and play various media content with many types of networked multimedia devices for home multimedia entertainment in residential environments. This paper addresses a new idea of home-based IoT networks where home-networked devices are able to communicate with others in a friendly, networked manner instead of traditional manual configurations and wired cabling operations. Accordingly, this paper proposes a novel intelligent media distribution system based on a home-based IoT network. The design of this system integrates UPnP, face recognition, intelligent human-machine interface, and family database technologies. UPnP-compatible HNDs With UPnP, networked devices can discover neighboring devices in a network. Face recognition is incorporated and so provides the UPnP networked devices with the capability of identifying the operating user in front of them. When a user moves in a home-based network, the intelligent human-machine interface allows a user to enforce any media content to be distributed to or displayed onto the UPnP-based device nearby the user. Furthermore, this paper presents a prototypical development, as well as a real demonstration with experimental UPnP-based network devices in home networks. Therefore, the study in this paper enables a ubiquitous media distribution service in home-based IoT network environments.
[Home automation, TV, residential environments, broadband networks, smart home, user interfaces, Servers, home-networked devices, IoT, home multimedia entertainment, UPnP-compatible HND, Databases, intelligent human-machine interface, Media content sharing, home networks, face recognition, Face, residential areas, media content sharing service implementation, intelligent media distribution system, Face recognition, broadband network penetration ratio, Media, Internet service providers, man-machine systems, Internet of Things, ubiquitous media distribution service, home-based IoT networks, media content sharing service design, digital home, Internet networks, networked multimedia devices, family database technologies, operating user identification]
RFID Vehicle Plate Number (E-Plate) for Tracking and Management System
2013 International Conference on Parallel and Distributed Systems
None
2013
Nowadays, internet are used almost in any application and field, even small item registered with code and update in database then can buy it by online system. Numbers of vehicles are significant increase every year and many cases of vehicle theft and missing thus internet of things (IoT) is a technology can be use to overcome the issues. This paper presents on the use of RFID vehicle plate number (e-plate) for tracking and management system. Started by design RFID e-plate antenna based and vehicle plate number size to achieve optimum performance by utilization of plate number size then an RFID chip attached to the plate. Proposed antenna of e-plate design using a low cost FR4 material and antenna band works at 902-928 MHz frequency for UHF RFID application, with result 3.8 dbi antenna gains there is performance improvement compared to the current tag antenna. The shape of antenna is rectangular and has a dimension of 300 mm x 100 mm, which is usually the typical size of the conventional vehicle registration plate number. Since vehicle e-plate used RFID chip and some information stored inside the tag then when RFID readers install at strategic point able capture vehicle information. All information collected by readers can be analyzed for vehicle tracking, monitoring and transportation management system.
[RFID e-plate antenna design, frequency 902 MHz to 928 MHz, transportation management system, radiofrequency identification, Materials, UHF RFID application, Vehicles, UHF antennas, object tracking, antenna gains, vehicle monitoring, Antenna radiation patterns, Antenna, Antenna measurements, rectangular antenna, RFID chip, low cost FR4 material, RFID readers, RFID, traffic engineering computing, Internet of Things, vehicle information, tracking and management system, RFID vehicle plate number, antenna band, vehicle registration plate number, vehicle e-plate, e-Plate, computerised monitoring, Internet, Radiofrequency identification, vehicle tracking]
A Novel Query Tree Protocol Based on Partial Responses for RFID Tag Anti-collision
2013 International Conference on Parallel and Distributed Systems
None
2013
In the RFID system, when two or more tags respond their IDs to the reader simultaneously, wireless signal collision occurs and no tag can be identified successfully by the reader. How to reduce such collisions in order to speed up the identification performance is thus important. There are many anti-collision protocols proposed to solve the tag collision problem. They can be categorized into two classes: ALOHA-based and tree-based protocols. The query tree (QT) protocol is a famous tree-based protocol having many advantages. It is stateless and uses no on-tag memory to keep protocol states, it is a plain protocol and uses no special techniques, such as bit-tracking, ID-revising, and re-identification. In this paper, we propose a stateless and plain tree based anti-collision protocol, called PRQT, by using tag ID partial responses to speed up tag identification. We also conduct simulation experiments for PRQT and compare it with QT in terms of the number of iterations to identify tags. As we will show, the PRQT protocol uses less numbers of iterations to identify tags than the QT protocol.
[RFID tag anti-collision, Protocols, radiofrequency identification, Conferences, on-tag memory, anti-collision, tag ID partial responses, Information management, access protocols, query tree protocol, ID-revising, PRQT protocol, wireless signal collision, ALOHA-based protocol, tree-based protocol, bit-tracking, trees (mathematics), tag identification, anti-collision protocols, ID re-identification, RFID, protocol states, tag collision problem, RFID system, Computer science, Simulation, query tree, Radiofrequency identification]
Genetic Algorithm Based Weight Optimization for Minimizing Sidelobes in Distributed Random Array Beamforming
2013 International Conference on Parallel and Distributed Systems
None
2013
This paper proposes solution to optimize the peak side lobes level (PSLL) in a distributed random antenna array (RAA) when locations of the nodes in the array cannot be manipulated. Using the conventional beam forming method, RAA produces a poor beam pattern with high side lobe level, which greatly reduces the performance and the efficiency of the antenna. Existing literature focuses on finding the best position of antenna placement in RAA to lower the side lobes. This is not feasible when the user has no autonomy over the position of the antenna elements. Our proposed solution achieves beam pattern with much lower PSLL regardless of the array size and number of nodes in the array. The proposed method also enables up to 40% of energy savings when the size of array is small and 20% of savings when bigger array size is considered.
[Array signal processing, antenna placement, energy savings, array size, genetic algorithms, side lobe reduction, Optimization, Biological cells, random antenna array, Genetic algorithms, genetic algorithm based weight optimization, RAA, peak side lobes level, antenna radiation patterns, antenna arrays, distributed random antenna array, distributed random array beamforming method, Arrays, minimisation, Antenna arrays, Antenna radiation patterns, array signal processing]
Hardware Implementation of Math Module Based on CORDIC Algorithm Using FPGA
2013 International Conference on Parallel and Distributed Systems
None
2013
This paper discusses the implementation of math hardware module based on CORDIC algorithm to solve trigonometry, hyperbolic and exponential function on FPGA. CORDIC is one of the hardware efficient and iteration based algorithms that is used to implement various transcendental functions such as trigonometry, hyperbolic, exponential and so forth. In addition, by using this algorithm, the hardware requirement and cost are less as only shift registers, adders and ROM are required. Thus, the design is implemented on FPGA since it provides a versatile and inexpensive way for implementation. The design is then further interfaced with 4x4 matrix keypad and 16x2 character LCD to build a simple math hardware module for real time application. The coding of algorithm was written in Verilog HDL and the verification is done firstly by using simulation results of the ModelSim and then using the implementation on Alter a DE1 board with the design interfaced with keypad and LCD to display the results.
[Algorithm design and analysis, iterative methods, Altera DE1 board, field programmable gate arrays, mathematics computing, FPGA, Verilog HDL, hardware description languages, adders, hyperbolic function, digital arithmetic, Computer architecture, 16x2 character LCD, Hardware, iteration based algorithms, CORDIC algorithm, Mathematical model, matrix keypad, keyboards, liquid crystal displays, trigonometry, exponential function, read-only storage, LCD, CORDIC, Equations, FPGA design, ROM, transcendental functions, peripheral interfaces, math hardware module implementation, shift registers, ModelSim, 4x4 matrix keypad, Hardware design languages, Field programmable gate arrays]
Utilizing Human Intelligence in a Crowdsourcing Marketplace for Big Data Processing
2013 International Conference on Parallel and Distributed Systems
None
2013
Crowd sourcing emerging as a distributed problem-solving model can dispatch tasks to a large number of human workers. The workers' human intelligence can be utilized to execute tasks which cannot be efficiently performed by computer-based systems. A crowd sourcing environment supports various data processing tasks, for example, data collection, data annotation, data validation, data classification, and natural language processing. In particular, the crowd sourcing environment can be applied for big data processing as well. In this paper, we propose Human-Intelligence-as-a-Service (HIaaS) which is a cloud computing service model utilizing human intelligence in a crowd sourcing marketplace to process big data. We also propose an optimization model based on stochastic programming for provisioning human workers in a HIaaS-based marketplace. Numerical studies are performed to evaluate the optimization model. A video rating system handling a big data set is an illustrative example analyzed in the studies. The results show that the model can efficiently reduce the cost to provision workers for processing a big data job.
[Uncertainty, video rating system, Data handling, data validation, stochastic programming, Information management, HIaaS, Optimization, crowdsourcing marketplace, cloud computing service model, data classification, cloud computing, big data, Contracts, crowdsourcing, Computational modeling, natural language processing, Big Data, distributed problem-solving model, optimization model, data collection, Data storage systems, human intelligence, data annotation, big data processing, human-intelligence-as-a-service]
Feature Reduction for Anomaly Detection in Manufacturing with MapReduce GA/kNN
2013 International Conference on Parallel and Distributed Systems
None
2013
Manufacturing data is an important source of knowledge that can be used to enhance the production capability. The detection of the causes of defects may possibly lead to an improvement in production. However, the production records generally contain an enormous set of features. It is almost impossible in practice to monitor all features at once. This research proposes the feature reduction technique, which is designed to identify a subset of informative features that are representatives of the whole dataset. In our methodology, manufacturing data are pre-processed and adopted as inputs. Subsequently, the feature selection process is performed by wrapping Genetic Algorithm (GA) with the k-Nearest Neighborhood (kNN) classifier. To improve the performance, the proposed technique was parallelized with MapReduce. The results show that the number of features can be reduced by 50% with 83.12% accuracy. In addition, with MapReduce on the cloud, the performance can be increased by 17.5 times.
[Algorithm design and analysis, pattern classification, manufacturing data, Genetic Algorithm, Manufacturing Data, MapReduce GA, anomaly detection, kNN, k-Nearest Neighbor, genetic algorithms, feature reduction technique, k-nearest neighborhood classifier, manufacturing data processing, Genetic algorithms, MapReduce, genetic algorithm, Feature Selection, Accuracy, security of data, production records, Feature extraction, Manufacturing, Monitoring, Testing]
Parallel molecular computation of modular-multiplication based on tile assembly model
2013 International Conference on Parallel and Distributed Systems
None
2013
DNA computing is a new method for computation using the technology in molecular biology. The enormous parallel computing ability of DNA computing brings new opportunities and challenges to the development of cryptography. DNA cryptography is a cutting-edge sciences which combines classical cryptogram and molecular computing. Finite field GF(2n) is one of the most commonly used mathematic sets for cryptography. This paper proposes a parallel molecular computing system to compute the modular-multiplication, an operation combining multiplication and reduction, over finite field GF(2n). The operation of reduction is executed after the completion of the operation of multiplication. An instance of computing modular-multiplication is introduced to show the details of our system. The time complexity is &#x0398;(n) and the space complexity is &#x0398;(n2).
[molecular biology, Assembly systems, finite field GF, modular-multiplication computing, tile assembly model, mathematic sets, Cryptography, parallel molecular computing system, Assembly, DNA computing, cryptogram, biocomputing, parallel algorithms, Computational modeling, reduction operation, Modular-multiplication, Tile assembly model, time complexity, cryptography, parallel molecular computation, Galois fields, Finite field GF(2n), Tiles, DNA, DNA cryptography, molecular biophysics, cutting-edge sciences, space complexity, computational complexity]
On the HDT with the Tree Representation for Large RDFs on GPU
2013 International Conference on Parallel and Distributed Systems
None
2013
Nowadays, semantic web technology depends on interexchange and integration of RDF data for various aspects of each social communication. The searching for possible answer to the related topics across the open data source obviously becomes a massive task. In this research, we study the RDF query with parallel processing on the GPU. In particular, in this paper, we consider the compact data representation for the RDFs which can enable importing more data to the GPUs memory to enable parallel search in the GPU. The key idea is the use of compressed data type such as HDT before going the search on GPUs. Loading the HDT file to the GPUs straightforwardly and perform searching may not be the good solutions. Thus, this work presents the tree representation from the HDT data which can com-pact the HDT triples, ease the GPU memory transfer, and enable the GPU parallel search. With the HDT representation, the size is reduced from the original RDF about 10%-30%. Together with the tree array representation, we can reduce the redundant terms from in HDT triples by 30%-50% for the test cases.
[SPARQL, Dictionaries, Instruction sets, RDF query, Graphics processing units, HDT triples, Resource description framework, parallel processing, GPU, RDF, tree array representation, RDF data, Java, semantic Web, HDT representation, trees (mathematics), compact data representation, Indexes, graphics processing units, open data source, RDF Binary compression, GPU parallel search, semantic Web technology, HDT data, Java Framework, GPU memory transfer, Arrays]
A Dynamic Hybrid Resource Provisioning Approach for Running Large-Scale Computational Applications on Cloud Spot and On-Demand Instances
2013 International Conference on Parallel and Distributed Systems
None
2013
Testing and executing large-scale computational applications in public clouds is becoming prevalent due to cost saving, elasticity, and scalability. However, how to increase the reliability and reduce the cost to run large-scale applications in public clouds is still a big challenge. In this paper, we analyzed the pricing schemes of Amazon Elastic Compute Cloud (EC2) and found the disturbance effect that the price of the spot instances can be heavily affected due to the large number of spot instances required. We proposed a dynamic approach which schedules and runs large-scale computational applications on a dynamic pool of cloud computational instances. We use hybrid instances, including both on-demand instances for high priority tasks and backup, and spot instances for normal computational tasks so as to further reduce the cost without significantly increasing the completion time. Our proposed method takes the dynamic pricing of cloud instances into consideration, and it reduces the cost and tolerates the failures for running large-scale applications in public clouds. We conducted experimental tests and an agent based Scalable complex System modeling for Sustainable city (S3) application is used to evaluate the scalability, reliability and cost saving. The results show that our proposed method is robust and highly flexible for researchers and users to further reduce cost in real practice.
[Cloud computing, Adaptation models, workflow scheduling, program testing, reliability, agent based scalable complex system modeling for sustainable city, Fault tolerance, Fault tolerant systems, EC2, pricing schemes, Cities and towns, scheduling, spot instance, cloud computing, Amazon elastic compute cloud, dynamic pricing, cost reduction, fault tolerance, disturbance effect, Dynamic scheduling, cloud computational instances, S3 application, software agents, software fault tolerance, dynamic hybrid resource provisioning approach, cloud spot, resource provisioning, spot price, on-demand instances, public clouds, running large-scale computational applications, cost analysis, pricing]
Cost Optimization for Scientific Workflow Execution on Cloud Computing
2013 International Conference on Parallel and Distributed Systems
None
2013
Scientific workflow applications generally require various levels of computing power over the course of execution. The applications then often take advantage of Cloud computing due to its cost-effective, pay-as-you-go pricing model. However, the scientific workflow executions must be planned wisely in order to minimize total cost of the resource usage. In addition, lateness of completing some workflows may result in high penalty cost. In this paper, the scheduling algorithm based on GA and PSO is proposed for optimizing the workflow execution. The experiment to evaluate the scheduling efficiency is performed on the simple workflow engine developed by the authors. The result is then compared to the existing algorithms including HEFT, GA, PSO, and PSO-SA. The result shows that the proposed GAPSO algorithm has a good potential to give the minimum cost when execution time is restricted.
[Algorithm design and analysis, Schedules, particle swarm optimisation, scheduling algorithm, Engines, Genetic algorithms, Workflow Scheduling, Scheduling algorithms, resource allocation, Sociology, workflow engine, scheduling, GA, cloud computing, Hybrid GAPSO, pay-as-you-go pricing model, cost reduction, scientific workflow executions, workflow execution optimization, genetic algorithms, Statistics, PSO, resource usage, total cost minimization, cost optimization, scheduling efficiency evaluation]
Multi-resource Aware Congestion Control in Data Centers
2013 International Conference on Parallel and Distributed Systems
None
2013
Network has been widely reported as a bottleneck of data center applications. However, current researches of congestion control are unaware of multiple resources consuming and decrease all flows when congestion, ignoring some involved flows may not be the faults. In this paper, we propose a novel multi-resources aware congestion control framework MRTCP to provide a fine-grain control on flows when congestions appear. MRTCP exploits a multi-tuple vector model to measure multi-resources provision and consumption, and develops a novel metric RB (Resource Balance) to denote the heterogeneous amounts of resources employed by each flow. It analyzes which resources are being the bottlenecks that lead to congestions, calculates the responsibility of each flow to this congestion, and then adjusts their sending rates respectively. Our experiment results demonstrate that MRTCP is able to optimize network multi-resources utilization and improve network throughput without adding obvious packets delays.
[Measurement, network multiresource utilization, telecommunication congestion control, Data center network, multituple vector model, Random access memory, Graphics processing units, congestion control, Throughput, multiresource provision, network throughput, Vectors, computer centres, multiresource aware congestion control framework MRTCP, fine grain control, data center applications, multiple resources, packet delays, Bandwidth, resource balance, Resource management, multi-resource sharing, MRTCP]
Availability Modeling and Evaluation of Cloud Virtual Data Centers
2013 International Conference on Parallel and Distributed Systems
None
2013
Availability of the service delivered by cloud providers is one of the most important QoS factors of the service level agreements between providers and customers. Since current Infrastructure-as-a-Service providers use virtualization technology to manage data centers, virtual data centers (VDCs) have become a popular infrastructure for cloud computing. In order to study the service availability, a stochastic activity network (SAN) model is presented in this paper. The proposed SAN model can be appropriately used to investigate the impact of different characteristics and policies on service availability of VDCs.
[Availability, service level agreements, virtual reality, cloud comput-ing, Conferences, quality of service, virtualisation, QoS factors, computer centres, infrastructure-as-a-service providers, stochastic activity network, VDC, service delivery, virtualization technology, virtual data center, SAN model, cloud providers, cloud computing, stochastic processes, service availability modeling, cloud virtual data centers]
Automatic Cloud Bursting under FermiCloud
2013 International Conference on Parallel and Distributed Systems
None
2013
Cloud computing is changing the infrastructure upon which scientific computing depends from supercomputers and distributed computing clusters to a more elastic cloud-based structure. The service-oriented focus and elasticity of clouds can not only facilitate technology needs of emerging business but also shorten response time and reduce operational costs of traditional scientific applications. Fermi National Accelerator Laboratory (Fermilab) is currently in the process of building its own private cloud, FermiCloud, which allows the existing grid infrastructure to use dynamically provisioned resources on FermiCloud to accommodate increased but dynamic computation demand from scientists in the domains of High Energy Physics (HEP) and other research areas. Cloud infrastructure also allows to increase a private cloud's resource capacity through "bursting" by borrowing or renting resources from other community or commercial clouds when needed. This paper introduces a joint project on building a cloud federation to support HEP applications between Fermi National Accelerator Laboratory and Korea Institution of Science and Technology Information, with technical contributions from the Illinois Institute of Technology. In particular, this paper presents two recent accomplishments of the joint project: (a) cloud bursting automation and (b) load balancer. Automatic cloud bursting allows computer resources to be dynamically reconfigured to meet users' demands. The load balance algorithm which the cloud bursting depends on decides when and where new resources need to be allocated. Our preliminary prototyping and experiments have shown promising success, yet, they also have opened new challenges to be studied.
[Algorithm design and analysis, Computers, Cloud computing, Laboratories, high energy physics instrumentation computing, high energy physics, Resource Provisioning, operational costs, supercomputers, Korea Institution of Science and Technology Information, resource allocation, private cloud resource capacity, FermiCloud, Illinois Institute of Technology, cloud computing, Load balancing, service-oriented architecture, Automation, scientific computing, load balance algorithm, Virtual machining, cloud federation, Fermi national accelerator laboratory, elastic cloud-based structure, HEP, automatic cloud bursting, distributed computing clusters, Cloud, Load management, Cloud Bursting, service-oriented focus, Fermilab]
Optimizing Event Polling for Network-Intensive Applications: A Case Study on Redis
2013 International Conference on Parallel and Distributed Systems
None
2013
In today's data centers supporting Internet-scale computing and I/O services, increasingly more network-intensive applications are deployed on the network as a service. To this end, it is critical for the applications to quickly retrieve requests from the network and send their responses to the network. To facilitate this network function, operating system usually provides an event notification mechanism so that the applications (or the library) know if the network is ready to supply data for them to read or to receive data for them to write. As a widely used and representative notification mechanism, epoll in Linux provides a scalable and high-performance implementation by allowing applications to specifically indicate which connections and what events on them need to be watched. As epoll has been used in some major systems, including KV systems, such as Redis and Memcached, and web server systems such as NGINX, we have identified a substantial performance issue in its use. For the sake of efficiency, applications usually use epoll's system calls to inform the kernel exactly of what events they are interested in and always keep the information up-to-date. However, in a system with demanding network traffic, such a rigid maintenance of the information is not necessary and the excess number of system calls for this purpose can substantially degrade the system's performance. In this paper, we use Redis as an example to explore the issue. We propose a strategy of informing the kernel of the interest events in a manner adaptive to the current network load, so that the epoll system calls can be reduced and the events can be efficiently delivered. We have implemented the strategy, named as FlexPoll, in Redis without modifying any kernel code. Our evaluation on Redis shows that the query throughput can be improved by up to 46.9% on micro benchmarks, and even up to 67.8% on workloads emulating real-world access patterns. FlexPoll can be extended to other applications and event libraries built on the epoll mechanism in a straightforward manner.
[epoll system calls, I/O services, Subscriptions, data centers, Throughput, network-intensive applications, Servers, Performance evaluation and modeling, Internet-scale computing, epoll mechanism, Operating systems, NGINX, Benchmark testing, network function, Kernel, representative notification mechanism, operating system kernels, operating system, Redis, kernel code, event polling, network traffic, Storage systems and networks, Web server systems, Linux, FlexPoll, Workload characterization, Writing, event notification mechanism, Internet, Memcached]
Benchmarking Large Scale Cloud Computing in Asia Pacific
2013 International Conference on Parallel and Distributed Systems
None
2013
Cloud services allows organizations to run their applications faster, under better manageability and less maintenance so that they can focus on their core business. In this paper we are benchmarking various public clouds (Microsoft Azure, Amazon EC2 and the Australian National eResearch Collaboration Tools and Resources Cloud (NeCTAR)) that are commonly used by academic organizations in Asia Pacific to gauge their performances using variety of tests.
[Cloud computing, large-scale public cloud computing benchmarking, cloud services, Amazon EC2, Computational modeling, organization business, Random access memory, Australian National eResearch Collaboration Tools and Resources Cloud, NeCTAR, Multithreading, academic organizations, Cloud Computing, Asia Pacific, Organizations, Benchmark testing, Microsoft Azure, cloud computing, Local area networks, organisational aspects]
Particle Swarm Optimization-Based Impurity Function Band Prioritization Using Weighted Majority Voting for Feature Extraction of High Dimensional Data Sets
2013 International Conference on Parallel and Distributed Systems
None
2013
In recent years, with the improvement of sensor technologies, the volumes of remote sensing data are increased dramatically. The feature extraction of hyper spectral remotely sensed images can reduce such high-dimensional datasets, solve the big data problem, avoid the Hughes phenomena and improve the classification performance. Accordingly, this paper presents a framework for feature extraction of hyper spectral imagery, which consists of two approaches, referred to as parallel particle swarm optimization (PPSO) band selection and weighted voting impurity function (WVIF) band prioritization. The highly correlated bands of hyper spectral imagery can be grouped first into the some modules by PPSO band selection algorithm to coarsely reduce high-dimensional datasets, and these highly correlated band modules can then be analyzed with the statistical relationship between bands and classes by WVIF band prioritization method to finely select the most important feature bands form the datasets. Furthermore, a PPSO algorithm based on modern graphics processing unit (GPU) architecture using NVIDIA compute unified device architecture (CUDA) technology is using in this paper. It can improve the computational speed of PPSO band selection to group the high correlated band modules. The effectiveness of the proposed PPSO/WVIF framework is evaluated by MASTER and AVIRIS hyper spectral images. The experimental results demonstrated that the proposed method not only could reduction the dimension of datasets, but also can offer a satisfactory classification performance and computational speed.
[MASTER hyperspectral images, parallel architectures, particle swarm optimisation, Graphics processing units, PPSO band selection computational speed, high dimensional datasets, Training, AVIRIS hyperspectral images, feature extraction, highly correlated band modules, NVIDIA compute unified device architecture CUDA technology, Hughes phenomena, PPSO-WVIF framework, Big Data, weighted majority voting, remote sensing, Particle swarm optimization, graphics processing units, particle swarm optimization-based impurity function band prioritization, sensor technologies, particle swarm optimization band selection, big data problem, hyperspectral imagery bands, hyperspectral remotely sensed image feature extraction, weighted voting impurity function band prioritization, Impurities, parallel particle swarm optimization band selection, graphics processing unit, Feature extraction, remote sensing data volumes, graphics processing unit architecture, statistical analysis, hyperspectral imaging, hyperspectral images, Hyperspectral imaging]
Accelerating the Calculation of Scattering of Complex Targets from Background Radiation with CUDA, OpenACC and OpenHMPP
2013 International Conference on Parallel and Distributed Systems
None
2013
Graphics Processing Unit (GPU) is used to accelerate the calculation of scattering of complex target from background radiation in infrared spectrum. Compute Unified Device Architecture (CUDA), OpenACC, and Hybrid Multicore Parallel Programming (OpenHMPP) implementations are presented. In all our implementation, scattering of background radiation in different directions are calculated in parallel. A personal desktop with 2 NVIDIA GTX GeForce 590 with an Intel i7 CPU is used in our experiment. In CUDA, by using shared memory to buffer the background radiation and BRDF parameters and tuning the grid organization, we achieve a speedup of 197x. OpenACC implementation is realized by inserting the parallel loop construct with reduction clause before the loop in original serial code. By utilization of data clause and tuning number of gangs used, a speedup of 158.9x is obtained. In OpenHMPP implementation, the loop iterating over incident direction of original code is transformed to the codelet function and we achieve a speedup of 160.7x. Our effort makes the calculation of complex target in real time possible.
[Instruction sets, parallel architectures, Graphics processing units, Programming, personal desktop, light scattering, NVIDIA GTX GeForce 590, scattering of target, GPU, Optimization, parallel programming, BRDF parameters, background radiation scattering, shared memory systems, OpenACC, codelet function, parallel loop construct, background radiation, grid organization tuning, Atmospheric modeling, Computational modeling, Scattering, compute unified device architecture, graphics processing units, computational electromagnetics, OpenHMPP, CUDA, data clause, infrared spectrum, graphics processing unit, Intel i7 CPU, shared memory, reduction clause, hybrid multicore parallel programming]
Further Improvement on GPU-Based Parallel Implementation of WRF 5-Layer Thermal Diffusion Scheme
2013 International Conference on Parallel and Distributed Systems
None
2013
The Weather Research and Forecasting (WRF) model has been widely employed for weather prediction and atmospheric simulation with dual purposes in forecasting and research. Land-surface models (LSMs) are parts of the WRF model, which is used to provide information of heat and moisture fluxes over land and sea-ice points. The 5-layer thermal diffusion simulation is an LSM based on the MM5 soil temperature model with an energy budget made up of sensible, latent, and radiative heat fluxes. Owing to the feature of no interactions among horizontal grid points, the LSMs are very favorable for massively parallel processing. The study presented in this article demonstrates the parallel computing efforts on the WRF 5-layer thermal diffusion scheme using Graphics Processing Unit (GPU). Since this scheme is only one intermediate module of the entire WRF model, the involvement of the I/O transfer does not occur in the intermediate process. By employing one NVIDIA GTX 680 GPU in the case without I/O transfer, our optimization efforts on the GPU-based 5-layer thermal diffusion scheme can reach a speedup as high as 247.5x with respect to one CPU core, whereas the speedup for one CPU socket with respect to one CPU core is only 3.1x. We can even boost the speedup to 332x with respect to one CPU core when three GPUs are applied.
[weather prediction, NVIDIA GTX 680 GPU, Instruction sets, Graphics processing units, Weather forecasting, Registers, parallel processing, Runtime, weather forecasting, WRF 5-layer thermal diffusion scheme, massively parallel processing, moisture fluxes, land points, Compute Unified Device Architecture (CUDA), parallel computing, weather research and forecasting model, sea-ice points, Atmospheric modeling, Computational modeling, thermal diffusion, 5-layer thermal diffusion simulation, CPU socket, GPU-based parallel implementation, geophysics computing, speedup, CPU core, 5-layer thermal diffusion, graphics processing units, MM5 soil temperature model, atmospheric simulation, Graphics Processing Unit (GPU), radiative heat fluxes, graphics processing unit, land-surface models, Weather Research and Forecasting (WRF)]
Edge Detection for Hyperspectral Images Using the Bhattacharyya Distance
2013 International Conference on Parallel and Distributed Systems
None
2013
In this paper, we proposed a new edge detection algorithm for hyper spectral images using the Bhattacharyya distance. First, the principal component analysis is applied to hyper spectral images and then dominant eigenimages are selected. To compute the Bhattacharyya distance, four block pairs of each pixel are extracted: up-down, left-right, diagonal-left down and diagonal-right-down. From each pair of blocks, we compute the Bhattacharyya distance, which was used as edge information. Experiments show promising results compared to the conventional Sobel filter.
[Wiener filters, diagonal-right-down block pair extraction, Image edge detection, eigenimages, up-down block pair extraction, Sobel filter, hyperspectral, diagonal-left down block pair extraction, edge detection algorithm, Image segmentation, feature extraction, Eigenvalues and eigenfunctions, geometry, edge detection, left-right block pair extraction, hyperspectral imaging, hyperspectral images, Bhattacharyya distance, principal component analysis, Hyperspectral imaging, Principal component analysis]
Message from the general co-chairs IEEE ICPADS 2014
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
It is a great honor to extend our personal greeting to each and every one of you who are attending the 20th IEEE International Conference on Parallel and Distributed Systems (IEEE ICPADS 2014) in the wonderful city of Hsinchu. Since established in 1992, this is the fourth time we have this conference in Hisnchu, Taiwan. ICPADS has been a major international forum to bring together scientists, engineers, and users from all over the world to discuss and exchange the advancement in of parallel and distributed systems technology.
[]
Message from the program co-chairs IEEE ICPADS 2014
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
On behalf of the 20th IEEE International Conference on Parallel and Distributed Systems (ICPADS 2014) Organizing Committee, we are very pleased to announce that more than three hundred researchers and contributors from the world submitted their papers to share their research results and new ideas. The objective of this conference to provide a major international forum for scientists, engineers, and users to exchange and share their experiences, new ideas, and latest research results on all aspects of parallel and distributed computing systems.
[]
Organizing Committee
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Provides a listing of current committee members and society officers.
[]
Technical program committee
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Provides a listing of current committee members and society officers.
[]
Workshop organizers
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Provides a listing of current committee members and society officers. The conference also offers a note of thanks and lists its reviewers.
[]
Keynote speech
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
These keynote discusses the following: IoT and Interactive Design; TSUBAME3.0 towards 4.0 and Toward Convergence of Extreme Computing and Big Data; Personalized Smart Health and Big Data Cyber Infrastructure.
[interactive design, personalized smart health, parallel system, big data cyber infrastructure, Big Data, interactive systems, distributed system, TSUBAME3.0, Internet of Things, parallel processing, health care, IoT]
PADS: Passive detection of moving targets with dynamic speed using PHY layer information
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Device-free passive detection is an emerging technology to detect whether there exists any moving entities in the area of interests without attaching any device to them. It is an essential primitive for a broad range of applications including intrusion detection for safety precautions, patient monitoring in hospitals, child and elder care at home, etc. Despite of the prevalent signal feature Received Signal Strength (RSS), most robust and reliable solutions resort to finer-grained channel descriptor at physical layer, e.g., the Channel State Information (CSI) in the 802.11n standard. Among a large body of emerging techniques, however, few of them have explored full potentials of CSI for human detection. Moreover, space diversity supported by nowadays popular multi-antenna systems are not investigated to the comparable extent as frequency diversity. In this paper, we propose a novel scheme for device-free PAssive Detection of moving humans with dynamic Speed (PADS). Both amplitude and phase information of CSI are extracted and shaped into sensitive metrics for target detection; and CSI across multi-antennas in MIMO systems are further exploited to improve the detection accuracy and robustness. We prototype PADS on commercial WiFi devices and experiment results in different scenarios demonstrate that PADS achieves great performance improvement in spite of dynamic human movements.
[Phase measurement, channel state information, object detection, 802.11n standard, dynamic speed, Covariance matrices, PHY layer information, passive moving target detection, Accuracy, antenna arrays, received signal strength, CSI, Eigenvalues and eigenfunctions, Robustness, MIMO communication, WiFi devices, PADS, human detection, Support vector machines, MIMO systems, device-free passive detection, multiantenna systems, RSS, Feature extraction, dynamic human movements, wireless LAN]
A distributed approach for top-k star queries on massive information networks
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Massive information networks, such as the knowledge graph by Google, contain billions of labeled entities. Star queries, which aim to identify an entity, given a set of related entities, are common on such networks. Answering star queries can be modeled as a graph pattern matching problem. Traditional approaches apply graph indices to accelerate the query processing. Unfortunately, it is so costly that it is nearly infeasible to build indices on billion node graphs since the time or storage complexity of most indexing techniques is super-linear to the graph size. In this paper, we propose an algorithm to identify the top-k best answers for a star query. Instead of using expensive indices, our algorithm utilizes novel bounding techniques to derive the top-k best answers efficiently. Further, the algorithm can be implemented in a distributed manner scaling to billions of entities and hundreds of machines. We demonstrate the effectiveness and the efficiency of our approach through a series of experiments on real-world information networks.
[Distributed System, Google, top-k star query, bounding technique, Big Data, Star Query, information networks, indexing techniques, query processing, distributed approach, Top-k Query, Upper bound, Query processing, distributed algorithms, massive information networks, star query answering, Clustering algorithms, Motion pictures, Acceleration, Information Network, Pattern matching, Indexing, Billion-Node Graph]
Hierarchical prediction based task scheduling in hybrid data center
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Cloud computing can help data center consolidate batch and gratis tasks with over-provisioned production applications, and fulfill their diverse resource demands and performance objectives with high scalability and flexibility. One challenge in this hybrid data center is that the dramatic fluctuation of batch and gratis workload may impact performance of production applications, cause task failure, decrease efficiency, and waste computing resources. One way to tackle the challenge is to reduce resource allocation to prevent host overload by delay scheduling tasks if resources are predicted in short. In this paper, we propose hierarchical prediction method for hybrid workload. We use last-state based ARMA model to predict stationary process of production workload, and use feedback based online AR model to predict the vibrated workload of batch and gratis tasks. Evaluation shows that the hierarchical prediction based task scheduling can reduce host overload by more than 85 percent, reduce tasks evicted and killed by more than 60 percent, and reduce 40 percent of average task scheduling delay.
[batch tasks, autoregressive moving average processes, Predictive models, hybrid data center, hierarchical prediction, processor scheduling, Training, resource allocation, last-state based ARMA model, hierarchical prediction method, cloud computing, hierarchical prediction based task scheduling, batch workload, Computational modeling, production applications, gratis workload, gratis tasks, computer centres, production workload, vibrated workload prediction, resource demands, feedback based online AR model, stationary process prediction, task scheduling, autoregressive moving average model, hybrid workload]
Optimization of serial and parallel communications for parallel geometric multigrid method
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
The parallel multigrid method is expected to play an important role in large-scale scientific computing on post-peta/exa-scale supercomputer systems, and it also includes serial and parallel communication processes which are generally expensive. In the present work, new format for sparse matrix storage based on sliced Ellpack-Itpack (ELL) format is proposed for optimization of serial communication in data transfer through memories, and hierarchical coarse grid aggregation (hCGA) is introduced for optimization of parallel communication by message passing. The proposed methods are implemented for pGW3D-FVM, a parallel code for 3D groundwater flow simulations using the multigrid method, and the robustness and performance of the code was evaluated on up to 4,096 nodes (65,536 cores) of the Fujistu FX10 supercomputer system at the University of Tokyo. The parallel multigrid solver using the sliced ELL format provided performance improvement in both weak scaling (25%-31%) and strong scaling (9%-22%) compared to the code using the original ELL format. Moreover, hCGA provided excellent performance improvement in both weak scaling (1.61 times) and strong scaling (6.27 times) for flat MPI parallel programming model.
[flow simulation, application program interfaces, hCGA, University of Tokyo, parallel multigrid solver, Switches, iterative solvers, hierarchical coarse grid aggregation, Sparse matrices, performance improvement, Optimization, parallel programming, multicore, Convergence, postexa-scale supercomputer systems, pGW3D-FVM parallel code, parallel geometric multigrid method, strong scaling, weak scaling, Robustness, sparse matrix storage, communication, parallel computing, serial communication optimization, message passing, Multigrid methods, computational fluid dynamics, sliced ELL format, 3D groundwater flow simulations, parallel communication optimization, multigrid, Supercomputers, groundwater, matrix storage format, postpeta scale supercomputer systems, sliced Ellpack-Itpack format, flat MPI parallel programming model, Fujistu FX10 supercomputer system, large-scale scientific computing, data transfer, sparse matrices]
Topology shaping for time synchronization in wireless sensor networks
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Time synchronization plays an important role in wireless sensor networks (WSNs). Due to the unique features of WSNs such as remote deployment, low-cost hardware, and restrictive energy supply, accurate and robust time synchronization is still a challenging task. Many approaches have been proposed to improve the performance and efficiency of time synchronization. Existing schemes, however, do not pay enough attention to the impacts of varying network topology properties, including network diameter, degree distribution, and the like. They can experience unexpected performance degradation in many real systems. To address these issues, we present a novel approach, which tries to improve the performance of existing time synchronization algorithms by introducing a virtual overlay. We propose an integrated model, called topo-refiner, which encodes the impact of different network topology features to the precision and robustness of time synchronization. Also, we design a novel optimization algorithm to build a virtual overlay from the underlying communication network. Our approach is orthogonal to existing clock synchronization methods, and can improve their performance by operating these methods atop the virtual layer. Finally, in order to verify the effectiveness of our approach, we conduct extensive simulations on synthetic and real system traces, as well as testbed experiments. Results show that topo-refiner is practical, and quickly adapts to varying time synchronization accuracy requirements for applications in WSNs.
[Algorithm design and analysis, topo-refiner integrated model, Laplace equations, network diameter, wireless sensor networks, time synchronization, communication network, clock synchronization, topology shaping, telecommunication network topology, Topology, Synchronization, Optimization, synchronisation, Wireless sensor networks, Network topology, WSN, varying network topology properties, degree distribution, virtual overlay]
Interference-aware channel allocation algorithm with game theoretic approach for cognitive radio networks
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
In cognitive radio networks, the Secondary Users (SUs) are allowed to utilize unused portions of the licensed spectrum to enhance the performance and efficiency of the channel resource in networks. However, with the increasing number of wireless devices and users competence for the limited channel resource, the channel allocation problem has become an important issue in the cognitive radio networks. Moreover, the interference in communication has become an important factor that affects the efficiency of transmission in cognitive radio networks. In this paper, we proposed an algorithm with game theoretic approach to solve the problem of channel allocations in cognitive radio networks, based on interference aware information between communication pairs. By game theory, the channel allocations of SUs are proposed by their perceived utilities associated with possible actions of neighboring users. The effectiveness of the communication links is determined by the bandwidth of the available channels to the SUs and the heterogeneous interference range between the communication links. Our proposed algorithm can achieve Nash Equilibrium convergence in the game theory.
[licensed spectrum, Nash Equilibrium, game theory, co-channel interference, wireless device, Cognitive radio, channel resource, Game theory, interference (signal), secondary user, Nash equilibrium convergence, cognitive radio, Games, game theoretic approach, Channel allocation, interference-aware channel allocation algorithm, Resource management, channel allocation, Interchannel interference, cognitive radio network, Cognitive radio networks]
F-Loc: Floor localization via crowdsourcing
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Traditional fingerprint based localization techniques mainly rely on infrastructure support such as GSM, Wi-Fi or GPS. They work by war-driving the entire indoor spaces which is both time-consuming and labor-intensive. With recent advances of smartphone and sensing technologies, sensor-assisted localization techniques leveraging on mobile phone sensing are emerging. However, sensors are inherently noisy, making this technique challenging for real deployment. In this paper, we present F-Loc, a novel floor localization system to identify the floor level in a multi-floor building on which a mobile user is located. It does not need to war-drive the entire building. Leveraging on crowdsourcing and mobile phone sensing, we collect users' Wi-Fi traces and accelerometer readings. Through advanced clustering and cluster manipulating techniques, we are able to build the Wi-Fi map of the entire building, which can then be used for floor localization. We conduct both simulation and field studies to demonstrate the accuracy, scalability, and robustness of F-Loc. Our field study in a 10-floor building shows that F-Loc achieves an accuracy of over 98%.
[IEEE 802.11 Standards, crowdsourcing, Wi-Fi, sensing technology, floor localization, Mobile communication, Elevators, smart phones, multifloor building, floor localization system, fingerprint based localization technique, F-Loc system, smart phone technology, Mobile Phone Localization, Floor Localization, Clustering algorithms, Wireless Fidelity, Accelerometer, sensor-assisted localization techniques, Wi-Fi map, Sensors, Acceleration, wireless LAN]
Energy-efficient mobile data collection in energy-harvesting wireless sensor networks
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Environmental energy harvesting technologies have provided potential for battery-powered wireless sensor networks to have perpetual network operations. To design a robust network that can adapt to not only temporal but also spatial variations of ambient energy sources, in this paper, we utilize mobility to circumvent communication bottlenecks, by employing a mobile data collector, called SenCar. We propose a two-stage approach for mobile data collection. In the first stage, SenCar makes stops at a subset of selected sensor locations to collect data packets in a multi-hop fashion. We provide a selection algorithm to search for sensor locations with most residual energy while guaranteeing a bounded tour length. Then we design a distributed data gathering algorithm to achieve maximum network utility by adjusting data rates, link scheduling and flow routing that adapts to spatial temporal environmental energy variations. The effectiveness and efficiency of the proposed algorithms are validated by extensive numerical results.
[radio links, energy-efficient mobile data collection, link scheduling, wireless sensor networks, Mobile communication, telecommunication scheduling, two-stage approach, mobile data gathering, mobile data collector, Batteries, Energy harvesting, Optimization, spatial temporal environmental energy variation, environmental energy-harvesting wireless sensor network, bounded tour length, SenCar, mobile radio, distributed data gathering algorithm, energy harvesting, Routing, battery-powered wireless sensor network, Wireless sensor networks, flow routing, distributed algorithms, telecommunication network routing, energy source, Data collection, energy conservation, convex optimization, Mobile computing]
fAHRW+: Fairness-aware and locality-enhanced scheduling for multi-server systems
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
This paper discusses scheduling issues of multi-server systems. There are three desirable properties of multi-server scheduling: load balancing, fairness and locality. The three properties are often conflicting with each other. There is no scheduling scheme that possesses all the three properties. In this paper, we first propose the fairness-aware highest random weight (fHRW) scheduling algorithm as an attempt to achieve fairness and locality. fHRWtries to service packets proportionally according to priorities of flows and schedule the packets from the same flow onto the same server. Then, we solve the imbalanced load issue of fHRW by improving the hash function HRW to an adaptive HRW (AHRW). fAHRW is more efficient (in terms of load balancing) and fair than fHRW, but it still may suffer unfairness in some cases. We further enhance fAHRW to fAHRW+ by proposing a new hash function AHRW+ that considers fairness as well as locality. Extensive simulations have been carried out to evaluate the performance of fAHRW+. The results show that fAHRW+ can provide good load balancing, locality and fairness.
[fairness, Schedules, load balancing, locality, multiserver system, Scheduling, Servers, locality-enhanced scheduling, Global Positioning System, Scheduling algorithms, resource allocation, hash function, fHRW scheduling algorithm, scheduling, fairness-aware highest random weight, Load management, fAHRW+]
esDMT: Efficient and scalable deterministic multithreading through memory isolation
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Deterministic multithreading (DMT) system is well-known to eliminate the harmful program behaviors caused by nondeterminism, i.e., always proceeding the program execution into the same thread schedule for the same given input. To achieve this goal, two kinds of schedules are enforced by existing DMT systems. 1) A mem-based schedule ensures the determinism with the total order of the shared memory accesses, and 2) A sync-based schedule makes it by only enforcing the total order of the synchronization operations. Mem-schedule achieves full determinism but suffers from prohibitive overhead; while sync-schedule mitigates this overhead but cannot ensure the determinism for the race schedules, i.e., part determinism. Much recent research is devoted to the hybrid schedule combining the determinism of mem-schedule and efficiency of sync-schedule. However, they suffer from the practicability and scalability problems due to the defects of their technical characteristics, such as trace collection in advance and huge schedule memoization. To address the above problem, this paper proposes esDMT, an efficient and scalable DMT system using a new technique of memory isolation. It can improve the efficiency by proceeding the execution of each thread in parallel within its private virtual memory, and defers the determinism guarantee by updating private memory into shared memory in a deterministic order according to deterministic lock algorithm, thus further reducing the overhead of inter-thread waiting. In contrast to the previous hybrid work avoiding the nondeterminism of race schedules offline based on the enormous historical records, our key insight is to eliminate the nondeterminism of race schedules online at runtime. Our experimental results on PARSEC benchmarks show that esDMT eliminates the nondeterminism successfully, almost gains the same performance as the sync-schedule (with &lt;;18% slowdown compared with pthread library at most), and manifests good scalability on an 8-core machine.
[Schedules, esDMT system, inter-thread waiting, Instruction sets, Scalability, Nondeterminism, harmful program behaviors, private virtual memory, processor scheduling, Runtime, mem-based schedule, deterministic lock algorithm, shared memory systems, data race, determinism guarantee, memory isolation, race schedules, nondeterminism, shared memory accesses, multi-threading, synchronization operations, virtual storage, scalable deterministic multithreading, PARSEC benchmarks, program execution, Computer crashes, Synchronization, thread schedule, synchronisation, memory commit, deterministic order, deterministic multithreading, sync-based schedule, Clocks]
Traffic-aware frequency scaling for balanced on-chip networks on GPGPUs
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
General-purpose computing on graphics processing units (GPGPU) can provide orders of magnitude more computing power than general purpose processors (CPU) for highly parallel applications. For such parallel applications, the memory traffic pattern of GPGPUs behaves considerably different from that of CPUs. This gives rise to opportunities for optimizing the on-chip interconnection network (NoC) of GPGPUs. In this work, we first investigate the characteristics of GPGPU memory traffic of typical benchmarks and categorize the memory traffic patterns. Different traffic patterns require different throughput in the request and reply paths of the NoC to match the network load. To meet this requirement, we examine the feasibility of scaling the network frequency dynamically to balance the throughput of the request and reply networks. The decision is guided by monitoring some shader cores to identify the memory traffic pattern. Performance evaluation shows that this dynamic frequency tuning design can achieve up to 27% improvement in terms of execution speedup compared to a baseline setting and 7.4% improvement on average.
[execution speedup, NoC, network load, Heuristic algorithms, CPU, request networks, reply networks, general purpose processors, balanced on-chip networks, GPGPU, traffic-aware frequency scaling, Bandwidth, Benchmark testing, shader cores, Monitoring, on-chip interconnection network, network-on-chip, performance evaluation, highly parallel applications, graphics processing units, Tuning, integrated circuit interconnections, memory traffic patterns, general-purpose computing, network frequency, dynamic frequency tuning design, Pattern matching]
An improved simulated annealing heuristic for static partitioning of task graphs onto heterogeneous architectures
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
We present a simulated annealing based partitioning technique for mapping task graphs, onto heterogeneous processing architectures. Task partitioning onto homogeneous architectures to minimize the makespan of a task graph, is a known NP-hard problem. Heterogeneity greatly complicates the aforementioned partitioning problem, thus making heuristic solutions essential. A number of heuristic approaches have been proposed, some using simulated annealing. We propose a simulated annealing method with a novel NEXT STATE function to enable exploration of different regions of the global search space when the annealing temperature is high and making the search more local as the temperature drops. The novelty of our approach is two fold: (1) we go a step further than the existing scientific literature, considering heterogeneity at levels of task parallelism, data parallelism and communication. (2) We present a novel algorithm that uses simulated annealing to find better partitions in the presence of heterogeneous architectures, data parallel execution units, and significant data communication costs. We conduct a statistical analysis of the performance of the proposed method, which shows that our approach clearly outperforms the existing simulated annealing method.
[Temperature distribution, simulated annealing, Heuristic algorithms, task graph partitioning, heterogeneous processing architecture, Linear programming, task graph mapping, static partitioning, NEXT STATE function, parallel processing, Standards, task parallelism, makespan minimization, NP-hard problem, simulated annealing heuristic, data parallelism, Simulated annealing, Computer architecture, Parallel processing, data communication cost, statistical analysis, data parallel execution unit, computational complexity, communication level]
Scaling and analyzing the stencil performance on multi-core and many-core architectures
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Stencils are among the most important and time-consuming kernels in many applications. While stencil optimization has been a well-studied topic on CPU platforms, achieving higher performance and efficiency for the evolving numerical stencils on the more recent multi-core and many-core architectures is still an important issue. In this paper, we explore a number of different stencils, ranging from a basic 7-point Jacobi stencil to more complex high-order stencils used in finer numerical simulations. By optimizing and analyzing those stencils on the latest multi-core and many-core architectures (the Intel Sandy Bridge processor, the Intel Xeon Phi coprocessor, and the NVIDIA Fermi C2070 and Kepler K20x GPUs), we investigate the algorithmic and architectural factors that determine the performance and efficiency of the resulting designs. While multi-threading, vectorization, and optimization on cache and other fast buffers are still the most important techniques that provide performance, we observe that the different memory hierarchy and the different mechanism for issuing and executing parallel instructions lead to the different performance behaviors on CPU, MIC and GPU. With vector-like processing units becoming the major provider of computing power on almost all architectures, the compiler's inability to align all the computing and memory operations would become the major bottleneck from getting a high efficiency on current and future platforms. Our specific optimization of the complex WNAD stencil on GPU provides a good example of what the compiler could do to help.
[Multi-core architecture, Instruction sets, NVIDIA Fermi C2070 GPU, Graphics processing units, Stencil, Registers, program compilers, Optimization, Intel Xeon Phi coprocessor, Microwave integrated circuits, vectorization, Kepler K20x GPU, optimization, Computer architecture, seven-point Jacobi stencil, multithreading, compiler, Kernel, numerical simulation, Many-core architecture, multiprocessing systems, multicore architecture, many-core architecture, stencil performance, graphics processing units, Intel Sandy Bridge processor, graphics processing unit, WNAD stencil, Optimizations]
Directory Lookaside Table: Enabling scalable, low-conflict, many-core cache coherence directory
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Maintaining hardware cache coherence on future CMPs becomes increasingly important and difficult as the number of cores keeps accelerating in mainstream multicore chips. The simple snooping-bus coherence scheme is not suitable due to its limited scalability. The sparse coherence directory approach may incur extra cache invalidations due to a topological mismatch between the coherence directory and the directories of all cache modules. In this paper, we propose an innovative CMP coherence directory that has three important properties. First, the directory has a simple set-associative design with small associativity. The number of directory entries matches the total number of cache blocks. Second, an augmented Directory Lookaside Table (DLT) allows blocks to be displaced from their primary sets in the coherence directory for alleviating hot-set conflicts. Third, to avoid expensive presence bits, each copy of a block along with the located core ID occupies a separate directory entry. Performance evaluations based on multithreaded and multi-programmed workloads demonstrate significant advantages of the proposed CMP directory over directories with traditional set-associative or skewed associative designs.
[Performance evaluation, Protocols, cache storage, Sparse directory, DLT, multithreaded workload, hardware cache coherence, Cache coherence, skewed associative design, Directory-based protocol, directory lookaside table, set-associative design, many-core cache coherence directory, multiprocessing systems, Multicore processing, multi-threading, CMP, Snooping-bus protocol, CMP coherence directory, Indexes, Coherence, Organizations, multicore chip, Arrays, multiprogrammed workload]
dIRIEr: Distributed Influence Maximization in social network
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
In this paper, for the first time, we tackle the scalability problem of Influence Maximization (IM) via distributed computing. First, we propose a distributed IM algorithm based on IRIE, one of the most state-of-the-art IM algorithms. Then an incremental updating method is proposed to reduce the overhead of repeated computation. Furthermore, based on some new insights, we redesign our algorithm with a strategy, which we call reservoir, to accumulate increments and delay exchange between machines. Experiments on real-world and synthetic networks show our redesigned algorithm, i.e. dIRIEr (distributed IRIE with Reservoir), reduces communication traffic dramatically and speeds up continuously as more machines are added in. dIRIEr can handle giant networks with hundreds of millions of nodes where centralized algorithms become infeasible.
[IM scalability problem, Social network services, Computational modeling, distributed IRIE with reservoir algorithm, Synchronization, social network, distributed algorithm, distributed IM algorithm, dIRIEr algorithm, directed graphs, distributed algorithms, IRIE algorithm, parallel, incremental updating method, Reservoirs, social networking (online), influence maximization, Delays, Mirrors, Integrated circuit modeling, distributed influence maximization]
EasiMG: A method of maximizing lifetime for group request in web-based sensor network
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
The lightweight RESTful protocols, such as CoAP and SeaHttp, have been proposed for web-based sensor network (WBSN), which provides web service on resource-constrained devices. In general, because sensing data are spatially correlated in sensor network, it is efficient to request a group of devices located in nearby area. Thus group requesting is a typical way to provide web service for resource-constrained devices in WBSN. However, it is a critical problem that how to make an optimal assignment of nodes for a group of requests to maximize network lifetime. In this paper, we address this problem in the scenario where nodes have different initial energy, and they can process in-network group request with branch and combine methods supported by SeaHttp. We prove this problem is NP-complete and transform the problem into an edge-weighted semi-matching problem in bipartite graph using the fat tree construction algorithm. Finally we propose an approximation algorithm to solve the problem. Simulation results show that our approach prolong lifetime of the network by 29.11% on average, which is more competitive when it is applied in a high concurrency scenario compared with traditional methods.
[Protocols, wireless sensor networks, graph theory, network lifetime maximization, semimatching problem, Approximation methods, web service, WBSN, lightweight RESTful protocol, optimisation, approximation algorithm, Bipartite graph, CoAP, in-network group request, approximation theory, resource-constrained device, EasiMG, Routing, NP-complete problem, bipartite graph, SeaHttp, fat tree construction algorithm, Web services, Vegetation, Approximation algorithms, web-based sensor network, Internet, computational complexity]
Intactness verification in anonymous RFID systems
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Radio-Frequency Identification (RFID) technology has fostered many object monitoring systems. Along with this trend, tagged objects' value and privacy become a primary concern. A corresponding important problem is to verify the intactness of a set of tagged objects without leaking tag identifiers (IDs). However, existing solutions necessitate the knowledge of tag IDs. Without tag IDs as a priori, this paper studies intactness verification in anonymous RFID systems. We identify three critical solution requirements, that is, deterministic verification, anonymity preservation, and scalability. We propose Cardiff and Divar, two crypto-free, lightweight protocols that isolate tag IDs from intactness verification and satisfy solution requirements. Cardiff explores tag cardinality as intactness proof while Divar leverages Direct-Sequence Spread Spectrum (DSSS) enabled RFID. Both analytical and simulation results demonstrate that Cardiff and Divar can satisfy the requirements of accuracy, privacy, and scalability.
[Protocols, radiofrequency identification, Scalability, spread spectrum communication, Servers, Security, deterministic verification, scalability, DSSS, cryptofree lightweight protocol, Privacy, tag identifier, Spread spectrum communication, telecommunication network reliability, tag ID, Intactness verification, object monitoring system, Monitoring, Missing-tag detection, code division multiple access, intactness verification, radiofrequency identification technology, direct-sequence spread spectrum, Anonymous RFID system, Radiofrequency identification, anonymous RFID system]
Leverage similarity and locality to enhance fingerprint prefetching of data deduplication
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Data deduplication has been widely used at data backup system due to the significantly reduced requirements of storage capacity and network bandwidth. However, the performance of data deduplication gradually decreases with the growth of deduplicated data. This is because the volume of fingerprints grows significantly with the increase of backup data, and a large portion of fingerprints have to be stored on disk drives. This incurs frequent disk accesses to locate fingerprints and blocks the process of data deduplication. Furthermore, the fingerprints belonging to the same file may be discretely stored on disk drives. This generates random and small disk accesses, and results in significant performance degradation when the fingerprints are referred. Additionally, a single fingerprint may appear only once during a backup process. This results in very low cache hit ratio due to lacking temporal locality. This paper proposes to employ file similarity to enhance the fingerprint prefetching, thus improving the cache hit ratio and the performance of data deduplication. Furthermore, the fingerprints are arranged sequently in terms of the backup data stream to maintain the locality and promote the performance. Experimental results demonstrate that the proposed idea can effectively reduce the number of fingerprint accesses going to disk drives, decrease the query overhead of fingerprints, thus significantly alleviating the disk bottleneck of data deduplication.
[fingerprint prefetching, data deduplication, Fingerprint recognition, locality, network bandwidth, cache hit ratio, Radio access networks, data backup system, Data deduplication, storage management, disk drive, similarity, disk access, storage capacity, disk bottleneck, data handling, Digital audio players]
Providing hybrid block storage for virtual machines using object-based storage
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
This paper presents the design, implementation, and evaluation of a multi-tiered storage system called MOBBS, which provides hybrid block storage for Virtual Machines (VMs) on top of object-based storage infrastructure. MOBBS is mainly motivated by the gap between the lack of studies on hybrid block storage for VMs and the increasing prevalence of hybrid storage systems. By stripping disk images into partitions and intelligently storing them on different storage tiers according to real-time workload patterns, MOBBS achieves efficient use of multiple storage devices and relieves the burden of data placement. Leveraging the benefits of object-based storage, MOBBS is able to dynamically perform non-disruptive and fine-grained data migration between storage tiers and distribute the complexity of data migration across entire storage nodes. Such designs enable our system to deliver storage for VMs with high scalability and availability under an efficient use of SSDs. We evaluated a Ceph implementation of MOBBS using both block and file system workloads. The results comprehensively demonstrate MOBBS's effectiveness in performance improvement as well as efficient utilization of different storage devices.
[Performance evaluation, multitiered storage system, Scalability, Throughput, virtualisation, Servers, storage device utilization, performance improvement, MOBBS, storage management, data placement, nondisruptive-fine-grained data migration, cloud computing, Availability, real-time workload patterns, Ceph implementation, SSD, hybrid block storage, object-based storage infrastructure, storage tiers, disk image stripping, Virtual machining, file system workload, block system workload, Layout, virtual machines, VM, storage nodes]
Reducing lock contention on multi-core platforms
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
As multi-core platforms with hundreds or even more quantities of cores are popular, system optimization issues, including lock contentions, start to puzzle programmers who work on multi-core platforms. Locks are more convenient and clear than lock-free operations (for example, transactional memory) for multi-core programmers. However, lock contention has been recognized as a typical impediment to the performance of shared-memory parallel programs. This paper mainly discusses two important reasons that cause lock contention, including large critical sections and frequent lock requests. For current solutions, it is hard for programmers to find the locations of large critical sections and good scheme to reduce lock contentions on hot critical sections. This paper proposes FFlocker, a series of runtime solutions that reduce lock contention caused by the two issues. FFlocker includes a profiling algorithm to find the locations of large critical sections. Based on the profiling scheme, it binds the threads acquiring the same locks onto the same core. We evaluate our techniques with three benchmarks. The results show that FFlocker offers better performance than Function Flow and OpenMP.
[frequent lock requests, multiprocessing systems, Multicore processing, Instruction sets, program diagnostics, FFlocker, OpenMP, lock contention, Switches, large critical sections, runtime, Function Flow, parallel programming, runtime solutions, lock contention reduction, Runtime, Parallel programming, Multi-core, multicore platforms, profiling algorithm, Libraries, Time complexity]
NICE: Network-aware VM Consolidation scheme for Energy Conservation in Data Centers
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Energy conservation and network performance have become two of the most important issues in data center as the scale of cloud services continues growing. Recent researches usually consider these two issues separately. Energy conservation mainly deals with hosts, which reduces total energy consumption by consolidating virtual machine(VM)s to fewer hosts, and network performance mainly deals with network scalability and energy efficiency, which improves data center network(DCN) scalability by applying new network topologies or routing schemes and improves DCN energy efficiency by consolidating trafile. In this paper, we jointly consider these two issues and define Combined VM Consolidation (CVC) problem. We prove that CVC is NP-complete and is inapproximable by a factor of 3/2 &#x03B5; unless P = NP. Next, we propose NICE: Network-aware VM Consolidation scheme for Energy Conservation in Data CEnter to solve CVC. Instead of taking the unrealistic hypothesis that migration cost is negligible, a common assumption in most literatures, we precisely analyze VM migration cost according to real-trace experiments in a 6-server testbed via VMware. Massive simulations validate the efficiency of NICE, In all, to the best of our knowledge, we arc the first work to combine VM consolidation with network optimization and migration cost.
[combined VM consolidation, network-aware VM consolidation scheme for energy conservation in data centers, network optimization, NP-complete problem, computer centres, energy saving, DCN, VM migration cost, CVC, optimisation, power aware computing, Nonvolatile memory, virtual machine, data center, cloud service, migration cost, virtual machines, NICE, Silicon, VM consolidation, cloud computing, computational complexity, data center network]
GRapid: A compilation and runtime framework for rapid prototyping of graph applications on many-core processors
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Many applications use graphs to represent and analyze data, but the effective deployment of graph algorithms on many-core processors is still a challenge. Many-core devices offer higher peak performance than multi-core devices; however, many-core programming is still a specialized skill. While compilation and runtime frameworks for parallelizing graph applications on multi-core CPUs exist, there is still a need for comparable frameworks for many-core devices. We propose GRapid: a compilation and runtime framework that generates efficient parallel implementations of generic graph applications for multi-core CPUs, NVIDIA GPUs and Intel Xeon Phi. Applications are expressed using a platform-agnostic programming API. Our source-to-source compiler performs platform-specific code transformations and optimizations, handles data transfers between the host and the coprocessor, and generates several functionally-equivalent code variants for the target platform. Our runtime library provides an efficient dynamic memory management scheme for applications where the graph topology is modified during processing. We used the proposed framework to rapidly generate efficient implementations of four graph applications on different target processors. Such rapid prototyping and re-targeting capability has obvious programmability advantages, but it can also be used to quickly identify target processors that better match the application's computational needs.
[Graph applications, multiprocessing systems, source-to-source compiler, application program interfaces, software prototyping, rapid prototyping, generic graph applications, re-targeting capability, Containers, Programming, parallel graph processing, Data structures, Synchronization, GRapid, parallel processing, program compilers, runtime library, compilation framework, Runtime, platform-agnostic programming API, source-to-source transformation, Memory management, Parallel processing, many-core processors]
Rhymes: A shared virtual memory system for non-coherent tiled many-core architectures
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
The rising core count per processor is pushing chip complexity to a level that hardware-based cache coherency protocols become too hard and costly to scale someday. We need new designs of many-core hardware and software other than traditional technologies to keep up with the ever-increasing scalability demands. A cluster-on-chip architecture, as exemplified by the Intel Single-chip Cloud Computer (SCC), promotes a software-oriented approach instead of hardware support to implementing shared memory coherence. This paper presents a shared virtual memory (SVM) system, dubbed Rhymes, tailored to new processor kinds of non-coherent and hybrid memory architectures. Rhymes features a two-way cache coherence protocol to enforce release consistency for pages allocated in shared physical memory (SPM) and scope consistency for pages in percore private memory. It also supports page remapping on a percore basis to boost data locality. We implement and test Rhymes on the SCC port of the Barrelfish OS. Experimental results show that our SVM outperforms the pure SPM approach used by Intel's software managed coherence (SMC) library by up to 12 times through improved cache utilization for applications with strong data reuse patterns.
[Protocols, software-oriented approach, Random access memory, shared virtual memory system, Intel single-chip cloud computer, Software managed coherence, Cache coherence, hardware-based cache coherency protocol, shared memory systems, Libraries, System-on-chip, cluster-on-chip architecture, shared memory coherence, multiprocessing systems, Barrelfish OS, operating systems, SVM system, Non-coherent many-core architectures, Support vector machines, SCC, Coherence, SMC library, operating systems (computers), Software, Rhymes system, noncoherent tiled many-core architecture]
Heterogeneous CPU-GPU computing for the finite volume method on 3D unstructured meshes
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
A recent trend in modern high-performance computing environments is the introduction of accelerators such as GPU and Xeon Phi, i.e. specialized computing devices that are optimized for highly parallel applications and coexist with CPUs. In regular compute-intensive applications with predictable data access patterns, these devices often outperform traditional CPUs by far and thus relegate them to pure control functions instead of computations. For irregular applications however, the gap in relative performance can be much smaller, and sometimes even reversed. Thus, maximizing overall performance in such systems requires that full use of all available computational resources is made. In this paper we study the attainable performance of the cell-centered finite volume method on 3D unstructured tetrahedral meshes using heterogeneous systems consisting of CPUs and multiple GPUs. Finite volume methods are widely used numerical strategies for solving partial differential equations. The advantages of using finite volumes include built-in support for conservation laws and suitability for unstructured meshes. Our focus lies in demonstrating how a workload distribution that maximizes overall performance can be derived from the actual performance attained by the different computing devices in the heterogeneous environment. We also highlight the dual role of partitioning software in reordering and partitioning the input mesh, thus giving rise to a new combined approach to partitioning.
[Multicore processing, Instruction sets, Particle separators, mathematics computing, Graphics processing units, mesh partitioning, mesh generation, central processing unit, partial differential equation, mesh reordering, finite volume methods, graphics processing units, parallel processing, 3D unstructured tetrahedral mesh, Three-dimensional displays, high-performance computing environment, cell-centered finite volume method, data access pattern, Bandwidth, heterogeneous CPU-GPU computing, heterogeneous system, graphics processing unit, Hardware, partitioning software]
Peak Power Management for scheduling real-time tasks on heterogeneous many-core systems
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
The number and diversity of cores in on-chip systems is increasing rapidly. However, due to the Thermal Design Power (TDP) constraint, it is not possible to continuously operate all cores at the same time. Exceeding the TDP constraint may activate the Dynamic Thermal Management (DTM) to ensure thermal stability. Such hardware based closed-loop safeguards pose a big challenge in using many-core chips for real-time tasks. Managing the worst-case peak power usage of a chip can help toward resolving this issue. We present a scheme to minimize the peak power usage for frame-based and periodic real-time tasks on many-core processors by scheduling the sleep cycles for each active core and introduce the concept of a sufficient test for peak power consumption for task feasibility. We consider both inter-task and inter-core diversity in terms of power usage and present computationally efficient algorithms for peak power minimization for these cases, i.e., a special case of &#x201C;homogeneous tasks on homogeneous cores&#x201D; to the general case of &#x201C;heterogeneous tasks on heterogeneous cores&#x201D;. We evaluate our solution through extensive simulations using the 48-core SCC platform and gem5 architecture simulator. Our simulation results show the efficacy of our scheme.
[Schedules, dynamic thermal management, on-chip systems, Real-time, many-core, TDP constraint, task analysis, power aware computing, thermal design power, scheduling, Real-time systems, closed-loop safeguards, Peak power managment, Power demand, Multicore processing, multiprocessing programs, Minimization, thermal stability, Partitioning algorithms, peak power management, real-time tasks, real-time systems, Thermal management, heterogeneous many-core systems, DTM]
ADAS: Adjust directional antenna with sensor hints
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Recently, directional antenna has shown great potential to deploy in indoor environments and significantly improve wireless network capacity. Due to its directionality, the orientation of antenna becomes a critical issue especially for moving devices while exchanging data with directional AP. Previous work largely focused on scenarios, in which the clients are stable at most of the time and addressed the issue of moving device by remeasurement of the whole network information such as received signal strength, conflict graph. However, re-collection of such information will introduce significant overhead. To adjust directional antenna quickly and wisely in real-time, in this paper, we introduce ADAS, a system that adjust orientation of directional antenna with sensor hints from mobile devices. The key idea of ADAS system is to obtain the movement behavior and location information from sensors equipped in mobile devices, then the AP adapts its orientation accordingly. We implement ADAS system on commercial directional antenna and evaluate its performance under different configurations. The experiment results demonstrate that directional AP in ADAS system could adapt to mobility with less overhead.
[Accelerometers, Portable computers, wireless sensor networks, ADAS system, directional AP, adjust directional antenna, indoor environment, mobile device, directive antennas, wireless network capacity, Directional antennas, received signal strength, Directive antennas, sensor hint, directional antenna, Acceleration, Smart phones, mobile handsets]
Time synchronization for underwater sensor networks based on multi-source beacon fusion
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Time synchronization is the foundation for collaboration among sensor nodes. Acoustic communication and restricted mobility of sensor nodes are two unique characteristics of underwater sensor networks (UWSNs), negating the effect of terrestrial synchronization schemes. Existing UWSN time synchronization protocols focus on point to point synchronization, incompatible for large-scale networks. Moreover, some of them require special hardwares and deployment conditions. In this paper, we propose MulSync, a scalable synchronization protocol for multi-hop UWSNs. MulSync includes the synchronization communication scheme to exploit acoustic communication nature of broadcast. The skew and offset are estimated by performing linear regression three times over a set of time stamp pairs gathered through message exchange. Three linear regressions are exploited to make full use of the time reference information delivered from multi-source beacons. Simulation results demonstrate that MulSync achieves high accuracy at low message overhead and time cost.
[large-scale networks, Protocols, multisource Beacon fusion, message exchange, regression analysis, linear regression, Acoustics, multihop UWSN, sensor nodes, sensor node mobility, Broadcasting, time reference information, protocols, MulSync, terrestrial synchronization schemes, time synchronization, Linear regression, Estimation, acoustic, acoustic communication, Synchronization, synchronisation, underwater acoustic communication, fusion, underwater sensor networks, synchronization communication scheme, UWSN time synchronization protocols, Propagation delay, scalable synchronization protocol]
Virtual keyboard for head mounted display-based wearable devices
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Wearable devices eliminate the need of physically taking out a mobile device before operating on it and are emerging as the next wave of mobile systems. Head-mounted display (HMD) is a key building block of wearable devices, and offers users immediate access to relevant information in a glance. However, most existing user input mechanisms accompanying HMDs are designed for interactive information exploration rather than for extended text entry. This paper describes the design, implementation and evaluation of a text input system for HMDs called Air Typing, which requires only a standard camera and is shown to be comparable in effectiveness to single-hand text input on tablet computers in a lab setting. Air Typing features a novel two-level virtual keyword layout, which substantially improves the typing speed by cutting down unnecessary hand movements during typing and greatly simplifies the associated image processing task by doing away with fine-grained matching between fingertips and keys. The current Air Typing prototype incorporates an OpenCV-based virtual key press detection algorithm that runs on the featured two-level virtual keyboard. In our tests, an experienced user's typing speeds of one-hand text input and of two-hand text input under Air Typing are 13 and 15 words per minute (WPM), respectively.
[featured two-level virtual keyboard, two-level virtual keyword layout, mobile systems, image processing task, tablet computers, Engines, interactive information exploration, head mounted display-based wearable devices, typing in air, hand tracking, cameras, Presses, gesture recognition, feature extraction, fingertip detection and tracking, user typing speeds, virtual keyboard, notebook computers, HMD, air typing prototype, single-hand text input system, keyboards, Thumb, two-hand text input, one-hand text input, helmet mounted displays, fine-grained matching, image matching, mobile device, OpenCV-based virtual key press detection algorithm, Head mounted display, Layout, Keyboards, keyboard layout, Skin, extended text entry, mobile handsets]
LDSN: Localization scheme for double-head maritime Sensor Networks
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Ocean covers nearly 71% of our planet's surface, yet 95% of the ocean remains unexplored by human being, and wireless sensor networks are envisioned to perform monitoring tasks over the large portion of our world. However, deploying wireless sensor networks on the sea poses many challenges and for maritime surveillance security applications we may need to deploy sensors both on the sea surface and underwater for three-dimensional detection. In this paper, we propose a hybrid ocean sensor networks called Double-head maritime Sensor Networks (DSNs), which combine the advantages of wireless sensor networks and underwater acoustic sensor networks. By leveraging the unique characteristics of DSNs, we design a localization scheme LDSN which is consisted of two algorithms SML and FLA. We first use SML to localize moored anchor nodes as seed nodes. After the underwater sensor networks have been localized, the floating double-head nodes can figure out its instant position via FLA algorithm. We evaluate the scheme by simulations and the results show that the scheme can achieve a high localization accuracy.
[Sea surface, double-head maritime sensor network, wireless sensor networks, marine communication, moored anchor node, wireless sensor network, three-dimensional detection, FLA algorithm, Underwater Sensor Networks, maritime surveillance security application, Wireless Sensor Networks, Tides, LDSN, Wireless sensor networks, hybrid ocean sensor network, SML, Surveillance, seed node, sensor placement, localization scheme, underwater acoustic sensor network, floating double-head node, Localization, Underwater acoustics]
Improving utilization through dynamic VM resource allocation in hybrid cloud environment
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Virtualization is one of the most fascinating techniques because it can facilitate the infrastructure management and provide isolated execution for running workloads. Despite the benefits gained from virtualization and resource sharing, improved resource utilization is still far from settled due to the dynamic resource requirements and the widely-used over-provision strategy for guaranteed QoS. Additionally, with the emerging demands for big data analytic, how to effectively manage hybrid workloads such as traditional batch task and long-running virtual machine (VM) service needs to be dealt with. In this paper, we propose a system to combine long-running VM service with typical batch workload like MapReduce. The objectives are to improve the holistic cluster utilization through dynamic resource adjustment mechanism for VM without violating other batch workload executions. Furthermore, VM migration is utilized to ensure high availability and avoid potential performance degradation. The experimental results reveal that the dynamically allocated memory is close to the real usage with only 10% estimation margin, and the performance impact on VM and MapReduce jobs are both within 1%. Additionally, at most 50% increment of resource utilization could be achieved. We believe that these findings are in the right direction to solving workload consolidation issues in hybrid computing environments.
[long-running virtual machine service, Protocols, VM Resource Dynamic Allocation, batch workload executions, Containers, hybrid cloud environment, VM Migration, virtualisation, Yarn, parallel processing, infrastructure management, MapReduce, dynamic resource requirements, resource allocation, QoS, Benchmark testing, cloud computing, dynamic resource adjustment mechanism, hybrid computing environments, improved resource utilization, data analysis, dynamic VM resource allocation, virtualization, VM service, cluster utilization, Dynamic scheduling, dynamic memory allocation, quality of service, Hybrid Cloud Environment, big data analytic, Memory management, resource sharing, virtual machines, workload consolidation issues, Resource management]
FENet: An SDN-based scheme for virtual network management
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Virtual networking is vital to efficient resource management in Clouds, and it is in fact one of the main services provided by many Cloud Computing platforms. Virtual network management needs to meet specific requirements, including tenant isolation and adaption to virtual machines' lifecycle. Most of the existing schemes for virtual network management are based on the use of overlay networks in order to achieve a desirable degree of flexibility. However, these schemes suffer from a common limit, i.e. relatively high performance penalty due to a complicated forwarding process. We address this performance concern by developing a new management scheme, FENet, which makes use of Software-Defined Networks (SDN) to create virtual networks and manage them via the SDN controller programs. We present the design of an SDN controller, with the definition of flow entry rules based on the OpenFlow protocol and the specification of a routing algorithm. The results from our experimental evaluation show that our SDN-based prototype can control virtual network interconnections and tenant isolation appropriately. FENet achieves about 30% better network performance than the management scheme based on OpenVPN and lower latency in comparison with the traditional bridging scheme.
[resource management, Protocols, OpenFlow protocol, Ports (Computers), OpenVPN, OpenFlow, Control systems, virtual network interconnections, virtual network management, Overlay networks, software-defined networks, Cloud Computing, virtual machines lifecycle, IP networks, cloud computing, SDN, telecommunication control, FENet, SDN controller programs, flow entry rules, software defined networking, Routing, virtual networks, routing algorithm, routing protocols, overlay networks, virtual machines, complicated forwarding process, Logic gates, tenant isolation]
Be a good neighbour: Characterizing performance interference of virtual machines under xen virtualization environments
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
With the rapid development of virtualization techniques, modern data centers move into a new era of cloud in recent years. Despite numerous advantages such as high resource utilization and rapid service scalability, current virtualization techniques don't guarantee perfect performance isolation among virtual machines sharing the physical machine, which may lead to unstable and unpredictable user-perceived application performance in clouds. Therefore, understanding and modeling performance interference among collocated applications is of utmost importance. However, the hypervisor and guest OSes usually run independent resource schedulers and are invisible into each other, thereby making accurately characterizing performance interference a non-trivial work. In this paper, we first present a comprehensive experimental study on performance interference of different combinations of benchmarks, observing that virtual CPU floating overhead between multiple physical CPUs, and VMEXITs, i.e., the control transitions between the hypervisor and VMs, constitute the key source of performance interference. In order to characterize the performance interference effects, we measure both the application-level and VM-level characteristics from the collocated applications and then build a novel interference prediction framework based on kernel canonical correlation analysis. Our evaluations first show the practicability of KCCA in finding reliable correlation, and further confirm the high accuracy and great applicability of our interference model with a low prediction error of no more than 7.9%.
[Xen, resource schedulers, Correlation, application-level characteristics, Xen virtualization environments, physical machine, data centers, Switches, interference prediction framework, kernel canonical correlation analysis, virtualisation, VMEXITs, virtual CPU floating overhead, Degradation, cloud, resource allocation, performance interference, performance modeling, KCCA, Benchmark testing, scheduling, cloud computing, resource utilization, Context, Interference, performance evaluation, guest OSes, computer centres, VM-level characteristics, physical CPUs, Virtual machine monitors, virtual machines, operating systems (computers)]
Accelerating the iterative linear solver for reservoir simulation on multicore architectures
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Modern petroleum reservoir simulation serves as a primary tool for quantitatively managing reservoir production and planning new fields. It involves repeatedly solving the Jacobian of a set of strong nonlinear partial differential equations governing the mass and energy conduction and conservation. Most of the existing reservoir simulators adopt iterative solver with multiple stages of preconditioners, in which the incomplete LU (ILU) factorization is an outstanding universal smoother. However, it turns out that when the degree of freedom of each grid grows, ILU usually becomes the bottleneck of the solver. Moreover, ILU is difficult to parallelize due to its inherent data dependency. In this paper, we developed a sparse iterative solver with parallelized ILU and triangular solve using block-wise data structure. Compared with the state of art iterative solver on 14 industrial reservoir simulation matrices, the proposed ILU is 5.2x faster (on average) than the state of art iterative solver because of the block-wise data structure, which leads to 2.2x speedup on the total solver runtime. In addition, parallel ILU and triangular solve are developed to further accelerate the solver. To tackle the strong data dependency in ILU and triangular solve, we first partition the algorithm into separated tasks and construct a data flow graph to represent the data dependency. Then, tasks are scheduled in parallel according to the topological order of the data flow graph. On an 8-thread multicore architecture, we achieved another 3.6x speedup on ILU factorization, and 3.3x on triangular solve with good scalability.
[iterative methods, energy conduction, production engineering computing, data flow graphs, 8-thread multicore architecture, Sparse matrices, reservoir production planning, ILU factorization, Jacobian matrices, Runtime, nonlinear partial differential equations, topological order, incomplete LU factorization, scheduling, sparse matrix, triangular solve, data structures, iterative linear solver, data flow graph, Mathematical model, reservoir production management, Multicore processing, Computational modeling, sparse iterative solver, Iterative solver, industrial reservoir simulation matrices, petroleum industry, parallelized ILU, nonlinear differential equations, matrix algebra, block-wise data structure, data dependency, multicore architectures, petroleum reservoir simulation, hydrocarbon reservoirs, energy conservation, Reservoirs, production planning, Jacobian, partial differential equations, task scheduling, Multicore architecture]
A hybrid on-chip network with a low buffer requirement
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
As the CMOS technology develops, the number of buffers required in a network-on-chip increases with flit width. This increase of buffers provides more power and area overhead to a network router. This paper proposes a hybrid packet-switched and circuit-switched network in which the total buffer requirement depends on only the width of the short message and buffer depth, and does not increase with the network width. The performance is maintained through a low latency circuit-switch by using a simple reverse path reservation method. The simulation results indicated that a considerable amount of power and area can be saved by the buffer reduction, whereas performance is maintained.
[Protocols, network-on-chip, Pipelines, Ports (Computers), Switches, Telecommunication traffic, hybrid network, packet-switched network, complimentary metal oxide semiconductors, hybrid on-chip network, circuit-switched network, simple reverse path reservation method, buffer reduction, Switching circuits, Network-on-chip, CMOS technology, network width, circuit-switched, Nickel, buffer requirement]
Cache-aware sparse matrix formats for Kepler GPU
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Scientific simulations often require solving extremely large sparse linear equations, whose dominant kernel is sparse matrix vector multiplication. On modern many-core processors such as GPU or MIC, the operation has been known to pose significant bottleneck and thus would result in extremely poor efficiency, because of limited processor-to-memory bandwidth and low cache hit ratio due to random access to the input vector. Our family of new sparse matrix formats for many-core processors significantly increases the cache hit ratio and thus performance by segmenting the matrix along the columns, dividing the work among the many core up to the internal cache capacity, and aggregating the result later on. Performance studies show that we achieve up to x3.0 speedup in SpMV and x1.68 in multi-node CG, compared to the best vendor libraries and competing new formats that have been recently proposed such as SELL-C-&#x03C3;.
[multiprocessing systems, Graphics processing units, linear equation, cache storage, processor-to-memory bandwidth, Sparse matrices, graphics processing units, cache hit ratio, matrix multiplication, many-core processor, Kepler GPU, graphics processing unit, Libraries, Hardware, cache-aware sparse matrix format, cache capacity, Vector processors, Matrix converters, sparse matrix vector multiplication]
ACDT: Architected Composite Data Types trading-in unfettered data access for improved execution
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
With Exascale performance and its challenges in mind, one ubiquitous concern among architects is energy efficiency. Petascale systems projected to Exascale systems are unsustainable at current power consumption rates. One major contributor to system-wide power consumption is the number of memory operations leading to data movement and management techniques applied by the runtime system. To address this problem, we present the concept of the Architected Composite Data Types (ACDT) framework. The framework is made aware of data composites, assigning them a specific layout, transformations and operators. Data manipulation overhead is amortized over a larger number of elements and program performance and power efficiency can be significantly improved. We developed the fundamentals of an ACDT framework on a massively multithreaded adaptive runtime system geared towards Exascale clusters. Showcasing the capability of ACDT, we exercised the framework with two representative processing kernels - Matrix Vector Multiply and the Cholesky Decomposition - applied to sparse matrices. As transformation modules, we applied optimized compress/decompress engines and configured invariant operators for maximum energy/performance efficiency. Additionally, we explored two different approaches based on transformation opaqueness in relation to the application. Under the first approach, the application is agnostic to compression and decompression activity. Such approach entails minimal changes to the original application code, but leaves out potential application-specific optimizations. The second approach exposes the decompression process to the application, hereby exposing optimization opportunities that can only be exploited with application knowledge. The experimental results show that the two approaches have their strengths in HW and SW respectively, where the SW approach can yield performance and power improvements that are an order of magnitude better than ACDT-oblivious, hand-optimized implementations. We consider the ACDT runtime framework an important component of compute nodes that will lead towards power efficient Exascale clusters.
[multithreaded adaptive runtime system, ACDT framework, Adaptive systems, Instruction sets, architected composite data types, compress/decompress engines, data management techniques, Sparse matrices, database management systems, program performance, Engines, power consumption, application-specific optimizations, power consumption rates, Cholesky decomposition, Runtime, optimisation, power aware computing, exascale clusters, transformation opaqueness, energy efficiency, processing kernels, exascale performance, power efficiency, data composites, multi-threading, decompression activity, transformation modules, Matrix decomposition, unfettered data access, petascale systems, data manipulation overhead, memory operations, data movement, energy conservation, exascale systems, performance efficiency, Arrays, matrix vector multiply, sparse matrices]
Combine thread with memory scheduling for maximizing performance in multi-core systems
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
The growing gap between microprocessor speed and DRAM speed is a major problem that computer designers are facing. In order to narrow the gap, it is necessary to improve DRAM's speed and throughput. Moreover, on multi-core platforms, DRAM memory shared by all cores usually suffers from the memory contention and interference problem, which can cause serious performance degradation and unfairness among parallel running threads. To address these problems, this paper proposes techniques to take both advantages of partitioning cores, threads and memory banks into groups to reduce interference among different groups and grouping the memory accesses of the same row together to reduce cache miss rate. A memory optimization framework combined thread scheduling with memory scheduling (CTMS) is proposed in this paper, which simultaneously minimizes memory access schedule length, memory access time and reduce interference to maximize performance for multi-core systems. Experimental results show CTMS is 12.6% shorter in memory access time, while improving 11.8% throughput on average. Moreover, CTMS also saves 5.8% of the energy consumption.
[Schedules, Instruction sets, cache storage, memory contention, DRAM memory, memory interference, optimisation, microprocessor speed, DRAM chips, multicore system, DRAM speed, energy consumption, energy, multiprocessing systems, memory optimization, interference problem, cache miss rate, Interference, microprocessor chips, Scheduling, memory access schedule length, Thread scheduling, memory access time, performance, Memory management, thread scheduling, memory scheduling, Resource management, Clocks]
Sensor-free corner shape detection by wireless networks
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Due to the rapid growth of the smartphone applications and the fast development of the Wireless Local Area Networks (WLANs), numerous indoor location-based techniques have been proposed during the past several decades. Floorplan, which defines the structure and functionality of a specific indoor environment, becomes a hot topic nowadays. Conventional floorplan techniques leverage smartphone sensors combined with WiFi signals to construct the floorplan of a building. However, existing approaches with sensors cannot detect the shape of a corner, and the sensors cost huge amount of energy during the whole floorplan constructing process. In this paper, we propose a sensor-free approach to detect the shape of a certain corner leveraging WiFi signals without using sensors on smartphones. Instead of utilizing traditional wireless communication indicator Received Signal Strength (RSS), we leverage a finer-grained indicator Channel State Information (CSI) to detect the shape of a certain corner. The evaluation of our approach shows that CSI is more robust in sensor-free corner shape detection, and we have achieved over 85% detection accuracy in simulation and over 70% detection accuracy in real indoor experiments.
[Wireless, Correlation, Shape, wireless communication indicator, channel state information, Channel State Information, floorplan technique, Transmitters, received signal strength, CSI, smartphone sensor, indoor radio, Mathematical model, wireless network, Localization, floorplan constructing process, Fading, Indoor environments, Receivers, WLAN, wireless local area network, RSS, WiFi signal, sensor-free corner shape detection, Floorplan, smartphone application, indoor location-based technique, wireless LAN, Smartphone]
Hand-to-Hand instant message communication: Revisiting Morse code
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
In this paper, we propose a novel vibration-based communication system for Bluetooth-equipped smartphones. Smartphones are commonly utilized through the display; however, sometimes users would like to exchange pieces of information with nearby peers, without shifting their focus from current task. Additionally, this enables communication in cases without visual or sound contact. For this purpose, we developed a novel application called Hand-to-Hand on Bluetooth Communication (H2BCom). We use the common gestures of tapping and touching the smartphone display for sending a Morse coded message over a Bluetooth channel. On the receiving side, the Morse coded message is presented as vibration of the smartphone. We show the design, implementation and evaluation of H2BCom on Android phones. In our user study, we found that using the Morse code was difficult for beginners, but the users'skill improved after taking a tutorial of the Morse code.
[Bluetooth, Communication systems, channel coding, Receivers, Bluetooth-equipped smartphone, Encoding, smart phones, Electronic mail, hand-to-hand on Bluetooth communication channel, Haptic communications, Morse code, radio receivers, vibration-based communication system, morse coded message, Vibrations, H2BCom, hand-to-hand instant message communication, smartphones, wireless channels, instant messaging, Smart phones, vibration-based communications]
Towards social botnet behavior detecting in the end host
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Social botnet utilizing online social network (OSN) as Command and Control channel (C&amp;C) has caused enormous threats to Internet security. Server-side detection approaches mainly target on suspicious accounts, which cannot identify the specific bot hosts or processes. Host-side approaches target on suspicious process behaviors which are not robust enough to face the challenges of frequent variants and novel social bots. In this paper, we propose a novel social bot behavior detecting approach in the end host. Because social bot binaries or source codes are not easy to collect, we first design a novel social botnet, named wbbot, based on Sina Weibo. We analyze it from two aspects, wbbot architecture and wbbot behaviors. Second, we analyze the host behaviors of existing social botnets which come from public websites, other researchers, and our implementations. We identify six critical phases: infection, pre-defined host behaviors, establishment of C&amp;C, receive the commands of botmaster, execution of social bot commands, and return the results. Third, we present our detection system which consists of three components: host behavior monitor, host behavior analyzer, and detection approach. We present behavior tree-based approach to detect social bot. After constructing the suspicious behavior tree, we match it with the template library to generate detection result. Finally, we collect real-world social botnet traces to evaluate the performance. We would like to share them for academic research. The results indicate that our system has an acceptable false positive rate of 29.6% and remarkable false negative rate of 4.5%. However, compared with other detection tools, our detection result is still remarkable.
[invasive software, false positive rate, false negative rate, Internet security, command-and-control channel, Servers, host behavior analyzer, social botnet behavior detection approach, wbbot behavior, Man machine systems, Monitoring, online social network, Sina Weibo, tree similarity, wbbot architecture, host behavior monitor, OSN, host behavior, botnet traces, server-side detection approach, Keyboards, social networking (online), Mice, Internet, social botnet]
EasiCAE: A runtime framework for efficient sensor sharing among concurrent IoT applications
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Traditional wireless sensor networks (WSNs) can be integrated into Internet and be regarded as its sensing infrastructure, which supports development and running of multiple third-party applications simultaneously. Therefore, due to constrained resource of sensor nodes, it is necessary to establish a runtime framework to improve sensor sharing efficiency for concurrent third-party applications. This paper presents EasiCAE, a concurrent applications runtime framework, to enhance sensor sharing efficiency greatly by incorporating task allocation with redundancy elimination. In brief, EasiCAE decompose the applications into tasks and distributes tasks to the sensors which will bring the least energy to run them. EasiCAE has three salient features. Firstly, we define task-sensor correlation to indicate how many samplings of a sensor can be shared with the new task. Secondly, EasiCAE reduces energy consumption by assigning tasks to a sensor with higher task-sensor correlation. Finally, a light-weight merging algorithm is proposed to eliminate redundant samplings for the assigned sensors. Experimental results show that EasiCAE reduces energy consumption by 31% to 79% compared with existing methods, while introducing tolerable overheads. We also evaluate EasiCAE with various influencing parameters, showing that the performance of EasiCAE increases stably as the network scale and the number of concurrent applications increases.
[Algorithm design and analysis, Energy consumption, Correlation, wireless sensor networks, concurrent IoT application, Redundancy, wireless sensor network, Internet of Things, EasiCAE, third-party application, WSN, concurrency control, sensor sharing, Sensors, runtime framework, Data communication, Resource management]
SLA-based energy aware scheduling of precedence-constrained applications on DVFS-enabled clusters
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
The energy aware scheduling problem has been a critical issue in high-performance clusters owing to their high operation cost, environmental impact, and low reliability. An existing technique to reduce energy consumption of applications is dynamic voltage/frequency scaling (DVFS). In this paper, we develop an energy aware scheduling algorithm called EASLA for precedence-constrained applications in the context of Service Level Agreement (SLA) on DVFS-enabled cluster systems. Due to the dependencies among tasks and makespan extension, there may be some slacks under used. The main idea of the EASLA algorithm is to distribute each slack to a set of tasks and scale frequencies down to try to minimize energy consumption. Specifically, it first finds the maximum set of independent tasks for each task, and then iteratively allocates each slack to the maximum independent set whose total energy reduction is the maximal. Randomly generated graphs and two real-world applications are tested in our experiments. The experimental results show that our scheduling algorithm can save up to 22.68% and 12.01% energy consumption compared with GreedyDVS and EvenlyDVS algorithms, respectively.
[Algorithm design and analysis, workstation clusters, Dynamic Voltage/Frequency Scaling, Energy consumption, Schedules, dynamic voltage/frequency scaling, graph theory, randomly generated graphs, SLA-based energy aware scheduling algorithm, Radio spectrum management, power aware computing, precedence-constrained applications, energy consumption, Service Level Agreement, makespan extension, high-performance clusters, Computational modeling, total energy reduction, EASLA algorithm, DVFS-enabled cluster systems, Scheduling, DAG, service level agreement, energy consumption minimization, Processor scheduling, Cluster Computing, energy consumption reduction, scale frequencies, Energy Aware Scheduling]
Extending checksum-based ABFT to tolerate soft errors online in iterative methods
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
As the size and complexity of high performance computers increase, more soft errors will be encountered during computations. Algorithm-Based Fault Tolerance (ABFT) has been proved to be a highly efficient technique to detect soft errors in dense linear algebra operations including matrix multiplication, Cholesky and LU factorization. While ABFT can also be applied to a iterative sparse linear algebra algorithm via applying it to every individual matrix-vector multiplication in the algorithm, it often introduces considerable overhead. In this paper, we propose novel extensions to ABFT to not only reduce the overhead but also protect computations that can not be protected by existing ABFT. Instead of maintaining checksums in every individual matrix-vector multiplication, we modified the algorithms so that checksums established at the beginning of the algorithms can be maintained at every iterations throughout the algorithms. Because soft errors in most iterative sparse linear algebra algorithms will propagate from one iteration to another, we do not have to verify the correctness of the checksums at each iteration to detect errors. By reducing the frequency of verification, the fault tolerance overhead can be greatly reduced. Experimental results demonstrate that, when used with local diskless checkpoints together, our approach introduces much less overhead than the existing ABFT techniques.
[Checkpointing, checkpointing, iterative methods, fault tolerance overhead reduction, Iterative Methods, matrix decomposition, Error Detection, Sparse matrices, parallel processing, Fault tolerance, Fault tolerant systems, Iterative methods, online soft error tolerance, Cholesky factorization, Symmetric matrices, LU factorization, algorithm-based fault tolerance, Soft Errors, software fault tolerance, matrix-vector multiplication, diskless checkpoints, matrix multiplication, ABFT, iterative sparse linear algebra algorithm, Fault detection, checksum-based ABFT extension, Diskless Checkpoint, sparse matrices, soft error detection]
MRTune: A simulator for performance tuning of MapReduce jobs with skewed data
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
MapReduce is a programming model designed by Google that has been widely used for both high performance computing and big data processing. Although the programming model is simple, it is very challenging to conduct performance tuning for a MapReduce job, considering the complexities of the configuration parameters and various tradeoffs between the performance gain of the optimization approaches and the extra overhead they bring about. One naive way to address this issue is to run the MapReduce jobs repeatedly using different combinations of configuration parameters and optimization methods, then select the one with the shortest running time. However, real execution is impractical because the combinations may be too many and the time of one run of each combination may be too long. Therefore, it is desirable if we can efficiently estimate the runtime of a job without real execution using only the input data and the configuration parameter settings of the cluster. In this paper, we propose a novel MapReduce simulator called MRTune for runtime estimation of MapReduce jobs. MRTune takes the key distribution of input data into consideration and can work well even when the key distribution of data is skewed. Moreover, MRTune can estimate the runtime of a job in the presence of unpredictable task failures. We evaluate MRTune implementing MapReduce jobs with Zipfian distributed input data. The result shows that MRTune can estimate the runtime of MapReduce jobs with high accuracy and efficiency while the key distribution of input data is skewed. We also conduct two case studies to analyse the impact of data skew and task failures on a MapReduce job.
[skewed data, Complexity theory, MRTune implementation, parallel programming, MapReduce, runtime estimation, configuration parameter complexities, Zipfian distributed input data, unpredictable task failures, programming model, job runtime estimation, software performance evaluation, Google, simulator, Computational modeling, performance tuning, overhead, optimization approach, performance gain, MRTune, Tuning, Data models, data handling, Reduce simulator, MapReduce jobs, skew]
TSUBAME-KFC: A modern liquid submersion cooling prototype towards exascale becoming the greenest supercomputer in the world
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Modern supercomputer performance is principally limited by power. TSUBAME-KFC is a state-of-the-art prototype for our next-generation TSUBAME3.0 supercomputer and towards future exascale. In collaboration with Green Revolution Cooling and others, TSUBAME-KFC submerges compute nodes configured with extremely high processor/component density, into non-toxic, low viscosity oil with high 260 Celsius flash point, and cooled using ambient / evaporative cooling tower. This minimizes cooling power while all semiconductor components kept at low temperature to lower leakage current. Numerous off-line in addition to on-line power and temperature sensors are facilitated throughout and constantly monitored to immediately observe the effect of voltage/frequency control. As a result, TSUBAME-KFC achieved world No.1 on the Green500 in Nov. 2013 and Jun. 2014, by over 20% c.f. the nearest competitors.
[temperature sensors, Servers, parallel machines, Temperature sensors, online power, semiconductor components, flash point, Power measurement, power aware computing, TSUBAME-KFC, cooling towers, cooling power, ambient cooling tower, Cooling, next-generation TSUBAME3.0 supercomputer, Green500, high processor/component density, leakage current, Supercomputers, frequency control, modern supercomputer performance, nontoxic low viscosity oil, greenest supercomputer, Temperature measurement, leakage currents, evaporative cooling tower, green revolution cooling, liquid submersion cooling prototype, future exascale, green computing, evaporation, compute nodes, voltage control]
Building a large-scale direct network with low-radix routers
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Communication locality is an important characteristic of parallel applications. A great deal of research shows that utilizing the characteristic will favor most applications. Aiming at communication locality, we present a hierarchical direct network topology to accelerate neighbor communication. Combining mesh topology and complete graph topology, it can be used to optimize local communication and build large-scale network with low radix routers. Analyzing the characteristic of hierarchical topology, we find the presented topology has high cost performance and excellent expandability. We also design two minimum path routing algorithms and compare them with Mesh, Dragonfly and PERCS topologies. The results show the saturated throughput of hierarchical topology is nearly 40% with uniform random trace and 70% with local communication model of 4K nodes. That indicates high scalability for applications with local communication and cost efficiency for uniform random trace.
[Algorithm design and analysis, graph topology, communication locality, hierarchical network, virtual higt radix router, Scalability, co-design, telecommunication network topology, Routing, PERCS topology, Topology, mesh topology, scalability, large-scale direct network, Program processors, Network topology, telecommunication network routing, Bandwidth, telecommunication network reliability, dragonfly topology, low-radix router, parallel application, minimum path routing algorithm]
HFA: A Hint Frequency-based approach to enhance the I/O performance of multi-level cache storage systems
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
With the enormous and increasing user demand, I/O performance is one of the primary considerations to build a data center. Several new technologies in data centers, such as tiered storage [33], prompt the widespread usage of multi-level cache techniques. In these storage systems, the upper level storage typically serves as a cache for the lower level, which forms a distributed multi-level cache system. However, although many excellent multi-level cache algorithms are proposed to improve the I/O performance, they still have potential to be enhanced by investigating the history information of hints [28]. To address this challenge, in this paper, we propose a novel Hint Frequency-based Approach (HFA), to improve the overall multi-level cache performance of storage systems. The main idea of HFA is using hint frequencies (the total number of demotions/promotions by employing demote/promote hints) to efficiently explore the valuable history information of data blocks among multiple levels. HFA can be applied with several popular multi-level cache algorithms, such as Demote, Promote, Hint-K, etc. Simulation results show that, compared to original multi-level cache algorithms such as Demote, Promote and Hint-K, HFA can improve the I/O performance by up to 20% under different I/O workloads.
[Algorithm design and analysis, Performance evaluation, demote algorithm, HFA, Heuristic algorithms, Cache storage, cache storage, input-output programs, distributed multilevel cache system, History, Servers, upper level storage, IO performance, data blocks, multilevel cache performance, Multi-level Cache, tiered storage, Promote, computer centres, Storage Systems, hint frequency-based approach, promote algorithm, Performance Evaluation, data center, Simulation, user demand, valuable history information, multilevel cache storage systems, Demote, Hint-K algorithm]
Where should the threads go? Leveraging hierarchical data locality to solve the thread affinity dilemma
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
We are proposing a novel framework that ameliorates locality-aware parallel programming models, by defining a hierarchical data locality model extension. We also propose two hierarchical thread partitioning algorithms. These algorithms synthesize hierarchical thread placement layouts that targets minimizing the program's overall communication costs. We demonstrate the effectiveness of our approach using the NAS Parallel Benchmarks implemented in Unified Parallel C (UPC) using a modified Berkeley UPC Compiler and runtime system. We achieved performance gains of up to 88% in performance by applying the placement layouts our algorithms suggest.
[Measurement, multi-threading, Instruction sets, data locality model extension, locality-aware parallel programming model, NAS parallel benchmark, Partitioning algorithms, communication cost minimization, program compilers, runtime system, Many-cores, hierarchical thread partitioning algorithm, Hierarchical locality, Berkeley UPC compiler, unified parallel C, Data locality, Clustering algorithms, Benchmark testing, data handling, minimisation, thread affinity dilemma, Kernel, Message systems]
Enhancing application performance through OpenFlow enabled multi-homed devices
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
The increasing demand for mobile data usage is overwhelming our wireless network. However, most of these devices are equipped with multiple network interfaces (multi-homed) which when properly exploited have the potential to increase bandwidth capacity and ensure reliable connectivity. But some technical challenges are restricting our multimode devices to using one single interface at a time. In this paper, we advocate for the use of multiple interfaces simultaneously, which can achieve reliable network connectivity, allow applications to distribute their traffic over multiple interfaces, and aggregate the capacity of different paths. We proposed an OpenFlow-based Multiple WLAN Interface (OpenMWF) prototype, designed by embedding OpenFlow network resource controller and a data forwarding open virtual switch (OVS) in the network stack of the device operating system. A proof of concept implementation and evaluation was carried out using Web-based Real-Time Communication (WebRTC) interactive video conferencing and multi-source downloading utility (Aria2c) applications use cases. The result showed the approach yielded benefits for applications performance, over single interface usage.
[Performance evaluation, Wireless LAN, bandwidth capacity, device operating system, WebRTC interactive video conferencing, Switches, device drivers, OpenFlow, mobile data usage, OpenFlow network resource controller, Network interfaces, OVS, application performance, data forwarding open virtual switch, network interfaces, mobile computing, Bandwidth, telecommunication network reliability, wireless network, OpenMWF prototype, Web-based real-time communication, SDN, multisource downloading utility, mobile radio, Interference, reliable network connectivity, network stack, computer network performance evaluation, Aria2c applications, Multiple networks interface, traffic distribution, Open vSwitch, OpenFlow-based multiple WLAN interface, wireless LAN, OpenFlow enabled multihomed devices, telecommunication traffic, WebRTC]
Wireless transmission modeling for Vehicular Ad-hoc Networks
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Modeling wireless transmission in stringent networks such as VANETs is a challenging task. This requires mathematically incorporating all the environmental effects present within such a dynamics atmosphere. The key attributes to model the wireless channel are physical constraints inherent to such networks such as lack of permanent infrastructure, limited knowledge in relation to the position of vehicles as well as interference that effects the strength of receive signal at each position of vehicles. The selection of an appropriate transmission model plays a key role in the routing decisions for VANET. This paper investigates such wireless transmission models for vehicular communication. It identifies the situations where a particular model can be beneficial. The paper also provides an insight into the use of practical parameters in theoretical transmission models. An analysis of the proposed transmission model is presented. The performance of different transmission models in terms of receive signal strength (RSS) is also presented. These results help to select a transmission model that suits best to a particular VANET communication scenario.
[vehicular ad hoc networks, Receive Signal Strength (RSS), Receivers, VANET, Routing, Transmission Modeling, vehicular ad-hoc network, modelling, vehicular communication, Vehicles, Wireless communication, vehicle position, RSSI, physical constraint, RSS, Vehicular ad hoc networks, wireless transmission modeling, receive signal strength, wireless channel, Random variables, wireless channels, Mathematical model]
Channel holding time of packet sessions in all-IP cellular networks
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
In this paper, we analyze the packet session channel holding time for packet-switched cellular networks. Channel holding time depends on mobility which is characterized by cell residence time in this paper. We apply Hyper-Erlang distribution model to investigate the packet session channel holding time in which a session will experience active and idle periods. In terms of new session, handoff session in busy mode, handoff session with new arrival packets in idle mode and handoff session without arrival packet in idle mode, we demonstrate that the effective channel holding time is exponentially distributed if and only if the cell residence time is exponentially distributed. The analytical results provide a new approach to evaluate traffic performance and system design in packet-switched cellular networks.
[arrival packets, mobility, Exponential distribution, packet switching, Hyper-Erlang distribution model, cell residence time, mobility management (mobile radio), Analytical models, handoff session, busy mode, channel holding time, packet session, idle mode, Density functional theory, idle periods, IP networks, traffic performance, packet session channel holding time, Laplace equations, system design, analysis, active periods, packet sessions, all-IP cellular networks, Data transfer, Data models, packet-switched cellular networks, Integrated circuit modeling, cellular radio]
Design and analysis of software defined Vehicular Cyber Physical Systems
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
VCPS (Vehicular Cyber Physical Systems) is a special kind of networked cyber physical system in which each vehicle is regarded as a communication unit. Vehicle's movement is restricted by road and environment in VCPS, while traditional random mobility model and waypoint mobility model cannot reflect the realistic vehicle traces. In VCPS, with the high speed of vehicles, the network topology undergoing tremendous changes all the time, which greatly undermines the stability of communication between vehicles. The diversity and complexity of traffic scenarios in VCPS have also increased the difficulty of designing an efficient and stable routing protocol. In this paper, we creatively combine SDN (Software Defined Networking) and VCPS together and propose a new VCPS communication architecture, which enable VCPS to be manageable by remote controller. SD-VCPS can flexibly change routing policies depending on different traffic scenes or traffic periods, adjusting the topology of VCPS to adapt to different network requirements. We further present a new location-based routing protocol for SD-VCPS, and corroborate the efficiency of our proposed framework by experiments using network simulator NS3.
[radio direction-finding, telecommunication network management, software defined vehicular cyber physical system, Switches, traffic scenario, mobility management (mobile radio), Vehicles, location-based routing protocol, Location-Based Routing, software radio, VCPS, Routing protocols, SDN, stability, vehicular ad hoc networks, Base stations, waypoint mobility model, random mobility model, remote controller, NS3 network simulator, software defined networking, telecommunication network topology, Routing, network topology, Communication Protocol, computer network management, routing protocols, Software Defined Networking, telecommunication traffic]
Performance analysis of HPC applications with irregular tree data structures
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Adaptive mesh refinement (AMR) numerical methods utilizing octree data structures are an important class of HPC applications, in particular the solution of partial differential equations. Much effort goes into the implementation of efficient versions of these types of programs, where the emphasis is often on increasing multi-node performance when utilizing GPUs and coprocessors. By contrast, our analysis aims to characterize these workloads on traditional CPUs, as we believe that single-threaded intra-node performance of critical kernels is still a key factor for achieving performance at scale. Especially irregular workloads such as AMR methods, however, exhibit severe underutilization on general purpose processors. In this paper, we analyze the single core performance of two state-of-the-art, highly scalable adaptive mesh refinement codes, one based on the Fast Multipole Method (FMM) and one based on the Finite Element Method (FEM), when running on a x86 CPU. We examined both scalar and vectorized implementations to identify performance bottlenecks. We demonstrate that vectorization can provide a significant benefit in achieving high performance. The greatest bottleneck to peak performance is the high fraction of non-floating point instructions in the kernels.
[Algorithm design and analysis, HPC application, AVX, fast multipole method, SIMD, parallel processing, PAPI, MANGLL, Program processors, finite element method, Fast Multipole Method, Polynomials, octrees, Kernel, octree data structure, Finite Element Method, FMM, mesh generation, partial differential equation, adaptive mesh refinement, AMR numerical method, FEM, PVFMM, Bridges, Octrees, HPC, Finite element analysis, partial differential equations, irregular tree]
Parallel exact spliced alignment on GPU
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Due to the exponential growth of biological DNA sequence databases, some parallel gene prediction solutions on different high performance platforms have been proposed. Nevertheless, few exact parallel solutions to the spliced alignment problem to gene prediction in eukaryotic organisms have been proposed and none of these solutions use GPUs as the target platform. In this paper, we present the development of two GPU accelerators for an exact solution to the spliced alignment problem applied to gene prediction. Our main contributions are: (a) the identification of two forms to exploit parallelism in the spliced alignment algorithm; (b) two GPU accelerators that achieve speedups up to 52.62 and 90.86, respectively, when compared to a sequential implementation. The accelerators performance scales with input data size, outperforming related work results; (c) a particular organization for the data structures of the accelerators in order to optimize their efficiency; (d) a potential parallelism analysis of the biological data set with the goal of measuring the amount of parallelism that would in fact be available to be exploited by a parallel implementation; and (e) an accurate performance estimation model that enabled estimating the accelerators performance, before implementing them.
[gene prediction, Graphics processing units, parallel exact spliced alignment, parallelism, parallel processing, GPU, parallelism analysis, genetics, Parallel processing, Prediction algorithms, data structures, eukaryotic organisms, Spliced alignment, Bioinformatics, accelerator, Estimation, GPU accelerators, biological data set, parallel implementation, Gelfand algorithm, exponential growth, spliced alignment problem, graphics processing units, parallel gene prediction solutions, DNA, performance estimation model, bioinformatics, biological DNA sequence databases, high performance platforms]
A distributed real-time operating system built with aspect-oriented programming for distributed embedded control systems
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
The paper presents a method to build a distributed real-time operating system for distributed embedded control systems using aspect-oriented programming. We define aspects to weave distributed computing mechanisms to an existing real-time operating system. By using the aspects, we can build a distributed operating system without modifying the original source code. This improves the maintainability of the source code of a real-time operating system family. We have applied the aspects to an OSEK OS and have got a distributed operating system that provides location-transparent system calls for task management and inter-task synchronization. The evaluation results show that the overhead of aspect-oriented programming is practically small enough.
[source code (software), task management, distributed real-time operating system, OSEK OS, distributed computing mechanisms, Synchronization, distributed embedded control systems, Operating systems, automotive control, embedded systems, real-time systems, intertask synchronization, aspect-oriented programming, operating systems (computers), Real-time systems, Weaving, distributed control systems, source code maintainability, location-transparent system, distributed operating systems]
Physically based parallel ray tracer for the Metropolis light transport algorithm on the Tianhe-2 supercomputer
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Developing an efficient and highly scalable ray tracer for the Metropolis light transport algorithm is becoming increasingly important as the request for photorealistic images becomes a common trend. Although the Metropolis light transport algorithm has produced some of the most realistic images to date, it usually takes a great amount of time to render an image. The development of an efficient and highly scalable ray tracer for the Metropolis light transport algorithm is hard due in large part to the irregular memory access patterns, the imbalanced workload of light-carrying paths and the complicated mathematical model and complex physical processes. In this paper, we present a highly scalable physically based parallel ray tracer for the Metropolis light transport algorithm. Firstly, we present the idea of snapshot and sub-snapshot, then propose a novel assignment partitioning algorithm for compute nodes and CPU cores since the demand-driven assignment partitioning algorithms don't work. Secondly, we propose a physically based parallel ray racing framework for the Metropolis light transport algorithm, which is based on a master-worker architecture. Finally, we discuss the issue of granularity of the assignment partitioning and some optimization strategies for improving overall performance, then a hybrid scheduling strategy combining a static and dynamic scheduling strategy is described. Experiments show that our physically based ray tracer almost reaches linear speedup by using 26,400 CPU cores on the Tianhe-2 supercomputer. Our ray tracer is more efficient and highly scalable.
[hybrid scheduling, EMTP, optimization strategies, physically based ray tracing, ray tracing, parallel machines, photorealistic images, light-carrying paths, distributed computing, assignment partitioning, optimisation, hybrid scheduling strategy, Metropolis light transport algorithm, bidirectional path tracing, mainframes, dynamic scheduling strategy, Tianhe-2 supercomputer, CPU cores, Partitioning algorithms, master-worker architecture, demand-driven assignment partitioning algorithms, mathematical model, static scheduling strategy, physically based parallel ray tracer, complex physical processes, subsnapshot]
An efficient implementation of PBKDF2 with RIPEMD-160 on multiple FPGAs
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
A weakness of many security systems is the strength of the chosen password or key derivation function. We show how FPGA technology can be used to effectively attack cryptographic applications with a password dictionary. We have implemented two independent PBKDF2 cores each using four HMAC cores with pipelines calculating a RIPEMD-160 hash to derive encryption keys together with one resource optimized AES-256 XTS core for direct decryption on a Xilinx Spartan6-LX150 FPGA. Our design targets TRUECRYPT containers, but may be applied to similar encryption tools with little adaption. In order to save resources and maximize speed, we have further optimized the RIPEMD-160 hash function for this purpose. Our design executed on the multi-FPGA system RIVYERA S6-LX150 containing 128 S6-LX150 FPGAs, finally reaches a peak performance of about 245,000 passwords per second.
[TRUECRYPT containers, field programmable gate arrays, reconfigurable high-performance computing, FPGA, Containers, Encryption, encryption keys, RIPEMD-160, field programmable gate array, HMAC cores, security systems, known-plaintext dictionary attack, Ciphers, cryptographic applications, AES-XTS, multiFPGA system, Tablet computers, cryptography, encryption tools, resource optimized AES-256 XTS core, password dictionary, FPGA technology, key derivation function, RIPEMD-160 hash function, direct decryption, Software, Xilinx Spartan6-LX150 FPGA, PBKDF2 cores, Field programmable gate arrays, RIVYERA S6-LX150, PBKDF2]
Layout-aware expandable low-degree topology
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
System expandability becomes a major concern for highly-parallel computers and datacenters, because their number of nodes gradually increases year by year. In this context we propose a low-degree expandable topology and its floor layout in which a cabinet or node set can be newly inserted by connecting short cables to a single existing cabinet. Our graph analysis shows that the proposed topology has low diameter, low average shortest path length and short aggregate cable length comparable to existing topologies with the same degree. When incrementally adding nodes and cabinets to the proposed topology, its diameter and average shortest path length increase modestly. Flit-level network simulation results show that the proposed topology has lower latency for three synthetic traffic patterns as expected from graph analysis. Our event-driven network simulation results show that the proposed topology provides a comparable performance to 2-D torus even for bandwidth-sensitive parallel applications.
[aggregate cable length, data centers, system expandability, small-world networks, interconnection networks, parallel processing, Network topology, Network expandability, event-driven network simulation, Labeling, high-performance computing, Context, bandwidth-sensitive parallel application, computer networks, telecommunication network topology, graph analysis, Routing, Supercomputers, Topology, computer centres, Aggregates, layout-aware expandable low-degree topology, path length, highly-parallel computers, network topologies]
Achieving cost effective cloud video services via fine grained multicore scheduling
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Cloud computing that possesses highly accessible and elastic computing resources perfectly matches the demands of video services, which employ massive storage and intensive computational power to store, transmit, compress, enhance, and analyze the videos, uploaded from commodity devices and surveillance cameras. However, most existing video processing programs are neither designed to run on parallel environments nor able to efficiently utilize the computational power of cloud platforms, which not only wastes the computing resources but also increases the cost of using cloud platforms. In this paper, we present three strategies to enhance the multicore utilization for video processing, namely producer-consumer model, intra-process overlapping, and inter-process overlapping. We experimented our strategies on a video enhancement program, which performs decoding, dehazing, and encoding, and the results showed the CPU utilization can be improved up to 31% for an 8 core instance, which can significantly reduce the cost in a long run.
[Cloud computing, fine grained multicore scheduling, processor scheduling, cost effective cloud video services, image enhancement, intra-process overlapping, cloud computing, commodity devices, cost reduction, data compression, Job shop scheduling, Multicore processing, massive storage, video encoding, parallel environments, video cameras, Encoding, cloud platforms, Decoding, video coding, video enhancement program, decoding, elastic computing resources, surveillance cameras, video processing programs, video decoding, CPU utilization, video dehazing, Streaming media, Central Processing Unit, inter-process overlapping, producer-consumer model]
pbitMCE: A bit-based approach for maximal clique enumeration on multicore processors
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Maximal clique enumeration (MCE) is a fundamental problem in graph theory. It plays a vital role in many network analysis applications and in computational biology. MCE is an extensively studied problem. Recently, Eppstein et al. proposed a state-of-the-art sequential algorithm that uses degeneracy based ordering of vertices to improve the efficiency. In this paper, we propose a new parallel implementation of the algorithm of Eppstein et al. using a new bit-based data structure. The new data structure not only reduces the working set size significantly but also by enabling the use of bit-parallelism improves the performance of the algorithm. We illustrate the significance of degeneracy ordering in load balancing and experimentally evaluate the impact of scheduling on the performance of the algorithm. We present experimental results on several types of synthetic and real-world graphs with up to 50 million vertices and 100 million edges. We show that our approach outperforms Eppstein et al.'s approach by up to 4 times and also scales up to 29 times when run on a multicore machine with 32 cores.
[Algorithm design and analysis, pbitMCE, computational biology, load balancing, Heuristic algorithms, graph theory, vertices, degeneracy based ordering, parallel algorithm, multicore machine, parallel processing, processor scheduling, multicore, resource allocation, bit-based approach, bit-based data structure, maximal clique enumeration, scheduling, data structures, maximal clique, network analysis applications, sequential algorithm, multiprocessing systems, Multicore processing, degeneracy, parallel implementation, bit-parallel, Data structures, Sockets, Memory management, bit-parallelism, multicore processors]
Lightweight online power monitoring and control for mobile applications
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Limited battery power has long been a challenge for mobile applications. As a result, the work in power monitoring and management has attracted great interests. In this paper, we propose a model to estimate power consumption of mobile applications at run-time, based on application-specific per-action power profiling. In addition, we have developed on-line optimization techniques which help maximize users' experience while conserving power. Our power model is lightweight and flexible, in that it can be used by any mobile applications as a plugin, and it can support user-defined optimization mechanisms. This approach has been evaluated using a case study, a mobile application for field studies, and the experimental results show that our model accurately captures power consumption of the application, and the model can be used to optimize the power consumption based on users' needs.
[Energy consumption, Power demand, mobile application, Mobile communication, Batteries, on-line optimization technique, Optimization, lightweight online power monitoring, power consumption estimation, mobile computing, optimisation, power aware computing, application-specific per-action power profiling, Probability density function, user-defined optimization mechanism, Mathematical model]
A fine-grained indoor localization using multidimensional Wi-Fi fingerprinting
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Although fingerprint based localization is promising for indoor applications, its accuracy still remains a huge challenge. Most of existing approaches rely on the Radio Signal Strength (RSS) to generate fingerprints. However, merely using RSS is unable to accurately localize objects since such an one-dimensional fingerprint will be seriously influenced by the interference and multi-path effect in the indoor environment. In this paper, we propose a new localization approach based on multidimensional Wi-Fi fingerprint. Instead of only using RSS to construct fingerprint, we employ RSS, transmitted power, and channel information to construct an integrated fingerprint. The extended fingerprint enables fine-grained localization and tracking services. We also deign a cosine similarity based matching algorithm and enhanced particle filter mechanism to achieve accurate localization and tracking. Extensive experiment and implementation results show that the new fingerprint and proposed algorithms can achieve an accuracy within two meters in 90% of testing points, while demonstrating a good adaptability to complex indoor environments.
[fingerprint based localization, IEEE 802.11 Standards, Indoor environments, Estimation, particle filtering (numerical methods), particle filter, one-dimensional fingerprint, Global Positioning System, Wireless communication, Accuracy, Databases, RSS, fine-grained indoor localization, radio signal strength, indoor radio, wireless LAN, multidimensional Wi-Fi fingerprinting]
BusCast: Flexible and privacy preserving message delivery using urban buses
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
With the popularity of intelligent mobile devices, enormous urban information has been generated and required by the public. In response, ShanghaiGrid (SG) aims to providing abundant information services to the public. With fixed schedule and urban-wide coverage, an appealing service in SG is to provide free message delivery service to the public using buses, which allows mobile device users to send messages to locations of interest via buses. The main challenge in realizing this service is to provide efficient routing scheme with privacy preservation under highly dynamic urban traffic condition. In this paper, we present an innovative scheme BusCast to tackle this problem. In BusCast, buses can pick up and forward personal messages to their destination locations in a store-carry-forward fashion. For each message, BusCast conservatively associates a routing graph rather than a fixed routing path with the message in order to adapt the dynamic of urban traffic. Meanwhile, the privacy information about the user and the message destination is concealed from both intermediate relay buses and outside adversaries. Both rigorous privacy analysis and extensive trace-driven simulations demonstrate the efficacy of BusCast scheme.
[privacy preserving message delivery, anonymous communication, Relays, message destination, routing scheme, traffic analysis attacks, Bismuth, vehicular networks, ShanghaiGrid, BusCast scheme, privacy information, urban bus, Routing, routing graph, traffic engineering computing, privacy analysis, trace-driven simulation, transportation, backward unlinkability, message delivery, urban traffic condition, data privacy, Delays, information service, intelligent mobile device]
A distributed spectrum sharing algorithm in cognitive radio networks
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
In this paper we study a social welfare maximization problem for spectrum sharing in cognitive radio networks. To fully use the spectrum resource, the spectrum owned by the licensed primary user (PU) can be leased to secondary users (SUs) for transmitting data. We first formulate the social welfare of a cognitive radio network, considering the cost for the primary user sharing spectrum and the utility gained for secondary users transmitting data. The social welfare maximization is a convex optimization, which can be solved by standard methods in a centralized manner. However, the utility function of each secondary user always contains the private information, which leads to the centralized methods disabled. To overcome this challenge, we propose an iterative distributed algorithm based on a pricing-based decomposition framework. It is theoretically proved that our proposed algorithm converges to the optimal solution. Numerical simulation results are presented to show that our proposed algorithm achieves optimal social welfare and fast convergence speed.
[iterative methods, radio networks, distributed spectrum sharing algorithm, spectrum resource, Decomposition, Convergence, cognitive radio networks, optimisation, cognitive radio, optimization, Bandwidth, Pricing, Cost function, secondary users, Distributed algorithms, numerical simulation, radio spectrum management, iterative distributed algorithm, social welfare maximization, Cognitive radio, licensed primary user, pricing-based decomposition framework, convex optimization, Resource management, cognitive radio network, social welfare maximization problem]
Enhancing scalability in distributed storage systems with Cauchy Reed-Solomon codes
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
System scaling becomes essential and indispensable for distributed storage systems due to the explosive growth of data volume. As fault-protection is also a necessity in large-scale distributed storage systems, and Cauchy Reed-Solomon (CRS) codes are widely deployed to tolerate multiple simultaneous node failures, this paper studies the scaling of distributed storage systems with CRS codes. In particular, we formulate the scaling problem with an optimization model in which both the post-scaling encoding matrix and the data migration policy are assumed to be unknown in advance. To minimize the I/O overhead for CRS scaling, we first derive the optimal post-scaling encoding matrix under a given data migration policy, and then optimize the data migration process using the selected postscaling encoding matrix. Our scaling scheme requires the minimal data movement while achieving uniform data distribution. To validate the efficiency of our scheme, we implement it atop a networked file system. Extensive experiments show that our scaling scheme reduces 7.94% to 58.87%, and 39.52% on average, of the scaling time over the basic scheme.
[Algorithm design and analysis, Strips, reliability, uniform data distribution, CRS code, optimal post-scaling encoding matrix, data migration, Optimization, scalability, fault-protection, Reed-Solomon codes, large-scale distributed storage system, data migration policy, optimisation, Cauchy Reed-Solomon, Libraries, postscaling encoding matrix, scaling, optimization model, performance evaluation, Encoding, matrix algebra, encoding matrix, Layout, I-O overhead minimization, cauchy Reed-Solomon code, multiple simultaneous node failure]
Optimising memory management for Belief Propagation in Junction Trees using GPGPUs
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Belief Propagation (BP) in Junction Trees (JT) is one of the most popular approaches to compute posteriors in Bayesian Networks (BN). Such approach has significant computational requirements that can be addressed by using highly parallel architectures (i.e., General Purpose Graphic Processing Units) to parallelise the message update phases of BP. In this paper, we propose a novel approach to parallelise BP with GPGPUs, which focuses on optimising the memory layout of the BN tables so to achieve better performance in terms of increased speedup, reduced data transfers between the host and the GPGPU, and scalability. Our empirical comparison with the state of the art approach on standard datasets confirms significant improvements in speedups (up to +594%), and scalability (as our method can operate on networks whose potential tables exceed the global memory of the GPGPU).
[Algorithm design and analysis, GPGPUs, belief propagation, memory layout optimization, Instruction sets, Particle separators, belief maintenance, parallel architecture, Indexes, general purpose graphics processing unit, BN, graphics processing units, GPGPU, BP, junction trees, storage management, memory management, Belief Propagation on Junction Trees, Memory management, Parallel processing, belief networks, Junctions, Bayesian network]
Toward multi-target autotuning for accelerators
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Producing high-performance implementations from simple, portable computation specifications is a challenge that compilers have tried to address for several decades. More recently, a relatively stable architectural landscape has evolved into a set of increasingly diverging and rapidly changing CPU and accelerator designs, with the main common factor being dramatic increases in the levels of parallelism available. The growth of architectural heterogeneity and parallelism, combined with the very slow development cycles of traditional compilers, has motivated the development of autotuning tools that can quickly respond to changes in architectures and programming models, and enable very specialized optimizations that are not possible or likely to be provided by mainstream compilers. In this paper we describe the new OpenCL code generator and autotuner OrCL and the introduction of detailed performance measurement into the autotuning process. OrCL is implemented within the Orio autotuning framework, which enables the rapid development of experimental languages and code optimization strategies aimed at achieving good performance on new platforms without rewriting or hand-optimizing critical kernels. The combination of the new OpenCL autotuning and TAU measurement capabilities enables users to consistently evaluate autotuning effectiveness across a range of architectures, including several NVIDIA and AMD accelerators and Intel Xeon Phi processors, and to compare the OpenCL and CUDA code generation capabilities. We present results of autotuning several numerical kernels that typically dominate the execution time of iterative sparse linear system solution and key computations from a 3-D parallel simulation of solid fuel ignition.
[Performance evaluation, parallelism level, Graphics processing units, GPUs, architectural heterogeneity, TAU, parallel processing, program compilers, code optimization strategy, Optimization, autotuning tool, multitarget autotuning, Computer architecture, architectural parallelism, Hardware, AMD accelerator, Kernel, high-performance computing, NVIDIA accelerator, Orio autotuning framework, autotuning, Generators, Intel Xeon Phi processor, accelerator design, accelerators, OpenCL]
Accelerating HPCG on Tianhe-2: A hybrid CPU-MIC algorithm
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
In this paper, we propose a hybrid algorithm to enable and accelerate the High Performance Conjugate Gradient (HPCG) benchmark on a heterogeneous node with an arbitrary number of accelerators. In the hybrid algorithm, each subdomain is assigned to a node after a three-dimensional domain decomposition. The subdomain is further divided to several regular inner blocks and an outer part with a flexible inner-outer partitioning strategy. Each inner task is assigned to a MIC device and the size is adjustable to adapt the accelerator's computational power. The only outer part is assigned to CPU and the thickness of boundary size is also adjustable to maintain load balance between CPU and MICs. By properly fusing the computational kernels with preceding ones, we present an asynchronous data transfer scheme to better overlap local computation with the PCI-express data transfer. All basic HPCG kernels, especially the time-consuming sparse matrix-vector multiplication (SpMV) and the symmetric Gauss-Seidel relaxation (SymGS), are extensively optimized for both CPU and MIC, on both algorithmic and architectural levels. On a single node of Tianhe-2 which is composed of an Intel Xeon processor and three Intel Xeon Phi coprocessors, we successfully obtain an aggregated performance of 50.2 Gflops, which is around 1.5% of the peak performance.
[SpMV, Multigrid V-cycle, load balancing, Instruction sets, central processing unit, coprocessors, inner-outer partitioning strategy, Sparse matrices, parallel processing, many integrated core, HPCG, Optimization, Intel Xeon Phi coprocessor, symmetric Gauss-Seidel relaxation, Microwave integrated circuits, Intel Xeon processor, resource allocation, PCI-express data transfer, SymGS, high performance conjugate gradient, Kernel, Red-black symmetric Gauss-Seidel, multiprocessing systems, Conjugate Gradient, Tianhe-2, sparse matrix-vector multiplication, hybrid CPU-MIC algorithm, HPCG benchmark, peripheral component interconnect, Data transfer, asynchronous data transfer scheme, Heterogeneous computing, Coprocessors, three-dimensional domain decomposition]
Continuous similarity join on data streams
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Similarity join plays an important role in many applications, such as data cleaning and integration, to address the poor data quality problem. Most of the existing studies focused on performing similarity join on static datasets but few studies realized running it on dynamic data streams. With the development of network technology, the data accessing paradigm has transferred from disk-oriented mode to online data streams, which makes performing similarity join in continuous query on data streams become a novel query processing paradigm. Different from static dataset, data stream is unbounded, continuous and unpredictable. The significant differences pose serious challenges, such as real-time query performance. To this end, we study the problem of continuous similarity join on data streams in this paper, which is based on edit distance metric and filter-and-verify framework with sliding-window semantics. Two subcases of this problem are studied, including self similarity join on a single data stream and similarity join on two streams. We introduced the basic window based sliding window model to facilitate the update of sliding window and its index. More details of our method, including signature extraction schemes, filtering and verification algorithms, re-evaluation strategies are discussed respectively. Finally, extensive experimental results show that our method works efficiently on real data streams.
[continuous query, real-time query performance, Heuristic algorithms, query processing, filter-and-verify framework, data accessing paradigm, similarity join, Filtering algorithms, Real-time systems, verification algorithm, disk-oriented mode, continuous similarity join, edit distance, Filtering, signature extraction scheme, sliding-window semantics, data stream, sliding window, edit distance metric, data quality problem, basic window based sliding window model, reevaluation strategy, Query processing, static dataset, data cleaning, network technology, dynamic data stream, query processing paradigm, string matching, filtering algorithm, data integration, online data stream, Indexing]
A fully generalized over operator with applications to image composition in parallel visualization for big data science
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
The over operator is commonly used for &#x03B1;-blending in various visualization techniques. In the current form, it is a binary operator and must respect the restriction of order dependency, hence posing a significant performance limit. This paper proposes a fully generalized version of this operator. Compared with its predecessor, the fully generalized over operator is not only n-operator compatible but also any-order friendly. To demonstrate the advantages of the proposed operator, we apply it to the asynchronous and order-dependent image composition problem in parallel visualization for big data science and further parallelize it for performance improvement. We conduct theoretical analyses to establish the performance superiority of the proposed over operator in comparison with its original form, which is further validated by extensive experimental results in the context of real-life scientific visualization.
[Algorithm design and analysis, Availability, image processing, real-life scientific visualization, parallel visualization, big data science, Pipelines, Color, performance superiority, Big Data, image composition, performance limit, performance improvement, &#x03B1;-blending, order dependency, Data visualization, data visualisation, Bismuth, order-dependent image composition problem, Parallel visualization, Big data, binary operator, big data, asynchronous image composition problem]
Using surrogate-based modeling to predict optimal I/O parameters of applications at the extreme scale
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
On petascale systems, the selection of optimal values for I/O parameters without taking into account the I/O size and pattern can cause the I/O time to dominate the simulation time, compromising the application's scalability. In this paper, we adopt and adapt an engineering method called surrogate-based modeling to efficiently search for the optimal I/O parameter values and accurately predict the associated I/O times at the extreme scale. Our approach allows us to address both the search and prediction in a short time, even when the application's I/O is large and exhibits irregular patterns.
[Computational modeling, Scientific applications, I/O modeling and tuning, Predictive models, Peta- and exascale computing, input-output programs, Response surface methodology, optimal I/O parameter values, parallel processing, petascale systems, Runtime, QMCPack, surrogate-based modeling, I/O times, Polynomials, Data models, Irregular I/O pattern, Kernel, engineering method, exascale computing]
Simplifying index file structure to improve I/O performance of parallel indexing
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Complex indexing techniques are needed to reduce the time of analyzing massive scientific datasets, but generating these indexing data structures can be very time consuming. In this work, we propose a set of strategies to simplify the index file structure and to improve the I/O performance during index construction using FastQuery, which is a parallel indexing and querying system for scientific data. FastQuery has been used to analyze data from various scientific applications, including a trillion plasma particles simulation. To accelerate query process, FastQuery uses FastBit to build indexes, and then stores the indexes into file system through parallel scientific data format libraries, such as HDF5. Although these data format libraries are designed to support more complex multi-dimensional arrays, we observed that it still takes considerable work to map the indexing data structures into arrays, especially on parallel machines. To address this problem, in this paper, we attempt to minimize the I/O time by storing indexes into our self-defined binary data format. By fully controlling the data structure, we can minimize the I/O synchronization overhead and explore more efficient I/O strategy for storing indexes. Our experiments of indexing a trillion particle dataset using 20,000 cores of a supercomputer show that the proposed binary I/O driver can reach 85% of the peak I/O bandwidth on the system, and achieves a speedup of up to 4X in terms of the total execution time comparing to the previous FastQuery implementation with HDF5 I/O driver.
[data analysis, indexing, I/O performance improvement, HDF5 I/O driver, Storage system, input-output programs, parallel processing, parallel indexing, query processing, parallel machine, querying system, Bitmap indexing, Bandwidth, Writing, Parallel I/O, Libraries, data structures, Arrays, index file structure simplification, Indexing, FastQuery]
LEB-MAC: Load and energy balancing MAC protocol for energy harvesting powered wireless sensor networks
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Ambient energy from solar, vibration, heat and wind provide alternative energy sources to power sensors and extend the lifetime of wireless sensor networks which have traditionally been powered by batteries. This paper aims to enhance the performance of energy harvesting powered wireless sensor networks in three aspects: relaying, scheduling, and medium access control. To better adapt to the characteristics of energy harvesting, an asynchronous receiver-initiated duty-cycling approach is preferred in energy harvesting powered wireless sensor networks. This reduces the duty cycle of senders, and regulates the active and sleep intervals according to the energy levels of sensors. When nodes run out of power and need time to recharge, network holes or voids develop, forcing data packets to be routed via other paths, like detours. The proposed relaying strategy aims to prevent holes by balancing the load across the network according to nodes' energy harvesting characteristics. This is a natural consequence of the asynchronous duty cycling by scheduling transmission based on the receiver's availability. The simulation results show that our scheme outperforms in terms of sender duty cycle, end-to-end delay and delivery ratio, especially in challenged conditions where other protocols fail.
[Schedules, wireless sensor networks, load balancing, wind energy source, relaying strategy, telecommunication scheduling, vibration energy source, access protocols, availability, Energy harvesting, end-to-end delay, resource allocation, asynchronous receiverinitiated duty-cycling approach, energy balancing, data packet, Energy states, energy harvesting powered wireless sensor network, scheduling transmission, sender duty cycle, Receivers, power sensor, energy harvesting, medium access control, radio receivers, MAC, Wireless sensor network, Wireless sensor networks, heat energy source, LEB-MAC protocol, routing protocols, solar energy source, Media Access Protocol, asynchronous duty cycling]
Real-time and passive wormhole detection for wireless sensor networks
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Wormhole attack is one of the severe threats to wireless sensor and ad hoc networks. Most of the existing countermeasures either require specialized hardware or demand high network overheads in order to capture the specific symptoms induced by the wormholes, which in result, limits their applicability. In this paper, we exploit an inevitable symptom of wormholes and present Pworm, a passive wormhole detection and localization system based upon the key observation that a large amount of network traffic will be attracted by the wormholes. The proposed passive and real-time scheme silently observes the variations in network topology to infer the wormhole existence. Our approach relies solely on network routing information and does not necessitate specialized hardware or poses rigorous assumptions on network features. We evaluate our system performance through extensive simulations of 100 to 500 nodes for various network scales and show that Pworm is well suited for false alarms, scalability and time delay.
[radio direction-finding, wireless sensor networks, wireless sensor network, passive wormhole attack detection, Wormhole Attack, scalability, Network topology, sensor placement, telecommunication network reliability, Real-time systems, Hardware, Passive Detection, Pworm system, network routing, localization system, ad hoc network, false alarm, telecommunication network topology, Routing, Ad hoc networks, time delay, Topology, network topology, Wireless Sensor Networks, network traffic, Wireless sensor networks, ad hoc networks, telecommunication traffic]
Optimal bandwidth allocation with dynamic multi-path routing for non-critical traffic in AFDX networks
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Avionic networks exerting the Avionics Full-Duplex Switched Ethernet (AFDX) protocol utilize a small amount of the bandwidth to transmit critical traffics. As there is an increasing demand on data exchange for non critical applications, it is of great interest to make use of the physically available capability of the network through optimal bandwidth allocation. In this paper, the problem of bandwidth allocation in AFDX networks is treated in the framework of Network Utility Maximization (NUM). In the present work, multi-path routing is used for non-critical applications to explore the available bandwidth and to improve system performance. The optimization problem is decomposed into a rate update subproblem and a traffic routing subproblem linked together by a pricing dynamic system. A distributed algorithm for bandwidth allocation with multi-path routing is developed and the convergence of the algorithm is proven using Lyapunov stability theory. Some issues related to the implementation of the devolved algorithm in the context of real AFDX networks are addressed and the corresponding solutions are provided. Finally, TrueTime based simulations conform the viability and the applicability of the proposed approach.
[optimization problem, Protocols, data exchange, Avionics Full-Duplex Switched Ethernet (AFDX), Heuristic algorithms, optimal bandwidth allocation, Optimization, optimisation, Bandwidth, dynamic multipath routing, avionics full-duplex switched Ethernet, rate update subproblem, Bandwidth Allocation, AFDX networks, NUM, Virtual Link (VL), avionic networks, Routing, multipath channels, avionics, bandwidth allocation, AFDX protocol, telecommunication network routing, traffic routing subproblem, network utility maximization, Channel allocation, noncritical traffic, Lyapunov stability, Network Utility Maximization (NUM), Delays, telecommunication traffic, Lyapunov methods]
Nowhere to hide: An empirical study on hidden UHF RFID tags
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Radio Frequency Identification (RFID) techniques are widely used in many ubiquitous applications. The most important usage of RFID techniques is to read the tags within a reader's interrogation area such that the objects attached with those tags can be identified. In real practice, it is common that a tag is physically in the interrogation range, but cannot be read by the reader, due to the multipath effect and other interference. This phenomenon, namely the hidden tag problem, is a big challenge to achieve high identification rate. To address this problem, most prior works depend on empirical or measurement-based methods to tune the transmission power for readers. Such a case-by-case solution is impractical for generic implementation. In this paper, we theoretically and experimentally explore the reasons why hidden tag problem occurs. To alleviate its impact, we propose a unified and measurable model, PAL, to formulate this problem and its impact. Different from previous works, our solution is generic and fully compatible with existing EPC C1G2 protocol. The analysis and measurement based on our model can help to design and deploy RFID systems with high identification rate.
[empirical-based method, radiofrequency identification, measurement-based method, Interference, radiotelemetry, RFID, hidden UHF RFID tag, reader interrogation area, radiofrequency identification technique, multipath effect, radiofrequency interference, Hidden Tag, hidden tag problem, Receiving antennas, Directive antennas, Empirical Study, ubiquitous application, PAL, EPC C1G2 protocol, interference, Mathematical model, Impedance, protocols, Radiofrequency identification]
Optimizing Seam Carving on multi-GPU systems for real-time image resizing
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Image resizing is increasingly important for picture sharing and exchanging between various personal electronic equipments. Seam Carving is a state-of-the-art approach for effective image resizing because of its content-aware characteristic. However, complex computation and memory access patterns make it time-consuming and prevent its wide usage in real-time image processing. To address these problems, we propose a novel algorithm, called Non-Cumulative Seam Carving (NCSC), which removes main computation bottleneck. Furthermore, we also propose an adaptive multi-seam algorithm for better parallelism on GPU platforms. Finally, we implement our algorithm on a multi-GPU platform. Results show that our approach achieves a maximum 140&#x00D7; speedup on a two-GPU system over the sequential version. It only takes 0.11 second to resize a 1024&#x00D7;640 image by half in width compared to 15.5 seconds with the traditional seam carving.
[image processing, NCSC, Image recognition, memory access patterns, real-time image processing, picture exchanging, parallelism, picture sharing, personal electronic equipments, adaptive multiseam algorithm, real-time image resizing, graphics processing units, parallel processing, noncumulative seam carving, multiGPU systems, Streaming media, content-aware characteristic]
A fast integral image generation algorithm on GPUs
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Integral image, also known as summed area table is a two-dimensional table generated from an input image. Each entry in the table stores the sum of all pixels which locate on the top-left corner of the entry in the input image. Integral image is a very popular and important algorithm in computer vision and computer graphics applications. Especially in real-time computer vision, it is usually used to accelerate calculating the sum of a rectangular area. Integral image algorithm is memory-bounded. There are two typical existed image integral algorithms on GPUs. The first is the Scan-Scan algorithm. The second is the Scan-Transpose-Scan algorithm, which adopts three steps to generate the integral image. The first and the third steps are scan. In order to achieve coalesced global memory access in the third step, a transpose step is added. In this paper, we propose a novel blocked integral algorithm, which has three stages. The first stage is intra-block reduction. The second stage is auxiliary matrix scan and the third stage is intra-block scan. Compared with the Scan-Scan algorithm, our proposed scheme reduces the global memory accesses. At the same time, less local synchronizations and less load imbalance are achieved. Compared with the Scan-Transpose-Scan algorithm, our proposed algorithm only needs about half of the global memory accesses. At the same time, coalesced memory access is achieved. We implemented these three algorithms with OpenCL so that they can run on both Nvidia and AMD GPUs. We also designed an auto-tuning framework to search optimal parameters for different size of input matrix on those two platforms. The experiment result shows that our proposed algorithm gets the best performance compared with the two existed typical integral algorithms.
[Algorithm design and analysis, fast integral image generation algorithm, Instruction sets, intra-block scan stage, blocked integral algorithm, Graphics processing units, scan-scan algorithm, auto-tuning framework, GPU, Nvidia GPU, two-dimensional table, computer graphics application, Integral image, scan-transpose-scan algorithm, Kernel, AMD GPU, graphics processing units, Memory management, computer vision, graphics processing unit, Less Global Memory access, OpenCL, Arrays, intra-block reduction stage, Blocked, Manganese]
WorkQ: A many-core producer/consumer execution model applied to PGAS computations
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Partitioned global address space (PGAS) applications, such as the Tensor Contraction Engine (TCE) in NWChem, often apply a one-process-per-core mapping in which each process iterates through the following work-processing cycle: (1) determine a work-item dynamically, (2) get data via one-sided operations on remote blocks, (3) perform computation on the data locally, (4) put (or accumulate) resultant data into an appropriate remote location, and (5) repeat the cycle. However, this simple flow of execution does not effectively hide communication latency costs despite the opportunities for making asynchronous progress. Utilizing nonblocking communication calls is not sufficient unless care is taken to efficiently manage a responsive queue of outstanding communication requests. This paper presents a new runtime model and its library implementation for managing tunable &#x201C;work queues&#x201D; in PGAS applications. Our runtime execution model, called WorkQ, assigns some number of on-node &#x201C;producer&#x201D; processes to primarily do communication (steps 1, 2, 4, and 5) and the other &#x201C;consumer&#x201D; processes to do computation (step 3); but processes can switch roles dynamically for the sake of performance. Load balance, synchronization, and overlap of communication and computation are facilitated by a tunable nodewise FIFO message queue protocol. Our WorkQ library implementation enables an MPI+X hybrid programming model where the X comprises SysV message queues and the user's choice of SysV, POSIX, and MPI shared memory. We develop a simplified software mini-application that mimics the performance behavior of the TCE at arbitrary scale, and we show that the WorkQ engine outperforms the original model by about a factor of 2. We also show performance improvement in the TCE coupled cluster module of NWChem.
[MPI+X hybrid programming model, runtime model, cluster module, PGAS, Engines, software libraries, FIFO message queue protocol, one-process-per-core mapping, Runtime, PGAS computations, resource allocation, many-core producer/consumer execution model, WorkQ engine, MPI shared memory, shared memory systems, Libraries, protocols, on-node producer, Load modeling, message passing, consumer processes, Computational modeling, WorkQ library implementation, Tensor Contractions, tunable work queues, synchronisation, POSIX, Tensile stress, NWChem, Quantum Chemistry, Performance Evaluation, tensor contraction engine, SysV message queues, Producer/Consumer, runtime execution model, work-processing cycle, TCE, partitioned global address space, synchronization, load balance, Global Arrays, Electronics packaging]
Hybrid-view programming of nuclear fusion simulation code in the PGAS parallel programming language XcalableMP
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Recently, the Partitioned Global Address Space (PGAS) parallel programming model has emerged as a usable distributed memory programming model. XcalableMP (XMP) is a PGAS parallel programming language that extends base languages such as C and Fortran with directives in OpenMP-like style. XMP supports a global-view model that allows programmers to define global data and to map them to a set of processors, which execute the distributed global data as a single thread. In XMP, the concept of a coarray is also employed for local-view programming. In this study, we port Gyrokinetic Toroidal Code - Princeton (GTC-P), which is a three-dimensional PIC code developed at Princeton University to study the microturbulence phenomenon in magnetically confined fusion plasmas, to XMP as an example of hybrid memory model coding with the global-view and local-view programming models. In local-view programming, the coarray notation is simple and intuitive compared with Message Passing Interface (MPI) programming while the performance is comparable to that of the MPI version. Thus, because the global-view programming model is suitable for expressing the data parallelism for a field of grid space data, we implement a hybrid-view version using a global-view programming model to compute the field and a local-view programming model to compute the movement of particles. The performance is degraded by 5-25% compared with the original MPI version, but the hybrid-view version facilitates more natural data expression for static grid space data (in global-view model) and dynamic particle data (in local-view model), and it also increases the readability of the code for higher productivity.
[XcalableMP language, Gyrokinetic Toroidal Code - Princeton, nuclear engineering computing, MPI, Programming, Nuclear Fusion Simulation, programming languages, C language, parallel programming, Program processors, Partitioned Global Address Space, Distributed databases, hybrid-view programming, nuclear fusion simulation code, message passing, message passing interface, Computational modeling, Toroidal magnetic fields, PIC code, local-view programming, global-view programming, PGAS parallel programming language, XcalableMP, GTC-P, nuclear fusion, partitioned global address space, Data models, Arrays, distributed memory programming model, Fortran language]
GPU acceleration of finding frequent patterns over large biological sequence
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Biological frequent patterns usually correspond to the important function (or structure) in biological sequences. Along with the rapid growth of biological sequences, it is significant to find frequent patterns over a large bio-sequence efficiently. However, most of existing algorithms need to produce lots of short patterns or projected databases, which influence the efficiency badly and also increase the cost of space. Graphics processing units (GPUs) embracing many core computing devices, have been extensively applied to accelerate computation performance in many areas. In order to meet the demand of biologists, we redefine the frequent pattern problem with length constraints for finding frequent patterns. We present pruning optimization method for the serial algorithm (POSA), and based on this technique, we propose a parallel algorithm (POPA) which not only reduces the time complexity with a low space cost but also obtains better performance on CUDA. To validate the presented algorithms, we implemented the algorithms on multiple-core CPU and various GPU devices. Also, CUDA optimization techniques are applied to speed up calculation in the paper. Finally, experimental results show that compared with the serial algorithm on CPU with six cores, POSA achieves 1.2~4.5 speedup, and POPA gains 3~20 speedup.
[POPA parallel algorithm, CUDA optimization technique, Graphics processing units, Biology, Data mining, Parallel algorithms, GPU acceleration, pruning optimization method for the serial algorithm, Large Biological Sequence, biological frequent pattern, Frequent Pattern, parallel algorithms, pattern classification, POSA, time complexity, Indexes, graphics processing units, CUDA, biological sequence, Compute Unified Device Architecture, many core computing device, bioinformatics, graphics processing unit, Arrays, Acceleration]
Improving large-scale storage system performance via topology-aware and balanced data placement
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
With the advent of big data, the I/O subsystems of large-scale compute clusters are becoming a center of focus. More applications are putting greater demands on end-to-end I/O performance. These subsystems are often complex in design. They comprise of multiple hardware and software layers to cope with the increasing capacity, capability, and scalability requirements of data intensive applications. However, the sharing nature of storage resources and the intrinsic interactions across these layers make it a great challenge to realize end-to-end performance gains. This paper proposes a topology-aware strategy to balance the load across resources, to improve the per-application I/O performance. We demonstrate the effectiveness of our algorithm on an extreme-scale compute cluster, Titan, at the Oak Ridge Leadership Computing Facility (OLCF). Our experiments with both synthetic benchmarks and a real-world application show that, even under congestion, our proposed algorithm can improve large-scale application I/O performance significantly, resulting in both a reduction in application run time as well as a higher resolution of simulation run.
[High Performance Computing, OLCF, Switches, topology-aware data placement, Parallel File System, Routing, input-output programs, Indexes, balanced data placement, Frequency control, large-scale systems, storage management, Performance Evaluation, resource allocation, extreme-scale compute cluster, large-scale storage system performance, Benchmark testing, Libraries, Oak Ridge Leadership Computing Facility, Storage Area Network, Resource management, per-application I/O performance, Titan]
MapReduce based parallel suffix tree construction for human genome
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Genome indexing is the basis for many bioinformatics applications. Read mapping(sequence alignment) is one such application where the goal is to align millions of short reads against reference genome. Several tools are available for read mapping which rely on different indexing techniques to expedite the alignment process. However, many of these contemporary alignment programs are sequential, memory intensive and cannot be easily scaled for larger genomes. Suffix tree is one of the most widely used data structures for indexing strings (genomes). Building a scalable suffix-tree based tool is particularly challenging due to the difficulties involved in parallel construction of the suffix tree. Several suffix tree construction techniques have been proposed till date with focus on space-time tradeoff. Most of these existing works address the construction issue for uniprocessor and cannot be easily extended to utilize modern multi-processor systems. In this paper we investigate and propose a MapReduce based parallel construction of suffix tree. We demonstrate the performance of the algorithm over commodity cluster using up to 32 nodes each having 8GB of primary memory.
[map-reduce, Genomics, Random access memory, data structure, commodity cluster, parallel processing, read mapping application, uniprocessor system, parallel, genome indexing technique, genomics, data structures, Bioinformatics, bioinformatics application, indexing, Buildings, trees (mathematics), Data structures, Partitioning algorithms, MapReduce based parallel suffix tree construction, space-time tradeoff, genome, sequence alignment application, bioinformatics, multiprocessor system, human genome, suffix tree, Indexing]
Performance evaluation of OpenDaylight SDN controller
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Enourmous amount of data has resulted into large data centres. Virtual machines and virtual networks are an integral part of large data centres. As a result, software defined network controllers have emerged as viable solution to manage such networks. The performance analysis of network controllers is generally carried out through benchmarking. Although several benchmarking studies exist, recently launched OpenDaylight SDN Controller is not considered in any benchmarking study yet. In this paper, we present initial results of benchmarking of Open-Daylight SDN Controller with well known Floodlight controller. We present latency and throughput results of OpenDaylight SDN Controller and Floodlight under different scenarios. We find that OpenDaylight SDN Controller has low average responses as compared to Floodlight. We also note that the standard benchmarking tool - Cbench - has no support for real traffic patterns in a data centre, since data centre traffic is considerably complex. In addition to benchmarking of OpenDaylight SDN Controller, we propose modifications in Cbench to accommodate models of real traffic in data centres. We also discuss our initial implementation.
[Protocols, data centre traffic, Switches, OpenFlow, Throughput, large data centre, Benchmark testing, OpenDaylight SDN controller, Java, telecommunication control, Cbench, software defined networking, performance evaluation, virtual network, benchmarking tool, Benchmarking, Floodlight controller, real traffic pattern, computer centres, computer network performance evaluation, OpenDaylight, virtual machine, virtual machines, Software, software defined network controller, Software Defined Networking, telecommunication traffic, performance analysis]
ConSnap: Taking continuous snapshots for running state protection of virtual machines
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
The reliability of data and services hosted in a virtual machine (VM) is a top concern in cloud computing environment. Continuous snapshots reduces the data loss in case of failures, and thus is prevailing for providing protection for long-running systems. However, existing methods suffer from long VM downtime, long snapshot interval and significant performance overhead. In this paper, we present ConSnap, a system designed to enable taking fine-grained continuous snapshots of virtual machines without compromising VM performance. First, ConSnap adopts the COW (copy-on-write) manner to save the memory pages in a lazy way, and thus decrease the snapshot interval to dozens of milliseconds. Second, we only save the incremental memory pages on the basis of the last snapshot in each epoch to reduce the snapshot duration, and thus mitigate VM performance loss. Third, we propose a multi-granularity space reclamation strategy, which merges the unused snapshot files to achieve storage space saving, as well as fast recovery. We have implemented ConSnap on QEMU/KVM and conducted several experiments to verify its effectiveness. Compared with the stop-and-copy based incremental snapshots, ConSnap reduces the performance loss by 71.1% ~ 10.2% under Compilation workload, and 14.5% ~ 4.7% for the Ftp workload, when the interval varies from 1s to 60s.
[KVM, Instruction sets, Aerospace electronics, Degradation, fine-grained continuous snapshots, storage space saving, copy-on-write, Bandwidth, data protection, QEMU, cloud computing, copy-on-write manner, multigranularity space reclamation strategy, running state protection, VM performance, Virtual machining, incremental memory pages, ConSnap, space reclamation, long-running systems, virtual machine, virtual machines, COW manner, Virtualization, time 1 s to 60 s, continuous snapshots]
HARP: Towards enhancing data recency for eventually consistent data stores
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
To attain high performance and remain available during network partitions or node failures, modern distributed systems often sacrifice recency guarantees, which can provide a uniform view on recent versions of data items for different clients. In this work, we consider the problem of increasing the probability of data recency while preserving low response latency and maintaining high availability on top of an eventually consistent data store. To solve the problem, we propose HARP, an approach that can enhance data recency in a highly available way. Based on HARP, we implement an agent layer to detect stale reads and resolve the conflicts, and by leveraging widely deployed data store technologies, we build a data storage system. We compare the prototype system to Cassandra, and experimentally prove that our method produces low overhead (less than 10%) based on the eventually consistent configuration and, for most workloads, achieves better performance than the Cassandra's strong &#x201C;read your writes&#x201D; configurations.
[Availability, storage allocation, stale read detection, client-server systems, probability, Cassandra system, overhead, low-response latency preservation, recency guarantees, data recency probability, data recency enhancement, eventually-consistent data stores, Semantics, agent layer, Distributed databases, Null value, HARP, network partitioning, distributed systems, Data models, Safety, node failures, Clocks]
Combination feature for image retrieval in the distributed datacenter
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Since the emergence of cloud datacenters provides an enormous amount of resources easily accessible to people, it is challenging to provide an efficient search framework in such a distributed environment. However, traditional search techniques only allow users to search images over exact-match keywords through a centralized index. These methods are insufficient to meet requirements of content based image retrieval (CBIR) and more powerful search frameworks are needed. In this paper, we present LCFIR, an effective image retrieval framework for fast content location in the distributed situation. It adopts the peer-to-peer paradigm and combines color and edge features. The basic idea is to construct multiple replicas of an image's index through exploiting the property of Locality Sensitive Hashing (LSH). Thus, the indexes of similar images are probabilistically gathered into the same node without the knowledge of any global information. The empirical results show that the system is able to yield high accuracy with load balancing, and only contacts a few number of the participating nodes.
[empirical analysis, Cloud computing, Combination feature, load balancing, color features, distributed environment, content location, edge features, Content based image retrieval, Locality sensitive hashing, Image color analysis, resource allocation, feature extraction, LCFIR, CBIR, Peer-to-peer, edge detection, image colour analysis, cloud computing, LSH, Image edge detection, Image retrieval, locality sensitive hashing, cloud datacenters, Indexes, computer centres, content-based retrieval, combination feature, probabilistically gathered similar-image indexes, Query processing, image retrieval, distributed datacenter, Feature extraction, Peer-to-peer computing, content based image retrieval, multiple image index replica construction, peer-to-peer paradigm]
GlobLease: A globally consistent and elastic storage system using leases
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Nowadays, more and more IT companies are expanding their businesses and services to a global scale, serving users in several countries. Globally distributed storage systems are employed to reduce data access latency for clients all over the world. We present GlobLease, an elastic, globally-distributed and consistent key-value store. It is organised as multiple distributed hash tables (DHTs) storing replicated data and namespace. Across DHTs, data lookups and accesses are processed with respect to the locality of DHT deployments. We explore the use of leases in GlobLease to maintain data consistency across DHTs. The leases enable GlobLease to provide fast and consistent read access in a global scale with reduced global communications. The write accesses are optimized by migrating the master copy to the locations, where most of the writes take place. The elasticity of GlobLease is provided in a fine-grained manner in order to precisely and efficiently handle spiky and skewed read workloads. In our evaluation, GlobLease has demonstrated its optimized global performance, in comparison with Cassandra, with read and write latency less than 10 ms in most of the cases. Furthermore, our evaluation shows that GlobLease is able to bring down the request latency under an instant 4.5 times workload increase with skewed key distribution (a zipfian distribution with an exponent factor of 4) in less than 20 seconds.
[Availability, Geo-replication, multiple distributed hash tables, Protocols, DHTs, data lookups, Scalability, GlobLease, elastic globally-distributed and consistent key-value storage system, Elasticity, distributed processing, Routing, Standards, data reduction, storage management, namespace storage, data replication storage, Distributed Storage, IT company, Peer-to-peer computing]
POPS: A popularity-aware live streaming service
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Live streaming has become very popular. Many systems, such as justin.tv, have emerged. They aim to collect user live-streams and serve them to the viewers using broadcasting servers. However, the huge variation in the total number of viewers and the great heterogeneity among streams popularity generally implies over-provisioning, leading to an important resource waste. In this paper, we show that there is a trade-off between the number of servers involved to broadcast the streams and the bandwidth usage among the servers. We also stress the importance to predict streams popularity in order to efficiently place them on the servers. We propose POPS: a live streaming service using popularity predictions to map live-streams on the servers.
[popularity-aware live streaming service, broadcasting server, Bandwidth, POPS, Streaming media, Broadcasting, video streaming, IPTV, Servers, Multimedia communication, video servers, YouTube]
Fast GPU parallel N-Body tree traversal with Simulated Wide-Warp
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
The Barnes-Hut algorithm is a widely used approximation method for the N-Body simulation problem. The irregular nature of this tree walking code presents interesting challenges for its computation on parallel systems. Additional problems arise in effectively exploiting the processing capacity of GPU architectures. We propose and investigate the applicability of software Simulated Wide-Warps (SWW) in this context. To this extent, we explicitly deal with dynamic irregular patterns in data accesses with data remapping and data transformation, by controlling execution flow divergence of threads. We present a new compact data-structure for the tree layout, GPU parallel algorithms for tree transformation and parallel walking using SWW. Benefits of our techniques are in transposing the tree algorithm to execute regular patterns to match the GPU model. Our experiments show significant performance improvement over the best known GPU solutions to this algorithm.
[Manycore Computing, Instruction sets, Graphics processing units, Registers, SWW software, Parallel algorithms, GPGPU, Software Simulated Wide-Warp, Barnes-Hut algorithm, N-Body, approximation method, GPU parallel algorithm, data transformation, data remapping, Kernel, Implicit Octree, parallel algorithms, parallel system, Accelerator Computing, Indexes, graphics processing units, fast GPU parallel n-body tree traversal, CUDA, GPU architecture, n-body simulation problem, Layout, Octrees, simulated wide-warp software, execution flow divergence, data access, Barnes-Hut, graphics processing unit]
Session-based fault-tolerant design patterns
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Despite offering reliability against dropped and reordered packets, the widely adopted Transmission Control Protocol (TCP) provides nearly no recovery options for longterm network outages. When the network fails, developers must rollback the application to some coherent state on their own, using error-prone solutions. Overcoming this limitation is, therefore, a deeply investigated and challenging problem. Existing solutions range from transport-layer to application-layer protocols, including additions to TCP, usually transparent to the application. None of these solutions is perfect, because they all impact TCP's simplicity, performance or ubiquity, if not all. To avoid these shortcomings, we contain TCP connection crashes inside a single session layer exposed as a sockets interface. Based on this interface, we create a blocking and a non-blocking fault-tolerant design pattern. We explore the blocking design in an open source File Transfer Protocol (FTP) server and perform a thorough evaluation of performance, complexity and overhead of both designs. Our results show that using one of the patterns to tolerate TCP connection crashes, in new or existing applications, involves a very limited effort and negligible penalties.
[TCP, open source file transfer protocol server, Protocols, public domain software, session-based fault-tolerant design pattern, reliability, Fault-Tolerance, Servers, system recovery, Design Pattern, Fault tolerance, sockets interface, Connection Failure, Fault tolerant systems, file servers, Java, Session Layer, open source FTP server, Receivers, connection failure, application-layer protocol, transmission control protocol, Sockets, transport protocols, fault tolerant computing]
Scheduling time-sensitive multi-tier services with probabilistic performance guarantee
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Web applications grow tremendously in both scale and scope, the application patterns turn to be more and more sophisticated. It is important but challenging for service providers to lower the operational costs without degrading user experiences, especially in the case where a service provider's profit is closely related to the user experience (e.g. response time.) In this paper, we study the problem of efficiently scheduling multi-tier time sensitive applications on distributed computing platforms with respect to the user's Quality of Service (QoS) requirements. The efficiency refers to the QoS satisfaction with low average response times. The service provider must ensures that service requests be served successfully before end-to-end deadlines with certain probabilities. To solve this problem, we propose an approach to judiciously assign a deadline for each service tier. An application request is dropped if any one of its services misses its deadline. Our simulation results demonstrate that our approach can statistically guarantee the required QoS more efficiently than the other widely applied methods (e.g. acceptance control, first-come-first-serve, deterministic sub deadline assignment, etc.) irrespective of whether the resources are shared or not by multiple different applications.
[QoS satisfaction, service provider, Quality of service, Servers, resource allocation, scheduling, application request, Silicon, Mathematical model, sub deadline assignment, Probabilistic logic, probabilistic performance guarantee, multi-tier services, time-sensitive applications, quality of service, QoS statistical guarantee, user experience, time-sensitive multitier service scheduling, Web application, distributed computing platform, resource sharing, Internet, Time factors, Resource management, user QoS requirement]
SmartQ: A question and answer system for supplying high-quality and trustworthy answers
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Question and Answer (Q&amp;A) systems aggregate the collected intelligence of all users to provide satisfying answers for questions. A well-developed Q&amp;A system should incorporate features such as high question response rate, high answer quality, a spam-free environment for users. Previous works use reputation systems to achieve the goals. However, these reputation systems evaluate a user with an overall rating for all questions the user has answered regardless of the question categories, thus the reputation score does not accurately reflect the user's ability to answer a question in a specific category. We propose SmartQ: a reputation based Q&amp;A System. SmartQ employs a category and theme based reputation management system to evaluate users' willingness and capability to answer various kinds of questions. The reputation system facilitates the forwarding of a question to favorable experts, which improves the question response rate and answer quality. Also, SmartQ incorporates a lightweight spammer detection method to identify potential spammers. Our trace-driven simulation on PeerSim demonstrates the effectiveness of SmartQ in providing a good user experience. We then develop a real application of SmartQ and deploy it for use in a student group in Clemson University. The user feedback shows that SmartQ can provide high-quality answers for users in a community.
[Fans, Question and answer system, Communities, spam-free environment, lightweight spammer detection method, SmartQ, question and answer system, reputation based Q&amp;A system, Mathematical model, Real application, Social network services, question category, PeerSim, Clemson University, spammer detection, user experience, trace-driven simulation, trustworthy answers, user willingness evaluation, high-quality answers, Aggregates, reputation score, Delays, reputation system, question answering (information retrieval), trusted computing, IEEE Potentials, theme based reputation management system]
A privacy-aware cloud service selection method toward data life-cycle
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Recent years have witnessed the rapid development of cloud computing, which leverages its unique services to cloud customers in a pay-as-you-go, anytime, anywhere manner. However, a significant barrier to the adoption of cloud services is that users fear data leakage and loss of privacy if their sensitive data is processed in the cloud. Hence, the cloud customer must be able to select appropriate services according to his or her privacy and security needs. In this paper, we propose a novel cloud service selection method called PCSS, where a cloud service is estimated based on its capability of privacy protection (CoPP) covering the entire life-cycle of users' data. A scalable assessment index system with a 2-level hierarchy structure is constructed to analyze and quantify the CoPP of cloud service. The first-level index is composed of all stages of data life-cycle and the second-level index involves privacy-aware security mechanisms at each stage. We employ a fuzzy comprehensive evaluation technique to count the privacy-preserving value of security mechanism. An AHP- based approach is exploited to decide the impact weight of different security mechanisms to the CoPP of each stage. By calculating a comprehensive CoPP metric of all life-cycle stages, all cloud services can be sorted and recommended to users. An example analysis is given, and the reasonableness of the proposed method is proved. Comprehensive experiments have been conducted, which demonstrate the effectiveness of the proposed method by the comparison with the baseline method at the service selection performance.
[Data privacy, 2-level hierarchy structure, second-level index, fuzzy set theory, Security, analytic hierarchy process, Privacy, fuzzy comprehensive evaluation technique, privacy-aware cloud service selection method, scalable assessment index system, first-level index, privacy-aware, cloud computing, cloud customer, data life-cycle, Filtering, privacy-aware security mechanisms, data leakage, CoPP, Phase locked loops, privacy-preserving value, user data life-cycle, PCSS, security needs, AHP- based approach, data privacy, service selection performance, cloud service selection, privacy loss, capability of privacy protection]
P2P email encryption by an identity-based one-way group key agreement protocol
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
As a result of high-tech companies such as Google, Yahoo, and Microsoft offering free email services, email has become a primary channel of communication. However, email service providers have traditionally offered little in the way of message privacy protection. This has made emails, of which billions are sent around the world on any day, an attractive data source for personal identity information thieves. Google was one of the first companies to provide substantial email privacy protection when they began using the HTTPS always-on option to encrypt messages sent through their email service, Gmail. Unfortunately, Gmail's encryption option does not offer true point-to-point encryption since the encrypted emails are decrypted and stored in plaintext form on Google's servers. This type of approach poses a security vulnerability which is unacceptable to security-minded users such as highly sensitive government agencies and private companies. For these users, true point-to-point encryption is needed. This paper introduces an identity-based one-way group key agreement protocol and describes a point-to-point email encryption scheme based on the protocol. Both the security proofs and the efficiency analysis, with experimental results, of the new scheme are provided.
[message encryption, Protocols, cryptographic protocols, data source, email service providers, electronic mail, encryption option, Electronic mail, Encryption, Servers, email privacy protection, HTTPS, One-way group-key agreement, computer crime, identity-based one-way group key agreement protocol, message privacy protection, Microsoft, personal identity information thieves, Google, high-tech companies, P2P encryption, Identity-based encryption, Gmail, point-to-point email encryption scheme, Bilinear pairings, free email services, P2P email encryption, Yahoo, Public key, data privacy, security proofs, point-to-point encryption, security vulnerability, decryption]
An efficient detection scheme for urban traffic condition using volunteer probes
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
In urban areas, traffic congestion has become a common phenomenon resulting in a great waste of time and fuel. Thus, the real-time detection for road condition becomes a significant challenge for both scientists and engineers. Recently, the technology of vehicular ad-hoc networks (VANETs) has been utilized to alleviate this problem, and becomes an important component of the intelligent transportation system (ITS). In this paper, we propose a novel detection scheme for urban traffic condition only by volunteer probes. The travel time for detected road segment is used to evaluate the traffic condition. To reduce the influence of individuals, a T Window Algorithm (TWA) is designed for calculating the average travel time of probes. For performance optimization, three kinds of Adaptive T Window Algorithm (ATWA) are put forward to resize window according to the probe density. Besides, on analysis of travel time composition, a congestion line is built to identify the traffic status of road segment. Finally, all the approaches are validated in a realistic scenario. The simulation results show that our scheme can detect urban traffic condition effectively.
[Algorithm design and analysis, window algorithm, Roads, vehicular ad-hoc networks, ITS, intelligent transportation system, Vehicles, ATWA algorithm, adaptive T window algorithm, Prediction algorithms, traffic congestion, Probes, T window algorithm, vehicular ad hoc networks, average travel time, VANET, intelligent transportation systems, traffic engineering computing, volunteer probes, volunteer probe, Motion segmentation, VANETs, urban traffic condition, traffic condition detection, Delays, road condition detection scheme]
An access point to device association technique for optimized data transfer in mobile grids
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
In a mobile grid computing framework where mobile devices are used as computing resources, minimizing the task offloading time remains an important issue. A task is an independent unit of execution consisting of a input data volume for execution and optionally a target-specific executable. We consider a mobile grid infrastructure where mobile devices are connected via Wi-Fi network and the grid infrastructure has a set of tasks (i.e. a set of data volumes) to be transferred to a subset of the mobile devices. In a Wi-Fi network, mobile devices usually associate themselves to the access points (APs) having the strongest radio signal. In this paper, we address the problem of AP activation (by frequency assignment) and association of AP with devices in the context of minimizing the overall data-transfer completion time. We present a constraint based formulation and also a heuristic as solutions. Simulations results are presented which contrast our proposed methods with some of the earlier works.
[data volume, access point, Schedules, mobile grid computing framework, grid computing, Wi-Fi network, mobile grid infrastructure, device association technique, task offloading time, Wireless and Mobile Computing, AP, mobile computing, data-transfer completion time, computing resource, Frequency Assignment, Tin, Wireless Fidelity, AP-device association, wireless LAN]
Enhanced HIP-based micro-mobility and macro-mobility management by proactive signaling scheme
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
In current HIP (Host Identity Protocol) architecture, the mobility procedure consist of a three-way handshake between the mobile node (MN) and its peers. This procedure lead to a stopping communication between MN and its correspondent's nodes (CN), which results in packets loss and delay if the procedure takes some time. HIP is not optimized for micro-mobility. In the literature, several solutions have been proposed, but each requires a signaling procedure during handover. In this paper, the proposed scheme does not require any signaling at the micro-mobility and reduces signaling overhead during macro-mobility. Indeed our mobility scheme is looked ahead-based, thus the mobile node is able to know his prospective network prefix before link on associated future point of attachment. Three signaling types have been introduced and mobility information exchange is before Handover. The performance evaluation is based on an analytical model and tests are in a real environment. OpenHIP is implemented for testing performance of the proposed mobility scheme, unlike other schemes are content to do their tests in a simulated environment.
[micromobility management, Host Identity Protocol, macromobility management, Macromobility, OpenHIP, mobility management (mobile radio), mobility information exchange, Relays, Hip, signalling protocols, Micromobility, proactive signaling scheme, network prefix, HIP architecture, mobile node, Handover, host identity protocol, stopping communication, Peer-to-peer computing, IP networks, Manganese]
Extending access point service coverage area through opportunistic forwarding in multi-hop collaborative relay WLANs
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
In multi-hop WLANs, it is more efficient to let some clients relay traffic for the clients whose distances are a bit far away from the AP, which increases the transmission efficiency. The existing solutions for multi-hop WLANs try to choose one or two fixed best relays to forward packets. However, the links between far-away clients and the AP are intermittently connected, and end-to-end paths may not exit. In order to fill this gap, using opportunistic forwarding, our solutions do not fix a relay; the source simply broadcasts its signal, any client who received the signal will relay the traffic. In this paper, our object is to extend the connectivity by opportunistic forwarding for far-away clients in multi-hop WLAN accesses. The original contributions made by this paper include: 1) We develop a general model to analyze the throughput of opportunistic forwarding in multi-hop WLANs. 2) We classify the clients into three groups, and propose a centralized opportunistic routing relay algorithm for each group of clients to achieve the optimal system throughput. 3) We propose a distributed opportunistic routing relay protocol for clients' disconnected and connected actions. Extensive simulations have been done in NS-2. The simulation results show that our protocol can significantly increase the service's coverage and connectivity of the entire network, compared with those methods by traditional one-hop and multi-hop relay.
[radio links, Wireless LAN, Protocols, Heuristic algorithms, forward packet, multi-hop, Throughput, access point service coverage area, centralized opportunistic routing relay algorithm, transmission efficiency, Relays, one-hop relay, opportunistic routing, Spread spectrum communication, multihop collaborative relay WLAN access, relay networks (telecommunication), Routing, NS-2 simulation, WLAN, optimal system throughput, AP, relay traffic, opportunistic forwarding, routing protocols, wireless LAN, distributed opportunistic routing relay protocol, telecommunication traffic]
Improving smart grid authentication using Merkle Trees
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
The electrical power grid forms the functional foundation of our modern societies, but in the near future our aging electrical infrastructure will not be able to keep pace with our demands. As a result, nations worldwide have started to convert their power grids into smart grids that will have improved communication and control systems. A smart grid will be better able to incorporate new forms of energy generation as well as be self-healing and more reliable. This paper investigates a threat to wireless communication networks from a fully realized quantum computer, and provides a means to avoid this problem in smart grid domains. We discuss and compare the security aspects, the complexities and the performance of authentication using public-key cryptography and using Merkel Trees. As a result, we argue for the use of Merkle Trees as opposed to public key encryption for authentication of devices in wireless mesh networks (WMN) used in smart grid applications.
[Computers, Merkle Trees, smart grid authentication, smart power grids, Complexity theory, wireless mesh networks, Quantum computing, public key cryptography, wireless communication networks, Smart grids, power grids, control system, Merkle trees, public-key cryptography, quantum computer, RSA, smart grid, communication system, trees (mathematics), power engineering computing, electrical infrastructure, electrical power grid, energy generation, Authentication, Public key, message authentication, quantum computing, Vegetation, WMN, security aspects, quantum-computer attacks]
Smart MapReduce cloud: Applying extra processing to intermediate data on demand
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Cloud computing is the emerging and attractive technology and provides users with various services in a pay-as-you-go manner. Cloud computing nowadays does not limit resources of the services in a cloud to the computers that are far away from users and connected to each other in a data center with high speed networks at the same geographic location. Cloud computing may present a cloud to users by connecting resources at multiple geographic locations. By connecting resources at multiple geographic locations to organize a cloud, cloud computing may meet problems of communication interception, congestion, and interruption. Cloud computing should have a way to supply extra processing on demand for certain links between computers separated geographically. Since a MapReduce cloud is the key to the success of the large-scale computation, cloud computing can use the Smart MapReduce Cloud (SMRC) proposed in this paper to apply extra processing to intermediate data on demand while intermediate data is delivered among computers in the MapReduce cloud. In experiments, cloud computing is tested with several popular MapReduce applications to observe performances of data encryption and compression via XOR and GZIP functions in SMRC.
[Computers, Cloud computing, Interrupters, Encryption, GZIP function, parallel processing, communication interruption, MapReduce, Runtime, Databases, Cloud Computing, Intermediate Data, XOR function, cloud computing, high speed networks, intermediate data, data compression, data encryption, communication interception, pay-as-you-go manner, SMRC, communication congestion, data center, large-scale computation, smart MapReduce cloud, XOR, data handling, GZIP, Joining processes, geographic location]
RWFS: Design and implementation of file system executing access control based on user's location
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
In this research, we designed and implemented Real-World File System (RWFS), which can manage files as if we can put them onto or pick them up from the places of the real world. RWFS regards the places of the real world as directories of the file system by associating a directory with a place. We create directories called Real-World Directory (RWD) which forms a hierarchical structure to reflect the natural property of places. In addition to the conventional access rights of read, write, and execute as implemented in other file systems, RWFS accommodates utilizing location information of the target user in access rights; RWFS can decide whether or not the user can access to a particular file or directory based on the user's location. Therefore, accessible files for a user change depending on the user's location. This mechanism enables creating information that can be read or written by users who physically stay at a particular place. We evaluated this system by measuring turnaround time to operate the file system together with simulation.
[IEEE 802.11 Standards, real-world file system, Conferences, real-world directory, hierarchical structure, Mobile handsets, Servers, user location based file system, File systems, RWFS, Information services, authorisation, turnaround time, Permission, file organisation, access control, file system directories, RWD]
A victim and rescuer communication model in collapsed buildings/structures
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
In a disaster hit area, infrastructure networks may be disconnected and main communications between victims and rescuers are not available. An interesting question is that how many people are trapped in a collapsed building and how close they are to a nearby rescuer's position. A smart-phone is widely used today and it normally has a Wi-Fi function to connect to the Internet. The phone can be used for saving victims' lives. In this paper, a communication model is proposed for a victim to communicate with others and also a rescuer to communicate with victims. State diagrams of the victim and rescuer models are proposed along with pseudo codes for the model operation. An example communication scenario is presented to demonstrate how the model works.
[IEEE 802.11 Standards, Communication systems, Computational modeling, Buildings, Wi-Fi function, buildings (structures), emergency management, building collapse, smart phone, smart phones, Batteries, collapsed building rescue, mobile computing, Communication model, Earthquakes, mobile communications, state diagram, Delays, Internet, victim communication model, wireless LAN, rescuer communication model, DTN]
On message reachability of gossip algorithms in degree-biased peer-to-peer networks
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
In peer-to-peer networks, each node directly connects to other nodes without access points. This type of network system is useful for information sharing by using mobile devices (e.g., smart phones). On message delivery over the network, it is very difficult to assume the static routing if each node is assumed to move. In this paper, we suppose to use gossip-style epidemic message dissemination and show the performance evaluation of several gossip algorithms in terms of network topology. Specifically, we focus on the distribution of links in the network. Our results clarified the characteristics of those algorithms on the topologies that are biased the degree distribution locally.
[Algorithm design and analysis, peer-to-peer computing, message reachability, degree-biased peer-to-peer network, information sharing, gossip algorithm, gossip-style epidemic message dissemination, Routing, Topology, network topology, Standards, Network topology, mobile devices, Peer-to-peer computing, Mathematical model, degree distribution]
P2P overlay for CDN-P2P being aware of the upload capacity of participants
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
In this paper, we propose a method to maintain a Peer-to-Peer (P2P) overlay in live streaming systems based on a hybrid of Content Delivery Network (CDN) and P2P Technology. The key idea of the proposed method is to explicitly take into account the upload capacity of each peer in selecting peers which can directly receive the live stream from a cache server in the CDN. In addition, it prepares several backup peers to support the task of upload in such a way that a stream is fed to a P2P client if the local buffer at the client becomes scarce. The performance of the proposed scheme is evaluated by simulation.
[peer-to-peer computing, live streaming, Media, CDN-P2P network, Servers, Indexes, P2P overlay, Content distribution networks, rebuffering, overlay networks, Bandwidth, Tin, content delivery network, Peer-to-peer, P2P technology, Peer-to-peer computing, peer-to-peer network]
Energy-aware multipath routing for data aggregation in wireless sensor networks
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Data aggregation in wireless sensor networks is widely used to collect data in an energy efficient manner to eliminate redundant data transmission so that prolong the network lifetime. To meet the data aggregation needs in wireless sensor networks, this paper proposes a novel multi-path routing algorithm, called EAD, to process in-network data aggregation. For each sensor on the routing paths, EAD evaluates its neighbors based on the residual energy, deviation angle and distance, and selects the k neighbors with the minimal evaluation costs as its forwarding nodes in order to balance energy consumption of the wireless sensor network on the premise of ensuring the reliability and performance. Simulation results show that EAD can effectively prolong network lifetime, reduce latency and ensure the reliability by adjusting the weight of each influencing factor.
[energy-aware multipath routing, wireless sensor networks, residual energy, wireless sensor network, forwarding cost, EAD, in-network data aggregation, Batteries, data aggregation, data transmission redundancy elimination, Standards, Aggregates, telecommunication network routing, energy efficiency, energy conservation, k neighbor, deviation angle, multipath routing, energy consumption, Monitoring, Erbium]
Fault-Tolerant bi-directional communications in web-based applications
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
The Hypertext Transfer Protocol (HTTP) and the Transmission Control Protocol (TCP) are the most popular protocols used in the development of web-based applications. Despite their popularity, the use of these protocols brings two limitations to applications and systems that require reliable interactive real-time communications: 1) HTTP forces applications to work in a request-response paradigm, even if a reply is not necessary, not allowing the server to send anything to a client without the client explicitly requesting it; 2) TCP provides no recovery options for network outages, thus forcing developers to write their own error-prone, complex, and ad hoc solutions. In this paper we introduce a solution that offers both bi-directional and reliable communication to web-based applications, even in presence of connection failures. To make this possible, we combine the idea behind WebSockets and a Session-Based Fault-Tolerant design pattern.
[TCP, Protocols, session-based fault-tolerant design pattern, Bi-Directional Communication, request-response paradigm, Web-Based Applications, Web-based applications, Fault-Tolerance, Servers, WebSockets, Fault tolerance, fault-tolerant bidirectional communications, network outages, Connection Failure, Fault tolerant systems, connection failures, computer network reliability, client-server systems, recovery options, hypertext transfer protocol, HTTP, transmission control protocol, Standards, WebSocket, Sockets, transport protocols, fault tolerant computing]
Routing table status influence of Monitoring Kad
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Kad is the most popular P2P file sharing system. Monitoring Kad peers' lookup traffic is an important work for the analysis and optimization of Peer-to-Peer (P2P) network. During the monitoring process, we find that the peer's status significantly influences the monitoring results. Each lookup action changes the searching peer's routing table status, and it may break the monitoring process. In this paper, we analyze the changes in the routing table to verify its effect on the monitoring process. If the distance between the target ID and searcher's Kad ID is in within a certain critical range, previous searches may cause future searches to fail with high probability. We estimate the boundary of this critical range. The experiments performed on eMule shows that such a critical range exist, and that deploying more than 1024 IP addresses cannot help to improve the success rate of the monitoring process.
[routing table status, peer-to-peer computing, Routing, Indexes, P2P file sharing system, monitoring Kad, P2P, Kad, routing table status influence, eMule, Publishing, Monitor, telecommunication network routing, lookup action, Vegetation, Critical Region, peer-to-peer system, Peer-to-peer computing, IP networks, Monitoring, peer-to-peer network]
DASH: A duplication-aware flash cache architecture in virtualization environment
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
With the rapid development of multi-core and multi-threading technologies, the performance gap between CPU and storage system is widening year by year, causing the storage system to be the bottleneck of the whole system performance. To alleviate this situation, flash memory has been used as the caching device of HDDs. On the other hand, cloud computing is becoming more and more popular and mature in industry field. As the key building block of it, virtualization technology allows several virtual machines (VMs) running on one single physical machine simultaneously, most of which usually run the same or similar operating systems and applications. In this scenario, flash cache will be occupied by many duplicate data blocks. However, existing flash cache architectures and replacement policies don't take this observation into consideration, which greatly limits the efficient use of the flash cache. In this paper, we propose a new duplication-aware flash cache architecture (DASH). In this architecture, flash cache is organized to cache only one copy of the duplicate data blocks, which can notably expand the effective cache capacity, making more I/O requests hit in the cache. Moreover, this architecture can reduce the amount of data written to flash cache, and thus the life span of flash device can be significantly prolonged. Experiments based on realistic applications show that, in some situations, our cache architecture can improve the cache hit ratio by 5 times, reduce the average I/O latency by 63% and eliminate flash cache writes by 81%.
[Performance evaluation, storage system, data deduplication, virtualization environment, CPU, cache storage, input-output programs, virtualisation, rapid development, Flash memories, DASH, software architecture, flash memories, multithreading technologies, virtualization technology, Computer architecture, I/O requests, flash cache architecture, cloud computing, Kernel, multiprocessing programs, multi-threading, duplication-aware flash cache architecture, Virtual machining, replacement strategy, Organizations, virtual machines, VM, multicore technologies, Virtualization]
Construct a simply and quickly platform to solving linear systems
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
The scientific computing is important research for industrial and society. And, the linear system becomes more important in scientific computing. However, the linear system solvers have many combinations. How to rapidly selecting a best method to solving matrices is expensive. In this paper, we present a linear system solvers platform, which offer easily and quickly interface to users.
[Linear systems, Scientific computing, Multicore processing, scientific computing, mathematics computing, Random access memory, Sparse matrices, linear system solvers platform, linear system problem, Parallel processing, sparse matrix, iterative solver, preconditioner, sparse matrices, Graphical user interfaces]
User behavior prediction: A combined model of topic level influence and contagion interaction
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
People post, share and adopt short text/multimedia messages in OSNs every day. Understanding and being able to predict user behaviors in OSNs can be helpful for several areas such as viral marketing and advertisement. In this paper we propose a probabilistic model which combines the impacts from message interactions and topic level social influence to predict the user behavior of adopting contagions. Using two datasets: a collected Weibo data and a DBLP citation network, we testify that the combined model could predict user behavior more accurately.
[multimedia messages, Computational modeling, contagion interaction, topic level social influence, electronic messaging, human factors, probability, Predictive models, Probabilistic logic, Multimedia communication, multimedia computing, OSNs, Weibo data, probabilistic model, message interactions, DBLP citation network, user behavior prediction, viral marketing, advertisement, social networking (online), short text messages, Data models, Bayes methods, Mathematical model]
DLBS: Decentralized load balancing scheme for event-driven cloud frameworks
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
With the development of cloud computing, more and more applications are moving to a distributed fashion to solve problems. These applications usually contain complex iterative or incremental procedures and have a more urgent requirement on low-latency. Thus many event-driven cloud frameworks are proposed. To optimize this kind of frameworks, an efficient strategy to minimize the execution time by redistributing work- loads is needed. Nowadays, load balance is a critical issue for the efficient operation of cloud platforms and many centralized schemes have already been proposed. However, few of them have been designed to support event-driven frameworks. Besides, as the cluster size and volume of tasks increases, centralized scheme will lead to a bottleneck of master node. In this paper, we demonstrate a decentralized load balancing scheme named DLBS for event-driven cloud frameworks and present two technologies to optimize it. In our design, schedulers are placed in every node for independently load-monitoring, autonomous decision-making and parallel task-scheduling. With the help of DLBS, master frees from the burden and tasks are executed with lower latency. We analyze the excellence of DLBS theoretically and proof it through simulation. At last, we implement and deploy it on a 64-machine cluster and demonstrate that it performs within 20% of an ideal scheme, which are consistent with simulation results.
[Algorithm design and analysis, Cloud computing, Decentralized load balancing, Event-driven framework, Programming, parallel task-scheduling, parallel processing, load-monitoring, Processor scheduling, resource allocation, decentralized load balancing scheme, event-driven cloud frameworks, Clustering algorithms, Load management, system monitoring, DLBS, cloud computing, Probes, autonomous decision-making]
Model to estimate the size of a Hadoop cluster - HCEm
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
This paper describes a model which aims to estimate the size of a cluster running Hadoop framework for the processing of large datasets at a given timeframe. As main contributions it denes (i) a light layer of optimization for MapReduce jobs, (ii) presents a model to estimate the size cluster for a Hadoop framework and (iii) performs tests using a real environment - the Amazon Elastic MapReduce. The proposed approach works with the MapReduce to dene the main configuration parameters and determines computational resources of hosts in the cluster in order to meet the desired runtime for the requirements of a given workload requirement. Thus, the results show that the proposed model is able to avoid to over-allocation or sub-allocation of computing resources on a Hadoop cluster.
[Hadoop cluster size estimation, Computational modeling, Random access memory, Hadoop, Big Data, Amazon Elastic MapReduce, Complexity theory, Performance Model, parallel processing, Optimization, MapReduce, Memory management, Clusters, Data models, Virtualization, HCE<sub>m</sub>, MapReduce jobs]
Streaming in NoSQL
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Present NoSQL databases are passive entities, where users proactively access the databases. While NoSQL databases are scalable due to their horizontal scale-out designs, data items stored in potentially very large databases are difficult to retrieve in terms of access delay, programmability and usability. In this paper, we advocate supporting events in NoSQL. By introducing publishers and subscribers to NoSQL, our proposed NoSQL data store is capable of delivering data items that users are interested in. Additionally, our NoSQL database decouples publishers and subscribers such that application developers can emphasize on data manipulation without paying attention to communications for delivering data. We formally discuss the design requirements for streaming in NoSQL, and present a prototype implementation that addresses the design issues. We also outline our ongoing works in this paper.
[Availability, Measurement, NoSQL database, Databases, data manipulation, Silicon, data handling, Not Only Structured Query Language, Servers, relational databases, data delivery, Monitoring]
Performance study of virtual fence unit using Wireless Sensor Network in IoT environment
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Wireless Sensor Networks (WSN) have been widely used in various monitoring applications including virtual fencing. With the integration of Internet of Things (IoT) in which millions of devices, systems and services will be interconnected online, it brings high capabilities for sensing, intercommunicate and actuation. This paper presents the experiments and performance analysis of virtual fence unit consists of microwave motion detector and IEEE 802.15.4 WSN for maximum sensing range integrate with IoT. In particular, the analysis is focusing on the maximum sensing range in terms of azimuth angle, height, sensitivity level for indoor and outdoor implementation. With the help of IoT, sensor which detected motion on weak detection angle or signal could communicate through internet and get signal from nearby sensor for better detection signal. The WSN platform is developed using Octopus II sensor nodes while the microwave motion detector is HB100 which detect movement using Doppler effects. Octopus II will uplink all signal through internet and provide corresponding action of virtual fencing when intrusion detected. Results show maximum sensing range of virtual fence unit is decreasing as azimuth angle increasing. With high sensitivity level of virtual fence unit, the maximum sensing range of virtual fence unit is larger than the maximum sensing range of virtual fence unit at normal sensitivity level. Results also show the virtual fence unit has higher maximum sensing range in indoor environment than outdoor environment.
[azimuth angle, wireless sensor networks, Zigbee, wireless sensor network, IEEE 802.15.4 WSN, maximum sensing range, Doppler effect, indoor environment, Internet of Things (IoT)Introduction, IoT environment, Wireless communication, HB100 microwave motion detector, Wireless Sensor Network (WSN), Azimuth, Detectors, Octopus II, HB100, Indoor environments, Octopus II sensor nodes, indoor implementation, Internet of Things, outdoor, Wireless sensor networks, outdoor implementation, Sensitivity, Doppler effects, indoor, indoor communication, virtual fence unit]
Distributed sensor device resource-object connection based on service delivery platform
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
A system for providing a distributed device resource-object-connection service based on a service delivery platform (SDP) is described. The system includes an SDP and proxy. The SDP configures to define distributed service functions as enablers, generates a convergence service by combining the enablers, and provides the generated convergence service. The proxy configures to connect a distributed device and an SDP to allow the SDP to use the distributed device as a resource, and define and use the distributed device as an enabler. The system are capable of defining distributed service functions as well as distributed sensors as enablers, and thereby allowing the distributed sensors to be used in the same sense as service-function enablers.
[Protocols, service delivery platform, mobile cloud, distributed processing, Web servers, Simple object access protocol, Data mining, Wearable, information services, distributed sensor device resource-object connection, Convergence, convergence service, Service Delivery, Logic gates, distributed service function, SDP, service-function enabler]
ArPat: Accurate RFID reader positioning with mere boundary tags
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
The Radio Frequency IDentification (RFID) technology provides a promising solution to location discovery in indoor environments. Existing RFID reader positioning algorithms usually use all the collected reference tags to determine the position of the target reader, and thus are time-consuming as well as susceptible to the communication irregularity between the reader and reference tags. Especially, they usually perform poorly when the target reader is near the wall or at the corner. In this paper, we propose ArPat, an Accurate RFID reader Positioning algorithm that uses mere boundary reference Tags to calculate the position of the reader. ArPat uses only boundary tags to determine the position of the target reader, which effectively mitigates the negative impact of communication irregularity on the localization accuracy. The localization accuracy of ArPat is higher than 0.2 ft when the space between references tags is 1 ft. Compared with state-of-the-art solutions for RFID reader positioning, ArPat improves localization accuracy by up to 42 percent and 36 percent on average. Furthermore, it uses a geometric approach rather than iterative optimization approaches employed by previous solutions, making it superior in time efficiency. Compared with previous solutions, the computational time of ArPat is nearly two orders of magnitude less. This is critical for a localization system to provide real time location discovery and tracking services.
[Accuracy, radiofrequency identification, RFID reader positioning, geometric approach, ArPat technology, service tracking, RFID reader positioning algorithm, iterative optimization approach, Radiofrequency identification, RFID boundary tags, service location discovery]
M2M-enabled real-time Trip Planner
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Uncertainty is a key factor that prevents a commuter from using public transportation system. More and more transportation agencies are incorporating real-time Trip Planners to empower commuters with opportune information. However, such systems require continuous status updates from the vehicles and involves expensive communication cost. In this paper we propose an architecture that takes advantage of Machine-to-Machine Communication concepts and provides a degree of intelligence to the vehicles, to alleviate unnecessary communication between the vehicles and the Trip Planner.
[Intelligent Transportation Systems, Ground penetrating radar, vehicles status updates, Transportation, Containers, commuter uncertainty, transportation agencies, Trip Planner, intelligent transportation systems, intelligent transportation system, public transportation system, M2M-enabled real-time trip planner, Machine-to-Machine Communication, road vehicles, machine-to-machine communication, public transport, Monitoring, architecture]
Development of energy efficient ZigBee module
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
This study investigated and developed an energy efficient ZigBee wireless module using hardware technique. The MG2455-F48 RF SoC chip was used in developing the ZigBee wireless module. This chip consists of 2.4GHz RF part and 8051 family microcontroller. ZigBee modules are powered by a battery because it has to be movable. Energy saving of the ZigBee module has been one of important issues because its battery life time is limited. Nowadays many energy efficient software algorithms have been developed for ZigBee modules, that are used in data transmission between nodes. A node means a ZigBee module. In order to save energy in the ZigBee module, software algorithm was not used in this study, but we used the hardware technique. The microcontroller of ZigBee controls power of the sensors which connected on the ZigBee module. Sensors are active only when they need to transmit data and in other case the module switches to the power down mode and save energy.
[MG2455-F48 RF SoC chip, telecommunication power management, Zigbee, Batteries, ZigBee module, energy saving, Wireless communication, Radio frequency, Wireless sensor networks, Zirconium, frequency 2.4 GHz, 8051 family microcontroller, data transmission, Power Down Mode, Battery, energy conservation, MG2455, Sensors, system-on-chip, microcontrollers, energy efficient ZigBee wireless module, hardware technique, Sensor]
A quorum-based channel hopping scheme for jamming resilience
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Jamming attacks have become wireless security threats disrupting reliable RF communication. Frequency Hopping Spread Spectrum (FHSS) is a widely used technique for anti-jamming wireless communications. However, FHSS relies on the pre-shared secret key between the communication pair. However, secure key exchange is challenging. Recently, Uncoordinated Frequency Hopping (UFH) schemes have been studied to achieve resilient key establishment in presence of a jammer. However, existing UFH schemes either suffer from unguaranteed rendezvous. In this paper, we introduce an Jamming-Resilient Asynchronous-Symmetric UFH scheme, AJCH, for guaranteeing rendezvous.
[Algorithm design and analysis, telecommunication security, frequency hopping spread spectrum, FHSS, pre-shared secret key, Entropy, spread spectrum communication, jamming resilience, Security, Jamming, uncoordinated frequency hopping, quorum-based channel hopping scheme, Wireless communication, anti-jamming wireless communications, Spread spectrum communication, jamming attack, resilient key establishment, jamming, jamming-resilient asynchronous-symmetric UFH scheme, Receivers, frequency hop communication, MAC, jamming attacks, secure key exchange, wireless security threats, private key cryptography, frequency hopping, quorum system]
An improved realistic mobility model and mechanism for VANET based on SUMO and NS3 collaborative simulations
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
The information field is undergoing a new round of technological revolution from the Internet to the Internet of things. Vehicle ad-hoc network (VANET), an application of the internet of things using in Intelligent Transportation System (ITS), has already attracted broad attention in the world in recent years. It mainly provides communications between vehicle-to-vehicle and vehicle-to-infrastructure, which significantly improve road transport efficiency, reduce energy consumption and ease traffic congestion. In this paper, we developed a client to make SUMO and NS3 work parallel by TraCI (Traffic Control Interface) in NS3. It helps NS3 get SUMO's information and sends instructions to change the states of vehicles and traffic lights. We present a realistic road traffic model with kinds of vehicles and intelligent traffic lights. The model is built in SUMO (Simulation of Urban Mobility). We use OpenStreetMap to generate a realistic map near the bund in Shanghai. The traffic flow is built according to a survey which makes us get meaningful and reliable statistics. A mechanism of changing the traffic lights dynamically is introduced to minimize traffic jams and give high priority to emergency vehicle. As a result, the waiting time and the duration of the vehicles in the scenario have reduced significantly after using the mechanism. The emergency vehicle's waiting time is less than others.
[OpenStreetMap, telecommunication congestion control, traffic control interface, Roads, simulation of urban mobility, ITS, intelligent transportation system, power consumption, Vehicles, realistic mobility model, NS3, Analytical models, Internet of things, Intelligent Traffic Lights, emergency vehicle, vehicle ad hoc network, Traffic control, traffic jams, vehicle-to-infrastructure, energy consumption, vehicle-to-vehicle, vehicular ad hoc networks, road traffic, SUMO, telecommunication power management, Shanghai, VANET, intelligent transportation systems, Internet of Things, OpenstreetMap, Vehicular ad hoc networks, Collaboration, NS3 collaborative simulations, Internet, TraCI, intelligent traffic lights]
A Decreasing k-means algorithm for the Disk Covering Tour Problem in wireless sensor networks
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
This paper studies a Disk Covering Tour Problem (DCTP) for reducing the energy consumption of a mobile robot's movement to provide services for sensor nodes in a wireless sensor network (WSN). Given a set of locations of sensor nodes and a starting location of mobile robot, the DCTP is to find a minimum cost tour of a sequence of tour stops for the mobile robot to serve sensor nodes by keeping every sensor node within a specified distance of a tour stop. We propose an algorithm, called Decreasing k-means (Dk-means), to find an approximate solution to the DCTP. The idea is to select a minimum number of disks or circles of a fixed radius to cover all sensor nodes, and then to find a minimum cost tour passing all disk centers. The simulation results show the proposed algorithm outperforms the related CSP (Covering Salesman Problem) algorithm and the QiF algorithm.
[radio direction-finding, Energy consumption, wireless sensor networks, Heuristic algorithms, decreasing k-means Algorithm, wireless sensor network, mobile robot movement, mobile robots, Mobile robots, disk covering tour problem, Wireless communication, Dk-means algorithm, Clustering algorithms, Robot sensing systems, energy consumption, k-means algorithm, covering salesman problem, CSP, telecommunication control, DCTP, QiF algorithm, disk covering problem, Wireless sensor networks, WSN, sensor node location]
Characteristics and perspectives of wearable smart devices and industrial ecosystem
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Wearable smart devices industry is booming up and being highlighted by the market recently as an alternative of post smart phone industry. Wearable smart devices have an intrinsic nature that their visual style should be very important like fashion as well as the sophisticated function is. They also have unique characteristics of industrial ecosystem different from the ecosystem of the smart phone industry. In this paper, we insist that there are a huge number of vertical markets for wearable smart device product and services. We also suggests that the small and medium companies should be more aggressive to advance such vertical markets with the application products and services with components as well as solutions.
[Industries, Ecosystems, Companies, smart phones, IoT, wearable computers, wearable smart device industry, wearable device, mobile computing, open platform, vertical market, Software, style, Biomedical monitoring, smart phone industry, Smart phones, industrial ecosystem, fashion]
A wearable virtual coach for Marathon beginners
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Marathon is very popular in recent years. However, finishing the game is no easy task, especially for beginners. Regular practices and training are needed. With the availability of wearable devices, it is possible to develop virtual coaches that monitor the progresses of individual runners closeup and guide them through tailor-made training schedules. Unfortunately, most existing wearable devices only record physiological signals of the runners and rely on off-line processing to provide feedbacks. In this paper, we present the design and development of an on-line virtual coach, which performs real-time tracking and analysis of the physiological status of the runner and suggests appropriate adjustments on the exercise intensity. The proposed virtual coach is a pure software solution and can work with any wearable device that monitors the heart rate and running speed of the runner. The main challenge of our system is to predict when the runner will reach the various running states and instruct the runner to adjust the speed just ahead of time so that her/his body can react in time to maintain the required training intensity. Experiments on real users show that our proposed algorithms can correctly predict the running states of the runners and help them to better maintain the required intensity to maximize the training effects.
[exercise intensity, virtual reality, physiological signals, marathon beginners, runner progress monitoring, heart rate monitoring, wearable virtual coach, Training, real-time tracking, running speed, physiological models, online virtual coach, training effects, Real-time systems, physiological status analysis, Monitoring, wearable devices, Stress, cardiology, Radio access networks, software solution, Heart rate, wearable computers, training schedules, running states, training intensity, Biomedical monitoring, sport]
Transmission characteristics of hybrid structure yarns for e-textiles
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Twisted Copper Filaments (TCF) have been made by a yarn covering process in order to transmit signals and powers for electronic textiles. The 560 den. polyurethane filaments were covered in S-twist direction by urethane-coated copper wires. Final filaments were found to be changed in resonance frequency mainly due to the change of di-electricity and thus capacitance caused by PET covered on it. It have been concluded that while resonance frequency was primarily determined by filament length and dielectric constant of covering yarns, S11 and S21 were mainly determined by measurement length and ply number.
[hybrid structure yarn, dielectric function, Optical fiber networks, Frequency measurement, Electrical resistance measurement, Yarn, dielectric constant, Optical fiber devices, polymers, textile fibres, hybrid structure yarns, Copper, electronic textiles, urethane-coated copper wires, e-textiles, permittivity, twisted copper filaments, yarn covering process, polyurethane filaments, smart clothing, conducting fibers, digital textile, Resonant frequency, data transmission, electronic products, yarn, PET]
An Efficient Query Tree protocol for RFID tag anti-collision
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Anti-Collision is one of the most important problems of the RFID technology. In this paper, we propose an Efficient Query Tree (EQT) protocol to improve both the Query Tree (QT) protocol and the Collision Tree (CT) protocol. The main idea of the EQT protocol is to reduce as much as possible the timeslots used to transmit bits between the reader and tags, so that the time of tag identification procedure is shortened and the energy consumption is lessened. In the EQT protocol, the timeslots structure, the query and responses between the reader and tags are carefully redesigned to allow tags to transmit fewer bits. We analyze and simulate the EQT protocol and compare it with the QT and the CT protocols. The simulation results show the EQT protocol outperforms the other two protocols in terms of the tag identification time.
[Energy consumption, tag identification procedure, Protocols, radiofrequency identification, telecommunication congestion control, Heuristic algorithms, RFID, CT protocol, tag anti-collision, timeslots reduction, Internet of things, efficient query tree protocol, Query Tree (QT) protocol, RFID tag anticollision, Simulation, Cities and towns, collision tree protocol, Collision Tree (CT) protocol, protocols, energy consumption, Radiofrequency identification, EQT protocol, bits transmission]
Incenter-based nearest feature space method for hyperspectral image classification using GPU
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
In this paper a novel technique based on nearest feature space (NFS), known as incenter-based nearest feature space (INFS), is proposed for supervised hyperspectral image classification. Due to the class separability and neighborhood structure, the traditional NFS can perform well for classification of remote sensing images. However, in some instances, the overlapping training samples might cause classification errors in spite of the high classification accuracy of NFS for normal cases. In response, the INFS is proposed to overcome this problem in this paper. INFS method makes use of the incircle of a triangle which is tangent to its three sides and form a INFS. In addition, an incenter can be calculated by three training samples of the same class efficiently. Furthermore, in order to speed up the computation performance, this paper proposes a parallel computing version of INFS, namely parallel INFS (PINFS). It uses a modern graphics processing unit (GPU) architecture with NVIDIA's compute unified device architecture (CUDA) technology to improve the computational speed of INFS. Experimental results demonstrate the proposed INFS approach is suitable for land cover classification in earth remote sensing. It can achieve the better performance than NFS classifier when the class sample distribution overlaps. Through the computation of GPU by CUDA, we can also gain better speedup.
[computational speed, NVIDIA, nearest feature space classifier, image classification, parallel architectures, Graphics processing units, PINFS, land cover classification, Training, Accuracy, supervised hyperspectral image classification, Computer architecture, class separability, parallel INFS, parallel computing, land cover, incenter-based nearest feature space method, earth remote sensing, geophysical image processing, remote sensing, CUDA technology, compute unified device architecture, graphics processing units, neighborhood structure, GPU architecture, incenter-based nearest feature space, graphics processing unit, triangle incircle, graphics processing unit architecture, remote sensing images, hyperspectral imaging, Principal component analysis, Hyperspectral imaging]
Fine-grained parallel implementation of edge-directed Image Interpolation on GPU
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Edge-directed interpolation is widely used to enhance visual performance of remote sensing image. Compared with traditional bi-cubic interpolation and bilinear interpolation, a great number of matrix operations will appear as it is getting better visual performance. CUDA (Compute Unified Device Architecture) offers tremendous performance in many high-performance computing areas. Edge-directed interpolation can be mapped to this architecture (CUDA) readily. However, parallel schemes based on CUDA are generally decomposed into coarse-grained tasks, which is suitable for thread blocks. In this paper, a parallel approach of fine-grained edge-directed interpolation is proposed. Based on CUDA, the process of parallel interpolation for one missing pixel is assigned to 4*4 threads for the reason that majority of matrix operations are related to 4*4 matrix. This task division strategy minimizes resource pressure of thread-blocks. Our calculating scheme is expressed in terms of increasing parallelism that is efficiently implemented on the GPU. By employing one NVIDIA GTX480 GPU and one NVIDIA GTX590 GPU in the case with asynchronous I/O transfer, our GPU optimization efforts on fine-grained edge-directed interpolation scheme finally achieve a speedup of 69.8x with respect to its CPU counterpart C code running on one CPU core of Intel core(TM) i7-920.
[Algorithm design and analysis, task division strategy, edge-directed interpolation, Visualization, fine-grained parallel implementation, Instruction sets, Image edge detection, parallel architectures, Graphics processing units, matrix operations, compute unified device architecture, Matrix decomposition, fine-grained, graphics processing units, GPU, CUDA, asynchronous I/O transfer, Interpolation, edge-directed image interpolation, NVIDIA GTX590 GPU, CUDA(Compute Unified Device Architectur), edge detection, image resolution, NVIDIA GTX480 GPU]
Compression technique for DubaiSat-2 images based on the DCT blocks
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
This paper presents an image compression technique for DubaiSat-2 based on Discrete Cosine Transform (DCT). Initially, the image is divided into non-overlapping sub-blocks and transformed to a frequency domain using DCT. Then the thresholding technique is applied to eliminate lower energy coefficients. The resultant coefficients are quantized at a user specified bit rates. The proposed method is tested on both gray scale and color images. The performance of the proposed method is analyzed using Peak Signal-to-Noise Ratio (PSNR).
[DubaiSat-2 image, gray scale image, Quantization, Frequency conversion, frequency domain, peak signal-to-noise ratio, discrete cosine transform, Image coding, Quantization (signal), Bit rate, Thresholding, image colour analysis, Discrete cosine transforms, Peak Signal-to-Noise Ratio (PSNR), data compression, discrete cosine transforms, PSNR, energy coefficient, thresholding technique, Color, DCT blocks, Image compression, Discrete Cosine Transform (DCT), image compression technique, DubaiSat-2, Mean Square Error (MSE), color image, image coding]
Budgeted mini-batch parallel gradient descent for support vector machines on Spark
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Mini-batch gradient descent (MBGD) is an attractive choice for support vector machines (SVM), because processing part of examples at a time is advantageous when disposing large data. Similar to other SVM learning algorithms, MBGD is vulnerable to the curse of kernelization when equipped with kernel functions, which results in unbounded linear growth in model size and update time with data size. This paper presents a budgeted mini-batch parallel gradient descent algorithm (BMBPGD) for large-scale kernel SVM training which can run efficiently on Apache Spark. Spark is a fast and general engine for large-scale data processing which is originally intended to deal with iterative algorithms. BMBPGD algorithm has constant space and time complexity per update. It uses removal budget maintenance method to keep the number of support vectors (SVs). The experiment results show that BMBPGD achieves higher accuracy than SVMWithSGD algorithm in MLlib on Spark environment, and it takes much shorter time than LibSVM.
[Algorithm design and analysis, kernel functions, constant space complexity, budgeted minibatch parallel gradient descent algorithm, BMBPGD, Complexity theory, removal budget maintenance method, Accuracy, model size, curse-of-kernelization, large-scale kernel SVM training, constant time complexity, gradient methods, update time, parallel algorithms, data analysis, support vector machines, MLlib, data size, kernel method, Sparks, stochastic gradient descent, Apache Spark, Support vector machines, large-scale learning, large-scale data processing, budget maintenance, Spark, unbounded linear growth, computational complexity, mini-batch gradient descent]
Sensor deployment by a robot in an unknown orthogonal region: Achieving full coverage
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
When deploying a wireless sensor network in an unknown environment, commonly referred to as Region of Interest (ROI), the main goal is for the entire region to be covered by the sensing ranges of the deployed sensors. While this goal of full coverage is easily achieved in presence of human intervention, it becomes problematic if the region is dangerous or inaccessible to human. An approach recently proposed to solve the problem is to use a robot to deploy the sensors; the main advantages respect to the alternative of employing mobile sensors are the reduced costs (due to manufacture and maintenance cost of common static sensors vs. mobile ones) and the reduced complexity of the coordination and control algorithms. Indeed several solution algorithms to achieve deployment of sensors by a robot in an unknown region have been proposed in the literature. Unfortunately, even when restricted to orthogonal regions (e.g., city maps, building plans, etc), all the existing algorithms fail to achieve full coverage of the ROI. Specifically, following the existing protocols, the robot would leave uncovered areas near either the boundaries or critical areas (e.g. areas that are linked to the rest of the region by a narrow corridor). In this paper we present an algorithm that overcomes these problems and guarantees that the deployment of the sensors by the robot achieves full coverage in any simply connected orthogonal ROI, whose topology is unknown to the robot. The proposed algorithm has minimal requirements: it does not need GPS but only local orientation by the robot; the communication range of a deployed sensor is limited to its deployed neighbours, and the robot has a similar range; the total number of sensors used is minimal. Also minimal are the robot's memory requirements, the total amount of robots movements and of communication between robot and sensors.
[Protocols, robot local orientation, Robot kinematics, Image edge detection, sensor placement, region of interest, Robot sensing systems, Mobile communication, mobile robot, sensor deployment, mobile robots, ROI]
The design of LLVM-based shader compiler for embedded architecture
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
The increasing hand-held devices has resulted in widespread use of Apps. To sustain the Apps performance, most developers use Graphic Processing Units (GPUs) as hardware accelerator recently. As a result, GPUs become the basic system requirement in the smart hand-held devices. With the improvement of GPUs techniques, the rendering pipeline has become programmable by using the shading language proposed from DirectX and OpenGL. A shader compiler is essential to compile the shading language into GPU assembly in order to run on programmable rendering pipelines. In this thesis, we describe a case study for the process to implement an OpenGL-ES 2.0 shader compiler for new GPU designed by an ITRI team. The compiler is based on LLVM and OpenGL framework Mesa. We use the front-end from Mesa that translates the OpenGL-ES shading language (GLSL ES) 2.0 to TGSI intermediate form and the Gallium3D device driver framework in Mesa will translate the TGSI to LLVM bitcode. Finally, we use the LLVM as backend to compile the bitcode to assembly NV gpu program4. In our preliminary experimental results, we use the gpu simulator provided from ITRI and GLES benchmark to show that our LLVM based shader compiler is able to generate reliable codes for basic OpenGL-ES programs.
[Pipelines, LLVM-based shader compiler, DirectX, Graphics processing units, graphic processing unit, Registers, GPU techniques, OpenGL-ES 2.0 shader compiler, graphics processing units, program compilers, shader compiler, rendering pipeline, LLVM, mobile device GPU, OpenGL-ES, embedded systems, embedded architecture, Rendering (computer graphics), Hardware, Mesa framework, rendering (computer graphics), Assembly, shading language, Gallium3D device driver framework]
On the and-or-scheduling problems
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
In the and-or scheduling model, a project consists of several tasks. Each task has a duration attribute. A task can be performed only when all of its requirements are satisfied. After a task is completed, more requirements become satisfied. A characteristic of the AOscheduling projects is that a requirement may be satisfied in several ways. Several questions concerning AOscheduling might be interesting, including whether the project can be completed, the earliest time a project can be completed, the minimal number of processors needed to complete the project, and assigning tasks to processors, etc. We use Petri nets and segment graphs to analyze AOscheduling projects.
[Computational modeling, project task, Petri nets, graph theory, duration attribute, Interference, segment graph, and-or-scheduling problem, Computer science, and-or scheduling, Program processors, AOscheduling project, Processor scheduling, scheduling, Delays, Petri net]
RHJoin: A fast and space-efficient join method for log processing in MapReduce
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Equi-join is heavily used in MapReduce-based log processing. With the rapid growth of dataset sizes, join methods on MapReduce are extensively studied recently. We find that existing join methods usually cannot get high query performance and affordable storage consumption at the same time when faced with a huge amount of log data. They either only optimize one aspect but significantly sacrifice the other or have limited applications. In this paper, after analyzing characteristics of the workloads and underlying MapReduce, we present a join method with specific optimizations for log processing called RHJoin (Repartition Hash Join) and its implementation on Hadoop. In RHJoin, reference tables are partitioned in the pre-processing step, the log table is partitioned on the map side and hash join is executed on the reduce side. The shuffle procedure of MapReduce is also optimized by removing the sort step and overlapping the execution of mappers and reducers. Comprehensive experiments show that RHJoin achieves high query performance with only a small extra storage cost, and has wide application circumstances for log processing.
[log data, RHJoin method, Scalability, query performance, Join, parallel programming, storage consumption, MapReduce, MapReduce shuffle procedure, Big data, data handling, repartition hash join method, Log Processing, MapReduce-based log processing]
Effective multi-GPU communication using multiple CUDA streams and threads
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
In the context of multiple GPUs that share the same PCIe bus, we propose a new communication scheme that leads to a more effective overlap of communication and computation. Multiple CUDA streams and OpenMP threads are adopted so that data can simultaneously be sent and received. A representative 3D stencil example is used to demonstrate the effectiveness of our scheme. We compare the performance of our new scheme with an MPI-based state-of-the-art scheme. Results show that our approach outperforms the state-of-the-art scheme, being up to 1.85&#x00D7; faster. However, our performance results also indicate that the current underlying PCIe bus architecture needs improvements to handle the future scenario of many GPUs per node.
[Context, message passing, multi-threading, Instruction sets, parallel architectures, OpenMP, multi-GPU, Graphics processing units, MPI, CUDA streams, OpenMP threads, Synchronization, representative 3D stencil, graphics processing units, communication scheme, GPU, overlap communication with computation, multiGPU communication, PCIe bus architecture, CUDA, Three-dimensional displays, Data transfer, Kernel]
Atomic reduction based sparse matrix-transpose vector multiplication on GPUs
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Sparse Matrix-Transpose Vector Product (SMTVP) is a frequently used computation pattern in High Performance Computing applications. It is typically solved by transposition followed by a Sparse Matrix-Vector Product (SMVP) in current linear algebra packages. However, the transposition process can be a serious bottleneck on modern parallel computing platforms. A previous work proposed a relatively complex data structure for efficiently computing SMTVP with multi-core CPUs, but it proved to be inefficient on GPUs. In this work, we show that the Compressed Sparse Row (CSR) based SMVP algorithm can also be efficient for SMTVP computation on modern GPUs. The proposed method exploits atomic operations to perform the reduce operation in the computation of each inner product of a row in the transposed matrix and the vector. Experimental results show that the simple technique can outperform the SMTVP flow of transposition plus SMVP released in the CUSPARSE package by up to 405-fold.
[CSR, Instruction sets, Laboratories, mathematics computing, Graphics processing units, sparse matrix-transpose vector multiplication, Throughput, Sparse matrices, Indexes, graphics processing units, GPU, atomic operation, atomic reduction, matrix multiplication, CSR based SMVP algorithm, CUSPARSE package, SMTVP computation, compressed sparse row, sparse matrix-transpose vector product, sparse matrices, transposition plus SMVP]
Efficient management of large DMA memory buffers in microdrivers
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Microdrivers run entirely in user space. The paradigm of such user-space device drivers has already shown to improve reliability and portability over pure kernel-space drivers. While the microdriver approach has been successful so far for low throughput and high latency use-cases such as USB, little research has been done on high performance applications. The Portable Driver Architecture (PDA) is a library which supports programming of microdrivers for high-speed PCI devices. This paper presents how it is possible to simplify large DMA buffer handling by using I/O Memory Management Units (IOMMU). Additionally, we optimized our library for intra-node communication in non-uniform memory access architectures.
[Performance evaluation, driver reliability, portable driver architecture, nonuniform memory access architecture, DMA memory buffer management, Random access memory, device drivers, user-space device driver, PDA, high-speed PCI device, microdriver approach, kernel-space driver, storage management, driver portability, intra-node communication, Sockets, Memory management, peripheral component interconnect, Libraries, microdriver programming, Resource management, input-output memory management units, Personal digital assistants]
RBPP: A row based DRAM page policy for the many-core era
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Memory requests in many-core systems are interleaved with each other and the locality of many-core systems decreases heavily. Page policies in traditional single core systems are not effective when it comes to many-core systems, because the open-page policy needs much locality of memory requests and the close-page policy takes no advantage of the remaining locality of many-core systems. There are some related memory page management policies, but their high complexity makes them unsuitable to many-core systems. They either need too much modification in operating systems or have large area and power overhead. To overcome these shortcomings of current page policies, in this paper, we propose the row based page policy, that is, RBPP, for the many-core systems, which tracks the row addresses of memory requests to each bank and uses row addresses as the indicator to decide whether or not to close the row buffer when the active memory request finished. We evaluate the proposed RBPP via Gem5 and DRAMSim2, and the results show that row based page policy can decrease the average memory latency by 14.7% and 4.0% over the open-page policy and the close-page policy, respectively. And the area overhead of row based page policy is decreased by 91.4 % and 91.5% over access based page policy and two-level predictor page policy, respectively.
[row based page policy, Random access memory, memory latency, Gem5, many-core, row based DRAM page policy, Registers, DRAMSim2, History, RBPP scheme, storage management, Operating systems, two-level predictor page policy, page policy, RBPP, Benchmark testing, DRAM chips, DRAM, open-page policy, Radiation detectors, operating systems, memory requests, close-page policy, memory page management policies, many-core systems, operating systems (computers), Delays, access based page policy]
HIOPS-KV: Exploiting multiple flash solid-state drives for key value stores
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Current key value stores rely on DRAM based inmemory architectures where scalability is limited by high power and low density of DRAM. As an alternative, flash SSDs has been explored because of the merits of low power, high density and high internal parallelism. However, the unpredictable latency caused by SSD internal resource conflicts challenges the use of flash SSDs. To address this issue, we present HIOPS-KV a storage I/O software stack for key value stores. HIOPS-KV exploits multiple solid-state drives (SSDs) to control the latencies. With replicas, HIOPS-KV avoids structural collisions which cause long latency operations by spreading colliding operations to distinct devices. For evaluation, we integrated HIOPS-KV into memcached on a low cost high IOPS SSD system built with PC components. At 32 YCSB clients, our system was capable of 117k ops/sec with 263 us average latency showing approximately 4ms at the 99th percentile latency.
[Performance evaluation, in-memory architecture, Random access memory, Throughput, scalability, Design, flash memories, Virtual memory, Ash, Computer architecture, DRAM, Flash, multiple flash solid-state drive, HIOPS-KV, YCSB client, SSD, key value store, Indexes, YCSB, flash SSD, spreading colliding operation, Software, Performance, low-power electronics, storage I-O software stack, Memcached]
Paraio: A scalable network I/O framework for many-core systems
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Many high-performance networked applications are designed using the event-driven paradigm. In many-core era, hundreds or even thousands of processor cores can be utilized to serve more clients. However, data race and load imbalance in current event-driven hybrid models will be a key bottleneck which challenges developers to fully exploit many-core resources to develop high-performance networked applications. In this paper, we extend the symmetric multi-thread event-driven model and present Paraio, a scalable network I/O framework to improve performance of networked applications such as web servers and software-defined network (SDN) controllers. In order to maximize the degree of parallelism in the event-based application execution, Paraio features the shared-data marking method that divides the event-processing logic and marks event handlers from the essential shared data perspective. In Paraio runtime, workloads are balanced among threads by an efficient work stealing, and new connection is allocated according to threads' load to obtain a fast response. Evaluation on web server and SDN controller, shows that Paraio applications with work stealing achieve better performance and scalability.
[processor cores, work stealing, Instruction sets, parallelism, event-processing logic, Throughput, software-defined network controllers, load imbalance, Runtime, many-core system, resource allocation, high-performance networked applications, event-driven hybrid models, data race, multithread architecture, Message systems, Load modeling, event-driven paradigm, multiprocessing systems, multi-threading, scalable network I/O framework, software defined networking, Paraio, Web servers, many-core resources, shared-data marking method, SDN controllers, event-driven model, symmetric multithread event-driven model, many-core systems, event-based application execution]
Orchestrating safe streaming computations with precise control
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Streaming computing is a paradigm of distributed computing that features networked nodes connected by first-in-first-out data channels. Communication between nodes may include not only high-volume data tokens but also infrequent and unpredictable control messages carrying control information, such as data set boundaries, exceptions, or reconfiguration requests. In many applications, it is necessary to order delivery of control messages precisely relative to data tokens, which can be especially challenging when nodes can filter data tokens. Existing approaches, mainly data serialization protocols, do not exploit the low-volume nature of control messages and may not guarantee that synchronization of these messages with data will be free of deadlock. In this paper, we propose an efficient messaging system for adding precisely ordered control messages to streaming applications. We use a credit-based protocol to avoid the need to tag data tokens and control messages. For potential deadlocks caused by filtering behavior and global synchronization, we propose deadlock avoidance solutions and prove their correctness.
[Protocols, global synchronization, deadlock avoidance solutions, reconfiguration requests, control message delivery, data set boundaries, Process control, Receivers, messaging system, Indexes, Synchronization, parallel processing, distributed computing, filtering behavior, streaming computation safety, first-in-first-out data channels, data tokens, System recovery, Streaming media, credit-based protocol, protocols, parallel computing, data serialization protocols]
Accelerated variance reduction methods on GPU
2014 20th IEEE International Conference on Parallel and Distributed Systems
None
2014
Monte Carlo simulations have become widely used in computational finance. Standard error is the basic notion to measure the quality of a Monte Carlo estimator, and the square of standard error is defined as the variance divided by the total number of simulations. Variance reduction methods have been developed as efficient algorithms by means of probabilistic analysis. GPU acceleration plays a crucial role of increasing the total number of simulations. We show that the total effect of combining variance reduction methods as efficient software algorithms with GPU acceleration as a parallel-computing hardware device can yield a tremendous speed up for financial applications such as evaluation of option prices and estimation of joint default probabilities.
[Algorithm design and analysis, Graphics processing units, option prices, probabilistic analysis, parallel processing, GPU acceleration, Monte Carlo methods, variance reduction, Pricing, financial applications, financial data processing, Mathematical model, accelerated variance reduction methods, Joints, variance reduction methods, Estimation, probability, graphics processing units, parallel-computing hardware device, Monte Carlo simulations, software algorithms, computational finance, option pricing, pricing, Monte Carlo estimator, default probability estimation]
Preface
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Organizing Committee
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Provides a listing of current committee members and society officers.
[]
Program Committee
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Provides a listing of current committee members and society officers.
[]
A Collaborative Reputation System Based on Credibility Propagation in WSNs
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Trust and reputation systems are widely employed in WSNs to help decision making processes by assessing trustworthiness of sensor nodes in a data aggregation process. However, in unattended and hostile environments, more sophisticated malicious attacks, such as collusion attacks, can distort the computed trust scores and lead to low quality or deceptive service as well as undermine the aggregation results. In this paper we propose a novel, local, collaborative-based trust framework for WSNs that is based on the concept of credibility propagation which we introduce. In our approach, trustworthiness of a sensor node depends on the amount of credibility that such a node receives from other nodes. In the process we also obtain an estimates of sensors' variances which allows us to estimate the true value of the signal using the Maximum Likelihood Estimation. Extensive experiments using both real-world and synthetic datasets demonstrate the efficiency and effectiveness of our approach.
[telecommunication security, Maximum likelihood estimation, reputation systems, wireless sensor networks, collaborative reputation system, collaborative-based trust framework, trust systems, credibility propagation, data aggregation, maximum likelihood estimation, iterative filtering, Temperature measurement, Computer science, Wireless sensor networks, sensor nodes, WSN, Aggregates, data aggregation process, Collaboration, decision making, Robustness, reputation system, collusion attacks]
Fast Execution of Simultaneous Breadth-First Searches on Sparse Graphs
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
The construction of efficient parallel graph algorithms is important for quickly solving problems in areas such as urban planning, social network analysis, and hardware verification. Existing GPU implementations of graph algorithms tend to be monolithic and thus contributions from the literature are typically rebuilt rather than reused. Recent work has focused on traversal-based abstractions that efficiently execute a single breadth-first search or enact algorithms in the &amp;ldquo;think like a vertex&amp;rdquo; paradigm. However, graph analytics such as the all-pairs shortest paths problem, diameter computations, betweenness centrality, and reachability querying require the execution of many such graph traversals. Typically, these traversals are independent of one another and can thus be executed in parallel. This paper presents multi-search, a simple abstraction that is designed for graph algorithms requiring many breadth-first searches that can be executed simultaneously. Although algorithms have implicitly leveraged this abstraction in the past, we provide an explicit, reusable implementation that efficiently maps this abstraction to the GPU, performing more than twice as fast as previous approaches on large graphs of varying diameter. This approach allows us to scale our APSP implementation to graphs with millions of vertices using a single GPU whereas prior approaches were either constrained to much smaller graph instances or required large supercomputers to process graphs of similar size. To show the flexibility of our abstraction, we use it to express betweenness centrality and achieve more than a 5.82x average speedup over parallel CPU implementations from existing frameworks and a 2.24x average speedup over a manual, highly optimized GPU implementation of the algorithm.
[Algorithm design and analysis, Measurement, parallel algorithms, multisearch, APSP implementation, graph theory, Graphics processing units, sparse graphs, Manuals, all-pairs shortest path, Search problems, simultaneous breadth-first searches, tree searching, graphics processing units, GPU, Parallel processing, Hardware]
MatrixMap: Programming Abstraction and Implementation of Matrix Computation for Big Data Applications
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
The computation core of many big data applications can be expressed as general matrix computations, including linear algebra operations and irregular matrix operations. However, existing parallel programming systems such as Spark do not have programming abstraction and efficient implementation for general matrix computations. In this paper, we present MatrixMap, a unified and efficient data-parallel system for general matrix computations. MatrixMap provides powerful yet simple abstraction, consisting of a distributed data structure called bulk key matrix and a computation interface defined by matrix patterns. Users can easily load data into bulk key matrices and program algorithms into parallel matrix patterns. MatrixMap outperforms current state-of-the-art systems by employing three key techniques: matrix patterns with lambda functions for irregular and linear algebra matrix operations, asynchronous computation pipeline with optimized data shuffling strategies for specific matrix patterns and in-memory data structure reusing data in iterations. Moreover, it can automatically handle the parallelization and distribute execution of programs on a large cluster. The experiment results show that MatrixMap is 12 times faster than Spark.
[Machine learning algorithms, linear algebra matrix operations, data shuffling strategies, Programming, Big Data applications, data reuse, Sparse matrices, parallel processing, Machine Learning, asynchronous computation pipeline, data structures, parallel matrix patterns, Matrix Computation, bulk key matrix, lambda functions, programming abstraction, Computational modeling, distributed data structure, Graph Processing, Big Data, Data structures, data-parallel system, Sparks, in-memory data structure, matrix algebra, Linear algebra, MatrixMap, Parallel Programming]
Mosaic: Towards City Scale Sensing with Mobile Sensor Networks
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
We introduce Mosaic, a mobile sensor network system for city scale sensing. Currently, we focus on accurate PM2.5 measurements. We design and implement low-cost smart sensors for this purpose. Those sensors are specifically designed for moving vehicles such as buses and taxies. The mobile sensing capability of our system enables a wide coverage of a city with a modest number of sensors. We incorporate a three-layer architecture in Mosaic. Mosaic faces many practical challenges which need to be properly addressed, e.g., reliable node hardware and software design, and accurate PM2.5 measurements. We have built three versions of Mosaic nodes. We present a novel data calibration approach combining traditional Artificial Neural Network (ANN) and Support Vector Machines (SVMs). We launch real-world deployments of Mosaic in two main cities in China -- Hangzhou and Ningbo. Our initial efforts show that Mosaic is a feasible approach for city scale sensing and our system can produce highly accurate PM2.5 measurements.
[mobile sensing capability, artificial neural network, ANN, wireless sensor networks, data calibration approach, mobile sensor network system, Ningbo city, Mobile communication, SVM, taxies, Vehicles, mobile computing, China, Cities and towns, Sensors, Hangzhou city, Mosaic, Monitoring, low-cost smart sensors, city scale sensing, moving vehicles, mobile radio, support vector machines, Air quality, Calibration, town and country planning, buses, PM2.5 measurements, smart cities, intelligent sensors, three-layer architecture, neural nets]
Jamming with Power Boost: Leaky Waveguide Vulnerability in Train Systems
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Modern-day train operations rely on wireless communications. Unlike other mobile systems, the train vehicle operations are tightly interwound with and remain physically close to the railway and the trackside infrastructure, providing a suitable platform to deploy leaky-waveguide-based communication. Due to the train system's safety-critical application and its exposure to the public, it is critical to address security in train communications. To investigate the availability of leaky waveguide communications, we first study prior leaky waveguide implementations in train systems and, based on those studies, construct a model to characterize the path loss of inside-waveguide propagation and the repeater implementations. Using our model, we analyze the jamming impact and contrast with jamming in free space without a waveguide. As a result, we establish that jamming the waveguide takes advantage of the waveguide infrastructure to extend its impact beyond the traditional jamming range and breaks the spatial dependence on the jamming source.
[leaky waveguide vulnerability, safety-critical application, Coaxial cables, railway communication, inside-waveguide propagation, Atmospheric modeling, jamming, waveguides, train system, leaky waveguide communication, Electromagnetic waveguides, Jamming, power boost, wireless communication, Wireless communication, Wireless sensor networks, mobile system, train vehicle operation, train communication, Rail transportation]
Low Cost IoT Software Development - Ingredient Transformation and Interconnection
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Sensing/actuating ingredients are software running on smart devices which are widely deployed in physical spaces and integrated into our daily life. An Internet of Things (IoT) application can be composed of multiple sets of sensing/actuating ingredients and each ingredient can satisfy specific requirement of users by processing specific physical data collected from physical world or committing an action. However, users' requirements for functionalities of ingredients and overall goals of IoT applications are diverse and may change. The application diversity leads to a rise in complexity as well as the cost of developing and maintaining the system software of an IoT application. In this work, we aim to reduce the design complexity and development cost of IoT application software. To achieve this goal, we use the software architecture PMDA which was proposed in our previous work. Here, we present a component-based formal methodology, namely FMDA, to transform the components of PMDA into sets of software modules as sensing/actuating functionality required. At the same time, we propose a connection mechanism, namely FM-CA, which can match and establish the interconnection between any two sensing/actuating ingredients as required. The behavior of FMDA and FMCA have been investigated. The evaluation results show that FMCA is more effective in reducing the cost if the requirements changing frequently or the failure probability of matching the required sensing/actuating ingredients is small.
[component-based formal methodology, FMCA, low-cost IoT software development, Transforms, physical spaces, sensing/actuating ingredients, Complexity theory, software architecture, connection mechanism, transform, Software architecture, software development cost reduction, Computer architecture, Sensors, Internet of Things application, physical data, object-oriented programming, FMDA, software modules, Internet of Things, software maintenance, IoT application, formal methodology, Software systems, smart devices, failure probability, software design complexity reduction, software metrics]
Relay and Power Splitting Ratio Selection for Cooperative Networks with Energy Harvesting
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
This paper addresses the problem of joint relay and power splitting ratio selection along with power allocation for an energy harvesting (EH) cooperative network, where the source and the relays can harvest energy from natural sources (e.g., solar) and radio frequency (RF) signals, respectively. To effectively use the harvested energy from the source, the relays employ the power splitting technique to scavenge energy from RF signals radiated by the source. We formulate this problem into a non-convex constrained optimization problem with the objective of maximizing system payoff, which is defined as the difference between system transmission benefit and system energy cost, and meanwhile minimizing system outage probability in both offline and online settings. In particular, we consider both direct transmission and relay transmission in this paper. Relay transmission is selected dynamically based on network channel conditions and available energy of EH nodes. Our simulation results reveal that considering direct transmission and selecting relay transmission and power splitting ratio dynamically can greatly improve system performance.
[simultaneous information and energy transfer, RF signals, relay transmission, Energy harvesting, Relays, outage probability, System performance, power splitting ratio selection, telecommunication network reliability, relay and power splitting ratio selection, cooperative network, concave programming, relay networks (telecommunication), generalized outer approximation (GOA), probability, Cooperative systems, Receivers, radiofrequency signal, energy harvesting, power allocation, nonconvex constrained optimization problem, RF signal, cooperative communication, cooperative networks, joint relay selection, Resource management]
A Memetic Algorithm for Dynamic Shortest Path Routing on Mobile Ad-hoc Networks
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
The shortest path routing (SPR) problem is a well-known challenge in the field of mobile network routing. The aim is to find the least cost path that connect a specific source node with a specific destination node. Although there are numerous algorithms to solve SPR, most of them consider only static environments in which the network topology and link-cost never change. A network with dynamic topologies and cost are indeed more challenging but more practical in real world applications. This paper presents a memetic algorithm for dynamic SPR (DSPR) problems in a mobile network. The proposed approach consists of three stages: genetic algorithm, local search and elitism-based immigrants procedure. Genetic algorithm (GA) is applied in the first stage to explore the search space and generate a new set of solutions. The generated solutions are further improved in the second stage by a local search algorithm. In third stage, an elitism-based immigrants procedure is activated to handle the dynamic changes by maintaining the diversity of the search process. The performance of the proposed algorithm has been evaluated on dynamic shortest path routing problem instances under both cyclic and acyclic environments. The study shows that, on both circumstances, the proposed algorithm is very stable with regards to dynamic network changes. This method is highly competitive compared to state-of-the-art algorithms in the literature as it outperformed these algorithms on all instances of dynamic routing during evaluation.
[mobile network routing, Heuristic algorithms, elitism-based immigrants procedure, acyclic environments, local search, Mobile ad hoc networks, Genetic algorithms, genetic algorithm, local search algorithm, mobile ad-hoc networks, Sociology, mobile ad hoc networks, source node, search problems, Memetics, dynamic topologies, telecommunication network topology, Routing, genetic algorithms, network topology, Statistics, memetic algorithm, telecommunication network routing, cyclic environments, dynamic shortest path routing]
FlashLite: A High Performance Machine for Data Intensive Science
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Data is predicted to transform the 21st century, fuelled by an exponential growth in the amount of data captured, generated and archived. Traditional high performance machines are optimized for numerical computing rather than IO performance or for supporting large memory applications. This paper discusses a new machine, called FlashLite, which addresses these challenges. The paper describes the motivation for the design, and discusses some driving application themes.
[Technological innovation, High Performance Computing, Computational modeling, Predictive models, exponential growth, parallel processing, Analytical models, FlashLite, IO Intensive Computing, Data Intensive Science, Memory management, numerical computing, numerical analysis, high performance machines, data intensive science, Data models, Real-time systems, large memory applications]
Big Data Analytics-Enhanced Cloud Computing: Challenges, Architectural Elements, and Future Directions
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
The emergence of cloud computing has made dynamic provisioning of elastic capacity to applications on-demand. Cloud data centers contain thousands of physical servers hosting orders of magnitude more virtual machines that can be allocated on demand to users in a pay-as-you-go model. However, not all systems are able to scale up by just adding more virtual machines. Therefore, it is essential, even for scalable systems, to project workloads in advance rather than using a purely reactive approach. Given the scale of modern cloud infrastructures generating real time monitoring information, along with all the information generated by operating systems and applications, this data poses the issues of volume, velocity, and variety that are addressed by Big Data approaches. In this paper, we investigate how utilization of Big Data analytics helps in enhancing the operation of cloud computing environments. We discuss diverse applications of Big Data analytics in clouds, open issues for enhancing cloud operations via Big Data analytics, and architecture for anomaly detection and prevention in clouds along with future research directions.
[Cloud computing, data analysis, Computational modeling, operating system, Big Data, Virtual machining, anomaly detection, Big Data analytics, virtual machine, cloud data center, virtual machines, operating systems (computers), information generation, Big data, Real-time systems, on demand applications, cloud computing, Business]
iCal: Intervention-free Calibration for Measuring Noise with Smartphones
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
It is valuable for the public to get access to real-time noise level information. Unfortunately, it is generally difficult for ordinary people to access real-time noise level information because of limited noise information stations and increased burden of carrying professional noise level meters. Being equipped with a high-quality microphone, a smartphone can potentially serve as a handy noise level meter. However, the straightforward use of sound measurements from smartphones leads to large measurement errors. As a result, it is essential to calibrate a smartphone before it can be used for noise level measurements. Little work has been done on automatic smartphone calibration for noise measurement purposes. In this paper we design a system called iCal for calibrating smartphones for accurate noise level measurements. The system consists of two key components: node-based calibration and crowdsourcing-based calibration. The node-based calibration enables an individual smartphone to do offline calibration, but suffers a slow-start issue. Complementing the node-based calibration, the crowdsourcing-based calibration leverages the power of crowdsourcing to maintain a lookup table, which a smartphone user can consult to find an approximate offset specific to its smartphone model. Thus, the slow-start issue can be effectively mitigated. The salient feature of iCal is human intervention free. We have implemented iCal on the android platform and experimental results show that the calibration error is as low as 3 dbA.
[Measurement errors, noise level measurements, node-based calibration, computerised instrumentation, smart phones, Calibration, Noise measurement, acoustic noise measurement, Noise level, Microphones, crowdsensing, mobile computing, intervention-free calibration, lookup table, Measurement uncertainty, iCal, Android platform, noise, crowdsourcing-based calibration, calibration, Smartphone, Smart phones]
OutSense: Out-of-Band Sensing with ZigBee Sensors for Channel Adaptation in Wireless LANs
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Wireless local area networks (WLANs) are pervasive but crowded nowadays. It is of great importance for access points (APs) to adapt to the changing traffic conditions. Exiting approaches for channel selection largely rely on local channel assessment and adopt greedy selection strategies. They suffer a major limitation that an AP fail to take various traffic demands of clients into account. We have witnessed that wireless sensor networks are increasingly deployed everywhere. A ZigBee sensor operates on the 2.4G radio spectrum which overlaps the spectrum used by most WiFi APs. As a result, a ZigBee sensor is able to sense the traffic of different AP channels. Motivated by this important observation, we present the design, implementation and evaluation of OutSence, a system that enables APs to takes traffic volumes of clients into account. It makes use of channel utilization sensed by ZigBee sensors and allows an AP to select a channel of good performance. The salient feature of OutSence is that it exploits in-situ ZigBee sensors for APs to quickly adapt to short-term traffic variations (e.g., order of minutes). We have fully implemented OutSence on Telos B sensor nodes and off-the-self APs. Extensive experiments have been conducted and conclusive results demonstrate that OutSence effectively improves overall WLAN performance.
[wireless sensor networks, Zigbee, wireless sensor network, channel selection, OutSense, channel adaptation, traffic sensing, frequency 2.4 GHz, Microprocessors, Computer architecture, Out-of-Band Sensing, Sensors, channel allocation, Zigbee Sensors, access point, out-of-band sensing, greedy algorithms, Interference, WLAN, short-term traffic variation, AP channel, channel assessment, wireless local area network, WiFi AP, Telos B sensor node, Channel allocation, greedy selection strategy, wireless LAN, Zigbee sensor, IEEE 802.11 Standard, telecommunication traffic, Channel Adaptation]
Learning Resource Management Specifications in Smartphones
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Over the past few years we have observed a phenomenal growth of smartphones. Smartphones are equipped with various hardware and software resources such as Bluetooth, camera and gravity sensors. If these resources are not managed appropriately, it may cause severe problems such as battery drains and system crashes. However, the specifications of resource management are usually implicit. In this paper, we investigate the problem of mining resource management specifications from off-the-shelf apps. Our key insight is that if a set of operations to a resource are frequently performed in a specific order, it must contain the specifications of how to manage the resource. We design a tool named Automatic Resource Specification Miner (ARSM), to automatically extract resource management specifications in smartphones. In our experiments, ARSM can mine tens of rules from 100 top rated Android apps within six hours. Our work is orthogonal to existing studies on diagnosing smartphone apps. With the resource management specifications discovered, ARSM can help them pinpoint more bugs in apps.
[program debugging, Programming Specification Mining, Humanoid robots, data mining, resource management specifications learning, ARSM tool, Smartphone Application, Batteries, Data mining, formal specification, off-the-shelf applications, Android (operating system), Android applications, learning (artificial intelligence), software resources, smart phones, Mobile Computing, smart phone application diagnosis, resource management specifications mining, hardware resources, Feature extraction, automatic resource specification miner tool, Resource management, Androids, automatic resource management specifications extraction, application bug detection, Smart phones]
BEP: Bit Error Pattern Measurement and Analysis in IEEE 802.11
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
The IEEE 802.11 is a set of Media Access Control (MAC) and Physical Layer (PHY) specifications which concern the Wireless Local Area Network (WLAN) service. However, most IEEE 802.11 WLAN services are easily affected by external elements, such as the homogeneous interference caused by the high-density deployment of IEEE 802.11 devices, the attenuation effect caused by complicated indoor obstacles, and the heterogeneous interference caused by other devices which operate out of unlicensed 2.4GHz ISM bands. In this paper, we first present a method to capture IEEE 802.11n Bit Error Patterns (BEP) under the network effect such as the homogeneous interference and the signal attenuation caused by obstacles. We separate the two issues by showing the specific BEP distributions under different channel conditions. In addition to the IEEE 802.11n BEP analysis, we further simulated the impact of the LTE-Unlicensed (LTE-U) signal to the IEEE 802.11ac at the 5GHz, and analyzed similar BEPs through a purely experiment based method.
[IEEE 802.11, IEEE 802.11ac, Wireless LAN, homogeneous interference, Long Term Evolution-Unlicensed, attenuation, PHY specification, attenuation effect, Quality of service, Channel State Information, BEP analysis, physical layer specification, bit error pattern, frequency 2.4 GHz, LTE-U, Attenuation, multi-access systems, IEEE 802.11n Standard, Channel state information, Long Term Evolution, PHY, ISM band, Interference, signal attenuation, WLAN, media access control, BEP, MAC, wireless local area network, LTE-U signal, wireless LAN, IEEE 802.11n, Legacy Mode, heterogeneous interference]
Unobtrusive Posture Recognition via Online Learning of Multi-dimensional RFID Received Signal Strength
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Activity recognition is a core component of ubiquitous computing applications (e.g., fall detection of elder people) since many of such applications require an intelligent environment to infer what a person is doing or attempting to do. Unfortunately, the success of existing approaches on activity recognition relies heavily on people's involvement such as wearing battery-powered sensors, which might not be practical in real-world situations (e.g., people may forget to wear sensors). In this paper, we propose a device-free, real-time posture recognition technique using an array of pure passive RFID tags. In particular, posture recognition is treated as a machine learning problem where a series of probabilistic model is built via learning how the Received Signal Strength Indicator (RSSI) from the tag array is distributed when a person performs different postures. We also design a segmentation algorithm to divide the continuous, multidimensional RSSI data stream into a set of individual segments by analyzing the shape of the RSSI data. Our approach for posture recognition eliminates the need for the monitored subjects to wear any devices. To the best of our knowledge, this work is the first on device-free posture recognition using low cost, unobtrusive RFID technology. Our experimental studies demonstrate the feasibility of the proposed approach for posture recognition.
[multidimensional RFID received signal strength, radiofrequency identification, probability, ubiquitous computing, Passive RFID tags, image segmentation algorithm, probabilistic model, machine learning problem, Support vector machines, device-free real-time posture recognition technique, RSSI, image segmentation, unobtrusive posture recognition, pose estimation, continuous multidimensional RSSI data stream, passive RFID tag array, Arrays, Received signal strength indicator, learning (artificial intelligence), Monitoring, online learning, activity recognition]
A Service-Oriented Mobile Cloud Middleware Framework for Provisioning Mobile Sensing as a Service
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Emerging Mobile Phone Sensing (M-Sense) systems enable a flexible large scale wireless sensing capability and also reduce the need of establishing the infrastructure of Wireless Sensor Network for collecting sensory information in the Internet of Things applications. M-Sense has been applied in numerous scenarios including mobile-health systems, environmental monitoring, vehicle ad hoc network, mobile social network, and so on. The drawback of existing M-Sense systems in terms of privacy, trust, less efficiency of participating in multiple sensing networks, has motivated the next generation sensing service provisioning approach. This paper introduces a generic service-oriented Mobile Host Sensing as a Service provisioning framework that allows a mobile device to provide sensing data to multiple parties based on mobile Web services. The proposed framework consists of the hybrid workflow-based control system, the dynamic Utility Cloud service, and the service provisioning scheduling model to enhance the quality of service provisioning. The prototype has been tested on real mobile devices and the details of the performance evaluation are presented.
[telecommunication security, mobile health system, trust, Cloud computing, sensory information collection, environmental monitoring, Mobile communication, privacy, flexible large scale wireless sensing capability, mobile computing, mobile social network, hybrid workflow-based control system, service provisioning scheduling model, service-oriented, mobile Web service, M-Sense system, vehicle ad hoc network, dynamic utility cloud service, service-oriented mobile host sensing as a service provisioning framework, Sensors, mobile Web services, cloud computing, service-oriented architecture, Internet of Things application, middleware, mobile radio, service-oriented mobile cloud middleware framework, mobile cloud, quality of service, Internet of Things, sensing data, mobile device, Wireless sensor networks, security of data, Web services, emerging mobile phone sensing, mobile sensing as a service, data privacy, quality of service provisioning, Mobile computing, Smart phones, workflow management system]
Participant-Density-Aware Privacy-Preserving Aggregate Statistics for Mobile Crowd-Sensing
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Mobile crowd-sensing applications produce useful knowledge of the surrounding environment, which makes our life more predictable. However, these applications often require people to contribute, consciously or unconsciously, location-related data for analysis, and this gravely encroaches users' location privacy. Aggregate processing is a feasible way for preserving user privacy to some extent, and based on the mode, some privacy-preserving schemes have been proposed. However, existing schemes still cannot guarantee users' location privacy in the scenarios with low density participants. Meanwhile, user accountability also needs to be considered comprehensively to protect the system from malicious users. In this paper, we propose a participant-density-aware privacy-preserving aggregate statistics scheme for mobile crowd-sensing applications. In our scheme, we make use of multi-pseudonym mechanism to overcome the vulnerability due to low participant density. To further handle sybil attacks, based on the Paillier cryptosystem and non-interactive zero-knowledge verification, we advance and improve our solution framework, which also covers the problem of user accountability. Finally, the theoretical analysis indicates that our scheme achieves the desired properties, and the performance experiments demonstrate that our scheme can achieve a balance among accuracy, privacy-protection and computational overhead.
[mobile crowd-sensing, user accountability, cryptography, participant-density-aware privacy-preserving aggregate statistics scheme, Mobile handsets, noninteractive zero-knowledge verification, Sybil attacks, Servers, participant-density, multipseudonym mechanism, Privacy, mobile computing, Aggregates, aggregate statistics, privacy-preservation, data privacy, Sensors, Paillier cryptosystem, Cryptography, mobile crowd-sensing applications, Principal component analysis, statistics]
EasiCrawl: A Sleep-Aware Schedule Method for Crawling IoT Sensors
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
With rapid development of smart hardwares and networking protocols, more and more IoT sensors are becoming publicly accessible through the Internet. Many semantic enhanced IoT sensors store the captured events in their description files, making the build of a generic IoT search engine possible. Crawling the events captured by these sensors is a fundamental step towards building this IoT search engine. However, this step faces a challenge due to sensors' sleep behavior and limited energy supply. Using traditional web access strategy for IoT application may cause unpredictable latency in receiving events with low power efficiency. In this paper, firstly the issue how to crawl newly captured events from periodically sleeping sensors is formulated as a schedule problem, which can be solved by constrained optimization. We take expected latency as the optimization object, as this indicates whether the wanted events can be gathered by crawlers in time. Then a sleep-aware schedule method, named EasiCrawl, is proposed for achieving near-optimal expected latency in receiving events. Finally, EasiCrawl is evaluated by simulations and a case study with real-world data from Xively. The simulation results show that EasiCrawl has lower latency than the periodic and greedy crawl strategy.
[smart hardwares, Schedules, search engines, crawling IoT sensors, Crawlers, Sensor phenomena and characterization, networking protocols, Sensor systems, Internet of Things, sleep-aware schedule method, Intelligent sensors, sensor sleep behavior, optimisation, sensors, EasiCrawl, Search engines, scheduling, Web access strategy, Internet, generic IoT search engine, semantic enhanced IoT sensors, constrained optimization, greedy crawl strategy]
Crowdsourcing Based Mobile Location Recognition with Richer Fingerprints from Smartphone Sensors
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
With the rapid advancements of mobile computing, mobile location recognition is becoming an important and useful service, which recognizes the logical locations of places/scenes that users are interested in, instead of physical coordinates. Most of the existing mobile location recognition systems utilize the image as visual fingerprint of a place, and need to construct a large-scale visual fingerprint database in advance. However, collecting visual fingerprints is a labor-intensive and time-consuming procedure. In order to address this problem, we propose a novel crowdsourcing-based framework, and leverage a variety of sensors embedded in smartphones to collect richer location fingerprints for exploring their positive effects. To achieve higher recognition accuracy, we propose an object-centric fingerprint searching which can sufficiently take advantage of smartphone sensors and determine more accurate searching space than the traditional user-centric method. We build a crowdsourcing-based database with richer fingerprints and implement a location recognition system, called CrowdLR. Extensive experiments verify that our object-centric method can achieve promising results maintaining around 10% precision higher than the user-centric method.
[fingerprint identification, Crowdsourcing, Visualization, Mobile Location Recognition, Fingerprint, object-centric fingerprint searching, Fingerprint recognition, Search problems, smart phones, Servers, Smartphone Sensor, CrowdLR, Global Positioning System, Object-centric, mobile computing, Databases, mobile location recognition, smartphone sensor, Sensors, crowdsourcing-based database, visual fingerprint]
Adaptive Path Profiling Using Arithmetic Coding
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Path profiling, which aims to trace a program's execution path, has been widely adopted in various areas such as record and replay, program optimizations, performance diagnosis, and etc. Many path profiling approaches have been proposed in the literature, including B.L. algorithm, and PAP. Unfortunately, both approaches suffer from large tracing overhead for representing long execution paths. In this paper, we propose AdapTracer, a path profiling approach based on arithmetic coding. There are two salient features in Adap-Tracer. First, it is space efficient by adopting a path profiling algorithm based on arithmetic coding. Second, it is adaptive by explicitly considering the execution frequency of each edge. We have implemented AdapTracer to profile Android applications. Our experimental evaluation uses modified JGF benchmarks to show AdapTracer's efficiency. Experimental results show that AdapTracer reduces the trace size by 44% on average and incurs execution overhead by 10% at most compared to PAP.
[Adaptation models, program diagnostics, adaptive path profiling approach, Humanoid robots, AdapTracer, Encoding, Decoding, program execution path traceability, modified JGF benchmarks, arithmetic codes, adaptive, Android (operating system), Path profiling, arithmetic coding, Android applications, Androids, Probes]
Fair Decentralized Congestion and Awareness Control for Vehicular Networks
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Challenges surrounding 802.11p vehicular networks have become more apparent in recent years within the research community. The exchange of periodical beacons on the control channel is mandatory to support higher layer applications such as active safety applications. However, the nature of such wireless vehicular networks imposes a lot of serious problems, especially the risk that the radio channel becomes quickly saturated. Thus, congestion control and awareness control are crucial mechanisms to control the load and ensure that each vehicle is able to communicate with its most relevant neighbours respectively. In this paper, we tackle jointly congestion and awareness control by taking into consideration 2-hops influence, hidden-terminals, SINR, received power, and a fair bandwidth indicator measuring the performance distribution among a coalition of vehicles in a highly volatile vehicular network. A two-steps proactive distributed algorithm called TPA&amp;plus;IAB is proposed to improve the fair bandwidth utilization. Each step of the algorithm achieves a distinct goal: Intelligent Adaptive Beaconing (IAB) is in charge of controlling the congestion while the Transmit Power Adaptation (TPA) is in charge of controlling which neighbour should be part of a coalition with respect to down-link channel qualities. Different from awareness control algorithms which assume an optimal channel model, here a more realistic model is used to estimate dynamically the best communication range based on a non-deterministic propagation model. Realistic simulations with NS-3 show that the solution presented outperforms the well-known D-FPAV algorithm in non-homogeneous graphs.
[Algorithm design and analysis, Measurement, telecommunication channels, D-FPAV algorithm, telecommunication congestion control, Heuristic algorithms, transmit power adaptation, fair decentralized congestion control, Vehicles, control channel, NS-3, Wireless communication, down-link channel qualities, Channel estimation, vehicular network, 802.11p vehicular networks, vehicular networks, Safety, mobile radio, radio channel, active safety, wireless vehicular networks, intelligent adaptive beaconing, nondeterministic propagation model, fair decentralized awareness control, wireless LAN, SINR]
HePA: Hexagonal Platform Architecture for Smart Home Things
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
In the internet era, where people are connected to each other, web server architectures have been developed and advanced. Now, we are witnessing the advent of the complex IoT (Internet of Things) era, where not only people, but also all devices on the planet are interconnected to each other and enormous amount of interactions is required. In this paper, we propose HePA (Hexagonal Platform Architecture), a platform architecture that is extremely scalable while maintaining required performance and reflecting requirements of the complex environment. We expect the HePA to become a reference architecture in this field.
[Protocols, Refrigerators, Ports (Computers), Smart homes, complex IoT, Internet of Things, HePA, hexagonal platform architecture, smart home things, Distributed Systems, Interoperability, home automation, Smart Home, file servers, Computer architecture, complex environment, platform architecture, Internet, Web server architectures]
An Estimation Maximization Based Approach for Finding Reliable Sensors in Environmental Sensing
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Emerging Internet of Things (IoT)-based environmental sensing projects provide large-scale sensing data from individual sensors with high reading frequencies. These readings are usually produced by commodity sensors with varied reliabilities, and inevitably contain noises and errors. Most existing data cleaning techniques focus on issues such as communication overhead reduction and energy preservation, and do not take advantage of the unaggregated data from individual sensors that IoT environmental sensing projects offer. In this paper, we propose an Expectation Maximization algorithm for finding reliable sensors in environmental sensing data that assumes the preservation of individual sensor readings and high reading frequencies. Our approach simultaneously finds the environmental feature model and the faulty state of the sensors. Our extensive experiments show that the proposed approach is significantly more effective than existing approaches. Particularly, in a case where reliable sensors and faulty sensors differ significantly in their readings, the maximum squared error for other approaches exceeds 200, but for our approach is only 1.23.
[Internet-of-Things, geophysical techniques, maximum squared error, Air quality, Cleaning, estimation maximization based approach, Batteries, data cleaning techniques, sensor faulty state, environmental sensing data, environmental sensing, environmental feature model, IoT environmental sensing projects, communication overhead reduction, reliable sensors, IoT-based environmental sensing projects, commodity sensors, Sensors, Internet, Reliability, expectation maximization algorithm, Monitoring, energy preservation]
Coordination Strategies for Agent Migrations in Wireless Sensor Networks
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Agent-based middleware platforms for wireless sensor networks (WSNs) have received a lot of attention during the last years, due to their great flexibility in re-programming, monitoring, handling, and optimizing the application as well as the whole system. Even though many algorithms have been proposed for the dynamic placement of agents within the WSN, they do not take into account the coordination aspects of such migrations. This not only may result in slow convergence but also in perpetual oscillations of agent migrations, degrading application and system performance. In this paper, we propose full-coordination, semi-coordination, and non-coordination agent migration strategies, and evaluate their convergence and network overhead. We also provide proofs that convergence is guaranteed when dynamic agent placement algorithms adopt our proposed strategies. Our results show that the semi-coordination strategy is superior in terms of both network overhead and convergence rate.
[Actuators, semicoordination agent migration strategy, wireless sensor networks, coordination strategy, Heuristic algorithms, network overhead minimization, WSNs, Middleware, telecommunication computing, Oscillators, agent-based middleware platforms, Convergence, Temperature sensors, network overhead, Wireless sensor networks, noncoordination agent migration strategy, agent migration, agent placement, full-coordination agent migration strategy, convergence rate, dynamic agent placement algorithms, application deployment, middleware]
An Adaptive and Compressive Data Gathering Scheme in Vehicular Sensor Networks
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
In vehicular sensor networks, probe vehicles can act as mobile sensors to monitor physical world and report to an urban sensing center. However, the distribution of probe vehicles is uneven over space and time. Data redundancy and vacancy are common phenomena for different spatiotemporal positions, which seriously degrade sensing efficiency and accuracy. To address this issue, we propose an adaptive and compressive data gathering scheme (AC-Sense) based on matrix completion theory. The scheme adaptively determines the locations where to obtain samples from so that the principal features of physical world can be captured with a reduced number of probe vehicles. The spatio-temporal correlation between sensor data is exploited to estimate the un-sampled data. Furthermore, we introduce a feedback mechanism to stabilize sensing performance according to the evaluation of data error. We perform extensive experiments based on real taxicab mobility traces and air quality data in Beijing. The experimental results show that the proposed scheme largely improves sensing efficiency while ensuring required data quality.
[Correlation, sensor data, wireless sensor networks, mobile sensors, matrix completion, sensing quality, adaptive data gathering scheme, feedback mechanism, Vehicles, feedback, sensing efficiency, spatio-temporal correlation, data redundancy, probes, AC-Sense, Sensors, Probes, urban sensing center, Monitoring, compressive data gathering scheme, mobile radio, Urban areas, data vacancy, urban sensing, vehicular sensor networks, Beijing, VSN, data communication, spatiotemporal positions, Spatiotemporal phenomena, taxicab mobility, matrix completion theory]
A Mixture Distribution Based System in BitTorrent-Like P2P Networks
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
In this paper, we develop a novel file sharing system based on a mixture distribution model working in the BitTorrentlike p2p networks. The BitTorrent's built-in &#x201C;tit-for-tat&#x201D; unchoking mechanism delays the initial file sharing process for newly joined peers as well as brings the problem of free-riding that peers only download from others but never contribute to the network. We demonstrate a file sharing mechanism which allows peers to share pieces according to different mixture distributions. The mechanism utilizes the historical contributions of peers in the network to inspire cooperation among peers, Along with the mixture distribution model, the peers can only obtain the whole file by contributing to the network continuously which deters the free-riding behaviors. We theoretically prove that the peers take the truthful revealing as their dominant strategy and our system can speed up the initial process of file sharing. The experiments show that the proposed system performs well and has good scalability, as well as prevents the free-riding problem elegantly.
[Algorithm design and analysis, Protocols, peer-to-peer computing, Scalability, BitTorrent-like P2P networks, mixture distribution based system, Thin film transistors, file sharing process, Servers, statistical distributions, free-riding problem, P2P, tit-for-tat unchoking mechanism delays, peer-to-peer applications, file sharing mechanism, file sharing system, Bandwidth, free-riding, BitTorrent, mixture distribution, Peer-to-peer computing, mixture models, TFT peer selection strategy]
Detecting and Mitigating P2P Eclipse Attacks
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Peer-to-Peer (P2P) protocols increasingly constitute the foundations for many large-scale applications as the inherently distributed nature of P2P easily supports both scalability and fault-tolerance. However, the decentralized design of P2P also exposes it to a variety of distributed threats with Eclipse Attacks (EAs) being a prominent type to impact P2P functionality. While the basic technique of divergent lookups has been demonstrated for suitability to mitigate EA, it can only (effectively) address limited variants of EAs. This paper investigates both the detection and mitigation potential of enhanced divergent lookups for handling complex EA scenarios. In addition, we propose an approach that can identify malicious peers with a high degree of accuracy. Our simulations have shown EA mitigation rates of up to 96% in case 25% of the peers are malicious. Also, our approach allows for anonymity-fostering, fully decentralized usage, and facilitating downstream mechanisms such as malicious peer removal.
[Protocols, Scalability, fault-tolerance, P2P functionality, downstream mechanisms, Overlay networks, complex EA scenario handling, EA mitigation rates, peer-to-peer protocols, P2P eclipse attack mitigation, protocols, anonymity fostering, peer-to-peer computing, Eclipse Attacks, Routing, Topology, Indexes, decentralized usage, computer network security, large-scale applications, P2P, fault tolerant computing, decentralized P2P design, Peer-to-peer computing, P2P eclipse attack detection]
Interaction between Network Partitioning and Churn in a Self-Healing Structured Overlay Network
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
We investigate the interaction between Network Partitioning and Churn (node turnover) in Structured Overlay Networks. This work is relevant both to systems with peaks of high stress (e.g., partitions, churn) or continuous high stress. It prepares the way for new application venues in mobile and ad hoc networks, which have high node mobility and intermittent connectivity, and undergo frequent changes in network topology. We evaluate existing overlay maintenance strategies, namely Correction-on-Change, Correction-on-Use, Periodic Stabilization, and Ring Merge. We define the reversibility property of a system as its ability to repair itself to provide its original functionality when the external stress is withdrawn. We propose a new strategy, Knowledge Base, to improve conditions for reversibility in the case of combined network partitioning and churn. By means of simulations, we demonstrate reversibility for overlay networks with high levels of partition and churn and we make general conclusions about the ability of the maintenance strategies to achieve reversibility. We propose a model, namely Stranger Model, to generalize the impact of simultaneous network partitioning and churn. We show that this interaction causes partitions to eventually become strangers to each other, which makes full reversibility impossible when this happens. Using this model, we can predict when irreversibility arrives, which we verify via simulation. However, high levels of one only, network partitioning or churn, do not hinder reversibility. In future work we will extend these results to real systems and experiment with applications that take advantage of reversibility.
[correction-on-change overlay maintenance strategy, Ring Overlay Merge, Predictive models, churn, Structured Overlay Networks, Network Partition and Churn, knowledge base strategy, Overlay networks, node turnover, knowledge based systems, correction-on-use overlay maintenance strategy, Maintenance engineering, Routing, Partition Tolerance, self-healing structured overlay network, mobile networks, ring merge overlay maintenance strategy, Complex systems, Stress, overlay networks, network partitioning, stranger model, Peer-to-peer computing, ad hoc networks, periodic stabilization overlay maintenance strategy]
Finding High-Level Topics and Tweet Labeling Using Topic Models
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Making sense of Twitter data streams is challenging due to the extremely high volume of data. One way to address this challenge is to consider these data streams as containing a set of high-level topics. In this research we address the problem of: given a collection of tweets about K high-level topics, how to find topic words that describe these topics as well as how to label each tweet with one of the K topics using a topic modeling approach. Current research has shown that applying topic modeling algorithms directly on tweets does not lead to good results. Hence one approach is to group related tweets together, so as to form a single &#x201C;pseudo-document&#x201D;, which is more informative than a single tweet. In this paper we evaluate different grouping schemes found in the literature and propose a new grouping scheme utilizing named entities and word collocations. Results show that our proposed scheme performs better than the existing approaches, to a some extent for all the test cases, and for both finding high-level topics and tweet labeling.
[pseudodocument, grouping scheme, text analysis, content-analysis, Blogs, social media mining, topic modeling algorithms, Media, Twitter, named entities, topic-modeling, grouping schemes, Twitter data streams, high-level topic model, word processing, microblogging, Distributed databases, Tagging, tweet labeling, social networking (online), Labeling, Resource management, word collocations]
Stethoscope: A Sustainable Runtime Debugger for Wireless Sensor Networks
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Debugging wireless sensor networks (WSNs) is notoriously difficult, due to the resource constraints on the sensors and distributed running of the debugged programs. Many bugs only manifest themselves during the actual operation of a network, thus requiring runtime debugging of the sensor program. A WSN debugger has to meet two important design criteria, namely saving energy and preserving responsiveness to normal system/network events during debugging. In this paper, we propose Stethoscope, a sustainable runtime debugger for WSNs. In Stethoscope, we devise a new technique called Quick Switch, which enables dynamic binary instrumentation in the RAM instead of the program flash. By incorporating a light-weight hooking mechanism, Stethoscope ensures runtime responsiveness of the debugged program. We implement Stethoscope and demonstrate its advantages with respect to efficacy, energy consumption, and memory cost.
[program debugging, quick switch, random-access storage, wireless sensor networks, Random access memory, Debugging, Switches, wireless sensor network, dynamic binary instrumentation, telecommunication computing, sustainable runtime debugger, Wireless sensor networks, Runtime, Sensor Network, Energy, WSN debugger, program flash, Stethoscope, Ash, light-weight hooking mechanism, stethoscope, Debug, RAM]
Decentralized Dynamic Participation in Participatory Sensing: A Correlated-Equilibrium Game Approach
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Participatory sensing has become a compelling and viable paradigm for large scale sensing data collection in recent years, with smartphone workers recruited from a public crowd. Since the quality of the sensing data is directly related to the performance of the workers, it is then crucial to enable effective participation for a large number of workers. In this paper, we propose a decentralized dynamic participation game framework for participatory sensing systems with heterogeneous sensing processes and smartphone workers. Based on economic studies, we characterize the behaviour of workers with the idea of correlated equilibrium. We propose a regret matching based participation algorithm to track the set of correlated equilibria in a distributed manner, where the knowledge of competitors' preference information is not required. Simulation results demonstrate that our algorithm achieves good convergence and effective participation.
[Economics, smartphone, Heuristic algorithms, Participatory sensing, correlated-equilibrium game approach, Medical services, game theory, decentralized dynamic participation game framework, smart phones, sensing data, correlated equilibrium, regret matching based participation algorithm, mobile computing, participatory sensing systems, Games, Data collection, smartphones, Sensors, Resource management, heterogeneous sensing processes]
Context-Aware Recruitment Scheme for Opportunistic Mobile Crowdsensing
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
The ubiquity of mobile devices coupled with the advances in Internet of Things (IoT) technologies has led to the development of large-scale applications that can collect information about people and their environments in real-time. Such applications are referred to as Mobile Crowdsensing (MCS). In MCS, tasks are allocated to participants (mobile devices) by a remote server according to the application requirements. The key challenge is reducing the energy consumption of the participating mobile devices. One of the effective approaches to reduce energy consumption of MCS applications is to improve efficiency of task allocation. An efficient task allocation approach can optimize several aspects of MCS applications such as task coverage (minimum number of participants required for a MCS task), data quality, and sensing costs. In this paper, we propose a novel Context-Aware Task Allocation (CATA) approach that aims to allocate sensing tasks to the best participant set while improving energy efficiency in MCS applications. Another important feature of the proposed CATA approach is that it preserves the privacy of participants' by only disclosing the less sensitive data to the server. The proposed approach employs local and global task allocation methods to enable two levels of data sharing and privacy. We describe the series of experiments that were conducted to validate our proposed approach in terms of coverage and efficiency.
[IoT technology, Mobile communication, Mobile handsets, Servers, Recruitment, mobile computing, power aware computing, CATA approach, Mobile Crowdsensing, context-aware task allocation approach, Context-aware systems, Sensors, opportunistic mobile crowdsensing, Internet of Things technology, Context, Opportunistic sensing, Internet of Things, context-aware recruitment scheme, mobile device, data privacy, Resource management, MCS, energy-efficiency, data sharing, Mobile computing]
Accelerating Crowdsourcing Based Indoor Localization Using CSI
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Indoor localization is of importance for many applications. Crowdsourcing individual users' measurements can provide accurate localization without costly site-survey. However, crowdsourcing based approaches suffer from the cold start problem, in which at the beginning of system deployment, there are insufficient users to contribute their measurements, resulting in inaccurate and time-inefficient localization. In this paper, we propose a hybrid indoor localization method to solve such problem, called ACIL. We first employ the inertial navigation technique to localize some core positions or paths. To tackle the inaccuracy problem, we propose an effective method that utilizes the channel state information (CSI) of wireless signals for accurate distance estimation. This method is based on a new observation: there is a ripple-like fading pattern in wireless signals upon moving objects. Leveraging this observation, our system is capable of calculating the distance of human's movement and his/her direction. We also propose a graph-matching algorithm to setup the correlation between the trajectory and floor map. With those extra obtained location information, the impact of cold start issue will be significantly mitigated, while the LBS can be guaranteed with high localization accuracy. Extensive experiments show that the effectiveness in the human localization and movement detection. Extensive experiments validate the great performance of our protocol in case of various human locations and diverse channel conditions.
[indoor navigation, human localization, wireless signals, graph theory, location information, scenario-free, Fingerprint recognition, channel state information, localization accuracy, indoor localization, core position localization, Channel State Information, Wireless communication, RSS reading, navigation, Databases, fading channels, core path localization, graph-matching algorithm, CSI, distance estimation, human movement distance, Trajectory, indoor radio, ACIL, fingerprinting, Floors, human movement direction, ripple-like fading pattern, floor map, Crowdsourcing, inertial navigation, human movement detection, crowdsourcing, channel condition, inertial navigation technique, hybrid indoor localization method, trajectory, user measurement crowdsourcing, RSSI, crowdsourcing based indoor localization, Inertial navigation, cold start issue]
PEVTS: Privacy-Preserving Electric Vehicles Test-Bedding Scheme
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Electric Vehicle (EV) infrastructure is relatively new in many countries. Due to the recency of an EV infrastructure, it is important to carry out a series of testing programs. Furthermore, authenticity for collection of data is necessary for testing programs in order to provide accurate results. At the same time, user privacy should not cease since tracing one's daily logistic movements or behaviour from the EV testing programs means breaching one's privacy. In this paper, we propose a novel solution PEVTS for enabling both data authenticity and user privacy concurrently. Our proposed system provides great flexibility to the authority to choose any arbitrary set of authenticated users for testing in every time period. At the same time, it provides anonymity for all participating users. Yet it can trace any vehicle within a time period for statistical purpose. We give a detailed description of our system. We also implement the prototype of our system to show its practicality.
[electrical engineering computing, Data privacy, electric vehicles, testing, PEVTS, Anonymity, Test-bed, privacy-preserving electric vehicles test-bedding scheme, data authenticity, Authenticity, Privacy, user privacy, Electric Vehicle, Authentication, Electric vehicles, data privacy, Cryptography, Testing]
Differentially Private Wireless Data Publication in Large-Scale WLAN Networks
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Wireless trace data play an important role in wireless network researches. However, publishing the raw WLAN traces poses potential privacy risks of network users. Therefore, it is necessary to sanitize users' sensitive information before these traces are published, and provide high data utility for wireless network researches as well. Although some existing works based on various anonymization methods have started to address the problem of sanitizing WLAN traces, the anonymization techniques cannot provide strong and provable privacy guarantees. Differential Privacy is the only framework that can provide strong and provable privacy guarantees. However, we find that existing studies on differential privacy fail to provide effective data utility on multi-dimensional and large-scale datasets. Aim at WLAN trace datasets that have unique characteristics of multi-dimensional and large-scale, this paper proposes a privacy-preserving data publishing algorithm which not only satisfies differential privacy but also realizes high data utility. Furthermore, the theoretical analysis shows the noise variance of our sanitization algorithm is O(logo(1) n/&#x03F5;2) which indicates the algorithm can achieve a higher data utility on large-scale datasets. Moreover, from the results of extensive experiments on an large-scale WLAN trace dataset, we also show that our sanitization algorithm can provide high data utility.
[Algorithm design and analysis, sanitization algorithm, Data privacy, Wireless LAN, large-scale WLAN networks, Differential privacy, differentially private wireless data publication, wireless local area networks, privacy-preserving data publishing algorithm, utility, WLAN trace dataset, computer network security, Privacy, Wireless sensor networks, Wireless networks, noise variance, data privacy, data utility, wireless LAN, wireless traces, data publication data]
Attribute-Based Keyword Search Efficiency Enhancement via an Online/Offline Approach
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Searchable encryption is a primitive, which not only protects data privacy of data owners but also enables data users to search over the encrypted data. Most existing searchable encryption schemes are in the single-user setting. There are only few schemes in the multiple data users setting, i.e., encrypted data sharing. Among these schemes, most of the early techniques depend on a trusted third party with interactive search protocols or need cumbersome key management. To remedy the defects, the most recent approaches borrow ideas from attribute-based encryption to enable attribute-based keyword search (ABKS). However, all these schemes incur high computational costs and are not suitable for mobile devices, such as mobile phones, with power consumption constraints. In this paper, we develop new techniques that split the computation for the keyword encryption and trapdoor/token generation into two phases: a preparation phase that does the vast majority of the work to encrypt a keyword or create a token before it knows the keyword or the attribute list/access control policy that will be used. A second phase then rapidly assembles an intermediate ciphertext or trapdoor when the specifics become known. The preparation work can be performed while the mobile device is plugged into a power source, then it can later rapidly perform keyword encryption or token generation operations on the move without significantly draining the battery. We name our scheme Online/Offline ABKS. To the best of our knowledge, this is the first work on constructing efficient multi-user searchable encryption scheme for mobile devices through moving the majority of the cost of keyword encryption and token generation into an offline phase.
[Cloud computing, attribute-based encryption, mobile phones, Search problems, Encryption, Servers, attribute-based keyword search, trapdoor generation, power consumption constraints, mobile computing, power aware computing, preparation phase, authorisation, data protection, keyword encryption, data privacy protection, token generation, intermediate ciphertext, Keyword search, multiuser searchable encryption scheme, information retrieval, cryptography, Offline, attribute list-access control policy, Multi-Owner/Multi-User, data user setting, online ABKS, offline ABKS, Searchable Encryption, mobile devices, data owners, encrypted data sharing, Mobile Devices, Power Consumption, Online]
STUMP - STalling offline password attacks Using pre-hash ManiPulations
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Offline password cracking has seen significant advances in recent years. This is mainly due to a dramatic increase in accessible computational speeds and the increased exploitation of GPUs for parallel processing. Cheaper and faster hardware, combined with new techniques, have allowed inexpensive GPUs to crack passwords at rates which only supercomputers could achieve previously. One inexpensive mitigation technique that we have uncovered is built on the core idea of pre-hash password manipulations. Our technique is named STUMP. Through rigorous empirical analysis, we demonstrate that STUMP can prevent offline parallel attacks - including pre-computed attacks utilizing rainbow tables - from cracking 99.718% of passwords that are &lt;;8-characters in length; STUMP has also shown to completely prevent the attacker from cracking passwords that are &#x2265; 8 characters in length i.e., (100% secure). Finally, for all cases, STUMP can be employed to stall the attacks - regardless of whether the attack is a laborious brute-force technique or a more intelligent dictionary attack - as neither will return the user's original password.
[brute-force technique, empirical analysis, Dictionaries, Graphics processing units, GPUs, offline parallel attacks, parallel processing, GPU, STUMP, security, dictionary attacks, authorisation, Parallel processing, Hardware, offline password cracking, password cracking, salts, dictionary attack, Radiation detectors, cryptography, brute-force attacks, stalling offline password attack-using-prehash manipulations, graphics processing units, rainbow tables, Authentication]
Execution Time Measurement of Virtual Machine Volatile Artifacts Analyzers
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Due to a rapid revaluation in a virtualization environment, Virtual Machines (VMs) are target point for an attacker to gain privileged access of the virtual infrastructure. The Advanced Persistent Threats (APTs) such as malware, rootkit, spyware, etc. are more potent to bypass the existing defense mechanisms designed for VM. To address this issue, Virtual Machine Introspection (VMI) emerged as a promising approach that monitors run state of the VM externally from hypervisor. However, limitation of VMI lies with semantic gap. An open source tool called LibVMI address the semantic gap. Memory Forensic Analysis (MFA) tool such as Volatility can also be used to address the semantic gap. But, it needs to capture a memory dump (RAM) as input. Memory dump acquires time and its analysis time is highly crucial if Intrusion Detection System IDS (IDS) depends on the data supplied by FAM or VMI tool. In this work, live virtual machine RAM dump acquire time of LibVMI is measured. In addition, captured memory dump analysis time consumed by Volatility is measured and compared with other memory analyzer such as Rekall. It is observed through experimental results that, Rekall takes more execution time as compared to Volatility for most of the plugins. Further, Volatility and Rekall are compared with LibVMI. It is noticed that examining the volatile data through LibVMI is faster as it eliminates memory dump acquire time.
[invasive software, semantic gap, public domain software, virtualization environment, Random access memory, Virtual Machine Introspection, captured memory dump analysis, virtualisation, Intrusion Detection System, storage management, Semantics, spyware, Semantic gap, execution time measurement, hypervisor, Malware, live virtual machine RAM dump, Kernel, digital forensics, malware, Hypervisor, Virtual machining, LibVMI open source tool, virtual machine introspection, memory forensic analysis tool, Rootkit, Virtual machine monitors, intrusion detection system, Memory Forensic Analysis, virtual machine volatile artifact analyzers, virtual infrastructure privileged access, advanced persistent threats, rootkit, virtual machines]
Reliable Routing Protocol for Delay Tolerant Networks
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
On post disaster scenarios, communication infrastructure can be seriously compromised, generating intermittent or null Internet access. Delay Tolerant Networks (DTNs) are a promising communication mechanism able to deal with connection disruptions enabling communication for affected people. DTNs forward messages through untrusted devices, which have better probability to reach destination. However, they are susceptible to attacks where participants forge their metrics in order for them to appear as a better alternative to route messages, thus most traffic is attracted to them. This problem is known as the blackhole attack. In this work we propose a routing protocol that verifies participants' interactions using the Guy Fawkes protocol for an encounter-based routing protocol which routes messages based on the interactions of nodes. We propose a transmission ticket in order to achieve accountability in the actions of nodes. Routing decisions are based on the past tickets collected by the nodes. Our protocol creates a more reliable routing path by preventing the creation of fake interactions, and therefore blackhole attacks. Results show that our protocol reduces the number of messages attracted by malicious peers performing a blackhole attack, maintaining good delivery rates and low overhead for different network scenarios.
[Measurement, reliability, malicious peers, Security, security, Routing protocols, reliable routing protocol, traffic, Context, communication infrastructure, route messages, routing path, disasters, Routing, delay tolerant networks, blackhole attack, Guy Fawkes protocol, disaster scenarios, Internet access, encounter-based routing protocol, routing protocols, communication mechanism, Peer-to-peer computing, Internet, blackhole attacks, DTN]
Robust Traffic Classification with Mislabelled Training Samples
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Traffic classification plays the significant role in the network security and management. However, accurate classification is challenging if the training data is contaminated with unclean traffic. Recent researches often assume clean training data, and hence performance reduced on real-time network traffic. To meet this challenge, in this paper, we propose a robust method, Unclean Traffic Classification (UTC), which incorporates noise elimination and suspected noise reweighting. Firstly, UTC eliminates strong noisy training data identified by a consensus filtering with multiple classifiers. Furthermore, UTC estimates the relevance of remaining training data and learns a robust traffic classifier. Through a number of experiments on a real-world traffic dataset, we show that the new method outperforms existing state-of-the-art traffic classification methods, under the extremely difficult circumstance with unclean training data.
[pattern classification, mislabelled training samples, noise elimination, network security, UTC, Ports (Computers), random forest, Noise measurement, machine learning, Training, unclean traffic classification, suspected noise reweighting, computer network management, consensus filtering, unclean Internet data, Training data, Clustering algorithms, Robustness, Internet, telecommunication traffic, traffic classification]
CER-IOS: Internal Resource Utilization Optimized I/O Scheduling for Solid State Drives
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Modern Solid State Drives (SSDs) integrate more internal resources to get higher performance and capacity. Improving internal resource utilization by exploiting internal parallelism is important to enhance the performance of SSDs. Unfortunately, the internal resource utilization of SSDs is limited at runtime in practice because of the practical access conflicts to internal resources. In this paper, we propose a Conflict Eliminated Requests Based I/O Scheduler (CER-IOS) to better utilize internal parallelism of flash chips by scheduling I/O requests in a more fine-grained way. We introduce Conflict Eliminated Requests (CERs) in which parallelizable memory requests are grouped during the process of address translation in Flash Translation Layer. To schedule conflicting requests, we propose a small CER size prioritized resource distribution scheme, that ensures internal resources can always be distributed to valuable conflicting requests to further improve the efficiency of resource utilization. Our extensive experimental evaluation results show that CER-IOS provides significant improvement of resource utilization at runtime and reduces average I/O latency largely compared to state-of-the-art I/O schedulers implemented in operating systems.
[flash chips, parallelizable memory requests, Access Conflict, CER-IOS, parallel processing, internal resource utilization, resource distribution scheme, conflict eliminated requests based I/O scheduler, Runtime, flash memories, resource allocation, Operating systems, Ash, solid state drives, Parallel processing, I/O Scheduler, SSD, address translation process, flash memory, Scheduling, Resource Utilization, flash translation layer, Flash Memory, Internal Parallelism, Solids, Resource management]
RDMA-Based Cooperative Caching for a Distributed File System
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Efficient caching of file data is critical in order to achieve high performance in data-intensive applications. However, only a limited amount of memory is usually available to cache files in client nodes even on high-performance computing platforms. Cooperative caching is an approach that enables client nodes to share memory for file caching and thereby provide a large amount of memory for the file cache in the aggregate. Many studies have confirmed the efficacy of applying cooperative caching to distributed file systems. However, to the best of our knowledge, no study has evaluated an implementation of cooperative caching integrated into a modern distributed file system running on a high-speed network. In this paper, we propose a method that improves the performance of a distributed file system oriented to high-performance computing by integrating cooperative caching into it. In the proposed method, the metadata server of the distributed file system maintains information about the cache in all client nodes, and provides clients with the predicted cache location of any requested file. Further, InfiniBand RDMA is utilized to achieve fast cache transfer between the page caches of client nodes. Implementation of the proposed method in the Gfarm distributed file system and measurement of the performance of three real-world data-intensive applications indicate that the proposed method achieves a maximum speedup of 5.8%.
[RDMA, data-intensive applications, Cooperative caching, Fuses, InfiniBand, remote direct memory access, Metadata, cache storage, InfiniBand RDMA, client node page caches, Servers, parallel processing, cache transfer, file caching, Memory management, Distributed databases, metadata server, Gfarm distributed file system, distributed databases, Distributed file systems, cooperative caching, Kernel, high-performance computing]
Accelerating Cloud Storage System with Byte-Addressable Non-Volatile Memory
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
As building block for cloud storage, distributed file system uses underlying local file systems to manage objects. However, the underlying file system, which is limited by metadata and journaling I/O, significantly affects the performance of the distributed file system. This paper presents an NVM-based file system (referred to as NV-Booster) to accelerate object access for storage node. The NV-Booster leverages byte-addressability and persistency of nonvolatile memory (NVM) to speedup metadata accesses and file system journaling. With NV-Booster, metadata is kept in NVM and accessed in byte-addressable manner through memory bus, while object is stored on hard disk and accessed from I/O bus. In addition, proposed NV-Booster enables fast object search and mapping between object ID and on-disk location with an efficient in-memory namespace management. NV-Booster is implemented in kernel space with NVDIMM and has been extensively evaluated under various workloads. Our experiments show that NV-Booster improves Ceph performance up to 10X, compared to the Ceph with existing local file systems.
[Cloud computing, NV-Booster system, storage node, byte-addressable nonvolatile memory, distributed file system, Random access memory, cloud storage system, NVM persistency, file system journaling, Metadata, Local File Syetm, NVM byte-addressability, Phase change materials, Cloud Storage, Non-volatile Memory, storage management, File systems, Nonvolatile memory, object search, in-memory namespace management, cloud computing, NVM-based file system, Ceph performance, Memory management, object mapping, metadata access]
In-memory Query System for Scientific Dataseis
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
The growing gap between compute performance and I/O bandwidth coupled with the increasing data volumes has resulted in a bottleneck to the traditional post-simulation data processing method. Hence in-situ computing and query-driven data analysis are important techniques to minimize data movement. By taking advantage of the growing memory capacity on supercomputers, we developed an in-memory query system for scientific data analysis. Our approach is a combination of bitmap indexing, spatial data layout re-organization, distributed shared memory, and location-aware parallel execution. Our evaluations using real scientific datasets showed that we can aggregate the memory capacity from thousands of computes nodes to analyze a 750GB simulation dataset without transferring data to remote nodes or storage systems. Comparing to traditional solutions based on out-of-core parallel file systems, we achieve significant higher query performance.
[distributed shared memory, I/O bandwidth, memory capacity aggregation, supercomputers, scientific data, in-memory query system, query processing, Analytical models, database indexing, In-situ computing, location-aware parallel execution, spatial data layout re-organization, Data analysis, Computational modeling, indexing, postsimulation data processing method, bitmap indexing, parallel databases, query-driven analysis, in-situ computing, scientific data analysis, memory capacity, distributed shared memory systems, scientific information systems, data movement minimization, Data models, data volumes, Arrays, compute performance analysis, scientific datasets, Indexing, query-driven data analysis]
A Genetic Programming Approach to Design Resource Allocation Policies for Heterogeneous Workflows in the Cloud
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
When dealing with very large applications in the cloud, higher costs do not always result in better turnaround times, particularly for complex workflows with multiple task dependencies. Thus, resource allocation policies are needed that can determine when using expensive but faster resources is best and when it is not. Manually developing such heuristics is time consuming and limited by the subjective beliefs of the developer. To overcome such impediments, we present an automatic method that designs and evaluates a large set of policies using a genetic programming approach. Our method finds a robust set of policies that adapt to changes in workload while using resources efficiently. Our results show that our genetic programming designed policies perform better than greedy and other human designed policies do.
[Cloud computing, genetic programming, resource allocation policies, heterogeneous workflows, Grammar, genetic algorithms, Statistics, Computer science, resource allocation, Sociology, Genetic programming, Resource management, cloud computing]
MDCP: Measurement-Aware Distributed Controller Placement for Software Defined Networks
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
The rapid development of software defined measurement has significantly improved network measurement and monitoring. The key challenge for software defined measurement is to design a low-cost measurement framework which has minimum impact on the network. The state-of-the-art approaches mainly focus on reducing the measurement overhead by sampling or aggregation. However, little attention has been devoted to eliminating this issue in the physical layer. We observe that the placement of the controllers significantly affects the measurement overhead for software defined measurement. Based on this observation, we rethink software defined measurement frameworks and propose a novel scheme to minimize the measurement overhead. Our approach is application-agnostic, cost-effective and robust to traffic dynamics. We formulate the measurement-aware distributed controller placement (MDCP) problem as a quadratic integer programming problem, which takes both the synchronization cost and the flow statistics collection cost into account. Due to its high computational complexity, we develop two novel algorithms to efficiently approximate near-optimal placements. In particular, we employ an algorithm with an approximation ratio of 1.61 to obtain the placement in the discrete approximation algorithm. We conduct experiments on over 240 real network topologies and the results demonstrate the effectiveness of MDCP. Trace-driven simulations verify that our proposal is robust to traffic dynamics and can reduce 40% of the measurement overhead on average.
[MDCP, integer programming, software defined measurement, software defined networks, software defined networking, Control systems, Physical layer, flow statistics collection cost, Synchronization, quadratic programming, discrete approximation algorithm, trace-driven simulation, computer network management, quadratic integer programming problem, network monitoring, network measurement, MDCP problem, Approximation algorithms, Software, measurement-aware distributed controller placement, Software measurement, synchronization cost, Monitoring, computational complexity]
Electricity Cost Minimization in Distributed Clouds by Exploring Heterogeneity of Cloud Resources and User Demands
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Distributed clouds, consisting of multiple data centers located at different geographical locations, provide a plethora of services to users. They however consume enormous amounts of electricity to power their data centers. The electricity bill is almost 30%-50% of their operational costs. Minimizing the electricity cost of distributed clouds thus is crucial to reduce the operational cost of their cloud service providers. In this paper, we study the problem of minimizing the electricity cost of a distributed cloud, by exploring the heterogeneities of cloud resources and user demands, and time-varying electricity prices, for which we first propose a two-stage optimization framework: dispatching user task requests to different data centers by incorporating the resource demands of the task requests, the workload, and the electricity price in each data center, and energy consumption profiles of different servers in each data center; followed by further energy optimization within each data center through consolidating Virtual Machines (VMs) to different servers to improve the resource utilization ratio. One critical constraint on such task dispatch and VM consolidation is to meet various user Service Level Agreements (SLAs), which include average task scheduling delays and resource demand violation limitations. Under the proposed framework, we then devise efficient scheduling algorithms for task dispatching and VM consolidations, while keeping both the average scheduling delay and resource demand violation limitation of each admitted task met. We finally evaluate the performance of the proposed algorithms through experimental simulations, using real data sets - the real electricity prices and task traces. Experimental simulation results demonstrate that the proposed algorithms are promising.
[Cloud computing, Energy consumption, electricity cost minimization, resource demand violation limitation, two-stage optimization framework, SLA, time-varying electricity price, Servers, contracts, computer centres, Optimization, cloud resource utilization ratio, service level agreement, data center, virtual machine, distributed cloud computing, average scheduling delay, Distributed databases, virtual machines, Dispatching, Delays, VM consolidation, cloud computing, minimisation]
Scalable and Fault-Tolerant Cloud Computations: Modelling and Implementation
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
This paper presents a formal model for science clouds, capable of predicting and controlling resources scalably, as well as its implementation as an open source solution, called Chiminey. The feasibility of Chiminey is shown using case studies on biophysics and structural chemistry computations. Big data is acquired from scientific instruments such as synchrotrons and atomic force microscopes. The model takes into account the architecture of the overall parallel and distributed system including large-scale data sources; data sinks, for example petabyte research data stores; and cluster or cloud virtual resources and infrastructures characterised by users in simple parameters upfront. Chiminey is developed to control large numbers of processes and to provide a reliable computing and data management, which can be used by researchers without having to learn extensive infrastructure concepts and technologies.
[fault-tolerant cloud computations, Cloud computing, data management, parallel architectures, public domain software, biophysics chemistry computations, large-scale data sources, science clouds, reliability, distributed system, cloud virtual resources, data sinks, Fault tolerance, Fault tolerant systems, IP networks, cloud computing, scalable cloud computations, Science cloud, parallel system, fault tolerance, Computational modeling, structural chemistry computations, Big Data, modelling, software fault tolerance, Connectors, formal model, open source Chiminey solution, HPC, visualisation]
Using Analytical Models to Bootstrap Machine Learning Performance Predictors
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Performance modeling is a crucial technique to enable the vision of elastic computing in cloud environments. Conventional approaches to performance modeling rely on two antithetic methodologies: white box modeling, which exploits knowledge on system's internals and capture its dynamics using analytical approaches, and black box techniques, which infer relations among the input and output variables of a system based on the evidences gathered during an initial training phase. In this paper we investigate a technique, which we name Bootstrapping, which aims at reconciling these two methodologies and at compensating the cons of the one with the pros of the other. We analyze the design space of this gray box modeling technique, and identify a number of algorithmic and parametric trade-offs which we evaluate via two realistic case studies, a Key-Value Store and a Total Order Broadcast service.
[Cloud computing, black box techniques, Computational modeling, bootstrap machine learning performance predictors, key-value store, Knowledge based systems, Predictive models, cloud environments, analytical models, Training, Analytical models, total order broadcast service, computer bootstrapping, performance modeling, white box modeling, gray box modeling technique, Prediction algorithms, cloud computing, learning (artificial intelligence), elastic computing, software performance evaluation]
Multi-objective Optimization Algorithm Based on BBO for Virtual Machine Consolidation Problem
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Cloud computing is a promising technology having ability to influence the way of the provision of computing and storage resources through virtual machine (VM). VM Consolidation is an efficient way to improve power efficiency and quality guarantee for on-demand services. However, it is an integer programming problem and as well as a NP-hard problem to find optimal solutions within polynomial time. In this paper, the VM consolidation problem is formulated as a multi-objective optimization problem, which has three conflicting objectives, i.e., reducing power consumption, achieving good load balancing and shortening VM migration time. We propose a multi-objective optimization algorithm based on biogeography-based optimization (BBO) for the VM consolidation problem, which is named as MBBO/DE: Multi-objective Biogeography-Based Optimization algorithm hybrid with Differential Evolution. It utilizes cosine migration model, differential strategies and Gaussian mutation model to improve the quality of habitats and the ability of finding optimal solutions. Experiments have been conducted to evaluate the effectiveness of MBBO/DE using synthetic and real-world instances. Experimental results show that MBBO/DE obtains a better performance while simultaneously reducing power consumption and achieving good load balancing within a satisfactory time as compared to genetic algorithm (GA), differential evolution (DE), ant colony optimization (ACO) and BBO.
[Cloud computing, load balancing, integer programming, virtual machines consolidation, multiobjective optimization algorithm, Servers, on-demand services, Optimization, BBO, Genetic algorithms, resource allocation, cosine migration model, computing resources, multiobjective biogeography-based optimization algorithm hybrid-with-differential evolution, Gaussian mutation model, polynomial time, differential strategies, cloud computing, biogeography-based optimization, conflicting objectives, Power demand, storage resources, optimal solutions, MBBO/DE, Virtual machining, power efficiency improvement, VM consolidation problem, NP-hard problem, power consumption reduction, Gaussian processes, virtual machines, virtual machine consolidation problem, Load management, quality guarantee, integer programming problem, multi-objective optimization, VM migration time reduction, computational complexity]
Resource Provision for Batch and Interactive Workloads in Data Centers
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
In this paper we describe a scheduling framework that allocates resources to both batch jobs and interactive jobs simultaneously in a private cloud with a static amount of resources. In the system, every job has an individual service level agreement (SLA), and violating the SLA incurs penalty. We propose a model to formally quantify the SLA violation penalty of both batch and interactive jobs. The analysis on the interactive jobs focuses on queuing analysis and response time. The analysis on batch jobs focuses on the non-preemptive job scheduling for multiple processing units. Based on this model we also propose algorithms to estimate the penalty for both batch jobs and interactive jobs, and algorithms that reduce the total SLA violation penalty. Our experiment results suggest that our system effectively reduces the total penalty by allocating the right amount of resources to heterogeneous jobs in a private cloud system.
[Algorithm design and analysis, Cloud computing, Schedules, Heuristic algorithms, data centers, nonpreemptive job scheduling, contracts, processor scheduling, Multiprocessing systems, static resources, multiple processing units, resource allocation, Scheduling algorithms, interactive jobs, queuing analysis, Parallel processing, cloud computing, heterogeneous jobs, Client-sever systems, multiprocessing systems, queueing theory, batch workload, total SLA violation penalty reduction, Dynamic scheduling, Scheduling, interactive workload, computer centres, service level agreement, resource provisioning, batch jobs, response time, Time factors, Resource management, private cloud system]
Cost-Effective Resource Configuration for Cloud Video Streaming Services
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Video streaming services are migrating to cloud environments for the economic expense with good scalability. However, cloud providers offer flexible resource configurations, e.g., on-demand, reserved and spot instances, with significant different pricing policies, of which one single configuration is suboptimal for cloud video streaming services. In this paper, we propose hybrid configuration schemes of cloud video streaming services to reduce the cost. To achieve this goal, we first introduce a lightweight prediction algorithm to predict the future video traffic. With the predicted video traffic, we then give the Hybrid-R hybrid configuration scheme by configuring both on-demand and reserved instances, and the Hybrid-RS hybrid configuration scheme by further configuring spot instances. Our evaluations using traces from real video service providers show that our configuration schemes can reduce cost by at least 20% compared to the unoptimized ones with negligible overhead.
[cost reduction, Cloud computing, Quality of service, cloud environments, Virtual machining, cloud video streaming services, cost-effective resource configuration, lightweight prediction algorithm, resource allocation, pricing policies, Education, Pricing, on-demand instances, Streaming media, Prediction algorithms, resource configurations, real video service providers, video streaming, cloud providers, cloud computing, video traffic, pricing, hybrid-RS hybrid configuration scheme]
Service Level Agreement(SLA) Based SaaS Cloud Management System
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Cloud computing has emerged as a new computing paradigm which has revolutionized the IT industry. It has particularly transformed the licensing of software products which are now being offered as a Service on pay-as-you-go basis. This has tremendously increased the complexity for software providers as they now have to not only manage their resources on which software are hosted but also they need to provide expected Quality of Service for customers. The Quality of Service (QoS) required by customers is guaranteed using a legal document SLA (Service Level Agreement). Current, resource management systems do not cater to the needs of a Software as a Service (SaaS) provider who requires to provide flexible and low cost services while not affecting their profit and market share. Most of them focus either at infrastructure level or at platform level. This work fills this gap by proposing a novel SLA based resource management system designed after analysing requirements of SaaS in Clouds. The proposed system is implemented using latest technologies and can scale in and out depending on updates in the user demand. We present the architectural design and evaluate the implementation with a real case study in a real Cloud environment.
[Cloud computing, software as a service, SaaS cloud management system, resource management systems, Software as a service, Quality of service, SLA, quality of service, contracts, service level agreement, resource allocation, Resource management, cloud computing, Monitoring]
WattTime: Novel System Power Model and Completion Time Model for DVFS-Enabled Servers
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Power consumption costs takes up to half of the operational expenses of data centers, making power management a critical concern. Advances in processor technology provide fine-grained control over operating frequency of processors and this control can be used to trade off power for performance. We show that existing power models incorrectly assume quadratic relationship between power and frequency, leading to higher inaccuracy in prediction. Moreover, existing performance models have significant error margins while predicting performance of memory or file-intensive tasks and HPC applications due to negligence of the combined effects of frequency and CPU variations on the task execution time. In this paper, we empirically derive power and completion time models using linear regression with CPU utilization and operating frequency as parameters. We validate our power model on several Intel and AMD processors by predicting within 2-7% of measured power. We validate our completion time model using five kernels of NASA Parallel Benchmark suite and five CPU, memory and file-intensive benchmarks on four heterogeneous systems and predicting within 1-6% of observed performance.
[AMD processors, network servers, modeling, regression analysis, Predictive models, linear regression, Intel processors, completion time, Servers, power consumption, heterogeneous systems, Power measurement, power aware computing, Benchmark testing, operating frequency, power, NASA Parallel Benchmark suite, frequency scaling, Power demand, Multicore processing, Computational modeling, WattTime, microprocessor chips, DVFS-enabled servers, CPU utilization, dynamic voltage and frequency scaling, system power model, completion time model, DVFS]
Thread Count Prediction Model: Dynamically Adjusting Threads for Heterogeneous Many-Core Systems
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Determining an appropriate thread count for a multithread application running on a heterogeneous many-core system is crucial for improving computing performance and reducing energy consumption. This paper investigates the interrelation between thread count and computing performance of applications, and designs a prediction model of the optimum thread count on the basis of Amdahl's law combined with regression analysis theory to improve computing performance and reduce energy consumption. The prediction model can estimate the optimum tread count relying on the program running behaviors and the architecture characteristics of heterogeneous many-core system. Using the estimated optimum thread count, the number of the active hardware threads and processing cores on the many-core processor is dynamically adjusted in the process of thread mapping to improve the energy efficiency of entire heterogeneous many-core system. The experimental results show that, using this paper proposed thread count prediction model, on an average, the computing performance is improved by 48.6%, energy consumption is reduced by 59%, and additional overhead introduced is 2.03% compared with that of the traditional thread mapping for the PARSEC benchmark programs run on an Intel MIC heterogeneous many-core system.
[Energy consumption, Instruction sets, thread count prediction model, regression analysis, Prediction model, Predictive models, thread mapping, Microwave integrated circuits, power aware computing, Amdahls law, energy efficiency, Benchmark testing, Message systems, multiprocessing systems, Multicore processing, multi-threading, PARSEC benchmark programs, Computing performance, Intel MIC system, Heterogeneous many-core system, energy conservation, heterogeneous many-core systems, Energy efficiency, energy consumption reduction, Heterogeneous computing, Optimum thread count]
PROP: Using PCIe-Based RDMA to Accelerate Rack-Scale Communications in Data Centers
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
In order to reduce the demands on bandwidth of core layer network, data center operators usually assign tasks of the same job to servers that are located in the same rack, leading to the fact that 80% of the traffic originated from servers retains in the same rack. As a result, providing sufficient network capacity inside racks becomes critical to the Quality-of-Service of current data center applications. In this paper, we propose PROP, a novel hybrid network architecture which leverages PCIe-based RDMA to reinforce rack-scale connectivity in data centers. In our design, intra-rack bulk data transfers will be accelerated by a dedicated high-bandwidth PCIe-compliant network while complemented with the existing Ethernet network. In addition, we develop a proprietary PCIe-based RDMA hardware which can allow the servers in the same rack to exchange data in main memory without involving the operating system and the processors. We also implement a software stack to enable existing socket-based applications to transparently utilize the proposed dedicated network system. As the preliminary stage, this paper focuses on exploiting the unique design point and implements an FPGA-based prototype to validate the technical feasibility of the proposed architecture.
[data centers rack-scale connectivity, PCIe-based network, field programmable gate arrays, PROP, Switches, remote direct memory access, rack-scale communications, FPGA-based prototype, Servers, Network interfaces, rack-scale communication, PCIe-compliant network, Computer architecture, Bandwidth, hybrid network architecture, data center network architecture, RDMA, serial I/O bus standard, software stack, Receivers, PCI Express, computer centres, Ethernet network, peripheral interfaces, file organisation, PCIe-based RDMA hardware, intra-rack bulk data transfers]
Energy-Aware Caching
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
To achieve higher performance, cache sizes have been steadily increasing in computer processors and network systems. But caches are often over-provisioned for peak demand and underutilized in typical non-peak workloads. As caches consume substantial power, this results in significant amounts of wasted energy. To address this, existing works turn off parts of the cache when they do not contribute to higher performance. However, while these methods are effective empirically, they lack provable performance bounds. In addition, existing works focus on processor caches and are not applicable to network caches where data size and cost can vary. In this paper, we study the energy-aware caching (EAC) problem, and seek to minimize the total cost incurred due to cache misses and energy consumption. We propose three algorithms to solve different variants of this problem. The first is an optimal offline algorithm that runs in O(kn log n) time for a size k cache and n cache accesses. Then, we propose a simple online algorithm for uniform data size and cost that is 2 + h/(h-h+1 competitive compared to an optimal algorithm with a size h &#x2264; k cache. Lastly, we propose a 2 + h-1/(h-h+1) competitive online algorithm that allows arbitrary data sizes and costs. We give an efficient implementation of the algorithm that takes O(log k) amortized time per cache access, and also present an adaptive version that reacts to workload patterns to achieve better real-world performance. Using trace driven simulations, we show our algorithm has substantially lower cost than algorithms focused on maximizing cache hit rates or minimizing energy usage alone.
[Computers, Energy consumption, algorithms, O(log k) amortized time, Turning, cache storage, uniform data size, competitive online algorithm, Program processors, power aware computing, Loading, cache access, energy usage minimization, cache sizes, computer processors, energy consumption, cost minimization, Load modeling, cache misses, optimal offline algorithm, energy-aware caching, O(kn log n) time, energy efficient computing, arbitrary data sizes, Caching, Memory management, energy conservation, network systems, workload patterns, cache hit rate maximization, computational complexity]
Fast and Accurate Simulation of Multithreaded Sparse Linear Algebra Solvers
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
The ever growing complexity and scale of parallel architectures imposes to rewrite classical monolithic HPC scientific applications and libraries as their portability and performance optimization only comes at a prohibitive cost. There is thus a recent and general trend in using instead a modular approach where numerical algorithms are written at a high level independently of the hardware architecture as Directed Acyclic Graphs (DAG) of tasks. A task-based runtime system then dynamically schedules the resulting DAG on the different computing resources, automatically taking care of data movement and taking into account the possible speed heterogeneity and variability. Evaluating the performance of such complex and dynamic systems is extremely challenging especially for irregular codes. In this article, we explain how we crafted a faithful simulation, both in terms of performance and memory usage, of the behavior of qr_mumps, a fully-featured sparse linear algebra library, on multi-core architectures. In our approach, the target high-end machines are calibrated only once to derive sound performance models. These models can then be used at will to quickly predict and study in a reproducible way the performance of such irregular and resource-demanding applications using solely a commodity laptop.
[Algorithm design and analysis, numerical algorithm, parallel architectures, mathematics computing, Predictive models, directed acyclic graph, dynamic scheduling, portability, Runtime, faithful simulation, Computer architecture, numerical analysis, task-based runtime system, Parallel processing, sound performance model, Libraries, sparse linear algebra library, multithreaded sparse linear algebra solver, multi-threading, multicore architecture, parallel architecture, DAG, monolithic HPC scientific application, dynamical scheduling, directed graphs, Linear algebra, high performance computing, sparse matrices]
Implementation of an Accurate and Efficient Compensated DGEMM for 64-bit ARMv8 Multi-Core Processors
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
This paper presents an implementation of an accurate and efficient compensated Double-precision General Matrix Multiplication (DGEMM) based on OpenBLAS for 64-bit ARMv8 multi-core processors. Due to cancellation phenomena in floating point arithmetic, the results of DGEMM may not be as accurate as expected. In order to increase the accuracy of DGEMM, we compensate the error introduced by its dot product kernel (GEBP) by applying an error-free transformation to rewrite the kernel in assembly language. We optimize the computations in the inner kernel through exploiting loop unrolling, instruction scheduling and software-implemented register rotation to exploit instruction level parallelism (ILP). We also conduct a priori error analysis of the derived CompDGEMM. Our compensated DGEMM is as accurate as the existing quadruple precision GEMM using MBLAS, but is up to 6.4x faster. Our parallel implementation achieves good performance and scalability under varying thread counts across a range of matrix sizes evaluated.
[Algorithm design and analysis, assembly language, Error analysis, Registers, ILP, parallel processing, floating point arithmetic, GEBP, processor scheduling, instruction scheduling, a priori error analysis, loop unrolling, word length 64 bit, compensated DGEMM, double-double arithmetic, MBLAS, Libraries, OpenBLAS, dot product kernel, Kernel, microcontrollers, error compensation, multiprocessing systems, CompDGEMM, Multicore processing, BLAS, error analysis, ARMv8 multicore processors, cancellation phenomena, error-free transformation, instruction level parallelism, software-implemented register rotation, matrix multiplication, rounding error, 64-bit ARMv8 processor, double-precision general matrix multiplication, quadruple precision GEMM]
Communication Models for Distributed Intel Xeon Phi Coprocessors
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
The emergence of accelerator technology in current supercomputing systems is changing the landscape of supercom-puting architectures. Accelerators like GPGPUs and coprocessors are optimized for parallel computation while being more energy efficient. Their computational power per watt plays a crucial role in developing exaflop systems. Today's accelerators come with some limitations. They require a local host to configure and operate them. In addition, the number of host CPUs and accelerators does not scale independently. Another problem is the unbalanced communication between distributed accelerators. New communication frameworks are developed to optimize the internode communication. In this paper, four communication models using the Intel Xeon Phi coprocessor technology are compared. The Intel Xeon Phi coprocessor is based on the Intel Many Integrated Cores technology. It is an attractive accelerator due to its embedded Linux operating system, up to 1 TFLOPS of performance on a single chip, and its x86 64 compatibility. DCFA-MPI, MVAPICH2-MIC, and HAM-Offload are compared against the communication architecture for network-attached accelerators (NAA). Each communication model optimizes a different layer of the MIC communication architecture. The NAA approach makes the accelerator device independent from a local host system. Furthermore, it enables the accelerator to source and sink network traffic. Workloads can be dynamically assigned during run-time in an N to M ratio between CPUs and accelerators. The latency, bandwidth, and performance of the MPI communication layer of a prototype implementation are evaluated.
[parallel computation, application program interfaces, supercomputing architecture, Intel Xeon Phi coprocessor technology, MVAPICH2-MIC, CPU, parallel processing, network-attached accelerator, Microwave integrated circuits, HAM-Offload, MIC communication architecture, Computer architecture, Bismuth, supercomputing system, Intel many integrated core technology, Hardware, Libraries, DCFA-MPI, MPI communication layer, multiprocessing systems, distributed accelerator, internode communication, distributed Intel Xeon Phi coprocessor, NAA, Linux, Linux operating system, x86 64 compatibility, Software, Coprocessors]
SIMD Code Translation in an Enhanced HQEMU
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
HQEMU is a multi-threaded and retargetable dynamic binary translator built on top of QEMU and LLVM. It combines the fast and reliable code translation in the TCG (Tiny Code Generator) of QEMU and the rich optimizations in LLVM to achieve high performance for both short running and long running applications. One weakness of HQEMU lies in the lack of efficient SIMD instruction translation. This work investigates on how to remedy that. Two approaches have been designed and tested. One simple approach is to modify the help function to emit LLVM vector IR, and a more complete approach is to add a newly introduced vector IR in the TCG phase. Although both approaches can exploit the SIMD instructions of the host machine, the second and more complete approach has superior runtime as well as compile time advantages.
[TCG phase, multi-threading, tiny code generator, multi-threaded dynamic binary translator, SIMD code translation, SIMD instruction translation, Switches, Generators, SIMD, program compilers, retargetable dynamic binary translator, LLVM vector IR, Optimization, Neon, Runtime, LLVM, Emulation, Benchmark testing, HQEMU, QEMU, Vector IR, TCG IR]
Versioning Architectures for Local and Global Memory
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Future supercomputer systems will face serious reliability challenges. Among failure scenarios, latent errors are some of the most serious and concerning. Preserving multiple versions of critical data is a promising approach to deal with such errors. We are developing the Global View Resilience (GVR) library, with multi-version global arrays as one of the key features. This paper presents three array versioning architectures: flat array, flat array with change tracking, and log-structured array. We use a synthetic workload that mimics the memory access patterns of radix sort, N-body simulation, and matrix multiplication, comparing the three array architectures in terms of runtime performance, memory requirements, and version restoration costs. The experiments show that the flat array with change tracking is the best architecture in terms of runtime performance, for versioning frequencies of 10-5 ops-1 or higher matching the second best architecture or beating it by up to 23 times, whereas the log-structured array is preferable for low memory usage, since it saves up to 98% of memory compared with a flat array.
[multiversion global array, memory requirement, GVR library, N-body simulation, runtime performance, Runtime, change tracking, Global View Resilience, Libraries, flat array, Kernel, distributed array, version restoration cost, supercomputer system, performance evaluation, Resilience, matrix multiplication, memory architecture, multi-versioning, memory access pattern, versioning frequency, Memory management, Arrays, benchmark testing, versioning architecture, global view resilience library, radix sort, log-based data structures]
Power Capping: What Works, What Does Not
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Peak power consumption is the first order design constraint of data centers. Though peak power consumption is rarely, if ever, observed, the entire data center facility must prepare for it, leading to inefficient usage of its resources. The most prominent way for addressing this issue is to limit the power consumption of the data center IT facility far below its theoretical peak value. Many approaches have been proposed to achieve that, based on the same small set of enforcement mechanisms, but there has been no corresponding work on systematically examining the advantages and disadvantages of each such mechanism. In the absence of such a study, it is unclear what is the optimal mechanism for a given computing environment, which can lead to unnecessarily poor performance if an inappropriate scheme is used. This paper fills this gap by comparing for the first time five widely used power capping mechanisms under the same hardware/software setting. We also explore possible alternative power capping mechanisms beyond what has been previously proposed and evaluate them under the same setup. We systematically analyze the strengths and weaknesses of each mechanism, in terms of energy efficiency, overhead, and predictable behavior. We show how these mechanisms can be combined in order to implement an optimal power capping mechanism which reduces the slowdown compared to the most widely used mechanism by up to 88%. Our results provide interesting insights regarding the different trade-offs of power capping techniques, which will be useful for designing and implementing highly efficient power capping in the future.
[Energy consumption, Compiler, Power demand, Instruction sets, overhead, data centers, Servers, computer centres, Power Optimization, Rapl, slowdown reduction, Power Capping, Runtime, power aware computing, power capping mechanisms, energy efficiency, Benchmark testing, energy conservation, Hardware, Dvfs]
Improving Application Concurrency on GPUs by Managing Implicit and Explicit Synchronizations
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Originally designed to be used as dedicated coprocessors, GPUs have progressively become part of shared computing environments, such as HPC servers and clusters. Commonly used GPU software stacks (e.g., CUDA and OpenCL), however, are designed for the dedicated use of GPUs by a single application, possibly leading to resource underutilization when multiple applications share the GPU resources. In recent years, several node-level runtime components have been proposed to target this problem and allow the efficient sharing of GPUs among concurrent applications. The concurrency enabled by these systems, however, is limited by synchronizations embedded in the applications or implicitly introduced by the GPU software stack. This work targets this problem. We first analyze the effect of explicit and implicit synchronizations on application concurrency and GPU utilization. We then design runtime mechanisms to bypass these synchronizations, along with a memory management scheme that can be integrated with these synchronization avoidance mechanisms to improve GPU utilization and system throughput. We integrate these mechanisms into a recently proposed GPU virtualization runtime named Sync-Free GPU (SF-GPU), thus removing unnecessary blockages caused by multitenancy, ensuring any two applications running on the same device experience limited to no interference, maximizing the level of concurrency supported. We also release our mechanisms in the form of a software API that can be used by programmers to improve the performance of their applications without modifying their code. Finally, we evaluate the impact of our proposed mechanisms on applications run in isolation and concurrently.
[GPU software stacks, application program interfaces, Graphics processing units, software API, virtualisation, GPU virtualization runtime, multitenancy, shared computing environments, Concurrent computing, storage management, Runtime, resource allocation, implicit synchronization, system throughput, Hardware, Kernel, concurrency (computers), Context, GPU resource utilization, concurrent applications, runtime mechanism design, runtime components, application concurrency, graphics processing units, synchronisation, virtual machines, explicit synchronization, Sync-Free GPU, memory management scheme, synchronization avoidance mechanisms]
Parallel Simulated Annealing with MRAnneal
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Simulated annealing algorithms, which repeatedly make small changes to candidate solutions to find approximately optimal ones, are a common method for approximating solutions to computationally expensive optimization problems. While using multiple machines to perform such computations in parallel is attractive as a means to reduce the running time, execution in a cluster environment requires substantial software infrastructure to cope with the challenges of a distributed system. In this paper, we introduce MRAnneal, a framework that simplifies the implementation of parallel simulated annealing algorithms. MRAnneal allows users to explicitly trade-off running time and the quality of approximate solutions by supplying only a small number of automatically tuned parameters. Our experimental results demonstrate that implementing applications using MRAnneal is straightforward and that such implementations yield approximate solutions quickly, even for applications without intuitive serial approximation heuristics.
[parallel algorithms, Annealing, Runtime, MRAnneal, simulated annealing, parallel simulated annealing algorithms, Estimation, Simulated annealing, Approximation algorithms, Software]
PSO-SVR: A Hybrid Short-term Traffic Flow Forecasting Method
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Accurate short-term flow forecasting is important for the real-time traffic control, but due to its complex nonlinear data pattern, getting a high precision is difficult. The support vector regression model (SVR) has been widely used to solve nonlinear regression and time series predicting problems. This paper presents a Hybrid PSO-SVR forecasting method to get a higher precision with less learning time; this method uses Particle Swarm Optimization (PSO) to search optimal SVR parameters. And to find a PSO that is more proper to SVR parameters searching, this paper proposes three kinds of strategies to handle the particles flow out the searching space, according to comparison, one of the strategies can make PSO get the optimal parameters more quickly, this paper calls the PSO using this strategy as fast PSO. Furthermore, to handle the precision's decline caused by the noises in the original data, this paper proposes a hybrid PSO-SVR method with historical momentum based on the similarity of historical short-term flow data. The forecasting results of extensive comparison experiments indicate that proposed model can get more accurate forecasting result than other state-of-the-art algorithms; and when the data containing noises, the method with historical momentum still deserves accurate forecasting.
[support vector machines, particle swarm optimisation, SVR parameters searching, Artificial neural networks, regression analysis, hybrid short-term traffic flow forecasting method, Predictive models, Traffic flow forecasting, traffic engineering computing, particle swarm optimization, short-term, Forecasting, PSO-SVR, Support vector machines, support vector regression, historical short-term flow data, support vector regression model, Real-time systems, Data models, Kernel]
A Power-Aware Symbiotic Scheduling Algorithm for Concurrent GPU Kernels
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
The past several years have witnessed significant performance improvements in High-Performance Computing (HPC), due to the incorporation of GPUs as co-processors. On one hand, GPU devices are growing significantly in terms of the available number of cores and the memory hierarchy; as a result, effective utilization of the available GPU resources while limiting the system power consumption has become an issue of rising importance. On the other hand, GPU vendors are providing additional supporting features to make this easier, such as enabling concurrent execution of multiple kernels, and providing on-board power sensors that can accessed through software. Amidst these new developments, we are faced with new opportunities for efficiently scheduling GPU computational kernels under performance and power constraints. In this paper, we propose a power-aware scheduling technique that carries out both performance and power optimizations for concurrent GPU kernels. We have observed that for GPU kernels that are deployed for concurrent execution, the order in which the programmer specifies their invocation can significantly alter the execution time and the power draw. We attribute this behavior to the relative synergy (or lack thereof) among kernels that are launched within close proximity of each other. Accordingly, we define performance metrics for computing the extent to which kernels are symbiotic, as well as power metrics for reducing the overall power consumption. Both metrics are estimated by modeling the kernels' complementary resource requirements and execution characteristics. We then propose a power-aware symbiotic scheduling algorithm to obtain a concurrent kernel launch schedule with improved performance and reduced power consumption. Experimental studies are conducted on the Cray XK7 supercomputer with an NVIDIA K20 GPU in each node. The results demonstrate the efficacy of the proposed algorithm-based approach, which can be readily adopted by programmers with minimal programming effort and risk.
[Measurement, symbiotic kernels, Instruction sets, Graphics processing units, memory hierarchy, kernel complementary resource requirement characteristics, Cray XK7 supercomputer, High-Performance Computing, coprocessors, GPU, processor scheduling, Concurrent Kernel Execution, power aware computing, resource allocation, GPU resources, on-board power sensors, power-aware scheduling technique, Kernel, high-performance computing, power-aware symbiotic scheduling algorithm, Symbiosis, NVIDIA K20 GPU, Power demand, GPU vendors, concurrent GPU kernels, concurrent execution, performance evaluation, Scheduling, graphics processing units, Algorithm, CUDA, GPU devices, concurrency control, system power consumption, Performance, GPU computational kernel scheduling, Power]
High-Performance Parallel Location-Aware Algorithms for Approximate String Matching on GPUs
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Approximate string matching has been widely used in many applications, including deoxyribonucleic acid sequence searching, spell checking, text mining, and spam filters. The method is designed to find all locations of strings that approximately match a pattern in accordance with the number of insertion, deletion, and substitution operations. Among the proposed algorithms, the bit-parallel algorithms are considered to be the best and highly efficient algorithms. However, the traditional bit-parallel algorithms lacks the ability of identifying the start and end positions of a matched pattern. Furthermore, acceleration of the bit-parallel algorithms has become a crucial issue for processing big data nowadays. In this paper, we propose two kinds of parallel location-aware algorithms called data-segmented parallelism and high-degree parallelism as means to accelerate approximate string matching using graphic processing units. Experimental results show that the high-degree parallelism on GPUs achieves significant improvement in system and kernel throughputs compared to CPU counterparts. Compared to state-of-the-art approaches, the proposed high-degree parallelism achieves 11 to 105 times improvement.
[Algorithm design and analysis, data-segmented parallelism, parallel algorithms, nondeterministic finite automaton, parallel algorithm, CPU, Registers, Business process re-engineering, Levenshtein distance, graphics processing units, GPU, graphic processing units, bit-parallel algorithm, mobile computing, parallel location-aware algorithms, approximate string matching, high-degree parallelism, Digital signal processing, Parallel processing, Approximation algorithms, string matching, Pattern matching]
GPU-Accelerated Nick Local Image Thresholding Algorithm
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Binarization plays an important role in document image processing, particularly in degraded document images. Among all local adaptive image thresholding algorithms, the Nick method has shown excellent binarization performance for degraded document images. However, local image thresholding algorithms, including the Nick method, are computationally intensive, requiring significant time to process input images. In this paper, we propose three CUDA GPU parallel implementations of the Nick local image thresholding algorithm for faster binarization of large images. Our experimental results show that the GPU-accelerated implementations of the Nick method can achieve up to 150x performance speedup on a GeForce GTX 480 compared to its optimized sequential implementation.
[image binarization, input image processing, Instruction sets, Image processing, parallel architectures, Graphics processing units, Programming, document image processing, GPU-accelerated Nick local adaptive image thresholding algorithm, graphics processing units, parallel programming, degraded document images, CUDA GPU Programming, GPU acceleration, CUDA GPU parallel implementations, image thresholding, Memory management, Loading, image segmentation, GeForce GTX 480, Kernel]
CloudFreq: Elastic Energy-Efficient Bag-of-Tasks Scheduling in DVFS-Enabled Clouds
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Energy consumption imposes a significant cost for data centers in providing cloud services. Many studies explore the opportunities to save power by energy-efficient task scheduling based on the technique of dynamic voltage and frequency scaling (DVFS). However, most of them assume that energy budgets and/or deadline constraints are known in advance. But these information can hardly be acquired in general computing environments, such as cloud computing, and job rejections caused by restricted constraints are intolerable to guarantee the service-level agreement (SLA). Moreover, previous works prefer to provide &#x201C;black-box&#x201D; algorithms with little consideration on adjustability, and cannot satisfy runtime requirements in performance and energy-saving. This paper proposes an elastic energy-efficient algorithm called CloudFreq for bag-of-tasks scheduling in DVFS-enabled clouds. CloudFreq enables a model of elastic, adjustable energy-efficient scheduling without any prior knowledge of constraints, and then eliminates job rejections accordingly. CloudFreq also provides an entry for operators to scale system performance at runtime. Experimental results demonstrate that the proposed algorithm can effectively perform energy-efficient scheduling without constraints, and has the capability of making an appropriate tradeoff to improve the weighted balance between schedule length and energy-saving.
[DVFS-enabled clouds, Cloud computing, Energy consumption, Time-frequency analysis, service-level agreement, data centers, bag-of-tasks, SLA, dynamic voltage and frequency scaling technique, contracts, elastic energy-efficient bag-of-tasks scheduling, Program processors, power aware computing, Scheduling algorithms, scheduling, cloud computing, energy consumption, deadline constraints, CloudFreq, black-box algorithms, Scheduling, energy-saving, energy-efficient, energy budgets, dynamic voltage and frequency scaling, elastic]
Inline Data Deduplication for SSD-Based Distributed Storage
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Data deduplication is used to overcome two issues on Solid State Drives (SSDs). One is price per GB of storage space, and the other is the write limit or disk endurance. By eliminating duplicate data, the deduplication system improves storage efficiency and protects SSD from unnecessary writes. CAFTL is a known solution for deduplication on SSD. We propose a system architecture for inline deduplication based on existing protocol of The Hadoop Distributed File System (HDFS), aiming at addressing performance challenges for primary storage. However, simply applying CAFTL to SSDs in a cluster does not work well. Two routing algorithms are presented and evaluated using selective real-life data sets. Compared to prior work, one routing algorithm (MMHR) may improve the deduplication ratio by 8% at minimal costs while the other (FFFR) can achieve about 30% higher deduplication ratio with tradeoff on chunk level fragmentation. A new research problem of chunk assignment into more than one node for deduplication is also formulated for more studies in this area.
[storage efficiency, Cloud computing, cluster, inline data deduplication system, distributed storage, Metadata, deduplication, file system, parallel processing, MMHR, chunk level fragmentation, routing algorithms, storage management, network operating systems, Distributed databases, Clustering algorithms, distributed databases, SSD-based distributed storage, solid state drives, chunk assignment, duplicate data, SSD, Systems architecture, HDFS, Routing, Indexes, selective real-life data sets, FFFR, CAFTL, routing algorithm, system architecture, deduplication ratio, disk endurance, Hadoop distributed file system]
FRAME: Fair Resource Allocation in Multi-process Environments
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
As the technology trend moves toward manufacturing many-core systems with hundreds of processing cores, the problem of efficiently managing multiple parallel jobs on such massively parallel systems becomes increasingly important. With traditional time-sharing each process assumes it is the only running process. This assumption can easily lead to system oversubscription and thereby, losing overall performance due to frequent context switches. Space-sharing techniques allocate a certain number of hardware cores to each process and by that malleable processes can set their parallelism level to their allocated number of cores, hence avoiding oversubscription. However, finding the optimal spatial allocation is not a trivial task. In this paper we propose FRAME, a resource allocation technique to maximize a system's overall utility and fairness, running multiple malleable processes with CPU-bound workloads. First, we formalize the resource allocation problem as an NP-hard problem. Then, we use approximation techniques and convex optimization theory to find the optimal solution to the formulated problem, in pseudo-polynomial time. Our evaluation results show that our method is very fast and efficient in finding the optimal solution to the resource allocation problem. Also, the results suggests that the found solution increases the system's overall utility by 48%, in average, with regard to the best alternative allocation policy.
[resource management, space-sharing technique, Instruction sets, Throughput, optimal spatial allocation, malleable processes, FRAME, parallel programming, parallel jobs, hardware core allocation, resource allocation, convex optimization theory, Parallel processing, Hardware, Convex functions, oversubscription, multiprocessing systems, Multicore processing, CPU-bound workloads, resource allocation technique, convex programming, system oversubscription, fair resource allocation in multiprocess environments, parallel systems, NP-hard problem, Parallel programming, many-core systems, convex optimization, pseudopolynomial time, Resource management, time-sharing technique, space-sharing]
sRSA: High Speed RSA on the Intel MIC Architecture
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
RSA cryptography provides key functions for signing/verifying digital signatures and encrypting/decrypting shared secrets, and is broadly deployed to ensure secure endto-end communications nowadays. However, the adoption of RSA-enabled applications is fairly limited mainly due to the large computation overheads, in particular, of cryptographic operations with the RSA private key. In this paper, we design and implement sRSA, a high speed RSA on the new Intel Many Integrated Core (MIC) architecture. We introduce several optimization strategies to sRSA for the MIC architecture without jeopardizing the security level. For example, 1) sRSA explicitly and efficiently vectorizes the underlying cryptographic primitives with the 512-bit vector registers; 2) It also integrates the advanced algorithmic features of fast RSA variants and many other fine-grained implementations; 3) It is thoroughly designed to resist applicable RSA system attacks, i.e., the factoring attacks and the CRT exponent attack. In the end, we evaluate the performance of sRSA and compare it with the industry-standard OpenSSL. The benchmark result shows that sRSA retains a comparable latency with, but demonstrates a much higher throughput than OpenSSL on both CPU and MIC based Phi coprocessor.
[Algorithm design and analysis, high speed RSA, RSA, multiprocessing systems, High Performance Computing, cryptographic primitives, Registers, coprocessors, SIMD, Intel MIC architecture, many integrated core architecture, coprocessor, Vectorization, OpenSSL, Intel MIC, Microwave integrated circuits, public key cryptography, vector registers, Intel Phi, Public key, Computer architecture, sRSA, RSA system attacks, Coprocessors]
Cross-Matching Large Astronomical Catalogs on Heterogeneous Clusters
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Cross-matching astronomical catalogs is a central operation in astronomical data integration and analysis. As current commodity clusters typically consist of heterogeneous processors including both multi-core CPUs and GPUs, we study how to efficiently cross-match large astronomical catalogs on such clusters. Specifically, we develop a three-phase common algorithm for parallel cross-match, and optimize it for a single GPU, multiple GPUs on a node, and a heterogeneous cluster of multiple nodes, respectively. Furthermore, we study the performance impact of data chunk size and that of inter-node communication mechanisms in the cluster. Our results show that, with suitable design choices and optimizations, cross-matching billion-record catalogs was completed under 10 minutes on a seven-node CPU-GPU cluster.
[Algorithm design and analysis, workstation clusters, heterogeneous cluster, Astronomical catalogs, astronomical catalogs cross-matching, astronomical data integration, three-phase common algorithm, Graphics processing units, Search problems, Servers, parallel processing, GPU, Heterogeneous cluster, astronomy computing, Clustering algorithms, Cross-match, inter-node communication mechanisms, data analysis, microprocessor chips, seven-node CPU-GPU cluster, graphics processing units, astronomical data analysis, parallel cross-match, data integration, Indexing]
Realizing Extremely Large-Scale Stencil Applications on GPU Supercomputers
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
The problem of deepening memory hierarchy towards exascale is becoming serious for applications such as those based on stencil kernels, as it is difficult to satisfy both high memory bandwidth ad capacity requirements simultaneously. This is evident even today, where problem sizes of stencil-based applications on GPU supercomputers are limited by aggregated capacity of GPU device memory. Locality improvement techniques such as temporal blocking is known to preserve performance, but integrating the technique into existing stencil applications results in substantially higher programming cost, especially for complex applications and as a result are not typically utilized. We alleviate this problem with a run-time GPU-MPI process virtualization library we call HHRT that automates data movement across the memory hierarchy, and a systematic methodology to convert and optimize the code to accommodate temporal blocking. The proposed methodology has shown to significantly eases the adaptation of real applications, such as the whole-city airflow simulator embodying more than 12,000 lines of code; with careful tuning, we successfully maintain up to 85% performance even with problems whose footprint is four time larger than GPU device memory capacity, and scale to hundreds of GPUs on the TSUBAME2.5 supercomputer.
[Solid modeling, message passing, application program interfaces, HHRT, Atmospheric modeling, Computational modeling, Conferences, Graphics processing units, run-time GPU-MPI process virtualization library, GPU supercomputers, temporal blocking, stencil kernels, graphics processing units, parallel machines, extremely large-scale stencil, locality improvement techniques, whole-city airflow simulator, TSUBAME2.5 supercomputer, GPU device memory, GPU device memory capacity]
Video Replication over a New Architecture DASH-DMS
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Nowadays, the video traffic represents more than 65% of global traffic and in the Internet forecasts, it will reach 79% in 2018 in both wired and wireless environments. Hence, major content providers such as Netflix, Youtube, Hulu, and Vudu are leveraging HTTP-based multimedia transmission with adaptative streaming with different solution in order to guarantee a quality of service (QoS). Moreover, in 2012, a new standard called Dynamic Adaptive Streaming over HTTP (DASH) which enables adaptation of the media bitrate to varying throughput conditions by offering multiple representations of the same content is proposed. In this paper, we present a fairness architecture, called DASH-DMS, in combination with the new Dynamic Adaptive Streaming over HTTP (DASH) standard, in order to guarantee a certain QoS under changing conditions in the available bandwidth. DASH-DMS is a hybrid architecture which combines both 2-tiers and 3-tiers architectures. Moreover, we present a replication algorithm to enable load balancing video servers and improve the global QoS. Furthermore, in order to improve the availability of our system, we use a fault tolerance policy. Simulations conducted along this paper show that our proposition significantly outperforms existing and state-of-the-art approaches.
[Quality of service, hypermedia, HTTP-based multimedia transmission, adaptative streaming, wired environments, load balancing video servers, Servers, Voltage control, video servers, Fault tolerance, resource allocation, QoS, Bit rate, Bandwidth, video streaming, Load balancing, DASH-DMS, video traffic, video replication, media bitrate, Streaming video, global Internet traffic, quality of service, 3-tiers architectures, transport protocols, new DASH-DMS architecture, wireless environments, 2-tiers architectures, Streaming media, dynamic adaptive streaming over HTTP, Replication, Internet, telecommunication traffic]
Optimizing Complex Spatially-Variant Coefficient Stencils for Seismic Modeling on GPU
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
The Explicit Time Evolution (ETE) method is an innovative Finite-Difference (FD) type method to simulate the wave propagation in acoustic media with higher spatial and temporal accuracy. However, different from FD, it is difficult to achieve an efficient GPU design because of the poor memory access patterns caused by the off-axis points and spatially-variant coefficients. In this paper, we present a set of new optimization strategies for ETE stencils according to the memory hierarchy of NVIDIA GPU. To handle the problem caused by the complexity of the stencil shapes, we design a one-to-multi updating scheme for shared memory usage. To alleviate the performance damage resulted from the poor memory access pattern of reading spatially-variant coefficients, we propose a stencil decomposition method to reduce un-coalesced global memory access. Based on the state-of-the-art GPU architecture, combining with existing spatial and temporal stencil blocking schemes, we manage to achieve 9.6x and 9.9x speedups compared with a well-tuned 12-core CPUs version for 37-point and 73-point ETE stencils, respectively. Compared with a well-tuned MIC version, the best speedups for the 2 type stencils are 3.7x and 4.7x. Our designs leads to an ETE method that is 31.2x faster than conventional CPU-FD method and make it a practical seismic imaging technology.
[parallel architectures, memory access patterns, CPU-FD method, spatial stencil blocking schemes, Graphics processing units, seismology, RTM, complex spatially-variant coefficient stencil optimisation strategy, GPU, Optimization, acoustic wave propagation, Microwave integrated circuits, Three-dimensional displays, seismic imaging technology, off-axis points, Computer architecture, FWI, one-to-multiupdating scheme, Mathematical model, explicit time evolution method, multiprocessing systems, Computational modeling, seismic modeling, GPU design, spatially-variant coefficients, Explicit Time Evolution, NVIDIA GPU memory hierarchy, MIC, geophysics computing, spatially-variant coefficient stencil, microprocessor chips, stencil decomposition method, graphics processing units, seismic imaging, wave propagation, temporal stencil blocking schemes, ETE method, finite-difference type method, FD type method, acoustic media, forward modeling, un-coalesced global memory access reduction]
Self-Tuned Software-Managed Energy Reduction in InfiniBand Links
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
One of the biggest challenges in high-performance computing is to reduce the power and energy consumption. Research in energy efficiency has focused mainly on energy consumption at the node level. Less attention has been given to the interconnect, which is becoming a significant source of energy-inefficiency. Although supercomputers undoubtedly require a high-performance interconnect, previous work has shown that network links have low average utilization. It is therefore possible to save energy using low-power modes, but link wake-up latencies must not lead to a loss in performance. This paper proposes the Self-tuned Pattern Prediction System (SPPS), a self-tuned algorithm for energy proportionality, which reduces interconnect energy consumption without needing any application-specific configuration parameters. The algorithm uses prediction to discover repetitive patterns in the application's communication, and it is implemented inside the MPI library, so that existing MPI programs do not need to be modified. We build on previous work, which showed how the application structure can be successfully exploited to predict the communication idle intervals. The previous work, however, required the manual adjustment of a critical idle interval length, whose value depends on the application and has a major effect on energy savings. The new technique automatically discovers the optimal value of this parameter, resulting in a self-tuned algorithm that obtains large interconnect energy savings at little performance cost. We study the effectiveness of our approach using ten real applications and benchmarks. Our simulations show average energy savings in the network links of up to 21%. Moreover, the link wake-up latencies and additional computation times have a negligible effect on performance, with an average penalty less than 1%.
[Energy consumption, application program interfaces, interconnect energy consumption, Switches, Manuals, energy savings, supercomputers, parallel processing, power consumption, self-tuned algorithm, power aware computing, interval length, Bandwidth, energy efficiency, Prediction algorithms, Libraries, energy consumption, high-performance computing, wake-up latencies, message passing, low-power modes, Supercomputers, MPI library, self-tuned pattern prediction system, MPI programs, energy proportionality, self-tuned software-managed energy reduction, InfiniBand Links, energy conservation, communication idle intervals]
VH-DSI: Speeding up Data Visualization via a Heterogeneous Distributed Storage Infrastructure
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Visualizing and analyzing large-scale datasets are both critical and challenging, as they require substantial resources for data processing and storage. While the speed of supercomputers continues to set higher standard, the I/O systems have not kept in pace, resulting in a significant performance bottleneck. To alleviate the I/O bottleneck for scientific visualization applications, we propose a Visualization via a Heterogeneous Distributed Storage Infrastructure (VH-DSI) solution to improve I/O speed and accelerate overall visualization performance. VH-DSI replaces the traditional parallel file system with a distributed file system to support visualization applications. A new scheduling algorithm HeterSche is proposed in VH-DSI to assign computing tasks to data nodes with the consideration of cluster heterogeneity and data locality. VH-DSI also includes a design to support POSIX-IO for distributed file system. The performance evaluation has shown that the proposed VH-DSI solution can achieve significant performance improvement for visualization applications. Compared to the traditional visualization, the VH-DSI solution reduces the response time by at least 5 times. The HeterSche scheduling algorithm is capable to speed up visualization compared to other scheduling algorithms especially for large scale datasets.
[cluster heterogeneity, Pipelines, distributed file system, I/O, scientific visualization applications, large-scale dataset visualization, HeterSche scheduling algorithm, data storage, I/O speed improvement, response time reduction, File systems, Scheduling algorithms, large-scale dataset analysis, data locality, Distributed databases, data visualisation, Computer architecture, distributed databases, scheduling, data visualization, heterogeneous distributed storage infrastructure, Data Intensive Scientific Computing, Heterogeneous Cluster, data processing, Parallel Computing, performance evaluation, POSIX-IO, computing task assignment, VH-DSI, Data visualization, scientific information systems, data nodes, Distributed Computing, Scientific Visualization, visualization performance acceleration]
Scaling Monte Carlo Tree Search on Intel Xeon Phi
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Many algorithms have been parallelized successfully on the Intel Xeon Phi coprocessor, especially those with regular, balanced, and predictable data access patterns and instruction flows. Irregular and unbalanced algorithms are harder to parallelize efficiently. They are, for instance, present in artificial intelligence search algorithms such as Monte Carlo Tree Search (MCTS). In this paper we study the scaling behavior of MCTS, on a highly optimized real-world application, on real hardware. The Intel Xeon Phi allows shared memory scaling studies up to 61 cores and 244 hardware threads. We compare work-stealing (Cilk Plus and TBB) and work-sharing (FIFO scheduling) approaches. Interestingly, we find that a straightforward thread pool with a work-sharing FIFO queue shows the best performance. A crucial element for this high performance is the controlling of the grain size, an approach that we call Grain Size Controlled Parallel MCTS. Our subsequent comparing with the Xeon CPUs shows an even more comprehensible distinction in performance between different threading libraries. We achieve, to the best of our knowledge, the fastest implementation of a parallel MCTS on the 61 core (&amp;equals; 244 hardware threads) Intel Xeon Phi using a real application (47 times faster than a sequential run).
[Instruction sets, Xeon CPU, Monte Carlo methods, Intel Xeon Phi, Monte Carlo Tree Search, Many-core, Computer architecture, Parallel processing, shared memory systems, Libraries, Hardware, Work-sharing, Grain size, parallel algorithms, Monte Carlo tree search, work-sharing FIFO queue, grain size controlled parallel MCTS, Scheduling, tree searching, shared memory scaling, work-stealing, Work-stealing, Games, threading libraries, Scaling]
Comparison of Single Source Shortest Path Algorithms on Two Recent Asynchronous Many-task Runtime Systems
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
With the advent of the exascale era, new runtimes and algorithm design techniques need to be explored. In this paper, we investigate performance of three different single-source shortest path algorithms in two relatively recent asynchronous many-task runtime systems AM++ and HPX-5. We identify the underlying set of differential features for these runtimes, and we compare and contrast the performance of &#x0394;-stepping algorithm, Distributed Control based algorithm, K-level Asynchronous algorithm in AM++ and in HPX-5, for which we also include chaotic implementation. We observe that specific runtime characteristics or lack thereoff and different graph inputs can impact the feasibility of an algorithmic approach.
[Algorithm design and analysis, Computational modeling, HPX-5, &#x0394;-stepping algorithm, graph traversal, distributed control based algorithm, Synchronization, parallel processing, single-source shortest path algorithms, K-level asynchronous algorithm, Runtime, AM++, High Performance ParalleX 5, Decentralized control, distributed algorithms, asynchronous many-task runtime systems, Approximation algorithms, Libraries, SSSP, active message framework]
Algorithmic Acceleration of Parallel ALS for Collaborative Filtering: Speeding up Distributed Big Data Recommendation in Spark
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Collaborative filtering algorithms are important building blocks in many practical recommendation systems. For example, many large-scale data processing environments include collaborative filtering models for which the Alternating Least Squares (ALS) algorithm is used to compute latent factor matrix decompositions. In this paper, we propose an approach to accelerate the convergence of parallel ALS-based optimization methods for collaborative filtering using a nonlinear conjugate gradient (NCG) wrapper around the ALS iterations. We also provide a parallel implementation of the accelerated ALS-NCG algorithm in the Apache Spark distributed data processing environment, and an efficient line search technique as part of the ALS-NCG implementation that requires only one pass over the data on distributed datasets. In serial numerical experiments on a linux workstation and parallel numerical experiments on a 16 node cluster with 256 computing cores, we demonstrate that the combined ALS-NCG method requires many fewer iterations and less time than standalone ALS to reach movie rankings with high accuracy on the MovieLens 20M dataset. In parallel, ALS-NCG can achieve an acceleration factor of 4 or greater in clock time when an accurate solution is desired; furthermore, the acceleration factor increases as greater numerical precision is required in the solution. Furthermore, the NCG acceleration mechanism is efficient in parallel and scales linearly with problem size on synthetic datasets with up to nearly 1 billion ratings. The acceleration mechanism is general and may also be applicable to other optimization methods for collaborative filtering.
[Algorithm design and analysis, conjugate gradient methods, collaborative filtering, line search technique, search engines, NCG wrapper, scalable methods, Linux workstation, Recommendation systems, matrix decomposition, MovieLens 20M dataset, Optimization, Convergence, optimisation, acceleration factor, latent factor matrix decompositions, Motion pictures, Apache Spark distributed data processing environment, collaborative filtering algorithms, clock time, synthetic datasets, parallel optimization algorithms, parallel algorithms, matrix factorization, least squares approximations, alternating least squares algorithm, parallel ALS-based optimization method, NCG acceleration mechanism, accelerated ALS-NCG algorithm, Big Data, Sparks, large-scale data processing environments, Apache Spark, ALS iterations, recommender systems, algorithmic acceleration, nonlinear conjugate gradient wrapper, parallel numerical experiments, distributed Big Data recommendation, Collaboration, Acceleration]
Hardware-Centric Analysis of Network Performance for MPI Applications
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
As the scale of high-performance computing systems increases, optimizing inter-process communication becomes more challenging while being critical for ensuring good performance. However, the hardware layer abstraction provided by MPI makes it difficult to study application communication performance over the network hardware, especially for collective operations. We present a new approach to network performance analysis based on exposing low-level communication metrics in a flexible manner and conducting hardware-centric analysis of these metrics. We show how low-level network metrics can be revealed using Open MPI's Peruse utility, without interfacing with the hardware layer. A lightweight profiler, ibprof, was developed to aggregate these metrics from message passing events at a cost of &lt;;1% runtime overhead for communication in NPB kernel and application benchmarks. We also developed a flexible visualization module for the Boxfish analysis tool to analyze our communication profile over the physical topology of the network. Using case studies, we demonstrate how our approach can identify communication anomalies in network applications and guide performance optimization strategies.
[Measurement, application program interfaces, Ports (Computers), parallel processing, MPI applications, Network topology, network performance, Boxfish, application communication performance, high-performance computing systems, Boxfish analysis tool, Libraries, Hardware, Performance analysis, message passing, Peruse, message passing interface, Open MPI Peruse utility, program diagnostics, Topology, low-level communication metrics, hardware-centric analysis, ibprof lightweight profiler, profiling, Data visualization, Open MPI]
Building Blocks for a System-Wide Power and Thermal Management Framework
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Next generation Exascale systems face the difficult challenge of managing the power and thermal constraints that come from packaging more transistors into a smaller space while adding more processors into a single system. To combat this, HPC center operators are looking for methodologies to save operational energy. Energy consumption in an HPC center is governed by the complex interactions between a number of different components. Without a coordinated and system-wide perspective on reducing energy consumption, isolated actions taken on one component with the intent to lower energy consumption can actually have the opposite effect on another component, thereby canceling out the net effect. For example, increasing the setpoint (or ambient temperature) to save cooling energy can lead to increased compute-node fan power and increased chip leakage power. This paper presents the building blocks required to develop and implement a system-wide framework that can take a coordinated approach to enact thermal and power management decisions at compute-node (e.g., CPU speed throttling) and infrastructure levels (e.g., selecting optimal setpoint). These building blocks consist of a suite of models that inform the thermal and power footprint of different computations, and present relationships between computational properties and datacenter operating conditions.
[HPC center operators, multiprocessing systems, Random access memory, chip leakage power, parallel processing, Temperature measurement, Semiconductor device measurement, thermal management framework, Program processors, Power measurement, power aware computing, thermal constraints, cooling energy saving, compute-node fan power, cooling, datacenter operating conditions, high performance computing, Mathematical model, energy consumption reduction, energy consumption, system-wide power management framework, Clocks, Next generation Exascale systems]
DAG Scheduling for Heterogeneous Systems Using Biogeography-Based Optimization
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Efficient scheduling algorithm is critical for DAG-based applications to obtain high-performance in heterogeneous computing systems. In comparison with heuristic-based algorithms, meta-heuristic based scheduling algorithms can produce better results by searching in a guided manner. Biogeography-based optimization (BBO) is a recently proposed optimization technique which has shown less parameters, faster convergency, and superior performance than existing meta-heuristics. In this article, we introduce this novel optimization technique into the field of DAG scheduling. To reduce scheduling overhead, the proposed algorithm only encodes task mapping while using a heuristic strategy to determine task ordering. Moreover, it uses heuristic-based algorithms as baseline algorithms to obtain better results. We evaluate the BBO-based scheduling algorithm using three real world DAG-based applications under various parameter settings. The results show that the BBO-based scheduling algorithm outperforms the state-of-the-art meta-heuristic based algorithms.
[parallel algorithms, task mapping, Computational modeling, biogeography-based optimization technique, heterogeneous computing systems, DAG scheduling algorithm, Biogeography, directed acyclic graph, Scheduling, BBO-based scheduling algorithm, Optimization, optimisation, meta-heuristic based scheduling algorithms, task ordering, Scheduling algorithms, heuristic-based algorithms, directed graphs, heterogeneous system, scheduling, task scheduling, biogeography-based optimization, DAG application, Periodic structures]
SHAFT: Supporting Transactions with Serializability and Fault-Tolerance in Highly-Available Datastores
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Guaranteeing transaction semantics in a highly available and fault tolerant manner is desirable to application developers. Besides, it is a very valuable feature for database-backed applications. In this paper, we propose SHAFT to support transactions with serializability in highly-available datastores, which partition, distribute and replicate data across datacenters. SHAFT is a transactional replication protocol guaranteeing Serializability, High Availability and Fault Tolerance simultaneously for transactions. Laying its basis on the Paxos algorithm, SHAFT guarantees serializability by a two-phase locking procedure in a fault-tolerant manner. Different from other transactional replication protocols like MDCC, SHAFT allows a client to actively abort a transaction. SHAFT also allows flexible data partition, replication and distribution, a proper combination of which can reduce costs and improve performance. SHAFT performs well even under failures. Our experiments show that SHAFT outperforms MDCC, which outperforms other synchronous transactional replication protocols, e.g. Megastore.
[Protocols, transaction semantics, transactional replication protocol, SHAFT, isolation, Partitioning algorithms, datastores, Proposals, consistency, fault-tolerant property, Shafts, Fault tolerance, storage management, high availability, two-phase locking procedure, Fault tolerant systems, Distributed databases, concurrency control, serializability, fault tolerant computing, data handling, protocols, Paxos algorithm, transaction]
Efficient Algorithms for Scheduling XML Data in a Mobile Wireless Broadcast Environment
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
This paper tackles the key scheduling problem of reducing the overall wait time of mobile clients in wireless data broadcast systems. It is observed that in periodic broadcast, new mobile clients may join in and existing mobile clients may leave anytime; in on-demand broadcast, high uplink communication cost may occur as all clients have to submit their queries every time. These are likely to degrade existing broadcasting approaches. In this work, we study the scheduling problem of XML data broadcast in a hybrid mode, where the system supports both periodic broadcast and on-demand broadcast services at the same. By taking the structural similarity between XML documents into account, only a small portion of mobile clients would be involved in the scheduling process and all mobile clients can be served more effectively. In this way, communication cost at the client side can be reduced greatly. A formal theoretical analysis of the proposed technique is presented. Based on the analysis, a novel clustering-based scheduling algorithm is developed. Moreover, we utilize an aging method to predict the distribution of incoming queries based on small samples of queries from mobile clients. Finally, we evaluate the approach through a set of experiments and the results show that it can significantly improve access efficiency for mobile clients.
[Schedules, mobile radio, aging method, XML data scheduling process, Mobile communication, wireless data broadcast systems, high uplink communication cost, structural similarity, Servers, Wireless communication, query processing, mobile computing, Scheduling algorithms, mobile clients, pattern clustering, XML, on-demand broadcast services, clustering-based scheduling algorithm, scheduling, Mathematical model, periodic broadcast]
LSRB-CSR: A Low Overhead Storage Format for SpMV on the GPU Systems
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Sparse matrix vector multiplication (SpMV) is a basic building block of many scientific applications. Several GPU accelerated SpMV algorithms for the CSR format suffer from workload unbalance for irregular matrices. In this paper, we propose a new auxiliary array assisted CSR format called local segmented reduction based CSR (LSRB-CSR), which enables synchronization free preprocessing and efficient SpMV algorithm with the light weight auxiliary arrays. It is efficient for both regular matrices and irregular matrices with tiny preprocessing overhead. We compare our LSRB-CSR based SpMV algorithm with the CSR-based SpMV from cuSPARSE, the SpMV algorithm based on segmented reduction adopted by CUDPP library, and the CSR5-based SpMV algorithm for both regular and irregular sparse matrices. Compared to cuSparse, our LSRB-CSR based SpMV algorithm could improve the performance by 26% on regular matrices and up to 4750% on irregular matrices. Compared to CUDPP, our LSRB-CSR based SpMV algorithm could improve the average SpMV performance by 210% on regular matrices and 250% on irregular matrices. Our LSRB-CSR based SpMV algorithm has comparable performance as the CSR5 based SpMV algorithm for regular matrices, and achieves better performance over the CSR5 based SpMV algorithm for irregular matrices. Experimental results show that the conversion overhead from the CSR to the LSRB-CSR is only 1/10 of the overhead from the CSR to the CSR5 on average.
[SpMV, local segmented reduction based CSR, Instruction sets, CUDPP library, irregular sparse matrices, Graphics processing units, Sparse Matrices, Sparse matrices, CSR5, parallel processing, GPU, storage management, regular sparse matrices, Libraries, synchronization free preprocessing, sparse matrix vector multiplication, GPU systems, CSR, cuSPARSE, SpMV algorithm, Synchronization, graphics processing units, LSRB-CSR, matrix multiplication, cuSparse, auxiliary array assisted CSR format, low overhead storage format, Arrays, Acceleration]
Contention-Aware Scheduling for Asymmetric Multicore Processors
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Asymmetric multicore processors (AMPs) have been proposed as an energy-efficient alternative to symmetric mul-ticore processors (SMPs). However, AMPs derive their performance from core specialization, which requires co-running applications to be scheduled to run on their most appropriate core types. Despite extensive research on AMP scheduling, developing an effective scheduling algorithm remains challenging. Contention for shared resources is a key performance-limiting factor, which often renders existing contention-free scheduling algorithms ineffective. We introduce a contention-aware scheduling algorithm for ARM's big.LITTLE, a commercial AMP platform. Our algorithm comprises an offline stage and an online stage. The offline stage builds a performance interference model for an application by training it with a set of co-running applications. Guided by this model, the online stage schedules a workload by assigning its applications to their most appropriate core types in order to minimize the performance degradation caused by contention for shared resources. Our model can accurately predict the performance degradation of an application when co-running with other applications with an average prediction error of 9.60%. Compared with the default scheduler provided for ARM's big.LITTLE and the speedup-factor-driven scheduler, our contention-aware scheduler can improve overall system performance by up to 28.32% and 28.51%, respectively.
[offline stage, multiprocessing systems, Multicore processing, online stage, big.LITTLE, Interference, Scheduling, asymmetric multicore processors, speedup-factor-driven scheduler, AMP scheduling, Degradation, regression model, Program processors, Scheduling algorithms, performance interference, contention-aware scheduling algorithm, scheduling, contention-aware scheduling, commercial AMP platform, ARM, Asymmetric Multi-core Processor, performance interference model]
Interactive Consistency in Practical, Mostly-Asynchronous Systems
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Interactive consistency is the problem in which n nodes, where up to t may be byzantine, each with its own private value, run an algorithm that allows all non-faulty nodes to infer the values of each other node. This problem is relevant to critical applications that rely on the combination of the opinions of multiple peers to provide a service. Examples include monitoring a content source to prevent equivocation or to track variability in the content provided, and resolving divergent state amongst the nodes of a distributed system. Previous works assume a fully synchronous system, where one can make strong assumptions such as negligible message delivery delays and/or detection of absent messages. However, practical, real-world systems are mostly asynchronous, i.e., they exhibit only some periods of synchrony during which message delivery is timely, thus requiring a different approach. In this paper, we present a thorough study on practical interactive consistency. We leverage the vast prior work on broadcast and byzantine consensus algorithms to design, implement and evaluate a set of algorithms, with varying timing assumptions and message complexity, that can be used to achieve interactive consistency in real-world distributed systems. We provide a complete, open-source implementation of each proposed interactive consistency algorithm by building a multi-layered stack of protocols that include several broadcast protocols, as well as a binary and a multi-valued consensus protocol. Most of these protocols have never been implemented and evaluated in a real system before. We analyze the performance of our suite of algorithms experimentally by engaging in both single instance and multiple parallel instances of each alternative.
[Algorithm design and analysis, Protocols, Asynchronous, public domain software, distributed processing, Complexity theory, communication complexity, Integrated circuits, byzantine consensus algorithms, message complexity, Interactive consistency, mostly-asynchronous systems, multilayered protocol stack, broadcast protocols, broadcast consensus algorithms, Byzantine, multivalued consensus protocol, binary consensus protocol, practical interactive consistency algorithm, Consensus, Agreement, real-world distributed systems, Peer-to-peer computing, Delays, multiple parallel instances, Reliability, message delivery delays]
Fusion: Privacy-Preserving Distributed Protocol for High-Dimensional Data Mashup
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
In the last decade, several approaches concerning private data release for data mining have been proposed. Data mashup, on the other hand, has recently emerged as a mechanism for integrating data from several data providers. Fusing both techniques to generate mashup data in a distributed environment while providing privacy and utility guarantees on the output involves several challenges. That is, how to ensure that no unnecessary information is leaked to the other parties during the mashup process, how to ensure the mashup data is protected against certain privacy threats, and how to handle the high-dimensional nature of the mashup data while guaranteeing high data utility. In this paper, we present Fusion, a privacy-preserving multi-party protocol for data mashup with guaranteed LKC-privacy for the purpose of data mining. Experiments on real-life data demonstrate that the anonymous mashup data provide better data utility, the approach can handle high dimensional data, and it is scalable with respect to the data size.
[Data privacy, Protocols, cryptographic protocols, Mashups, data mining, mashup, privacy, distributed environment, mashup process, privacy threats, Couplings, anonymization, mashup data protection, privacy-preserving distributed protocol, Distributed databases, data protection, Data models, guaranteed LKC-privacy, high-dimensional data mashup, data integration, privacy-preserving multiparty protocol]
A Habit-Based SWRL Generation and Reasoning Approach in Smart Home
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
In this paper, we propose a habit-based SWRL generation and reasoning approach in smart home. Definition and recognition of habits of daily living can provide humanized smart home for assisted living application, especially for people with memory deficits. This paper presents Recognizing Habit of Daily Living(RHDL) by discovering and monitoring smart home context information. The habit and habit association of using electrical appliances are defined explicitly for the first time. The generation rules between habit/complex habit and SWRL are designed, and the reasoning is based on the Semantic Web Rule Language(SWRL). The ontology model for the RHDL is designed and the prototype system of RHDL is implemented using protege and Jess tools.
[Ontology, Smart homes, Smart home, Ontologies, protege tools, smart home, Cognition, domestic appliances, Temperature sensors, RHDL, ontology model, habit-based SWRL generation-reasoning approach, Monitoring, Context, semantic Web, OWL, smart home context information discovery, recognizing habit of daily living, home automation, Habit, semantic Web rule language, smart home context information monitoring, Jess tools, ontologies (artificial intelligence), SWRL, electrical appliances]
A Framework for Modeling and Assessing Security of the Internet of Things
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Internet of Things (IoT) is enabling innovative applications in various domains. Due to its heterogeneous and wide scale structure, it introduces many new security issues. To address the security problem, we propose a framework for security modeling and assessment of the IoT. The framework helps to construct graphical security models for the IoT. Generally, the framework involves five steps to find attack scenarios, analyze the security of the IoT through well-defined security metrics, and assess the effectiveness of defense strategies. The benefits of the framework are presented via a study of two example IoT networks. Through the analysis results, we show the capabilities of the proposed framework on mitigating impacts of potential attacks and evaluating the security of large-scale networks.
[Measurement, Security Analysis, large-scale networks, security assessment, Computational modeling, security metrics, Body area networks, Security, Internet of Things, Attack Graphs, Wireless communication, Analytical models, Hierarchical Attack Representation Model, Network topology, security of data, graphical security models, IoT networks, defense strategies effectiveness, security modeling]
Secrecy Outage Probability of Two-Path Successive Relaying in Physical Layer Security
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Relaying is a promising technique to improve wireless physical-layer security. Existing literature shows that a full-duplex relay can further improve the secrecy capacity and secrecy outage probability compared to conventional half-duplex relay, but this comes at a price of sophisticated implementation. For sake of easy implementation, two-path successive relaying has been proposed to emulate the full-duplex relay by scheduling a pair of half-duplex relays to assist the source transmission alternately. However, the performance of two-path successive relaying in secrecy communication remains unexplored. This paper proposes a secrecy two-path successive relaying protocol for a scenario with one source, one destination and two half-duplex relays. The relays operate alternately in a time division mode to forward messages continuously from source to destination in the presence of an eavesdropper. To further confuse the eavesdropper, the source and relay are scheduled to transmit jamming signals at appropriate intervals. Analytical results on the secrecy outage probability reveals that the proposed protocol is able to deliver the target secrecy rate when the SNR of the eavesdropping channels are high. In addition, the secrecy outage probability of the proposed protocol is the joint secrecy outage probability of the relay pair. Numerical simulations show that the proposed protocol achieves the highest ergodic secrecy capacity and lowest secrecy outage probability compared to the existing half duplex relaying, full duplex relaying and full duplex jamming schemes.
[telecommunication security, Physical layer secrecy, Protocols, secrecy outage probability, Security, Relays, Jamming, full duplex relaying, IoT, Wireless communication, two-path successive relaying, half-duplex relays, secrecy capacity, SNR, numerical analysis, scheduling, eavesdropping channels, protocols, source transmission, ergodic secrecy capacity, secrecy communication, numerical simulations, half duplex relaying, jamming, Interference, wireless physical-layer security, secrecy two-path successive relaying protocol, full duplex jamming schemes, half-duplex relay, time division mode, physical layer security, Capacity planning]
Implementation of Relay-Based Emergency Communication System on Software Defined Radio
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
The loss of communication system during natural disaster is known to hamper the efficiency of disaster response. In most cases, the paralyzed communication system often appears as one of the factors which delays the disaster response and complicates the rescue operation. However, if the communication systems including the mobile network are in working conditions, they are usually congested. Hence there is a need for an improved emergency communication system (ECS) to provide reliability and improve system performance during emergency. Furthermore, most of the works on ECS are based on theoretical simulations which can not capture the real world scenario. Taking into account the structure of the internet of things, which includes sensing, computing and communicating, this paper presents an ECS which consists of a wireless relay node, a disaster sensing and detection system. The relay node serves as a temporary communication station to help victims equipped with mobile phones to maintain a reliable communication link with the next available base station in the vicinity. Meanwhile, the sensors are used to detect abnormal changes in environmental parameters such as temperature and water level. The design is implemented on software defined radios. The simulated and measured results shows significant improvement in terms of data and bit error rate over the direct communication scheme.
[Base stations, Emergency communication system, paralyzed communication system, communication link, disasters, relay-based emergency communication system, relaying, software defined radio, bit error rate, Relays, Temperature sensors, Temperature measurement, Wireless communication, software radio, natural disaster, internet of things, disaster response, disaster sensing, base station]
A Store-and-Delivery Based MAC Protocol for Air-Ground Collaborative Wireless Networks for Precision Agriculture
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Due to rapid population growth, the demand for food is also elevating, which inspires farmers to embrace precision agriculture to increase production by exploiting predictive analytics on relevant real-time data. The exactitude of a prediction is vital to decide the next course of actions to be taken to compensate current demands, which again relies on a competent data acquisition technique. The Media Access Control (MAC) protocols have significant contribution in designing data acquisition technique. In this paper, we propose a new Store-and-Delivery base MAC (SD-MAC) protocol for Air-Ground Collaborative Wireless Networks (AGCWNs) to acquire data efficiently from the sensing devices which are deployed in the agricultural field. Our proposed protocol takes into consideration of the factors of network architecture and transforms them into advantages to attain higher throughput. The performance of the proposed protocol is evaluated using simulations and involving another such protocol, where the proposed protocol outperforms the other protocol.
[radio networks, AGCWN, SD-MAC protocol, Data acquisition, access protocols, predictive analytic, air-ground collaborative wireless network, agriculture, store-and-delivery based MAC protocol, precision agriculture, data acquisition technique, media access control protocol, Production, Media Access Protocol, Agriculture, data acquisition, Sensors]
Conceptual Survey on Data Stream Processing Systems
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
The present paper gives an overview about the state of the art technology within the area of data stream processing systems. Although the area of stream processing systems is not new, it is receiving a greater interest in the light of current business trends like the Internet of Things (IoT). The comparison of systems thereby includes several aspects such as a look into their architectures as well as into the responsibilities of the corresponding system components. A ranking or recommendations for one or more system(s) is not part of the work.
[Java, Storm, Throughput, Topology, Sparks, Internet of Things, Flink, Stream Processing, Storms, data stream processing systems, system components, Market research, Samza, data handling, Spark]
Software Defined Networking for Communication and Control of Cyber-Physical Systems
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Cyber-physical Systems (CPS) combine human-machine interaction, the physical world around us, and software aspects by integrating physical systems with communication networks. Opportunities and research challenges are largely interconnected with the three core sub-domains of CPS &amp;mdash; computation, communication and control. The current state of the art of the legacy communication technology is one of the major hindrances limiting the evolution of CPS. Most specifically, innovation in communication is restricted with existing routing and switching technologies leaving no practical methods for researchers to test their new ideas. Software Defined Networking (SDN), through the realization of OpenFlow, separates network control logic from the underlying physical routers and switches. This phenomenon allows researchers to write high-level control programs specifying the behavior of the core networks used to implement CPS and thus, enable innovation in next generation communication architectures for CPS. In this paper, we propose a SDN architecture for industrial automation. Network design requirements are extracted from formal component specifications which support the generation of automatic network configurations. The proposed SDN architecture aims to leverage Industry 4.0 and Smart Factories, to bring together industrial automation installations with networking and Internet technologies.
[PROFINET, CPS, OpenFlow, Control systems, Industry 4.0, formal specification, network design requirements, formal component specifications, Computer architecture, communication subdomain, industrial automation, Real-time systems, SDPROFINET, Automation, smart factories, Industrial Automation, SDN architecture, Smart Factories, software defined networking, Topology, Formal specifications, computation subdomain, cyber-physical systems, Belts, Software Defined Networking, automatic network configurations, control subdomain]
Traffic Offloading with Mobility in LTE HeNB Networks
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
In these years, the traffic is rapidly increasing in mobile communication networks. The increasing traffic seriously consumes the bandwidth of the core network. The 3GPP proposes a series of traffic offloading solutions in the Long Term Evolution-Advanced (LTE-A) system in which part of traffic from the core network is migrated to the Internet. Two traffic offloading methods are designed for the Home eNodeB (HeNB) networks: (1) Local IP Access (LIPA), which provides User Equipments (UEs) with the ability to communicate with other objects (e.g., UEs and servers) located in the same local HeNB network via HeNB without accessing the core network, and (2) Selected IP Traffic Offload at Local Network (SIPTO@LN), which provides UEs with the ability to connect to the Internet via HeNB without going to the core network. Several studies tried to improve 3GPP traffic offloading methods; however, those methods have no or little support of mobility. In this paper, we propose two methods to offload the traffic in Local HeNB Network (LHN) with better mobility support than existing methods. The first method, Local Access Traffic Offload (LATO), enhances the LIPA function by providing UEs with the ability to hand over into and out of the LHN. The second method, Global Access Traffic Offload (GATO), enhances the SIPTO function by providing UEs with the ability to hand over between the LHNs.
[mobility, LIPA function, 3G mobile communication, Traffic Offload, 3GPP traffic offloading methods, LHN, LTE HeNB Networks, IP Traffic Offload, IP networks, User Equipments, Long Term Evolution, mobile communication networks, Local IP Access, global access traffic offload, Long Term Evolution-advanced, LTE-A, Local Network, Thin film transistors, local access traffic offload, HeNB, Hidden Markov models, Logic gates, traffic offloading solutions, core network, Internet, telecommunication traffic, home eNodeB networks]
The Performance Survey of in Memory Database
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
To satisfy the ever-increasing performance demand of Big Data and critical applications the data management needs to offer the flexible schema, high availability, light weight replica, high volume and high scalability features so as to facilitate the transaction. The in memory database (IMDB) eliminates the I/O bottleneck by storing data in main memory. We give a deeper analysis of current main-stream IMDB systems performance which focuses on the data structure, architecture, volume, concurrency, availability and scalability. The V3 performance model is proposed to evaluate the Velocity, Volume and Varity of the 19 IMDB systems, in order to highlight the candidates with realtime transaction and high volume processing capacity coordinately. Test results clearly demonstrate that NewSQL is better at dealing with high-frequency trading models. To fully utilize the advantages of the multi-core and many-core processors capability improvements, a three-level optimization design strategy, which includes the memory-access level, the kernel-speedup level and the data-partition level also be proposed using the hardware parallelism for achieving task-level and data-level parallelism of IMDB programs, guarantees the IMDB could accelerate the real-time transaction in an efficient way. We believe that IMDB should become a compulsive option for enterprise users.
[transaction processing, IMDB, data management, Scalability, many-core processor capability improvement, Random access memory, data structure, V3 performance model, data architecture, Servers, database management systems, multicore processor capability improvement, NewSQL, Concurrent computing, data storage, Databases, data availability, Real-time systems, data structures, In Memory Database, three-level optimization design strategy, data-partition level, data volume, data scalability, real-time transaction, memory-access level, Trading System, volume processing capacity, Big Data, data concurrency, in-memory database, SQL, hardware parallelism, Performance Evaluation, Memory management, task-level parallelism, concurrency control, data-level parallelism, kernel-speedup level, velocity-volume-and-varity evaluation, Memory Computing]
Dynamic Web Service Composition Based on State Space Searching
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Web service composition problem was considered as a planning problem by previous research. However, many factors constantly affect the QoS and results of invocation of web services, thus the environment of web services is dynamic. As result, web service composition problem should be considered as an uncertain planning problem. This paper uses Markov property to deal with the uncertain planning problem for service composition. According to the uncertainty model, we propose a reinforcement learning method to compose web services. Without knowing the transition function and reward function, our uncertain planning method uses an estimated value function to approach a real function and is able to obtain a composite service. The results of experiments show that our method can effectively reduce computing time of the service composition.
[optimal policy, Markov property, Uncertainty, Dynamic, Heuristic algorithms, Quality of service, quality of service, uncertain planning problem, transition function, Web services, QoS, Learning (artificial intelligence), dynamic Web service composition problem, Markov processes, Planning, reward function, Web service composition, learning (artificial intelligence), reinforcement learning method, state space searching]
Cloud Based Monitoring of Timed Events for Industrial Automation
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
This paper presents ideas towards automatic monitoring of industrial automation devices by using a cloud based monitoring infrastructure. We are in particular aiming at fine grained timed properties that can be described using formal specification techniques such as behavioral types. Possible applications are in the areas of remote maintenance and servicing as well as commissioning and testing. Our work is based in the IEC 61499 standard.
[Cloud computing, Automation, cloud based monitoring infrastructure, formal specification techniques, production engineering computing, process monitoring, formal specification, industrial automation devices, timed event monitoring, Automata, remote maintenance, industrial control, Timing, cloud computing, IEC Standards, control engineering computing, IEC 61499 standard, Monitoring, Clocks]
JellyFish: Online Performance Tuning with Adaptive Configuration and Elastic Container in Hadoop Yarn
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
MapReduce is a popular computing framework for large-scale data processing. Practical experience shows that inappropriate configurations can result in poor performance of MapReduce jobs, however, it is challenging to pick out a suitable configuration in a short time. Also, current central resource scheduler may cause low resource utilization, and degrade the performance of the cluster. This paper proposes an online performance tuning system, JellyFish, to improve performance of MapReduce jobs and increase resource utilization in Hadoop YARN. JellyFish continually collects real-time statistics to optimize configuration and resource allocation dynamically during execution of a job. During performance tuning process, JellyFish firstly tunes configuration parameters by reducing the dimensionality of search space with a divide-and-conquer approach and using a model-based hill climbing algorithm to improve tuning efficiency; secondly, JellyFish re-schedules resources in nodes by using a novel elastic container that can expand and shrink dynamically according to resource usage, and a resource re-scheduling strategy to make full use of cluster resources. Experimental results show that JellyFish can improve performance of MapReduce jobs by an average of 24% for jobs run for the first time, and by an average of 65% for jobs run multiple times compared to default YARN.
[central resource scheduler, tuning efficiency improvement, divide and conquer methods, Heuristic algorithms, adaptive configuration, divide-and-conquer approach, Containers, Performance Tuning, Yarn, parallel programming, job execution, MapReduce, resource allocation, configuration optimization, resource re-scheduling strategy, scheduling, elastic container, Real-time systems, Monitoring, online performance tuning system, JellyFish, MapReduce computing framework, YARN, Tuning, resource usage, model-based hill climbing algorithm, large-scale data processing, MapReduce job performance improvement, resource utilization improvement, search space dimensionality reduction, data handling, Distributed Computing, Resource management, Hadoop Yarn]
Cloud-Based Analysis and Control for Robots in Industrial Automation
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
We present an architecture for cloud-based analysis of manufacturing environments and derived robot actions. We report on our implemented facility that combines visual cloud-based analysis of a robot environment and derived actions of robots plus appropriate means for human interaction with these cloud based services via hand gestures to interact with analysis results and robots. The facility will enable study of the software engineering of most-feasible combinations of human interaction interfaces with remote equipment.
[manufacturing environments, Cloud computing, human-robot interaction, derived robot actions, visual cloud-based analysis, remote equipment, cloud-based analysis, hand gestures, factory automation, Cloud-based automation, user interfaces, Service robots, Robot kinematics, industrial robots, User interfaces, Robot sensing systems, Cameras, software engineering, human interaction interfaces, cloud computing, robot environment, cloud based services, control engineering computing, Software engineering]
Online Replacement of Distributed Controllers in Software Defined Networks
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
To deploy Software Defined Networks (SDN) in large-scale datacenters, distributed controllers need to be used to achieve scalability and reliability. In such kind of continuously running systems, maintenance often involves online replacement of specified nodes in distributed controllers to upgrade their hardware/software, while the service interruption of controllers and processing errors are not allowed. In addition, load-balancing needs to be re-considered since the hardware of controllers may become heterogeneous due to the replacement. To address this problem, this paper proposes ORDIC, an Online Replacement method of DIstributed SDN Controller in a disruption-free manner. ORDIC enables the safe disruption-free replacement of a specified controller while ensuring network switches utilize resources of the new controller efficiently. In this paper, we build the prototype system on Floodlight to demonstrate our design and test the performance. The experimental results show that: during the replacement process, ORDIC can efficiently allocate appropriate loads to the new controller, so as to improve the overall performance of the distributed controller platform.
[software defined networks, Packet loss, Control systems, Floodlight, Load Balancing, resource allocation, Online Replacement, computer facilities, Hardware, distributed control, telecommunication control, ORDIC, load-balancing, Distributed Controllers, Process control, large-scale data centers, software defined networking, Software defined networking, telecommunication switching, Software Defined Networking(SDN), Load management, Software, network switches, disruption-free replacement, online replacement method of distributed SDN controller]
Towards a Human-Centred Approach in Modelling and Testing of Cyber-Physical Systems
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
The ability to capture different levels of abstraction in a system model is especially important for remote integration, testing/verification, and manufacturing of cyber-physical systems (CPSs). However, the complexity of modelling and testing of CPSs makes these processes extremely prone to human error. In this paper we present our ongoing work on introducing human-centred considerations into modelling and testing of CPSs, which allow for agile iterative refinement processes of different levels of abstraction when errors are discovered or missing information is completed.
[iterative methods, human-centred approach, program testing, program verification, software prototyping, human factors, cyber-physical system testing, Interoperability, cyber-physical system verification, CPS modelling, cyber-physical system manufacturing, agile iterative refinement processes, Testing, Context, cyber-physical system modelling, Cyber-physical systems, system model, CPS testing, human error, cyber-physical systems, remote integration, missing information, Software, Concrete, Planning]
[Publisher's information]
2015 IEEE 21st International Conference on Parallel and Distributed Systems
None
2015
Provides a listing of current committee members and society officers.
[]
Foreword
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Organizing Committee
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Provides a listing of current committee members and society officers.
[]
Workshop Organizing Committees
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Provides a listing of current committee members and society officers.
[]
Keynotes
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
These keynote speeches discuss the following: Database meets deep learning: challenges and opportunities; IoT: towards a connected era-research direction and social impacts; theory and optimization of multicore memory performance; and On sensorless sensing for the Internet of Everything.
[multiprocessing systems, Internet of Everything, database management system, performance evaluation, social impacts, database management systems, Internet of Things, IoT, storage management, deep learning, sensors, sensorless sensing, learning (artificial intelligence), socio-economic effects, multicore memory performance optimization]
A Distributed Auction Approach to Crowdsourced Sensing over Smartphones
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Crowdsourcing distributed sensing to smartphones has become an appealing paradigm for harnessing the power of the crowd for collecting and sharing information. Market-driven auctions have been recognized as an effective way to matching sensing service demand and supply. A crowdsourcer acts as an auctioneer and smartphone workers act as bidders who are willing to supply sensing services to crowdsourcers. A number of auction mechanisms have been proposed. Unfortunately, most of them assume there is only one crowdsourcer, which is not true in the real world. In this paper, we consider the design of distributed auctions for a crowdsourced sensing market with multiple crowdsourcers and many smartphone workers. We employ a distributed reverse auction that works in a fully distributed fashion. The mechanism requires no one to release its private information. With extensive simulations, we demonstrate that our distributed mechanism achieves the optimal social welfare, and produces better performance than a competing algorithm.
[mobile computing, crowdsourced sensing, crowdsourcing, resource allocation, distributed reverse auction, Conferences, Crowdsourced sensing distributed auction, smart phones, smartphones, smartphone convergence, equilibrium]
A Hybrid Approach Based on Collaborative Filtering to Recommending Mobile Apps
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
With the rapid emergence of mobile devices, smart phones have penetrated into every aspect of people's daily life. The explosive growth of mobile applications makes it difficult for mobile users to find suitable and interesting applications. Mobile app recommendation has been explored by many researchers and some industry solutions are proposed for mobile users. Collaborative filter (CF) is a popular technique in recommendation system, but it requires explicit feedback data like ratings, which are usually difficult to be collected. Many application markets provide keywords search functions and recommend applications with high download counts. However, downloading an application is a vague indicator of whether the user truly likes that application, as the user may probably uninstall that application immediately after it has been installed. Some other industry solutions involve users' personal data such as social networking, which may result in privacy leaks. In this paper, we propose two hybrid models based on collaborative filtering to make mobile app recommendations. We leverage RFD (Recency, Frequency, Duration) model to label the preference for users over applications from users' usage data. The first model, namely, improved item-oriented collaborative filtering (IIOCF), improves the performance of the item-oriented approach by leveraging the latent factor model to discover latent factors among applications. The second model (HLF) is a hybrid model of the latent factor and item-oriented approach. This model sums the predictions of the latent factor model and item-oriented approach, thereby capturing the advantages of both approaches. Our experiment results over 6,568 applications and 25,302 users clearly show that HLF model has better performance than both the item-oriented and latent factor approach.
[Industries, collaborative filtering, recency-frequency-duration model, hybrid models, HLF, RFD Model, Mobile communication, Frequency measurement, social networking, Sparse matrices, mobile app recommendation, application ratings, mobile computing, mobile applications, keyword search functions, mobile users, item-oriented approach, Filtering, hybrid collaborative filtering, Computational modeling, latent factor model, smart phones, RFD model, feedback data, user personal data, recommender systems, IIOCF, Collaboration, improved item-oriented collaborative filtering, mobile devices, hybrid model of latent factor]
Bat with Good Eyesight: Using Acoustic Signal and Image to Achieve Accurate Indoor Localization
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Despite very significant efforts on smartphonebased indoor localization, highly accurate and practical method remains an open problem. To guarantee accuracy, robustness, and practicality, in this paper, we propose SITE, a novel scheme uses acoustic Signal and phone Images to achieve accurate and robust indoor posiTion systEm. Our key observation is that if the simultaneously computed locations according to different sets of acoustic sources vary small, the positioning result is close to the true physical location. Based on the pre-deployed acoustic sources, SITE first tracks the direction of smartphone relative to an individual acoustic source according to proactively generated doppler effect in rough horizontal plane. Given m (m &gt;= 3) acoustic sources, SITE can compute the relative coordinate of the phone in floor plan. By respectively selecting different sets of acoustic sources to compute the related coordinates, SITE can make sure whether these positioning results satisfy the requirement of positioning accuracy. If not, using images captured by phone camera, SITE exploits the synergy between its acoustic-based localization (coarse-grained) and the relative positions in reconstructed 3D point cloud by VisualSFM technique to refine the positioning result using acoustic signals. We have built a prototype of the SITE system and conducted evaluations in real testbed. Experimental results show that SITE is excellent in accuracy, robust and valuable in practical application.
[image processing, SITE, Acoustic Signal, Two dimensional displays, Buildings, Images, Acoustics, smart phones, acoustic-based localization, Doppler effect, generated Doppler effect, Indoor Localization, VisualSFM, Three-dimensional displays, smartphone based indoor localization, Cameras, indoor radio, Mathematical model, Smartphone, acoustic signal processing]
Delay Tolerant Routing for Cognitive Radio Vehicular Ad Hoc Networks
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Cognitive Radio Vehicular Ad Hoc Networks (CRVANETs) can resolve the conflicts between limited spectrum resource and vehicular communication service needs, and thus it receives much concern. Most existing routing solutions for Cognitive Radio Ad Hoc Networks (CRAHNs) or VANETs cannot be directly applied to CR-VANETs due to the high mobility of vehicles and varying availability of spectrums. To fill this gap, with the proliferation of delay tolerant applications, this paper presents a delay tolerant routing strategy for CR-VANETs with the object of maximizing the packet delivery ratio from the source to the destination. The proposed routing scheme is composed of two parts: the routing policy and the message replication policy. In the first policy, we select a concurrent forwarding set under the restriction of CR spectrum holes, and choose an optimized routing path for the next hop. In the second policy, we propose a hybrid and adaptable method relying on Drop-Oldest and Drop-Most strategies, to reduce the delivery cost. Through extensive simulations, we demonstrate that the proposed delay tolerant routing mechanism provides with better packet delivery ratio compared to state-of-the-art protocols.
[vehicular ad hoc networks, Roads, delay tolerant routing, Routing, spectrum resource, CRVANET, Cognitive radio, vehicular communication, packet delivery ratio, Vehicles, cognitive radio ad hoc networks, routing, Cognitive Radio Vehicular Ad Hoc Networks, cognitive radio, Vehicular ad hoc networks, telecommunication network routing, delay tolerant, Routing protocols, Delays]
Distributed Optimal Source Coding Rate Allocation for Data Aggregation in Wireless Sensor Networks
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
In wireless sensor networks (WSNs), there usually exist spatial overlap and data correlation among sensors. Reducing data redundancy is crucial for prolonging network lifetime inWSNs. Source coding is an effective method for data aggregation to reduce data redundancy. However, source coding was regarded as an independent problem in previous work. Little work pays attention to optimal coding rate and associates it with underlying protocols. In this paper, we adopt Slepian-Wolf theorem to achieve the boundary of coding rate, and propose a cross-layer optimization framework to give the optimal source coding rate and flow allocation. We seek to establish a structure-free, multipath transmission model. To the best of our knowledge, this is the first work to solve the optimal source coding rate allocation problem in WSNs. Our extensive simulation results demonstrate that the proposed framework can reduce network traffic and extend network lifetime significantly.
[data correlation, source coding, wireless sensor networks, Source coding, source coding rate allocation, Redundancy, Data aggregation, Slepian-Wolf theorem, Cross-layer design, Entropy, data aggregation, Information entropy, network traffic, Wireless sensor networks, WSN, multipath transmission model, Source coding rate, Resource management, protocols, telecommunication traffic, network lifetime, Flow allocation]
Distributed Real-Time Pricing Scheme for Local Power Supplier in Smart Community
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
In this paper, we consider the real-time pricing problem for a small scale local power supplier (LPS) in a smart energy community. The LPS supplies power to the residential users (RUs) in a local area and sells the remaining power to the main grid. Since the selling price to the main grid is relative low, LPS intends to sell more power to the RUs with an appropriate price. The LPS determines the price based on the proposed pricing scheme to maximize its revenue. The price is informed to RUs through the communication infrastructure. According to the announced price of LPS, each RU schedules its power consumption to maximize its utility. We model the interactions between the local power supplier and all users as a one-leader multi-followers Stackelberg game, where the LPS acts as the leader and RUs act as the followers. To address this problem, a distributed algorithm based on information exchange between the LPS and RUs is proposed. Simulation results show that the distributed algorithm converges to the Stackelberg equilibrium.
[Schedules, multifollowers Stackelberg game, Stackelberg game, smart power grids, power grid, power generation scheduling, power consumption, Stackelberg equilibrium, energy storage, Pricing, Real-time systems, Distributed algorithms, Power demand, smart grid, distributed real-time pricing, smart energy community, distributed real-time pricing scheme, RU schedules, Renewable energy sources, distributed algorithm, information exchange, local power supplier, Games, power distribution, demand side management, electricity supply industry, residential users, pricing, demand response]
Drone-Based Wireless Relay Using Online Tensor Update
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
In the wireless communication, there are many cases where the transmission path is obstructed by unknown objects. With the rapid development of the drone technology in recent years, the drones are advocated to serve as mobile relays to forward data streams. However, the challenges are that data transmission may suffer severe signal attenuation due to the existence of the obstructions and it is challenging to find the best location for mobile relays due to the dynamic environment and unpredictable interference. To address the problem, this paper proposes an approach that a drone can automatically find the location with the optimal link quality. We design a novel algorithm, named Path-sampling Online Tensor Update (POTU), to estimate the link quality in the space and find the optimal location. Furthermore, the algorithm is practical to the real applications due to the simplicity of implementation. In the experiment, we construct a realistic scene and compare the performance of our algorithm with the classic and the state-of-the-art algorithms. As a result, POTU outperforms existing methods in achieving the trade-off between time cost and estimation accuracy.
[Algorithm design and analysis, estimation theory, mobile relays, POTU, Mobile communication, signal attenuation, mobility management (mobile radio), path-sampling online tensor update, Relays, wireless communication, drone technology, radiofrequency interference, Tensile stress, autonomous aerial vehicles, data transmission, Cameras, Data communication, Drones, relay networks (telecommunication), drone-based wireless relay, link quality estimation]
EHR: Routing Protocol for Energy Harvesting Wireless Sensor Networks
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
A well-designed energy-efficient routing protocol is an indispensable part for prolonging the lifetime of wireless sensor networks (WSNs) because a sensor node usually has limited energy. Many research efforts are contributed on routing design in WSNs. With the development of green technology, the energy harvesting technique is being applied to real WSNs. Therefore, existing routing protocols are not suitable for such new WSNs with energy harvesting. In this paper, we concentrate on designing a novel routing protocol, named energy harvesting routing (EHR), which takes energy harvesting as one major factor into routing design to improve the energy efficiency. First, we introduce a hybrid routing metric combining the effect of residual energy and energy harvesting rate. Then we propose an updating mechanism allowing every node to maintain dynamic energy information of its neighbors. Based on the hybrid metric and the neighbor information, EHR is able to locally select the optimal next hop. Extensive simulations are conducted to evaluate the performance of EHR. Results demonstrate that EHR outperforms existing routing protocols in energy harvesting WSNs in term of the energy efficiency.
[Greedy algorithms, Energy consumption, wireless sensor networks, Energy-efficient routing protocol, telecommunication power management, energy harvesting, routing protocol, Routing, Energy harvesting, Wireless sensor networks, WSN, energy harvesting routing, routing protocols, energy efficiency, energy conservation, Routing protocols]
Enabling Mobile Device Coordination over Distributed Shared Memory
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Distributed shared memory-based coordination has the advantage of simplifying the coordination logic to read/write operations over the illusionary local memory. However, it is notoriously challenging to come up with a cost-effective implementation of the distributed shared memory. The implementation becomes more challenging in mobile environments, due to the resource constraints and the more rapid changes in the computing context. To this end, we propose the Mobile Distributed Shared Memory (MDSM) middleware to facilitate the development of mobile coordination applications. The key constructs in the shared memory are shared registers. Shared registers with different read/write patterns are implemented to facilitate flexible coordination. The registers also have different consistency semantics, to enable efficient tradeoff between data consistency and data access cost. An application framework is proposed to simplify the implementation of mobile coordination, relying on the middleware support from MDSM. A case study is conducted to demonstrate the usage of MDSM, where a soccer game application for the mobile phone is developed. Experimental evaluation is conducted to quantify different options of the consistency-latency tradeoff in the case study. The performance measurements show the cost-effectiveness of eventual consistency in this game. We also verify the read/write traces to further explain why eventual consistency practically performs better than it can guarantee.
[read-write operations, soccer game application, distributed shared memory, mobile coordination middleware, Mobile communication, Mobile handsets, Registers, flexible coordination, formal logic, mobile computing, mobile device coordination, coordination logic, computer games, MDSM middleware, eventual consistency, consistency-latency tradeoff, Sensors, middleware, atomicity, resource constraints, mobile distributed shared memory middleware, performance evaluation, data integrity, Middleware, data consistency, performance measurements, mobile phone, Games, distributed shared memory systems, Data models, shared registers, data access cost]
Identifying a New Non-Linear CSI Phase Measurement Error with Commodity WiFi Devices
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
WiFi technology has gained a wide prevalence for not only wireless communication but also pervasive sensing. A wide variety of emerging applications leverage accurate measurements of the Channel State Information (CSI) information exposed by commodity WiFi devices. Due to hardware imperfection of commodity WiFi devices, the frequency response of internal signal processing circuit is mixed with the real channel frequency response in passband, which makes deriving accurate channel frequency response from CSI measurements a challenging task. In this paper, we conduct an extensive empirical studies on CSI measurements and identify a non-negligible non-linear CSI phase error, which cannot be compensated by existing calibration strategies targeted at linear CSI phase errors. We conduct intensive analysis on the properties of such non-linear CSI phase errors and find that such errors are prevalent among various WiFi devices. Furthermore, they are stable along time and for different time-of-flight but related to the received signal strength indication (RSSI) of the received signal, the band frequency and the specific radios used between a transmission pair. Based on these key observations, we infer that the IQ imbalance issue in the direct-down-conversion architecture of commodity WiFi devices is the root source of the non-linear CSI phase errors. Our findings are essential to CSI-based applications and call for new practical strategies to remedy non-linear phase errors.
[internal signal processing circuit, Phase measurement, error compensation, nonnegligible CSI phase error compensation, signal processing, Receivers, channel state information, empirical study, real channel frequency response, Frequency measurement, calibration strategies, non-linear phase errors, Channel State Information (CSI), nonlinear CSI phase measurement error, Measurement uncertainty, commodity Wi-Fi devices, received signal strength indication, Signal processing, Frequency response, wireless LAN, IEEE 802.11 Standard, measurements]
OR-Play: An Optimal Relay Placement Scheme for High-Quality Wireless Network Services
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
With the development of wireless communication and social network, wireless network service demands have increased rapidly in recent years. To amplify the wireless signals and expand the coverage of wireless networks, wireless relay nodes are introduced. This paper addresses the problem of finding an optimal deployment of access points and wireless relay nodes in an arbitrary environment to provide all the potential users with higher quality wireless network services. Our ambition is to maximize the coverage rate and to minimize the energy consumption of the relay nodes. Correspondingly, we design a scheme named OR-Play: an optimal relay placement scheme to provide high-quality wireless services, which consists of three phases. First, OR-Play provides an area coverage for an arbitrary area. We use the virtual force model to determine the positions of wireless devices, including access points and relay nodes, and thus extend the network lifetime. In the second phase, OR-Play selects access points by a 2-approximation algorithm for the metric k-center problem. In the third phase, we define a new problem: k-minimum energy broadcasting trees. We design a distributed greedy strategy to determine the broadcasting trees, based on which the power of relay nodes are precisely assigned. Finally, the simulation results validate the effectiveness and efficiency of OR-Play.
[radio networks, metric k-center problem, access points, Force, telecommunication services, distributed greedy strategy, wireless network services, Relays, wireless communication, social network, Wireless networks, Broadcasting, relay networks (telecommunication), optimal relay placement, k-minimum energy broadcasting trees, wireless networks, Wireless sensor networks, mobile communication, wireless relay nodes, OR-play, 2-approximation algorithm, Approximation algorithms, radiocommunication, social networking (online), network lifetime]
Physical Object Model for Smart Environments with Temporal Capability
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
This paper presents the Physical Object Model (POM), a data model for smart environments, that creates a smart spaces hierarchy to oversee spaces, things and users and the interactions between them. To demonstrate our design, we developed a POM server and a POM client. The server is in charge of creating such a smart space that can register, monitor and control many clients. The clients, running on smart things, use a pre-defined API to communicate with the server to query surrounding devices or to advertise services they provide to the space. To record clients' impact and activities inside the server, we also developed a logging component that saves every change to the space to a database. Using the logging component, we provided a temporal API to request the state of space at a given point in time in the past. To evaluate our system performance against real-life scenarios, we made a simulator that takes profile inputs and performs tests against the POM server.
[POM server, real-life scenarios, data model, application program interfaces, data analysis, temporal API, physical object model, Aerospace electronics, Servers, Internet of Things, Time Persistence, smart spaces hierarchy, Hospitals, temporal capability, state of space, Event Management, Data models, Temperature control, POM client, smart things, logging component, Smart devices, Monitoring, Data Models]
ppNav: Peer-to-Peer Indoor Navigation for Smartphones
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Most of existing indoor navigation systems work in a client/server manner, which needs to deploy comprehensive localization services together with precise indoor maps a prior. In this paper, we design and realize a Peer-to-Peer navigation system, named ppNav, on smartphones, which enables the fast-to-deploy navigation services, avoiding the requirements of pre-deployed location services and detailed floorplans. ppNav navigates a user to the destination by tracking user mobility, promoting timely walking tips, and alerting potential deviations, according to a previous traveller's trace experience. Specifically, we utilize the ubiquitous WiFi fingerprints in a novel diagrammed form and extract both radio and visual features of the diagram to track relative locations and exploit fingerprint similarity trend for deviation detection. Consolidating these techniques, we implement ppNav on commercial mobile devices and validate its performance in real environments. Our results show that ppNav achieves delightful performance, with an average relative error of 0.9m in trace tracking and a maximum delay of 9 samples (about 4.5s) in deviation detection.
[Legged locomotion, indoor navigation, Visualization, fast-to-deploy navigation services, user mobility tracking, Navigation, peer-to-peer computing, peer-to-peer indoor navigation, localization services, smart phones, mobility management (mobile radio), Synchronization, peer-to-peer, predeployed location services, indoor navigation systems, mobile devices, Feature extraction, Market research, smartphones, indoor radio, ppNav, IEEE 802.11 Standard, ubiquitous WiFi fingerprints, sequential fingerprints]
PUZZLE: Enhancing Throughput by Covering Neighbor's Blanks
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
The coexistence between Long Term Evolution in an Unlicensed spectrum (LTE-U) and Wi-Fi has been widely recognized by the many worldwide telecommunication organizations as an urgent problem which needs to be solved. Carrier Sense Adaptive Transmission (CSAT) is an adaptive time division scheme, proposed by Qualcomm, which straightforwardly avoids signal collision between LTE-U and Wi-Fi under the 5GHz frequency domain. However, in this paper, we first illustrate how CSAT negatively impacts on Wi-Fi. Furthermore, we propose another novel concept, PUZZLE, to enhance the function of CSAT in both Wi-Fi and the overall throughput. The novelty of PUZZLE is in where it exploits the feasibility of sharing the medium from the frequency space. PUZZLE is supported by a series of logical Hardware-In-the-Loop (HIL) experiments and tested under a given spectral scenario. The final result shows that, according to the five kinds of spectral utilization scenarios we define and when compared to the traditional time division approaches, the maximum average enhancements on throughput are almost 40% on Wi-Fi and 28% on the overall network.
[hardware-in-the loop simulation, carrier sense adaptive transmission, long term evolution, SystemVue, Throughput, Degradation, LTE-U, telecommunication organizations, Long Term Evolution, Wi-Fi, frequency 5 GHz, PUZZLE, Qualcomm, Interference, 802.11ac, LAA, signal collision, Machine-to-machine communications, Hardware-In-The-Loop, hardware-in-the-loop, Delays, wireless LAN, CSAT, adaptive time division scheme, IEEE 802.11 Standard]
QoE-Driven Cross-Layer Design for Device-to-Device Video Delivery
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
This paper investigates the video delivery on the underlay device-to-device (D2D) communications. Considering that the distributed scheme requires less signaling overhead than the centralized one, we exploit Stackelberg game to establish a QoE-driven distributed cross-layer video delivery model to optimize the parameters which include the RB-level transmission power at the physical layer, a group of RBs at the data link layer and source coding rate at the application layer. In the model, the evolved NodeB (eNB), as a leader, plays an important role in guiding D2D pairs to reuse wireless resources intelligently. D2D pairs, as followers, achieve the goal of maximizing their individual quality of experience (QoE) by competing selfishly in a non-cooperative game. We prove the existence of the Stackelberg equilibrium for the established Stackelberg game model in mathematics and propose a QoE-driven Distributed Cross-layer Video-delivery (QDCV) scheme to reach the Stackelberg equilibrium. Extensive simulations show that the proposed scheme can achieve the Stackelberg equilibrium of the whole system in a relatively short time and our proposed scheme improves the average MOS by about twice compared with the existing QoE scheme.
[Video delivery, Stackelberg game, QoE-driven, Quality of service, underlay device-to-device communications, Device-to-device communication, QoE-driven distributed cross-layer video-delivery scheme, RB-level transmission power, D2D pairs, D2D communications, signaling overhead, source coding rate, Stackelberg equilibrium, device-to-device video delivery, video streaming, Mathematical model, Cross-layer Design, source coding, evolved NodeB, Source coding, Interference, game theory, telecommunication signalling, data link layer, wireless resources, quality of experience, physical layer, mobile communication, Games, Streaming media, application layer]
Resource Scheduling Based on Improved FCM Algorithm for Mobile Cloud Computing
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
With the development of mobile devices, mobile cloud computing is becoming increasingly important. One of the basic questions in mobile cloud computing is how to match user demand with cloud server resources. Based on Improved FCM (IGAFCM) Algorithm, this paper proposes a scheduling scheme which is provided for mobile resources to cluster continuously, so as to reduce the size of the matching requirements during the search. Moreover, Experiments have proved that matching strategy is dynamically adjusted according to the matching score and feedback training.
[Algorithm design and analysis, Cloud computing, matching score, FCM algorithm, Mobile cloud computing, Mobile communication, Scheduling, Classification algorithms, resource clustering, resource scheduling, mobile cloud computing, mobile computing, Processor scheduling, Clustering algorithms, scheduling, cloud computing, feedback training]
Smart-DJ: Context-Aware Personalization for Music Recommendation on Smartphones
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Providing personalized content on smartphones is significant in ensuring user experience and making mobile applications profitable. The existing approaches mostly ignore the rich personalized information from user interaction with smartphones. In this paper, we address the issue of recommending personalized music to smartphone users and propose Smart-DJ. Smart-DJ incorporates an evolutionary model called Incremental Regression Tree, which incrementally collects contextual data, music data and user feedback to characterize his/her personal taste of music. An efficient recommending algorithm is designed to make accurate recommendations within bounded latency. We implement Smart-DJ and evaluate its performance through analysis and real-world experiments. The results demonstrate that Smart-DJ outperforms the state-of-arts approaches in terms of recommendation accuracy and overhead.
[Context, Smart-DJ, context-aware personalization, User Feedback, Personalization, evolutionary model, Mobile communication, Context-awareness, Recommendation. Incremental Regression Tree, mobile computing, recommender systems, music, incremental regression tree, music recommendation, Music, Feature extraction, smartphones, learning (artificial intelligence), Smart phones, Recommender systems, Context modeling]
Smartphone Virtualization
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Virtualization plays a pivotal role in the success of cloud computing service models and is applied extensively in modern public and private data centers. However, its adoption on end user devices such as laptop/desktop computers and cell phones is relatively scarce, mainly because convincing use cases for client device virtualization have proven elusive so far. As smartphones emerge as the linchpin of everyday computing and communication for regular people and application download becomes a fact of life, the Bring Your Own Cloud (BYOD) problem, in which corporate employees connect their own smartphones to the corporate networks for office work, has put most enterprises in an unenvious position of making a difficult choice between corporate security and employee productivity. One effective solution to the BYOD problem is smartphone virtualization, which provides multiple virtual smartphones on a physical smartphone, and enables a user to use a highly secure but not so flexible virtual smartphone in the work environment and a less secure but more flexible virtual smartphone when outside the work environment. This paper describes the design and implementation of a comprehensive smartphone virtualization system called Brahma, which consists of a virtualized smartphone element and a virtual mobility infrastructure element, and presents the detailed evaluation results of the first Brahma prototype on a commercial smartphone.
[Cloud computing, corporate employees, corporate security, virtualisation, Servers, Security, Bring Your Own Cloud problem, Bring Your Own Device, virtual mobility infrastructure element, Hardware, cloud computing, office work, Context, virtualization, smart phones, corporate networks, comprehensive smart phone virtualization system, employee productivity, virtual mobility infrastructure, virtual smart phones, sensor redirection, BYOD problem, physical smart phone, Brahma, personnel, Virtualization, Smartphone, state isolation, Smart phones]
TIP: Time-Efficient Identification Protocol for Unknown RFID Tags Using Bloom Filters
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Unknown tag identification is an important issue in large-scale RFID systems especially in automatic inventory control and asset tracking. Unknown tags refer to the tags, which have entered into the surveillance area, but not been identified by the reader yet. Many protocols have been proposed for unknown tag identification. However, they still suffer from a couple of drawbacks. Firstly, most of them leverage the classical EDFSA protocol in the identification phase, which is considerably time-consuming. Secondly, they do not filter known tags out so that known tags interfered with the unknown tag ID transmission process, especially when the density of known tags is high. To this end, we propose TIP, which is a time-efficient identification protocol for unknown RFID tags using Bloom filter. To avoid the interference of known tags to unknown tags, we employ the bloom filter to deactivate known tags. To make TIP protocol be time-efficient, we introduce an indicator vector for unknown tags so that each unknown tag transmits its ID only once during the holistic identification process. Extensive experimental results demonstrate that the proposed protocol achieves about 12% up to 74% time reduction during the identification process when the density of unknown tags ranges from 1% to 67%, compared with the existing protocols.
[Protocols, radiofrequency identification, surveillance area, indicator vector, Identification, Electronic mail, access protocols, Servers, EDFSA protocol, radiofrequency interference, tag ID transmission process, data structures, asset tracking, tag identification, Probabilistic logic, RFID, Bloom filter, Bloom filters, RFID tags, vectors, time-efficient identification protocol, automatic inventory control, RFID systems, enhanced dynamic framed slotted ALOHA, Unknown tag, TIP]
A Peer-to-Peer File Sharing System over Named Data Networking
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Named Data Networking (NDN), a promising Future Internet Architecture design, requires new experimental applications to demonstrate its performance and feasibility. Through designing, implementing, and evaluating NDNMaze, i.e., an NDN version of a widely deployed peer-to-peer file sharing application called IPMaze, we find that NDNMaze has a simpler system architecture with improved performance and flexibility. The innovative messaging mechanism and distributed hash tables (DHT) in NDNMaze simplified the implementations of key system components such as user management, nearest neighbor discovery, file discovery and distributions. To systematically evaluate the performance, we simulate both versions with NS-3 simulator, and collect a broad range of performance metrics including hop count, data request latency, data request efficiency, and network transmission efficiency. Our experimental results show that NDNMaze achieves better performance than IPMaze due to the NDN's advantages in content-centric data distribution and sharing. Our work shedslight for distributed application design in NDN.
[future Internet architecture, peer-to-peer computing, Systems architecture, Named Data Networking, NDNMaze, Servers, peer-to-peer, distributed hash tables, Computer architecture, TCPIP, peer-to-peer file sharing system, file organisation, content-centric data distribution, Peer-to-peer computing, Internet, IPMaze, named data networking, Indexing, distributed application design]
A Potential Field Based Framework for Publish/Subscribe Service in P2P Cloud
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
As Internet is developing rapidly, new information is generated with an incredibly high speed. In this situation, publish/subscribe services provided by Cloud providers are used to recommend useful information to the users in the Internet. However, the middleware models and routing algorithms in existing publish/subscribe approaches cannot perform efficiently in the P2P Cloud, which is one of the novel Cloud types that belongs to the distributed Cloud. The P2P Cloud brings challenge to publish/subscribe services because it has larger scale and more complicated network architecture than the traditional Cloud. In this paper, a Potential Field based service framework is proposed to provide high-quality publish/subscribe services in P2P Cloud. A middleware is designed in the framework to match subscribe requests and published information. Furthermore, a Potential Field Model is constructed. Under the model, event brokers are organized as P2P overlay for routing on request message. The simulation results show our proposed method has advantages in recall rate, response time, and bandwidth cost against other methods.
[Cloud computing, bandwidth cost, event brokers, distributed Cloud, routing, P2P overlay, P2P cloud, Clustering algorithms, Potential Field Model, cloud computing, Monitoring, middleware, Potential energy, peer-to-peer computing, Biological system modeling, Routing, Cloud Service Framework, Publish/Subscribe, network architecture, P2P Cloud, request message, recall rate, Web services, response time, overlay networks, telecommunication network routing, potential field-based service framework, cloud providers, publish/subscribe service]
A Weighted Network Model Based on Node Fitness Dynamic Evolution
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Many complex networks in practice can be described by weighted networks. Currently, most existing weighted network models only consider the node strength in evolving conditions, but neglect the influence of node attraction on network evolution. In this paper, we propose an accurate and practical weighted evolving network model based on node fitness dynamic evolution, which takes both node strength and node attraction into consideration. Our theoretical analysis and numerical simulations have demonstrated the scale-free property of the network model, which has been widely observed in many real-world networks. Additionally, the phenomenon that very few nodes possess greater fitness is observed via numerical simulations of our network model, which can be referred to as the fitness property of network. Our network model's dual assessment of node strength and node attraction leads to fewer node clustering and stronger robustness of the whole network than other existing network growth models.
[network growth models, numerical simulations, Social network services, node attraction, node strength, Electronic mail, complex networks, theoretical analysis, scale-free property, real-world networks, Dynamic Evolution, Analytical models, The Fitness Property, pattern clustering, Power-law Distribution, Complex networks, numerical analysis, Numerical simulation, node clustering, Numerical models, Mathematical model, weighted evolving network model, Node Fitness, node fitness dynamic evolution]
An Efficient Parallel Approach of Parsing and Indexing for Large-Scale XML Datasets
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
MapReduce is a widely adopted computing framework for data-intensive applications running on clusters. We propose an approach to exploit data parallelisms in XML processing using MapReduce in Hadoop. Our solution seamlessly integrates data storage, labelling, indexing, and parallel queries to process a massive amount of XML data. Specifically, we introduce an SDN labelling algorithm and a distributed hierarchical index using DHTs, we develop an efficient data retrieval approach called B-SLCA. More importantly, we design an advanced two-phase MapReduce solution that is able to efficiently address the issues of labelling, indexing, and query processing on big XML data. We implemented our solution on a real-world Hadoop cluster processing the real-world datasets. Our experimental results show that SDN outperforms NCIM by up to a factor of 1.36 with an average of 1.17, our BSLCA outperforms BwdSLCA by up to a factor of 1.96 with an average of 1.2.
[data-intensive applications, data retrieval, large-scale XML datasets, real-world datasets, parsing, parallel processing, MapReduce, query processing, storage management, DHT, data storage, database indexing, Semantics, data parallelisms, parallel queries, Labeling, SDN, distributed hierarchical index, parallel query processing, B-SLCA, indexing, Keyword search, two-phase MapReduce solution, Hadoop cluster processing, Distributed Programming, Hadoop, Big Data, Encoding, Query processing, grammars, SDN labelling algorithm, Big XML, XML, Big XML Data, Parallel Programming, Indexing, XML processing]
DMNS: A Framework to Dynamically Monitor Simulated Network
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
With rapid development of network simulation technology, monitoring system has become an essential tool for the researching and testing of network space activities. However, the current monitoring technologies of simulated networks cannot satisfy the requirements in terms of flexibility and efficiency. This paper proposes a framework called DMNS to dynamically monitor simulated network. With DMNS, users are able to customize the monitored objects and monitoring actions to meet the requirements of flexibility. In addition, the administrators could dynamically change the monitoring rules for saving resources based on callback mechanism. DMNS also considers the requirements of large-scale distributed simulation. Specifically, it leverages message oriented middleware to achieve efficient monitoring message information transmission to guarantee the robustness of the monitoring system. We implement a prototype of DMNS in a network range system and demonstrate the effectiveness by a case study.
[dynamic simulated network monitoring framework, large-scale distributed simulation, message passing, Conferences, digital simulation, network space activities, distributed simulation, computer network performance evaluation, DMNS, message oriented middleware, monitoring message information transmission, callback mechanism, network range system, dynamic monitoring, callback, network simulation, middleware, resource saving]
DOCO: An Efficient Event Matching Algorithm in Content-Based Publish/Subscribe Systems
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
The content-based publish/subscribe systems are attracting more and more attention in Internet applications due to their intrinsic time, space, and synchronization decoupling properties. With the increase in system scale, the efficiency of event matching becomes more critical for system performance. However, most existing methods suffer significant performance degradation when the system has large volumes of subscriptions. This paper presents DOCO (DOuble COmbination event matching algorithm) to improve the efficiency of event matching in content-based publish/subscribe systems. Via assembling the attributes in the attribute space by pairs, a novel index structure is built up for classification of the subscriptions. On the arrival of an event, the event matching process is only carried out on some related units of the index structure, thus, the number of subscriptions that involved in the event matching process is reduced. A series of experiments are designed to verify the performance of the proposed algorithm, and a comparison with other event matching algorithms is also carried out. The experimental results show that DOCO can improve the efficiency of event matching in content-based publish/subscribe systems.
[Algorithm design and analysis, double combination event matching algorithm, pattern classification, message passing, pattern matching, subscription classification, Conferences, performance evaluation, Internet applications, index structure, Partitioning algorithms, performance degradation, event matching, Indexes, content-based publish-subscribe systems, attribute space, double combination, DOCO, synchronization decoupling properties, Approximation algorithms, Silicon, event matching efficiency improvement, Acceleration, publish/subscribe systems]
DScheduler: Dynamic Network Scheduling Method for MapReduce in Distributed Controllers
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
MapReduce is the most widely used distributed computing framework due to its excellent parallelism and scalability in dealing with large-scale data. It is one of the most important research point in distributed computing field to improve the performance of MapReduce application in data center network. OpenFlow protocol makes it possible to schedule network resource dynamically to provide better link bandwidth for shuffle traffic. Current OpenFlow-based scheduling method runs on a single controller, which cannot meet the needs of excessive switch requests in large scale data center networks. The performance of those scheduling method will decrease obviously due to some conflict problem when they run on distributed controllers. This paper proposed DScheduler, a dynamic network scheduling method for distributed controllers. DScheduler is running as an application on each SDN controller and avoid a majority of conflict problems in scheduling with small cost by using lock and communication between each controller. We implement a prototype system on Floodlight to demonstrate our design and test the performance. Experimental results show that DScheduler has a significant effect on decreasing the occurrence times of conflict situations and improving the performance of openflow-based scheduling method on distributed controllers.
[Schedules, distributed control, OpenFlow protocol, Switches, OpenFlow, DScheduler, Dynamic scheduling, computer centres, OpenFlow-based scheduling method, MapReduce, distributed-controller, distributed computing framework, Processor scheduling, distributed controllers, performance, dynamic schedule, Bandwidth, large-scale data, scheduling, Software, data handling, protocols, dynamic network scheduling method]
Garlic Cast: Lightweight and Decentralized Anonymous Content Sharing
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Anonymous content sharing over the Internet protects user privacy and content confidentiality. Most overlay anonymous communication protocols employ some relay nodes as the proxies to forward content and require relays to perform cryptography or coding operations on messages. They have two major limitations. First, extra computation overhead may discourage overlay nodes from serving as relays. Second, long forwarding latency at relays makes an anonymous path easier to fail under network churn. In this paper, we present a lightweight and decentralized anonymous content sharing system named Garlic Cast, which requires near-zero computation cost on relays and does not rely on any centralized service. Garlic Cast uses random walks to find proxies in overlay networks and an security-enhanced Information Dispersal Algorithm to search and deliver content files. We have implemented a prototype of Garlic Cast and performed extensive simulation on real overlay topologies. Evaluation results show that the throughput of Garlic Cast is higher than that of RSA-based anonymous routing by over two orders of magnitude. Garlic Cast provides high level of anonymity and is robust to various attacks.
[decentralized anonymous content sharing, Protocols, overlay anonymous communication protocols, real overlay topologies, Relays, content distribution, Overlay networks, security enhanced information dispersal algorithm, relay nodes, user content, coding operations, Cryptography, file sharing, lightweight anonymous content sharing, Anonymity, Routing, cryptography operations, computer network security, garlic cast, overley networks, user privacy, overlay networks, data privacy, Peer-to-peer computing, Internet, near zero computation cost]
Modeling Traffic of Big Data Platform for Large Scale Datacenter Networks
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Prior to deployment, network designers often use simulators to pre-evaluate the performance of designed network with artificial network traffic. The traditional way of separating network design from real applications will not only result in over-designed network configurations, wasting money and energy, but also miss the real network demands of applications, degrading system performance. In this paper, we provide a method to model the network traffic of current popular big data platforms, which can observably improve the matching between network design and applications. The new method extracts communication behavior from the popular big data applications and replays the behavior instead of the packet traces. Experiments show that the traffic generated by the model is almost match the real traffic and the model can easily scale to thousands of nodes.
[pattern matching, modeling traffic, Big Data applications, parallel processing, Network topology, big data platform, over-designed network configurations, large scale data center networks, Big data, hadoop, Load modeling, storm, Computational modeling, Big Data, datacenter, Topology, traffic modeling, computer centres, computer network performance evaluation, artificial network traffic, Storms, communication behavior, Big Data platform, Data models, telecommunication traffic]
SHSA: A Method of Network Verification with Stateful Header Space Analysis
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
With the emergence of hybrid software-defined network (SDN) that contains switches and all kinds of middleboxes, there are a lot of obvious problems that have been brought up in verifying data plane consistency. However, recent study in network verification neglected the dynamic data plane verification induced by stateful middleboxes. To handle this limitation, we propose a new method, Stateful Header Space Analysis (SHSA), to verify reachability and detect loops in hybrid software-defined network with stateful middleboxes. Moreover, we optimize the validation process on the base of header space analysis (HSA) and enhance the scalability of our verification algorithm. To validate the applicability of SHSA, we implement four kinds of stateful middleboxes by Open vSwitch and simulate the hybrid network. The experimental results indicate that our method could verify the dynamic data plane accurately. Compared the time cost between SHSA and HSA in Stanford University's backbone network, results show that the efficiency of our method is 30 percent higher than the latter approximately.
[dynamic data plane, middleboxes, Heuristic algorithms, SHSA, network verification method, Ports (Computers), data plane consistency, Middleboxes, hybrid software-defined network, Network topology, reachability, Stateful Middleboxes, SDN, Hybrid Software Defined Network, Load modeling, Loop, Firewalls (computing), Transfer functions, software defined networking, network verification, Reachability, Data Plane Consistency, stateful header space analysis, Open vSwitch, dynamic data plane verification]
Study of Intra- and Interjob Interference on Torus Networks
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Network contention between concurrently running jobs on HPC systems is a primary cause of performance variability. Optimizing job allocation and avoiding network sharing are hence crucial to alleviate the potential performance degradation. In order to do so effectively, an understanding of the interference among concurrently running jobs, their communication patterns, and contention in the network is required. In this work, we choose three representative HPC applications from the DOE Design Forward Project and conduct detailed simulations on a torus network model to analyze both intra-and interjob interference. By scrutinizing the communication behaviors of these applications, we identify relationships between these behaviors and the possible interference introduced by different job placement policies. Our analyses illuminate a path toward communication pattern awareness in job placement on HPC systems.
[Shape, DOE design forward project, Torus, network contention, parallel machines, parallel processing, job placement policies, Degradation, Three-dimensional displays, interjob interference, Job placement, application communication behaviors, network sharing, Interference, Crystals, Supercomputers, performance degradation, performance variability, job allocation optimization, job communication patterns, job contention, HPC systems, intrajob interference, Resource management, torus network model]
Virtual Path Assignment Based on Load Balancing for SDNs
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Software Defined Networking (SDN) has emerged as a new paradigm that offers the programmability required to dynamically configure and control a network. In this paper, we present a path assignment method based on load balancing(PA-LB) for a centralized controller to calculate optimal end-to-end virtual paths over the underlying network infrastructure, considering multiple requests and load balance simultaneously. In the paper, theoretical analysis shows that the path selection is NP-hard, and strongly polynomial time inapproximable under the assumption P not equal to NP. We also give a constant-factor polynomial time approximation algorithm for this problem under some bounded parameters. Extensive simulation results, over a wide range of underlying network topologies and input parameters, demonstrate that the proposed method outperforms traditional shortest path first (SPF) approaches not only in paths mapping number, but also in network load balance. Under the condition of limited resources, such as bandwidth, the proposed method has a better performance with the increase of the number of nodes. Moreover, optimal load balance can be kept in the network, in order to avoid network congestion and minimize the average packet transmission latency.
[centralised control, telecommunication congestion control, load balancing, network congestion avoidance, Control systems, constant-factor polynomial time approximation algorithm, Load Balancing, centralized controller, Path Assignment, Network topology, resource allocation, virtual path assignment, average packet transmission latency minimization, Bandwidth, PA-LB, SDN, Load modeling, network infrastructure, software defined networking, telecommunication network topology, path selection, Topology, NP-hard problem, optimal end-to-end virtual path calculation, network topologies, Load management, Approximation algorithms, network control, Software Defined Networking, minimisation, computational complexity]
A Comparison of Road-Network-Constrained Trajectory Compression Methods
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
The popularity of location-acquisition devices has led to a rapid increase in the amount of trajectory data collected. The large volume of trajectory data causes the difficulties of storing and processing the data. Various trajectory compression methods are therefore proposed to deal with these problems. In this paper, we overview the existing road-network-constrained trajectory compression methods and propose a novel classification based on the features leveraged by them. We also propose new methods that fill in the research blanks indicated by the classification. We conduct a thorough comparison among the existing and new road-network-constrained trajectory compression methods. The performances of the methods are studied via various metrics on real-world dataset. We make new discoveries regarding the performances and the scalability of existing methods, and provide guidelines of road-network-constrained trajectory compression for various scenarios.
[data compression, road traffic, spatio-temporal data, Roads, Scalability, trajectory data storage, network theory (graphs), compression algorithm, Routing, Encoding, Compression algorithms, Presses, road-network-constrained trajectory compression methods, directed graphs, Trajectory compression, road network, trajectory data processing, Trajectory, location-acquisition devices, moving object database, trajectory data]
A Complicated Task Solution Scheme Based on Node Cooperation for Wireless Sensor Networks
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Traditional task solution schemes in Wireless Sensor Networks (WSNs) are mainly not suitable for complicated task processing due to high energy consumption and long processing delay. In this paper, we proposed an energy efficient Complicated Task Solution scheme for real-time task processing based on node Cooperation (CTSC), which consists of two main phases: task grouping and task allocation. In the task grouping phase, complicated tasks are divided into different groups based on task graph. In the task allocation phase, based on node cooperation, different group tasks are allocated to different nodes by using bid invitation. Thus, multiple tasks can be processed in parallel, which ultimately reduces task processing delay and limits communication overheads. Simulation results show that CTSC is much more suitable for large scale WSNs. In addition, CTSC outperforms related works in terms of shorter response time of task processing and less energy consumption.
[Algorithm design and analysis, node cooperation, task graph, Energy consumption, complicated task solution scheme, wireless sensor networks, telecommunication power management, graph theory, task grouping, task solution, cooperative communication, Wireless sensor networks, task allocation, WSN, communication overheads, Energy efficiency, Real-time systems, Delays, Resource management, CTSC]
A Reliable Depth-Based Routing Protocol with Network Coding for Underwater Sensor Networks
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
With the rapid development of marine technology, underwater sensor networks (UWSNs) are gradually evolving from research to practice in recent years. Practicability and reliability are two major concerns for routing protocols in UWSNs. As localization is not necessary in depth-based routing protocol (DBR), it has an outstanding practicability than other geographic routing protocols. However, the reliability is not well ensured. In this paper, we propose an innovative depth-based routing with network coding improving routing reliability while preserving the intrinsic distributed manner of DBR and introducing little time delay and energy cost. Moreover, a simple analytical performance model where ideal MAC is assumed is proposed to derive the analytical delivery ratio for our DBR-NC and DBR protocols. This analytical model is validated by simulation results. The extensive simulation results show that the proposed DBR-NC protocol outperforms (over 15%) the state of art DBR protocols in terms of packet delivery ratio. We also show that our DBR-NC will not introduce much extra delay and energy consumptions.
[network coding, geographic routing protocols, wireless sensor networks, marine communication, Routing, Encoding, time delay, marine technology, routing reliability, underwater sensor networks, routing protocols, telecommunication network reliability, Network coding, energy cost, Routing protocols, reliable depth-based routing protocol, UWSN, DBR-NC protocols, energy consumptions, Reliability, Distributed Bragg reflectors]
Accomplishing Information Consistency under OSPF in General Networks
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
In this paper, we design an LAP based routing algorithm in General Networks (GN) to solve the problem of information consistency of the full network under OSPF with the following operations: (i) decomposing GN into one or more Single-link Networks (SNs) with the approach of depth-first walk, (ii) re-composting the SNs to a network with regular topology structure by adding links, (iii) searching the undirected complete graph of three nodes round by round until it converges to a simple network topology based on region binding, and (iv) processing different converged network topologies with different LAP based routing algorithms. The proposed algorithm is compared with Dijkstra algorithm over some random network topologies. Simulation results show that the proposed algorithm can solve the problem of information consistency of the full network under OSPF and has better performance than Dijkstra algorithm.
[Algorithm design and analysis, single-link networks, depth-first walk, undirected complete graph search, network theory (graphs), Routing, Topology, general networks, tree searching, LAP based routing algorithm, Network topology, regular topology structure, routing protocols, region binding, open shortest path first problem, Logic gates, limitation arrangement principle, Routing protocols, information consistency, converged network topologies, OSPF]
An Optimized RM Algorithm by Task Affinity on Multi-Core Processor
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Scheduling of real-time tasks on a multi-core processor is challenging due to the execution time being a nondeterministic value. Through studying the relationship between task affinity and execution time, we propose an accelerated multi-core real-time scheduling algorithm (RM-&#x03BB;) for periodic and dependent real-time tasks on a homogeneous multi-core processor based on acceleration between tasks to obtain a real-time scheduling scheme with less resource utilization. We adopt an acceleration factor matrix to represent the degree of affinity and develop a real-time scheduling model to find the best accelerated pair. The heterogeneous multi-core architectures can execute tasks by sharing their dependent data on L1 Cache. The results demonstrate our approach can loosens the schedulability constraints of RM (maximum improvement of 25%) so that an un-schedulable real-time tasks set on a single-core processor might be schedulable, and for those still hard to be scheduled, could be made schedulable on a multi-core processor.
[Homogeneous multi-core system, task affinity, L1 cache, multiprocessing systems, Multicore processing, RM algorithm, periodic real-time tasks, RM-&#x03BB;, Scheduling, homogeneous multicore processor, Periodic real-time task, accelerated multicore real-time scheduling algorithm, dependent real-time tasks, acceleration factor matrix, Real-time scheduling, FPGA Beehive, Scheduling algorithms, schedulability constraints, real-time systems, scheduling, Real-time systems, Acceleration, Resource management, heterogeneous multicore architectures]
An Optimized RPL Protocol for Wireless Sensor Networks
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
For the problems, such as low efficiency and serious energy loss of RPL routing protocol are caused since the RPL node's rank value calculation is mainly based on nodes hop, the objective function is not clearly, the choice of the optimal parent node is too simple and the node only saves a single available path, we design IRPL (Improved RPL) protocol to improve the original RPL. We propose LCI index based on life cycle as path selection objective function. The index takes various factors into consideration, for instance, link quality, node energy, energy consumption rate, throughput, data rate and congestion detection factor. According to node energy, hop and congestion detection factor, we optimize the calculation method of rank. Our method redesigns parent node selection strategy. Not only does the strategy select the best parent node by using the improved index, but also saves other parent nodes that meet the conditions. Meanwhile, we propose a multipath scheme by using the DODAG structure, and use the scheme to solve the congestion problem. The simulation results show that the scheme shows better performance in terms of network load, end-to-end delay, packet delivery ratio, percentage of the optimal parent node change and energy consumption.
[multi-path, Energy consumption, wireless sensor networks, network load, link quality, wireless sensor network, rank value calculation, data rate, Throughput, packet delivery ratio, multipath scheme, IRPL protocol, end-to-end delay, LCI index, Routing protocols, energy loss, life cycle, telecommunication power management, parent node selection strategy, Routing, energy consumption rate, node energy, Indexes, congestion detection factor, RPL protocol, optimal parent node change, path selection objective function, routing protocols, Delays, improved RPL protocol, RPL routing protocol, network lifetime]
FPGA-Based Parallel Implementation of SURF Algorithm
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
SURF (Speeded up robust features) detection is used extensively in object detection, tracking and matching. However, due to its high complexity, it is usually a challenge to perform such detection in real time on a general-purpose processor. This paper proposes a parallel computing algorithm for the fast computation of SURF, which is specially designed for FPGAs. By efficiently exploiting the advantages of the architecture of an FPGA, and by appropriately handling the inherent parallelism of the SURF computation, the proposed algorithm is able to significantly reduce the computation time. Our experimental results show that, for an image with a resolution of 640x480, the processing time for computing using SURF is only 0.047 seconds on an FPGA (XC6SLX150T, 66.7 MHz), which is 13 times faster than when performed on a typical i3-3240 CPU (with a 3.4 GHz main frequency) and 249 times faster than when performed on a traditional ARM system (CortexTM-A8, 1 GHz).
[Algorithm design and analysis, parallel algorithms, field programmable gate arrays, FPGA, frequency 66.7 MHz, time 0.047 s, object detection, FPGA-based parallel implementation, frequency 3.4 GHz, frequency 1 GHz, Interpolation, SURF, parallel computing algorithm, Parallel processing, Feature extraction, general-purpose processor, Hardware, Real-time systems, High Level Synthesis, SURF algorithm, parallel computing, Field programmable gate arrays, computational complexity]
Hierarchically Social-Aware Incentivized Caching for D2D Communications
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
The data caching in Device-to-Device (D2D) networks enables the quick data access in mobile networks. The D2D channels allows content sharing when two devices are in close proximity which can help improve resource utilization and network capacity. Due to the selfish nature of users, they wish to get as much replication as possible in the opportunistic connections, seeking to maximize their own profit. However, caching resources for other nodes may lead cost to the node who serves as cache. It lacks incentives for mobile nodes to cache for other peers in D2D network. In this paper, we use an incentive method to make mobile nodes cache for others and aim to minimize the total cost of getting object data in the network. The total cost is occurred by the cache placement of cache nodes and accessing cost of the other nodes. We consider the social ties and physical distance as the factors for the cost. We model the data cache problem as a socially-aware payment game, and we introduce a hierarchical caching scheme to incentive nodes to cache, which use the user relationship to construct the cost function. In order to model user relationship, we divide the network into three categories in perspective a node: self, friends and strangers. We obtain the Nash equilibrium of the game and propose a heuristic algorithm to solve the cache placement problem. The extensive simulation results show that our algorithm gain significant cache benefit.
[content sharing, Nash equilibrium, cache storage, Device-to-device communication, Electronic mail, cache placement problem, D2D communications, heuristic algorithm, device-to-device networks, data caching, D2D channels, Social relationship, Cost function, resource utilization, mobile nodes, incentive method, socially-aware payment game, mobile radio, hierarchical caching scheme, D2D communication, game theory, Mobile nodes, incentive schemes, social-aware incentivized caching, mobile networks, Game theory, Caching, Games, data access, Peer-to-peer computing, cache nodes, caching resources]
HMF: Heatmap and WiFi Fingerprint-Based Indoor Localization with Building Layout Consideration
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
In recent years, WiFi fingerprint-based localization has received much attention due to its deployment practicability. Although existing works show WiFi fingerprinting can achieve good localization accuracy, the experiments were conducted under their own testbeds within a small area and a short period. In this work, we investigate the impact of different indoor environmental factors, such as temporal and spatial similarity, on the performance of WiFi fingerprinting. We find that, WiFi fingerprinting is highly environment-sensitive. In an open space, it is quite challenging to find spatially varying but temporally stable signatures for adjacent reference locations. To address this issue, we propose a heatmap-based WiFi fingerprinting (called HMF) by utilizing layout construction as an additional input to improve WiFi fingerprint localization in open space environment. Our experimental results show, HMF can improve existing WiFi fingerprinting schemes like Radar and Horus by 28% and 80% in moderately open space, e.g., a wide corridor.
[indoor navigation, HMF, Buildings, Wi-Fi fingerprint based indoor localization, Indoor environments, heat map based fingerprinting, Fingerprint recognition, mobility management (mobile radio), Databases, indoor environmental, Layout, Heating, heatmap localization, building layout, radionavigation, indoor radio, wireless LAN, IEEE 802.11 Standard]
mtCloudSim: A Flow-Level Network Simulator for Multi-Tenant Cloud
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Currently, novel topologies and advanced resource allocation strategies for multi-tenant cloud datacenters are two research hotspots. Due to the high convenience and efficiency, researchers tend to use simulation to evaluate the proposed topologies or strategies. However, the current network simulators do not support the multi-tenant cloud environment inherently. Moreover, the low simulation speed and high memory consumption limit the traditional packet-level simulators to estimate the scenario of large-scale datacenters. In this paper, we propose a new flow-level network simulator, mtCloudSim, to overcome the above issues. The simulator estimates the data flow's behavior in the real world, i.e., 1) increasing the sending rate when the network is not busy and 2) suspending when the congestion occurs. Bandwidth isolation is inherently provided and users are allowed to define bandwidth requirement for the experiments with our simulator. Object-oriented programming (OOP) makes it easy to evaluate novel network topologies. The tracing system is also able to generate abundant and detailed statistics for experiments. The experiments demonstrate that mtCloudSim is available for multi-tenant cloud evaluation.
[Cloud computing, Bandwidth Guarantee, network simulators, memory consumption, bandwidth isolation, Network Evaluation, mtCloudSim, large-scale datacenters, Servers, Network Simulator, Multi-tenant Cloud, Network topology, resource allocation, Bandwidth, packet-level simulators, Bandwidth Allocation, multitenant cloud datacenters, cloud computing, object-oriented programming, Computational modeling, Object oriented modeling, flow-level network simulator, Topology, computer centres, OOP, network topologies]
Optimized Virtual Network Functions Migration for NFV
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Combining with software-defined networking and IT virtualization technologies, Network Function Virtualization (NFV) has been proposed as an important technology to speed up deployment of new network services. VNF (Virtual Network Function) migration is a critical step to redeploy virtual network functions for providing better network services. Previous work in virtual network function migration primarily focused on the migration mechanism, including maintaining internal state consistency and reducing migrating time. In this paper, we address the problem of optimally migrating virtual network functions. As the computing and network resource requirement of virtual network functions have been changed, these virtual network functions have to be migrated to meet the computing and network resource constraints. As the SDN controllers conducted technology is used to migrate the virtual network functions, the migration cost depends on the buffer size of controllers and the time of transferring the internal state of virtual network functions. A cost model is proposed to evaluate the migration cost. The problem of optimal virtual network functions migration with satisfying computing and network resource constraints is NP-hard. A heuristic algorithm is proposed for computing the approximate solution. The effectiveness of the algorithms is validated by simulations evaluation.
[IT virtualization technologies, network function virtualization, software-defined networking, Heuristic algorithms, Computational modeling, Migration, software defined networking, virtual network functions migration, Middleboxes, migration cost model, virtualisation, Optimization, heuristic algorithm, heuristic programming, NP-hard problem, virtual Network Function, network resource constraints, Approximation algorithms, Delays, Network function virtualization, SDN, computational complexity, NFV]
PROAR: A Weak Consistency Model for Ceph
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
The primary-copy consistency model used in Ceph cannot satisfy the low latency requirement of write operation required by users. In this paper, we propose a weak consistency model, PROAR, based on a distributed hash ring mechanism to allow clients to only commit data to the primary node and synchronize data to replication nodes asynchronously in Ceph. Based on the distributed hash ring mechanism, the low latency requirement of write operation can be met. In addition, the workload of the primary node can be reduced while that of replication nodes can be more balanced. We have evaluated the proposed scheme on a Ceph storage system with 3 storage nodes. The experimental results show that PROAR can reduce about 50% write overhead compared to that of Ceph and has a more balanced workload around all the replication nodes.
[object storage, Cloud computing, replication nodes, replicated databases, Computational modeling, write operation, Ceph, Ceph storage system, weak consistency, primary-copy consistency model, Electronic mail, Servers, storage management, PROAR, distributed hash ring mechanism, Semantics, Clustering algorithms, Distributed databases, cloud storage, weak consistency model, data synchronization, storage nodes]
TaxiCast: Efficient Broadcasting of Multimedia Advertisements in Vehicular Ad-Hoc Networks
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Traditional vision based vehicular advertising methods can only support planar advertisements. If there are obstacles blocking the line-of-sight between the advertiser and customers, the efficiency drops quickly. With the proliferation of vehicular ad-hoc networks (VANETs), advertisements can be disseminated through wireless means. However, the utility of advertisements still decay over time so that the advertiser will require fast delivery to achieve higher rewards. In this paper, we consider a taxi based multimedia advertisements broadcasting scenario where the taxis act as the advertising sources. To solve the contradiction between limited communication capacity and big data size, we propose TaxiCast which can achieve very good performance. It first applies signal strength based coding and decoding to obtain the demandings of the surrounding vehicles. Then it solves the advertisements selection process as a knapsack problem. We also consider the reward decay of advertisements and conflicts between the taxis. We conduct simulations in both fixed reward case and decayed reward case. The result shows that our scheme achieves better performance than the existing strategies.
[vehicular ad hoc networks, signal strength based coding, multimedia advertisement, knapsack problem, Vehicular Ad-hoc Networks, vehicular ad-hoc networks, vision based vehicular advertising, Ad hoc networks, Multimedia communication, encoding, Public transportation, decoding, Vehicles, broadcasting, Digital multimedia broadcasting, signal strength based decoding, TaxiCast, taxi based multimedia advertisements broadcasting, Advertising, multimedia communication]
Very Low-Resolution Face Recognition via Semi-Coupled Locality-Constrained Representation
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Recognition tasks in very low-resolution (VLR) images are more challenging than those in high-resolution (HR) due to lack of adequate discriminative information. Previous VLR and HR coupled learning scheme limits both the representation and discriminative ability of features. In this work, we propose a semi-coupled locality-constrained representation (SLR) approach to learn the discriminative representations and the mapping relationship between VLR and HR features simultaneously. Both VLR and HR local manifold geometries are coded during representation, while the learned mapping function improves the manifold consistency by transforming VLR features to HR ones. Finally, the resolutionrobust features are fed into a sparse representation based classifier (SRC) to predict the face labels. The proposed algorithm gives better performance than many state-of-the-art VLR recognition algorithms.
[Dictionaries, VLR images, SRC, Image reconstruction, mapping function, Manifolds, Training, high-resolution algorithms, Semi-coupled Locality-constrained Representation, manifold consistency, sparse representation based classifier, face recognition, Face, learning (artificial intelligence), image resolution, semi-coupled locality-constrained representation, Image resolution, Face recognition, Face Hallucination, HR local manifold geometries, coupled learning scheme, Face Recognition, very low-resolution face recognition, image representation, SLR approach, face labels, Very Low-Resolution]
Watch Traffic in the Sky: A Method for Path Selection in Packet Transmission between V2V from Macro Perspective
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Vehicle-to-Vehicle (V2V) communication is a vital component of vehicular ad-hoc networks (VANET) under the situation that infrastructure for vehicle-to-infrastructure (V2I) has not been well deployed due to its cost and suffering. However, messages transmission path is so challenge to be found without infrastructures supporting that packets are inevitably spread in a sparsely or competitive area, in which they should avoid being trapped because of its poor communication links between vehicles, which eventually lowers down the performance of messages propagation. In this paper, we analyze the relationship between vehicular geographical distribution and packets propagation of VANET in a realistic large-scale urban scenario. It is demonstrated that, from a macro perspective, we could guide the path selection of data propagation between source and destination through V2V communication on the basis of the feature of vehicle density in different geographic locations. Furthermore, to be further close to the actual traffic environment, we model the vehicular geographical distribution by four typical real scenes to present the real environment and develop appropriate messages propagation strategies respectively.
[vehicular ad hoc networks, Protocols, vehicle-to-vehicle communication, messages transmission path, vehicular ad-hoc networks, macro perspective, VANET, Routing, path selection, Automobiles, Relays, messages propagation, packets propagation, vehicle density, Vehicular ad hoc networks, vehicular geographical distribution, Delays, V2V, vehicle-to-infrastructure, telecommunication traffic, packet transmission, geographic locations]
A Provably Secure Blind Signature Based on Coding Theory
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Blind signature can be deployed to preserve user anonymity and is widely used in digital cash and e-voting. As an interactive protocol, blind signature schemes require high efficiency. In this paper, we propose a code-based blind signature scheme with high efficiency as it can produce a valid signature without many loops unlike existing code-based signature schemes. We then prove the security of our scheme in the random oracle model and analyze the efficiency of our scheme. Since a code-based signature scheme is post-quantum cryptography, therefore, the scheme is also able to resist quantum attacks.
[code-based scheme, provably secure blind signature, signature schemes, post-quantum cryptography, provably secure, blind signature, digital cash, Decoding, Electronic mail, encoding, quantum attacks, coding theory, e-voting, FSB hash, Public key, quantum cryptography, Games, random oracle model, digital signatures, interactive protocol]
A Users Collaborative Scheme for Location and Query Privacy
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
In location based services (LBSs) of snapshot query, both of query and location privacy should be preserved. Most preceding schemes usually employed a trusted third party (TTP) to achieve k-anonymity, and perturb the real query and location with other k-1 users, but this scheme may be influenced by the single point failure and performance bottleneck. In order to solve this problem, another scheme to achieve k-anonymity with collaborative users through short-range communication was proposed. After deeply analyzing the inherent drawbacks of existing schemes of this type, we have found that most of them mainly focus the location privacy and usually neglect the correlation between query and location. In order to preserve the query privacy, based on the user collaborative scheme, we propose a scheme with query information divide and exchange with random users (short for QDER). This scheme can provide preserving service for both query and location privacy simultaneously. Then we analyze the attack from passive and active adversaries and use entropy to measure the privacy level. Finally, security analysis and experimental evaluations further verify the effectiveness and efficiency.
[security analysis, Roads, query privacy, QDER, Entropy, Servers, Security, query and location privacy, collaborative users, query processing, Privacy, entropy, location privacy, Collaboration, query block divide, Resists, data privacy, location-based services, users collaborative scheme, query information divide and exchange with random users]
An Approach of Anti-Eavesdropping Linear Network Coding in Wireless Network
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
The existing anti-eavesdropping researches on wireless network coding are mainly based on the assumption that the eavesdroppers can only monitor limited channels and don't cooperate with each other. But in real settings, the eavesdroppers share overheard information, which makes data leakage come true. Moreover, these approaches just encrypt or permutate the coding coefficients to prevent the adversaries from understanding the transmission data, they can't resist pollution attacks, such as forgery and tamper attacks. In this case, we propose a novel secure linear network coding scheme that is based on IBC(Identity-Based Cryptography) algorithm and has the characteristic of preventing eavesdropping and pollution attacks. Theoretical analysis and experiment demonstrate that our scheme guarantees the data security for each node, which significantly prevents the nodes from being eavesdropped and resists pollution attacks. Furthermore, our scheme enriches the approaches of anti-eavesdropping research in wireless network coding.
[telecommunication security, network coding, Linear Network Coding, tamper attacks, data security, cryptography, Encoding, identity-based cryptography algorithm, Encryption, Security, IBC Algorithm, wireless network coding, pollution attacks, Pollution, forgery, Public key, anti-eavesdropping linear network coding, Eavesdrop, Network coding]
Automatic Security Bug Classification: A Compile-Time Approach
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Program security bugs pose a great threat to users' privacy and security. A great deal of effort, e.g., runtime defense, dynamic detection, and static detection, has been conducted to attempt to be aware of the existence of security bugs. Most of the prior work focuses on detecting the security bugs. They report a mixed set of security bugs, regardless of whether the elements in the set are useful to the developers for the debugging. In this paper, we are instead devoted to automatically classifying the security bugs for the purpose of the productivity to the developers. Our insight is that the existing common security bugs can be featured by a simple rule that can be further simplified into a mathematical assertion problem. Based on this insight, we propose a Compile Time Error Segregator (CTES), which can automatically classify the security bugs into three categories, including deterministic bugs, internal indeterministic bugs, and external indeterministic bugs. The core idea of achieving the above includes three steps: 1) building a rule library according to the feature of each type of security bugs (e.g., buffer overflow, null-pointer dereference, and divide-by-zero), 2) obtaining the requisite information appearing in the rule, 3) verifying if the rule is established. If so, a deterministic bug is found, otherwise, a novel inverse taint analysis is further performed to distinguish the remaining two categories. We implement CTES on top of LLVM (3.5.0), running in parallel with normal compile procedure. Our experimental results on micro-benchmark and 14 real programs demonstrate the efficiency of CTES, and also show that CTES is able to precisely make the reported security bugs well-classified into three-categories.
[Productivity, CTES, pattern classification, program debugging, internal indeterministic bugs, mathematical assertion problem, external indeterministic bugs, parallelism, Security, program compilers, compile time error segregator, bug classification, Runtime, security, security of data, LLVM, Computer bugs, Semantics, automatic security bug classification, Syntactics, Libraries, compile time, rule library, rule matching]
Baseline Is Fragile: On the Effectiveness of Stack Pivot Defense
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Return-Oriented Programming (ROP) has become a widespread technique in recent software exploits. Various defenses have been proposed to thwart ROP, including randomization, Control-Flow Integrity (CFI), etc. However, ROP attacks have not been eliminated completely yet. Recently, ROP defenses based on stack pivot detection are put forward. In this paper, we investigate the checking mechanism in existing stack pivot defenses, including ROPGuard, Microsoft EMET, PBlocker and a detecting device design. They check validity of stack pointer with stack boundary information stored in system structure, e.g., Thread Information Block (TIB) in Windows. These stack pivot checkers are effective to detect ROP attacks on the premise that the baseline is safely stored. However, we find this assumption is unreliable because users have read-write access to TIB structure, which means stack range information can be tampered in user mode by an attacker, while existing solutions don't mention how to protect these baseline data. In this paper, we propose an attack method to bypass stack pivot checks through corrupting stack border value in TIB and prove that our attack can overcome current solutions indeed through case studies. Further, we discuss possible countermeasures to enhance security of current stack pivot defenses.
[Computers, stack pivot defense, PBlocker, Aerospace electronics, Registers, Code reuse attack, Security, thread information block, return-oriented programming, stack pivot checkers, stack pivot detection, Detectors, stack border value, Enhanced Mitigation Experience Toolkit, Stack pivot, TIB, stack pointer, Return-Oriented Programming, Microsoft EMET, security of data, detecting device design, Software, ROPGuard, Payloads, Thread Information Block]
CC-Paxos: Integrating Consistency and Reliability in Wide-Area Storage Systems
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Data replication is widely used in geo-distributed storage systems, and strong consistency is preferred for correctness and programming simplicity at the application layer. To address the inefficiency and insufficiency of the causal consistency model, a strong consistency model named distributed context consistency is defined. It explicitly defines the necessary dependencies among distributed clients to effectively reduce falsepositive dependencies among operations. A consensus algorithm named CC-Paxos is proposed to implement this distributed context consistency model. It exploits timestamps for operation sequencing in distributed contexts and adopts fine-granularity dependency checking to effectively reduce the number of potential conflicts. Experimental results show that, compared with implementations using causal+ consistency model in the upper layer and Egalitarian Paxos in system layer, CC-Paxos can significantly decreases latency and increases throughput with no sacrifice on scalability.
[Context, Algorithm design and analysis, wide area storage systems, Distributed Context, distributed context consistency, software reliability, Programming, distributed processing, falsepositive dependencies, Throughput, Consensus, Sequential analysis, CC-Paxos, data replication, Distributed Context Consistency, data handling, integrating consistency, geodistributed storage systems, distributed context consistency model, Reliability, cloud computing, Context modeling, Egalitarian Paxos]
Multi-User Location Correlation Protection with Differential Privacy
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
In the big data era, with the rapid development of location-based applications, GPS enabled devices and big data institutions, location correlation privacy raises more and more people's concern. Because adversaries may combine location correlations with their background knowledge to guess users' privacy, such correlation should be protected to preserve users' privacy. In order to deal with the location disclosure problem, location perturbation and generalization have been proposed. However, most proposed approaches depend on syntactic privacy models without rigorous privacy guarantee. Furthermore, many approaches only consider perturbing the locations of one user without considering multi-user location correlations, so these techniques cannot prevent various inference attacks well. Currently, differential privacy has been regarded as a standard for privacy protection, but there are new challenges for applying differential privacy in the location correlations protection. The privacy protection not only should meet the needs of users who request location-based services, but also should protect location correlation among multiple users. In this paper, we propose a systematic solution to protect location correlations privacy among multiple users with rigorous privacy guarantee. First of all, we propose a novel definition, private candidate sets which are obtained by hidden Markov models. Then, we quantify the location correlation between two users by using the similarity of hidden Markov models. Finally, we present a private trajectory releasing mechanism which can preserve the location correlations among users who move under hidden Markov models in a period of time. Experiments on real-world datasets also show that multi-user location correlation protection is efficient.
[Data privacy, Correlation, the similarity of hidden Markov models, private trajectory releasing mechanism, Probability distribution, private trajectory releasing, location correlation, Privacy, hidden Markov models, multiuser location correlation protection, privacy protection, Hidden Markov models, data protection, Big data, location-based services, Trajectory, differential privacy]
Opportunistic Probe: An Efficient Adaptive Detection Model for Collaborative Intrusion Detection
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
The number of network intrusions, such as large-scale stealthy scans, worms, and distributed denial-of-service (DDoS) attacks, has significantly increased. Collaborative intrusion detection system (CIDS) becomes an essential part for analyzing multiple network security simultaneously. The trust-based packet filter method using Bayesian inference tries to decrease the processing burden, but overhead network packets make that performance and accuracy are still open issues. In this paper, we propose an Opportunistic Probe model, which is a transport entity that carries encrypted characteristic attributes from trusted host to the checking host. A Detection Time Optimization Algorithm is proposed to determine the trusted period of hosts during which the unnecessary detection can be reduced. The case study and experimental analysis demonstrates the effectiveness, scalability and robustness of the proposed approach.
[Adaptation models, Adaptive systems, transport entity, cryptography, CIDS, Probe, Computer crime, Trusted, Optimization, opportunistic probe model, collaborative intrusion detection system, Opportunistic, encrypted characteristic attributes, detection time optimization algorithm, IP networks, Cryptography, adaptive detection model, Probes]
Outsourcing Large-Scale Systems of Linear Matrix Equations in Cloud Computing
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
With the increasing development of cloud computing, how to securely outsource prohibitively expensive computation to unfaithful clouds has caught more and more attention. Large-scale systems of linear matrix equations (LME) are commonly deployed in scientific and engineering fields, which is a computationally complex task. Thus, it is necessary to design a protocol for practically outsourcing large-scale systems of LME to a malicious cloud. For this purpose, we propose a protocol called OutLME in this paper. In OutLME, we adopt a special permutation technique for the client to transform the original linear matrix equation into a randomized one and decrypt the result returned from cloud into the right one of original problem. As to robust cheating resistance, we propose an effective verification algorithm by utilizing the algebraic property of matrix-vector operations. In addition, both the chosen permutation technique and result verification mechanism incur close-to-zero additional cost on both the cloud and the client, so our proposed protocol is efficient. In the end, the theoretical analysis and the experimental evaluation are provided to demonstrate the validity of OutLME.
[Cloud computing, Protocols, algebraic property, OutLME, Computational modeling, LME, permutation technique, linear matrix equations, vectors, outsourcing large-scale systems, protocol, linear matrix inequalities, outsourcing, scientific fields, engineering fields, verification algorithm, Outsourcing, Large-scale systems, Mathematical model, Cryptography, cloud computing, matrix-vector operations]
RuleCache: A Mobility Pattern Based Multi-Level Cache Approach for Location Privacy Protection
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
With widespread pervasion of Location-Based Service (LBS), location privacy has attracted more and more attention. In the traditional LBSs model, users need to repeatedly send their location information to the provider's servers or an intermediate Anonymizing Server (AS) for service, which has a lot of disadvantages, such as the low reutilization ratio of the information, the high threat of un-trusted service providers and the excessive reliance on AS. The latest cache method gives a new idea, but it does not take users' mobile behavior into account. To address these issues, this paper presents a multi-level location privacy protection method, RuleCache, which combines the users' mobility patterns with cache and utilize the cache content of distributed neighbors to protect location privacy. Moreover, when the user has to send query to the LBS server, we propose a Cloaking Region Generating Algorithm (CRGA) to achieve protection which considers the factor of query probability and data timeliness to increase cache contribution rate and update outdated data in time, respectively. We carry out extensive simulation and evaluation show that our RuleCache have a higher performance than many other methods on communication cost, cache hit ratio and storage cost.
[Algorithm design and analysis, reutilization ratio, cloaking region generating algorithm, location information, Companies, untrusted service provider threat, Mobile communication, cache storage, mobility management (mobile radio), multilevel location privacy protection method, Servers, Security, cache hit ratio, location-based service, cache contribution rate, Location-Based Service, query processing, Privacy, mobile computing, cache content utilization, location privacy, CRGA, data protection, mobility pattern, LBS server, cache, probability, RuleCache, query probability, multilevel cache approach, LBS model, Collaboration, k-anonymity]
Towards a Framework to Facilitate the Mobile Advertising Ecosystem
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
To date, app developers are allowed to monetize their apps in two services: in-app advertising and in-app billing. Of these two, in-app billing is not prevalently used by users, whereas in-app advertising is considered an important funding source for developers. However, this service incurs a number of criticisms: (1) users must passively receive all mobile ads while using apps, (2) users get nothing from viewing or clicking ads, (3) ad networks transfer user private information to remote servers in an unencrypted format without user consent, and (4) negative impressions brought from irrelevant ads may harm the advertised brands. To overcome these problems, we propose In-App AdPay, a framework that combines the advantages of "in-app advertising" and "in-app billing" together so that ad networks can overtly ask users' permissions in order to serve more tailored ads, but in return, advertisers will pay targeted users' virtual transactions within the app (e.g., coins in mobile games) via a secure channel. While mobile users can be brought into the monetization loop, it will be technically and legitimately easier for ad networks to study users. We implemented the proof-of-concept framework and conducted a test with 42 volunteers. Based on these studies, we believe that "In-App AdPay" would balance user privacy and user experience without interfering with the existing monetization arrangements. Lastly, we reveal how tracked-by-consent users react in different test scenarios and value the permissions used in ad libraries.
[remote servers, Ecosystems, mobile advertising ecosystem, in-app billing, app advertising, Mobile Advertising, Mobile communication, advertising, ecology, mobile computing, Usable Privacy, Usability Testing &amp; Surveys, monetization loop, Libraries, inapp advertising, Advertising, proof-of-concept framework, app developers, Google, advertised brands, User Interface, Media, unencrypted format, virtual transactions, secure channel, Unpaired Two-Sample T-Test, user consent, ad networks transfer, Mobile computing]
A Novel Method of Keyword Query for RDF Data Based on Bipartite Graph
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
As huge amounts of the semantic Web data have sprung up, RDF data query becomes an important research topic. RDF data based on graph structure can keep correlation information and semantic information, so more and more keywords query methods model RDF data as RDF graph. But current query techniques on graph suffer from several drawbacks, like low precision, high query response time, high parallel implementation cost, and so forth. To address these problems, a novel method of keyword query for RDF data based on bipartite graph is proposed. Specifically, we first construct RDF data as bipartite graph with node labels in which all text information is encapsulated to support relationship query. And then we design a keyword expansion query algorithm which includes keywords expansion, keyword matching, and the construction of query result subgraphs. Moreover, the keyword expansion technology effectively solves the problem of delivering the same object description words and also improves the query precision. Finally, in experiments using large real-world dataset, our solution outperformed the state-of-the-art in terms of precision and query response time.
[keyword matching, Correlation, large-real-world dataset, graph theory, keyword expansion query algorithm, RDF Data, object description words, Resource description framework, Keyword Query, keyword query method, query processing, precision value, synonym expand, Bipartite graph, node labels, correlation information, query precision improvement, Symmetric matrices, semantic Web, RDF graph, Indexes, bipartite graph, RDF data query, text information, query result subgraph construction, query response time, Data models, relationship query, Time factors, graph structure, semantic information, Bipartite Graph, Anti-symmetric adjacency matrix]
A Unified Access Manner for Storage-Class Memory
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Persistent memory has the potential to become universal storage for memory and storage uses. Unfortunately, our current programming model is still only geared to a two-level storage model supporting different semantics to access volatile memory and non-volatile hard disk. This, however, leads a performance gap in accessing the same persistent memory stemming from the difference in the aforementioned semantics and the performance degradation introduced by not appropriately adapting to characteristics of SCM. We propose a general-purpose SCM-access methodology for persistent data on universal storage to hide such semantics difference. Specifically, we design the Persistent Memory Server Engine (PMSE), a special user-level process based on a unified optimization and transactional mechanism, that provides a group of general calls related to allocation, optimization, transactions of persistent memory and a fine-grained data abstraction for persisting data. Our prototype evaluation of PMSE shows that, compared to several state-of-the-art solutions at the file-system and persistent-heap levels, PMSE can provide 4&#x00D7; better performance than kernel-based persistent-memory file system, 1.7&#x00D7; and 1.5&#x00D7; better performance than user-level persistent-memory file system and persistent heap, under the write-intensive workloads. For traditional applications, applications with IOs achieve similar performance to those without IOs.
[Random access memory, Metadata, persistent memory allocation, input-output programs, persistent memory transaction, Storage-Class Memory, File systems, Semantics, transactional mechanism, general-purpose SCM-access methodology, Persistent Memory, persistent memory optimization, DRAM chips, user-level process, fine-grained data abstraction, file-system levels, persistent memory server engine, volatile memory, Data structures, persistent memory stemming, unified optimization, PMSE, storage-class memory, Memory management, write-intensive workloads, two-level storage model, nonvolatile hard disk, persistent-heap levels]
Application-Aware and Software-Defined SSD Scheme for Tencent Large-Scale Storage System
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Tencent, one of the biggest Internet companies in China, contains billions of users and over 600-PB data, and leverages thousands of SSDs in the storage system to improve system performance and obtain energy savings. Existing commercial SSDs however fail to meet the needs of the ultra largescale applications due to not matching the service patterns. In order to address this problem and deliver high performance, we propose an application-aware and software-defined SSD scheme for Tencent applications, called TSSD. TSSD explores and exploits the business characteristics of Tencent, which facilitates the efficient use of SSDs. TSSD is software-defined by packaging each flash chip as a fully independent and concurrent storage unit. Each concurrent unit can be mounted as a character device, which allows the application layer to manage the flash chips in a more efficient manner, while optimizing the data layout. TSSD further employs a host-target FTL (TFTL) that uses a dedicated interface in the application layer, which efficiently connects the application layer with flash chips. Application layer hence becomes more accurately by using the flash memory chip-level information from TFTL, including the storage utilization, the degree of wear, etc. Moreover, TFTL is a programmable FTL and provides a programmable interface to the application layer. According to the running states of SSDs and workload information, TSSD makes use of the programmable interface to efficiently improve the performance of the FTL, wear leveling, and garbage collection for the specified applications. Extensive experiments use the real-world datasets from the commercial storage systems of Tencent. The results demonstrate that TSSD significantly improves the storage system performance and meets the needs of the Tencent's large-scale business applications.
[Performance evaluation, workload information, character device, wear leveling, Companies, flash memory chip-level information, energy savings, storage system performance improvement, garbage collection, real-world datasets, TFTL, performance improvement, storage management, programmable FTL, flash memories, application-aware software-defined SSD scheme, China, Bandwidth, business characteristics, Software-Defined, Tencent large-scale storage system, wear degree, programmable interface, data layout optimization, host-target FTL, storage utilization, Memory management, TSSD, application layer, solid-state drive, Application-Aware, Internet, Time factors, concurrent storage unit, Solid-State Drive (SSD), business data processing, FTL]
Catenae: Low Latency Transactions across Multiple Data Centers
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Serving requests with low latency while data are replicated and maintained consistently across large geographical areas, e.g. in multiple data centers (DCs), is challenging. We propose Catenae, a transaction framework that provides serializable transaction support for data replicated in multiple DCs. Catenae leverages periodic replicated epoch messages to reduce synchronization delay among DCs, which results in the reduction of commit latency of transactions. It employs and extends a transaction chain concurrency control algorithm to speculatively execute transactions in DCs with maximized execution concurrency and determinism of transaction ordering. As a result, Catenae is able to commit a transaction with half a RTT to a single RTT across DCs in most of the cases. Evaluations with TPC-C benchmark have shown that Catenae significantly outperforms Paxos Commit over 2-Phase Lock and Optimistic Concurrency Control. Catenae doubles the throughput and halves the commit latency comparing to the both approaches.
[transaction processing, Protocols, data centers, transaction chain concurrency control algorithm, commit latency reduction, Throughput, Distributed Storage System, Concurrent computing, synchronization delay reduction, RTT, Catenae, low latency transactions, Geo-replicated, Geo-distributed, Concurrency control, Partitioning algorithms, Synchronization, computer centres, transaction ordering, 2PL, OCC, periodic replicated epoch messages, Transaction, concurrency control, TPC-C, Delays, Paxos Commit, Speculative Transaction, DC]
Checking the Inconsistent Data in Concurrent Systems by Petri Nets with Data Operations
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
The general Petri nets are not suitable to model the data operations of concurrent read and coverable write. Therefore, Petri net with data operations (PN-DO) is defined, which extends contextual nets with write arcs and some other components. Its execution semantics are defined, and a new method is proposed to construct its reachability graph that is of a smaller scale than traditional reachability graph. Based on this kind of reachability graph, we propose a method to check the errors of inconsistent data and missing data. Meanwhile, case studies are given to illustrate the effectiveness of our methods.
[data operations, reachability analysis, inconsistent data checking, contextual nets, Petri nets, concurrent system, execution semantics, programming language semantics, reachability graph, Concurrent computing, Petri net with data operations, PN-DO, Semantics, Distributed databases, concurrent systems, data-flow errors, write arcs, Writing, Data models, Mathematical model, Petri net]
CloudBB: Scalable I/O Accelerator for Shared Cloud Storage
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Current shared cloud storage cannot provide sufficient I/O throughput for data-intensive HPC applications. Moreover, the consistency policy used in most shared cloud storage can cause parallel I/O applications to fail due to unexpected file inconsistencies. In order to resolve these problems, we propose a novel fast, scalable and fault tolerant filesystem called CloudBB (Cloud-based Burst Buffer). Unlike conventional filesystems, CloudBB creates an on-demand two-level hierarchical storage system and caches popular files to accelerate I/O performance. Since CloudBB supports multiple metadata servers, CloudBB is also highly scalable. In addition, by using file replication, failure detection and recovery techniques, CloudBB is resilient to failures. Furthermore, we implement CloudBB by using FUSE so that existing applications can run seamlessly and benefit from all of the CloudBB's capabilities without code modification. To validate the effectiveness of CloudBB, we evaluate performance of real data-intensive HPC applications in Amazon EC2/S3. The results show CloudBB improves performance by up to 28.7 times while reducing cost by up to 94.7% compared to the ones without CloudBB.
[FUSE, Cloud computing, data-intensive applications, fault tolerant filesystem, Buffer storage, Metadata, Throughput, cache storage, CloudBB, performance improvement, Computer architecture, cloud computing, Amazon EC2/S3, failure recovery technique, data-intensive HPC applications, cost reduction, meta data, replicated databases, metadata servers, burst buffer, on-demand two-level hierarchical storage system, file replication, Ions, I/O performance, scalable I/O accelerator, failure detection technique, cloud-based burst buffer, shared cloud storage, file caching, Acceleration]
DEC: An Efficient Deduplication-Enhanced Compression Approach
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Data compression is widely used in storage systems to reduce redundant data and thus save storage space. One challenge facing the traditional compression approaches is the limitation of compression windows size, which fails to reduce redundancy globally. In this paper, we present DEC, a Deduplication-Enhanced Compression approach that effectively combines deduplication and traditional compressors to increase compression ratio and efficiency. Specifically, we make full use of deduplication to (1) accelerate data reduction by fast but global deduplication and (2) exploit data locality to compress similar chunks by clustering the data chunks which are adjacent to the same duplicate chunks. Our experimental results of a DEC prototype based on real-world datasets show that DEC increases the compression ratio by 20% to 71% and speeds up the compression throughput by 17%~183% compared to traditional compressors, without sacrificing the decompression throughput by leveraging deduplication in traditional compression approaches.
[data compression, deduplication-enhanced compression approach, DEC, Scalability, Redundancy, Throughput, Compressors, Storage Systems, compression ratio, data reduction, Delta Deduplication, pattern clustering, data locality, Traditional Compression, Prototypes, data chunk clustering, Indexing, Data Locality]
Exploiting the Data Redundancy Locality to Improve the Performance of Deduplication-Based Storage Systems
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
The chunk-lookup disk bottleneck and the read amplification problems are two great challenges for deduplication-based storage systems and restrict the applicability of data deduplication for large-scale data volumes. Previous studies and our experimental evaluations have shown that the amount of redundant data shared among different types of applications is negligible. Based on the observations, we propose AA-Plus which effectively groups the hash index of the same application together and divides the whole hash index into different groups based on the application types. Moreover, it groups the data chunks of the same application together on the disks. The extensive trace-driven experiments conducted on our lightweight prototype implementation of AA-Plus show that compared with AA-Dedupe, AA-Plus significantly speeds up the write throughput by a factor of up to 6.9 and with an average of 3.1, and speeds up the read throughput by a factor of up to 3.3 and with an average of 1.9.
[Data Deduplication, Redundancy Locality, Redundancy, AA-Plus, Throughput, hash index, Virtual machining, Indexes, Storage Systems, storage management, Performance Evaluation, write throughput, Bandwidth, deduplication-based storage systems, data redundancy locality, data chunks, AA-Dedupe, Explosives, Acceleration]
High Throughput Log-Based Replication for Many Small In-Memory Objects
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Online graph analytics and large-scale interactive applications such as social media networks require low-latency data access to billions of small data objects. These applications have mostly irregular access patterns making caching insufficient. Hence, more and more distributed in-memory systems are proposed keeping all data always in memory. These in-memory systems are typically not optimized for the sheer amount of small data objects, which demands new concepts regarding the local and global data management and also the fault-tolerance mechanisms required to mask node failures and power outages. In this paper we propose a novel two-level logging architecture with backup-side version control enabling parallel recovery of in-memory objects after node failures. The presented fault-tolerance approach provides high throughput and minimal memory overhead when working with many small objects. We also present a highly concurrent log cleaning approach to keep logs compact. All proposed concepts have been implemented within the DXRAM system and have been evaluated using two benchmarks: The Yahoo! Cloud Serving Benchmark and RAMCloud's Log Cleaner benchmark. The experiments show that our proposed approach has less memory overhead and outperforms state-of-the-art in-memory systems for the target application domains, including RAMCloud, Redis, and Aerospike.
[low-latency data access, Cloud computing, in-memory objects, online graph analytics, graph theory, Random access memory, Throughput, small data objects, Graph-based database models, cache storage, Fault tolerance, Main memory, fault-tolerance mechanisms, Aerospike, Distributed databases, DRAM chips, cloud computing, global data management, Remote replication, Buffering, Java, RAMCloud Log Cleaner benchmark, replicated databases, information retrieval, Yahoo! Cloud Serving Benchmark, access patterns, Redis, concurrent log cleaning approach, high throughput log-based replication, configuration management, Secondary storage, Data centers, Memory management, Flash memory, distributed in-memory systems, distributed memory systems, DXRAM system, B-trees, two-level logging architecture, local data management, fault tolerant computing, Peer-to-peer computing, benchmark testing, backup-side version control, Reliability, large-scale interactive applications, node failures, parallel recovery]
Improving MLC Flash Performance with Workload-Aware Differentiated ECC
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
The adoption of small geometries and multi-level cell (MLC) technologies significantly expands the capacity and drops the price of flash memory, which, at the same time, noticeably degrades the performance and reliability of the devices. As incremental-step pulse programming (ISPP) scheme is used to increase the programming accuracy for MLC cells, there is a trade-off between the SSD write performance and raw storage reliability. What's more, ECC is widely used in SSDs to provide error-tolerance ability. Therefore, if we could use stronger ECC to increase error correction strength, a low-cost write with coarser step sizes could be applied in the ISPP scheme to promote the write performance. However, stronger ECC scheme may hurt the read performance due to the increased decoding complexity and latency. In this paper, we propose a workload-aware differentiated ECC scheme to improve the SSD write performance without sacrificing the read performance. The main idea is to dynamically classify the logical pages into three categories: write-only, readonly, and overlapped part. For write-only logical pages, low-cost write with strong ECC scheme will be applied to increase the write performance. For write logical pages in the overlapped part, the low-cost writes with strong ECC will be selectively used based on their relative write and read hotness. While for any read logical pages encoded with a stronger ECC, we will rewrite them with the normal-cost write and ECC scheme if their hotness exceed a pre-defined threshold. The evaluation results show that our workload-aware differentiated ECC scheme could reduce the write and read response times by 48% and 11% on average, respectively. Even compared with the latest previous work, our workload-aware design can still gain about 4% write performance and 11% read performance improvements.
[Performance evaluation, solid state disk, decoding complexity, integrated circuit reliability, raw storage reliability, Programming, ISPP, LDPC, MLC flash performance improvement, flash memories, incremental-step pulse programming, device reliability, Bloom Filter, overlapped logical pages, Parity check codes, Flash Performance, error correction codes, workload-aware differentiated ECC, Particle separators, programming accuracy, BCH, performance evaluation, flash memory, Decoding, read-only logical pages, decoding, error correction strength, write-only logical pages, error-tolerance ability, ISPP scheme, SSD write performance improvement, multilevel cell technologies, Threshold voltage, Error correction codes, computational complexity]
Improving Write Performance of LSMT-Based Key-Value Store
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Key-value stores are widely used to provide much higher read and write throughput than traditional SQL databases. LSMT (log structure merge tree) based key-value store, as one type of key-value stores, is applied in many practical systems since it could eliminate random writes and provide good read performance at the same time. However, the data residing in disk needs compaction operations from time to time, which takes a large amount of I/O resources. Since disk access speed is much slower than DRAM and most data resides in disks, the compaction operation will significantly influence the system performance. In this paper, we propose a grouped level structure, which divides each level in LSMT into multiple groups. Also, we propose a new compaction method for the grouped level structure to reduce the compaction I/O overhead. Our experiments show that the grouped level structure saves about 55% to 78% I/O resource of compaction, so it improves the write throughput by 69% to 284%, but only reduces the read throughput by 5% to 9%. It improves the overall throughput by 30% to 69% with read dominated workloads of 25% write operations and 75% read operations.
[log structure merge tree, LSMT-based key-value store, Throughput, Compaction, Key-Value, Indexes, Micromechanical devices, High performance computing, Layout, Log Structured Merge Tree, grouped level structure, compaction I/O overhead, data structures, write performance]
Lifting Wavelet Compression Based Data Aggregation in Big Data Wireless Sensor Networks
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
The redundancy of sensing data in wireless sensor networks (WSNs) gives rise to longer transmission delays and more energy consumption. In this paper, we focus on the energy-efficient data redundancy elimination and compression with the objective of recovering the original data. To balance aggregation load of a large-scale WSN, we propose a novel energy-efficient dynamic clustering algorithm by utilizing spatial correlation, which can achieve a distributed compressive data aggregation in each cluster head. Furthermore, we propose a distributed fast data compression approach based on eliminable lifting wavelet to reduce the amount of raw data. Also, it offers high fidelity recovery for the raw data. Extensive experimental results demonstrate that our clustering method based on data correlation clustering (CDSC) for data aggregation outperforms other methods on prolonging network lifetime and reducing the amount of data transmitted. In particular, our data compression aggregation algorithm can achieve 98.4% recovery accuracy when the compression ratio equals 1.3333.
[Correlation, compressed sensing, wireless sensor networks, large-scale WSN, energy-efficient dynamic clustering, wavelet transforms, Data compression, Big data wireless sensor networks, distributed compressive data aggregation, data aggregation, energy-efficient data compression, Compressive sensing, Clustering algorithms, Distributed databases, energy consumption, lifting wavelet compression based data aggregation, Spatial data correlation clustering, data compression, Big Data wireless sensor networks, Redundancy, Data aggregation, Big Data, spatial correlation, clustering method based on data correlation clustering, CDSC, Wavelet data aggregation, Wireless sensor networks, pattern clustering, data sensing redundancy, energy-efficient data redundancy elimination]
masFS: File System Based on Memory and SSD in Compute Nodes for High Performance Computers
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Hard Disk Drive (HDD) based storage systems in high performance computing (HPC), such as Lustre, are being confronted with great challenges from massive parallel data-intensive applications, and becomes one of the most critical factors that influence applications performance. In this paper, we analyze the performance of Lustre, and statistically examine the distribution of file sizes, I/O patterns for typical data-intensive applications, as well as the utilization of CPU and memory resources in TH-1A supercomputer from the last five years. We present masFS, a novel file system for HPC that exploits available memory and SSD resources on compute nodes with little interference to applications running on the nodes. The masFS supports POSIX interface with an unique namespace and provides comprehensive file service for applications. We design and implement masFS with memory and SSD, and have deployed and evaluated it on TH-1A. Experimental results show that masFS works as general storage system and can meet the requirement of variety of I/O loads. It can achieve a speedup of 14.5x for read and 8.8x for write respectively in running benchmarks, and an average 7.6x faster I/O time in a real-world data-intensive application compared to Lustre.
[masFS, Fuses, HDD based storage systems, High Performance Computing, general storage system, Metadata, file system, Servers, parallel machines, hard discs, I-O patterns, File systems, Data Migration, Dynamic Memory Management, Bandwidth, Computer architecture, POSIX interface, memory resources, mainframes, TH-1A supercomputer, random-access storage, real-world data-intensive application, SSD, SSD resources, hard disk drive, Supercomputers, Lustre, lile sizes, Remote Memory Storage, File System, high performance computers, namespace, compute nodes, disc drives]
NVMCFS: Complex File System for Hybrid NVM
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Due to the price limitation and the number of DIMM slot, Byte and Block addressable NVM devices should coexist in the massive Storage Class Memory(SCM). But they have many differences such as interface, access granularity, I/O performance and storage capacity. Therefore, the existing main memory and file system management algorithms cannot be applied in it directly. In this paper, we present a complex file system named NVMCFS for Hybrid NVM. The head-tail layout and space management based on two layer radix-tree is provided to unify logic space between two type NVM devices. The complex file structures, dynamic file data distributed strategy, buffer for an individual file and asymmetric call in strategy are used to speed up the access response and improve I/O performance. The hybrid consistent mechanism is given and it can reduce the performance loss NVMCFS. Finally, the prototype of NVMCFS is implemented and evaluated by various benchmark. Compared to Ext2 and Ext4 on PMBD, NVMCFS improves sequential read speed 4.4x and 5x, sequential write speed 2.8x and 1.9x, IOPS 45% and 62%, and has the similar I/O performance with PMFS. At the same time, NVMCFS reduces the total overhead of consistency by 50%~92% compared to Ext4.
[Performance evaluation, buffer storage, Hybrid Storage, Metadata, I/O performance, two layer radix-tree, asymmetric call, Computer science, Non-volatile Memory, complex file system, NVMCFS, Nonvolatile memory, File systems, Memory management, Layout, File System, complex file structures, tree data structures, buffer, SCM, hybrid NVM devices, file data distributed strategy, nonvolatile memory]
Optimizing Data Placement of MapReduce on Ceph-Based Framework under Load-Balancing Constraint
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Ceph has been widely used as a distributed object store and file system due to its high availability, reliability and scalability. Strategies of data placements in Ceph composed of heterogeneous clusters can greatly affect the system performance and load balancing. For a given application, it is critical to find the optimal data placement in Ceph, such that the completion time of the application can be minimized under the load-balancing constraint. This paper presents a novel Ceph-based framework that integrally considers the load balancing and the heterogeneities, including the computational capacity and the network bandwidth. The presented framework is suitable for the applications based on the principle of moving computation rather than data across clusters, such as MapReduce. According to the Ceph-based framework and the properties of MapReduce, we formulate the Mixed Integer Linear Programming (MILP) to obtain the optimal data placement. However, because of the large computational complexity of MILP, we devise an efficient algorithm to obtain the near-optimal solutions. The experimental results show that the proposed algorithm can achieve up to 25.6% improvement on system performance, compared with the original strategy implemented in Ceph.
[Performance evaluation, data placement optimization, object storage, mixed integer linear programming, load balancing, integer programming, MILP, distributed object store and file system, reliability, linear programming, parallel processing, scalability, MapReduce, resource allocation, data placement, network operating systems, Clustering algorithms, Distributed databases, near-optimal solutions, Bandwidth, Ceph, Ceph-based framework, performance evaluation, Partitioning algorithms, load-balancing constraint, network bandwidth, framework, system performance, Load management, heterogeneous clusters, data handling, Reliability, computational capacity, computational complexity]
ScalaRDF: A Distributed, Elastic and Scalable In-Memory RDF Triple Store
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
The Resource Description Framework (RDF) andSPARQL query language are gaining increasing popularity andacceptance. The ever-increasing RDF data has reached a billionscale of triples, resulting in the proliferation of distributed RDFstore systems within the Semantic Web community. However, theelasticity and performance issues are still far from settled inface of data volume explosion and workload spike. In addition, providers face great pressures to provision uninterrupted reliablestorage service whilst reducing the operational costs due to avariety of system failures. Therefore, how to efficiently realizesystem fault tolerance remains an intractable problem. In this paper, we introduce ScalaRDF, a distributed and elastic in-memoryRDF triple store to provision a fault-tolerant and scalable RDFstore and query mechanism. Specifically, we describe a consistenthashing protocol that optimizes the RDF data placement, dataoperations (especially for online RDF triple update operations)and achieves an autonomously elastic data re-distribution in theevent of cluster node joining or departing, avoiding the holisticoscillation of data storage. In addition, the data store is ableto realize a rapid and transparent failover through replicationmechanism which stores in-memory data replica in the next hashhop. The experiments demonstrate that query time and updatetime are reduced by 87% and 90% respectively compared to otherapproaches. For an 18G source dataset, the data redistributiontakes at most 60 seconds when system scales out and at most 100seconds for recovery when nodes undergo crash-stop failures.
[ScalaRDF, Protocols, Dictionaries, elastic in-memory RDF triple store, consistent hashing protocol, distributed system, Resource description framework, elastic data re-distribution, Database languages, scalability, distributed in-memory RDF triple store, query processing, RDF, Distributed databases, distributed databases, in-memory data replica, Monitoring, RDF data placement, fault tolerance, RDF store mechanism, RDF query mechanism, Indexes, hashing protocol, resource description framework, fault tolerant computing]
SLA-DO: A SLA-Based Data Distribution Strategy on Multiple Cloud Storage Systems
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
One of the major advantages of the cloud storage system is that it simplifies the time-consuming processes of hardware and software provisioning, deployment and distribution for users. Currently there is a tremendous increase in the scale of data generated as well as being consumed by applications on cloud storage systems, but besides availability metrics, few of cloud storage services provide data privacy guarantees in their Service Level Agreements (SLAs), which can be a major concern for the adoption of cloud storage services. The paper firstly proposes an analysis approach for the data privacy, then introduces privacy into the service quality, and defines a service quality evaluation model including privacy, availability, throughput, transfer time and operating cost. At last, a novel data distribution strategy called SLA-DO is presented. The comparison experiment results confirmed that SLA-DO strategy shows better performance in SLAs compliance, resource utilization, security guarantees and multiple cloud environmental adaptability than the data distribution policy adopted in HDFS and OpenStack.
[Measurement, Cloud computing, Data privacy, multiple cloud storage systems, storage optimization, OpenStack, availability, Security, contracts, SLAs, cloud, storage management, resource allocation, time-consuming process, throughput, Mathematical model, cloud computing, resource utilization, cloud environmental adaptability, SLA compliance, SLA-DO, SLA-based data distribution, transfer time, service level agreements, service quality, HDFS, cloud storage services, service quality evaluation model, quality of service, software provisioning, operating cost, security of data, data privacy, data distribution policy, Resource management, hardware provisioning]
The Implementation of Supporting Uniform Data Distribution with Software-Dened Storage Service on Heterogeneous Cloud Storage
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
In order to improve accessibility and efficiency of a cloud system, this work proposed a mechanism to integrate Ceph, HDFS and Swift based on the OpenStack. We first build a heterogeneous storage environment including Ceph, HDFS and Swift based on the open source OpenStack and then measure their performances. To integrate storage services of Ceph, HDFS and Swift, we propose a proportion-based file distribution mechanism. The proportion for file partition is dependent on the remaining storage capacity so that we can distribute those sub files to different storage. This mechanism also enhances the file security. In addition, a high usability user interface is provided so as to make the proposed system more friendly. Experimental results show the efficiency of our system.
[Cloud computing, Ceph, high usability user interface, uniform data distribution, Swift based on the OpenStack, software-dened storage service, HDFS, user interfaces, heterogeneous storage environment, Servers, storage management, Cloud service, Storage Service, heterogeneous cloud storage, Heterogeneous storage, Computer architecture, User interfaces, Data distribution, Hardware, proportion-based file distribution mechanism, Software-Defined Storage, cloud computing, Virtualization]
Utilizing SSD to Alleviate Chunk Fragmentation in De-Duplicated Backup Systems
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Data deduplication, which removes redundant data so that only one copy of duplicate blocks needs to be actually stored, has been implemented in almost all storage appliances, including archival and back-up systems, primary data storage, and SSD devices, to save storage space. However, as time goes and more duplicate blocks have been ingested into the system, the fragmentation problem emerges, that is, logically continuous data blocks of later stored datasets are dispersed in a large storage space and as a result restoring them requires a lot of extra disk accesses, significantly degrading restore performance and garbage collection efficiency. Existing approaches toward the fragmentation problem choose to sacrifice space savings for performance by selectively rewriting trouble-causing duplicate blocks when performing deduplication, even though they have already been stored elsewhere previously. However, rewriting chunks into the system impacts the backup process and reduces deduplication efficiency as many duplicate chunks are allowed in the system. In this work, we propose to deploy flash-based SSDs in the system to overcome the limitations of rewriting algorithms by taking advantage of the high performance provided by SSDs. Specifically, instead of rewriting, we migrate the trouble-causing blocks into an SSD storage in the background when encountering duplicate blocks. The idea is mainly motivated by the following two reasons. First, using a separate migrating process leverages the computing power provided by modern multi-core architecture. Second, typically restores are not performed immediately after backups. Therefore, there is no need to rewrite blocks on the critical path, which affects performance. We augment our proposal to two rewriting schemes and conduct comprehensive evaluations to evaluate its efficacy. Our results show that by provisioning a reasonable amount of SSD, the backup performance and deduplication efficiency can be significantly improved, while slightly increasing the amount of container reads associated with restore operations.
[Data Deduplication, rewriting systems, multiprocessing systems, chunk fragmentation alleviation, Buffer storage, Prefetching, flash-based SSD, SSD storage, multicore architecture, Backup, Containers, Fingerprint recognition, rewriting schemes, backup performance, deduplication efficiency, Indexes, Proposals, Fragmentation, Home appliances, flash memories, Storage, file organisation, Restore Performance, deduplicated backup systems]
Wamalloc: An Efficient Wear-Aware Allocator for Non-Volatile Memory
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Non-volatile memory(NVM) promises a DRAM replacement in computer systems due to its attractive characteristics. However, the low endurance problem limits its practical applications. In this paper, we propose Wamalloc, an efficient NVM memory allocator to extend the lifetime of NVM in the software level. An elaborate hybrid wear-leveling policy is proposed in this paper to achieve wear-leveling without hardware overhead. The evaluations show that the wear-leveling policy of Wamalloc outperforms that of NVMalloc from 3% to 30%, and the total memory consumption of Wamalloc outperforms that of NVMalloc about 60% and 10% under uniform and random workloads. In addition, the allocation performance of Wamalloc is better than the standard glibc malloc and NVMalloc by 98% and 97% under uniform workloads, 83% and 86% under random workloads.
[hybrid wear-leveling policy, Instruction sets, efficient wear-aware allocator, Random access memory, NVM memory allocator, DRAM replacement, software level, memory allocator, uniform workload, Phase change materials, Nonvolatile memory, Operating systems, Memory management, computer systems, total memory consumption, allocation performance, wear-leveling, DRAM chips, non-volatile memory, Resource management, Wamalloc, nonvolatile memory, random workload]
Zero-Chunk: An Efficient Cache Algorithm to Accelerate the I/O Processing of Data Deduplication
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Data deduplication is a technique to eliminate duplicated copies of data. It can save the storage space, reduce the amount of disk I/Os, then improve the system performance. There have been several popular deduplication algorithms such as SISL [30], Extreme Binning [1], Sparse Indexing [14], etc. These schemes use containers to aggregate data chunks for better performance. However, they either suffer from low cache hit ratios or inefficient cache utilization. To address this problem, we design Zero-Chunk, a new cache algorithm that balances the cache hit ratio and memory usage. In our method, we choose chunks whose fingerprints have all-zero remainders as pointers (called zero chunks), and aggregate the following chunks into their corresponding containers. And then, when the access patterns change, our method can eliminate cold data chunks and containers to maintain a low overhead. To demonstrate the effectiveness of Zero-Chunk, we conduct several simulations. The results show that, compared to Sparse Indexing (the most popular implementation method in data deduplication), Zero-Chunk improves the cache hit ratio by up to 5.2%, saves the memory consumption by more than 50.7%, and decreases the total number of I/Os by up to 17.3%, respectively.
[Algorithm design and analysis, data compression, cache algorithm, data deduplication, Containers, Deduplication, Backup Systems, cache storage, memory usage, cache hit ratio, zero-chunk, Computer science, Performance Evaluation, Memory management, I/O processing, Distributed databases, Indexing, Cache]
A Field-Based Model for Representing Dynamic and Evolving Features of Cloud Services
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
In cloud computing context services present the dynamic and evolving features, which greatly affects the accuracy and efficiency of service discovery. It is the prerequisite for supporting service discovery to capture and represent these features during the process of services organization and management. This paper proposes a field-based model to describe the dynamic and evolving features of cloud services in both qualitative and quantitative way, which is inspired by the Bohr atom model. The concept of energy level in Bohr model is used to represent the services status and demarcate cloud services, electrons jumping mechanism in Bohr model is used to depict services' dynamic and evolving features and analyze how services status are changed from one energy level to another. The field model of services provides abstractions to classify a set of services according to their status and mechanism to explain their changes and demarcation according to their potential energy variation. The concept of user acceptable services region is proposed to represent the search scope and method of user service discovery request in cloud services field model. The algorithms to generate field model of services and form user acceptable services region are designed, with which field-based service discovery algorithm is proposed. Based on QWS dataset, we conduct a series of experiments to evaluate and validate the effectiveness of field-based service model for organizing and discovering cloud services.
[Cloud computing, dynamic and evolving, service energy level, feature representation, cloud services field model, field-based service discovery algorithm, Quality of service, service field, potential energy variation, user acceptable services region, Analytical models, user service discovery request, Bohr atom model, Semantics, Organizations, energy level, Energy states, cloud computing, service jumping]
A Game-Theoretic Analysis of Pricing Strategies for Competing Cloud Platforms
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
In this paper, we analyse how multiple competing cloud platforms set effective service prices between Web service providers and consumers. We propose a novel economic framework to model this problem. Cloud platforms run double auction mechanisms, where Web service is commodity traded by service providers (sellers) and service consumers (buyers). Multiple cloud platforms compete against each other to attract service providers and consumers. Specifically, we use game theory to analyse the pricing policies of competing cloud platforms, where service providers and consumers can choose to participate in any of the platforms, and bid or ask for the Web service. The platform selection and bidding strategies of service providers and consumers are affected by the pricing policies and vice versa, and so we propose a co-learning algorithm based on fictitious play to analyse this problem. In more detail, we investigate a setting with two competing cloud platforms who can adopt either equilibrium k pricing policy or discriminatory k pricing policy. We find that, when both cloud platforms use the same type of pricing policy, they can co-exist in equilibrium, and they have an extreme bias to service providers or consumers when setting k. When both platforms adopt different types of policies, we find that all service providers and consumers converge to the discriminatory k pricing policy and so the two competing platforms can no longer co-exist.
[Algorithm design and analysis, Cloud computing, Web platform selection, Web service providers, double-auction mechanism, Game Theory, Pricing, economic framework, Mathematical model, cloud computing, learning (artificial intelligence), Distribution functions, Economics, equilibrium k-pricing policy, game theory, cloud platforms, bidding strategies, discriminatory k-pricing policy, Web Service, Pricing Strategies, Web service consumers, Competing Cloud Platforms, Web services, game-theoretic analysis, co-learning algorithm, pricing, Double Auction Mechanisms]
A Task Scheduling Method for Energy-Efficient Cloud Video Surveillance System Using a Time-Clustering-Based Genetic Algorithm
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Demands for cloud video surveillance systems are growing rapidly. Addressing to the issue of low energy-efficiency in cloud video datacenters, a task scheduling method using a time-clustering-based genetic algorithm is proposed. Firstly, an off-line scheduling model with SLA (service level agreement) time constraint is proposed after the analysis of the constrain relationship between the SLA and surveillance tasks. Then, a time-clustering-based genetic algorithm (TCGA) is proposed to solve the model for an optimal energy-efficient solution. According to the solution, the service quality is guaranteed and the total operating time of virtual machines is minimized. Meanwhile, idle virtual machines are shut down to reduce energy consumption. Simulations of large scale tasks scheduling are conducted. Several comparison experiments verify that the proposed method can improve the resource utilization greatly and achieve energy saving extremely.
[Energy consumption, Cloud computing, task scheduling method, optimal energy-efficient solution, SLA, Servers, contracts, Genetic algorithms, genetic algorithm, scheduling, cloud video surveillance, cloud computing, video surveillance, energy consumption, energy-efficient cloud video surveillance system, cloud video datacenters, Scheduling, genetic algorithms, service level agreement, time-clustering-based genetic algorithm, Surveillance, virtual machines, time clustering, task scheduling, Time factors]
An Energy-Aware Ant Colony Algorithm for Network-Aware Virtual Machine Placement in Cloud Computing
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
The energy cost is one of the major concerns for the cloud providers. Virtual machine placement has been demonstrated as an effective method for energy saving. In addition to constraints caused by the physical machine resources such as CPU and memory (PM-constraints), the constraints caused by the network resource such as bandwidth (Net-constraints) are also crucial, since virtual machines are not isolated and require communication with each other to exchange data. However, most current research on data center power optimization only focuses on server resource. As a result, the optimization results are often inferior, because server consolidation without considering the network may cause traffic congestion and thus degraded network performance. We take the traffic demands between virtual machines into consideration and formulate the virtual machine placement problem under both PM-constraints and Net-constraints to minimize the energy cost, and propose an approach based on ant colony optimization to solve the problem. We evaluate the expected performance of our proposed algorithm through a simulation study, providing strong indications to the superiority of our proposed solution.
[Cloud computing, CPU, Net-constraints, PM-constraints, Servers, Optimization, traffic demand, data center networking, power aware computing, network-aware virtual machine placement, ant colony optimization, memory constraints, Bandwidth, Mathematical model, cloud computing, ant colony optimisation, Power demand, energy cost minimization, virtual machine placement, server consolidation, Virtual machining, energy saving, network resource, energy-aware ant colony optimisation algorithm, virtual machines, energy cost, cloud providers, bandwidth constraints, minimisation, physical machine resources]
An Online Auction for Deadline-Aware Dynamic Cloud Resource Provisioning
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Auction mechanisms have recently been studied as an efficient approach for dynamic resource allocation in a cloud market. Existing mechanisms are mostly limited to the offline setting or execute jobs in continuous time slots. This work focuses on a practical case of online auction design, where users bid for future cloud resources for executing their batch processing jobs with hard deadline constraints. We design an online primal-dual auction framework for Virtual Machine (VM) allocation with social welfare maximization, which is truthful, computationally efficient, and guarantees a small competitive ratio. We leverage the framework of post price auctions to design our online primal-dual algorithm, where a bid is accepted if its expected execution cost in future time slots is smaller than its bidding price. We interpret the dual variables as marginal prices per unit of resource, and iteratively update it according to the allocated amount of resource. Theoretical analysis and trace-driven simulation studies validate the efficacy of the online auction framework, including both its computational efficiency and economic efficiency.
[Algorithm design and analysis, Online Auction, Cloud computing, VM allocation, batch processing jobs, Resource Provisioning, Dynamic scheduling, Virtual machining, social welfare maximization, deadline-aware dynamic cloud resource provisioning, Auction Design, resource allocation, hard deadline constraints, virtual machine, Cloud Computing, Batch production systems, Pricing, virtual machines, Resource management, cloud computing, online primal-dual auction framework]
Authenticated Spatio-Textual Similarity Joins in Untrusted Cloud Environments
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
A spatio-textual similarity join searches a spatio-textual data collection and reports the object pairs that satisfy the specified spatial distance threshold and textual similarity threshold. However, when the data owner outsources the join computations to a third-party cloud service provider, the service provider may send incomplete or incorrect join results to the data owner. In this paper, we propose a pairwise authentication scheme, a cluster based scheme and an order and bound based scheme to authenticate the results of spatiotextual similarity joins. Extensive experiments on a real-world dataset verify the effectiveness and efficiency of our proposed schemes in terms of various performance metrics.
[Cloud computing, text analysis, spatio-textual similarity joins, authenticated spatio-textual similarity joins, computation outsourcing, incorrect join, authorisation, query authentication, location-based services, cloud computing, untrusted cloud environments, third-party cloud service provider, cluster based scheme, spatio-textual data collection, Indexes, textual similarity threshold, Authentication, Public key, Data collection, incomplete join, order and bound based scheme, Outsourcing, pairwise authentication, trusted computing, spatial distance threshold]
Benchmarking Sentiment Analysis Approaches on the Cloud
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Social media resources such as Twitter provide global services for citizens to express opinions on people, products, events or even themselves. Often this data captures the mood (sentiment) of the tweeter. Accurate and timely extraction of sentiment from such big data can be used for many population-wide business and research scenarios. Whilst a range of sentiment analysis approaches has been taken, little systematic comparison of these approaches has been undertaken. The motivation of this paper is to investigate various sentiment analysis approaches and evaluate their accuracy and performance for Twitter-based sentiment analysis on major Cloud facilities across Australia. We consider especially the impact of training data on performance and accuracy of sentiment analysis. To support this, we present a Cloud-based architecture and its realization through an elastic, distributed, data processing system used for harvesting, analyzing and storing large-scale Twitter data sets.
[Algorithm design and analysis, Sentiment analysis, Cloud computing, Machine learning algorithms, distributed processing, distributed system, Twitter, large-scale Twitter data sets, cloud-based architecture, Sentiment Analysis, Training, social media resources, Cloud Computing, Training data, data processing system, cloud computing, data analysis, sentiment analysis, global services, cloud facilities, social networking (online), benchmarking sentiment analysis approaches, benchmark testing, Australia, Performance, elastic system]
Data Dissemination Protocols Based on Opportunistic Sharing for Data Offloading in Mobile Social Networks
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Due to the increasing popularity of smart mobile devices, the amount of mobile data communications has led to explosive growth of data traffic in cellular networks. Cellular networks have to face the challenge of huge communication traffic. Offloading data traffic through opportunistic communication among smart mobile devices is a promising solution to partially solve this problem since there is almost no monetary cost for it. Large amount of smart mobile devices can communicate each other using Bluetooth or WIFI Direct in short communication range and they can form an opportunistic mobile social network. The opportunistic communications among smart mobile devices can effectively reduce the amount of cellular data traffic. However, mobile users take a long time to obtain useful data. In order to reduce data communication latency, this paper proposes three data dissemination protocols named RRDP(Request-Reply Dissemination Protocol), RDP(Random Dissemination Protocol) and LDP(LRU Dissemination Protocol) respectively. The three proposed protocols are based on opportunistic sharing policy. Extensive NS-2 simulation results show that (1) on the campus situation, the user's access delay of RDP is 56.4% less than the RRDP and LDP is 44.8% less than RRDP. (2) in the vehicular environment, the user's access delay of RDP is 32.5% less than the RRDP and LDP is 28.1% less than RRDP. RDP is the best protocol.
[Protocols, data dissemination protocols, Bluetooth, Data dissemination, data offloading, random dissemination protocol, LRU policy, mobile computing, Random dissemination policy, opportunistic sharing, cellular data traffic, LDP, protocols, cellular networks, RDP, mobile data communications, Opportunistic communications, request-reply dissemination protocol, Request-Reply policy, Cellular traffic offloading, RRDP, Mobile nodes, Cellular networks, Access delay, LRU dissemination protocol, social networking (online), mobile social networks, Delays, cellular radio]
Efficient Snapshot KNN Join Processing for Large Data Using MapReduce
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
The kNN join problem, denoted by R &#x00D7;<sub>KNN</sub> S, is to find the k nearest neighbors from a given dataset S for each point in the query set R. It is an operation required by many big data applications. As large volume of data are continuously generated in more and more real-life cases, we address the problem of monitoring kNN join results on data streams. Specifically, we are concerned with answering kNN join periodically at each snapshot which is called snapshot kNN join. Existing kNN join solutions mainly solve the problem on static datasets, or on a single centralized machine, which are difficult to scale to large data on data streams. In this paper, we propose to incrementally calculate the kNN join results of time t<sub>i</sub> from the results of snapshot t<sub>i-1</sub>. Typically, for the data continuously generated on the data stream, we can get S<sub>i</sub> = S<sub>i-1</sub> + &#x0394;S<sub>i</sub> for the valid datasets of adjacent snapshots, where &#x0394;S<sub>i</sub> denotes the updated points between time t<sub>i-1</sub> and t<sub>i</sub>. Our basic idea is to first find the queries in R whose kNN results can be affected by the updated points in &#x0394;S<sub>i</sub>, and then update the kNN results of these small part of queries respectively. In this way, we can avoid calculating the kNN join results on the whole dataset S<sub>i</sub> in time t<sub>i</sub>. We propose an implementation of searching for affected query points in MapReduce to scale to large volume of data. In brief, the mappers partition the datasets into groups, and the reducers search for affected queries separately on each group of points. Furthermore, we present the enhanced strategies of data partitioning and grouping to reduce the shuffling cost and computational cost. Extensive experiments on real-world datasets demonstrate that proposed methods are efficient, robust, and scalable.
[Measurement, Algorithm design and analysis, KNN Join, Data Streams, k nearest neighbors, computational cost, data streams, Servers, Indexes, parallel processing, MapReduce, Large Data, query processing, Distributed databases, data partitioning, Robustness, Computational efficiency, data handling, learning (artificial intelligence), shuffling cost, snapshot KNN join processing]
Game Theoretic Energy Allocation for Renewable Powered In-Situ Server Systems
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
In-situ server systems are deployed in very special operating environment to handle in-situ workloads that are normally generated from environmentally sensitive areas or remote places that lack established utility infrastructure. This very special operating environment of in-situ servers urges such systems to be 100 percent powered by renewable energy. However, existing energy management schemes assume a hybrid supply of grid and renewable energy, hence are not well suited for 100 percent renewable powered in-situ server systems. In this paper, we tackle the problem of allocating harvested energy to 100 percent renewable powered server systems for optimizing both the overall system throughput and throughput of individual servers. From a game theoretic perspective, we model the energy allocation problem as a cooperative game among multiple servers and derive a Nash bargaining solution. Based on the Nash bargaining solution, we then propose a heuristic algorithm that determines the energy allocation strategies according to system energy states. Experimental results show that our proposed game theoretic approach achieves a high throughput from perspectives of both the overall system and individual servers.
[renewable energy sources, utility infrastructure, game theory, game theoretic energy allocation, renewable materials, Throughput, operating environment, Servers, renewable powered in-situ server systems, Nash bargaining solution, Game theory, heuristic algorithm, Renewable energy sources, renewable energy, Program processors, In-situ server, Games, energy management, Energy states, harvested energy, Renewable energy allocation, Resource management]
GDSW: A General Framework for Distributed Sliding Window over Data Streams
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
The big data era is characterized by the emergence of live data with high volume and fast arrival rate, it poses a new challenge to stream processing applications: how to process the unbounded live data in real time with high throughput. The sliding window technique is widely used to handle the unbounded live data by storing the most recent history of streams. However, existing centralized solutions cannot satisfy the requirements for high processing capacity and low latency due to the single-node bottleneck. Moreover, existing studies on distributed windows primarily focus on specific operators, while a general framework for processing various window-based operators is wanted. In this paper, we firstly classify the window-based operators to two categories: data-independent operators and data-dependent operators. Then, we propose GDSW, a general framework for distributed count-based sliding window, which can handle both of data-independent and data-dependent operators. Besides, in order to balance system load, we further propose a dynamic load balance algorithm called DAD based on buffer usage. Our framework is implemented on Apache Storm 0.10.0. Extensive evaluation shows that GDSW can achieve sub-second latency, and 10X improvement in throughput compared with centralized processing, when processing rapid data rate or big size window.
[GDSW, Distributed Processing, General Framework, data streams, DAD, Big Data, data-independent operators, Sparks, distributed sliding window, dynamic load balance algorithm, Distributed processing, Apache Storm, Storms, resource allocation, Semantics, Distributed databases, window-based operators, data-dependent operators, Parallel processing, Sliding Window, Real-time systems, data handling, Data Stream]
Implementation of an Energy Saving Cloud Infrastructure with Virtual Machine Power Usage Monitoring and Live Migration on OpenStack
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
This work implement a cloud infrastructure that can monitor the status of OpenStack and monitor the real-time status of virtual machine on OpenStack then achieve to energy saving through live migration. The projects of monitoring include the utilization of CPU, load of memory, and power consumption. These data show in real-time, completely monitor the real-time status of physical machines and virtual machines. It also record the utilization and power consumption of physical machines then show on this cloud infrastructure, to provide experimental evidence for the user as a reference. Base on the power consumption we monitoring, we can automatically allocate virtual machines on every physical machines by live migration, to balance the power consumption of every physical machines. Its not only can avoid idle and waste of resources but also can avoid reducing machine life because of the physical machines always keep in high usage, and achieve to power saving.
[Cloud computing, Dynamic Allocation, OpenStack, power consumption, Machine, automatic virtual machine allocation, power aware computing, Live Migration, resource allocation, live migration, virtual machine power usage monitoring, Hardware, cloud computing, Monitoring, Power demand, Virtual machining, Real-Time Monitoring, Cloudfrastructure, Virtual machine monitors, CPU utilization, energy saving cloud infrastructure, physical machines, virtual machines, Power Saving, Resource management]
Improving the Performance of Data Sharing in Dynamic Peer-to-Peer Mobile Cloud
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Mobile cloud computing has become an emerging computing paradigm to extend the capability of the mobile devices and it has gained increasing popularity in recent years. Existing studies mainly focus on how to leverage the computing capability of the individual device by employing the capability from remote cloud datacenters or local mobile cloud formed by nearby devices. Different from these studies, we investigate how to improve the performance of data sharing in the peer-to-peer mobile cloud, with the limited bandwidth and the presence of dynamic and unpredictable wireless channel state. Specifically, we first formulate the data transmission among devices as a utility maximization problem with the consideration of limited bandwidth, incentive participation and the QoE (Quality of Experience) heterogeneity, based on incorporating publish/subscribe component into the base station. Then, a dynamic online algorithm, which does not need the future context (e.g., channel state) of the mobile cloud, is developed to simultaneously make the decision of data transmission and communication interface selection. Rigorously theoretical analysis shows the optimality and the effectiveness of the proposed algorithm. Extensive experiments are conducted to verify the analysis results and the superiority of the proposed algorithm over existing strategies.
[Cloud computing, QoE, dynamic peer-to-peer mobile cloud, Mobile communication, Mobile handsets, performance improvement, local mobile cloud, dynamic online algorithm, mobile computing, optimisation, communication interface selection, quality of experience heterogeneity, Online Algorithm, wireless channels, Data communication, cloud computing, computing capability, remote cloud datacenters, software performance evaluation, middleware, publish-subscribe component, Base stations, message passing, utility maximization problem, peer-to-peer computing, Transmission Optimization, Mobile Cloud, quality of experience, mobile cloud computing, dynamic wireless channel state, data transmission, data communication, unpredictable wireless channel state, Data Sharing, Peer-to-peer computing, data sharing]
On Autonomous Service Migrations in the Cloud for Mobile Accesses
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
We study the problem of autonomous service migration in the cloud to satisfy an online sequence of mobile batch-request demands in a cost-effective way. As the origins of the mobile accesses frequently change over time, this problem is particularly important for time-bounded services to achieve enhanced QoS and cost effectiveness. Moving the service closer to its client locations not only reduces the service access latency but also minimizes the network costs for service providers. However, the migration comes at costs of bulk-data transfer and service disruption, as a result, increasing the overall service costs. To gain the benefits of service migration while minimizing the service costs, we propose an efficient search-based algorithm Dmig the service migration in an autonomous way. Compared with existing algorithms, the proposed algorithm is fully distributed, symmetric, and characterized by the effective use of historical access information to perform virtual migration that overcomes the limitation of traditional local search in cost reduction. To evaluate the algorithm, we compared it with some existing algorithms, and show that the proposed algorithm exhibits better performance by adapting to the changes of mobile access patterns in a cost effective way.
[autonomous service migration, Cloud computing, search-based algorithm, mobile access, dynamic virtual machine placement, Mobile communication, Servers, historical access information, Wireless communication, dynamic service migration, mobile computing, QoS, Bandwidth, cloud computing, time-bounded services, search problems, virtual migration, cost reduction, Dmig, Routing, Virtual machining, quality of service, mobile batch-request demands, virtual machines, cost effectiveness, service cost minimization, mobile access patterns, online sequence]
Secure Conjunctive Multi-Keyword Search for Multiple Data Owners in Cloud Computing
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Recently, secure search over encrypted cloud data has become a hot research spot and challenging task. Some secure search schemes have been proposed to try to meet this challenge. In this paper, we propose a conjunctive multi-keyword secure search scheme for multiple data owners. To guarantee data security and system flexibility in the multiple data owners environment, we design an ingenious secure query scheme that allows each data owner to adopt randomly chosen temporary keys to build secure indexes for different data files. An authorized data user does not need to know these temporary keys of constructing indexes and can instead randomly choose another temporary query keys to encrypt query keywords while the cloud can correctly perform keywords matching over encrypted data files. Extensive experiments demonstrate the correctness and practicality of the proposed scheme.
[Cloud computing, text analysis, multiple data owners, encrypted cloud data, Computational modeling, secure conjunctive multikeyword search, data security, secure query scheme, temporary query keys, system flexibility, cryptography, keywords matching, Encryption, Indexes, Servers, data files, secure index, Multiple data owners, query processing, Conjunctive keyword query, authorized data user, Secure query, cloud computing]
VNF Placement in Hybrid NFV Environment: Modeling and Genetic Algorithms
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
In this paper, we study the VNF placement problem in hybrid NFV environment, which is important during the transition from traditional networks to NFV networks. We first propose a new concept of hybrid NFV environment, which is more comprehensive and realistic than the former works. Then, we give out a novel model of VNF placement optimization to achieve lower bandwidth consumption and lower maximum link utilization simultaneously, with consideration of VNF combination. Next, to solve this problem, we propose four genetic algorithms, which are combinations of the frameworks of two existing algorithms (MOGA and NSGA-II) and our novel modifications. Simulation results show that, in our 4 algorithms Greedy-NSGA-II has the best performance. When compared with other two non-genetic algorithms (BM and Random), the average total bandwidth consumption of Greedy-NSGA-II is only 12.24% and 2.96% of theirs respectively, and the average maximum link utilization of Greedy-NSGA-II is only 25.04% and 13.81% of theirs respectively.
[Decision support systems, network function virtualization, Conferences, greedy algorithms, greedy-NSGA-II algorithm, Network Function Virtualization, average maximum link utilization, Encoding, genetic algorithms, virtualisation, Optimization, Genetic algorithms, genetic algorithm, MOGA algorithm, hybrid NFV environment, average total bandwidth consumption, Sociology, virtual machines, VNF Placement, multi-objective optimization, cloud computing, Virtualization, VNF placement optimization]
A Fine-Grained Parallel Intra Prediction for HEVC Based on GPU
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Intra prediction in HEVC is much more complex compared to the one in H.264 because of the more diversifications of the block sizes and prediction modes. The state-of-the-art researches for its parallelization only focus on block-level methods, which only take very limited advantage of GPUs. It is still a big challenge to implement fine-grained parallelism on GPU in consideration of the HEVC branch instructions and the different prediction formulae. We present a novel pixel-level parallelism method for the intra prediction of HEVC based on GPU combined with mode-level parallelism. By unifying not only the prediction formulae between angular mode and planar mode but also a predictor array, an algorithm based on look-up table is proposed to greatly reduce branches and improve prediction efficiency. With the help of look-up table algorithm, each pixel in a block can obtain the offset of corresponding reference pixels and find the value in the unifying predictor array at the same time which makes it possible to predict all pixels in parallel regardless of their relative positions in the block. The experimental results show that the proposed algorithm outperforms previous work and can reduce encoding time effectively.
[Algorithm design and analysis, angular mode, prediction efficiency improvement, intra prediction, Instruction sets, Graphics processing units, look-up table, HEVC, GPU, branch reduction, reference pixels, pixel-level parallelism method, fine-grained parallel intraprediction, Parallel processing, Prediction algorithms, block sizes, planar mode, mode-level parallelism, HEVC branch instructions, prediction formulae, Encoding, predictor array, video coding, graphics processing units, Standards, look-up table algorithm, fine-grained parallelism, encoding time reduction]
A Fine-Grained Parallel Power Flow Method for Large Scale Grid Based on Lightweight GPU Threads
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
This paper proposes a parallel Newton-Raphson Power Flow (PNPF) method which is suitable for GPU (Graphics Processing Unit). Aiming at the most time-consuming part of power flow-solving the sparse linear equations, an efficient hierarchy parallel solver is presented, in which LU decomposition and forward/back substitution were implemented in parallel level by level under the direction of path tree. To improve the efficiency of the method, a short-type path tree was formed, an optimization on data transfer and a self-adapted task-allocation algorithm was designed. In addition, the sparse Jacobian matrix and the right-hand side of the linear equations were also generated in parallel in a fine-grained pattern. At last, the developed GPU-based PNPF program has been tested on large-scale power systems of up to 23215 buses. The method provides a speedup of 3.91 times compared to mainstream commercial software, which proves the effectiveness and practicality of the algorithm proposed in this paper.
[Newton-Raphson method, Instruction sets, Graphics processing units, matrix decomposition, Sparse matrices, parallel processing, GPU, Load flow, power system analysis computing, Jacobian matrices, forward-back substitution, short-type path tree, LU decomposition, sparse linear equations, sparse Jacobian matrix, Mathematical model, parallel computing, large-scale power systems, lightweight GPU threads, large scale grid, graphics processing units, GPU-based PNPF program, power flow study, parallel Newton-Raphson power flow method, fine-grained parallel power flow method, buses, graphics processing unit, data transfer, Data transfer, load flow, self-adapted task-allocation algorithm, sparse linear system, path tree]
Accelerating Spark RDD Operations with Local and Remote GPU Devices
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Apache Spark is a distributed processing framework for large-scale data sets, where intermediate data sets are represented as RDDs (Resilient Distributed Datasets) and stored in memory distributed over machines. To accelerate its various computation intensive operations, such as reduction and sort, we focus on GPU devices. We modified Spark framework to invoke CUDA kernels when computation intensive operations are called. RDDs are transformed into array structures and transferred to GPU devices when necessary. Although we need to cache RDDs in GPU device memory as much as possible in order to hide the data transfer overhead, the number of local GPU devices mounted in a host machine is limited. In this paper, we propose to use remote GPU devices which are connected to a host machine via a PCI-Express over 10Gbps Ethernet technology. To mitigate the data transfer overhead for remote GPU devices, we propose three RDD caching policies for local and remote GPU devices. We implemented various reduction programs (e.g., Sum, Max, LineCount) and transformation programs (e.g., SortByKey, PatternMatch, WordConversion) using local and remote GPU devices for Spark. Evaluation results show that Spark with GPU outperforms the original software by up to 21.4x. We also evaluate the RDD caching policies for local and remote GPU devices and show that a caching policy that minimizes the data transfer amount for remote GPU devices achieves the best performance.
[Performance evaluation, resilient distributed datasets, parallel architectures, PCI-Express, Graphics processing units, cache storage, GPU, Spark RDD operations, Ethernet technology, RDD, Distributed databases, data structures, Kernel, reduction programs, CUDA kernels, RDD caching policies, Sparks, graphics processing units, Apache Spark, CUDA, GPU devices, data transfer overhead, transformation programs, PCIe over 10GbE, Acceleration, Arrays, array structures]
Accelerating the Simulation of Thermal Convection in the Earth's Outer Core on Tianhe-2
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Numerical simulation of thermal convection in the Earth's outer core requires extreme-scale computing due to the large temporal and spatial disparity, extreme physical parameters, rapid rotation and spherical geometry. In this work, the numerical simulation of the thermal convection in the Earth's outer core for CPU-MIC heterogeneous many-core systems is studied. Firstly, starting from a legacy parallel code based on the PETSc software package, a framework of the numerical simulation built on CPU-MIC heterogeneous many-core systems has been developed. Secondly, a sparse linear solver for CPUMIC heterogeneous many-core systems, which focuses on solving the two linear systems of the simulation, is presented and optimized. Thirdly, some computational kernels of the simulation, including sparse matrix-vector multiplication (SpMV) and polynomial preconditioner on distributed memory Xeon Phiaccelerated systems are implemented and optimized. In addition, in order to reduce the cost of data movement, we use methods to minimize the memory access, the PCI-E data transfer, and the MPI communication. Finally, some optimized measures are taken to the extended code. Experiments on Tianhe-2 Supercomputer show that as compared to the original code, our Xeon Phiaccelerated design is able to deliver 6.93x and 6.00x speedups for single MIC device and 64 MIC devices, respectively.
[application program interfaces, sparse linear solver, parallel processing, convection, storage management, Earth outer core, computational kernels, software packages, temporal disparity, parallel computing, message passing, multiprocessing systems, Tianhe-2 Supercomputer, geophysics computing, matrix multiplication, vectors, extreme physical parameters, geomagnetism, peripheral interfaces, Xeon Phi acceleration, thermal convection simulation, Linear systems, SpMV, Convection, spherical geometry, Earth, Microwave integrated circuits, data movement cost reduction, polynomial preconditioner, Earth's outer core, numerical analysis, PCI-E data transfer, Mathematical model, numerical simulation, Xeon Phi-accelerated systems, mainframes, extreme-scale computing, geophysical techniques, rapid rotation, CPU-MIC heterogeneous many-core systems, linear systems, legacy parallel code, memory access minimization, sparse matrix-vector multiplication, distributed memory, spatial disparity, Numerical simulation, PETSc software package, performance optimization, Acceleration, minimisation, sparse matrices, MPI communication, thermal convection]
An Energy-Efficient Scheduler for Throughput Guaranteed Jobs on Asymmetric Multi-Core Platforms
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
A recent trend in computing platforms is moving from homogeneous multi-core architectures toward heterogeneous and asymmetric multi-core. Therefore, the design of new schedulers for asymmetric multi-core platform has become an important issue. However, most of the existing schedulers focus on how to distinguish workloads suitable for performance "big" cores from those for power-efficient "little" cores, without considering how to distribute jobs to asymmetric cores running at adjustable frequency. In this paper, we propose an energy-efficient scheduler for throughput guaranteed jobs running on asymmetric multi-core platforms. The proposed scheduler not only determines the frequency of cores and job-to-core assignment in order to reduce energy consumption, but also schedules the jobs so that the throughput of all jobs are guaranteed. The simulation results indicate that the proposed scheduler consumes 40% less energy than the existing Global Task Scheduler with DVFS enabled.
[Schedules, Time-frequency analysis, Power demand, multiprocessing systems, Multicore processing, job throughput, homogeneous multicore architecture, throughput guaranteed jobs, Throughput, heterogeneous asymmetric multicore, Scheduling, Energy-efficient, asymmetric multicore platform, Asymmetric multicore, power aware computing, Processor scheduling, core frequency, scheduling, job-to-core assignment, Throughput Guaranteed Jobs, energy-efficient scheduler, Mathematical model, energy consumption reduction, job scheduling]
Balanced Parity Update Algorithm with Queueing Length Awareness for RAID Arrays
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
In parity-based RAID arrays, to update a data chunk, the corresponding parity chunk(s) must be updated accordingly so as to keep data consistency and availability. To achieve this, either read-modify-write (RMW) or read-construct-write (RCW) could be used. Traditional parity update algorithm always selects the one requiring fewer pre-reads so as to reduce the total number of I/Os, but it may aggravate the skewness of I/O queues on disks, and thus degrades the system performance. In this paper, we propose a balanced parity update algorithm with queueing length awareness, BPU, which takes the number of pre-reads, the skewness of I/O queues on disks, and real-time workload into consideration when selecting RCW or RMW to update parity chunks. We implement a prototype system with BPU to evaluate its performance. Experimental results show that the length of I/O queues on disks in a RAID array may be highly skewed when using traditional parity update algorithm, and thus severely degrades the system performance. With BPU, we can reduce the average response time by up to 10%. We also study the performance of BPU under different system configurations, and provide multiple insights for adjusting the parameters of BPU so as to optimize its performance.
[I/O queues, queueing length awareness, Heuristic algorithms, BPU, RCW, read-modify-write, Parity Update, parity chunk update, RAID, read-construct-write, storage management, redundant arrays of inexpensive disks, RAID arrays, System performance, Prototypes, Bandwidth, Skewness, Arrays, Time factors, Performance, RMW, Queue, Queueing analysis, balanced parity update algorithm]
Compile-Time Automatic Synchronization Insertion and Redundant Synchronization Elimination for GPU Kernels
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
In most of the GPU kernel programs, the synchronization statements are inserted manually by the programmers, which is very labor intensive, and error-prone. In this paper, we propose a synchronization optimization framework to automatically insert synchronization statements into the GPU kernels at compile time, while eliminating the redundant synchronization statements. We have shown that our framework can not only insert the synchronizations correctly, but also eliminate the redundant synchronizations, which outperforms the existing compiler frameworks that introduce redundant synchronizations using the most conservative strategy. Taking the GPU kernels as the input, our framework leverages data dependence analysis to insert synchronizations. We extend CETUS, a source-to-source compiler framework, to implement our synchronization optimization framework. Experimental results show that our proposed framework achieved 100% correctness by combining extensive evaluation and manual comparison. In addition, the number of synchronization statements in GPU kernels is reduced by 32.5%, and the number of synchronization statements executed is reduced by 28.2% on average by our synchronization optimization framework compared to the original GPU kernels.
[Algorithm design and analysis, compile-time automatic synchronization insertion, CETUS, SSA, Compiler, redundant synchronization elimination, Graphics processing units, GPU kernel programs, Synchronization, Indexes, data dependence analysis, graphics processing units, program compilers, GPU, Optimization, source-to-source compiler framework, Computer architecture, Data Dependence, Kernel]
Efficient Distributed Data Structures for Future Many-Core Architectures
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
We study general techniques for implementing distributed data structures on top of future many-core architectures with non cache-coherent or partially cache-coherent memory. With the goal of contributing towards what might become, in the future, the concurrency utilities package in Java collections for such architectures, we end up with a comprehensive collection of data structures by considering different variants of these techniques. To achieve scalability, we study a generic scheme which makes all our implementations hierarchical. We also describe a collection of techniques for further improving scalability in most implementations. We have performed experiments which illustrate nice scalability characteristics for some of the proposed techniques and reveal the performance and scalability power of the hierarchical approach. We distill the experimental observations into a metric that expresses the scalability potential of such implementations. We finally present experiments to study energy consumption aspects of the proposed techniques by using an energy model recently proposed for such architectures.
[scalability characteristics, energy consumption aspects, Java, partially cache-coherent memory, Protocols, multiprocessing systems, stack, message-passing, Scalability, Data structures, noncache-coherent memory, many-core architectures, Servers, Synchronization, concurrency utilities package, Java collections, distributed data structures, distributed databases, data structures, queue, hierarchical approach]
Enabling Tissue-Scale Cardiac Simulations Using Heterogeneous Computing on Tianhe-2
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
We develop a simulator for 3D tissue of the human cardiac ventricle with a physiologically realistic cell model and deploy it on the supercomputer Tianhe-2. In order to attain the full performance of the heterogeneous CPU-Xeon Phi design, we use carefully optimized codes for both devices and combine them to obtain suitable load balancing. Using a large number of nodes, we are able to perform tissue-scale simulations of the electrical activity and calcium handling in millions of cells, at a level of detail that tracks the states of trillions of ryanodine receptors. We can thus simulate arrythmogenic spiral waves and other complex arrhythmogenic patterns which arise from calcium handling deficiencies in human cardiac ventricle tissue. Due to extensive code tuning and parallelization via OpenMP, MPI, and SCIF/COI, large scale simulations of 10 heartbeats can be performed in a matter of hours. Test results indicate excellent scalability, thus paving the way for detailed whole-heart simulations in future generations of leadership class supercomputers.
[Performance evaluation, cellular biophysics, application program interfaces, load balancing, Calcium, Instruction sets, Calcium Handling, tissue-scale cardiac simulations, MPI, digital simulation, parallel machines, parallel processing, code parallelization, heterogeneous computing, resource allocation, Multiscale Cardiac Tissue Simulation, whole-heart simulations, Xeon Phi, Hardware, 3D tissue simulator, Mathematical model, Supercomputing, message passing, Computational modeling, calcium handling, human cardiac ventricle tissue, OpenMP, Tianhe-2 supercomputer, arrythmogenic spiral waves, biological tissues, physiologically realistic cell model, electrical activity, Supercomputers, cardiology, code tuning, complex arrhythmogenic patterns, ryanodine receptors, medical computing, heterogeneous CPU-Xeon Phi design, SCIF/COI, code optimization]
Exploiting Longer SIMD Lanes in Dynamic Binary Translation
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Recent trends in SIMD architecture have tended toward longer vector lengths and more enhanced SIMD features have been introduced in the newer vector instruction sets. However, legacy or proprietary applications compiled with short-SIMD ISA cannot benefit from the long-SIMD architecture, which supports improved parallelism and enhanced vector primitives, and thus only achieve a small fraction of potential peak performance. This paper presents a dynamic binary translation technique that enables short-SIMD binaries to exploit the benefits of the new SIMD architecture by rewriting short-SIMD loop code. We propose a general approach that translates loops consisting of short-SIMD instructions to machine-independent IR, conducts SIMD loop transformation/optimization at this IR level, and finally translates to long-SIMD instructions. Two solutions are presented to enforce SIMD load/store alignment, one for the problem caused by the binary translator's internal translation condition and one general approach using loop peeling optimization. The benchmark results show that an average speedup of 1.45X is achieved for NEON to AVX2 loop transformation.
[vector lengths, parallel architectures, Humanoid robots, dynamic binary translation technique, SIMD, parallel processing, vector instruction sets, binary translator internal translation condition, vectorization, Neon, Runtime, vector primitive enhancement, short-SIMD binaries, SIMD load alignment, short-SIMD instructions, Computer architecture, Parallel processing, loop peeling optimization, instruction sets, program control structures, dynamic loop peeling, machine-independent IR, SIMD lanes, SIMD loop optimization, SIMD architecture, short-SIMD loop code rewriting, Dynamic binary translation, Indexes, parallelism improvement, NEON, long-SIMD instructions, AVX2 loop transformation, SIMD loop transformation, Androids, SIMD store alignment, alignment]
Increasing Lifetime and Security of Phase-Change Memory with Endurance Variation
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Phase Change Memory (PCM) has emerged as a promising candidate for building the future main memory systems. However, the limited write endurance is one of the major obstacles for PCM to be practically applied. Traditional wear-leveling techniques try to uniformly balance the write traffics under both general applications and malicious attacks to enhance the PCM lifetime. However, these techniques fail to consider the endurance variation in PCM chips, and result in severe lifespan degradation since uniform write distribution leads to the weakest cell to be worn out much earlier. In this paper, we propose a weight-based algebraic wear-leveling (WAWL) scheme to balance wear rates (i.e., write traffics/endurance) in a secure manner according to the endurance distribution. In WAWL, the entire memory space is divided into multiple regions. When the number of the writes to a region reaches a threshold (i.e., swapping interval), the region is swapped with a randomly chosen region. The basic idea behind WAWL is that the swapping interval and the chosen probability of each region are variable and associated with the endurance metric of the region. By deploying suitable swapping interval and chosen probability, WAWL achieves uniform wear-rate distribution across the entire memory in an undetectable way. In addition, to reduce space consumption and alleviate performance degradation during region swapping, we propose a fine-grained swapping scheme which migrates the lines one-by-one between the candidate regions. Experimental evaluation driven by the various attacks demonstrates that WAWL significantly increases the PCM lifespan and improves security with slight performance degradation and affordable hardware overhead.
[Algorithm design and analysis, write endurance variation, wear leveling, Random access memory, Programming, lifetime, phase change memories, phase-change memory, endurance variation, Security, weight-based algebraic wear-leveling scheme, Phase change materials, Degradation, security, security of data, PCM security, WAWL, swapping interval, uniform wear-rate distribution, Hardware, PCM]
Joint Hybrid Frequent Value Cache and Multi-Coding for Data Bus Energy Saving
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
In the deep submicron technology domain, the on-chip buses consume considerable amount of total energy of embedded multi-core chip. Lots of techniques have been produced to reduce the bus energy consumption. FVE (Frequent Value Encoding) and FV-MSB (Frequent Value-Most Significant Bit) which exploit abundant value locality on the data buses, are effective methods for reducing data bus energy consumption. In this paper, we propose a method that exploits more value locality that is overlooked by the FVE and FV-MSB. We found that a significant amount of non-frequent values and low-order bits of partial frequent values, not captured by the FVE and FV-MSB, produced large number of switching activity. Therefore, we produce an bus energy saving method based on frequent values and multi-coding which can be used to further reduce the on-chip data bus switching activity. The simulation results show that our method can reduce the ratio of switching activity by 18.7% on the data bus lines, and obtain the maximum ratio of energy saving by 17.76% and the average ratio about 16.91%, with 70nm technology when the coupling factor &#x03BB; is 5. And the results also show that the method can still play a role when the technology size is further reduced in the future.
[Energy consumption, Switches, switching activity, coupling factor, bus energy saving, deep submicron technology, coupling capacitance, cache storage, frequent value encoding, bus energy consumption, System-on-chip, FV-MSB, multiprocessing systems, multicoding, Multicore processing, embedded multicore chip, microprocessor chips, Encoding, encoding, joint hybrid frequent value cache, Couplings, data bus energy saving, onchip buses, frequent value-most significant bit, frequent value, Capacitance, data bus energy consumption]
Large Page Address Mapping in Massive Parallel Processor Systems
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Large and sparse are the prominent characteristics of small-world graph. In many scientific domains, such as biomedical science and scientific computing, as small-word graph grows in scale, processing small-world graph poses severe challenges to address mapping in Massive Parallel Processing. Data driven computation, unstructured data organization, which are poor in spatial and temporal locality, which are high frequency of memory access, leads to large RAM footprint on address mapping management. This paper proposes a novel approach with special Implementation for massive parallel processors. In our technique, The block level address mapping table is stored in large pages in DDR3 memory. Considering the highly frequency in accessing memory, we maintain a big cache in RAM to store address mapping entries of data array recently searching. The search algorithm in searching the cache is binary search. The goal is to reduce address mapping overhead without excessively compromising system response time. This scheme is designed for our massive parallel coprocessor system. For reducing power consumption, we have an attempt to implement address mapping of each massive parallel coprocessor in Field-Programmable Gate Array(FPGA). The experiment have been conducted on a real System on chip(Soc). The result shows that when the number of processor node is 4096 and its frequency is 233MHz, The RAM cost is 2.4 MB in each processor, when there is missing, the largest response time is 160us, which is less than the mainstream software implementation in address translation. In the case of making full use of available storage resources, The hit ratio in graph problem could be achieve 100%.
[massive parallel processing, temporal locality, field programmable gate arrays, parallel architectures, graph theory, FPGA, Random access memory, system on chip, SoC, Massive Parallel Processing, cache storage, RAM footprint, Registers, coprocessors, field-programmable gate array, parallel processing, small-world graph, small-world, unstructured data organization, search problems, binary search, parallel coprocessor system, address translation, random-access storage, massive parallel processor systems, Indexes, block level address mapping table, software implementation, DDR3 memory, spatial locality, data driven computation, address mapping, Arrays, system-on-chip, large page address mapping, Field programmable gate arrays, Coprocessors, RAM]
Lightweight Dependency Checking for Parallelizing Loops with Non-Deterministic Dependency on GPU
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
General-purpose GPUs have been prevalent for a decade. Nevertheless, GPU programming remains an onerous job practically exclusive to veteran developers who must know both domain-specific knowledge and GPU architecture well. Although current parallelizing compilers that automatically parallelize and offload sizable loops onto the GPU have helped in unfettering the power of the GPU with minimal programming effort, there are still a family of loops that carry statically non-deterministic data dependencies and cannot be parallelized. To tackle this issue, we propose two lightweight dependency checking schemes that are very different from existing conservative compilers to assist parallelizing loops with non-deterministic data dependencies. Our schemes feature linear work complexity for memory operations, lower memory consumption compared to previous work, and minimal false positives by leveraging the lockstep execution on the GPU's SIMD lanes. Experiments done using microbenchmarking and real-life applications on the latest advanced AMD discrete GPUs show that our schemes can achieve 2.2 &#x00D7; speedup over existing solutions in dependency-free cases while only taking about 20% of time compared to existing solutions in the case with statically unproven loop-carried dependencies.
[parallelising compilers, Instruction sets, memory consumption, loop parallelization, Graphics processing units, parallelizing compilers, Dependency Checking, Programming, AMD discrete GPU, Registers, Complexity theory, parallel programming, GPGPU, general-purpose GPU, linear work complexity, Loop Parallelization, Kernel, nondeterministic data dependencies, GPU SIMD lanes, microbenchmarking, program control structures, lightweight dependency checking, graphics processing units, false positives, Code Generation;, lockstep execution, Memory management, memory operations]
Machine Learning Approach for the Predicting Performance of SpMV on GPU
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Sparse Matrix-Vector Multiplication (SpMV) kernel dominates the computing cost in numerous scientific applications. Many implementations based on different sparse formats were proposed recently for optimizing this kernel on the GPU side. Since the performance of the SpMV varies significantly according to the sparsity characteristics of the input matrix and the hardware features, developing an accurate performance model for this kernel is a challenging task. The traditional approach of building such models by analytical modeling is difficult in practice and requires a thorough understanding of the interaction between the GPU hardware and the sparse code. In this paper, we propose to use a machine learning approach to predict the performance of the SpMV kernel using several sparse formats (COO, CSR, ELL, and HYB) on GPU. We used two popular machine learning algorithms, Support Vector Regression (SVR) and Multilayer Perceptron neural network (MLP). Our experimental results on two different GPUs (Fermi GTX 512 and Maxwell GTX 980 Ti) show that the SVR models deliver the best accuracy with average prediction error ranging between 7% and 14%.
[Adaptation models, Support Vector Regression (SVR), Machine learning algorithms, mathematics computing, Graphics processing units, regression analysis, Performance modeling, Sparse matrices, GPU, MLP, SpMV kernel, SVR models, Kernel, linear algebra, multilayer perceptrons, support vector machines, Computational modeling, machine learning, graphics processing units, support vector regression, sparse matrix-vector multiplication, multilayer perceptron neural network, subroutines, Sparse Matrix-Vector multiplication (SpMV), Arrays, GPU computing, Multilayer Perceptron (MLP)]
Parallelizing Back Propagation Neural Network on Speculative Multicores
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Applications typically exhibit extremely different performance characteristics depending on the accelerator. Back propagation neural network (BPNN) has been parallelized into different platforms. However, it has not yet been explored on speculative multicore architecture thoroughly. This paper presents a study of parallelizing BPNN on a speculative multicore architecture, including its speculative execution model, hardware design and programming model. The implementation was analyzed with seven well-known benchmark data sets. Furthermore, it trades off several important design factors in coming speculative multicore architecture. The experimental results show that: (1) the BPNN performs well on speculative multicore platform. It can achieve similar speedup (17.7x to 57.4x) compared with graphics processors (GPU) while provides a more friendly programmability. (2) 64 cores' computing resources can be used efficiently and 4k is the proper speculative buffer capacity in the model.
[multiprocessing systems, Multicore processing, Instruction sets, speculative execution model, back propagation, Graphics processing units, multicore architecture, thread level speculation, Programming, backpropagation neural network, parallel programming, multicore, programming model, Neural networks, backpropagation, parallelized BPNN, hardware design, Hardware, neural nets, speculative multicore platform]
Selectively GPU Cache Bypassing for Un-Coalesced Loads
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
GPUs are widely used to accelerate general purpose applications, and could hide memory latency through massive multithreading. But multithreading can increase contention for the L1 data caches (L1D). This problem is exacerbated when an application contains irregular memory references which would lead to un-coalesced memory accesses. In this paper, we propose a simple yet effective GPU cache Bypassing scheme for Un-Coalesced Loads (BUCL). BUCL makes bypassing decisions at two granularities. At the instruction-level, when the number of memory accesses generated by a non-coalesced load instruction is bigger than a threshold, referred as the threshold of un-coalescing degree (TUCD), all the accesses generated from this load will bypass L1D. The reason is that the cache data filled by un-coalesced loads typically have low probabilities to be reused. At the level of each individual memory access, when the L1D is stalled, the accessed data is likely with low locality, and the utilization of the target memory sub-partition is not high, this memory access may also bypass L1D. Our experiments show that BUCL achieves 36% and 5% performance improvement over the baseline GPU for memory un-coalesced and memory coherent benchmarks, respectively, and also significantly outperforms prior GPU cache bypassing and warp throttling schemes.
[Data Cache, BUCL, Instruction sets, TUCD, Graphics processing units, Un-Coalesced Load Instruction, warp throttling schemes, cache storage, graphics processing units, memory access, GPU, Cache Bypassing, uncoalesced loads, selectively GPU cache bypassing, Multithreading, threshold of uncoalescing degree, Memory divergence, Computer architecture, Benchmark testing, bypassing decisions, Hardware, memory coherent benchmarks, Kernel]
Speeding Up Virtualized Transaction Logging with vTrans
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
In a virtualized environment, when multiple co-located relational database engines update their data files on a shared storage device simultaneously, the transaction log files, which be scattered within different guest image files, introduce the random logging I/O easily. To cope with this issue, we propose vTrans, a novel I/O driver of virtualized transaction log file. Generally, vTrans uses the split I/O driver design. In each guest operating system a front-end driver is added to identify the transaction logging semantic from specific database engine. At the hypervisor layer a dedicated back-end driver consolidates all transaction logging I/Os from guest font-end drivers and then persist them into a consecutive data area on the shared storage device, resulting in the relatively sequential logging I/O. We implement vTrans in a QEMU system which deployed MySQL InnoDB database engine. The experimental result shows that vTrans can effectively improve the performance of random logging I/O in a virtualized system.
[Performance evaluation, Cloud computing, Buffer storage, Relational databases, Disk I/O, transaction log files, front-end driver, virtualisation, guest image files, data files, guest operating system, Transaction log file, Engines, MySQL InnoDB database engine, virtualized transaction log file, random logging, Relational Database, shared storage device, vTrans, split I/O driver design, relational databases, Logging Performance, SQL, hypervisor layer, Virtual machine monitors, QEMU system, operating systems (computers), system monitoring, relatively sequential logging, multiple colocated relational database engines, Virtualization]
Study of Neocortex Simulations with GENESIS on High Performance Computing Resources
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
One significant challenge in neuroscience is understanding the cooperative behavior of large numbers of neurons. Models of neuronal networks allow scientists to explore the impact of differential neuronal connectivity using analysis techniques and information not available experimentally. However, modeling realistic neurobiological processes and encoding them in computer simulations is challenging, as increasing computing and data requirements are all of concern. In this work we study the performance of neocortex simulations using GEneral NEural SImulation System (GENESIS), a well-known multi-function brain simulation package, supported by high performance computing resources. The contribution of our work is threefold. First, we study the impact of platforms (i.e., single fat nodes versus high-end clusters) and their features on the performance and data generation for a small scale model of neocortex. Second, we assess the impact of the model complexity (i.e., number of cells and cell connectivity)on the performance and data generation for increasingly large versions of the neocortex model on high-end clusters. Third, we provide selected scientific results obtained by increasing the model complexity. We show that the more realistic and rigorous modeling of neocortex functions is computationally feasible but requires high performance computing resources to mitigate the growing computing and data requests of the associated simulations.
[Brain, Neural Simulations, High Performance Computing, general neural simulation system, I/O, Science, neocortex simulations, GENESIS, differential neuronal connectivity, parallel processing, cell connectivity, Mathematical model, Neocortex, Computational modeling, Biological system modeling, Neurons, neuroscience, Brain models, multifunction brain simulation package, Data, computer simulations, data generation, Analysis, Data models, neurophysiology, high-performance computing resources, neurobiological process]
System-Level Scalable Checkpoint-Restart for Petascale Computing
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Fault tolerance for the upcoming exascale generation has long been an area of active research. One of the components of a fault tolerance strategy is checkpointing. Petascale-level checkpointing is demonstrated through a new mechanism for virtualization of the InfiniBand UD (unreliable datagram) mode, and for updating the remote address on each UD-based send, due to lack of a fixed peer. Note that InfiniBand UD is required to support modern MPI implementations. An extrapolation from the current results to future SSD-based storage systems provides evidence that the current approach will remain practical in the exascale generation. This transparent checkpointing approach is evaluated using a framework of the DMTCP checkpointing package. Results are shown for HPCG (linear algebra), NAMD (molecular dynamics), and the NAS NPB benchmarks. In tests up to 32,752 MPI processes on 32,752 CPU cores, checkpointing of a computation with a 38 TB memory footprint in 11 minutes is demonstrated. Runtime overhead is reduced to less than 1%. The approach is also evaluated across three widely used MPI implementations.
[Checkpointing, application program interfaces, InfiniBand, Scalability, petascale level checkpointing, InfiniBand UD, MPI implementations, MPI, HPCG, unreliable datagram mode, Fault tolerance, Runtime, Libraries, Kernel, linear algebra, system level scalable checkpoint restart, Supercomputing, message passing, fault tolerance, Supercomputers, molecular dynamics, petascale computing, NAMD, fault tolerance strategy, Checkpoint-Restart, exascale generation, storage systems, virtualization mechanism]
Using Supercomputer to Speed up Neural Network Training
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Recent works in deep learning have shown that large models can dramatically improve performance. In this paper, we accelerated the deep network training using many GPUs. We have developed a framework based on Caffe called Caffe-HPC that can utilize computing clusters with multiple GPUs to train large models. Caffe[6] provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. And Caffe-HPC retains all the features of the original Caffe, the model trained on original Caffe can be continue to trained on Caffe-HPC. It provides a convenient solution for people who are using Caffe and want to speed up the training. Using an Asynchronous Stochastic Gradient Descent optimizer, We made a good acceleration on training a CNN model on ILSVRC[5] 2012 dataset. And we have compared the convergence of different SGD algorithms. We believe our work will makes it possible to train larger networks on larger training sets in a reasonable amount of time.
[supercomputer, parallel computation, Graphics processing units, supercomputing, parameter server, Servers, parallel machines, GPU, neural network, speed up neural network training, asynchronous stochastic gradient descent, Training, deep learning, stochastic processes, gradient methods, asynchronous stochastic gradient descent optimizer, mainframes, Computational modeling, graphics processing units, computing clusters, multimedia scientists, Neural networks, Machine learning, Acceleration, Caffe-HPC, neural nets]
A C-SVM Based Anomaly Detection Method for Multi-Dimensional Sequence over Data Stream
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Anomaly detection over multi-dimensional data stream has attracted considerable attention recently in various fields, such as network, finance and aerospace. In many cases, anomalies are composed of a sequence of multi-dimensional data, and it's necessary to detect this type of anomalies accurately and efficiently over data stream. Existing online methods of anomaly detection merely focus on the single-dimensional sequence. What's more, current studies about multi-dimensional sequence are mainly concentrated on static database. However, the anomaly detection for multi-dimensional sequence over data stream is much more difficult, due to the complexity of multidimensional sequence processing, the dynamic nature of data stream and the unbalance between normal and abnormal data. Facing these challenges, we propose an anomaly detection method for multi-dimensional sequence over data stream based on cost sensitive support vector machine (C-SVM) called ADMS. First, to improve the accuracy and efficiency, the ADMS transforms multi-dimensional sequences into feature vectors in a lossless way and prunes worthless features of these vectors. And then, the ADMS can detect abnormal sequences over dynamically imbalanced data stream by lively testing these vectors based on C-SVM. Experiments show that the false negative rate (FNR) of the ADMS is lower than 5%, the false positive rate (FPR) is lower than 7%, and the throughput is improved 42% by pruning worthless features. In addition, the AMDS performs well when there are concept drifts over the data stream.
[false positive rate, Feature selection, false negative rate, Heuristic algorithms, C-SVM, Transforms, multidimensional data stream, multidimensional data sequence, FPR, C-SVM-based anomaly detection method, FNR, Training, Data stream, Databases, abnormal sequence detection, abnormal data, ADMS, dynamically imbalanced data stream, feature selection, Testing, cost sensitive support vector machine, support vector machines, Multi-dimensional sequence, Anomaly detection, normal data, feature vectors, Concept drift, Hidden Markov models, Feature extraction, throughput improvement]
A Key Frame Selection Algorithm Based on Sliding Window and Image Features
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Network traffic associated with video increases sharply, how to choose the interested information for a number of Internet users is challenging. So, technologies and applications related with video, such as video search, video fast browsing, video index and storage are in great demand. Behind these technologies and applications, a core problem is how to quickly browse massive video data and obtain the main content of the video. To solve this problem, different key frame extraction algorithms have been proposed. Due to the diversity of video content, different video have different characteristics. So the design of general video key frame extraction algorithm to solve the problem is not the reality. The main trend for the problem is to design the key frame extraction algorithm based on the characteristics of the video itself. In this article, we mainly focus on videos with edited boundaries and shot conversions. Aiming at this kind of video, we have designed and implemented video key frame extraction algorithm based on sliding window, the global feature Gist and local feature point detection algorithm SURF. In this algorithm, we use Gist feature to construct the global scene information of frames, and the SURF keypoint detection algorithm to extract local keypoints as local feature for each frame. Then, shot segmentation based on sliding window and shot merging algorithm is applied to dividing the original video into several shots. After that, we select the most representative frames in each video shot as key frames. Finally we evaluate the result of the algorithm from the subjective and objective perspective. Results show that key frames extracted in the algorithm are of high quality and can basically cover the main content of the original video.
[Algorithm design and analysis, Visualization, global scene information, key frame extraction algorithms, shot merging algorithm, global feature point detection algorithm, shot segmentation, transforms, key-frame selection algorithm, video content, subjective perspective, Image color analysis, local feature point detection algorithm, feature extraction, image segmentation, Clustering algorithms, SURF keypoint detection algorithm, video key frames, video signal processing, edited-video boundaries, merging, Gist feature, sliding windowtatic video summarization, image features, sliding window, tatic video summarization, sliding windows, video characteristics, shot conversions, local keypoint extraction, Signal processing algorithms, Feature extraction, objective perspective, Detection algorithms]
Accelerating Deep Learning with Shrinkage and Recall
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Deep Learning is a very powerful machine learning model. Deep Learning trains a large number of parameters for multiple layers and is very slow when data is in large scale and the architecture size is large. Inspired from the shrinking technique used in accelerating computation of Support Vector Machines (SVM) algorithm and screening technique used in LASSO, we propose a shrinking Deep Learning with recall (sDLr) approach to speed up deep learning computation. We experiment shrinking Deep Learning with recall (sDLr) using Deep Neural Network (DNN), Deep Belief Network (DBN) and Convolution Neural Network (CNN) on 4 data sets. Results show that the speedup using shrinking Deep Learning with recall (sDLr) can reach more than 2.0 while still giving competitive classification performance.
[Convolution Neural Network (CNN), sDLr, CNN, Computational modeling, DNN, Deep Belief Network (DBN), Biological neural networks, Training, Support vector machines, shrinking deep learning with recall, DBN, classification performance, Training data, Machine learning, deep belief network, Acceleration, belief networks, learning (artificial intelligence), Deep Learning; Deep Neural Network (DNN), neural nets, deep neural network, convolution neural network]
An Energy-Efficient Implementation of LU Factorization on Heterogeneous Systems
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Energy consumption is increasingly becoming a critical issue in HPC. There is a broad consensus that future exascale-computing will be strongly constrained by energy consumption. Heterogeneous systems usually feature higher energy efficiency than homogeneous ones since the former employ coprocessors that provide higher GFlops/Watt than CPUs. Thus, it is of great importance to better utilize the coprocessors from an energy-efficiency standpoint. Dense LU factorization (LU) is a critical kernel that is widely used to solve dense linear algebra problems. However, existingheterogeneous implementations are typically designed to be CPU-centered, which rely highly on CPUs and thus suffer from large data transfer overheads via PCIe, hurting the energy efficiency of the entire computer system. We present a coprocessor-resident implementation of LU for a heterogeneous platform to improve energy efficiency without impeding performance by relieving the CPUs from performing unnecessary computations and reducing excessive data transfers via PCIe. In addition, several optimizations are judiciously employed to overlap the computation and communication between the CPUs and coprocessors. Validation on the Tianhe-2 supercomputer shows that our LU implementation gains higher performance, achieves higher energy efficiency, and features a better scalability than Intel MKL.
[Power demand, Scalability, LU factorization, mathematics computing, Tianhe-2 supercomputer, microprocessor chips, CPU, LU, coprocessors, matrix decomposition, Optimization, heterogeneous systems, coprocessor-resident implementation, power aware computing, Linear algebra, energy efficiency, heterogeneous system, Data transfer, Kernel, Coprocessors]
Analysis, Modeling, and Simulation of Hadoop YARN MapReduce
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Despite much work in simulation of MapReduce applications and in optimizing scheduling to minimize workload execution time, we still need models and tools to understand the performance of the individual tasks of a MapReduce application. Armed with this knowledge, we can characterize applications and model task operation to determine their performance in a given cluster setting. We have developed and analyzed a model of MapReduce tasks that abstracts their operation at a balanced level, avoiding too much detail that slows down analysis and requires knowledge of the inner workings of the cluster system, but at sufficient detail to be able to generalize to multiple application classes and cluster systems. The model, and an accompanying simulator, are evaluated against execution of real MapReduce applications, and show very promising results.
[Computational modeling, Hadoop, Containers, Yarn, parallel processing, Modeling, MapReduce, Analytical models, Fault tolerance, Simulation, data handling, Mathematical model, Resource management, cluster system, Hadoop YARN MapReduce]
Application-Level Determinism in Distributed Systems
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Deterministic and reproducible program execution eases the development and debugging of distributed systems. However, deterministic execution comes at high performance costs and is hard to achieve, especially when running on different hardware. In this paper we introduce the concept of application-level determinism and describe how the parallel programming model Spawn &amp; Merge can be used for scalable and deterministic distributed computation. Application-level deterministic applications yield reproducible deterministic results independent of the number of nodes participating in the computation, even though intermediate tasks may be executed in an unpredictable schedule. To achieve consistency independent of the order in which operations have been applied we present a new Operational Transformation algorithm, which mitigates the performance loss of introducing determinism with Spawn &amp; Merge. We show that such deterministic processing can scale across a cluster of compute nodes and discuss for which kind of workload the programming model is feasible. Furthermore, for high and low workloads, we evaluate the cost of adding determinism to be 28% and 40% higher than perfect parallel computation.
[program debugging, operational transformation algorithm, Roads, Application-level Determinism, Operational Transformation, Programming, deterministic program execution, parallel programming, determinism cost evaluation, Spawn &amp; Merge model, performance loss, Parallel processing, compute nodes cluster, scalable distributed computation, software performance evaluation, distributed system development, parallel algorithms, Computational modeling, Data structures, Reproducible Program Execution, Parallel Program Execution, Synchronization, reproducible program execution, Deterministic Distributed Systems, distributed system debugging, application-level determinism, parallel programming model, Data models]
Asynchronous Progress Design for a MPI-Based PGAS One-Sided Communication System
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Remote-memory-access models, also known as one-sided communication models, are becoming an interesting alternative to traditional two-sided communication models in the field of High Performance Computing. In this paper we extend previous work on an MPI-based, locality-aware remote-memory-access model with a asynchronous progress-engine for non-blocking communication operations. Most previous related work suggests to drive progression on communication through an additional thread within the application process. In contrast, our scheme uses an arbitrary number of dedicated processes to drive asynchronous progression. Further, we describe a prototypical library implementation of our concepts, namely DART, which is used to quantitatively evaluate our design against a MPI-3 baseline reference. The evaluation consists of micro-benchmark to measure overlap of communication and computation and a scientific application kernel to assess total performance impact on realistic use-cases. Our benchmarks shows, that our asynchronous progression scheme can overlap computation and communication efficiently and lead to substantially shorter communication cost in real applications.
[application program interfaces, asynchronous progress engine, MPI, asynchronous progression, parallel processing, Engines, remote memory access models, one sided communication models, one-sided, MPI-3 baseline reference, DART, data-locality, message passing, storage management chips, Computational modeling, asynchronous progress, communication cost, Indexes, PGAS one sided communication system, nonblocking communication operations, overlap, High performance computing, asynchronous progress design, Data transfer, high performance computing, Resource management, prototypical library implementation, Electronics packaging, locality aware remote memory access model]
CHIME: A Checkpoint-Based Approach to Improving the Performance of Shared Clusters
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Due to the limitation of resources, preemption frequently occurs in almost all the commercial cloud platforms, such as Google cluster and Amazon cluster. Since preemption can ensure that once the system is in heavy workload, high-priority tasks will be executed primarily and at the same time, some low-priority tasks will be killed immediately. Then when more resources are available, the killed tasks will restart to execute. Especially, during the peak time, some low-priority tasks could possibly be preempted and restarted repeatedly resulting in much more consuming precious resources including CPU cores, RAM and hard drives. Thanks to the checkpoint technology, it provides an efficient solution to addressing the preemption issue. But checkpoint technology has limitations, e.g., making checkpoint frequently will add redundant overhead to the cluster and cause I/O congestion. In this paper, by leveraging checkpoint technology, we designed a novel approach to improving the performance of shared clusters. Specifically, by checking the occupancy of resources periodically, making decisions to checkpoint or not and checkpointing for certain tasks, our method can reduce unnecessary checkpoints and exalt the performance of the whole cloud, especially tasks with low-priority. Extensive simulation experiments injecting tasks following the Google cloud trace logs were conducted to validate the superiority of our approach by comparing it with some baselines.
[Checkpointing, checkpointing, workstation clusters, Cloud computing, Google, Computational modeling, Priority, checkpoint policy with combined waiting list method, Shared Cluster, CHIME, Occupancy of Resources, Fault tolerance, Checkpoint, Cloud Computing, Memory management, Time factors, cloud computing, shared clusters]
CPU Frequency Tuning to Improve Energy Efficiency of MapReduce Systems
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Energy efficiency is a major concern in today's data centers that house large scale distributed processing systems such as data parallel MapReduce clusters. Modern power aware systems utilize the dynamic voltage and frequency scaling mechanism available in processors to manage the energy consumption. In this paper, we initially characterize the energy efficiency of MapReduce jobs with respect to built-in power governors. Our analysis indicates that while a built-in power governor provides the best energy efficiency for a job that is CPU as well as IO intensive, a common CPU-frequency across the cluster provides best the energy efficiency for other types of jobs. In order to identify this optimal frequency setting, we derive energy and performance models for MapReduce jobs on a HPC cluster and validate these models experimentally on different platforms. We demonstrate how these models can be used to improve energy efficiency of the machine learning MapReduce applications running on the Yarn platform. The execution of jobs at their optimal frequencies improves the energy efficiency by average 25% over the default governor setting. In case of mixed workloads, the energy efficiency improves by up to 10% when we use an optimal CPU-frequency across the cluster.
[Energy consumption, Predictive Models, data centers, Predictive models, Electronic mail, parallel processing, dynamic voltage, IO intensive, MapReduce, built-in power governors, Program processors, machine learning applications, Distributed databases, HPC cluster, learning (artificial intelligence), energy efficiency improvement, Yarn platform, microprocessor chips, data parallel MapReduce clusters, CPU-Frequency Tuning, Energy Efficiency, Tuning, power aware systems, pattern clustering, frequency scaling mechanism, Data models, large scale distributed processing systems, data handling, Distributed Computing, CPU frequency tuning, MapReduce jobs]
Energy Proportionality in Heterogeneous Data Center Supporting Applications with Variable Load
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
The increasing number of data centers raises serious concerns regarding their energy consumption. Although servers have become more energy-efficient over time, their idle consumption remains high, which is an issue as resources in data centers are often over-provisioned. This work proposes a novel approach for building data centers so that their energy consumption is proportional to load. A data center hence comprises heterogeneous machines carefully chosen for their performance and energy efficiency ratios. We focus on web applications whose load varies over time and design a scheduler that dynamically reconfigures the infrastructure to minimize its energy consumption according to current load and application requirements. Based on load forecasts, it takes reconfiguration decisions and performs actions such as migrating applications and switching machines on or off. The approach is evaluated considering a data center with heterogeneous resources, and the experiments show how to adjust the parameters of scheduling policies to save the most energy while satisfying Quality of Service (QoS) constraints.
[Energy consumption, heterogeneous resources, heterogeneous data center, Web applications, load requirements, Heterogeneous Infrastructure, Servers, application requirements, Variable Load Applications, Energy Proportionality, Program processors, power aware computing, Computer architecture, heterogeneous machines, energy efficiency-performance ratio, quality of service constraints, variable load applications, Power demand, Architecture, Buildings, QoS constraints, quality of service, computer centres, energy consumption minimization, energy proportionality, load forecasting, dynamic infrastructure reconfiguration, Dynamic Provisioning]
Evaluation of Flash-Based Out-of-Core Stencil Computation Algorithms for SSD-Equipped Clusters
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
This paper proposes a new scheme for solving data size requirements for a large-scale stencil computation, which are greater than the total size of the main memories of nodes in a cluster. It utilizes distributed flash SSDs over cluster nodes as an extension to the main memory with a locality-aware algorithm. Three algorithms with a different hierarchical blocking scheme for three memory tiers, namely, flash SSD, DRAM, and cache, are proposed, and they are evaluated in different platforms and flash devices. They utilize not only highly parallel asynchronous input/output in flash SSDs, but also appropriate blocking parameters by using an auto-tuning system named Blk-Tune. They also overcome the performance degradation caused by the non-uniform memory architecture (NUMA). The optimized algorithms for single nodes are extended for multi-nodes and evaluated in a cluster with traditional SATA SSDs, as well as with state-of-the-art flash devices, such as low-power and cost-effective M.2 NVMe flash SSDs. With the use of our scheme and distributed flash devices in a cluster, large-scale stencil problems can be solved with a limited number of nodes and a moderate size of main memories.
[Decision support systems, Blk-Tune, Conferences, Random access memory, memory hierarchy, cache storage, temporal blocking, out-of-core, Flash memories, hierarchical blocking scheme, mobile computing, optimisation, flash memories, Non-volatile memory, DRAM chips, large-scale stencil computation, stencil, locality-aware algorithm, flash-based out-of-core stencil computation algorithm, asynchronous I/O, access locality, nonuniform memory architecture, flash memory, SSD-equipped cluster, Tuning, memory architecture, Layout, SSD-equipped clusters, NUMA, auto-tuning]
ICE: A General and Validated Energy Complexity Model for Multithreaded Algorithms
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Like time complexity models that have significantly contributed to the analysis and development of fast algorithms, energy complexity models for parallel algorithms are desired as crucial means to develop energy efficient algorithms for ubiquitous multicore platforms. Ideal energy complexity models should be validated on real multicore platforms and applicable to a wide range of parallel algorithms. However, existing energy complexity models for parallel algorithms are either theoretical without model validation or algorithm-specific without ability to analyze energy complexity for a wide-range of parallel algorithms. This paper presents a new general validated energy complexity model for parallel (multithreaded) algorithms. The new model abstracts away possible multicore platforms by their static and dynamic energy of computational operations and data access, and derives the energy complexity of a given algorithm from its work, span and I/O complexity. The new model is validated by different sparse matrix vector multiplication (SpMV) algorithms and dense matrix multiplication (matmul) algorithms running on high performance computing (HPC) platforms (e.g., Intel Xeon and Xeon Phi). The new energy complexity model is able to characterize and compare the energy consumption of SpMV and matmul kernels according to three aspects: different algorithms, different input matrix types and different platforms. The prediction of the new model regarding which algorithm consumes more energy with different inputs on different platforms, is confirmed by the experimental results. In order to improve the usability and accuracy of the new model for a wide range of platforms, the platform parameters of ICE model are provided for eleven platforms including HPC, accelerator and embedded platforms.
[Algorithm design and analysis, SpMV algorithms, parallel algorithms, multi-threading, matmul algorithms, Computational modeling, Heuristic algorithms, ideal cache energy, Ice, Complexity theory, Parallel algorithms, HPC platforms, Analytical models, matrix multiplication, energy complexity models, multithreaded algorithms, power-aware and green computing, high performance computing, ICE model, energy complexity, energy models, dense matrix multiplication, computational complexity, sparse matrix vector multiplication]
Parallel Gene Upstream Comparison via Multi-Level Hash Tables on GPU
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
The region of DNA immediately in front of a gene body (also called upstream region) contains short (8-20 base) sequence motifs that help to control when that gene is turned on and off. Unfortunately, these motifs are generally unknown and commonly degenerate. In this work, we propose a motif-finding framework that, given a set of gene upstream regions, performs their all-to-all pairwise comparison and identifies all the motifs of length k (k-mers) that are common to any pair of upstream regions or differ in at most d characters. Our framework stores the k-mers found in each gene in a multi-level hash table. Our hash table design optimizes hash table comparison (rather than hash table insertion or lookup), is highly parallelizable and easily maps onto GPU. We propose four GPU kernels for pairwise hash table comparison, each leveraging a distinct parallelization approach. We study how different factors (the hash function, the number of buckets and the settings of other implementation-specific parameters) affect the performance of our implementation. Experimental results performed using an average-size yeast genome show that our fastest GPU kernel outperforms an 8-thread, cache-efficient CPU implementation by a factor of ~52x.
[computational biology, Motif Finding, Hamming distance, Radiation detectors, Genomics, Graphics processing units, Encoding, graphics processing units, parallel processing, GPU, GPU kernels, parallel gene upstream comparison, genetics, multilevel hash tables, pairwise hash table comparison, biology computing, DNA, Hash Tables, motif-finding framework, data structures, Bioinformatics]
PFrauDetector: A Parallelized Graph Mining Approach for Efficient Fraudulent Phone Call Detection
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
In recent years, fraud is becoming more rampant internationally with the development of modern technology and global communication. Due to the rapid growth in the volume of call logs, the task of fraudulent phone call detection is confronted with Big Data issues in real-world implementations. While our previous work, FrauDetector, has addressed this problem and achieved some promising results, it can be further enhanced as it focuses on the fraud detection accuracy while the efficiency and scalability are not on the top priority. Meanwhile, other known approaches suffer from long training time and/or cannot accurately detect fraudulent phone calls in real time. In this paper, we propose a highly-efficient parallelized graph-mining-based fraudulent phone call detection framework, namely PFrauDetector, which is able to automatically label fraudulent phone numbers with a "fraud" tag, a crucial prerequisite for distinguishing fraudulent phone call numbers from the normal ones. PFrauDetector generates smaller, more manageable sub-networks from the original graph and performs a parallelized weighted HITS algorithm for significant speed acceleration in the graph learning module. It adopts a novel aggregation approach to generate the trust (or experience) value for each phone number (or user) based on their respective local values. We conduct a comprehensive experimental study based on a real dataset collected through an anti-fraud mobile application, Whoscall. The results demonstrate a significantly improved efficiency of our approach compared to FrauDetector and superior performance against other major classifier-based methods.
[Fraudulent Phone Call Detection, parallel algorithms, fraud tag, Parallelized Weighted HITS Algorithm, data mining, Whoscall, hyperlink-induced topic search, parallelized graph-mining, anti-fraud mobile application, Telecommunication Fraud, Training, Computer science, mobile computing, Trust Value Mining, Training data, fraud, Parallel processing, Feature extraction, Communications technology, Real-time systems, fraudulent phone call detection, PFrauDetector, graph learning module, parallelized weighted HITS algorithm]
PIE: A Pipeline Energy-Efficient Accelerator for Inference Process in Deep Neural Networks
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
It has been a new research hot topic to speed up the inference process of deep neural networks (DNNs) by hardware accelerators based on field programmable gate arrays (FPGAs). Because of the layer-wise structure and data dependency between layers, previous studies commonly focus on the inherent parallelism of a single layer to reduce the computation time but neglect the parallelism between layers. In this paper, we propose a pipeline energy-efficient accelerator named PIE to accelerate the DNN inference computation by pipelining two adjacent layers. Through realizing two adjacent layers in different calculation orders, the data dependency between layers can be weakened. As soon as a layer produces an output, the next layer reads the output as an input and starts the parallel computation immediately in another calculation method. In such a way, computations between adjacent layers are pipelined. We conduct our experiments on a Zedboard development kit using Xilinx Zynq-7000 FPGA, compared with Intel Core i7 4.0GHz CPU and NVIDIA K40C GPU. Experimental results indicate that PIE is 4.82x faster than CPU and can reduce the energy consumptions of CPU and GPU by 355.35x and 12.02x respectively. Besides, compared with the none-pipelined method that layers are processed in serial, PIE improves the performance by nearly 50%.
[parallel computation, field programmable gate arrays, inference, Pipelines, DNN, FPGA, layer-wise structure, hardware accelerators, Xilinx Zynq-7000 FPGA, Zedboard development kit, deep neural networks, accelerator, computation time reduction, Neurons, none-pipelined method, inference mechanisms, PIE, Biological neural networks, Pipeline processing, calculation orders, pipeline energy-efficient accelerator, pipeline, data dependency, NVIDIA K40C GPU, inference process, Acceleration, Field programmable gate arrays, neural nets, Intel Core i7 CPU]
Reverse Engineering of Dynamic Parallel Program Behavior from Execution Traces
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Trace-driven simulation has been used widely for architectural exploitation in electronic system-level (ESL) designs for complex SoCs (system-on-chips). Most often the trace files are provided by third parties without source code. This makes it difficult to process and manipulate the trace files, e.g. to identify the region of interest (ROI), for efficient and more focused simulations. It is thus necessary to deduce high-level structures and patterns in the trace to facilitate such manipulations. This can be viewed as a reverse engineering process to derive the dynamic control flow of the original program from its execution trace. Furthermore, after the high-level structure is manipulated, it is also important to generate a new trace from the resultant structure that preserves the characteristics of the original trace for effective trace-driven simulations. The problem becomes even more difficult if the trace was generated from a parallel program because of the complex interactions between the multiple threads of executions. In this paper, a novel scheme to reverse-engineering the program execution trace is proposed, which can obtain the high-level dynamic control structure of the original parallel program as well as regenerate a similar trace from the derived structure, all without referencing the source code. The effectiveness of the proposed scheme is evaluated and verified with extensive experiments.
[electronic system-level designs, complex SoC, dynamic control flow, Reverse engineering, high-level dynamic control structure, circuit simulation, region of interest identification, ROI identification, trace files, Optimization, system-on-chips, program execution trace, Execution traces, integrated circuit design, System-on-chip, Message systems, ESL, multi-threading, Instruments, finite-state machine, Process control, dynamic parallel program behavior, reverse engineering, electronic engineering computing, trace-driven simulation, architectural exploitation, Computer science, trace patterns, parallel program, system-on-chip]
Scalable Single-Source SimRank Computation for Large Graphs
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
SimRank is an effective similarity measure between vertices in a graph, which has become a fundamental technique in graph analytics. Despite its popularity, computation of SimRank is often costly in both space and time, especially with the ever growing scale of graph data nowadays. In this paper, we focus on the computation of Single-Source SimRank: given a query vertex, return the similarities between this vertex and any other vertices in the graph. The traditional centralized SimRank algorithms are not efficient for this problem. To fully utilize the computing power of modern distributed systems, we propose sssSimRank, an efficient distributed algorithm based on the random walk model. Our algorithm achieves scalability via minimizing the total number, the space cost, and the matching time of random walks. We implement our approach on the popular distributed processing platform Spark. Experimental results demonstrate the effectiveness, efficiency and scalability of our method.
[Algorithm design and analysis, similarity measure, graph theory, query vertex, large graphs, sssSimRank algorithm, Optimization, query processing, random walk, Spark platform, graph vertices, distributed systems, Iterative methods, big data, graph analytics, data analysis, matching time, Computational modeling, Data processing, scalable single-source SimRank computation, Sparks, distributed algorithm, space cost, distributed algorithms, random walk model, SimRank, Spark, Time complexity]
suCAQR: A Simplified Communication-Avoiding QR Factorization Solver Using the TBLAS Framework
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
The scope of this paper is to design and implement a scalable QR factorization solver that can deliver the fastest performance for tall and skinny matrices and square matrices on modern supercomputers. The new solver, named scalable universal communication-avoiding QR factorization (suCAQR), introduces a simplified and tuning-less way to realize the communication-avoiding QR factorization algorithm to support matrices of any shapes. The software design includes a mixed usage of physical and logical data layouts, a simplified method of dynamic-root binary-tree reduction, and a dynamic dataflow implementation. Compared with the existing communication avoiding QR factorization implementations, suCAQR has the benefits of being simpler, more general, and more efficient. By balancing the degree of parallelism and the proportion of faster computational kernels, it is able to achieve scalable performance on clusters of multicore nodes. The software essentially combines the strengths of both synchronization-reducing approach and communication-avoiding approach to achieve high performance. Based on the experimental results using 1,024 CPU cores, suCAQR is faster than DPLASMA by up to 30%, and faster than ScaLAPACK by up to 30 times.
[Algorithm design and analysis, Shape, synchronization-reducing approach, Heuristic algorithms, mathematics computing, software design, Memory, matrix decomposition, supercomputers, dynamic-root binary-tree reduction, computational science application, multicore nodes, tall and skinny matrices, computational kernels, tree data structures, Kernel, scalable performance, mainframes, TBLAS framework, performance analysis and optimization, logical data layouts, CPU cores, performance evaluation, dataflow runtime system, suCAQR, physical data layouts, Computer science, square matrices, scalable QR factorization solver, dynamic dataflow implementation, Layout, data flow computing, scalable universal communication-avoiding QR factorization, high performance computing, simplified communication-avoiding QR factorization solver]
Tapas: An Implicitly Parallel Programming Framework for Hierarchical N-Body Algorithms
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Tapas is our new C++ programming framework for hierarchical algorithms such as N-body, on large scale heterogeneous supercomputers. Although N-body and their variants are widely used in scientific applications, their correct implementations are often difficult on such modern machines, as the algorithms are irregular, complex, and involve explicit task parallel programming over distributed nodes. Encapsulating the complexities in a library or a framework has been challenging due to irregular data access over massively distributed memory. Tapas solves this by converting the users clean implicit-style parallel program into an inspector-executor style code on heterogeneous multi-core, multi-node environment solely by the use of C++ template metaprogramming. A prototype implementation of the Fast Multipole Method on Tapas demonstrates a comparable performance and scaling as ExaFMM, the fastest hand-tuned implementation of FMM, as well as efficient usage of hundreds of GPUs. Specifically, the serial performance is 95% of ExaFMM, whereas the distributed-memory strong-scaling evaluation using up to 1500 CPU cores demonstrates 64% to 81% of the ExaFMM performance. The multi-GPU version of the Tapas-based FMM achieves a 5.15x speedup when executed on 100 nodes of TSUBAME2.5 with 300 GPUs.
[Algorithm design and analysis, distributed nodes, Force, distributed memory strong scaling evaluation, implicitly parallel programming framework, Programming, distributed processing, fast multipole method, scientific applications, Programming Framework, parallel programming, GPGPU, multiGPU version, Fast Multipole Method, N-body algorithms, multinode environment, Libraries, irregular data access, C++ programming framework, heterogeneous multicore, C++ languages, C++ language, graphics processing units, Standards, prototype implementation, inspector executor style code, Tapas, Approximation algorithms, C++ template metaprogramming, hierarchical N-body algorithms, parallel program, Hierarchical algorithms]
Timed Dataflow: Reducing Communication Overhead for Distributed Machine Learning Systems
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Many distributed machine learning (ML) systems exhibit high communication overhead when dealing with big data sets. Our investigations showed that popular distributed ML systems could spend about an order of magnitude more time on network communication than computation to train ML models containing millions of parameters. Such high communication overhead is mainly caused by two operations: pulling parameters and pushing gradients. In this paper, we propose an approach called Timed Dataflow (TDF) to deal with this problem via reducing network traffic using three techniques: a timed parameter storage system, a hybrid parameter filter and a hybrid gradient filter. In particular, the timed parameter storage technique and the hybrid parameter filter enable servers to discard unchanged parameters during the pull operation, and the hybrid gradient filter allows servers to drop gradients selectively during the push operation. Therefore, TDF could reduce the network traffic and communication time significantly. Extensive performance evaluations in a real testbed showed that TDF could reduce up to 77% and 79% of network traffic for the pull and push operations, respectively. As a result, TDF could speed up model training by a factor of up to 4 without sacrificing much accuracy for some popular ML models, compared to systems not using TDF.
[network communication, push operation, Servers, distributed ML systems, Machine Learning, Optimization, Training, Training data, pulling parameters, network traffic reduction, Bandwidth, TDF, learning (artificial intelligence), hybrid parameter filter, Open area test sites, Distributed System, Parameter Server, timed dataflow, performance evaluation, Communication Overhead, distributed machine learning systems, timed parameter storage system, Synchronization, pull operation, communication overhead reduction, data flow computing, drop gradients, hybrid gradient filter, communication time reduction, pushing gradients]
Towards Scalable Subgraph Pattern Matching over Big Graphs on MapReduce
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Big graph-structured data pervade our world, ranging from microworld such as gene regulatory networks to macroworld such as social networks. Subgraph matching is a fundamental operation for many graph applications, such as graph database and graph mining. However, existing sequential algorithms have limited applicability on large graphs because of the inherent NP-completeness of subgraph isomorphism and distributed graph storage. Therefore, there is a need to parallelize subgraph matching over big graph data in a distributed environment. With MapReduce as the backdrop, this paper proposes a new approach, named ParMa, for efficient subgraph matching on distributed platforms. It consists of alternate computation and communication phases. We first build a cost model and then propose approaches to optimize the execution process. Instead of existing parallel approaches which only considers intermediate result size, the proposed cost model takes the number of iteration invocations as the primary cost. Based on this, our optimizations mainly focus on the aspects that affects iteration number throughout the execution of matching. One is query decomposition. We propose an effective query decomposition approach to minimize the number of subqueries and their matches. The other is join processing. We introduce a suite of mechanisms, including join plan making, local join processing and join cost estimation, to join partial matches in an appropriate way to reduce its cost. Finally, our extensive experiments on both synthetic and real graphs demonstrated that ParMa outperforms the state-of-the-art solutions by considerable margins.
[join plan making, iterative methods, big graph-structured data, pattern matching, graph applications, graph theory, parallel processing, Optimization, MapReduce, subgraph isomorphism, local join processing, query processing, scalable subgraph pattern matching, Subgraph Matching, storage management, gene regulatory networks, distributed graph storage, Distributed databases, synthetic graphs, real graphs, ParMa, distributed platforms, Computational modeling, Social network services, Estimation, Graph Processing, Big Data, join cost estimation, inherent NP-completeness, parallelize subgraph matching, iteration invocations, Parallel Solution, graph mining, graph database, Distance measurement, query decomposition, Pattern matching, computational complexity]
Towards Seamless Resynchronization for Active-Active Database Clustering
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Database clustering is a well-established technology to improve the availability and scalability of a database service. Query replication and log replication are two popular methods for propagating database updates across a cluster of database servers. An important issue that received relatively scant attention and is the focus of this paper is how to add a new server to an active-active database cluster with minimum service disruption while maximizing the cluster's request processing concurrency during run time. Even though log replication allows attaching a new database server without stopping the attached database cluster's service, the asynchronous nature of its operation is incompatible with active-active clusters, where every cluster node is designed to share the load of servicing incoming read query queries. In contrast, query replication is synchronous and thus could readily support active-active clustering, but most existing query replication implementations need to halt a cluster's service when attaching a new server. In this paper, we present a database resynchronization scheme that achieves the best of both worlds: leveraging log replication to minimize the service disruption time associated with addition of new servers, while applying query replication during run time to maximize the parallelism of read query processing, and demonstrate its effectiveness in a product-grade database engine, PostgreSQL.
[High Availability, log replication, database resynchronization scheme, active-active database clustering, seamless resynchronization, Online Recovery, Active-Active Architecture, Servers, Synchronization, Database Resynchronization, synchronisation, query processing, Query processing, pattern clustering, Distributed databases, Database Clustering, Parallel processing, Load management, query replication]
Understanding Software Platforms for In-Memory Scientific Data Analysis: A Case Study of the Spark System
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Over the last five years, Apache Spark has become a major software platform for in-memory data analysis. Acknowledging its widespread use, we present a comprehensive study of system characteristics of Spark with a focus on scientific data analytics performing large-scale matrix operations. We compare its performance to SciDB, a disk-based platform for array data analysis. A benchmark, ArrayBench, is developed to evaluate the performance of matrix processing for scientific data analytics. ArrayBench is applied to data from a real biological workflow whose data inputs are in matrix form. Herein, we report the findings, which shed light on the improvement of Spark and SciDB and future development of large-scale scientific data analytics.
[Data analysis, data analysis, In-memory, Biology, software platforms, Sparks, Servers, matrix algebra, in-memory scientific data analysis, real biological workflow, large-scale matrix operations, apache spark, SciDB, Benchmark testing, Software, ArrayBench, Scientific Data Analytics, Spark, Arrays, disk-based platform]
A Categorical Approach in Handling Event-Ordering in Distributed Systems
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
The issue of event-ordering in distributed systems is crucial an connected to threat management. In this paper, we investigate the use of modifications for handling event-ordering. We employ category theory to strike a balance between Lamport clocks, which enforce global order but lose information about causality, and Vector clocks, which have more precise causality information but do not scale well. We extend previous work on Godement calculus and cartesian closed comma categories for information security management. Finally, we propose a framework for implementing our approach in the detection of threats and attacks in communication systems.
[communication systems, Conferences, event-ordering handling, event-ordering, distributed processing, threat management, threat detection, Lamport clocks, Calculus, categorical approach, Electronic mail, History, global order, Integrated circuits, Godement calculus, security of data, cartesian closed comma categories, attack detection, category theory, distributed systems, information security management, Clocks]
A Molecular Computation Model to Compute Inversion over Finite Field GF(2n)
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
With the advent of DNA computing, some traditional scientific disciplines present new developing orientations. One of them is DNA-based cryptography, a new developing interdisciplinary area which combines cryptography, mathematical modeling, biochemistry and molecular biology. There are some questions worth study that how to implement the arithmetic operations used in cryptosystem based on DNA computing. This paper proposes a DNA computing model to show how to calculate inversion over finite field GF(2n) with DNA self-assembly. 4567 types of computation tiles with 7 different functions assemble into the seed configuration with inputs to figure out the solution. The assembly time complexity is &#x0398;(n2) and the space complexity is &#x0398;(n4).
[assembly time complexity, biocomputing, Conferences, molecular computation model, DNA self-assembly, Inversion, Finite field GF(2n), GF(2n), inversion over finite field, DNA computing model, space complexity, computational complexity, DNA computing]
A Secure and Reliable Hybrid Model for Cloud-of-Clouds Storage Systems
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
With the maturity of the cloud infrastructure, many data are moved to the cloud. However, availability and security of the cloud data still remain the major concerns. To relieve these concerns, it has been proposed to disperse encoded data redundantly across multiple independent cloud providers, so that at least a certain number of data fragments are required for data recovery, thus called cloud-of-clouds (CoC) systems. As for system performance, there is always a trade-off among sufficient security or high availability with different settings on the number of fragments and the threshold value. Here in this paper we extend the scenario and propose a comprehensive hybrid model that provides a holistic framework taking into account the diverse reliability and security characteristics of the storage providers in a CoC system to enhance the enforcement of data protection. The availability and security of the system in our model is analyzed theoretically in a systematic way. Simulation results also show that with our model, by exploiting the asymmetrical characters of the CoC system, both security and availability can be achieved satisfactorily.
[cloud-of-clouds, dispersal method, Conferences, secure-reliable hybrid model, availability, CoC system, comprehensive hybrid model, security, data recovery, threshold value, data protection, cloud infrastructure, cloud providers, data fragments, cloud computing, cloud-of-clouds storage systems, data distributed system]
Coordinated Broadcast-Based Request-Reply and Group Management for Tightly-Coupled Wireless Systems
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
As the domain of cyber-physical systems continues to grow, an increasing number of tightly-coupled distributed applications will be implemented on top of wireless networking technologies. Some of these applications, including collaborative robotic teams, work in a coordinated fashion, whereby a distinguished node takes control decisions and sends commands to other nodes, which in turn perform the requested action/operation and send back a reply/acknowledgment. The implementation of such interactions via reliable point-to-point flows may lead to a significant performance degradation due to collisions, especially when the system operates close to the capacity of the communication channel. We propose a coordinated protocol which exploits the broadcast nature of the wireless medium in order to support this application-level interaction with a minimal number of message transmissions and predictable latency. The protocol also comes with group management functionality, allowing new processes to join and existing processes to leave the group in a controlled way. We evaluate a prototype implementation over WiFi, using a simulated setup as well as a physical testbed. Our results show that the proposed protocol can achieve significantly better performance compared to point-to-point approaches, and remains fully predictable and dependable even when operating close to the wireless channel capacity.
[point-to-point approaches, Protocols, tightly-coupled wireless systems, coordinated protocol, reliability, communication channel, predictable latency, Throughput, application-level interaction, broadcast communication, coordinated protocols, Wireless communication, message transmissions, wireless networking technologies, tightly-coupled distributed applications, control decisions, wireless channels, protocols, wireless systems, telecommunication control, Process control, request/reply interaction, wireless channel capacity, Ad hoc networks, predictability, group communication, coordinated broadcast-based request-reply, cyber-physical systems, wireless medium, group management, performance, Reliability, wireless LAN, WiFi, IEEE 802.11 Standard, telecommunication traffic, collaborative robotic teams]
Dual LWE-Based Fully Homomorphic Encryption with Errorless Key Switching
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Cloud computing raises new challenges for how to protect user privacy. Fully homomorphic encryption is one way to solve the problem. In this paper, we show a useful property of Dual-LWE assumption to construct Key-Switching procedure compared with LWE assumption without extra error term. Hence, we propose to construct "Errorless Key Switching" fully homomorphic encryption (FHE) scheme based on dual learning with errors (Dual-LWE) assumption. Specifically, we compile the Dual-LWE problem proposed by Gentry et.al. at STOC2008 and First-is-errorless LWE (Ferr.LWE) problem proposed by Brakerski et al. at STOC2013 into Dual-First-is-errorless LWE (Dual-Ferr.LWE) problem. Then, utilizing Dual-Ferr.LWE assumption to construct a various GPV(vGPV) scheme, and we use vGPV scheme as the fundamental building block to construct FHE with errorless key switching scheme. Lastly, under the assumption of decisional learning with errors(DLWE), we prove that our scheme is CPA secure.
[Cloud computing, Switches, dual learning with errors, cryptography, dual-first-is-errorless LWE, Dual-Ferr.LWE, Encryption, Zinc, Standards, first-is-errorless LWE, Dual learning with errors, Fully homomorphic encryption, fully homomorphic encryption, Public key, DLWE, errorless key switching, decisional learning with errors, Errorless Key Switching, cloud computing, learning (artificial intelligence), dual-LWE assumption]
IRMD: Malware Variant Detection Using Opcode Image Recognition
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Malware detection becomes mission critical as its threats spread from personal computers to industrial control systems. Modern malware generally equips with sophisticated anti-detection mechanisms such as code-morphism, which allows the malware to evolve into many variants and bypass traditional code feature based detection systems. In this paper, we propose to disassemble binary executables into opcodes sequences, and then convert the opcodes into images. By using convolutional neural network to compare the opcode images generated from binary targets with the opcode images generated from known malware sample codes, we can detect if the target binary executables is malicious. Theoretical analysis and real-life experiments results show that malware detection using visualized analysis is comparable in terms of accuracy, our approach can significantly improve 15% of detection accuracy when the detection set contains a large quantity of binaries and the training set is much smaller.
[binary targets, invasive software, Image recognition, opcode image recognition, Visualized analysis, Malware detection, Image reconstruction, malware sample codes, Opcode images, Training, Histograms, feature extraction, industrial control systems, Malware, image sequences, IRMD, convolutional neural network, Convolutional neural network, personal computers, antidetection mechanisms, code feature based detection systems, Neural networks, malware variant detection, visualized analysis, Feature extraction, code-morphism, image coding, image recognition, opcode sequences, feedforward neural nets]
MrBayes 3.2.6 on Tianhe-1A: A High Performance and Distributed Implementation of Phylogenetic Analysis
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Phylogenetic analysis has achieved extraordinary results in domains like species delimitation and evolutionary biology. An essential element behind this success has been the introduction of high performance computing techniques in the step of estimating the phylogenetic likelihoods. This paper describes the design and implementation of a distributed and CPU-GPU based heterogeneous computing system on parallelizing the analysis. The parallelization has been implemented in the state-of-the-art version of MrBayes, a widespread phylogeny reconstruction program. We benchmarked the method and another two GPU-based methods by using 8 distributed computing nodes on Tianhe-1A. The experimental results indicate that the proposed method outstrips BEAGLE and the nMC3 method by speedup factors of up to 1.98&#x00D7; and 1.68&#x00D7;, respectively. In comparison to the serially implemented MrBayes, a peak speedup of 188&#x00D7; is finally achieved by using 8 Tesla M 2050 GPUs. The proposed method is publicly available to facilitate further research on phylogenetic analysis.
[distributed computing nodes, phylogeny reconstruction program, Graphics processing units, Probability, Phylogeny, heterogenous computing, graphics processing units, GPU, distributed computing, CPU-GPU, high performance computing techniques, evolutionary computation, phylogenetic analysis, evolutionary biology, distributed implementation, phylogenetic likelihoods, Vegetation, MrBayes 3.2.6 on Tianhe-1A, Benchmark testing, high performance computing, Acceleration]
Resources Renting with Reserved and On-Demand Instances for Cloud Workflow Applications
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Cloud computing enables users to access different resources conveniently based on the "pay-as-you-go" model. However, the unit cost of this on-demand manner are usually higher than the reserved ones. Reallocating some high-usage on-demand instances to reserved instances can save considerable costs when renting resources from the cloud. It is a big challenge to determine the appropriate amount of reserved and on-demand instances in terms of users' requirements. In this paper, we consider deadline constrained workflow scheduling problems to minimize total renting costs with both reserved and on-demand instances. An integer programming model is constructed for the problem under study. A Precedence Tree based Heuristic (PTH) is developed which includes a dynamic initial schedule construction methods. Based on the initial schedule, an improvement procedure is presented. The proposed methods are compared with existing algorithms for the related makespan based workflow scheduling problem. Experimental and statistical results demonstrate the effectiveness and efficiency of the proposed algorithm.
[Cloud computing, Schedules, deadline constrained workflow scheduling, integer programming, PTH, Reserved and on-demand instances, dynamic initial schedule construction methods, Workflow Scheduling, resource allocation, Cloud Computing, user requirements, precedence tree based heuristic, Resource Allocation, high-usage on-demand instances, scheduling, resource renting, cloud computing, total renting cost minimization, Computational modeling, trees (mathematics), Dynamic scheduling, Virtual machining, integer programming model, makespan based workflow scheduling, cloud workflow applications, rental, Processor scheduling, pay-as-you-go model, Hybrid Resources Renting]
RESS: A Reliable Energy-Efficient Storage System
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Extracting high I/O performance from parallel file systems is no longer the only goal in modern data centres. As issues of the Energy Wall and the Reliability Wall become unavoidable, it is a demanding and challenging task to reduce energy consumption in large-scale storage systems in modern data centres while retaining acceptable systems reliability. Most energy conservation techniques inevitably have adverse impacts on the parallel disk systems. To address the reliability issues of energy-efficient parallel storage systems, we propose a reliable energy-efficient storage system called RESS, which aims at improving both energy efficiency and reliability of parallel storage systems by seamlessly integrating HDDs and SSDs. At the heart of the RESS is a transformative middleware layer, which reorganizes the I/O workload for the underlying parallel file systems. With the help of the middleware layer, RESS can distribute popular data to SSDs and put HDDs into the low-power mode under light workload conditions without modification of the parallel systems.
[efficiency improvement, Energy consumption, parallel storage system, low-power mode, energy wall, reliability, HDD, reliability improvement, Electronic mail, data centres, hard discs, energy-efficient parallel storage systems, power aware computing, Energy conservation, I/O workload, reliable energy-efficient storage system, middleware, reliability wall, parallel disk systems, SSD, parallel databases, I/O performance, energy conservation techniques, PLFS, Middleware, computer centres, Computer science, energy-efficient, RESS, light-workload conditions, PVFS, large-scale storage systems, parallel file systems, Reliability, energy consumption reduction, middleware layer, Energy storage, disc drives]
Accelerating Learning to Rank via SVM with OpenCL and OpenMP on Heterogeneous Platforms
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Support vector machine (SVM) is a popular algorithm for learning to rank, but the training speed of SVM is the bottleneck when dealing with large size data problems. Recently, heterogeneous computing platforms, such as graphics processing unit (GPU) and Many Integrated Core (MIC), have exhibited huge superiority in High Performance Computing domain. Open Computing Language (OpenCL) and Open Multi-Processing (OpenMP) are two popular parallel programming interface for different Heterogeneous Platforms. To resolve the speed problem of RSVM, comparison of the performance of different parallel programming models on different heterogeneous platforms is important. We designed OpenMPbased parallel learning to Rank SVM (PLRSVM) for multi-core CPU and MIC, and OpenCL-based PLRSVM for multi-core CPU, GPU and MIC. The experimental result shows the different performance between OpenMP based program and OpenCL based program. The OpenCL based program significantly speeds up training process of SVM and shows good portability on heterogeneous devices. The experiment also suggests that selection of suitable programming models according to the hardware platform and the structure of serial algorithm is an important step to acquire high performance of parallel algorithm.
[Algorithm design and analysis, serial algorithm, application program interfaces, Graphics processing units, parallel algorithm, SVM, GPU, Training, Microwave integrated circuits, hardware platform, OpenCL-based PLRSVM, learning (artificial intelligence), multicore CPU, RSVM, OpenMP-based parallel learning-to-rank SVM, multi-threading, support vector machines, parallel programming interface, OpenMP, MIC, Open Computing Language, Support vector machines, Leaning to rank, Parallel programming, support vector machine, heterogeneous platforms, Open MultiProcessing, OpenCL, Acceleration, heterogeneous device portability]
Context-Aware Video Object Proposals
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Recent advances in object proposals have been achieved obvious performance to speed up sliding window based object detection or recognition. However, the spatial-temporal object proposal of multi-objects in video is still a challenging problem. Applying the existing image methods frame by frame will result in three defects. First, no guarantee to keep the consistent proposal results, i.e., it is hard to avoid omitting objects even in consecutive or similar sequences. Second, the latent information contained in time dimension would not be made best use of to improve the detection rate. Third, due to the motion blur caused by motion flow, the efficiency of object proposals relying on contour or edge features would be definitely degraded. In this paper, we propose an efficient method for video object proposals. By introducing image method into context-aware framework, we get the improved detection rate compared to the frame by frame usage, while keeping a controllable computing efficiency. Firstly, the bounding boxes produced by image proposals are used as the input. Then the candidate windows are scored with contextual information by generating motion-based mapping boxes. To evaluate the multi-object proposal results, we build a specific dataset. Experiments show that the proposed method can improve the detection rate of the original image method, and especially achieve better performance when proposing a small set of bounding boxes.
[object recognition, spatial-temporal object proposal, motion flow, motion-based mapping boxes, object detection, video multi-object proposals, Electron tubes, Proposals, ubiquitous computing, edge features, controllable computing efficiency, contextual re-scoring, feature extraction, Motion pictures, edge detection, video signal processing, motion blur, multi-object detection dataset, Image edge detection, context-aware video object proposals, Optical imaging, sliding window based object detection, improved detection rate, sliding window based object recognition, Object detection, Software, motion based mapping]
Convolutional Neural Network Simplification Based on Feature Maps Selection
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
We present a feature maps selection method for convolutional neural network (CNN) which can keep the classifier performance when CNN is used as a feature extractor. This method aims to simplify the last subsampling layer of CNN by cutting the number of feature maps with Linear Discriminant Analysis (LDA). It is shown that our method can stabilize the classification accuracy and achieve runtime reduction by removing some feature maps of the last subsampling layer which have worst separability. And the result also lay the foundation for further simplification of CNN.
[structure simplification, pattern classification, Convolutional Neural Network, CNN, subsampling layer, Conferences, LDA, convolutional neural network simplification, classification accuracy, feature extractor, classifier performance, feature maps selection, Support vector machines, Runtime, Convolution, Neural networks, Feature extraction, Eigenvalues and eigenfunctions, feature map selection, separability analysis, statistical analysis, linear discriminant analysis, feature selection, feedforward neural nets]
Image Enhancement Based on Bi-Histogram Equalization with Non-Parametric Modified Technology
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
This paper presents a new image enhancement method using histogram equalization called Bi-Histogram Equalization with Non-parametric Modified Technology (BHENMT). Our proposed method consists of three steps: (i) The input original histogram is divided into two parts using the Otsu method. (ii) Then the histogram modification technique is used to control over enhancement and maximize entropy. (iii) Two sub images are enhanced by the traditional histogram equalization method using the corresponding modified histogram respectively and finally are merged into one output enhanced image. The experimental results show that BHENMT is better than other contrast enhancement methods according to subjective evaluation and various image objective evaluation measures, i.e. Entropy, AMBE and PSNR.
[Image objective evalution measure, Histogram clipping, Brightness, histogram modification technique, Entropy, Optimization, Histograms, optimisation, entropy, image enhancement, bihistogram equalization-with-nonparametric modified technology, Mathematical model, BHENMT, PSNR, bihistogram equalization, image enhancement method, image objective evaluation measures, Image segmentation, AMBE, Brightness preserving, Otsu method, input original histogram, entropy maximization, Image enhancement, Histogram equalization]
Multiple Cartesian K-Medoids for a Fine Quantization
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
K-means is a widely used method for the process of vector quantization in image retrieval, and its results will directly affect the subsequent retrieval quality. Although k-means is popular in image retrieval, it has some obvious disadvantages, such as randomness and sensitivity to outliers. This paper presents a new model, namely Multiple Cartesian K-medoids, to replace k-means for quantization and retrieval. The proposed model proceeds in two steps. The first step is to establish multiple K-medoids model to finely quantize feature vectors to codewords. Then, the second step establishes local linear search: adopt an inverted file for efficiently searching candidate nearest neighbors of a given query, and finally obtains accurate neighbors of the query by re-ranking these candidate neighbors with Euclidean distances of the original feature vectors. Experimental results show that the proposed method is effective, and substantially improves the search accuracy of the returned nearest neighbors.
[Vector quantization, K-medoids, Image retrieval, outlier sensitivity, multiple, Distortion, search accuracy, feature vector quantization, Indexes, Matrix decomposition, fine quantization, query processing, vector quantisation, vectors, multiple Cartesian K-medoids, local linear search, Euclidean distances, feature extraction, image retrieval, quantisation (signal), vector quantization, returned nearest neighbors, candidate nearest neighbors, search problems]
Online Object Tracking Based on Convex Hull Representation
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
This paper presents a novel tracking algorithm based on the convex hull representation model with sparse representation. The tracked object is assumed to be within the object convex hull and the candidate convex hull in the meanwhile. The object convex hull consists of a principle component analysis (PCA) subspace, and the candidate convex hull is constructed by all candidate samples with the sparsity constraint. Then we propose the objective function for our convex hull representation model, and design an iterative algorithm to solve it effectively. Finally, we present a tracking framework based on the proposed convex hull model and a simple online update scheme. Both qualitative and quantitative evaluations on some challenging video clips show that our tracker achieves better performance than other state-of-theart methods.
[Visualization, iterative methods, Target tracking, object convex hull, online object tracking, convex programming, Object tracking, simple online update scheme, convex hull, iterative algorithm, image set, Optimization, PCA subspace, principle component analysis, image representation, object tracking, Mathematical model, convex hull representation, principal component analysis, sparsity constraint, Principal component analysis, sparse representation, objective function]
Research on Semi-Supervised Learning for Hyperspectral Remote Sensing Imaging Classification Base on Confidence Entropy
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
The research of Hyperspectral classification is the hotpots at present. In this article, an effective semi-supervised classification method was proposed for hyperspectral image based on confidence entropy. The experimental results show that the proposed method can effectively improve the accuracy of classification and obtain better classification results for hyperspectral image data using few labeled samples.
[confidence entropy, image classification, semisupervised learning, hyperspectral image, geophysical image processing, remote sensing, Entropy, posterior probability, classification accuracy improvement, Support vector machines, Training, hyperspectral remote sensing imaging classification, entropy, semi-supervised learning, Mathematical model, learning (artificial intelligence), hyperspectral imaging, Hyperspectral imaging]
Sparse Autoencoder Based Deep Neural Network for Voxelwise Detection of Cerebral Microbleed
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
In order to detect cerebral microbleed more efficiently, we developed a novel computer-aided detection method based on susceptibility-weighted imaging. We enrolled five CADASIL patients and five healthy controls. We used a 20x20 neighboring window to generate samples on each slice of the volumetric brain images. The sparse autoencoder (SAE) was used to unsupervised feature learning. Then, a deep neural network was established using the learned features. The results over 10x10-fold cross validation showed our method yielded a sensitivity of 93.20&#x00B1;1.37%, a specificity of 93.25&#x00B1;1.38%, and an accuracy of 93.22&#x00B1;1.37%. Our result is better than Roy's method, which was proposed in 2015.
[cerebral microbleed, biomedical MRI, Image processing, MRI, learning (artificial intelligence), medical image processing, deep neural network, Biomedical imaging, SWI, sparse autoencoder, computer-aided detection method, Blood vessels, Brain models, unsupervised feature learning, susceptibility weighted imaging, Biological neural networks, Standards, Neurology, Patient monitoring, Sensitivity, voxelwise detection, Neural networks, CADASIL patients, cross validation, susceptibility-weighted imaging, neural nets, cerebral microbleed detection]
Steganalysis via Deep Residual Network
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Recent studies have demonstrated that a well designed deep convolutional neural network (CNN) model achieves competitive performances on detecting the presence of secret message in digital images, compared with the classical rich model based steganalysis. In this paper, we propose to investigate a category of very deep CNN model-the deep residual network (DRN), for steganalysis. DRN is suitable for steganalysis from two aspects. For the first, the DRN model usually contains a large number of network layers, which proves to be effective to capture the complex statistics of digital images. For the second, DRN's residual learning (ResL) method actively strengthens the signal coming from secret messages, which is extremely beneficial for the discrimination between cover images and stego images. Comprehensive experiments on standard dataset show that the DRN model achieves very low detection error rates for the state of arts steganographic algorithms. It also outperforms the classical rich model method and several recently proposed CNN based methods.
[image processing, secret messages, Error analysis, Computational modeling, Digital images, DRN residual learning, convolutional neural network, residual learning, ResL method, deep residual network, Training, detection error rates, Convolution, Steganalysis, Neural networks, steganography, stego images, Feature extraction, steganographic algorithms, neural nets, very deep CNN model, steganalysis, cover images]
Super Resolving of the Depth Map for 3D Reconstruction of Underwater Terrain Using Kinect
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
In recent years, sonar has been widely used for restoring the underwater terrain. Sonar imaging has the benefits such as long-range photographing, robust for turbidity water. However, it is not suitable for short-range imaging. Meanwhile, it also cannot meet the need of mining machine. Therefore, it is important to develop a 3D reconstruction method for short-range imaging. In this paper, we propose a Kinect-based underwater 3D image reconstruction method. To overcome the drawbacks of low accuracy of depth maps, we propose a novel super-resolution (SR) method, which uses the underwater dark channel prior dehazing, weight guided image SR, and inpainting. The proposed method considered the influence of mud sediments in water, it performs better than the traditional methods. The experimental results demonstrated that, after inpainting, dehazing and the super-resolution, it can obtain high accuracy depth maps.
[depth maps, Image resolution, short-range imaging, Sonar measurements, underwater terrain, Optical imaging, super resolution, image reconstruction, sonar imaging, super-resolution method, underwater acoustic communication, inpainting, Measurement by laser beam, depth map, Cameras, Kinect-based underwater 3D image reconstruction method, underwater dark channel prior dehazing, image resolution, weight guided image SR]
A Secure Data Deduplication Scheme Based on Differential Privacy
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
In cloud computing environment, especially in Big Data era, adversary may use data deduplication service supported by the cloud service provider as a side channel to eavesdrop users' privacy or sensitive information. In order to tackle this serious issue, in this paper, we propose a secure data deduplication scheme based on differential privacy. The highlights of the proposed scheme lie in constructing a hybrid cloud framework, using convergent encryption algorithm to encrypt original files, and introducing differential privacy mechanism to resist against the side channel attack. Performance evaluation shows that our scheme is able to effectively save network bandwidth and disk storage space during the processes of data deduplication. Meanwhile, security analysis indicates that our scheme can resist against the side channel attack and related files attack, and prevent the disclosure of privacy information.
[privacy information disclosure prevention, Conferences, Big Data, performance evaluation, disk storage space, cryptography, file encryption, network bandwidth, Data deduplication, cloud computing environment, hybrid cloud framework, convergent encryption algorithm, hybrid cloud, side channel attack, file attack, secure data deduplication scheme, data privacy, cloud computing, cloud service provider, differential privacy, convergent encryption]
Design and Implementation of Mixed Extended-Precision Package in MATLAB
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
This article describes a Matlab implementation of a mixed extended-precision package with three multiple-component formats. In the package, double-double, triple-double, and quad-double numbers, which are unevaluated sums of two, three and four IEEE double precision numbers respectively, are defined as Matlab classes including real and complex formats. We implement the four basic operations with mixed extended-precision based on these classes, and apply it to other operations and elementary functions. All operations and functions we presented are overloaded in Matlab. We experimentally evaluate this package that it allows the user to obtains more accurate numerical results with Matlab.
[multiple-component formats, Heuristic algorithms, Conferences, mathematics computing, Multiple-component, double-double numbers, Electronic mail, Mixed precision, MATLAB, System analysis and design, IEEE double-precision numbers, software packages, real formats, Libraries, quad-double numbers, Extended-precision, Computer science, Matlab classes, elementary functions, Overload, mixed extended-precision package, complex formats, triple-double numbers, number theory, Matlab]
[Publisher's information]
2016 IEEE 22nd International Conference on Parallel and Distributed Systems
None
2016
Provides a listing of current committee members and society officers.
[]
Message from General Chairs
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
The following topics are dealt with: cloud computing; storage management; parallel processing; virtual machines; mobile computing; resource allocation; computer centres; learning (artificial intelligence); graphics processing units; and wireless sensor networks.
[parallel systems, NP-hard problem, SSD, virtual machines, mobile devices, distributed systems, parallel processing]
Message from Program Chairs
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Conference Organization
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Provides a listing of current committee members and society officers.
[]
Workshop Organizing Committees
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Provides a listing of current committee members and society officers.
[]
Program Committee
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Provides a listing of current committee members and society officers.
[]
Keynotes
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Provides an abstract for each of the keynote presentations and may include a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.
[]
Event Description and Detection in Cyber-Physical Systems: An Ontology-Based Language and Approach
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
In this paper, we propose an ontology-based language, OntoEvent, for semantic complex event modeling and detection in Cyber-Physical Systems (CPS). We divide the core concepts of OntoEvent model into two levels: general concepts and domain-specific instances, to promise it can be dynamic extended for different CPS application domains. Complex events are modeled based on event ontology with logical and temporal operators, and these operators are extended by nature language synonymies. OntoEvent language is of rich expressiveness compared to other traditional languages. Based on OntoEvent, We propose an event detection model and elaborate its construction procedures. Experimental results prove that OntoEvent-based event detection model outperforms other selected models in processing efficiency, especially when processing multiple complex event ontologies.
[temporal operators, Cyber-Physical System, Conferences, Ontology, Description Language, Semantic, Ontologies, event detection model, Cyber-Physical Systems, cyber-physical systems, event description, OntoEvent language, multiple complex event ontologies, Semantics, ontology-based language, semantic complex event modeling, specification languages, ontologies (artificial intelligence), CPS application, nature language synonymies, event description language, Complex Event Processing, logical operators]
BARTON: Low Power Tongue Movement Sensing with In-Ear Barometers
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Sensing tongue movements enables various applications in hands-free interaction and alternative communication. We propose BARTON, a BARometer based low-power and robust TONgue movement sensing system. Using a low sampling rate of below 50 Hz, and only extracting simple temporal features from in-ear pressure signals, we demonstrate that it is plausible to distinguish important tongue gestures (left, right, forward) at low power consumption. We prototype BARTON with commodity earpieces integrated with COTS barometers for in-ear pressure sensing and an ARM micro-controller for signal processing. Evaluations show that BARTON yields 94% classification accuracy and 8.4 mW power consumption, which achieves comparable accuracy, but consumes 44 times lower energy than the state-of-the-art microphone-based solutions. BARTON is also robust to head movements and operates with music played directly from earphones.
[Irrigation, low power tongue movement sensing, Tongue, BARometer, robust TONgue movement sensing system, prototype BARTON, tongue gestures, hands-free interaction, tongue movements, In-Ear Barometers, feature extraction, Frequency response, Sensors, ARM micro-controller, Pressure sensors, power 8.4 mW, in-ear pressure sensing, Ubiquitous computing, barometers, COTS barometers, Microphones, Headphones, Human computer interaction, in-ear pressure signals, Ear, temporal features, human computer interaction, frequency 50.0 Hz]
A Time Series Classification Method for Battery Event Detection
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
The maintenance of batteries used in wireless mobile communication is an important practical problem. The experts can easily recognize the battery events, such as turning on, by watching the monitoring data. However it is infeasible to have experts watch the data all the time. There are devices that can report battery events. These devices sometimes report incorrect event. In order to solve this problem, we propose a time series classification framework to use the expert knowledge to build an accurate classifier, and then use the classifier to monitor the batteries in real time. We first propose an active learning method to efficiently collect the experts' labels for each event. Then we apply various feature extraction methods to convert each time series segment into a feature vector. Finally, we apply random forest classifier to perform the classification. Moreover, in practice, the labeled data is unbalanced, i.e. &gt;99% of the data instances belong to a single label. We use bootstrap to solve this problem. We test our method on a dataset for 500 batteries in 3 months. The results show that our method achieves a very high classification accuracy, using only less than 1% of the dataset as training set.
[Measurement, time series classification framework, Uncertainty, wireless mobile communication, active learning method, Batteries, bootstrap, feature extraction methods, Training, battery events, data instances, random forest classifier, feature extraction, classifier, time 3.0 month, learning (artificial intelligence), Monitoring, pattern classification, Time series analysis, batteries, battery event detection, time series segment, time series, battery management systems, power engineering computing, Active Learning, Event Detection, Feature extraction, Time Series Classification, time series classification method]
AppIS: Protect Android Apps Against Runtime Repackaging Attacks
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Apps repackaged through reverse engineering pose a significant security threat to the Android smart phone ecosystem. Previous solutions have mostly focused on the detection and identification of repackaged apps. Nevertheless, current app anti-repackaging services can only protect applications at a coarse level and get a significant performance overhead. These approaches can neither meet the performance requirements of Android nor achieve fine-grained protection against cumulative attack 1 at the same time. Specifically, these solutions rely on a fix-structure detecting engine and then will execute the same path at different times, which lead to the entire protection performs poorly when faced with dynamic cumulative attack, which is typical in real-world attack. This paper introduces AppIS, a reinforced anti-repackaging immune system, that is robust to app-repackaging attack scenarios. Unlike prior work, which mostly focuses on simple protection only from just one respect, our design exploits an interlocking guarding net with time diversity for the tamper-proofing of Android applications. The intuition underlying our design is that a dynamic and static combining method can provide a multi-level protection for the codes, core algorithm and sensitive data. We analyze and classify the existing threats on Android platform and furthermore abstract then model the repackaging attack scenarios. We then adopt a random controller used by the dispatcher to randomly construct guarding net with different structure every time. We have built a prototype of our design using Java Native Interface cross-layer calling mechanism for performance requirement. Results from a deployment of AppIS on several kinds of popular apps demonstrate that the new design can prevent our apps from cumulative attack without extra performance cost.
[runtime repackaging attacks, Reverse engineering, Humanoid robots, fix-structure detecting engine, Security, dynamic cumulative attack, multilevel protection, Runtime, Android (operating system), mobile computing, static combining method, Android application protection, Malware, fine-grained protection, Android smart phone ecosystem, dynamic combining method, Java, AppIS, application-repackaging attack scenarios, reverse engineering, smart phones, Java native interface cross-layer calling mechanism, performance overhead, coarse level, security of data, application antirepackaging services, Android application security repackaging attack, reinforced antirepackaging immune system, Androids, Smart phones]
Charge-Depleting of the Batteries Makes Smartphones Recognizable
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Many components of smartphones are used to generate device fingerprinting, such as screens, CPUs and various sensors. These device fingerprinting can be used to identify the smartphones. However, there are many restrictions with these device fingerprinting. Invariable information in screens and CPUs may lead to privacy risks. Moreover, strict experimental steps are required when fingerprinting the sensors. The effectiveness and effeciency of these device fingerprinting is reduced in practice. In this paper, we present a novel hardware fingerprinting based on the battery. Instead of relying on invariable information of the battery, we focus on the charge-depleting of the smartphone. The discrepencies on manufacturing of smartphones make that the charge-depleting is different when performs the same task. Moreover, charge-depleting information can easily be obtained without strict operating steps. We design a highly accurate algorithm to fingerprint the batteries which is based on the unsupervised learning. Besides, we stimulate the algorithm with different charge-depleting of tasks to improve the performance. We use 15 smartphones to evaluate the performance of the battery fingerprinting in both laboratory and public conditions. The experimental results show that battery fingerprinting is quite effective, the recognition accuracy rate can reach 86%.
[Performance evaluation, charge-depleting information, battery level, charge depleting, smart phones, Batteries, Object recognition, Task analysis, device fingerprinting, unsupervised learning, privacy risks, mobile computing, security, identification, battery fingerprinting, hardware fingerprinting, Feature extraction, secondary cells, data privacy, smartphones, Sensors, Smart phones]
Concatenating Road Take Me Home: Indoor Navigation Without Infrastructure Support
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Though outdoor navigation has been used for a long time, indoor navigation has not yet been put into practice. Recent works based on geomagnetic information require a user to follow the exact path of previous users, which limits its applicability. Meanwhile, the optimal navigation path may not be provided given a large number of paths with different sources and destinations. We present MeshMap, an efficient indoor navigation system by merging geomagnetic information of crowdsourcing paths. MeshMap combines geomagnetic information from multiple users' paths to construct a global navigation map. Then, MeshMap supports to find the optimal navigation path on the navigation map. Finally, MeshMap proposes a real-time tracking method to map user's position on the navigation path and provides real-time navigation hints. We implement MeshMap on Android and evaluate its performance in different environments including campus building, parking area and shopping mall. The evaluation results show the effectiveness of MeshMap.
[Legged locomotion, indoor navigation, Magnetometers, crowdsourcing, Indoor navigation, concatenating road take me home, MeshMap indoor navigation system, Sensory Data, Android, Magnetic separation, geomagnetic information, Indoor Navigation, geomagnetic navigation, infrastructure support, Real-time systems, Smartphone, Wireless fidelity, crowdsourcing paths]
Data Collection with Privacy Preserving in Participatory Sensing
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Participatory sensing has increasingly become a new paradigm of data collection from a wide physical area and a large population. One of the major challenges in participatory sensing is the privacy issue. Sensing data from smartphones may contain sensitive information such as user locations. Thus, it is of great importance to preserve privacy throughout the data collection process in participatory sensing. It is however very challenging because of the distributed nature of the network, many potential malicious attackers and the convergecast model of data collection. In this paper, we present a data collection approach which preserves user privacy in participatory sensing. In this approach, a smartphone node utilizes other smartphones as intermediate nodes to transfer its sensing data. In addition, asymmetric encryption is used to prevent malicious reverse tracking along the data forwarding route, hence anonymizing the originator of the data. We analyze the security of the approach and show that it achieves a high level of security. Extensive simulations demonstrate that the proposed approach has a low overhead.
[Data privacy, smartphone, data forwarding route, privacy preservation, participatory sensing, cryptography, privacy, smart phones, Servers, sensing data, smartphone node, data collection approach, mobile computing, Public key, asymmetric encryption, Data collection, data privacy, Sensors, Peer-to-peer computing, malicious reverse tracking, Smart phones]
Machine Learning (ML)-Based Air Quality Monitoring Using Vehicular Sensor Networks
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Due to its advantages of providing a large geographical coverage and having no strict limits on energy and sensing and processing capabilities, a vehicular sensor network (VSN) has recently emerged as a promising paradigm for air quality monitoring in an urban area. However, designing an efficient VSN-based air monitoring system has challenges due to the vehicles' heterogeneous temporal and spatial coverage and the relatively expensive communication cost over cellular networks. In this paper, we propose a machine learning (ML)-based Air quality Monitoring (MLAirM) system which aims at reducing communication and sensing costs by allowing vehicles to process the collected data in a distributed fashion. More specifically, in MLAirM, vehicles are first assigned to take measurements at sets of locations in a sensing area. The vehicle then utilizes a distributed machine learning algorithm to learn a local model of air quality based on its collected data. Finally, the vehicle sends parameters of its model to a monitoring center which combines multiple local models to build a global air quality map. Furthermore, assigning the sensing locations to vehicles can be viewed as a successful measurement probability aware location assignment problem. An integer linear optimization problem is formulated and a heuristic algorithm is proposed to find the solution. Simulations based on realistic vehicular traces are performed to compare the proposed MLAirM system with other approaches. The simulations results show that the MLAirM can achieve a similar accuracy of building the global air quality map with a significant reduction in communication and sensing costs compared to other approaches.
[wireless sensor networks, integer programming, sensing locations, monitoring center, geographical coverage, integer linear optimization problem, linear programming, heuristic algorithm, optimisation, vehicular sensor network, Distributed Machine Learning, Sensors, Pollution measurement, learning (artificial intelligence), cellular networks, Monitoring, air quality, mobile radio, MLAirM system, Atmospheric modeling, Air quality Monitoring system, Urban areas, probability, Air quality, machine learning, Vehicular Sensor Networks, VSN, global air quality map, Atmospheric measurements, measurement probability aware location assignment problem, distributed machine, Air Quality Monitoring, atmospheric techniques, sensing costs, cellular radio]
ML-NA: A Machine Learning Based Node Performance Analyzer Utilizing Straggler Statistics
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Current Cloud clusters often consist of heterogeneous machine nodes, which can trigger performance challenges such as the task straggler problem, whereby a small subset of parallel tasks running abnormally slower than the other sibling ones. The straggler problem leads to extended job response and deteriorates system throughput. Poor performance nodes are more likely to engender stragglers, and can undermine straggler mitigation effectiveness. For example, as the dominant mechanism for straggler alleviation, speculative execution functions by creating redundant task replicas on other machine nodes as soon as a straggler is detected. When speculative copies are assigned onto the poor performance nodes, it is hard for them to catch up with the stragglers compared to replicas run on fast nodes. And due to the fact that the performance heterogeneity is caused not only by static attribute variations such as physical capacity, but also dynamic characteristic uctuations such as contention level, analyzing node performance is important yet challenging. In this paper we develop ML-NA, a Machine Learning based Node performance Analyzer. By leveraging historical parallel tasks execution log data, ML-NA classies cluster nodes into different categories and predicts their performance in the near future as a scheduling guide to improve speculation effectiveness and minimize task straggler generation. We consider MapReduce as a representative framework to perform our analysis, and use the published OpenCloud trace as a case study to train and to evaluate our model. Results show that ML-NA can predict node performance categories with an average accuracy up to 92.86%.
[published OpenCloud trace, ML-NA, task straggler generation minimization, historical parallel tasks execution, Electronic mail, node performance analysis, straggler mitigation effectiveness, straggler alleviation, speculative execution functions, Task analysis, parallel processing, speculation effectiveness, Machine Learning, MapReduce, Degradation, Analytical models, resource allocation, extended job response, cluster nodes, Parallel processing, scheduling, poor performance nodes, Node Performance, Market research, cloud computing, learning (artificial intelligence), system throughput deterioration, Straggler Problem, static attribute variations, physical capacity, contention level, fast nodes, task straggler problem, Prediction, machine learning based node performance analyzer, heterogeneous machine nodes, performance challenges, pattern clustering, node performance categories, scheduling guide, Machine learning, system monitoring, redundant task replicas, cloud clusters, performance heterogeneity, straggler statistics]
MODE: A Context-Aware IoT Middleware Supporting On-Demand Deployment for Mobile Devices
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
With the development of Internet of Things (IoT), various mobile sensing devices emerge in the market, which brings great convenience to people's life. Middleware, as the connection platform of sensors and applications, has become increasingly important to solve the problem caused by diverse devices and changing environments. In this paper, we proposed MODE, a middleware that can dynamically change its deployment of function modules based on context awareness to adapt to environment changes. As a highly scalable middleware, MODE not only has basic tasks, but also provides developers with user-specific tasks, based on a highly-abstracted scripting language, which supports the application development in different scenarios and improves the efficiency. Besides, we design various function libraries in MODE that developers can load or unload them dynamically. MODE improves the efficiency, adaptability and scalability of IoT systems and applications.
[user-specific tasks, MODE, function modules, on-demand deployment, Servers, Task analysis, software libraries, scalability, IoT, connection platform, Runtime, mobile computing, authoring languages, diverse devices, Libraries, context awareness, Sensors, middleware, Context-aware services, environment changes, changing environments, adaptability, Internet of Things, Middleware, application development, Context-Aware IoT Middleware Supporting On-Demand Deployment, sensors, highly scalable middleware, mobile devices, mobile sensing devices]
WiSH: The Design and Implementation of a Real-Time System for Whole-Day Human Detection
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Sensorless sensing using wireless signals has been rapidly conceptualized and developed recently. Among numerous applications of WiFi-based sensing, human presence detection acts as a primary and fundamental function to boost applications in practice. Many complicated approaches have been proposed to achieve high detection accuracy, which, however, frequently omit various practical constraints like real-time capability, computation efficiency, sampling rates, deployment efforts, etc. A practical detection system that works in real world lacks. In this paper, we design and implement WiSH, a real-time system for contactless human detection that is applicable for whole-day usage. WiSH employs lightweight yet effective methods and thus enables detection under practical conditions even on resource-limited devices with very low signal sampling rates. We deploy WiSH on commodity desktops and customized tiny nodes in different everyday scenarios. The experimental results demonstrate superior performance of WiSH, achieving a detection accuracy of &gt;98% using a sampling rate of 20Hz with an average detection delay of merely 1.5s, which renders it a promising system for real-world deployment.
[Correlation, wireless sensor networks, Off-the-shelf WiFi, contactless human detection, time 1.5 s, object detection, Channel State Information, average detection delay, Wireless communication, signal sampling, Real-time systems, Sensors, human presence detection, Monitoring, signal detection, whole-day human detection, Real-time System, Human Detection, WiSH, WiFi-based sensing, Wireless sensor networks, frequency 20.0 Hz, resource-limited device, real-time systems, sensorless sensing, Feature extraction, wireless signal sampling rates, Wireless Sensing]
When User Interest Meets Data Quality: A Novel User Filter Scheme for Mobile Crowd Sensing
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Mobile crowd sensing has become a promising paradigm for mobile users to collect information. Considering that the task information push is not free and there are many users who are not interested in the current task or provide noisy sensing data, one of the imminent problems is how to recommend high-quality and interested users in real time and steer participators to collect data with adequate budgets. However, it is difficult to predict the data quality and users' interest without the validity of real data. In this paper, we propose a user recommender system where the users' data qualities for sensing tasks are derived from historical statistical data to filter out the non-interested and malicious users in current task. The aim is to recruit a sub-group of participators for efficient crowd sensing, in order to maximize the platform utility. We show that our problem is NP-hard, and model the recruitment process as a sub-modular problem. Finally, an approximation algorithm is designed to guarantee the platform utility and participators' profits. We evaluate our algorithm on simulated data set and the results indicate that the platform utility and data quality improves significantly.
[sensor fusion, Task analysis, Recruitment, Crowd Sensing, User recruitment, User Filtering, data quality, mobile computing, recommender systems, user recommender system, noisy sensing data, malicious users, NP-hard problem, Data integrity, user filter, Approximation algorithms, mobile crowd sensing, Sensors, mobile users, Recommender systems, Monitoring, computational complexity]
Toward Heterogeneity-Aware Device-to-Device Data Dissemination over Wi-Fi Networks
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
In the last few years, there has been an explosive growth of the number of mobile devices. This has come with a plethora of new applications and usages. Among these new usages, there are many occasions for which a content has to be disseminated to a large number of mobile devices (e.g., large-scale events providing a multi-media support, video streaming, ...). To cope with network bandwidth limitations, new approaches, leveraging device-to-device (D2D) communications have emerged. Obviously, one of the main problem that D2D-based approaches have to face is the energy consumption. Furthermore, there is usually a huge heterogeneity among the devices: some may benefit of a good, fully charged battery while others may have only a couple of hours left before a power failure; the network bandwidth can also differ a lot. In this paper, based on a previous work, we propose an approach to take into account devices heterogeneity while disseminating data using D2D communications. Our simulations show that it is possible to spare the weakest batteries without wearing too much the good ones nor degrading too much the performance. Furthermore, taking into account the devices bandwidth capabilities can help to increase the dissemination speed.
[heterogeneity-aware device-to-device data dissemination, Wi Fi Direct, Data dissemination, data dissemination, Mobile handsets, Device-to-device communication, Batteries, Servers, heterogeneity, D2D communications, device-to-device communications, Wi-Fi networks, multimedia support, network bandwidth limitations, device-to-device, efficient data dissemination, Bandwidth, mobile devices, video streaming, Energy efficiency, wireless LAN, energy consumption, Wireless fidelity, mobile handsets]
SpeAR: A Fast AR System with High Accuracy Deployed on Mobile Devices
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
The augmented reality(AR) technology can enrich a person actual life, and is attracting more and more attention. However, the tradition methods have some problems such as high time delay, high deployment cost and low accuracy. These problems greatly hinder the AR technology to ubiquitous applications. In this paper, we design a AR system deployed on mobile devices, named SpeAR, which leveraging the feature matching algorithm to fast recognize the object with high accuracy. In SpeAR, we employ the depth camera embedded in the mobile device to obtain the distance between mobile device and object to use as a feature of this object image. For the accurately matching, the flutter-free algorithm will be designed to extract more accurate image feature. For the fast matching, the shrunken SURF(sSURF) is proposed to match images combining the distance feature. We have implemented a prototype system to evaluate the actual performance. The experiment results show that our solution achieves an 141ms delay in object recognition in this system.
[object recognition, object image, Augmented Reality, augmented reality, fast matching, Mobile handsets, flutter-free algorithm, augmented reality technology, cameras, Matching, shrunken SURF, mobile computing, Databases, image feature extraction, feature extraction, depth camera, Computer vision, Delay effects, feature matching algorithm, Object recognition, image matching, SURF, AR technology, sSURF, distance feature, mobile devices, Feature extraction, Cameras, Mobile Devices, fast AR system, SpeAR, shrunken speeded-up robust feature]
SoundWrite II: Ambient Acoustic Sensing for Noise Tolerant Device-Free Gesture Recognition
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Acoustic sensing has brought forth the advances of prosperous applications such as gesture recognition. Specifically, ambient acoustic sensing has drawn many contentions due to the ease of use property. Unfortunately, the inherent ambient noise is the major reason for unstable gesture recognition. In this work, we propose &#x201C;SoundWrite II&#x201D;, which is an improved version of our previously designed system. Compared with our previous design, we utilize the two threshold values to identify the effective signals from the original noisy input, and leverage the MFCC (Mel frequency cepstral coefficient) to extract the stable features from different gestures. These enhancements could effectively improve the noise tolerant performance for previous design. Implementation on the Android system has realized the real time processing of the feature extraction and gesture recognition. Extensive evaluations have validated our design, where the noise tolerant property is fully tested under different experimental settings and the recognition accuracy could be 91% with 7 typical gestures.
[Gesture recognition, noise tolerant device-free gesture recognition, SoundWrite II, noise tolerant property, Android implementation, Mel frequency cepstral coefficient, Microphones, Device Free, Gesture Recognition, unstable gesture recognition, Acoustic Sensing, gesture recognition, inherent ambient noise, feature extraction, cepstral analysis, Feature extraction, Sensors, Android system, MFCC, Smart phones, acoustic signal processing, ambient acoustic sensing, noise tolerant performance]
Online Auctions with Dynamic Costs for Ridesharing
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Uber, Lyft and some other companies achieve great business success in the area of ridesharing. However, there are still many issues to be addressed, for instance, the lack of a well-designed rush hour (i.e., when supply is much less than demand) pricing strategy, the seat resources waste and so on. The online ridesharing model fits better to reality but brings significant design challenges. In this paper, we propose a complementary online auction design: No Preemption auction. The auction in ridesharing needs to deal with dynamic costs, which is different from most of the existing auctions and brings significant challenges. We design corresponding online auction mechanism for this model. Our mechanism is truthful with Risk Aversion Hypothesis, budget balanced (i.e., the payment is not less than the cost), computational efficient, individually rational, and make the best use of the seat resources. We give theoretical proof of the truthfulness, budget balance, computational efficiency and individual rationality. We also give a discussion about the general online ridesharing mechanism performance. Finally, we evaluate the performance of our mechanism based on real taxi-trace data in Shanghai. The results show that our mechanism can achieve rather good revenue and social welfare.
[rush hour pricing strategy, Preemption auction, Companies, traffic engineering computing, Automobiles, Surges, Public transportation, Risk Aversion Hypothesis, Uber, online auctions, complementary online auction design, Pricing, public transport, online ridesharing model, Internet, Resource management, pricing, Lyft]
HartSift: A High-Accuracy and Real-Time SIFT Based on GPU
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Scale Invariant Feature Transform (SIFT) is one of the most popular and robust feature extraction algorithms for its invariance to scale, rotation and illumination. It has been widely adopted in many fields, such as video tracking, image stitching, simultaneous localization and mapping (SLAM), structure from motion (SFM) and so on. However, high computational complexity constrains its further application in real-time systems. These systems have to make a tradeoff between accuracy and performance to achieve real-time feature extraction. They adopt other faster algorithms but with less accuracy, like SURF and PCA-SIFT. In order to address this problem, this paper proposes a GPU-accelerated SIFT using CUDA, named HartSift, which realizes high-accuracy and real-time feature extraction by making full use of computing resources of CPU and GPU within a single machine. Experiments show that, on the NIVDIA GTX TITAN Black GPU, HartSift can process an image within 3.14?10.57ms (94.61?318.47fps) according to the size of images. In addition, HartSift is 59.34?75.96 times and 4.01?6.49 times faster than OpenCV-SIFT (a CPU version) and SiftGPU (a GPU version), respectively. In the mean time, HartSift's performance and CudaSIFT's (the fastest GPU version so far) are almost the same, while HartSift's accuracy is much higher than CudaSIFT's.
[SIFT, GPU-accelerated SIFT, time 10.57 ms, Conferences, parallel architectures, feature extraction algorithms, transforms, graphics processing units, GPU, CUDA, high-accuracy SIFT, Scale Invariant Feature Transform, feature extraction, real-time feature extraction, high accuracy, real-time SIFT, NIVDIA GTX TITAN Black GPU, HartSift, real time]
High Resource Utilization Auto-Scaling Algorithms for Heterogeneous Container Configurations
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Auto-scaling is a technique that allocates resources according to dynamic workload. This paper focuses on auto-scaling with heterogeneous container configurations. The goal is to minimize the cost of container adjustments, and to reduce the resource insufficiency penalty, while maintaining high resource utilization. It is extremely difficult to achieve the minimal cost without knowing the future workloads in advance. Thus, we first propose an optimal dynamic programming algorithm that can scale optimally when given the future workload. This optimal solution is used as the baseline to evaluate other algorithms that do not have the future workload information. Then, we propose two greedy algorithms that do not need workload information in advance, and a heuristic algorithm that first predicts the workload of the next time step using Gradient Boosting Regression, then makes scaling decisions using the optimal dynamic programming algorithm. We evaluate these four algorithms with two realistic workload traces. The experiments show that when the cost to start new servers is much higher than resource insufficiency penalty, our short-term prediction approach will only increase the total cost by only 9.6%, and decrease the utilization by only 10%, when compared with the optimal dynamic programming that knows the future workload.
[scaling decisions, Cloud computing, Heuristic algorithms, optimal dynamic programming algorithm, minimal cost, regression analysis, Auto-scaling, Containers, container adjustments, heuristic algorithm, power aware computing, Container, resource allocation, resource insufficiency penalty, Prediction algorithms, Dynamic programming, cloud computing, total cost, gradient methods, dynamic workload, realistic workload traces, Resource utilization, greedy algorithms, dynamic programming, Virtual machining, optimal solution, future workload information, high resource utilization auto-scaling algorithms, Gradient Boosting Regression, heterogeneous container configurations, Resource management]
A Fast, General Storage Replication Protocol for Active-Active Virtual Machine Fault Tolerance
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Cloud computing enables more and more online services deployed in virtual machines (VMs), making fast VM fault tolerance particularly crucial. Unfortunately, despite much effort, achieving fast VM fault tolerance remains an open problem. A traditional way to provide VM fault tolerance is the active-passive approach, which frequently transfers tremendous updated states, including memory and storage, of a primary VM to a suspended secondary VM. The other emerging approach, namely the active-active approach, runs the secondary VM concurrently with the primary. Compared to active-passive, active-active is faster because it only performs the transfer when the externally visible states (e.g., network outputs) of the primary and secondary diverge. However, active-active aggravates the performance issue on I/O intensive workloads. In existing active-active systems, storage replication protocols hold updated storage states from both the primary and secondary on the secondary, incurring excessive I/O contention. For instance, both our evaluation and prior study show that a well-engineered active-active system, COLO, degrades the throughput of I/O intensive services by up to 61.6%. To tackle this open problem, this paper presents GANNET, a fast and general storage replication protocol for active-active VM fault tolerance systems. It greatly alleviates the I/O contention on the secondary's storage by efficiently buffering the updated disk states from both the primary and secondary VM in memory. GANNET carries a lightweight storage checkpoint algorithm to avoid consuming too much memory. GANNET is proved to be as reliable as existing storage replication protocols. We integrated GANNET into two popular active-active systems. Evaluation on six widely used services shows that GANNET incurred 15.9% overhead compared to the native executions and outperformed COLO's storage replication protocol by 1.2X~2.6X. GANNET's source code is available at github.com/hku-systems/gannet.
[checkpointing, Protocols, tremendous updated states, Buffer storage, COLO's storage replication protocol, secondary diverge, Switches, active-active virtual machine fault tolerance, active-active approach, Fault tolerance, storage management, Cloud Computing, primary VM, Fault tolerant systems, cloud computing, storage states, open problem, fault tolerance, active-passive approach, storage replication protocols, active-active VM fault tolerance systems, active-active system, Storage Replication, GANNET source code, Virtual machining, suspended secondary VM, Support vector machines, lightweight storage checkpoint algorithm, active-active aggravates the performance issue, virtual machines, primary diverge, fault tolerant computing, general storage replication protocol, popular active-active systems, Virtual Machine, Fault Tolerance]
AutoMJ: Towards Efficient Multi-way Join Query on Distributed Data-Parallel Platform
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
The multi-way join query has attracted considerable attention from research community for its importance in many big data analytic applications. For the multi-round multi-way join algorithm in distributed data-parallel platforms, the huge communication cost caused by shuffling large intermediate results over the network is the main bottleneck. The one-round multi-way join algorithm processes the join query in a single communication round, which can significantly reduce the communication cost in complex queries, including cyclic queries. However, the one-round method is not always superior to the multi-round method, because the intermediate result size of the multi-round method may the much smaller than the size of data shuffled in the one-round method. Therefore, it is challenging to choose the best multi-way join algorithm in practice. To solve this problem, in this paper, we present AutoMJ, an efficient framework for multi-way join queries. In AutoMJ, we propose a novel automatic join strategy selection model based on the size estimation of intermediate join results. AutoMJ chooses the multi-way join strategy with the minimal shuffle data size. In addition, we propose an optimized HyperCube algorithm for the one-round multi-way join. We have implemented the prototype of AutoMJ on the widely-used distributed data-parallel platform Apache Spark. Experiments show that for multi-way join queries with large intermediate results, the one-round join strategy can outperform the multi-round join strategy built in Spark SQL 1.2 - 159.3&#x00D7; faster. In contrast, the multi-round join strategy is 2.1 - 6.2&#x00D7; faster than the one-round method for the queries with small intermediate results. Experiments also show that the relative error of size estimation can be within 0.1 for the Twitter dataset and 0.25 for the Wikidata dataset. Furthermore, experiments verify that the automatic join strategy selection model is effective for choosing the optimal multi-way join algorithm.
[big data analytic applications, minimal shuffle data size, distributed data-parallel platform Apache Spark, intermediate join results, join size estimation, parallel processing, distributed computing, query processing, automatic join strategy selection model, cyclic queries, Hypercubes, multiround multiway, optimal multiway join algorithm, single communication round, data analysis, Computational modeling, Estimation, HyperCube shuffle, Spark SQL, Big Data, size estimation, Sparks, multi-way join, SQL, Apache Spark, multiway join query, optimized HyperCube algorithm, complex queries, Software, Wikidata dataset, AutoMJ]
Bloomfish: A Highly Scalable Distributed K-mer Counting Framework
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
K-mer counting is a fundamental operation in DNA research and genome analytics; its application includes estimating genome assembly, understanding similarities in genomic samples, and merging a newly processed genome with a reference genome. As the genome dataset becomes larger and larger, designing a highly optimized distributed-memory implementation becomes more and more important. Current distributed-memory solutions have two limitations: they have a high memory footprint, and they do not provide advanced optimizations for loading enormous genome datasets into memory. Based on these observations, we present Bloomfish, a distributed, memory-efficient, scalable solution to the limits of current work. To keep a low memory footprint, Bloomfish leverages the compact hash array design of the single-node Jellyfish system and the optimized workflow of the high-performance MapReduce framework Mimir. We have also codesigned Mimir's I/O to efficiently load enormous datasets. We ran Bloomfish on the Tianhe-2 supercomputer with large sequence datasets (up to 24 TB). Our results show that Bloomfish achieves unprecedented scalability in genome analytics.
[highly scalable distributed k-mer counting framework, DNA research, Genome analysis, Scalability, Genomics, single-node Jellyfish system, distributed-memory implementation, compact hash array design, optimized workflow, parallel processing, Optimization, K-mer counting, MapReduce, I/O optimization, optimisation, biology computing, newly processed genome, genomics, distributed-memory solutions, Bioinformatics, genome analytics, Performance and scalability, Memory efficiency, Tianhe-2 supercomputer, Supercomputers, Bloomfish, genome assembly, genomic samples, DNA, distributed memory systems, reference genome, data handling, Arrays, high-performance MapReduce framework Mimir]
COSY: An Energy-Efficient Hardware Architecture for Deep Convolutional Neural Networks Based on Systolic Array
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Deep convolutional neural networks (CNNs) show extraordinary abilities in artificial intelligence applications, but their large scale of computation usually limits their uses on resource-constrained devices. For CNN's acceleration, exploiting the data reuse of CNNs is an effective way to reduce bandwidth and energy consumption. Row-stationary (RS) dataflow of Eyeriss is one of the most energy-efficient state-of-the-art hardware architectures, but has redundant storage usage and data access, so the data reuse has not been fully exploited. It also requires complex control and is intrinsically unable to skip over zero-valued inputs in timing. In this paper, we present COSY (CNN on Systolic Array), an energy-efficient hardware architecture based on the systolic array for CNNs. COSY adopts the method of systolic array to achieve the storage sharing between processing elements (PEs) in RS dataflow at the RF level, which reduces low-level energy consumption and on-chip storage. Multiple COSY arrays sharing the same storage can execute multiple 2-D convolutions in parallel, further increasing the data reuse in the low-level storage and improving throughput. To compare the energy consumption of COSY and Eyeriss running actual CNN models, we build a process-based energy consumption evaluation system according to the hardware storage hierarchy. The result shows that COSY can achieve an over 15% reduction in energy consumption under the same constraints, improving the theoretical Energy-Delay Product (EDP) and Energy-Delay Squared Product (ED2P) by 1.33&#x00D7; on average. In addition, we prove that COSY has the intrinsic ability for zero-skipping, which can further increase the improvements to 2.25&#x00D7; and 3.83&#x00D7; respectively.
[Energy consumption, systolic arrays, data reuse, Radio frequency, power aware computing, Bandwidth, deep convolutional neural network (CNN), convolution, Hardware, zero-skipping, deep convolutional neural networks, energy consumption, RS dataflow, multiprocessing systems, multiple COSY arrays, microprocessor chips, on-chip storage, hardware storage hierarchy, 2D convolutions, resource-constrained devices, artificial intelligence applications, energy-efficient hardware architecture, systolic array, low-level energy consumption, energy consumption evaluation system, hardware architecture, Arrays, Convolutional neural networks, energy-efficiency, feedforward neural nets, low-level storage, CNN on Systolic Array]
D-Ary Cuckoo Filter: A Space Efficient Data Structure for Set Membership Lookup
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Many networking and distributed systems use Bloom filters and their variants for high speed set membership tests. Such probabilistic techniques provide very good space efficiency at the cost of a small fraction of false positive answers. The original Bloom filters do not permit deletion of items from the set and most attempts to extend Bloom filters to support deletions suffer from either space or time performance degradation. Recently, inspired by Cuckoo Hashing, Fan et. al. proposed a data structure called Cuckoo filter that achieves even better space performance than Bloom filters while supporting dynamic insertion and deletion of items. By allowing that each element has more than two candidate buckets, d-ary Cuckoo Hashing is capable of providing much higher space utilization. Motivated by this study, we generalize Cuckoo filter to d-ary Cuckoo filter for further reduction in space cost. The main difficulty here is that increasing the number of candidate buckets is not as easy as it appears because only fingerprints are available for the calculation of candidate locations. To solve this problem, we introduce the base-d digitwise xor operations as the foundation for computing the d candidate buckets of each element in a cyclic fashion. Theoretical analysis and experiment study show that d-ary Cuckoo filters can save up to one bit for each element at the cost of increased lookup and insertion performance.
[Data Structures Bloom Filters Cuckoo Filters Membership Tests, set theory, d-ary Cuckoo Hashing, Degradation, original Bloom filters, distributed systems, data structures, Testing, probabilistic techniques, probability, space performance, Probability, Data structures, Probabilistic logic, candidate buckets, d-ary Cuckoo filter, Standards, high speed set membership tests, Computer science, space efficiency, space cost, space efficient data structure, insertion performance, base-d digitwise XOR operations, time performance degradation]
Distributed Set Intersection and Union with Local Differential Privacy
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Privacy-preserving distributed set intersection and union have been widely applied in many scenarios and lots of work has paid attention to the problem. Existing solutions to privacy-preserving set intersection and union are built on secure multiparty computation protocols, which can theoretically solve it, but result in heavy computation and communication overhead. Worse still, most of the existing schemes cannot work once some participant fails. In this paper, we propose two differentially private approaches for distributed set intersection and union, respectively. In our schemes, each data contributor possesses a secret data set and perturbs it by randomized response technique to satisfy local differential privacy. Then the collector gathers all contributors' perturbed data sets and utilizes maximum likelihood estimation to gain an accurate estimation of intersection and union. Compared to existing schemes, the proposed schemes can dramatically reduce computation and communication overhead, and tolerate participant's failure. We formally prove that the proposed schemes satisfy local differential privacy, and leverage extensive experiments to evaluate the proposed approaches. The results indicate that our schemes have low computation and communication complexity, strong robustness and good utility.
[privacy-preserving distributed set intersection, Protocols, Computational modeling, Estimation, differentially private approaches, randomized response, communication complexity, distributed set intersection, distribution, maximum likelihood estimation, communication overhead, randomized response technique, Privacy, set intersection, secure multiparty computation protocols, local differential privacy, set union, Robustness, data privacy, data contributor, differential privacy]
Efficient Data Blocking and Skipping Framework Applying Heuristic Rules
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Data blocking has been an effective technique of data skipping to reduce data access and shorten query response time in query engines. By generating fine-grained, balanced blocks and corresponding metadata, a query may skip a block if the metadata indicates that the block does not contain relevant data. Obviously, the deciding factor of a promising blocking strategy depends on how to produce effective data layout in reasonable time that is expected to skip most data. In this paper, we propose several algorithms that drastically reduce the time complexity of existent blocking strategies based on workload analysis, at the cost of relatively small loss of estimated tuples could be skipped. Via theoretical analysis, we prove that the time complexity of our algorithms is apparently lower than that of ward algorithm. Afterwards, we demonstrate the whole blocking and skipping workflow, install it into Spark SQL and obtain experimental evaluation results. Experimental results show that our technique gains significant improvement in aspect of blocking efficiency compared to ward algorithm, while keeping almost the same level of skipping ability.
[metadata, query engines, Metadata, data skipping, heuristic rules, fine-grained blocks, Engines, data blocking, query processing, Clustering algorithms, meta data, workload analysis, Spark SQL, Data warehouses, time complexity, data blocking and skipping framework, workload, SQL, Layout, query response time, data access, balanced blocks, Time factors, Time complexity, computational complexity]
Efficient GPU-Based Query Processing with Pruned List Caching in Search Engines
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
There are two inherent obstacles to effectively using Graphics Processing Units (GPUs) for query processing in search engines: (a) the highly restricted GPU memory space, and (b) the CPU-GPU transfer latency. Previously, Ao et al. presented a GPU method for lists intersection, an essential component in AND-based query processing. However, this work assumes the whole inverted index can be stored in GPU memory and does not address document ranking. In this paper, we describe and analyze a GPU query processing method which incorporates both lists intersection and top-k ranking. We introduce a parameterized pruned posting list GPU caching method where the parameter determines how much GPU memory is used for caching. This method allows list caching for large inverted indexes using the limited GPU memory, thereby making a qualitative improvement over previous work. We also give a mathematical model which can identify an approximately optimal choice of the parameter. Experimental results indicate that this GPU approach under the pruned list caching policy achieves better query throughput than its CPU counterpart, even when the inverted index size is much larger than the GPU memory space.
[search engines, AND-based query processing, Instruction sets, highly restricted GPU memory space, Graphics Processing Units, Graphics processing units, cache storage, Indexes, graphics processing units, GPU, pruned list caching policy, query processing, Query processing, CPU-GPU transfer, Caching, Memory management, GPU query processing, Search engines, Query Processing, Acceleration, parameterized pruned posting list GPU caching, lists intersection]
Exploiting RDMA for Distributed Low-Latency Key/Value Store on Non-volatile Main Memory
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
In-memory key/value stores have been widely deployed in many large systems for high throughput and low latency data access. To offer durability and fault tolerance, many of them (e.g., Memcached, Redis) flush data to RAID or SSD in local and remote with primary backup replication (PBR), in which the RAID and network become the bottleneck of throughput and latency. Recently, two promising technologies are emerging, i.e., non-volatile memory (NVM) and remote direct memory access (RDMA). NVMs are integrating into the main memory for their high density, high access speed, low cost and byte addressability. And RDMA allows direct access to the memory of a remote machine without invoking the peer CPU, thus reducing latency. To overcome the limitations of traditional key/value stores, this paper explores the design of a distributed low-latency key/value store on non-volatile main memory by leveraging RDMA. NVM not only eliminates disk IO operations, but also makes fast repli- cation possible. For a PUT operation, we collect modifications (data and metadata) at the local NVM of the primary server and replicate them to the same positions of backup servers by RDMA writes. A prototype distributed key/value store, called NVDS, is designed and implemented. Extensive experiments on the micro benchmark show that NVDS achieves significant latency reduction, compared to existing key/value stores. It replicates a 32bytes key/value PUT operation to two backup servers in less than 2s and the whole request latency is less than 10s.
[memory size 32 Byte, NVM, high throughput data access, Random access memory, remote direct memory access, Throughput, Servers, remote machine, storage management, Nonvolatile memory, Semantics, Distributed databases, nonvolatile main memory, distributed low-latency key, primary backup replication, RDMA, latency reduction, random-access storage, fault tolerance, SSD, flush data, in-memory key, value PUT operation, Redis, byte addressability, low latency data access, RAID, Key value, Distributed storage, direct access, backup servers, NVDS, Low latency, Payloads, Memcached, value stores]
Exploring Synchronization in Cache Coherent Manycore Systems: A Case Study with Xeon Phi
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Intel Xeon Phi is a many-core architecture, featuring more than 50 cores and 200 hardware threads. Given this scale and its other distinctive architectural features, highly-concurrent applications on Xeon Phi may behave differently than on tradi- tional multi-core systems. Yet, concurrency issues especially for synchronization intensive applications on this platform have not been thoroughly analyzed. In this paper, we conduct an extensive analysis at multiple layers, from the underlying hardware cache- coherence protocol up to the user-level applications, aiming to present the most exhaustive study of synchronization on Xeon Phi. Through a range of benchmarks, we testify the feasibility and advantage of accelerating concurrent applications with Xeon Phi. Meanwhile, we identify severe scalability issues relevant to synchronization, and solutions to these issues are discussed. We believe this work can be used as guidelines both for designing better synchronization mechanisms and in optimizing concurrent applications in order to fully exploit the capability of Xeon Phi.
[Protocols, multiprocessing systems, Scalability, Instruction sets, hardware cache- coherence protocol, many-core architecture, cache storage, coprocessors, Synchronization, synchronisation, highly-concurrent applications, multicore systems, optimisation, cache coherent manycore systems, synchronization intensive applications, Intel Xeon Phi, optimization, Many-core, Computer architecture, Benchmark testing, Hardware, Concurrent applications]
Extending Blockchain Functionality with Statechain
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Blockchain has attracted great attention as the basis of cryptocurrencies such as Bitcoin, but its capabilities extend far beyond that. Blockchain has the potential to revolutionize applications because it provides a transparent, immutable, and append-only ledger that can be used to build new decentralized applications. However, it is hard to implement changes to Bitcoin without forking blockchain. As a result, Bitcoin has difficulty in adapting to new demands and extending new functionality. The traditional way to introduce new functionality is to modify Bitcoin's code base, but which will lead to some problems such as (a) the security of blockchains, (b) the allocation of currency, and (c) the waste of resources. We present a novel design, statechain, which uses Bitcoin blockchain to propagate application log. It can add new functionality without requiring blockchain fork changes from Bitcoin. Statechain enables application nodes to efficiently query the log as well as transfer log between blockchains. We detail how the application nodes achieve application-level consensus at each block. As far as we know, this technology has not been exploited. However, it has a great practical significance that enables the introduction of new functionality safer and makes the development of blockchain-based applications easier. We have used statechain to explore the field of transferring bitcoins and other cryptocurrencies directly between multiple blockchains.
[cryptocurrencies cross chain exchange;, Protocols, application log, Bitcoin blockchain, Bitcoin, cryptography, Bitcoins code base, Relays, cryptocurrencies, application-level consensus, electronic money, statechain, blockchain functionality extension, blockchain functionality, Peer-to-peer computing, financial data processing, application level-consensus, bitcoin]
Fast Parallel Recovery of Many Small In-Memory Objects
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Social media networks as well as online graph analytics operate on large-scale graphs with millions of vertices, even billions in some cases. Low-latency access is essential, but caching suffers from the mostly irregular access patterns of the aforementioned application domains. Hence, distributed in-memory systems are proposed keeping all data always in memory. These in-memory systems are typically not optimized for the sheer amounts of small data objects, which demands new concepts regarding the local and global data management as well as for the fault-tolerance mechanisms to mask server failures and power outages. In this paper, we propose a novel backup distribution and parallel recovery approach aiming at fast recovery of servers storing hundreds of millions of small objects. All proposed concepts have been implemented within the open source distributed system DXRAM and have been evaluated in the Microsoft Azure cloud with up to 72 high performance virtual machines in two scale-sets. For evaluation, we used two benchmarks: the Yahoo! Cloud Serving Benchmark and a recovery benchmark. The experiments show that the proposed recovery strategy is able to recover servers with 500,000,000 small data objects in less than 2 seconds and, also, to efficiently mask server failures under heavy load. Furthermore, DXRAM outperforms the state-of-the-art system RAMCloud in additional recovery experiments with large objects (2.4&#x00D7; faster) and even more with small objects (&gt; 9&#x00D7;).
[Cloud computing, recovery strategy, online graph analytics, parallel recovery approach, graph theory, Random access memory, Microsoft Azure cloud, open source distributed system DXRAM, RAMCloud, power outages, Servers, Cloud Serving Benchmark, novel backup distribution, Fault tolerance, storage management, Main memory, fault-tolerance mechanisms, Fault tolerant systems, Distributed databases, Storage recovery strategies, cloud computing, global data management, Remote replication, data objects, high performance virtual machines, small in-memory objects, Data centers, server failures, Flash memory, distributed in-memory systems, virtual machines, B-trees, social media networks, large-scale graphs, local data management, fault tolerant computing, low-latency access, Peer-to-peer computing, Power system reliability, Reliability, parallel recovery]
Feature Guided In-Situ Indices Generation and Data Placement on Distributed Deep Memory Hierarchies
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
In-situ analytics have been increasingly adopted by leadership scientific applications to gain fast insights into massive output data of simulations. Current practice buffers the output data in DRAM for analytics processing, constraining it to DRAM capacity un-used by the simulation. The rapid growth of data size requires alternative approaches to accommodating data-rich analytics, such as using solid-state disks (SSDs) to increase effective memory capacity. For this purpose, this paper explores software solutions for exploring the deep memory hierarchies expected on future high-end machines. Leveraging the fact that many analytics are sensitive to data features (regions- of-interest) hidden in the data being processed, the approach incorporates the knowledge of the data features into in-situ data management. It uses adaptive index creation/refinement to reduce the overhead of index management. In addition, it uses data features to predict data skew and improve load balance through controlling data distribution and placement on distributed staging servers. The experimental results show that such feature-guided optimizations achieve substantial improvements over state-of-the-art approaches for managing output data in-situ.
[DRAM capacity, data-rich analytics, data management, data distribution, distributed deep memory hierarchies, Random access memory, Servers, Analytical models, storage management, distributed staging servers, Feature Guided In-Situ Indices Generation, In-situ analytics, resource allocation, index management, data placement, adaptive index refinement, feature-guided optimizations, output data in-situ management, DRAM chips, solid-state disks, data features, data skew, distributed deep memory, R-tree, SSD, data size, Data processing, analytics processing, Octree, Indexes, adaptive index creation, In-situ Analytics, Octrees, leadership scientific applications, memory capacity, Data models, deep memory hierarchies, load balance, Indexing]
Fingerprinting Protocol at Bit-Level Granularity: A Graph-Based Approach Using Cell Embedding
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Traffic identification is defined as the act of ascertaining which application or service or protocol is contributing to the network traffic by using a fingerprint, which is a distinguishable unique pattern representing a particular applications traffic. The continual appearance of new applications and their frequent updates emphasize the need for automatic protocol fingerprints generation. In this paper, we propose BitGrapher, a novel graph-based approach that accurately infers protocol fingerprints at bit-level granularity for accurate traffic identification. The proposal is designed to accommodate to various protocol traces including text-based and binary-based protocols, and even possible unknown proprietary communication protocols. Our proposed approach introduces a new concept of cell that allows BitGrapher to encode protocol payloads as a graphical model, and then converts fingerprinting protocol problem as a series of graph operations (e.g., graph construction, pruning, partition). The key insight of the graphical model use cell embeddings that captures the distinguishable positions with their values and distinguishable correlation among them. We implement and evaluate BitGrapher on real-world traces, including DNS, QQLive, SopCast, SMB, HTTP, and SMTP, and our experimental results show that BitGrapher can accurately identify the protocol trace with an average precision of about 97.28% and an average recall of about 99.12%. We also compare the results of BitGrapher to two state-of-the-art approaches ProWord and ProDigger, which shows that BitGrapher provides significant improvements in precision and recall for protocol identification task.
[protocol identification task, Protocols, ProDigger, Terminology, graph theory, cell embedding, unknown proprietary communication protocols, Fingerprint recognition, Embedding, real-world traces, Task analysis, Protocol Identifiction, Graphical models, binary-based protocols, protocol payloads, protocols, text-based protocols, fingerprinting protocol, bit-level granularity, protocol problem, Tools, Protocol fingerprints, network traffic, protocol trace, graphical model, ProWord, BitGrapher, traffic identification, graph operations, telecommunication traffic, automatic protocol fingerprints generation, graph construction, Payloads]
GraphMP: An Efficient Semi-External-Memory Big Graph Processing System on a Single Machine
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Recent studies showed that single-machine graph processing systems can be as highly competitive as cluster-based approaches on large-scale problems. While several out-of-core graph processing systems and computation models have been proposed, the high disk I/O overhead could significantly reduce performance in many practical cases. In this paper, we propose GraphMP to tackle big graph analytics on a single machine. GraphMP achieves low disk I/O overhead with three techniques. First, we design a vertex-centric sliding window (VSW) computation model to avoid reading and writing vertices on disk. Second, we propose a selective scheduling method to skip loading and processing unnecessary edge shards on disk. Third, we use a compressed edge cache mechanism to fully utilize the available memory of a machine to reduce the amount of disk accesses for edges. Extensive evaluations have shown that GraphMP could outperform state-of-the-art systems such as GraphChi, X-Stream and GridGraph by 31.6&#x00D7;, 54.5&#x00D7; and 23.1&#x00D7; respectively, when running popular graph applications on a billion-vertex graph.
[single-machine graph processing systems, disk accesses, Computational modeling, efficient semiexternal-memory big graph processing, trees (mathematics), Graph Processing, Parallel Computing, Big Data, cache storage, Servers, disk I/O overhead, compressed edge cache mechanism, Microsoft Windows, selective scheduling method, Memory management, vertex-centric sliding window computation model, Venus, Data models, out-of-core graph processing systems, computation models, Load modeling, GraphMP]
HARS: A Hybrid Adaptive Routing Scheme for Underwater Sensor Networks
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Underwater sensor networks have many applications ranging from ocean monitoring, undersea exploration, target tracking, coastal surveillance, to disaster prevention. In multi-application scenarios, the network might need to handle different types of packets to satisfy the requirements of diverse data transmission metric. For example, multimedia-based applications may include different multimedia packets, such as voice, compressed images, even video streams with different quality of experience. To meet the requirements of such applications, in this paper, we propose a hybrid adaptive routing scheme (HARS) for drifting restricted floating ocean sensor networks (DR-OSNs), which exploits both surface wireless and underwater communication channels to fulfill different performance requirements. We evaluate the performance of the routing scheme and investigate the factors which affect the scheme. The simulation results demonstrate that the scheme achieves a reasonable performance for different communication channels and packet delivery.
[Sea surface, Adaptive systems, multimedia-based applications, wireless sensor networks, hybrid adaptive routing scheme, surface wireless channels, underwater communication channels, packet delivery, disaster prevention, Routing protocols, HARS, multimedia communication, performance requirements, drifting restricted floating ocean sensor networks, coastal surveillance, ocean monitoring, Routing, Underwater Sensor Networks, multimedia packets, underwater acoustic communication, Surveillance networks, Wireless sensor networks, underwater sensor networks, undersea exploration, telecommunication network routing, target tracking, Streaming media, diverse data transmission metric, multiapplication scenarios]
High Performance and Scalable Virtual Machine Storage I/O Stack for Multicore Systems
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Today extending virtualization technology into high-performance, cluster platforms generates exciting new possibilities, including dynamic allocation of resources to job, easier to share resources between different jobs, easy checkpointing of jobs, and deployment of job-specific work environment. However, there still exists an I/O scalability problem in virtualization layer which may impede virtualization technology to be widely used in high-performance computing. Because we meet a sharp performance degradation when a virtual machine uses the multiqueue high performance non-volatile storage device as the secondary storage. Such a problem is caused by the current virtual block I/O layer which uses only one I/O thread to handle all I/O operations to a virtualized storage device. As the number of I/O intensive workloads increases, the rate of mutex contention of the I/O thread is accelerated because only one of them is allowed to run at any given instant. Therefore, it is the key problem that should be settled immediately so as to improve block I/O performance in virtualization. In this paper, we propose a novel design of high performance block I/O stack to solve this problem. The workloads will be free of the I/O contention inside the hypervisor by using the proposed method which uses multi-threaded I/O threads to handle all I/O operations to one storage device in parallel. Meanwhile, we use switch-less mechanisms to reduce the overhead caused by sending notification between a VM and its hypervisor; and improve I/O affinity by assigning a distinct dedicated core to each I/O thread in order to eliminate unnecessary scheduling. The prototype system is implemented on Linux 3.19 kernel and Quick Emulator (QEMU) 2.3.1. We deploy it to the POWER8 server for a detailed evaluation. The experimental results show that the proposed architecture scales graciously with multi-core environment. For example, test on 10-ways parallel I/O intensive workloads gets an 800\\% increase than the single core implementation, indicating that the block I/O performance in a virtual machine is close to that of a bare metal system.
[Performance evaluation, Scalability, job-specific work environment, input-output programs, virtualisation, Block I/O, parallel processing, resource allocation, virtualization technology, I/O operations, virtualized storage device, Hardware, High-performance, high-performance computing, multiprocessing systems, Multicore processing, multi-threading, virtual storage, dynamic resource allocation, Virtual machining, multicore systems, Virtual machine monitors, multiqueue high performance nonvolatile storage device, scalable virtual machine storage, cluster platforms, virtual machines, multi-threaded I/O threads, Multi-threaded, I/O stack, Virtualization]
iCAST: Accelerating High-Performance Data Center Applications by Hybrid Electrical and Optical Multicast
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
One-to-many group communication is a performance bottleneck for high-performance data center applications, due to sending massive data from one source to hundreds of receivers. The state-of-the-art solutions utilize either electrical packet switch (EPS) or optical circuit switch (OCS) multicast to accelerate massive data disseminations. However, there exist competitions between multicast and unicast flows at core EPSes in the electrical multicast. Moreover, the optical multicast suffers from a non-negligible reconfiguration delay and exclusive optical links. In this paper we present iCAST, a system for reducing multicast flow completion time (MFCT) on a generic hybrid EPS/OCS network, which has multiple EPSes and OCSes supporting multicast. iCAST constructs multicast trees by integrating the electrical and optical multicast to fully utilize network resources, and seamlessly schedules flows between the static electrical and dynamic optical networks to reduce the configuration overhead. We evaluate the performance by implementing a small-scale hybrid EPS/OCS testbed and extending the high-performance framework MPICH to support iCAST. Experiments show that iCAST outperforms one order of magnitude in reducing MFCT compared with the ring algorithm. We also develop an event-based flow level simulator to evaluate the performance of iCAST at the scale of thousands of servers. Simulation results show that iCAST reduces the average MFCT by 32% and 28% compared to OCS and EPS respectively, and significantly outperforms binomial tree and ring algorithm by up to 64% and 46% respectively.
[Integrated optics, optical switches, packet switching, optical multicast, hybrid electrical multicast, Optical fiber networks, telecommunication scheduling, circuit switching, iCAST, high-performance framework MPICH, multicast communication, electrical multicast, event-based flow level simulator, Optical packet switching, Optical switches, computer networks, trees (mathematics), multicast flow completion time reduction, optical circuit switch, Optical receivers, small-scale hybrid EPS-OCS, computer centres, nonnegligible reconfiguration delay, MFCT, multicast, Data centers, massive data disseminations, multicast trees, generic hybrid EPS-OCS network, telecommunication network routing, exclusive optical links, static electrical networks, optical fibre networks, high-performance data center applications, telecommunication traffic, dynamic optical networks, data center network]
Loc-K: A Spatial Locality-Based Memory Deduplication Scheme with Prediction on K-Step Locations
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Memory deduplication is a technique to eliminate redundant data, save memory space and improve the performance of the whole system. There are several effective deduplication algorithms, which identify replicated data via comparing the content of different pages. However, although a few literatures utilize spatial locality to improve the efficiency of memory deduplication [1][2], they still have several limitations, such as low ratio of continuous distribution and high failure rate of prediction under unstable environments. To address these problems, in this paper, we design a new memory deduplication algorithm called &#x201C;Loc-K&#x201D;. On one hand, it utilizes logical addresses of different pages to ensure better continuity, which can gain better spatial locality. On the other hand, Loc-K predicts K potential duplication locations as the targets for page scanning, which improves the prediction hit ratio. Furthermore, Loc-K merges the duplicated pages directly to avoid regular searching routines. To demonstrate the effectiveness of our algorithm, we conduct several experimentations via implementation in Linux Kernel. The results show that, compared to the state-of-the-art memory deduplication algorithms, Loc-K increases the predictable opportunity by up to 97.8%, increases the prediction hit ratio by up to 96.5%, and reduces the duplication identification time by at least 34.3% respectively.
[operating system kernels, paged storage, K potential duplication locations, Conferences, page scanning, Prediction, continuous distribution, K-step locations, Duplication Identification, memory space, predictable opportunity, redundant data, memory deduplication algorithm, Performance Evaluation, duplicated pages, Linux, spatial locality, failure rate, Spatial Locality, Loc-K, Linux Kernel, Memory Deduplication]
Managing Persistent Objects with a Unified Access Framework in Persistent Memory
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Research on SCM tends to follow the conventional path: using part of persistent memory as persistent heap to recover data sets after power failure; and using part of persistent memory as persistent storage to save files abstracted by files systems. However, file abstraction causes a big performance gap between persistent heap and persistent storage. persistent data objects in persistent heap is difficult for reuse and management because of these objects losing their address meaningfulness while process is over. We propose a unified access framework to manage persistent objects in persistent heap, which provides a unified access manner for persistent objects both in persistence (storage) and in temporary (memory), not requiring serializing and de-serializing operations for file abstraction. Persistent objects are organized according to application data structures in temporary, and independently exist out of process in persistence, even the process is over. Compared to PMFS, PXFS, NV-Heap, PMSE can get 1.8X, 1.1X, 44% performance gains respectively under the workloads with mixed data sets (50% of reads, 50% of writes).
[object-oriented programming, persistent storage, Prefetching, Random access memory, Metadata, Data structures, persistent heap, unified access framework, storage management, application data structures, File systems, Persistent Object, persistent object management, Memory management, Loading, Persistent Heap, file abstraction, Persistent Memory, data set recovery, data structures, SCM, persistent memory, persistent objects, persistent data objects]
Virtual Machine Placement for Hybrid Cloud Using Constraint Programming
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Cloud computing is the widely spread paradigm of utility-computing that offers an "on-demand" internet-based access to configurable resources available within data centers. On one hand, public Cloud providers are well suited for highly available access to IT resources (infrastructure, platform and software), for sporadic use, or for elastic demands. On the other hand, private clouds could sometimes be preferred for security or privacy reasons, or for cost reasons due to a high frequency usage of services. However, in many cases a choice between public or private clouds does not fulfill all requirements of companies, and hybrid cloud infrastructures should be preferred. A hybrid cloud solution could, for example, answer sudden workload increase in private clouds, security or fault tolerance requirements, or even latency issues thanks to data-locality. Solutions have already been proposed to address hybrid cloud infrastructures, however most of the time the placement of a distributed software on such infrastructure has to be indicated manually. For this reason, the automation of software deployment on hybrid clouds is still under research. In this paper we propose new specific placement constraints and objectives adapted to hybrid clouds infrastructures within our placement solution, namely OptiPlace, and we address this problem through constraint programming. Furthermore, we evaluate the expressivity and performance of the proposed solution on a real case study.
[Cloud computing, virtual machine placement, hybrid cloud infrastructures, Companies, Programming, private clouds, Virtual machining, Entropy, OptiPlace, Servers, computer centres, public cloud providers, power aware computing, distributed software, virtual machines, cloud computing, constraint handling, Virtual machine placement, constraint programming, Hybrid Cloud]
tScale: A Contention-Aware Multithreaded Framework for Multicore Multiprocessor Systems
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
On the multicore and multiprocessor system, multithreaded applications which are kernel-intensive usually suffer from two kinds of performance issues, first one is frequent context switch between kernel/user mode. Another one is lock contention caused by non-scalable synchronization primitives (e.g., ticket spin lock) and may even result in performance degradation under heavy contention level. Unfortunately, current Linux threading model (i.e., NPTL) which adopts exception-based system call mechanism fails to reduce the excessive system call cost. Besides, conventional threading scheduler which is unconscious of lock contention also lacks the ability to limit the number of system-wide contending parallel threads. Both of them impede the application's throughput increment and may lead to the performance breakdown eventually. In this paper we propose a contention-aware threading framework to alleviate these two problems. Our proposed design is composed of two tightly contected components: system call batching via user-level thread library and a contention-aware scheduler based on non-work-conserving scheduling policy. The user-level threading library gathers multiple system call invocations transparently and deliverys these requests to the underlaying kernel working threads. Therefore, tScale improves application performance by reducing massive context switch cost. Then through continuing monitoring system-wide lock contention level and application's total throughput increment, tScale can quickly adjust the number of contending threads in order to sustain the maximum throughput. The prototype system is implemented on Linux 3.18.30 and Glibc 2.23. In microbenchmarks on a 32-core machine, experiment results show that our approach can not only improve the application throughput by up to 20% but also address the lock contention efficiently.
[Instruction sets, tScale, context switch cost reduction, Linux 3.18.30, lock contention, Switches, system call batching, multicore, storage management, kernel working threads, system-wide contending parallel threads, contention-aware scheduler, system-wide lock contention level monitoring, system call;, nonwork-conserving scheduling policy, contention-aware multithreaded framework, scheduling, application throughput, user-level threading library, multicore multiprocessor systems, Kernel, Message systems, operating system kernels, multiprocessing systems, multi-threading, ticket spin lock, performance degradation, Synchronization, system call invocations, user-level thread library, nonscalable synchronization primitives, Linux, thread scheduling, Spinning, Glibc 2.23]
Supervised Learning Based Algorithm Selection for Deep Neural Networks
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Many recent deep learning platforms rely on thirdparty libraries (such as cuBLAS) to utilize the computing power of modern hardware accelerators (such as GPUs). However, we observe that they may achieve suboptimal performance because the library functions are not used appropriately. In this paper, we target at optimizing the operations of multiplying a matrix with the transpose of another matrix (referred to as NT operation hereafter), which contribute half of the training time of fully connected deep neural networks. Rather than directly calling the library function, we propose a supervised learning based algorithm selection approach named MTNN, which uses a gradient boosted decision tree to select one from two alternative NT implementations intelligently: (1) calling the cuBLAS library function; (2) calling our proposed algorithm TNN that uses an efficient out-of-place matrix transpose. We evaluate the performance of MTNN on two modern GPUs: NVIDIA GTX 1080 and NVIDIA Titan X Pascal. MTNN can achieve 96% of prediction accuracy with very low computational overhead, which results in an average of 54% performance improvement on a range of NT operations. To further evaluate the impact of MTNN on the training process of deep neural networks, we have integrated MTNN into a popular deep learning platform Caffe. Our experimental results show that the revised Caffe can outperform the original one by an average of 28%. Both MTNN and the revised Caffe are open-source.
[Machine learning algorithms, MTNN, Graphics processing units, supervised learning, GPUs, NVIDIA GTX 1080, TNN algorithm, Deep Neural Networks, GPU, out-of-place matrix transpose, gradient boosted decision tree, hardware accelerators, deep learning platform, Matrix Multiplication, Libraries, cuBLAS library function, learning (artificial intelligence), Kernel, Linear Algebra, fully connected deep neural networks, Artificial neural networks, Transpose, graphics processing units, selection approach, suboptimal performance, algorithm selection, NVIDIA Titan X Pascal, Machine learning, decision trees, NT operation, Caffe, neural nets]
Scalable Blockchain Based Smart Contract Execution
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Blockchain, or distributed ledger, provides a way to build various decentralized systems without relying on any single trusted party. This is especially attractive for smart contracts, that different parties do not need to trust each other to have a contract, and the distributed ledger can guarantee correct execution of the contract. Most existing distributed ledger based smart contract systems process smart contracts in a serial manner, i.e., all users have to run a contract before its result can be accepted by the system. Although this approach is easy to implement and manage, it is not scalable and greatly limits the system's capability of handling a large number of smart contracts. In order to address this problem, we propose a scalable smart contract execution scheme that can run multiple smart contract in parallel to improve throughput of the system. Our scheme relies on two key techniques: a fair contract partition algorithm leveraging integer linear programming to partition a set of smart contracts into multiple subsets, and a random assignment protocol assigning subsets randomly to a subgroup of users. We prove that, our scheme is secure as long as more than 50% of the computational power is possessed by honest nodes. We then conduct experiments with data from existing smart contract system to evaluate the efficiency of our scheme. The results demonstrate that our approach is scalable and much more efficient than the existing smart contract platform.
[Protocols, Scalability, integer programming, scalable blockchain, decentralized systems, cryptography, linear programming, contracts, fair contract partition algorithm, scalability, smart contract, blockchain, Public key, smart contracts, distributed ledger, Load management, scalable smart contract execution scheme, Contracts, electronic commerce]
Routing in IoT Network for Dynamic Service Discovery
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Many existing IoT systems are built with predefined tasks and system architecture. To make better use of the large number of &#x201C;things&#x201D; availability in the IoT infrastructure, we consider to dynamically discover and compose IoT capabilities in the IoT network for new or dynamically arising tasks. To achieve the goal, we develop a semantic-based routing protocol for IoT service discovery. In the routing protocol to reduce the space requirement for routing, each node maintains a routing table with a &#x201C;telescopic view&#x201D;, i.e., more precise information about the capabilities of immediate neighbors and only a summary of capabilities for a further away neighborhood. We define an ontology of capabilities and design a capability summarization algorithm based on the ontology to realize the telescopic view concept. We also design an ontology coding scheme to further reduce the memory requirement for routing while enabling capability summarization. Our solutions significantly reduce the routing table size and, hence, is suitable for IoT devices with limited memory. Also, experimental results show that our approach can yield significantly lower network traffic and latency for IoT capability lookup when compared with existing semantic based routing algorithms.
[Internet-of-Things, Routing Table Summarization, Ontology Coding, Ontology, dynamic service discovery, Ontologies, Routing, Encoding, IoT service discovery, capability summarization algorithm, Internet of Things, Semantic-based Routing, Dynamic IoT Service Discovery, ontology of capabilities, Semantics, routing protocols, routing table, Cameras, ontologies (artificial intelligence), semantic-based routing protocol, Routing protocols, Peer-to-peer computing, IoT network routing]
RING: NUMA-Aware Message-Batching Runtime for Data-Intensive Applications
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
We present RING, a NUMA-aware Message-batching runtime system for in-memory data-intensive applications. This library allows users to focus on developing algorithms for big data analysis, rather than wrestling with synchronization, data consistency, and memory management. The goal of RING is to improve efficiency mainly for irregular applications, which means less CPU stall on local or remote memory access. RING adopts partitioned global address space (PGAS) model to manage the memory and leverages one-sided RDMA verbs to directly write the message into the NUMA-aware buffer in the remote node. A coroutine yields after posting a long-latency request, allowing considerable overlap of computation and communication. We compare our design with Grappa [21], the state-of-the-art DSM runtime. The experimental results show that RING is 42%~85% faster than Grappa on RandomAccess benchmark, and 1.4&#x00D7;~3.7&#x00D7; faster than Grappa on several graph benchmarks.
[long-latency request, one-sided RDMA, Runtime system, In-memory, PGAS, Task analysis, parallel processing, storage management, memory management, Runtime, RING, Bandwidth, Benchmark testing, RandomAccess benchmark, shared memory systems, NUMA-aware buffer, in-memory data-intensive applications, RDMA, Data intensive, data analysis, Big Data, performance evaluation, remote memory access, state-of-the-art DSM runtime, data consistency, Graph, Memory management, distributed shared memory systems, partitioned global address space model, NUMA-aware Message-batching runtime system, Electronics packaging, big data analysis, local memory access]
REMOLD: An Efficient Model-Based Clustering Algorithm for Large Datasets with Spark
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Density-based clustering algorithms have the distinctive advantage of discovering arbitrarily shaped clusters, but they usually require a procedure to compute the distance between every pair of data points, and this procedure is prohibitive for large datasets since it has quadratic computation complexity. In this paper, we propose a new distributed clustering algorithm, named REstore MOdel with Local Density estimation (REMOLD). Firstly, REMODL applies a balanced partitioning method to evenly divide an large dataset based on Local Sensitive Hashing (LSH). Then, it locally clusters each partition of the dataset, and uses a Gaussian model to represent each local cluster based on the observation that the density distribution of each local cluster shares similar shape with Gaussian distribution. Finally, these models are aggregated on a server where REMOLD restores global clusters based on these local Gaussian models. More specifically, model connection, which measures the density connectivity between two models, are defined to merge local models with an optimized procedure. In this aggregation, REMOLD requires low cost of network transmission for local Gaussian models, since the number of Gaussian models is often less than that of core objects for each partition. We evaluate REMOLD on three synthetic datasets and three real-world datasets on Spark, and the experiment results demonstrate that REMOLD is efficient and effective to find out clusters with complex shapes and it outperforms the established methods.
[Local Density estimation, quadratic computation complexity, Gaussian distribution, real-world datasets, REMOLD, global clusters, Clustering algorithms, density distribution, Gaussian model, complex shapes, REstore MOdel with Local Density estimation, Kernel, density estimation, Load modeling, synthetic datasets, Local Sensitive Hashing, arbitrarily shaped clusters, balanced partitioning method, density-based clustering, Computational modeling, distributed clustering, Estimation, density connectivity, model connection, Partitioning algorithms, Sparks, local Gaussian models, pattern clustering, Model-Based Clustering Algorithms, Spark, distributed clustering algorithm, optimized procedure, dataset, computational complexity]
Practical Concurrent Self-Organizing Lists
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Lists are one of the most fundamental and ubiquitous data structures. A self-organizing list can adjust the structure according to the request sequence to improve average access time. However, it is a challenge to obtain a practical concurrent self-organizing linked-list with lock-free property, which guarantees that some operation finishes when any thread executes a finite number of steps. This paper implements a novel lock-free self-organizing list whose items are moved to the front when being requested. The implementation uses a new log method and a new help mechanism. The log consists of operation nodes which record the execution history. Friend chains are formed during list traversal to help related operations. This paper also introduces a speed-up variation which is more pragmatic. Furthermore, this paper presents a comprehensive evaluation in experiments on non-uniform data set. Experiment demonstrates the advantages of the new linked-lists among other algorithms in throughput and scalability. On requests derived from real workloads, it achieves 22% to 184% speedup than the existing concurrent lists.
[lock-freedom, practical concurrent self-organizing linked-list, execution history, Instruction sets, help mechanism, Throughput, lock-free property, History, ubiquitous computing, average access time, linearizability, nonuniform data, request sequence, data structures, practical concurrent self-organizing lists, Monitoring, operation nodes, nonblocking, concurrent data structures, Data structures, list traversal, self-organizing lists, Pragmatics, Computer science, novel lock-free self-organizing list, log method, ubiquitous data structures, linked-lists, fundamental data structures]
PBUF: Sharing Buffer to Mitigate Flooding Attacks
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Software defined networking (SDN) is a promising network architecture, which decouples the control plane and data plane of a network. However, SDN opens some security challenges, such as man-in-the-middle attacks, spoofing attacks, flooding attacks and so on. In this paper, we focus on flooding attacks which consume the switch buffer and controller resource resulting in SDN framework resource overloaded. To prevent SDN framework from flooding attack, we present a defense approach called PBUF (Packet forwarding based on BUFfer sharing), which pools the idle switches to mitigate threat issues. This approach consists of buffer management and packet forwarding modules. The buffer management module gleans the statistics of incoming packets and then analyzes these statistics to estimate the buffer size by network calculus. Considering that a lot of table-miss packets will be generated and stored in buffer when the flooding attack is happening, the packet forwarding module is designed to forward these table-miss packets to idle switches to prevent the switch or controller to be overloaded. These table-miss packets will be buffered in idle switches and then sent to controller in a limited rate by generating packet_in messages. The simulation results show that PBUF is effective and only introduces a little overhead in SDN framework.
[Buffer storage, man-in-the-middle attacks, Switches, Calculus, Electronic mail, Security, calculus, buffer management module, SDN framework resource, spoofing attacks, SDN, Flooding Attack, flooding attack mitigation, Process control, PBUF, software defined networking, computer network management, switching networks, switch buffer, network calculus, packet forwarding based on buffer sharing, packet forwarding module, table-miss packets, statistical analysis, Performance]
Optimize the FP-Tree Based Graph Edge Weight Computation on Multi-core MapReduce Clusters
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
The FP-tree based edge weight computation (EWC for short) with MapReduce has demonstrated its remarkable performance for extracting weighted graphs from big data for data analysis. However, our investigation finds that existing algorithm includes unnecessary scan on the datasets as well as unnecessary information for the FP-tree construction, which prolong the runtime execution. In addition, applying inappropriate Reducers-to-cores mapping strategy may make it exhaust the resources and fail to complete the job execution. This paper designs, implements and evaluates an optimized FP-tree based graph EWC algorithm with MapReduce on Multi-core Clusters. First, we design a more compact FP-tree based EWC with 2-phase MapReduce, reducing one phase scan of the dataset. Second, we propose a reduced FP-tree data structure to reduce the FP-tree construction cost. Third, we examine two strategies for mapping Reducers to cores for EWC on each multi-core computer: one-Reducer-one-core and one-Reducer-multiple-cores. Finally, an empirical comparison performance study has been carried out on the optimized EWC algorithm against the existing one over a massive application dataset generated by a real social network. The results demonstrate that the optimized FP-tree based EWC algorithm obtains about 39% to 55% percentage improvement in execution time, and in the meantime achieves better scale-out and scale-up speedup. This paper's findings can also be applied to improve the scalability and efficiency of the parallel and distributed execution of applications involving large scale all-pairs set intersection computation over multi-core MapReduce clusters.
[Multicore Clusters, optimized EWC algorithm, Data mining, Task analysis, parallel processing, MapReduce, 2-phase MapReduce, optimisation, Itemsets, weighted graphs, Clustering algorithms, mapping Reducers, multicore MapReduce clusters, tree data structures, FP-tree construction cost, big data, compact FP-tree, multiprocessing systems, Multicore processing, data analysis, trees (mathematics), reduced FP-tree data structure, Big Data, Data structures, FP-tree, multicore computer, FP-tree based Graph Edge Weight Computation, graph EWC algorithm, Reducers-to-cores mapping strategy, one-Reducer-one-core, Feature extraction, one-Reducer-multiple-cores, dataset, Edge Weight Computation, Multi-core Clusters, Weighted Graph Extraction]
CMIP: Data Transmission Latency Optimization for Cooperative Group in Multi-cloud by Adaptive Routing
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
The proliferation of real-time applications, such as online gaming, video chatting, brings unprecedented pressure for multi-cloud application providers to secure the good quality of experience. These applications are designed for cooperative group users/members/clients and are expected to have low data transmission latency due to frequent interactions. We use total data transmission latency (TDTL) to represent the group latency in this paper. However, existing approaches perform poorly since they often ignore the feature of the cooperative group scenario and cannot provide a flexible rental cost model for application providers. Minimizing TDTL is challenging due to the difficulties in 1) Finding an effective method to transmit data through multi-cloud datacenters, 2) Taking the cooperative group scenario into consideration, 3) Providing a flexible transmission rental cost model. To this end, we propose CMIP, an innovative approach that aims to minimize the TDTL by renting virtual machines of well-connected datacenters as the proxy for the cooperative group scenario in multi-cloud providers. First, a weighted graph is constructed to illustrate the multi-cloud network. Second, we define a path latency model and a rental cost model for the cooperative group. After that, Yen's algorithm and mixed integer programming are adopted to optimize the TDTL with rental cost constraint. To study the performance of CMIP, we conduct extensive evaluations using multiple real data traces and compare it with related approaches. The comprehensive evaluation analysis shows that the CMIP achieves much lower total latency and is more flexible than other approaches.
[Cloud computing, weighted graph, integer programming, graph theory, data transmission latency, Servers, Optimization, CMIP, real-time applications, total data transmission latency optimization, Yen algorithm, multi-cloud, Real-time systems, Data communication, cloud computing, adaptive routing, Linear programming, TDTL minimization, cooperative group users/members/clients, multicloud network, multicloud application providers, computer centres, cooperative group, rental cost, virtual machines, mixed integer programming, rental cost constraint, flexible transmission rental cost model, Delays, multicloud datacenters, minimisation, path latency model]
Scalable Hash Ripple Join on Spark
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Hash Ripple join is an online aggregation algorithm that can rapidly give good approximate join results increases with the progress of the join operation and converges to the real result when the join finishes. Luo et al. proposed a parallel hash ripple join (PHRJ) that runs in a distributed setting. However, the PHRJ has two drawbacks when handling large-scale data: 1) PHRJ updates approximate results in fine grain which induces extra communication cost in a distributed environment 2) When data is out of memory, PHRJ cannot provide unbiased approximate result. In this thesis, a scalable hash ripple join is proposed that 1) runs on a distributed framework that can process distributed data in coarse-grain to speed up the join performance; 2) continuously gives unbiased and consistent approximate join results even in the presence of memory overflow; and 3) has good scalability handling growing amount of data. We have implemented a prototype of the scalable hash ripple join (SHRJ) algorithm on Spark. Experiment results show that SHRJ can give good approximate join result while taking less than 10% of the time of Spark's own join operator.
[join operator, distributed processing, data aggregation, storage management, scalable hash ripple join, PHRJ, Distributed databases, Clustering algorithms, parallel hash ripple join, consistent approximate, Big data, unbiased approximate, big data, distributed framework, online aggregation algorithm, Big Data, memory overflow, on-line aggregation, Sparks, Indexes, Standards, Computer science, Approximation algorithms, distributed join, Spark]
Ambula: Build Communication Lifeline of Corporations During Emergency
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Many corporations rely on Internet service provider (ISP) network to provide reliable communication services. However, the current communication networks are vulnerable to disruptive events, such as natural disaster or power outage. Such disastrous events may destroy multiple network facilities in a specific region and result in a long term recovery of ISP networks. The disconnected communication will lead to enormous economic loss even if corporation's infrastructure is not directly destroyed during the disaster. Therefore, corporations need a self-rescue mechanism to actively respond to the emergency instead of simply relying on the ISP. This paper proposes Ambula, an easy-to-deploy platform to realize fast congestion-aware recovery for corporation's communication lifeline. Our platform leverages current widely-deployed public cloud services to build a scalable peer-to-peer overlay routing system. By so doing, the corporation is capable of controlling the packets forwarding path to bypass the affected region and congested routes. To this end, Ambula first carefully selects a small set of virtual machines (VMs) from geographically distributed public clouds, and then apply the self-developed congestion-aware routing protocol to achieve automatic and fast routing recovery. Simulations on both random generated and real network topologies show that the high recovery ratio of 80% can be achieved. The congestion avoidance algorithm can significantly reduce the impact of congestion. Our prototype on Emulab shows it can recover within hundreds of milliseconds. To the best of our knowledge, no effective disaster recovery mechanism currently exists for corporations during emergency. Ambula will facilitate the business continuity management of corporations in present of hazard events.
[Cloud computing, geographic routing, disconnected communication, overlay network, congested routes, emergency, Internet service provider network, Relays, recovery ratio, easy-to-deploy platform, packet control, reliable communication services, Routing protocols, disastrous events, computer network reliability, communication networks, disasters, real network topologies, emergency management, corporations, congestion avoidance, self-rescue mechanism, public cloud services, routing protocols, virtual machines, Communication recovery, disruptive events, geographically distributed public clouds, congestion-aware routing, telecommunication congestion control, disaster failure, congestion-aware routing protocol, Ambula, power outage, natural disaster, automatic routing recovery, multiple network facilities, business continuity management, Communication networks, cloud computing, fast routing recovery, specific region, peer-to-peer computing, effective disaster recovery mechanism, random generated network topologies, communication lifeline, ISP networks, enormous economic loss, telecommunication network topology, Routing, fast congestion-aware recovery, scalable peer-to-peer overlay routing system, forwarding path, business continuity, long term recovery, Peer-to-peer computing, hazard events]
MCS: Memory Constraint Strategy for Unified Memory Manager in Spark
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Apache Spark is an increasingly popular distributed computation framework based on in-memory computations, which enables iterative or interactive applications to run faster. In Spark, memory management is the key to performance enhancement to avoid memory bloat problems. Compared with previous static memory manager, in Spark 1.6 and later versions, unified memory manager is implemented as the default memory management model, targeting to achieve optimal memory utilization by borrowing between storage and execution memory. However, the storage memory borrowed from execution memory may frequently be evicted when memory pressure arises. It is because of frequently re- computation during cache evicted and frequent garbage collection during shuffle in iterative applications. This situation will produce runtime overhead caused by garbage collection including cache eviction and cache re-computation. We propose a memory constraint strategy for unified memory manager in Spark to reduce runtime overhead caused by garbage collection by reducing the cache eviction size. We implement the strategy in Spark 1.6.1 using SparkPageRank, WordCount and GroupByTest to compare three different memory managers. Experimental results reveal that compared unified memory manager, memory constraint strategy can achieve better performance improvement with lower job runtime and garbage collection time when the dataset sizes or the iterations are increasing.
[memory pressure, cache storage, garbage collection, Task analysis, Optimization, optimal memory utilization, Runtime, Garbage Collection, storage memory, unified memory manager, default memory management model, distributed computation framework, Computational modeling, cache eviction, Borrow, in-memory computations, Memory Manager, cache re-computation, Data structures, Evict, Sparks, Apache Spark, execution memory, Memory management, memory bloat problems, Spark 1.6]
A Virtual Middleboxes Network Placement Algorithm in Multi-tenant Datacenter Networks
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Hardware middleboxes are widely used in current cloud datacenter to provide network functions such as firewalls, intrusion detection system, load balancers, etc. Unfortunately, they are expensive and unable to offer customized functions for individual tenant. To overcome this issue, there is an increasing interest in deploying software middleboxes to enable flexible security, network access functionality. This paper addresses the software middleboxes placement problem with minimum bandwidth guarantee. We first specify the model of tenants' requirement that specifies the need for virtual machines of application and middleboxes, as well as communication traffic. A virtual middlebox placement algorithm called MISSILE is then proposed to offer predictable network performance for each accepted tenant, and minimize datacenter bandwidth utilization. Extensive simulation results based on current large-scale datacenter networks verify that MISSILE is effective and provides network performance guarantee for tenants.
[Cloud computing, virtual middleboxes network placement algorithm, predictable network performance, Software algorithms, Middlebox Placement, datacenter bandwidth utilization, Middleboxes, MISSILE, computer centres, software middleboxes placement problem, computer network security, large-scale multitenant datacenter networks, security, intrusion detection system, Datacenter network, hardware middleboxes, network access functionality, Bandwidth, virtual machines, Hardware, Network function virtualization, cloud datacenter, telecommunication traffic, load balancers]
Accelerating Traditional File Systems on Non-volatile Main Memory
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
New non-volatile memory (NVM) technologies, e.g., Phase Change Memory (PCM), are emerging with main features such as byte addressability, cheapness and large size, and will be deployed to serve as main memory. For fast processing of big data, block device drivers have been developed for NVM such that traditional file systems (e.g., EXT4) operated on non-volatile main memory. However, the existing block driver blindly caches pages, which incurs unnecessary access delays. To overcome this drawback, this paper develops a novel block device driver, called NVMBD, for byte-addressable non-volatile main memory. It adopts two methods of processing block I/O requests, i.e., traditional block I/O and BMAP. On the one hand, traditional block I/O reads/writes 512B sectors with multiple load/store instructions. On the other hand, BMAP inserts NVM pages into the page cache tree when processing block I/O requests, which removes the page movement between the page cache and the NVM. BMAP involves in the management of page cache, but it is transparent to file systems. BMAP provides file systems with the DAX-like feature, improving the file system performance. Through comprehensive experiments, we demonstrate that the developed block device driver significantly improves the file access performance.
[Performance evaluation, random-access storage, NVM, BMAP, Random access memory, device drivers, page cache, file systems, phase change memories, cache storage, storage, Device drivers, byte-addressable nonvolatile main memory, Non-volatile main memory, Nonvolatile memory, System performance, NVMBD, tree data structures, PCM, block device driver, Kernel, page cache tree, block I/O request processing, phase change memory]
Ada-copy: An Adaptive Memory Copy Strategy for Virtual Machine Live Migration
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
In the cloud computing architecture, virtual machine (VM) live migration is a fundamental research topic which has drawn extensive attention from communities of industry and academy. It is critical to transfer the VM memory pages that contain essential state information to resume the VM on another host during virtual machine (VM) live migration. There are many memory copy methods, such as pre-copy and post-copy. However, these methods have two limitations: application generality and performance imbalance. In this paper, we propose Ada-copy (adaptive copy), an adaptive memory copy strategy for VM live migration. The basic idea of Ada-copy is that the memory copy method of a VM should be determined by its workload characteristics. Specifically, based on the variation of current dirty page rate of memory, Ada-copy can adaptively select the most appropriate migration method to copy memory pages, thus addressing the two limitations of existing memory copy methods. To evaluate the effectiveness of our proposed strategy, we experiment with the Ada-copy on a variety of migration tasks with different dirty page rate and diverse memory usage workloads. Evaluation results show, compared with traditional methods, Ada-copy can significantly reduce the total migration time by 26%, the VM downtime by 42% and the amount of pages transferred by 35% in average.
[Measurement, dirty page rate, memory copy strategy, cloud computing architecture, Predictive models, VM memory pages, Virtual machining, Ada-copy, VM live migration, total migration time, adaptive memory copy strategy, virtual machine live migration, postcopy, memory copy method, virtual machine, Memory management, live migration, virtual machines, precopy, workload characteristics, Mathematical model, cloud computing, Autoregressive processes]
An ARIMA Based Real-time Monitoring and Warning Algorithm for the Anomaly Detection
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
With the urgent demands of multi-parameter testing under the extreme environment,such as the deep water, upper air and deep underground etc., the fiber mechanical and thermal multi-parameter instrument is developed with its redominant advantages of accuracy, reliability, sensitivity and convenience. Most existing system usually defined a fixed threshold for the accident warning, which delayed reactions to the emergency. Thus, selecting an appropriate and adjustable threshold for anomaly detection is very necessary. In order to tackle this problem, a modified time series prediction model M-ARIMA is proposed in this paper. M-ARIMA can detect the emergency and achieve high real-time alarm rate. M-ARIMA is based on the dynamic variance and reduces the error rate of early warning caused by the normal fluctuations. Experimental results show that M-ARIMA can detect abnormities with a median accuracy of more than 92% and a median error of less than 10%.
[Heuristic algorithms, autoregressive moving average processes, data mining, Predictive models, real time warning, emergency, anomaly detection, thermal multiparameter instrument, multiparameter testing, dynamic threshold, real-time monitoring, early warning accuracy, Market research, warning algorithm, Real-time systems, Smoothing methods, Fluctuations, accident prevention, Time series analysis, time series, modified time series prediction model M-ARIMA, appropriate threshold, fiber mechanical, real-time alarm rate, error rate, real-time systems, accident warning, ARIMA, prediction, alarm systems, adjustable threshold]
CPU/GPU Collaboration Techniques for Transfer Learning on Mobile Devices
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
As mobile devices become more capable, the need for customization of mobile services becomes increasingly important for users. Nowadays, mobile device sensors are able to collect information from users throughout the day, which gives insight into their profiles. The advent of modern System-on-Chip architectures has enabled mobile devices to tackle machine learning-oriented problems heretofore reserved to desktop computers. The recent success of deep learning makes it a method of choice for understanding the complex user patterns on mobile devices. Unfortunately, training a deep neural network is often considered as too computationally intensive on mobile devices. To address this issue, we consider transfer learning, a technique that aims to take advantage of deep learning features that have been previously learned to improve the learning performance of another neural network. In this paper, we propose a deep learning framework TransferCL, that supports transfer learning on mobile devices. Our approach relies on the collaboration of the multicore CPU and the integrated GPU to accelerate deep learning computation on mobile devices. We consider three major issues - performance/portability tradeoff, power efficiency, and memory management, propose our approaches, and conduct experiments to evaluate them.
[Performance evaluation, transfer learning, System-on-Chip, Graphics processing units, TransferCL, microprocessor chips, Mobile handsets, graphics processing units, GPGPU, Training, deep learning, mobile computing, mobile service customization, Machine learning, heterogeneous system, mobile devices, Feature extraction, Hardware, CPU GPU Collaboration, OpenCL, learning (artificial intelligence), deep learning computation, machine learning-oriented problems]
D-Storm: Dynamic Resource-Efficient Scheduling of Stream Processing Applications
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Scheduling streaming applications in Data Stream Management Systems (DSMS) has been investigated for years. However, there lacks an intelligent system that is capable of monitoring application execution, modelling its resource usages, and then adjusting the scheduling plan under different sizes of inputs without requiring users' intervention. In this paper, we model the scheduling problem as a bin-packing variant and propose a heuristic-based algorithm to solve it with minimised inter-node communication. We also implement the D-Storm prototype to validate the efficacy and efficiency of our scheduling algorithm, by extending the Apache Storm framework into a self-adaptive MAPE (Monitoring, Analysis, Planning, Execution) architecture. The evaluation carried out on both synthetic and realistic applications proves that D-Storm outperforms the existing resource-aware scheduler and the default Storm scheduler by at least 16.25% in terms of the inter-node traffic reduction and yields a significant amount of resource savings through consolidation.
[minimised internode communication, bin-packing variant, Heuristic algorithms, telecommunication scheduling, resource savings, data stream management systems, Task analysis, heuristic-based algorithm, stream processing applications, scheduling plan problem, bin packing, Stream Processing, Runtime, internode traffic reduction, execution architecture, DSMS, self-adaptive monitoring, Resource Scheduling, resource-aware scheduler, Monitoring, telecommunication network planning, Dynamic scheduling, D-Storm prototype, Storm scheduler, analysis, Apache Storm framework, planning, Storms, self-adaptive MAPE architecture, intelligent system, dynamic resource-efficient scheduling algorithm]
Estimating Clustering Coefficient via Random Walk on MapReduce
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Clustering coefficient plays an important role in many real-world applications, such as social network analysis and community mining. However, it is expensive to compute exact clustering coefficient for large networks. In many real-world applications, estimating clustering coefficient via random walk method is preferable because it is efficient and accurate. MapReduce is a popular distributed programming framework for processing large datasets. In this paper, we propose an algorithm on MapReduce framework to estimate clustering coefficient based on random walk method. Experiments on a Hadoop cluster for large real-world graphs demonstrate that the proposed algorithm is accurate and efficient. Comparing to Doubling algorithm, a state-of-the-art distributed algorithm to implement random walk on MapReduce, the proposed algorithm runs much faster. The proposed algorithm also reduces I/O cost efficiently.
[Hadoop cluster, estimation theory, Social network services, Merging, graph theory, clustering coefficient, random processes, network theory (graphs), graph analysis, Distributed computing, parallel processing, MapReduce framework, MapReduce, random walk, distributed algorithm, Control engineering, pattern clustering, Clustering algorithms, random walk method, Approximation algorithms, data handling, Distributed algorithms, distributed programming, clustering coefficient estimation]
Kinetic Action: Performance Analysis of Integrated Key-Value Storage Devices vs. LevelDB Servers
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
With the rise of cloud storage and many data intensive applications, there is an unprecedented growth in the volume of unstructured data. In response, key-value object storage is becoming more popular for the ease with which it can store, manage, and retrieve large amounts of this data. Seagate recently launched Kinetic direct-access-over-Ethernet hard drives which incorporate a LevelDB key-value store inside each drive. In this work, we evaluate these drives using micro as well as macro benchmarks to help understand the performance limits, trade-offs, and implications of replacing traditional hard drives with Kinetic drives in data centers and high performance systems. We perform in-depth throughput and latency benchmarking of these Kinetic drives (each acting as a tiny independent server) from a client machine connected to them via Ethernet. We compare these results to a SATA-based and a faster SAS-based traditional server running LevelDB. Our sample Kinetic drives are CPU-bound, but they still average sequential write throughput of 63 MB/sec and sequential read throughput of 78 MB/sec for 1 MB value sizes. They also demonstrate unique Kinetic features including direct disk-to-disk data transfer. Our macro benchmarking using the Yahoo Cloud Serving Benchmark (YCSB) shows that mid-range LevelDB servers outperform the Kinetic drives for several workloads; however, this is not always the case. For larger value sizes, even these first generation sample Kinetic drives outperform a full server for several different workloads.
[Key -Value Store, tiny independent server, Yahoo Cloud Serving Benchmark, memory size 1.0 MByte, data centers, Drives, Throughput, Servers, integrated key-value storage devices, hard discs, storage management, Kinetic action, Benchmark testing, LevelDB key-value store, Hardware, cloud computing, Data Center Storage Architecture, Cloud Applications, computer centres, key-value object storage, unique Kinetic features, hard drives, mid-range LevelDB servers, Performance Evaluation, high performance systems, direct disk-to-disk data transfer, cloud storage, Software, unstructured data, generation sample Kinetic drives, Kinetic theory, macro benchmarking, direct-access-over-Ethernet hard drives, disc drives, data intensive applications]
Maximizing the Profit of Cloud Broker with Priority Aware Pricing
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
A practical problem facing Infrastructure-as-a-Service (IaaS) cloud users is how to minimize their costs by choosing different pricing options based on their own demands. Recently, cloud brokerage service is introduced to tackle this problem. But due to the perishability of cloud resources, there still exists a large amount of idle resource waste during the reservation period of reserved instances. This idle resource waste problem is challenging cloud broker when buying reserved instances to accommodate users' job requests. To solve this challenge, we find that cloud users always have low priority jobs (e.g., non latency-sensitive jobs) which can be delayed to utilize these idle resources. With considering the priority of jobs, two problems need to be solved. First, how can cloud broker leverage jobs' priorities to reserve resources for profit maximization? Second, how to fairly price users' job requests with different priorities when previous studies either adopt pricing schemes from IaaS clouds or just ignore the pricing issue. To solve these problems, we first design a fair and priority aware pricing scheme, PriorityPricing, for the broker which charges users with different prices based on priorities. Then we propose three dynamic algorithms for the broker to make resource reservations with the objective of maximizing its profit. Experiments show that the broker's profit can be increased up to 2.5&#x00D7; than that without considering priority for offline algorithm, and 3.7&#x00D7; for online algorithm.
[Cloud computing, nonlatency-sensitive jobs, profit maximization, Heuristic algorithms, Resource reservation, cloud resources, low priority jobs, PriorityPricing, idle resource waste problem, broker leverage jobs, pricing schemes, Pricing, reserved instances, dynamic algorithms, cloud brokerage service, cloud computing, Monitoring, resource reservations, Google, IaaS clouds, Priority, Virtual machining, fair pricing, cloud users, Infrastructure-as-a-Service cloud users, Fairness, priority aware pricing, reservation period, pricing, Brokerage]
Multi-objective Optimizations in Geo-Distributed Data Analytics Systems
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
In geographically distributed data centers, data analytics systems have recently been developed and optimized for such geo-distributed environments. With respect to various system operators' requirements on data analytics, existing studies have optimized systems for individual goals such as resource efficiency, per-job latency and fairness. However, the optimizations with multiple objectives simultaneously have been overlooked. Even worse, some objectives can be translated to discordant actions and their relationship can be impacted by the unique features of geo-distributed data analytics systems. For example, we have observed clear trade-off between fairness and resource efficiency. In this paper, we develop an efficient framework for multi-objective optimizations on geo-distributed data analytics systems. Specifically, we develop GeoSpark, an extension to Spark, which automatically performs a multi-objective optimization according to the system operators' preferences on different objectives. The multi-objective optimization is inherently intractable especially for large-scale workloads. Therefore, we propose an efficient online heuristic to approximate the optimal scheduling plan while achieving a lower bound guarantee in the worst case. Evaluation using synthetic workload shows that GeoSpark effectively performs the multi-objective optimizations based on system operators' preferences on different objectives. GeoSpark achieves up to 30% makespan reduction, 28% job latency reduction and better fairness guarantee compared with existing schedulers in Apache Spark in the geo-distributed setting.
[Data analysis, data analysis, system operators, Geo Distributed, geophysics computing, Downlink, Task analysis, Optimization, Apache Spark, multiobjective optimization, resource efficiency, optimisation, geographically distributed data centers, GeoSpark, geo-distributed data analytics systems, Bandwidth, scheduling, Multi Objective Optimization, Data Analytics, Uplink]
WSWDC: VLC Enabled Wireless Small-World Data Centers
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
The Visible Light Communication (VLC) has the potential to provide dense and fast connectivity at low cost. In this paper, we propose WSWDC, a novel VLC enabled wireless small-world data center. It employs VLC links to achieve a fully wireless data center network (DCN) across racks for the first time. The using of VLC links eliminates hierarchical switches and inter-rack cables, and thus reducing hardware investment, as well as maintenance cost. More precisely, to simplify the configuration and control operations, we propose three DCN design rationales: (1) fully-wireless, all inter-rack links are wireless; (2) easy-deployable, it is not necessary to change the existing infrastructure inside data center; (3) plug-and-play, no extra centralized control operations are required. Previous proposals, however, cannot achieve the three rationales simultaneously. To this end, we first use regular VLC links to interconnect racks as a regular grid DCN. To further exploiting the benefits of VLC links, a few random VLC links are carefully introduced to update the wireless grid DCN as a wireless small-world DCN. To avoid the potential interference among VLC links, we deploy VLC transceivers at different height on the top of each rack. In this way, VLC links would not interfere with others at each height level. Moreover, we design a greedy but efficient routing method for any pair of racks using their identifiers as inputs. Comprehensive evaluation results indicate that our WSWDC exhibits good network performance.
[wireless small-world DCN, inter-rack links, WSWDC, visible light communication, wireless links, Wireless communication, Network topology, wireless grid DCN, interconnect racks, small-world, hardware investment reduction, free-space optical communication, inter-rack cables, fully wireless data center network, optical links, optical transceivers, VLC enabled wireless small-world data centers, maintenance cost reduction, Data center network, topology design, computer networks, Interference, Routing, Light emitting diodes, VLC transceivers, routing method, regular VLC links, Topology, computer centres, Data centers, telecommunication network routing, random VLC links, plug-and-play, telecommunication traffic, DCN design rationales]
User Perceived Value-Aware Cloud Pricing for Profit Maximization of Multiserver Systems
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
With the rapid deployment of cloud computing infrastructures, understanding the economics of cloud computing has becoming a pressing issue for cloud service providers. However, existing pricing models rarely consider the dynamic interaction between user requests and the cloud service provider, thus can not accurately reflect the law of supply and demand in marketing. In this paper, we propose a pricing model based on the concept of user perceived value in the domain of economics that accurately capture the real supply and demand situation in the cloud service market. We then design a profit maximization scheme based on the presented dynamic pricing model that optimizes profit of the cloud service provider without violating user service-level agreement. Extensive experiments using data extracted from real-world applications validate the effectiveness of the proposed user perceived value-based pricing model. The proposed profit maximization scheme achieves 24.44% more profit as compared to the state of the art benchmarking methods.
[profitability, Cloud computing, profit maximization, Computational modeling, user perceived value-aware cloud pricing, profit maximization scheme, user perceived value, Servers, contracts, cloud service market, presented dynamic pricing model, cloud computing infrastructures, Supply and demand, dynamic pricing model, supply and demand, Pricing, Random variables, user service-level agreement, cloud computing, cloud service provider, multiserver systems, pricing]
Spark-Based Measurement and Analysis on Offline Mobile Application Market over Device-to-Device Sharing in Mobile Social Networks
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Recently how to select seeding users with large impacts in social networks has gained more and more attention in many studies. It has been demonstrated in a number of researches that the seeding users, playing vital roles in social groups, could be exploited to promote the dissemination of popular contents. Nevertheless, the existed algorithms, which are performed on small-scale data sets with limited feature dimensions, mostly base on unconsolidated hypotheses and measurements of data sets. Consequently, their performance and precision cannot be well evaluated and improved. In this paper, we firstly make comprehensive and large-scale measurements on 3.56 TBytes of real data sets related to Device-to-Device (D2D) content sharing activity traces from a popular D2D sharing application (APP). The mobile social networks generated by the offline content deliveries between users are presented and analyzed. Focusing on the seeding users' selection problem in social networks, we propose algorithms of weighted SeedRanks (SRs) to select the seeding users with accuracy. The algorithms are adapted to the parallel computing platform of Apache Spark with high performance. The results of the recurrent experiment on the large-scale D2D data sets prove the efficiency of our algorithms. Finally, we make conclusions and discuss future work.
[Data privacy, Time-frequency analysis, social groups, Conferences, Entropy, offline mobile application market, parallel processing, Content Dissemination, small-scale data sets, mobile computing, memory size 3.56 TByte, Spark-based measurement, parallel computing platform, Seeding Users, Influence Maximization, D2D sharing application, data analysis, Mobile Social Networks, Device-to-Device content sharing activity, Big Data, seeding users, large-scale measurements, Global Positioning System, Apache Spark, social networking (online), mobile social networks, Device-to-Device, offline content deliveries]
Shadow: Exploiting the Power of Choice for Efficient Shuffling in MapReduce
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
How to reduce the costly cross-rack data transferring is challenging in improving the performance of MapReduce platforms. Previous schemes mainly exploit the data locality in the Map phase to reduce the cross-rack communications. However, the Map locality based schemes may lead to highly skewed distribution of Map tasks across racks in the platform, resulting in serious load imbalance among different cross-rack links during Shuffling. Recent research results show that the slow Shuffling is the root cause of the MapReduce performance degradation. Very limited work has been done for speeding up the Shuffle phase. A notable scheme leverages the principle of the power of choice to balance the network loads on different cross-rack links during Shuffling for a specific type of sampling applications, where processing a random subset of the large-scale data collection is sufficient to derive the final result. The scheme launches a few additional tasks to offer more choices for task selection during Shuffling. However, such a scheme is designed for sampling applications and not applicable to general applications, where all the input data instead of a random subset is processed. In this work, we observe that with high Map locality, the network is mainly saturated in Shuffling but relatively free in the Map phase. A little sacrifice in Map locality may greatly accelerate Shuffling. Based on this, we propose a novel scheme called Shadow for Shuffle-constrained general applications, which strikes a trade-off between Map locality and Shuffling load balance. Specifically, Shadow iteratively chooses an original Map task from the most heavily loaded rack and creates a duplicated task for it on the most lightly loaded rack. During processing, Shadow makes a choice between an original task and its replica by efficiently pre-estimating the job execution time. We conduct extensive experiments to evaluate our Shadow design. Results show that Shadow greatly reduces the cross-rack skewness by 30.7% and the job execution time by 27.9% compared to existing schemes.
[Schedules, Power of choice, Shuffle phase, Task analysis, parallel processing, large-scale data collection, MapReduce, Degradation, resource allocation, data locality, Distributed databases, Map locality, Shuffling load balance, Shadow design, task selection, Job shop scheduling, Task scheduling, MapReduce performance degradation, Duplicated tasks, highly skewed distribution, Processor scheduling, MapReduce platforms, Data collection, file organisation, data handling]
Scheduling for Energy Efficiency and Throughput Maximization in a Faulty Cloud Environment
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
There is an increasingly prominent trend in many big data scientific applications to move a substantial portion of or even all of the computing workflow executions to a cloud environment, which calls for an effective and efficient solution to optimize the performance of such workflow applications. We focus on computing workflows of streaming applications, and consider a faulty cloud environment where both nodes and links may fail at a certain probability. We tackle a triobjective optimization problem that reduces the total energy consumption while enforcing a bound on the throughput, and a constraint on the reliability. A layer-based mapping algorithm is proposed to schedule each subtask in the workflow to an appropriate node in the cloud in order to achieve three objectives (energy, throughput, and reliability) in a distributed manner. The proposed scheme automatically recomputes a mapping solution adapting to the network changes after a certain period. The performance superiority of the proposed scheme is illustrated by an extensive set of comparisons with other existing methods.
[Energy consumption, Cloud computing, throughput objective, energy objective, reliability, Throughput, Servers, Optimization, energy efficiency, scheduling, throughput, cloud computing, computing workflow executions, Pareto optimisation, layer-based mapping algorithm, Big Data scientific applications, throughput maximization, Big Data, total energy consumption, triobjective optimization problem, Web services, faulty cloud environment, reliability objective, Data transfer, streaming applications, Reliability, Workflow scheduling]
Road Recognition Using Big Data of Coarse-Grained Vehicular Footprints
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
With more and more vehicles equipped with GPS tracking devices, there is increasing interest in building and updating maps using vehicular GPS footprints or traces. Most existing approaches for building maps rely on position traces from highly accurate positioning devices, which are sampled at a high frequency, e.g., 1 Hz. Typically these traces are purposely recorded by survey vehicles. In practice, however, commodity GPS devices have much lower accuracy. In addition, the sampling frequency is low (at around once per minute) in order to reduce communication cost. Building maps from coarse-grained vehicular GPS footprints is challenging due to the inherent noise in commodity GPS devices and the shape complexity of urban roads. In this paper, we propose a novel algorithm called RRA for recognizing urban roads with coarse-grained GPS footprints from probe vehicles moving in urban areas. The algorithm overcomes the challenges by pruning low quality GPS footprints, clustering GPS footprints on the same road segment and applying shape aware B-spline fitting. We have conducted empirical study with a real data set of GPS footprints and evaluation results demonstrate that our RRA algorithm achieves good performance. When there are 800 taxis and the time window for footprints collection is 2 hours, the coverage of roads for RRA is 60% and the rate of false positive is 5%, while the best alternative algorithm KDE Points can hardly recognize any road.
[Conferences, urban roads, Road Recognition, footprints collection, sampling frequency, low quality GPS footprints, frequency 1.0 Hz, road vehicles, position traces, time 2.0 hour, building maps, big data, road traffic, GPS tracking devices, coarse-grained GPS footprints, Big Data, coarse-grained vehicular GPS footprints, cartography, traffic engineering computing, positioning devices, road segment, Global Positioning System, road recognition, Vehicular Footprints, commodity GPS devices, survey vehicles, probe vehicles]
Online Flow Scheduling with Deadline for Energy Conservation in Data Center Networks
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
We study the problem of flow scheduling in data center networks. Using speed scaling, our aim is to find an online scheduling algorithm that minimizes the total energy consumption of the network by determining both the transmission order and rates of the arriving flows while providing a strict flow deadline guarantee. Observing the superlinear property of link power consumption, the key challenge is in constantly determining the minimum transmission rate for &#x201C;delay-tolerable&#x201D; flows without any priori knowledge. To leverage the flow arrival pattern, we propose a probability-based flow prediction model to capture the uncertainty of the network flows. Based on the prediction model, we propose a tunable online flow scheduling algorithm to solve the online flow scheduling problem effectively. By introducing a scaling factor on bandwidth allocation, this algorithm allows us to conduct arbitrary trade-offs between the conservative and aggressive behaviors in terms of energy conservation. The effectiveness of the proposed algorithm is validated through rigorous theoretical analysis and further confirmed by extensive numerical simulations.
[Energy consumption, flow scheduling, Predictive models, power consumption, transmission order, Scheduling algorithms, flow prediction model, energy efficiency, scheduling, Prediction algorithms, scaling factor, online flow scheduling problem, data center networks, traffic prediction, speed scaling, Data center network, probability, online algorithm, tunable online flow, link power consumption, Scheduling, total energy consumption, computer centres, strict flow deadline, bandwidth allocation, Data centers, delay-tolerable flows, online scheduling algorithm, minimum transmission rate, energy conservation, arriving flows, flow arrival pattern]
An Analytical Study of Recursive Tree Traversal Patterns on Multi- and Many-Core Platforms
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Recursive tree traversals are found in many application domains, such as data mining, graphics, machine learning and scientific simulations. In the past few years there has been growing interest in the deployment of applications based on graph data structures on many-core devices. A couple of recent efforts have focused on optimizing the execution of multiple serial tree traversals on GPU, and have reported performance trends that vary across algorithms. In this work, we aim to understand how to select the implementation and platform that is most suited to a given tree traversal algorithm and dataset. To this end, we perform a systematic study of recursive tree traversal on CPU, GPU and the Intel Phi processor. We first identify four tree traversal patterns: three of them performing multiple serial traversals concurrently, and the last one performing a single parallel level order traversal. For each of these patterns, we consider different code variants including existing and new optimization methods, and we characterize their control-flow and memory access patterns. We implement these code variants and evaluate them on CPU, GPU and Intel Phi. Our analysis shows that there is not a single code variant and platform that achieves the best performance on all tree traversal patterns, and it provides guidelines on the selection of the implementation most suited to a given tree traversal pattern and input dataset.
[Machine learning algorithms, multiprocessing systems, multiple serial tree traversals, recursive tree traversal patterns, Instruction sets, memory access patterns, Graphics processing units, Optimization methods, parallelism, graph data structures, graphics processing units, parallel processing, GPU, Runtime, many-core processor, parallel level order traversal, control-flow pattern, many-core platforms, Parallel processing, multicore platforms, tree data structures, learning (artificial intelligence), recursive tree traversal, Kernel]
An Efficient Label Routing on High-Radix Interconnection Networks
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Cost-effective adaptive routing has a significant impact on overall performance for high-radix hierarchical topologies, such as Dragonfly, which achieve a lower network diameter than traditional topologies, Torus and Fat tree, but exhibit a lower degree of adaptiveness for shortest-path rout- ing. Existing adaptive routing methods for those hierarchical topologies improve the adaptiveness by increasing path length, i.e. local or global adaptive routing, and thus suffer from complex and costly deadlock avoidance. This work aims to maximize the routing adaptiveness at the minimum cost of deadlock avoidance. We propose a label routing method for high-radix hierarchical networks. This label routing utilizes a co-design methodology and coordinates the two pipelines, input queue and routing computation, in the router microarchitec- ture. Packets in the input buffer are labeled by our routing algorithm depending on network states. We reorganize the input buffer and develop a label routing algorithm, named Green-Red Routing, GRR. GRR relaxes the requirement of using virtual channels to eliminate routing deadlock, and mitigates buffer resources dedicated to deadlock avoidance. GRR manages the buffer resources and balance its utilization elaborately, and achieve fully adaptive routing efficiently. We conduct extensive experiments to evaluate the performance of GRR on Dragonfly and compare it with state-of-the-art works. The results show that GRR achieves 10%-35% higher performance than existing routing algorithms under most traffic patterns.
[Adaptive systems, co-design, GRR, deadlock avoidance, Network topology, efficient label Routing, high-radix interconnection networks, cost-effective adaptive routing, routing computation, Green-Red Routing, traditional topologies, buffer resources, complex deadlock avoidance, high-radix hierarchical network, network diameter, Dragonfly, input buffer, network states, high-radix hierarchical networks, high-radix hierarchical topologies, telecommunication network topology, Routing, Topology, adaptive routing methods, buffer resource, costly deadlock avoidance, fully adaptive routing, shortest-path routing, path length, telecommunication network routing, Organizations, router microarchitecture, System recovery, Resource management, routing deadlock]
Automatic and Transparent Resource Contention Mitigation for Improving Large-Scale Parallel File System Performance
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Proportional to the scale increases in HPC systems, many scientific applications are becoming increasingly data intensive, and parallel I/O has become one of the dominant factors impacting the large-scale HPC application performance. On a typical large-scale HPC system, we have observed that the lack of a global workload coordination coupled with the shared nature of storage systems cause load imbalance and resource contention over the end-to-end I/O paths resulting in severe performance degradation. I/O load imbalance on HPC systems is generally a self-inflicted wound and mostly occurs between the I/O paths and resources consumed by each individual job. In this paper, we introduce TAPP-IO, a dynamic, shared load balancing framework for mitigating resource contention. TAPP-IO extends our previous work and solves two major limitations: First, it transparently intercepts file creation calls during runtime to balance the workload over all available storage targets. The usage of TAPP-IO requires no application source code modifications and is independent from any I/O middleware. The framework can be applied to almost any HPC platform and is suitable for systems that lack a centralized file system resource manager. Second, the framework proposes a new placement strategy to support not only file-per-process I/O, but also single shared file I/O. This opens the door to a new class of scientific applications that can leverage the placement library for improved performance. We demonstrate the effectiveness of our integration on the Titan system at the Oak Ridge National Laboratory. Our experiments with a synthetic benchmark and real-world HPC workload show that, even in a noisy production environment, TAPP-IO can improve large-scale application performance significantly.
[application source code modifications, High Performance Computing, TAPP-IO, severe performance degradation, Metadata, HPC platform, Load Balancing, scientific applications, parallel processing, large-scale application performance, Optimization, storage management, Titan system, file-per-process, resource allocation, centralized file system resource manager, file creation, large-scale parallel file system performance, Libraries, middleware, shared nature, global workload coordination, storage systems cause load imbalance, dynamic shared load balancing framework, Parallel File System, large-scale HPC application performance, available storage targets, single shared file, Single Shared File, Middleware, Data centers, Performance Evaluation, end-to-end I/O paths, Load management, HPC systems, Resource management]
Betweenness Centrality Revisited on Four Processors
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
The betweenness centrality measure has been widely adopted in various graph analytics applications, such as community detection and brain network analysis. Due to the high intensity of BC computation and rapid data growth, there have been a number of studies on parallel BC computation, either on CPUs or GPUs. However, there has not been a comprehensive comparative study on the BC algorithm on different processors. In this paper, we revisit shared-memory parallel BC computation on four kinds of processors, including multi-core CPUs, many-core GPUs, and two generations of Intel MIC processors. We find that, with suitable parallelization strategies and data-oriented optimizations, commodity multi-core CPUs are the fastest, followed by the second generation MIC. These two processors are faster than the state-of-the-art GPU implementations across all kinds of graphs. In comparison, the GPU outperforms the first generation MIC only on small-diameter graphs and is the slowest on the other kinds of graphs.
[graph theory, Graphics processing units, Switches, data-oriented optimizations, Parallel, GPU, betweenness centrality measure, Microwave integrated circuits, small-diameter graphs, many-core GPUs, Intel Xeon Phi, BC algorithm, Computer architecture, shared memory systems, graph analytics applications, Message systems, rapid data growth, Image edge detection, Intel MIC processors, suitable parallelization strategies, Betweenness centrality, graphics processing units, shared-memory parallel BC computation, second generation MIC, commodity multicore CPUs, Many-core processors, Graph algorithm]
Delay-Guaranteed Minimum Cost Forest for Uncertain Multicast
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Multicast can efficiently reduce the consumption of network resources by jointly serving multiple destinations with a single source node along a Steiner tree. Nowadays many applications employ a given replica system to improve the service quality; hence, each file and its replicas are usually distributed among multiple servers. In this setting, uncertain multicast is proposed as a novel and general model of multicast transfer. That is, a multicast group offers multiple source nodes instead of a single one. As a result, the multicast routing usually forms a forest, consisting of multiple isolated trees. In this paper, we focus on characterizing and constructing the minimum cost forest (D-MCF) for any uncertain multicast, which satisfies the constraint of end-to-end delay, between any pair of source and destination in the resulting forest at the same time. Prior methods for building a minimum cost forest for an uncertain multicast remain inapplicable to this new problem. Accordingly, we first show an observation about the D-MCF problem and formalize it as an Integer Programming model. We then prove that D-MCF is a NP-hard problem and accordingly design two efficient algorithms, the partition algorithm (PA) and the combination algorithm (CA), to approximate the optimal solution. PA first divides uncertain multicast into several deterministic multicast groups and then combines those routing trees for deterministic multicast. In contrast, CA combines each feasible unicast path for each destinations. Analyses and evaluations indicate that our two methods can produce more desired routing forest than prior method regardless of the delay bound. Also, our PA method can achieve better balance between the performance and time consumption than our CA method. The evaluation results show that PA can reduce 49.02% total cost at the cost of incurring extra 12.59% time consumption, significantly outperforming the CA method.
[Steiner trees, integer programming, delay-guaranteed, combination algorithm, uncertain multicast, partition algorithm, D-MCF problem, multicast communication, approximation algorithm, routing forest, computer networks, trees (mathematics), delay-guaranteed minimum cost forest, Routing, Partitioning algorithms, quality of service, integer programming model, NP-hard problem, Steiner tree, telecommunication network routing, Forestry, Vegetation, network resources consumption, Approximation algorithms, Delays, multicast routing]
Drowsy Register Files for Reducing GPU Leakage Energy
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
General-purpose graphics processing units (GPGPUs) usually employ a huge register file (RF) to support massive multithreading, which however, is also responsible for a large fraction of GPGPU's total power. In this paper, we propose three RF drowsy policies and evaluate their effectiveness on leakage energy reduction. In the first drowsy policy called immediate sleep (Drowsy-IS), registers keep staying in the drowsy mode unless they are accessed, which are then put into the drowsy mode again immediately to minimize the leakage energy consumption. The second policy named temporary awake (Drowsy-TA) holds the registers in the normal (i.e. active) mode for a certain period after being accessed to wait for the next access. The registers are placed into the drowsy mode until that period expires without any access activity. Finally, we propose an adaptive policy named Drowsy-RI which identifies the re-access interval for each register at run-time and lets registers wait for the predicted intervals before putting them into the drowsy mode. The experimental results show that compared to the baseline RF, Drowsy-IS achieves 91.7% RF leakage energy reduction on average at the cost of 4.4% performance degradation. Drowsy-TA leads to negligible performance overhead, and 82.8% leakage energy reduction. By balancing the energy saving and the performance overhead, Drowsy-RI saves more RF leakage energy (87.3%) than Drowsy-TA and achieves less performance degradation (2.7%) than Drowsy-IS.
[Energy consumption, RF leakage energy reduction, Instruction sets, Graphics processing units, Switches, Drowsy register files, Registers, Drowsy-RI, GPU, drowsy policy, Radio frequency, power aware computing, leakage energy, drowsy mode, massive multithreading, immediate sleep, energy consumption, Drowsy temporary awake, GPGPUs, power gating, multi-threading, drowsy cache, GPU leakage energy reduction, graphics processing units, leakage energy consumption, energy saving, huge register file, register file, RF drowsy policies, Logic gates, general-purpose graphics processing units]
GPU-Based Parallel Genetic Algorithm for Increasing the Coverage of WSNs
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Advances in wireless communication, digital systems and micro-electronic-mechanical system technologies led to the development of wireless sensor networks (WSNs) which are used in various critical real-world applications. The fact that WSNs are low cost and eliminate the need for infrastructure led to their replacing traditional networks in area/event monitoring and tracking applications. WSNs consist of small and resource-limited sensor nodes, due to which several problems arise in the WSN development process. One of these problems is coverage. Providing the best coverage with a minimum number of sensor nodes is an NP-hard problem known as the maximum coverage sensor deployment problem (MCSDP). Genetic Algorithms (GAs) have been proved effective in solving optimization problems in many different disciplines (increasing coverage in WSNs, image processing, route planning, etc.). In this study, a GPU-based parallel GA solution for increasing the coverage of a given homogeneous WSN topology in a 2-D Euclidean area is proposed which is the first time this technique is used and parallelized on GPUs to the best of our knowledge. Finally, performance results of the proposed algorithm are compared to the previous work with the emphasis on the achieved performance improvement.
[wireless sensor networks, tracking applications, Graphics processing units, communication complexity, GPU, route planning, Genetic algorithms, wireless communication, parallel genetic algorithm solution, Sociology, sensor placement, MCSDP, optimization, maximum coverage sensor deployment problem, Monitoring, image processing, telecommunication network topology, genetic algorithms, microelectronic-mechanical system technologies, Statistics, graphics processing units, WSN development process, digital systems, Wireless sensor networks, NP-hard problem, parallel GA solution, GPU-based parallel genetic algorithm, 2D Euclidean area, optimization problems, Electronics packaging]
Green Energy Aware Scheduling Problem in Virtualized Datacenters
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
With the generalization of cloud infrastructures usage, energy consumption has become a major issue. Scheduling heuristics have been proposed to optimize the resource usage of data center so as to take down the energy consumption. This paper tackles the problem with a different approach by taking into consideration the availability of renewable energy. First we formalize the green energy aware scheduling problem (GEASP) and propose a global model based on constraint programming as well as a search heuristic to solve it efficiently. The proposed model integrates the various aspects inherent to the dynamic planning in a data center: heterogeneous physical machines, various application types (i.e., active or online applications and batch applications), actions and energetic costs of turning ON/OFF physical machines, interrupting/resuming batch applications, CPU and RAM resource consumption, tasks migration, migration costs, and integration of green energy availability. The model can therefore reduce both the costs related to energy consumption and the carbon footprint of a data center. We evaluate the model against the state-of-the-art framework PIKA on real-world workload and solar power traces.
[Energy consumption, green energy availability, cloud infrastructures usage, Programming, GEASP, CPU, virtualisation, heterogeneous pm, Servers, scheduling heuristics, Task analysis, RAM resource consumption, tasks migration, renewable energy, power aware computing, resource allocation, Virtualized Datacenters, scheduling, cloud computing, constraint handling, energy consumption, constraint programming, heuristic, PIKA, computer centres, resource usage, green energy, Data centers, carbon footprint, data center, virtual machine, green energy aware scheduling problem, Green products, batch applications, virtual machines, migration, Data models, migration costs]
Hexe: A Toolkit for Heterogeneous Memory Management
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Heterogeneity in memory is becoming increasingly common in high-end computing. Several modern supercomputers, such as those based on the Intel Knights Landing or NVIDIA P100 GPU architectures, already showcase multiple memory domains that are directly accessible by user applications, including on-chip high-bandwidth memory and off-chip traditional DDR memory. The next generation of supercomputers is expected to take this architectural trend one step further by including NVRAM as an additional byte-addressable memory option. Despite these trends, allocating and managing such memory are still tedious tasks. In this paper, we present hexe, a highly flexible and portable memory allocation toolkit. Unlike other memory allocation tools such as malloc, memkind, and cudaMallocManaged, hexe presents a rich and portable memory allocation framework that allows applications to carefully and precisely manage their memory across the various memory subsystems available on the system. Together with a detailed description of the design and capabilities of hexe, we present several case studies where the flexible memory allocation in hexe allows applications to achieve superior performance compared with that of other memory allocation tools.
[NVRAM, Graphics processing units, Random access memory, modern supercomputers, KNL, portable memory allocation toolkit, parallel machines, off-chip traditional DDR memory, storage management, Heterogeneous Memory, NVIDIA P100 GPU architectures, Bandwidth, on-chip high-bandwidth memory, heterogeneous memory management, Intel Knights Landing, random-access storage, memory subsystems, MCDRAM, Tools, Hexe, highly flexible memory allocation toolkit, CUDA, memory allocation tools, high-end computing, Memory management, portable memory allocation framework, Memory Manganement, multiple memory domains, Resource management, byte-addressable memory option]
HiRy: An Advanced Theory on Design of Deadlock-Free Adaptive Routing for Arbitrary Topologies
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Recently proposed irregular networks can reduce the latency for both on-chip and off-chip systems with a large number of computing nodes and thus can improve the performance of parallel application. However, these networks usually suffer from deadlocks in routing packets when using a naive minimal path routing algorithm. To solve this problem, we focus attention on a lately proposed theory that generalizes the turn model to maintain the network performance with deadlock-freedom. The theorems remain a challenge of applying themselves to arbitrary topologies including fully irregular networks. In this paper, we advance the theorems to completely general ones. To apply the idea of the turn model to arbitrary topologies, we introduce a concept of regions that define continuous directions of channels on an n-dimensional space. Moreover, we provide a feasible implementation of a deadlock-free routing method based on our advanced theorem. To reduce the latency and the number of required Virtual Channels (VCs) with this method, a heuristic approach is introduced to reduce the number of prohibited turns between channels. Experimental results show that the routing method based on our proposed theorem can improve the network throughput by up to 138 % compared to a conventional deterministic minimal routing method. Moreover, it can reduce the latency by up to 2.9 % compared to another fully adaptive routing method.
[Adaptation models, telecommunication channels, High Performance Computing, on-chip systems, Switches, Virtual Channels, Deadlock free Routing Algorithm, Irregular Networks, Network topology, minimal path routing algorithm, parallel application, computing nodes, Two dimensional displays, virtual channels, telecommunication network topology, Routing, Topology, arbitrary topologies, deadlock-free adaptive routing method, VC, telecommunication network routing, System recovery, off-chip systems, deterministic minimal routing method, Interconnection Networks]
vMCA: Memory Capacity Aggregation and Management in Cloud Environments
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
In cloud environments, the VMs within the computing nodes generate varying memory demand profiles. When memory utilization reaches its limits due to this, costly (virtual) disk accesses and/or VM migrations can occur. Since some nodes might have idle memory, some costly operations could be avoided by making the idle memory available to the nodes that need it. In view of this, new architectures have been introduced that provide hardware support for a shared global address space that, together with fast interconnects, can share resources across nodes. Thus, memory becomes a global resource. This paper presents a memory capacity aggregation mechanism for cloud environments called vMCA (Virtualized Memory Capacity Aggregation) based on Xen's Transcendent Memory (Tmem). vMCA distributes the system's total memory within a single node and globally across multiple nodes using a user-space process with high-level memory management policies. We evaluate vMCA using CloudSuite 3.0 on Linux and Xen. Our results demonstrate a peak running time improvement of 76.8% when aggregating memory, and of 37.5% when aggregating memory and implementing our policies.
[Xen Transcendent Memory, Cloud computing, vMCA, memory demand profiles, Memory, Tmem, cloud environments, virtualisation, memory utilization, Aggregation, storage management, Virtual machine monitors, aggregating memory, resource allocation, Linux, Memory management, Virtualized Memory Capacity Aggregation, Coherence, virtual machines, Resource management, cloud computing, Virtualization, high-level memory management policies]
SA-PFRS: Semantics-Aware Page Frame Reclamation System in Virtualized Environments
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Page reclamation is one compelling way to overcommit memory in modern operating systems. To achieve wise reclamation, the Linux kernels employ page frame reclamation algorithm (PFRA) to reclaim pages based on the page usage view. However, due to the semantic gap problem in virtualized environments, the native PFRA suffers three types of unwise evictions including false eviction, superficial eviction and omitted eviction. This will lead to high swapping I/O activity and consequently limits the ability to overcommit memory. We present SA-PFRS, a Semantics-Aware Page Frame Reclamation System, to address this problem. SA-PFRS separates the memory pages allocated for guests from reclaimable candidates in host, explores how the pages are being used by guest OS, and adjusts the reclamation order in the view of guest. Then, SA-PFRS re-arranges the reclamation sequence of guest pages and host pages, so as to reclaim the memory pages in a global semantics-aware manner. This enables SA-PFRS to eliminate a large number of swapping I/O operations when memory is overcommitted. We implement a prototype of SA-PFRS in Linux kernel, and show its effectiveness through a set of experiments.
[Random access memory, Linux kernel, Virtual machine introspection, virtualisation, Servers, global semantics-aware manner, storage management, page frame reclamation algorithm, Semantics, Kernel, reclamation sequence, operating system kernels, virtualized environments, host pages, Paging, operating systems, semantic gap problem, Memory overcommitment, Linux, Memory management, page usage view, Page frame reclamation, SA-PFRS, reclamation order, semantics-aware page frame reclamation system, guest pages, memory pages]
Rethinking Multicore Application Scalability on Big Virtual Machines
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Virtual machine (VM) sizes keep increasing in the cloud. However, little attention has been paid to analyze and understand the scalability of multicore applications on big VMs with multiple virtual CPUs (VCPUs), assuming that application scalability on VMs can be analyzed in the same ways as that on physical machines (PMs). The paper demonstrates that, since hardware CPU resource is dynamically allocated to VCPUs, the executions of multicore applications on VMs show different scalability from those on PMs. The paper systematically studies how the virtualization of CPU resource changes execution scalability, identifies key application features and system factors that affect execution scalability on VMs, and investigates possible directions to improve scalability. The paper presents a few important findings. First, the execution scalability of applications on VMs is determined by different factors than those on PMs. Second, virtualization and resource sharing can improve scalability by nature. Thus, applications may show better scalability on VMs than on PMs. Linear scalability can be achieved even when there is substantial sequential computation. Third, there is still much space to further improve execution scalability by enhancing system designs. Better scalability can be achieved by increasing allocation period length and/or matching resource allocation and workload distribution.
[Scalability, virtualisation, multiple virtual CPUs, scalability, multicore, resource allocation, virtual machine sizes, linear scalability, big virtual machines, cloud computing, multiprocessing systems, Multicore processing, virtualization, multicore applications, hardware CPU resource, Virtual machining, CPU resource changes execution scalability, virtual machine, big VMs, physical machines, resource sharing, virtual machines, multicore application scalability, workload distribution, Central Processing Unit, Resource management, Spinning, Virtualization]
Regional Congestion Control in Datacenter Networks
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
The rapid deployment of cloud computing and online services poses great challenges for data center networks, and congestion control is one of the top concerns. Although numbers of proposals in different network layers have been put forward to alleviate the negative impact of congestion, the short-lived flows, which are latency-sensitive and constitute the majority of total traffic in data centers, still suffer severe performance degradation. Since the existing congestion control methods all rely on end hosts to perceive congestion and then adjust their network sending rate, the response time is relatively long when compared with the duration of short-lived flows, which increases latency significantly. In this paper, we propose RCC, a regional congestion control mechanism, which aims to respond to congestion more quickly and eliminate the mismatch mentioned above. Different from host-based mechanisms, RCC is implemented in the switch, which detects the congestion state and schedule the traffic around the congestion point locally, without sending feedback to the distal host. Evaluation has shown that, compared with the host-based mechanism, our method achieves better performance for short-lived flows and maintains stable buffer occupancy of the switch. In addition, mixed long- and short-lived flows which contend for the same bottleneck link can share the bandwidth more fairly.
[telecommunication congestion control, Conferences, computer networks, Throughput and Latency, Datacenter Networks, Congestion Control, computer centres, switching networks, host-based mechanisms, RCC, cloud computing, data center networks, regional congestion control mechanism, telecommunication traffic]
Portable Topology-Aware MPI-I/O
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Recent advances in storage devices are opening new opportunities in high-performance computing (HPC). Technologies such as solid-state drives (SSD) and non-volatile memories (NVM) are becoming increasingly popular because of the important gains they can represent for HPC. Indeed, novel architectures with deeper storage hierarchies populated with SSDs and/or NVM offer new ways to improve applications' performance. For instance, fast multilevel checkpointing or in-situ data analysis are some of the techniques that can be greatly improved thanks to these new technologies. However, optimizations made for one system can impose performance costs in another machine due to topology differences. To take advantage of increasingly complex systems, we propose extensions to MPI enabling codes to determine which nodes of a system share common features. Our approach provides a portable mechanism for resource discovery. It also lays the foundation for additional optimizations in checkpointing and in ROMIO. In this paper we present the design and implementation of such a feature and test it with multiple benchmarks. Our results demonstrate the benefits of this portable resource discovery functionality.
[Checkpointing, checkpointing, application program interfaces, NVM, portable resource discovery functionality, in-situ data analysis, storage, parallel processing, storage hierarchies, Nonvolatile memory, complex systems, Computer architecture, portable topology-aware MPI-i, solid-state drives, high-performance computing, burst buffers, message passing, data analysis, resource discovery, SSD, mpi, Supercomputers, Topology, Standards, topologies, performance costs, storage devices, Hard disks, fast multilevel checkpointing, HPC, communicator, ROMIO]
Parallel I/O Optimizations for Scalable Deep Learning
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
As deep learning systems continue to grow in importance, researchers have been analyzing approaches to make such systems efficient and scalable on high-performance computing platforms. As computational parallelism increases, however, data I/O becomes the major bottleneck limiting the overall system scalability. In this paper, we continue our efforts to improve LMDB, the I/O subsystem of the Caffe deep learning framework. In a previous paper we presented LMDBIO---an optimized I/O plugin for Caffe that takes into account the data access pattern of Caffe in order to vastly improve I/O performance. Nevertheless, LMDBIO's optimizations, which we henceforth call LMM (localized mmap), are limited to intranode performance, and these optimizations do little to minimize the I/O inefficiencies in distributed-memory environments. In this paper, we propose LMDBIO-DM, an enhanced version of LMDBIO-LMM that optimizes the I/O access of Caffe in distributed-memory environments. We present several sophisticated data I/O techniques that allow for significant improvement in such environments. Our experimental results show that LMDBIO-DM can improve the overall execution time of Caffe by more than 30-fold compared with LMDB and by 2-fold compared with LMDBIO-LMM.
[distributed-memory environments, Scalability, Caffe deep learning framework, input-output programs, LMDBIO-LMM, parallel processing, LMDBIO-DM, Optimization, Training, Distributed databases, data access pattern, scalable deep learning, LMDBIO's optimizations, Parallel processing, system scalability, computational parallelism, learning (artificial intelligence), high-performance computing platforms, LMDB, localized mmap, Machine learning, distributed memory systems, deep learning systems, Caffe, I/O subsystem, Scalable deep learning, parallel I/O optimizations]
Pangu: Towards a Software-Defined Architecture for Multi-function Wireless Sensor Networks
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Software-defined networking (SDN) is deemed as a promising direction to offer generalizability of wireless sensor networks (WSN). To introduce SDN into WSNs, however, means a series of non-trivial challenges due to the wireless and ad-hoc nature of WSNs. In this paper, we present our study towards a software-defined architecture for multi-function wireless sensor networks. Our proposal called Pangu is built upon the opportunistic routing protocol stack and introduces the concept of modality properties of sensor nodes. It enables centralized network control over a WSN while preserving the flexibility of underlying ad-hoc routing. We tackle the critical problems of the architecture design by presenting three essential components of Pangu. Moreover, we implement Pangu on a real-world testbed and evaluate it with various experiments.
[Protocols, wireless sensor networks, software defined networking, Routing, WSNs, Ad hoc networks, opportunistic routing protocol stack, Wireless Sensor Networks, software-defined architecture, centralized network control, Wireless communication, Software Defined Networks, Wireless sensor networks, sensor nodes, routing protocols, Computer architecture, network control, Resource management, ad hoc networks, multifunction wireless sensor networks, SDN, Network Architecture, Pangu, ad-hoc routing]
Agreement in Epidemic Data Aggregation
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Computing and spreading global information in large-scale distributed systems pose significant challenges when scalability, parallelism, resilience and consistency are demanded. Epidemic protocols are a robust and scalable computing and communication paradigm that can be effectively used for information dissemination and data aggregation in a fully decentralised context where each network node requires the local computation of a global synopsis function. Theoretical analysis of epidemic protocols for synchronous and static network models provide guarantees on the convergence to a global target and on the consistency among the network nodes. However, practical applications in real-world networks may require the explicit detection of both local convergence and global agreement (consensus). This work introduces the Epidemic Consensus Protocol (ECP) for the determination of consensus on the convergence of a decentralised data aggregation task. ECP adopts a heuristic method to locally detect convergence of the aggregation task and stochastic phase transitions to detect global agreement and reach consensus. The performance of ECP has been investigated by means of simulations and compared to a tree-based Three-Phase Commit protocol (3PC). Although, as expected, ECP exhibits total communication costs greater than the optimal tree-based protocol, it is shown to have better performance and scalability properties; ECP can achieve faster convergence to consensus for large system sizes and inherits the intrinsic decentralisation, fault-tolerance and robustness properties of epidemic protocols.
[local convergence, Protocols, total communication costs, Scalability, decentralised data, fault-tolerance, convergence, Three-Phase Commit protocol, Gossip-based protocols, Epidemic Consensus Protocol, Large-scale distributed computing, static network models, global target, Convergence, real-world networks, Fault tolerance, synchronous network models, Fault tolerant systems, Epidemic data aggregation, Epidemic protocols, Robustness, global information, protocols, stochastic processes, explicit detection, robustness properties, Distributed consensus, local computation, intrinsic decentralisation, information dissemination, global synopsis function, trees (mathematics), Data aggregation, network node, scalable computing, heuristic method, large-scale distributed systems, aggregation task, ECP, fully decentralised context, epidemic protocols, scalability properties, communication paradigm, global agreement, Decentralised algorithms]
Fine-Grained and Real-Time Gesture Recognition by Using IMU Sensors
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Gesture recognition by using Inertial Measurement Unit (IMU) sensors plays an important role in various Internet of Things (IOT) applications, e.g., smart home, intelligent medical system and so on. Traditional technologies usually utilize machine learning algorithms to train different gestures during the offline phase, then recognize the gesture during the online phase. However, such technologies cannot recognize these gestures without prior training. Even for the same gesture, with different gesture amplitude may result in unsuccessful recognition. On the other hand, if we change the person to perform the same gesture, the algorithms also fails. In order to overcome these drawbacks, we propose an approach, which will be able to track the human body motion in real-time and also recognize complicated gestures. Our experiments results show that, the successfully recognition rate of our algorithm is 100%. Furthermore, any part of the human body can be well tracked, the tracking accuracy can reach 0.06m.
[gesture amplitude, IMU, motion-tracking, gesture recognition, Conferences, Inertial Measurement Unit sensors, gesture-recognition, IMU sensors]
A FO-ADRC Based Neutral-Point Potential Balancing for Three-Level Inverter
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
The neutral-point balancing control of neutral-point-clamped (NPC) three-level inverter is concerned. To solve this problem, two steps are considered. First, a new state-space model of DC bus voltage is established to extract the fluctuation of neutral-point potential (NPP) as a special disturbance. Then, an advanced voltage control strategy is designed based on fractional-order active disturbance rejection control (FO-ADRC) to suppress NPP. The developed FO-ADRC can not only compensate the fluctuation of NPP without a complex process of parameter turning, but enhance the robustness of the NPC inverter. Experiments are performed on a 10kW prototype of three-phase three-level inverter, and the efficiency is verified.
[advanced voltage control strategy, Capacitors, DC bus voltage, neutral-point balancing control, Inverters, NPC inverter, Voltage control, NPP, invertors, neutral-point-clamped three-level inverter, fractional-order active disturbance rejection control (FO ADRC), Robustness, FO-ADRC based neutral-point potential balancing, Neutral-point balancing, Fluctuations, active disturbance rejection control, Transfer functions, developed FO-ADRC, state-space model, fractional-order active disturbance rejection control, Neutral-point-clamped (NPC) inverter, Robust control, state space model, special disturbance, NPC three-level inverter, three-phase three-level inverter, state-space methods, voltage control]
State Identification of Cabinets Based on Convolution Neural Network
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
The screen cabinet of power system is an important part in the substation, and the automatic extraction and identification of the information displayed on the cabinet is of great significance to the substation intelligent development process. This paper has presented a cabinet state information recognizing method based on convolution neural network, which uses LeNet5 framework to automatically identify the switch, indicator light status and other information on the cabinet.
[Substations, power system, ower system screen cabinet, Switches, recognition, Light emitting diodes, screen cabinet, substations, power engineering computing, cabinet state information, Convolution, Neural networks, substation intelligent development process, LeNet5 framework, Feature extraction, convolution, Monitoring, State identification, feedforward neural nets, automatic extraction, convolution neural network]
Using LSTM Networks to Identify False Data of Smart Terminals in the Smart Grid
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Smart grid is the next generation of power system which contains a large number of smart terminals. One of the most import issues in smart grid is to provide reliable communication between smart terminals and master station to ensure system security and stability. When implementing security protection method for smart grid master station, there are several challenges such as heterogeneity within the system that need to be addressed. In this paper, we propose a novel approach, which utilizing machine learning algorithms in the gateways of master station to help secure the the smart grid. We use Long Short-Term (LSTM) network in the master station gateways to detect abnormal data sent from smart terminals. We evaluate this approach and the results demonstrate that the method can effectively identify the illegal data and improve the security of smart grid.
[system stability, gateways, Conferences, smart grid master station, power system, long short-term network, recurrent neural nets, smart power grids, smart terminals, Security, power engineering computing, Machine Learning, system security, power system security, abnormal data detection, master station, illegal data, power generation protection, security of data, security protection method, learning (artificial intelligence), machine learning algorithms, LSTM networks, Smart Grid]
Vulnerability Detection in IoT Firmware: A Survey
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
With the development of Internet of Things(IoT), more and more smart devices are connected into the Internet. The security and privacy issues of IoT devices have received increasingly academic and industrial attentions. Vulnerability detection is the key technology to protect IoT devices from zero-day attacks. However, traditional methods and tools of vulnerability detection cannot be directly used in analyzing IoT firmware. This paper firstly reviews related works on vulnerability detection in IoT firmware, previous researches are classified into four types i.e. static analysis, symbolic execution, fuzzing on emulators and comprehensive testing. Then, this paper points out that the specificity of vulnerability detection in IoT firmware is to detect logical flaws in embedded binaries which are built on the MIPS architecture. Finally, this paper proposes a method based on fuzzing and static analysis to detect authentication bypass flaws in IoT embedded binary servers. The proposed method is proved to be effective by verifying known CVEs as well as discovering unknown ones.
[telecommunication security, zero-day attacks, vulnerability detection, Servers, emulators, authentication bypass flaws, IoT firmware, Static analysis, Fuzzing, firmware, authentication- bypass, comprehensive testing, MIPS architecture, logical flaws, Tools, static analysis, IoT embedded binary servers, IoT devices, Internet of Things, security of data, Authentication, symbolic execution, data privacy, smart devices, Internet, Microprogramming]
Exploring the Efficiency of Data Collection Schemes in Wireless Sensor Networks
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Being a core-enabling technology for next generation communication infrastructure, Wireless Sensor Networks (WSNs) are under heavy research curiosity since last couple of decades. Data collection and transmission are one of the fundamental operations in WSNs. Performance of data collection directly affects the efficiency and lifetime of WSNs. The comprehensive background knowledge of data collection schemes is essential for identification of possible future directions in the domain. In this paper, we provide a review of modern data collection schemes, organize them into appropriate classes and setup their taxonomy. We explore the performance of various data collection schemes and conduct comprehensive comparative analysis. Subsequently, we identify corresponding issues and challenges for further optimization of operating environments for WSNs.
[comprehensive comparative analysis, core-enabling technology, wireless sensor networks, Taxonomy, Routing, WSNs, data collection schemes, Data Collection, Classification algorithms, Wireless Sensor Networks, next generation communication infrastructure, Wireless sensor networks, Data Gathering, Data Routing, Clustering algorithms, data transmission, Data collection, Sensors]
HomeSpy: Inferring User Presence via Encrypted Traffic of Home Surveillance Camera
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Wireless cameras are widely deployed in homes and offices for security guarding, and play as an important part of smart home devices. Those security cameras, which are supposed to provide protection services, however, may in turn leak personal privacy that can result in security issues. In this paper, we reveal that attackers are able to eavesdrop the traffic of wireless cameras and analyze whether you are at home or not without entering the house. We propose HomeSpy, an attack tool that infers the house status by inspecting the bitrate variation of the wireless camera traffic. We implement HomeSpy on the Android platform and validate it on 3 cameras. The evaluation results show that HomeSpy can achieve a successful attack rate of 97.2%.
[user presence, Conferences, HomeSpy, security cameras, security guarding, wireless camera traffic, personal privacy leakage, protection services, cryptography, security issues, smart phones, home surveillance camera, cameras, mobile computing, encrypted traffic, Android platform, User Privacy, data privacy, Surveillance Camera, Traffic Analysis, wireless LAN, video surveillance, smart home devices, telecommunication traffic]
Location Prediction Based on User Mobile Behavior Similarity
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
In practical application, GPS trajectories are often sparse due to the sampling points lost or a new user appearing, which makes the accuracy of location prediction low based on a single user data. To solve this problem, this paper proposes a novel Markov location prediction approach based on user mobile behavior similarity clustering. First, this paper proposes a region partitioning method based on Voronoi diagram, transforms the GPS trajectories into region trajectories and predicts the locations over region trajectories. Second, this paper proposes a new approach to measure the similarity of users' mobile behavior by considering users' transferring features and regional features. Third?based on the mobile behavior similarity, this paper divides users into various groups and employs the first-order Markov model on the groups for location prediction. The experiments over real GPS trajectory dataset indicate that the proposed method is effective for location prediction.
[location based services, clustering probability vector, GPS trajectory dataset, Probability, Predictive models, computational geometry, GPS trajectories, regional features, region vector, Global Positioning System, transition probability matrix, Computer science, Voronoi diagram, pattern clustering, location prediction, Markov location prediction approach, user mobile behavior similarity clustering, Markov processes, mobile behavior similarity, Trajectory, region partitioning method, location prediction low, region trajectories]
SOLO: 2D Localization with Single Sound Source and Single Microphone
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Ultrasound based tracking and localization are more and more popular in recent years. However, to achieve 3D tracking or localization, at least three sound sources are needed. Unfortunately, there are only two sound sources available in most scenarios in daily life. To address this problem, we propose SOLO (Single sound sOurce LOcalization with single microphone), a novel approach to infer 2D information from single sound source. We found that sound with different frequencies from a signal source may have different strength distribution in different areas. Each area has a unique group of sound strengths of different frequencies. So we can take this question as a classify question. Based on this, we first obtain the signal strength in different frequencies by using STFT (Short-Time Fourier Transform). Next we measure 1D distance with traditional phase-based distance measuring approach. Then we take 1D distance and the sound strengths as features to a neural network to classify the sample into different classes, each representing a region in 2D space. With our system, we can distinguish 3 &#x00D7; 3 regions (5 cm &#x00D7; 5 cm for each region) with only single sound source and single microphone. SOLO is the first work to achieve 2D localization and tracking with only single sound source and single microphone as far as we know. This work make it possible to realize 3D tracking or localization with only two sound sources.
[Phase measurement, Fourier transforms, Tracking, ultrasound based tracking, STFT, Mobile handsets, Frequency measurement, neural network, 2D Localization, single sound source localization, phase-based distance measuring approach, distance measurement, microphones, 2D localization, microphone, 3D tracking, Single Soung Source, acoustic radiators, Single Microphone, Two dimensional displays, phase measurement, acoustic generators, Microphones, signal classification, 2D tracking, signal sources, 3D localization, Neural networks, 1D distance measurement, SOLO, short-time Fourier transform, ultrasound based localization]
Using Positioning Priorities for Accurate Anchor-Based Node Location over Wireless Sensor Networks
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Accurate node locations are critically demanded for various WSN applications. A widely adopted approach assumes a few nodes used as anchor nodes whose locations are known and the distances between nodes can be estimated. The accuracy of existing estimation can be improved due to the flip ambiguity (FA) phenomena. This paper presents an original anchor-based node location approach for better accuracy by using positioning priorities. We first comprehensively investigate the FA phenomena and study its effect factors, then introduce a new concept exploring the node positioning priority to avoid FA as much as possible. Finally an empirical simulation study has been carried out to evaluate our proposed approach against the simulated annealing based location (SAL), genetic algorithm based location (GAL) and collinearity-aware trilateration incorporated with the implicit collinearity method (TRI-ICD). The experimental results show that our proposed approach reduce location errors.
[wireless sensor networks, FA phenomena, simulated annealing based location, Geophysical measurements, node positioning priority, Flip Ambiguity, empirical simulation study, Simulated annealing, WSN applications, genetic algorithm based location, collinearity-aware trilateration incorporated with the implicit collinearity method, simulated annealing, Estimation, Node Location, genetic algorithms, flip ambiguity phenomena, Localization Priority, Wireless sensor networks, GAL, WSN, Simulation, anchor-based node location approach, Distance measurement, Nickel, TRI-ICD, SAL]
Multi-attribute Event Modeling and Prediction over Event Streams from Sensors
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
The paper studies on the modeling and prediction method for multi-attribute events, which exist widely in practical application. In view of the features of multi-attribute events, the paper proposes an event processing framework including the construction of training sample space based on multi-attribute event grid (MAEG), bi-direction SVR (Support Vector Regression) modeling method for prediction based on longitudinal modeling and transverse modeling, and model updating strategy based on ?-insensitive error. The performance of the proposed methods of the framework is demonstrated by extensive experiments with real datasets.
[regression analysis, Predictive models, training sample space, bi-direction SVR, Data mining, multi-attribute event grid, Training, MAEG, ?-insensitive error, &#x025B;- insensitive error, longitudinal modeling, transverse modeling, event processing framework, support vector machines, Computational modeling, Time series analysis, Support Vector Regression, sensors, multi-attribute event grid (MAEG), event streams, model updating strategy, Data models, data handling, multi-attribute event modeling and prediction, longitudinal modeling and transverse modeling]
Blockchain with Accountable CP-ABE: How to Effectively Protect the Electronic Documents
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
As a recently proposed public key primitive, attribute-base encryption(ABE) divided into Ciphertext-policy ABE (CP-ABE) and Key-policy ABE (KP-ABE) is a highly promising tool for the management and protection of data. And Blockchain, as one of the core technologies of Bitcoin that is the most representative cryptocurrency, has received extensive attentions recently. Supervision and privacy protection are two difficulties in blockchain. In this paper, a new scheme combining blockchain and accountable CP-ABE is proposed. In this scheme, each change of the data is recorded on the blockchain. And the different permissions of access are realized through ABE. If a malicious user shares a decryption key illegally, it will be considered illegal. Similarly, if the authority generates a decryption key for any unauthorized user, it also will be considered illegal. This scheme allows any third party to publicly verify the identity of a decryption key. And it is achievable for an auditor to publicly audit whether a malicious user or the authority should be responsible for an exposed decryption key, and the key abuser cannot deny it. At last this scheme is applied to the management of electronic documents.
[Industries, data management, attribute-base encryption, key abuser, exposed decryption key, Blockchain, Traceability, public key cryptography, data protection, Key-policy ABE, document handling, Bitcoin technologies, accountable CP-ABE, KP-ABE, KP, Internet of Things, CP, cryptocurrency, blockchain, privacy protection, Ciphertext-policy ABE, CP-ABE, Public key, public key primitive, Accountability, Writing, Electronic Documents, electronic document protection]
Blockchain-Based Government Information Resource Sharing
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Government information resource (GIR) sharing, an important means for efficient government working, requires new technology to enhance its reliability and security. Blockchain, as an emerging technology for building decentralized applications, is gradually penetrating various fields. In this paper, we present a technical combination that consists of Blockchain structure, network sharing model and consensus algorithms. With these techniques, we design and implement a Blockchain-based GIR sharing system (BGIRSS), which is decentralized, to make the sharing procedure more efficient. The results of a series of emulational experiments show that our system is securer and more reliable than conventional sharing schemes, and can effectively promote the sharing efficiency of GIRs with lower implementation cost.
[information resources, BGIRSS, Government, reliability, Blockchain, cryptography, government information resource, Blockchain structure, consensus, Data centers, sharing schemes, security, decentralization, Blockchain-based GIR sharing system, Information services, consensus algorithms, network sharing model, Peer-to-peer computing, Reliability, building decentralized applications, government data processing, Blockchain-Based Government Information Resource Sharing]
M2M Blockchain: The Case of Demand Side Management of Smart Grid
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
The purpose of this paper is to explore applications of blockchain technology related to demand side management of smart grid and to present an example that blockchain is used to facilitate machine-to-machine (M2M) interaction and frame an electricity market in the context of demand request. We use blockchain technology to record data derived from power flow calculation model and electricity price customization, and use smart contract to store transaction data and transfer assets automatically. Firstly, we establish a power flow calculation for microgrid operation system of 34 node master-slave control island, and the power flow is calculated and an optimal generator work adjustment is used. Then, according to the price customization, the participation mode of priced demand response is acquired. The presented scenario includes that a power management system and a generator which can actively adjust the power generation trading with each other over a blockchain. According to flow calculation and price customization, power management system generate smart contracts automatically. The final is that two sides complete the transaction and the load state of grid has been adjusted. This work contributes a proof-of-concept implementation of the scenario. This example verifies the feasibility of the method.
[priced demand response, 34 node master-slave control island, Microgrids, smart power grids, distributed power generation, Load flow, power generation economics, smart contract, power generation trading, flow calculation, Smart grids, power markets, blockchain technology, Contracts, record data, transfer assets, smart grid, electricity market, machine-to-machine interaction, demand request, power flow calculation model, electricity price customization, Generators, optimal generator work adjustment, power engineering computing, microgrid operation system, power management system, transaction data, demand side management, load flow, pricing, M2M blockchain, demand response]
Education Application of Blockchain Technology: Learning Outcome and Meta-Diploma
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
This paper proposes an education blockchain technology based on learning outcome, which is based on graduation requirement index of university, with professional certification and uses automated evaluation software as a tool. The course-learning outcome achievement values, which is based on the quantitative and qualitative combination of grades, process and evidence, the course name, learning outcome name (graduation requirement indicator) and the weight of the course, etc. are all record in the block. The conversion from evaluation of students' achievement to the post-job competence evaluation results is completed, and counterforces of student competency evaluation is send to the curriculum, which realize the continuous improvement of the curriculum.
[course name, OBE education, education application, students, Indexes, post-job competence evaluation results, block chain, automated evaluation software, Certification, course-learning outcome achievement values, graduation requirement indicator, quantitative combination, teaching space, educational courses, professional certification, Software, qualitative combination, computer aided instruction, educational institutions, meta-diploma, education blockchain technology, outcome name, student competency evaluation, Contracts]
[Publisher's information]
2017 IEEE 23rd International Conference on Parallel and Distributed Systems
None
2017
Provides a listing of current committee members and society officers.
[]
