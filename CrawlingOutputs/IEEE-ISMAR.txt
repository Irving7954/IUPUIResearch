1038
Calibration of a head-mounted projective display for augmented reality systems
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
In augmented reality (AR) applications, registering a virtual object with its real counterpart accurately and comfortably is one of the basic and challenging issues in the sense that the size, depth, geometry, as well as physical attributes of the virtual objects have to be rendered precisely relative to a physical reference, which is well known as the calibration or registration problem. This paper presents a systematic calibration process to address static registration in a custom-designed augmented reality system, which is based upon the recent advancement of head-mounted projective display (HMPD) technology. Following a concise review of the HMPD concept and system configuration, we present in detail a computational model for system calibration, describe the calibration procedures to obtain estimations of the unknown transformations, and include the calibration results, evaluation experiments and results.
[virtual object size, image registration, Displays, augmented reality, helmet mounted displays, static registration, Calibration, Augmented reality, virtual object registration, virtual object depth, head-mounted projective display, computational model, virtual object geometry, calibration, rendering (computer graphics)]
A mixed reality system with visual and tangible interaction capability: application to evaluating automobile interior design
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
This paper presents a mixed reality (MR) system with tangible interface as well as visual fusion in an MR space. The sense of touch is given by physical objects on which computer generated imagery is accurately registered and superimposed. The proposed approach is especially useful in industrial design where digital mockups and physical mockups are thoroughly utilized.
[Shape, physical mockups, Laboratories, image registration, haptic interfaces, augmented reality, tangible interaction, tangible interface, haptic interface, computer generated imagery, MR system, Physics computing, interior design evaluation, Feedback, MR space, Virtual reality, industrial design, visual interaction, visual fusion, object registration, Haptic interfaces, automobile industry, Automobiles, Augmented reality, user interface, computer graphics, mixed reality, Character generation, Image generation, digital mockups, automobile interior design]
Automated initialization for marker-less tracking: a sensor fusion approach
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
We introduce a sensor fusion approach for automated initialization of marker-less tracking systems. It is not limited in tracking range and working environment, given a 3D model of the objects or the real scene. This is achieved based on a statistical analysis and probabilistic estimation of the uncertainties of the tracking sensors. The explicit representation of the error distribution allows the fusion of different sensor data. This methodology was applied to an augmented reality system, using a mobile camera and several stationary tracking sensors, and can be easily extended to the case of any additional sensor. In order to solve the initialization problem, we adapt, modify and integrate advanced techniques such as plenoptic viewing, intensity-based registration, and ICP. Thereby, the registration error is minimized in 3D object space rather than in 2D image. Experimental results show how complex objects can be registered efficiently and accurately to a single image.
[Uncertainty, image registration, Sensor fusion, augmented reality, sensor fusion, Sensor systems, real scene, cameras, mobile camera, Space technology, augmented reality system, stationary tracking sensors, registration error, marker-less tracking system, intensity-based registration, probabilistic estimation, Target tracking, Statistical analysis, 3D object model, probability, error distribution, Augmented reality, Global Positioning System, 3D object space, automated initialization, Layout, plenoptic viewing, target tracking, ICP, Cameras, statistical analysis]
ASTOR: an autostereoscopic optical see-through augmented reality system
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
We present a novel autostereoscopic optical see-through system for augmented reality (AR). It uses a transparent holographic optical element (HOE) to separate the views produced by two, or more, digital projectors. It is a minimally intrusive AR system that does not require the user to wear special glasses or any other equipment, since the user would see different images depending on the point of view. The HOE itself is a thin glass plate or plastic film that can easily be incorporated into other surfaces, such as a window. The technology offers great flexibility, allowing the projectors to be placed where they are the least intrusive. ASTOR's capability of sporadic AR visualization is currently ideal for smaller physical workspaces, such as our prototype setup in an industrial environment.
[intrusive AR system, Glass, holographic optical elements, Holography, Holographic optical components, Displays, augmented reality, projection-based, optical see-through, autostereoscopy, Prototypes, data visualisation, computer displays, holographic optical element, Optical films, Mirrors, visual perception, sporadic AR visualization, Augmented reality, optical projectors, autostereoscopic optical see-through augmented reality system, Handheld computers, Numerical analysis, system, stereo image processing, ASTOR]
Hybrid tracking algorithms for planar and non-planar structures subject to illumination changes
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
Augmented reality (AR) aims to fuse a virtual world and a real one in an image stream. When considering only a vision sensor, it relies on registration techniques that have to be accurate and fast enough for on-line augmentation. This paper proposes a real-time, robust and efficient 3D model-based tracking algorithm monocular vision system. A virtual visual servoing approach is used to estimate the pose between the camera and the object. The integration of texture information in the classical non-linear edge-based pose computation provides a more reliable tracker. Several illumination models have been considered and compared to better deal with the illumination change in the scene. The method presented in this paper has been validated on several video sequences for augmented reality applications.
[Real time systems, Fuses, image registration, 3D model-based tracking algorithm monocular vision system, hybrid tracking algorithms, augmented reality, Visual servoing, virtual visual servoing approach, illumination changes, virtual world, tracking, registration techniques, Augmented reality, online augmentation, Machine vision, Layout, texture information integration, Lighting, computer vision, Streaming media, Cameras, Robustness, nonplanar structures]
Visual Longitudinal and Lateral Driving Assistance in the Head-Up Display of Cars
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
Most car accidents occur due to longitudinal collisions or lane departure. We assume that the number of such accidents can be reduced, if the driver knows more precisely, where the car is heading and at which distance it can stop. To provide drivers with this kind of anticipation, we have developed two augmented reality based visualization schemes for longitudinal and lateral driver assistance in the head-up display (HUD) of cars. One presentation scheme indicates the braking distance by a virtual bar on the road. The second scheme adds the visualization of a drive-path between the car and the bar, zoning the entire region that the car will pass before coming to a complete halt. We have tested both schemes in a driving simulator in comparison to a baseline without visual assistance. Our results show, among other findings, that the bar is preferred, that it supports driving performance and that it does not increase mental workload.
[Head-Up Display, Visualization, lateral driving assistance, Augmented Reality, Human factors, head-up display, Displays, augmented reality, longitudinal collisions, Augmented reality, Road accidents, driver information systems, data visualisation, Virtual reality, computer vision, Traffic control, Advanced Driver Assistance Systems, Usability, Mixed Reality, visual longitudinal driving assistance, visualization schemes, Human Factors, Driver circuits, Testing]
Vesp&#x2019;R: design and evaluation of a handheld AR device
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
This paper focuses on the design of devices for handheld spatial interaction. In particular, it addresses the requirements and construction of a new platform for interactive AR, described from an ergonomics stance, prioritizing human factors of spatial interaction. The result is a multi-configurable platform for spatial interaction, evaluated in two AR application scenarios. The user tests validate the design with regards to grip, weight balance and control allocation, and provide new insights on the human factors involved in handheld spatial interaction.
[Performance evaluation, handheld spatial interaction, human factor, Force, VesppsilaR design, human factors, Fatigue, augmented reality, Construction industry, multiconfigurable platform, Computer Graphics, interactive handheld augmented reality device, Ergonomics, Fingers, I.3.6 [Computer Graphics, human computer interaction, interactive devices, Sensors, ergonomics stance]
Continuous natural user interface: Reducing the gap between real and digital world
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
Augmented reality (AR) presentation enables the creation of natural user interfaces that employ the whole user's environment as interaction device. Additionally, by using hand based 3D interaction with gestures that have a physical meaning like grabbing, dragging, and dropping this leads to a user experience that is intuitive, since close to the real world's behavior. We propose a novel approach to an AR-based natural user interface, that goes one step further by enabling the contents of the interface to switch domains from a virtual instance in AR to a physical instance in the real-world. All instances stay associated and changes made to the physical instance will be reflected on the virtual one. Because the behavior of our interface in AR is in key aspects consistent with the real-world, the gap between those domains is made less salient. To demonstrate our concept, we have implemented an exemplary industrial use case. Our main contribution is the methodology for an intuitive interface we call continuous natural user interface (CNUI). Additionaly, we conducted a user study to investigate the acceptance of this kind of interface. Results indicate an ergonomic ease and after a training period also an increased performance when using our system.
[COMPUTER GRAPHICS, ergonomic, Multimedia systems, continuous natural user interface, H.5.2 [INFORMATION INTERFACES AND PRESENTATION, human factors, Switches, 3D interaction device, INFORMATION INTERFACES AND PRESENTATION, augmented reality, user experience, Augmented reality, Industrial training, gesture recognition, Ergonomics, Keyboards, real-digital world, Virtual reality, Computer graphics, User interfaces, Mice, human computer interaction, virtual instance]
Accurate real-time tracking using mutual information
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
In this paper we present a direct tracking approach that uses Mutual Information (MI) as a metric for alignment. The proposed approach is robust, real-time and gives an accurate estimation of the displacement that makes it adapted to augmented reality applications. MI is a measure of the quantity of information shared by signals that has been widely used in medical applications. Since then, and although MI has the ability to perform robust alignment with illumination changes, multi-modality and partial occlusions, few works propose MI-based applications related to object tracking in image sequences due to some optimization problems. In this work, we propose an optimization method that is adapted to the MI cost function and gives a practical solution for augmented reality application. We show that by refining the computation of the Hessian matrix and using a specific optimization approach, the tracking results are far more robust and accurate than the existing solutions. A new approach is also proposed to speed up the computation of the derivatives and keep the same optimization efficiency. To validate the advantages of the proposed approach, several experiments are performed. The ESM and the proposed MI tracking approaches are compared on a standard dataset. We also show the robustness of the proposed approach on registration applications with different sensor modalities: map versus satellite images and satellite images versus airborne infrared images within different AR applications.
[image registration, optical tracking, augmented reality, Entropy, real-time tracking, Histograms, optimisation, partial occlusion, optimization, displacement estimation, object tracking, Robustness, mutual information, Joints, Hessian matrices, image sequences, illumination, Estimation, geophysical image processing, multimodality, image sequence, Equations, target tracking, Hessian matrix, airborne infrared image, satellite image, Mutual information, MI-based application, sensor modality]
Accurate and robust planar tracking based on a model of image sampling and reconstruction process
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
It is one of the central issues in augmented reality and computer vision to track a planar object moving relatively to a camera in an accurate and robust manner. In previous studies, it was pointed out that there are several factors making the tracking difficult, such as illumination change and motion blur, and effective solutions were proposed for them. In this paper, we point out that degradation in effective image resolution can also deteriorate tracking performance, which typically occurs when the plane being tracked has an oblique pose with respect to the viewing direction, or when it moves to a distant location from the camera. The deterioration tends to become significantly large for extreme configurations, e.g., when the planar object has nearly a right angle with the viewing direction. Such configurations can frequently occur in AR applications targeted at ordinary users. To cope with this problem, we model the sampling and reconstruction process of images, and present a tracking algorithm that incorporates the model to correctly handle these configurations. We show through several experiments that the proposed method shows better performance than conventional methods.
[Degradation, Target tracking, Image resolution, Accuracy, Visual tracking, Image sampling and reconstruction, Cameras, Image formation process, Planar tracking, Image reconstruction]
Workshop 2: Classifying the AR presentation space
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Already 3D visualization environments provide a large design space not being investigated to the same extent as traditional WIMP-spaces. When using this design space in combination with AR, the design space even further grows. Information can not only be presented in a 3D space, AR also puts virtual information in relation to real objects, locations or events. The different properties of presentation in AR need to be investigated to develop a comprehensive set of dimensions of presentation principles.
[]
Scanning and tracking dynamic objects with commodity depth cameras
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
The 3D data collected using state-of-the-art algorithms often suffers from various problems, such as incompletion and inaccuracy. Using temporal information has been proven effective for improving the reconstruction quality; for example, KinectFusion [21] shows significant improvements for static scenes. In this work, we present a system that uses commodity depth and color cameras, such as Microsoft Kinects, to fuse the 3D data captured over time for dynamic objects to build a complete and accurate model, and then tracks the model to match later observations. The key ingredients of our system include a nonrigid matching algorithm that aligns 3D observations of dynamic objects by using both geometry and texture measurements, and a volumetric fusion algorithm that fuses noisy 3D data. We demonstrate that the quality of the model improves dramatically by fusing a sequence of noisy and incomplete depth data of human and that by deforming this fused model to later observations, noise-and-hole-free 3D models are generated for the human moving freely.
[Solid modeling, Heuristic algorithms, dynamic object tracking, color camera, nonrigid matching algorithm, reconstruction quality, Three-dimensional displays, Image color analysis, data visualisation, object tracking, image colour analysis, static scene, dynamic object scanning, Deformable models, KinectFusion, Microsoft Kinects, temporal information, texture measurement, volumetric fusion algorithm, image reconstruction, image texture, noisy 3D data, noise-and-hole-free 3D model, Cameras, commodity depth camera, Data models, geometry, solid modelling]
Importance weighted image enhancement for prosthetic vision: An augmentation framework
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Augmentations to enhance perception in prosthetic vision (also known as bionic eyes) have the potential to improve functional outcomes significantly for implantees. In current (and near-term) im-plantable electrode arrays resolution and dynamic range are highly constrained in comparison to images from modern cameras that can be head mounted. In this paper, we propose a novel, generally applicable adaptive contrast augmentation framework for prosthetic vision that addresses the specific perceptual needs of low resolution and low dynamic range displays. The scheme accepts an externally defined pixel-wise weighting of importance describing features of the image to enhance in the output dynamic range. Our approach explicitly incorporates the logarithmic scaling of enhancement required in human visual perception to ensure perceivability of all contrast augmentations. It requires no pre-existing contrast, and thus extends previous work in local contrast enhancement to a formulation for general image augmentation. We demonstrate the generality of our augmentation scheme for scene structure and looming object enhancement using simulated prosthetic vision.
[Visualization, Image resolution, Brightness, Dynamic range, augmentation framework, bionic eyes, looming object enhancement, Equations, pixel-wise weighting, Histograms, implantable electrode arrays resolution, image enhancement, enhancement scaling, prosthetics, computer vision, prosthetic vision, importance weighted image enhancement, scene structure, medical image processing, Prosthetics]
Efficient Computation of Absolute Pose for Gravity-Aware Augmented Reality
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
We propose a novel formulation for determining the absolute pose of a single or multi-camera system given a known vertical direction. The vertical direction may be easily obtained by detecting the vertical vanishing points with computer vision techniques, or with the aid of IMU sensor measurements from a smartphone. Our solver is general and able to compute absolute camera pose from two 2D-3D correspondences for single or multi-camera systems. We run several synthetic experiments that demonstrate our algorithm's improved robustness to image and IMU noise compared to the current state of the art. Additionally, we run an image localization experiment that demonstrates the accuracy of our algorithm in real-world scenarios. Finally, we show that our algorithm provides increased performance for real-time model-based tracking compared to solvers that do not utilize the vertical direction and show our algorithm in use with an augmented reality application running on a Google Tango tablet.
[multicamera system, smartphone, gravity-aware augmented reality, augmented reality, multi-camera system, sensor measurements, Three-dimensional displays, Accuracy, Google Tango tablet, real-time model-based tracking, image localization experiment, model-based tracking, Robustness, Gravity, Google, computer vision techniques, inertial sensor, IMU noise, smart phones, vertical vanishing points, Augmented reality, 3D correspondences, vertical direction, real-time systems, Cameras, 2D correspondences, Absolute pose, absolute pose]
Practical and Precise Projector-Camera Calibration
2016 IEEE International Symposium on Mixed and Augmented Reality
None
2016
Projectors are important display devices for large scale augmented reality applications. However, precisely calibrating projectors with large focus distances implies a trade-off between practicality and accuracy. People either need a huge calibration board or a precise 3D model [12]. In this paper, we present a practical projector-camera calibration method to solve this problem. The user only needs a small calibration board to calibrate the system regardless of the focus distance of the projector. Results show that the root-mean-squared re-projection error (RMSE) for a 450cm projection distance is only about 4mm, even though it is calibrated using a small B4 (250&#x00D7;353mm) calibration board.
[H.5.1 [INFORMATION INTERFACES AND PRESENTATION (e.g., Distortion, augmented reality, Calibration, RMSE, Augmented reality, large scale augmented reality, optical projectors, root-mean-squared re-projection error, cameras, Three-dimensional displays, calibration board, Lead, display devices, Cameras, INFORMATION INTERFACES AND PRESENTATION (e.g., Robustness, HCI), projector-camera calibration method, calibration]
Occlusion Matting: Realistic Occlusion Handling for Augmented Reality Applications
2017 IEEE International Symposium on Mixed and Augmented Reality
None
2017
Nowadays, visualizations in Augmented Reality have to be as realistic as possible with lowest possible computational cost. In this paper, we present a real-time solution to realize dynamic occlusions. Sometimes virtual objects are softly, partially or totally occluded by real objects. Incorrect and inaccurate occlusion handling breaks the illusion of co-existence between the real and virtual world on the one hand and can result in wrong depth perception on the other hand. Our approach formulates the occlusion problem as alpha matting problem. Instead of calculating the visibility for each pixel of the virtual objects we estimate a blending coefficient. This enables a seamless integration of virtual objects in the real world, even for fuzzy foreground objects (like hair). Our approach takes raw depth information of the real scene (e.g. obtained by a low cost depth sensor) to realize rough foreground background segmentation. The blending coefficient between transitions where depth values are typically noisy is estimated based on the color image. Experimental evaluations of several scenes demonstrate that our algorithm produces consistent and visually appealing occlusions between the real and virtual scene with low computational cost. Furthermore, we compare the results with related depth-based approaches and show that our algorithm overcomes previous limitations.
[wrong depth perception, Solid modeling, Alpha Matting, Realistic Occlusion, fuzzy set theory, alpha matting problem, Augmented Reality, Augmented Reality applications, Natural Image Matting, augmented reality, virtual world, virtual scene, Image color analysis, image segmentation, Real-time systems, virtual objects, Dynamic Occlusion Handling, realistic occlusion handling, Image edge detection, blending coefficient, Color, fuzzy foreground objects, Noise measurement, rough foreground background segmentation, Augmented reality, occlusion matting]
Spacedesign: a mixed reality workspace for aesthetic industrial design
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
Spacedesign is an innovative mixed reality (MR) application addressed to aesthetic design of free form curves and surfaces. It is a unique and comprehensive approach which uses task-specific configurations to support the design workflow from concept to mock-up evaluation and review. The first-phase conceptual design benefits from a workbench-like 3-D display for free hand sketching, surfacing and engineering visualization. Semitransparent stereo glasses augment the pre-production physical prototype by additional shapes, textures and annotations. Both workspaces share a common interface and allow collaboration and cooperation between different experts, who can configure the system for the specific task. A faster design workflow and CAD data consistency can be thus naturally achieved. Tests and collaborations with designers, mainly from automotive industry, are providing systematic feedback for this ongoing research. As far as the authors are concerned, there is no known similar approach that integrates the creation and editing phase of 3D curves and surfaces in virtual and augmented reality (VR/AR). Herein we see the major contribution of our new application.
[aesthetic industrial design, Design automation, Shape, Glass, CAD/CAM, augmented reality, design workflow, 3D display, user interfaces, Aerospace industry, free hand sketching, semitransparent stereo glasses, Design engineering, Prototypes, Virtual reality, Three dimensional displays, engineering visualization, image processing, automotive industry, free form curves, CAD, virtual realty, helmet mounted displays, automobile industry, Spacedesign, user interface, mixed reality workspace, data consistency, Data visualization, Collaborative work, engineering graphics, free form surfaces]
Interactive theatre experience in embodied + wearable mixed reality space
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
This paper presents an interactive theatre based on an embodied mixed reality space and wearable computers. Embodied computing mixed reality spaces integrate ubiquitous computing, tangible interaction and social computing within a mixed reality space, which enables intuitive interaction with physical world and virtual world. We believe it has potential advantages to support novel interactive theatre experiences. Therefore, we explored the novel interactive theatre experience supported in the embodied mixed reality space, and implemented live 3D characters to interact with user in such a system.
[Pervasive computing, Social network services, embodied mixed reality space, Humans, interactive theatre experience, Ubiquitous computing, augmented reality, tangible interaction, virtual world, ubiquitous computing, social computing, Augmented reality, intuitive interaction, wearable computers, physical world, humanities, Space technology, Wearable computers, Physics computing, Layout, Virtual reality, live 3D characters]
Tracking with omni-directional vision for outdoor AR systems
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
Most pose (3D position and 3D orientation) tracking methods using vision require a priori knowledge about the environment and correspondences between 3D environment features and 2D images. This environmental information is difficult to acquire accurately for large working volumes or may not be available at all, especially for outdoor environments. As a result, most pose tracking methods using vision are designed for small indoor working spaces. We track the pose of a moving camera from 2D images of the world. The pose of a camera is tracked through two 5 degree-of-freedom (DOF) motion estimations, which requires only 2D-to-2D correspondences. Therefore, the presented method can be applied to varied working space sizes including outdoor environments.
[Tracking, Motion estimation, Design methodology, 3D orientation, outdoor augmented reality systems, optical tracking, omni-directional vision, 5 degree-of-freedom motion estimation, augmented reality, 2D images, 3D position, Calibration, moving camera, Augmented reality, Graphics, pose tracking, Image databases, Space technology, Layout, computer vision, motion estimation, Cameras, 2D-to-2D correspondences]
Occlusion shadows: using projected light to generate realistic occlusion effects for view-dependent optical see-through displays
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
This paper presents projector-based illumination techniques for creating correct occlusion effects for optical see-through setups. We project view-dependent occlusion shadows onto the real surfaces that are located behind virtual objects. This results in a perfect occlusion of real objects by virtual ones. We have implemented and tested our approach in the context of the Virtual Showcase display. We describe a hardware extension for projecting light into the showcase and present our rendering techniques for displaying occlusion shadows for single and multi-user environments as well as for single and multi-light-projector configurations. We also report on the limitations of our system for multi-user situations and describe our experiences with a first experimental prototype.
[realistic occlusion effect generation, rendering techniques, augmented reality, hardware extension, multi light projector configurations, Prototypes, computer displays, Virtual reality, Computer graphics, view-dependent occlusion shadow projection, Hardware, virtual objects, Mirrors, rendering (computer graphics), Testing, multi-user environments, single-user environments, projected light, helmet mounted displays, projector-based illumination techniques, Lighting control, view-dependent optical see-through displays, Augmented reality, hidden feature removal, Virtual Showcase display, Computer displays, Optical sensors, real surfaces, single light projector configurations]
Circular data matrix fiducial system and robust image processing for a wearable vision-inertial self-tracker
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
A wearable low-power hybrid vision-inertial tracker has been demonstrated based on a flexible sensor fusion core architecture, which allows easy reconfiguration by plugging-in different kinds of sensors. A particular prototype implementation consists of one inertial measurement unit and one out-ward-looking wide-angle smart camera, with a built-in DSP to run all required image-processing tasks. The smart camera operates on newly designed 2D bar-coded fiducials printed on a standard black-and-white printer. The fiducial design allows having thousands of different codes, thus enabling uninterrupted tracking throughout a large building or even a campus at very reasonable cost. The system operates in various real-world lighting conditions without user intervention due to homomorphic image processing algorithms for extracting fiducials in the presence of very non-uniform lighting.
[Image processing, built-in DSP, optical tracking, Sensor fusion, robust image processing, augmented reality, sensor fusion, reconfiguration, black-and-white printer, circular data matrix fiducial system, inertial measurement unit, Measurement units, flexible sensor fusion core architecture, Smart cameras, Prototypes, Robustness, outward-looking wide-angle smart camera, homomorphic image processing algorithms, Wearable sensors, nonuniform lighting, campus, Printers, Intelligent sensors, large building, wearable low-power hybrid vision-inertial self-tracker, Digital signal processing, computer vision, 2D bar-coded fiducials, uninterrupted tracking, real-world lighting conditions]
Fata Morgana - a presentation system for product design
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
Mobile augmented reality applications promise substantial savings in time and costs for product designers, in particular, for large products requiring scale models and expensive clay mockups (e.g., cars). Such applications are novel and introduce interesting challenges when attempting to describe them to potential users and stakeholders. For example, it is difficult, a priori, to assess the nonfunctional requirements of such applications and anticipate the usability issues that the product designers are likely to raise. In this paper, we describe our efforts to develop a proof-of-concept AR system for car designers. Over the course of a year, we developed two prototypes, one within a university context, the other at a car manufacturer. The lessons learned from both efforts illustrate the technical and human challenges encountered when closely collaborating with the end user in the design of a novel application.
[car design, Fata Morgana project, augmented reality, Prototypes, computer displays, Virtual reality, product development, Online Communities/Technical Collaboration, technical presentation, presentation system, large products, mobile augmented reality applications, Head, automobiles, CAD, helmet mounted displays, Product design, Application software, Augmented reality, product design, Cameras, Rendering (computer graphics), engineering graphics, Usability, end user collaboration]
Accurate camera calibration for off-line, video-based augmented reality
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
Camera tracking is a fundamental requirement for video-based augmented reality applications. The ability to accurately calculate the intrinsic and extrinsic camera parameters for each frame of a video sequence is essential if synthetic objects are to be integrated into the image data in a believable way. In this paper, we present an accurate and reliable approach to camera calibration for off-line video-based augmented reality applications. We first describe an improved feature tracking algorithm, based on the widely used Kanade-Lucas-Tomasi tracker. Estimates of inter-frame camera motion are used to guide tracking, greatly reducing the number of incorrectly tracked features. We then present a robust hierarchical scheme that merges sub-sequences together to form a complete projective reconstruction. Finally, we describe how RANSAC-based random sampling can be applied to the problem of self-calibration, allowing for more reliable upgrades to metric geometry. Results of applying our calibration algorithms are given for both synthetic and real data.
[Tracking, optical tracking, RANSAC-based random sampling, augmented reality, robust hierarchical scheme, inter-frame camera motion estimation, Image reconstruction, metric geometry, projective reconstruction, motion estimation, Robustness, calibration, video signal processing, image sequences, intrinsic camera parameters, Motion estimation, camera tracking, Video sequences, sub-sequences, self-calibration, video sequence, Calibration, Augmented reality, feature tracking algorithm, Geometry, synthetic objects, extrinsic camera parameters, off-line video-based augmented reality, Kanade-Lucas-Tomasi tracker, Cameras, Sampling methods, accurate camera calibration]
Online 6 DOF augmented reality registration from natural features
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
We present a complete scalable system for 6 DOF camera tracking based on natural features. Crucially, the calculation is based only on pre-captured reference images and previous estimates of the camera pose and is hence suitable for online applications. We match natural features in the current frame to two spatially separated reference images. We overcome the wide baseline matching problem by matching to the previous frame and transferring point positions to the reference images. We then minimize deviations from the two-view and three-view constraints between the reference images and the current frame as a function of camera position parameters. We stabilize this calculation using a recursive form of temporal regularization that is similar in spirit to the Kalman filter. We can track camera pose over hundreds of frames and realistically integrate virtual objects with only slight jitter.
[Shape, camera pose tracking, image registration, optical tracking, Jitter, augmented reality, recursive temporal regularization, three-view constraints, scalable system, Fluid flow measurement, pre-captured reference images, virtual objects, Labeling, wide baseline matching problem, spatially separated reference images, Motion estimation, Two dimensional displays, online 6 DOF augmented reality registration, two-view constraints, point position transfer, image matching, Augmented reality, camera position parameters, jitter, Layout, camera pose estimation, Position measurement, Cameras, natural features]
Augmented urban planning workbench: overlaying drawings, physical models and digital simulation
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
There is a problem in the spatial and temporal separation between the varying forms of representation used in urban design. Sketches, physical models, and more recently computational simulation, while each serving a useful purpose, tend to be incompatible forms of representation. The contemporary designer is required assimilate these divergent media into a single mental construct and in so doing is distracted from the central process of design. We propose an augmented reality workbench called "Luminous Table" that attempts to address this issue by integrating multiple forms of physical and digital representations. 2D drawings, 3D physical models, and digital simulation are overlaid into a single information space in order to support the urban design process. We describe how the system was used in a graduate design course and discuss how the simultaneous use of physical and digital media allowed for a more holistic design approach. We also discuss the need for future technical improvements.
[Process design, spatial separation, graduate design course, Urban planning, Buildings, Laboratories, Digital simulation, augmented reality, digital simulation, temporal separation, Augmented reality, town and country planning, Analytical models, Luminous Table, Satellites, Physics computing, augmented reality workbench, 3D physical models, Traffic control, 2D drawings, engineering graphics, augmented urban planning workbench]
Single camera tracking of marker clusters: multiparameter cluster optimization and experimental verification
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
We have built a system for augmented reality visualization based on a single head mounted tracking camera. The camera includes an infrared illuminator and works in conjunction with a set of retro-reflective markers that are placed around the workspace. This marker frame configuration delivers excellent pose information, which translates to stable, jitter-free augmentation. In this article, we describe using the same single camera system for tracking relatively small marker clusters, which can be used for tool or instrument tracking. Tracking of such a marker cluster is more susceptible to noise compared to tracking of a marker frame, mainly due to its small image coverage. The sensitivity to noise is studied using Monte Carlo simulations and verified in an experimental setup. We achieved jitter-free augmentation with an optimized cluster design.
[Visualization, optical tracking, augmented reality visualization, Magnetic noise, augmented reality, pose information, Monte Carlo methods, single camera tracking, optimized cluster design, data visualisation, noise, single head mounted tracking camera, marker cluster tracking, retro-reflective markers, Instruments, multiparameter cluster optimization, infrared illuminator, Optical imaging, Educational institutions, stable jitter-free augmentation, Magnetic heads, tool tracking, Augmented reality, instrument tracking, marker frame configuration, Monte Carlo simulations, Cameras, Optical sensors, Optical noise]
MR Platform: a basic body on which mixed reality applications are built
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
This paper describes a platform package, called "MR Platform," which we have been implementing for research and development of augmented reality technology and applications. This package includes a parallax-less stereo video see-through HMD and a software development kit (SDK) for a Linux PC environment. The SDK is composed of a C++ class library for making runtime MR applications and related utilities such as a camera calibration tool. By using the SDK, the following functions are available: capturing video, handling a six degree-of-freedom (DOF) sensor, image processing such as color detection, estimating head position and orientation, displaying the real world image, and calibrating sensor placement and camera parameters of two cameras mounted on the HMD.
[Unix, C++ class library, head position estimation, platform package, Programming, augmented reality, PC environment, Research and development, software libraries, Virtual reality, research and development, MR Platform, calibration, image processing, object-oriented programming, software development kit, helmet mounted displays, C++ language, Application software, Augmented reality, Image sensors, Software packages, video capture, Linux, mixed reality applications, camera calibration tool, six degree-of-freedom sensor, color detection, Packaging, Cameras, programming environments, parallax-less stereo video see-through HMD]
Archeoguide: system architecture of a mobile outdoor augmented reality system
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
We present the system architecture of a mobile outdoor augmented reality system for the Archeoguide project. We begin with a short introduction to the project. Then we present the hardware we chose for the mobile system and we describe the system architecture we designed for the software implementation. We conclude this paper with the first results obtained from experiments we made during our trials at ancient Olympia in Greece.
[image processing, Archeoguide, experiments, augmented reality, Batteries, user interfaces, cultural heritage, Augmented reality, Image reconstruction, wearable computers, software architecture, Software design, mobile computing, ancient Olympia, Wearable computers, system architecture, Computer architecture, Computer graphics, archaeology, Rendering (computer graphics), Cameras, Hardware, mobile outdoor augmented reality system, historical sites]
Model-based visual tracking for outdoor augmented reality applications
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
Outdoor augmented reality (AR) applications rely on hybrid tracking (GPS, digital compass, visual) for registration. RSC has developed a real-time visual tracking system that uses visual cues of buildings in an urban environment for correcting the results of a conventional tracking system. This approach relies on knowledge of a CAD model of the building. It not only provides motion estimation, but also absolute orientation/position. It is based on the "visual servoing" approach, originally developed for robotics tasks. We have demonstrated this approach in real-time at a building on the NRL campus This poster shows the approach and results. The concept can be generalized to any scenario where a CAD model is available. This system is being prepared for integration into the NRL system BARS (Battlefield Augmented Reality System).
[Real time systems, absolute orientation, Image processing, urban environment, optical tracking, augmented reality, Visual servoing, GPS, model-based visual tracking, absolute position, motion estimation, visual servoing, hybrid tracking, Robots, military computing, Minimization methods, military systems, CAD model, Motion estimation, CAD, digital compass, real-time visual tracking system, Battlefield Augmented Reality System, visual cues, Augmented reality, Global Positioning System, outdoor augmented reality applications, real-time systems, buildings, Cameras, registration, Bars]
Hand tracking for interactive pattern-based augmented reality
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
Pattern-based augmented reality systems are considered the most promising approach for accurately registering virtual objects with real-time video feeds. The problem with existing solutions is the lack of robustness to partial occlusions of the pattern, which is important when attempting natural interactions with virtual objects. This paper describes a fast and accurate vision-based pattern tracking system that allows for autocalibrated 3D augmentation of virtual objects onto known planar patterns. The tracking system is shown to be robust to changes in pattern scale, orientation and, most importantly, partial occlusions. A method to detect a hand on top of the pattern is then described, along with a method to render the hand on top of the virtual objects.
[Real time systems, partial occlusion robustness, optical tracking, augmented reality, rendering, interactive pattern-based augmented reality, Augmented reality, hand tracking, virtual object registration, Computer science, vision-based pattern tracking system, planar patterns, Computer displays, gesture recognition, Magnetic separation, Councils, Object detection, autocalibrated 3D augmentations, Rendering (computer graphics), Robustness, Feeds, real-time video feeds, rendering (computer graphics), video signal processing]
A pragmatic approach to augmented reality authoring
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
In this paper we describe the augmented reality (AR) authoring system "PowerSpace" which allows fast and comfortable generation of AR worlds. The system presented uses the functionality of a 2D presentation program (Microsoft PowerPoint) as the basis for the composition of 3D content. An MS PowerPoint export is used to generate an XML-based extensible description of a presentation. This description is enriched by 3D content with the help of an editor, which is also part of the PowerSpace system. The content of this presentation is finally converted into 3D scenes and used in an AR-viewer.
[augmented reality viewer, editor, Authoring systems, Switches, augmented reality, augmented reality authoring, Power system modeling, Augmented reality, Engines, authoring systems, PowerSpace, augmented reality world generation, 3D scenes, Geometry, Graphics, 2D presentation program, Layout, XML-based extensible description, Rendering (computer graphics), Microsoft PowerPoint, Power generation, hypermedia markup languages, 3D content composition]
Alternative tools for tangible interaction: a usability evaluation
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
In this work we compare an in-house designed tangible user interface (TUI) with three alternative single-user tools through an empirical investigation. These three alternative tools are a 3D physical, a 2D cardboard, and a mathematical tool. We expected the 3D physical to perform best, followed by the TUI, the 2D cardboard, and the mathematical tool. A pilot study was first carried out, the results of which were used to design a major experiment. Participants solved the same positioning problem, each using one of the four tools. The mathematical tool was not used in the experiment. In the experiment, trial time, number of user operations, learning effect in both, and user satisfaction were measured. The TUI significantly outperformed the 2D cardboard tool. However, there was no significant difference between the TUI and the 3D physical tool. This justifies the value of researching TUI systems and carrying out usability studies with such systems.
[2D cardboard tool, human factors, augmented reality, tangible interaction, Electrical capacitance tomography, user interfaces, Technology planning, usability evaluation, experiment, input devices, positioning, mathematical tool, interactive devices, 3D physical tool, Virtual environment, direct manipulation, tangible user interface, Educational institutions, Time measurement, Augmented reality, Layout, User interfaces, Problem-solving, Usability, single-user tools, user satisfaction]
Inexpensive non-sensor based augmented reality modeling of curves and surfaces in physical space
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
Previous work in modeling curves and surfaces in augmented reality (AR) space has used expensive sensors such as magnetic sensors. In this work, we propose an augmented reality system where a user can model interesting surfaces with her hands, without expensive sensing systems. The system uses computer vision based methods for tracking the user's head and hand position. Using a glove and the tracking system, the user can draw smooth lines or surfaces with her hands in a physical space. The user can also intuitively modify the lines or surface created by pushing or pulling at the control points of lines or curves in a tangible manner.
[Computer vision, optical tracking, Color, CAD, glove, augmented reality, Sensor systems, Magnetic heads, computer vision based methods, Augmented reality, hand position tracking, Shape control, curves, head position tracking, Magnetic sensors, Fingers, inexpensive nonsensor based augmented reality modeling, surfaces, Virtual reality, computer vision, Automatic control, engineering graphics, data gloves]
Reconstructing while registering: a novel approach for markerless augmented reality
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
This paper addresses the registration problem for unprepared multi-planar scenes. An interactive process is proposed to obtain accurate results using only the texture information of planes. In particular, classical preparation steps (camera calibration, scene acquisition) are greatly simplified, since they are included in the on-line registration process. Results are shown on indoor and outdoor scenes. Videos are available at url http://www.loria.fr//spl tilde/gsimon/Ismar.
[planar texture information, Video sequences, image registration, on-line registration process, camera calibration, interactive process, scene acquisition, augmented reality, Calibration, image reconstruction, Application software, Multimedia communication, Augmented reality, unprepared multi-planar scenes, indoor scenes, Uniform resource locators, Learning systems, Magnetic sensors, Layout, Cameras, outdoor scenes, reconstruction, markerless augmented reality, calibration]
The control unit for a head mounted operating microscope used for augmented reality visualization in computer aided surgery
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
Two main concepts of head mounted displays (HMD) for augmented reality (AR) visualization exist, the optical and video-see through type. Several research groups have pursued both approaches for utilizing HMDs for computer aided surgery. While the hardware requirements for a video see through HMD to achieve acceptable time delay and frame rate seem to be enormous the clinical acceptance of such a device is doubtful from a practical point of view. Starting from previous work in displaying additional computer-generated graphics in operating microscopes, we have adapted a miniature head mounted operating microscope for AR by integrating two very small computer displays. To calibrate the projection parameters of this so called varioscope AR we have used Tsai's (1987) algorithm for camera calibration. Connection to a surgical navigation system was performed by defining an open interface to the control unit of the varioscope AR. The control unit consists of a standard PC with an dual head graphics adapter to render and display the desired augmentation of the scene. We connected this control unit to an computer aided surgery (CAS) system by the TCP/IP interface. In this paper we present the control unit for the HMD and its software design. We tested two different optical tracking systems, the Flash-point (Image Guided Technologies, Boulder, CO), which provided about 10 frames per second, and the Polaris (Northern Digital, Ontario, Can) which provided at least 30 frames per second, both with a time delay of one frame.
[varioscope AR, Visualization, Biomedical optical imaging, optical tracking systems, software design, microcomputer applications, optical tracking, biomedical equipment, augmented reality visualization, Flashpoint, Control systems, augmented reality, head mounted operating microscope, Surgery, data visualisation, computer displays, Computer graphics, open interface, calibration, medical image processing, projection parameter calibration, Delay effects, TCP/IP interface, computer-generated graphics, optical microscopes, helmet mounted displays, Magnetic heads, rendering, time delay, control unit, Augmented reality, Computer displays, computer aided surgery, PC, Microscopy, Polaris, surgical navigation system, head mounted displays, dual head graphics adapter, surgery, biomedical optical imaging]
ARVIKA-augmented reality for development, production and service
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
Augmented reality (AR) is a form of human-machine interaction where information is presented in the field of view of an individual. ARVIKA, funded by the German Ministry of Education and Research, develops this technology and applications in the fields of development, production, and service in the automotive and aerospace industries, for power and processing plants and for machine tools and production machinery. Up to now, AR has only been a subject of individual research projects and a small number of application-specific industrial projects on a global scale. The current state of the art and the available appliances do not yet permit a product-oriented application of the technology. However, AR enables a new, innovative form of human-machine interaction that not only places the individual in the center of the industrial workflow, but also offers a high potential for process and quality improvements in production and process workflows. ARVIKA is primarily designed to implement an augmented reality system for mobile use in industrial applications. The report presents the milestones that have been achieved after a project duration of a full three years.
[Assembly systems, Educational products, production, augmented reality, user interfaces, Home appliances, mobile computing, production machinery, Batch production systems, processing plants, Man machine systems, Automation, development, Vehicle crash testing, Educational technology, ARVIKA, aerospace industries, Augmented reality, power plants, automotive industries, Ergonomics, human-machine interaction, service, machine tools, engineering graphics]
Marker-less tracking for AR: a learning-based approach
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
Estimating the pose of a camera (virtual or real) in which some augmentation takes place is one of the most important parts of an augmented reality (AR) system. The availability of powerful processors and fast frame grabbers has made the use of vision-based trackers commonplace due to their accuracy as well as flexibility and ease of use. Current vision-based trackers are based on tracking of markers. The use of markers increases robustness and reduces computational requirements. However, their use can be very complicated, as they require maintenance. Direct use of scene features for tracking, therefore, is desirable. To this end, we describe a general system that tracks the position and orientation of a camera observing a scene without visual markers. Our method is based on a two-stage process. In the first stage, a set of features is learned with the help of an external tracking system during use. The second stage uses these learned features for camera tracking when the system in the first stage decides that it is possible to do so. The system is very general so that it can employ any available feature tracking and pose estimation system for learning and tracking. We experimentally demonstrate the viability of the method in real-life examples.
[Computer vision, Video sequences, optical tracking, markerless tracking, external tracking system, camera orientation tracking, augmented reality, Application software, scene features, Augmented reality, Computer displays, Ultrasonic imaging, camera position tracking, feature learning, Layout, camera pose estimation, computer vision, Streaming media, Cameras, vision-based trackers, Robustness]
Visual marker detection and decoding in AR systems: a comparative study
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
Visual markers are widely used in existing augmented reality (AR) applications. In most of such applications, the performance of an AR system depends highly on the tracking system for visual marker detection, tracking, and pose estimation. Currently, there are more than one marker based tracking/calibration systems available. It is thus desirable for the user to know which marker tracking system is likely to perform the best for a specific AR application. For this purpose, we compare several marker systems all using planar square coded visual markers. We present the evaluation results, both qualitatively and quantitatively, for the usability, efficiency, accuracy, and reliability. For a particular AR application, there are different marker detection and tracking requirements. Therefore, the purpose of this work is not to rank existing marker systems; instead, we try to analyze the strength and weakness of various aspects of the marker tracking systems and provide AR application developers with this information.
[Performance evaluation, Thyristors, visual marker decoding, efficiency, Tracking, Military computing, planar square coded visual markers, Computer aided manufacturing, optical tracking, reliability, augmented reality, accuracy, Decoding, Calibration, visual marker pose estimation, Augmented reality, Computer displays, usability, feature extraction, Cameras, visual marker detection]
3D live: real time captured content for mixed reality
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
We present a complete system for live capture of 3D content and simultaneous presentation in augmented reality. The user sees the real world from his viewpoint, but modified so that the image of a remote collaborator is rendered into the scene. Fifteen cameras surround the collaborator, and the resulting video streams are used to construct a three-dimensional model of the subject using a shape-from-silhouette algorithm. Users view a two-dimensional fiducial marker using a video-see-through augmented reality interface. The geometric relationship between the marker and head-mounted camera is calculated, and the equivalent view of the subject is computed and drawn into the scene. Our system can generate 384 /spl times/ 288 pixel images of the models at 25 fps, with a latency of < 100 ms. The result gives the strong impression that the subject is a real part of the 3D scene. We demonstrate applications of this system in 3D videoconferencing and entertainment.
[3D entertainment, augmented reality, real time captured content, teleconferencing, Delay, cameras, geometric relationship, Virtual reality, simultaneous presentation, rendering (computer graphics), video signal processing, 3D videoconferencing, 3D model, helmet mounted displays, 2D fiducial marker, live 3D content capture, Augmented reality, remote collaborator image rendering, Teleconferencing, Layout, mixed reality, Collaboration, Streaming media, shape-from-silhouette algorithm, Cameras, Rendering (computer graphics), video streams, video-see-through augmented reality interface, head-mounted camera, Pixel]
Tangible bits: designing the seamless interface between people, bits, and atoms
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
false
[Human computer interaction, Art, USA Councils, Laboratories, Collaborative work, Augmented reality]
Augmented-reality visualizations guided by cognition: perceptual heuristics for combining visible and obscured information
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
One unique feature of mixed and augmented reality (MR/AR) systems is that hidden and occluded objects an be readily visualized. We call this specialized use of MR/AR, obscured information visualization (OIV). In this paper, we describe the beginning of a research program designed to develop such visualizations through the use of principles derived from perceptual psychology and cognitive science. In this paper we surveyed the cognitive science literature as it applies to such visualization tasks, described experimental questions derived from these cognitive principles, and generated general guidelines that can be used in designing future OIV systems (as well improving AR displays more generally). We also report the results from an experiment that utilized a functioning AR-OIV system: we found that in relative depth judgment, subjects reported rendered objects as being in front of real-world objects, except when additional occlusion and motion cues were presented together.
[Visualization, cognition-guided augmented reality visualizations, hidden object visualization, Laboratories, Psychology, Displays, augmented reality, Guidelines, psychology, data visualisation, Virtual reality, motion cues, Cognitive science, visual perception, visible/obscured information combination, Application software, Augmented reality, hidden feature removal, relative depth judgment, perceptual heuristics, perceptual psychology, rendered objects, mixed reality, Rendering (computer graphics), obscured information visualization, occluded object visualization, cognitive science]
Communication behaviors of co-located users in collaborative AR interfaces
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
We conducted two experiments comparing communication behaviors of co-located users in collaborative augmented reality (AR) interfaces. In the first experiment, we compared optical, stereo- and mono-video, and immersive head mounted displays (HMDs) using a target identification task. It was found that differences in the real world visibility severely affect communication behaviors. The optical see-through case produced the best results with the least extra communication needed. Generally, the more difficult it was to use non-verbal communication cues, the more people resorted to speech cues to compensate. In the second experiment, we compared three different combinations of task and communication spaces using a 2D icon design task with optical see-through HMDs. It was found that the spatial relationship between the task and communication spaces also severely affected communication behaviors. Placing the task space between the subjects produced the most active behaviors in terms of initiatory body languages and utterances with least miscommunications.
[task spaces, initiatory body languages, Displays, augmented reality, user interfaces, target identification task, Videoconference, stereo-video head mounted displays, Visual communication, immersive head mounted displays, Space technology, nonverbal communication cues, groupware, real world visibility, interactive devices, 2D icon design task, video signal processing, spatial relationship, communication behaviors, communication spaces, utterances, Video sharing, helmet mounted displays, Time measurement, Augmented reality, collaborative augmented reality interfaces, mono-video head mounted displays, optical head mounted displays, optical see-through case, Collaboration, stereo image processing, speech cues, Collaborative work, Speech, co-located users]
Geometric and photometric registration for real-time augmented reality
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
This paper proposes an augmented reality system with correct representation of shading and shadow. To realize a seamless augmented reality system, we need to resolve certain problems. The geometric and photometric registration problems are particularly important. These problems require the position of light sources and user's viewpoint. The proposed system resolves these problems using a 3D marker which combines a 2D square marker and a mirror ball. The 2D marker and the ball are used to estimate the relationship between the real and virtual worlds and the positions of light sources in the real world, respectively.
[shading, shadow, image registration, computational geometry, augmented reality, photometry, photometric registration, mirror ball, 2D square marker, Light sources, light sources, 3D marker, Information science, Lighting, Mirrors, real/virtual world relationship, Photometry, Virtual environment, Augmented reality, Layout, real-time systems, light source position, user viewpoint, Cameras, Rendering (computer graphics), geometric registration, real-time augmented reality]
Practical solutions for calibration of optical see-through devices
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
Registration is a crucial task in a see-through augmented reality (AR) system. The importance stems not only from the fact that registration requires careful calibration but also from the necessity that any calibration procedure should take users into account. Tuceryan et al. (2002) proposed a general method for calibrating a see-through device based on dynamic alignment of virtual and real points. Although a powerful tool, our experiments showed that users find alignment of many points overwhelming. We introduce improvements to simplify the calibration process and increase the success rate. We first identified why calibration parameters differ from user to user and how this can be prevented by adopting particular configurations for the tracker sensor and display. This allowed us to re-use the existing calibrations. Furthermore, we have introduced a simpler model for the calibration that requires fewer user inputs, typically four, to calibrate the system.
[Computer vision, optical see-through devices, image registration, optical tracking, display, augmented reality, dynamic virtual point alignment, Calibration, Active appearance model, dynamic real point alignment, Optical devices, Augmented reality, see-through augmented reality system, Computer displays, Lighting, Virtual reality, computer vision, Computer errors, Cameras, registration, tracker sensor, calibration]
A flexible tracking concept applied to medical scenarios using an AR window
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
This paper presents an approach to use a semitransparent display as a kind of window into a patient in the context of medical augmented reality (AR) applications. Besides the presentation of the non-off-the-shelf display, the tracking aspects of such an application are the focus of the work presented. In order to allow augmentations of real objects by virtual ones on the display, the user (i.e. physician), the display, the object (i.e. patient) and optional instruments have to be tracked. If required, a tracking system consisting of more than one subsystem, e.g. optical tracking combined with electromagnetic tracking, is used to satisfy all the needs of such a medical application.
[medical augmented reality applications, image processing, Biomedical equipment, computer display, Biomedical optical imaging, Head, Instruments, electromagnetic tracking, optical tracking, Medical services, Holography, augmented reality, user interfaces, tracking, Augmented reality, flexible tracking concept, computer displays, semitransparent display, Liquid crystal displays, Flat panel displays, medical image processing, Biomedical imaging]
Exploring humanistic intelligence through physiologically mediated reality
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
We present a way of making the wearing of a lifelong electrocardiographic health monitor fun for a user. The health monitor is coupled with a reality mediator device to create physiologically mediated reality, i.e. mediated reality which alters a user's audiovisual perception of the world based upon their own electrocardiographic waveform. This creates an interesting audiovisual experience for the user, playing upon the poetic narrative of combining cardio-centric metaphors pervasive in everyday life (the heart as a symbol of love and centrality, e.g. "get to the heart of the matter") with ubiquitous occular-centric metaphors such as "see the world from my point of view". This audiovisual experience is further enhanced by combining music which alters the visual perception and also heightens the user's emotional response to their experience and, in doing so, further affects their heart(beat).
[virtual reality, lifelong electrocardiographic health monitor, spectral analysis, emotional response, patient monitoring, Visual perception, electrocardiography, physiologically mediated reality, Heart beat, music, Electrocardiography, Cardiology, occular-centric metaphors, video signal processing, cardio-centric metaphors, Low pass filters, audio-visual systems, audiovisual experience, physiology, Educational institutions, Spectral analysis, medical signal processing, reality mediator device, humanistic intelligence, Frequency, audiovisual perception, electrocardiographic waveform, Biomedical monitoring, Bars, poetic narrative]
Interactive multi-marker calibration for augmented reality applications
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
Industrial augmented reality (AR) applications require fast, robust, and precise tracking. In environments where conventional high-end tracking systems cannot be applied for certain reasons, marker-based tracking can be used with success as a substitute if care is taken about (1) calibration and (2) run-time tracking fidelity. In out-of-the-laboratory environments multi-marker tracking is needed because the pose estimated from a single marker is not stable enough. The overall pose estimation can be dramatically improved by fusing information from several markers fixed relative to each other compared to a single marker only. To achieve results applicable in an industrial context relative marker poses need to be properly calibrated. We propose a semiautomatic image-based calibration method requiring only minimal interaction within the workflow. Our method can be used off-line, or preferably incrementally online. When used online, our method shows reasonably good accuracy and convergence with workflow interruption of less than one second per incremental step. Thus, it can be interactively used. We illustrate our method with an industrial application scenario.
[Airplanes, Instruments, Computational fluid dynamics, convergence, optical tracking, augmented reality, accuracy, Magnetic heads, Calibration, run-time tracking fidelity, Augmented reality, Manufacturing industries, marker-based tracking, workflow interruption, Virtual reality, computer vision, pose estimation, Cameras, semiautomatic image-based calibration, Robustness, calibration, interactive multi-marker calibration]
The use of dense stereo range data in augmented reality
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
This paper describes an augmented reality system that incorporates a real-time dense stereo vision system. Analysis of range and intensity data is used to perform two functions: 1) 3D detection and tracking of the user's fingertip or a pen to provide natural 3D pointing gestures, and 2) computation of the 3D position and orientation of the user's viewpoint without the need for fiducial mark calibration procedures, or manual initialization. The paper describes the stereo depth camera, the algorithms developed for pointer tracking and camera pose tracking, and demonstrates their use within an application in the field of oil and gas exploration.
[Real time systems, algorithms, oil exploration, camera pose tracking, Laboratories, 3D orientation, Humans, optical tracking, augmented reality, 3D position, dense stereo range data, oil technology, gesture recognition, natural 3D pointing gestures, distance measurement, augmented reality system, pointer tracking, user fingertip, pen, Hardware, Stereo vision, natural gas technology, 3D tracking, Target tracking, pointing systems, Calibration, Augmented reality, Petroleum, 3D detection, stereo depth camera, real-time dense stereo vision system, real-time systems, user viewpoint, stereo image processing, Cameras, gas exploration]
A concept for the application of augmented reality in manual gas metal arc welding
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
The problem of creating manual welds of constant high quality results from missing optical information during the actual welding process. Due to the extreme brightness conditions in arc welding and the use of protective glasses, even experienced welders can hardly recognize details of the welding process and the environment. This paper describes a new research project for the development of a support system for the welder.
[image processing, manual gas metal arc welding, Portable computers, Welding, Brightness, protective glasses, head-mounted display, Glass, production engineering computing, augmented reality, helmet mounted displays, user interfaces, Data mining, extreme brightness conditions, Augmented reality, Image quality, Computer displays, arc welding, Cameras, Protection, missing optical information]
A study for image-based integrated virtual environment
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
In this paper, we propose a new approach to building a virtual environment by using raw data from a real environment. To manage and analyze different types of datasets from many sensors and databases, we build and integrate different types of environments with different types of datasets, user interactions and rendering methods. To integrate the environments, we merge rendered images of each virtual environment according to the user's viewpoint, depth information, and reliability of rendered images, and display the resultant image to the user. In addition, we propose an integration environment by using an environment manager, which determines the user's viewpoint with respect to the integrated environments according to the user's input, and compose a display image from rendered images of the integrant environments. The reliability of the rendered images is determined by data accuracy and resolution, display resolution and rendering errors from rendering methodologies. By integrating four constructed environments according to the user's viewpoint, we can construct a photorealistic large-scale environment from different types of datasets. Furthermore, we implemented the prototype system in CABIN and demonstrated the scalability of this architecture.
[Solid modeling, virtual reality, data resolution, CABIN, Displays, scalability, rendering errors, Prototypes, Virtual reality, display resolution, user interactions, rendering (computer graphics), data accuracy, image processing, databases, Image resolution, Virtual environment, rendered image reliability, Buildings, image-based integrated virtual environment, depth information, Power system modeling, environment manager, sensors, photorealistic large-scale environment, Data visualization, user viewpoint, Rendering (computer graphics), rendering methods, display image]
Diminishing head-mounted display for shared mixed reality
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
We propose a new scheme to recover the eye-contact between multiple users in a shared mixed-reality space. The eye-contact in a shared mixed-reality space is lost as the side effect of wearing head-mounted displays. We synthesize facial images in real-time with arbitrary poses and eye expressions by using several photographs of the user. The face images are overlaid in order to diminish the HMD in his partner's view for the recovery of eye-contact. The basic idea, facial image synthesis, and an experimental system to diminish HMD are presented in this paper.
[shared mixed reality, real-time facial image synthesis, Head, virtual reality, Shape, Merging, diminishing head-mounted display, Displays, helmet mounted displays, user photographs, Graphics, multiple users, eye contact recovery, Space technology, arbitrary poses, Virtual reality, Image generation, computer vision, Rendering (computer graphics), Systems engineering and theory, arbitrary eye expressions]
Experimental evaluation of augmented reality in object assembly task
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
This study evaluated the effectiveness of spatially overlaid instructions using augmented reality (AR) in an assembly task compared with other traditional media. Results indicate that overlaying 3D instructions on the workspace reduce error rate by 82%, particularly cumulative errors. Measurement of mental effort also suggests some of the mental workload is offloaded to the computer.
[Computer interfaces, spatially overlaid instructions, Error analysis, Humans, human factors, augmented reality, training, user interfaces, experimental evaluation, 3D instructions, computer based training, head mounted display, Assembly, image processing, NASA, mental workload, assembling, helmet mounted displays, Time measurement, Calibration, object assembly task, Augmented reality, Computer aided instruction, error rate, Computer errors, human computer interaction]
Testable design representations for mobile augmented reality authoring
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
This paper applies the idea of a continuously testable design representation to authoring of augmented realities for mobile devices.
[mobile augmented reality authoring, prototyping, testable design representations, augmented reality, user interfaces, Augmented reality, authoring systems, Wireless communication, mobile computing, Prototypes, Automata, mobile devices, Communications technology, Hardware, Iterative methods, Personal digital assistants, Mobile computing, Testing]
Seeing eye to eye: a shared mediated reality using EyeTap devices and the VideoOrbits gyroscopic head tracker
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
We present a system which allows wearable computer users to share their views of their current environments with each other. Our system uses an EyeTap: a device which allows the eye of the wearer to function both as a camera and a display. A wearer, by looking around his/her environment, "paints" or "builds" an environment map composed of images from the EyeTap device, along with head-tracking information recording the orientation of each image. The head-tracking algorithm uses a featureless image motion estimation algorithm coupled with a head mounted gyroscope. The environment map is then transmitted to another user, who, through their own head-tracking EyeTap system, browses the first user's environment solely by head motion, seeing the environment as though it were their own. As a result of browsing the transmitted environment map, the viewer builds and extends his/her own environment map, and thus this is a data-producing head-tracking system. These environment maps can then be shared reciprocally between wearers.
[head-tracking algorithm, augmented reality, user interfaces, tracking, head mounted gyroscope, Wearable computers, motion estimation, Gyroscopes, environment map, camera, wearable computer, Computer vision, gyroscopic head tracker, browsing, Video sharing, Motion estimation, featureless image motion estimation, Educational institutions, helmet mounted displays, Magnetic heads, gyroscopes, shared mediated reality, wearable computers, eye, Computer displays, Image databases, EyeTap devices, VideoOrbits, Cameras]
Augmented Chemistry: an interactive educational workbench
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
We report on some of the advantages tangible interaction can bring to chemistry education. We describe the realisation of a tangible user interface (TUI) called Augmented Chemistry (AC). A set of interactive tools work within this system. Using these tools, elements can be chosen from a booklet menu and composed into 3D molecular models. The tools are one way towards seamless integration of the physical and digital realms. Since multiple tools can be used concurrently, single and multiple users can use the system at a time. To use the system in an educational context, it was extended into an educational workbench drawing on haptic and aural augmentation. The design and implementation of our system required contributions from optics, mathematics, molecular chemistry, software engineering, and 3D programming, making it a truly interdisciplinary project. Future challenges lie in user acceptance, educational effect, and further system development.
[image processing, 3D molecular models, aural augmentation, Engineering drawings, Augmented Chemistry, tangible user interface, augmented reality, Mathematics, interactive educational workbench, Haptic interfaces, user interfaces, Programming profession, Chemistry, Optical design, booklet menu, chemistry computing, User interfaces, chemistry education, haptic augmentation, computer aided instruction, Chemical elements, user acceptance, Software engineering, Mathematical programming]
Bread Crumbs: a technique for modelling large outdoor ground features
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
This paper presents a new technique we have created, known as Bread Crumbs, which allows the modelling of large outdoor ground features using a mobile augmented reality system and a user's physical presence. Using this technique, we demonstrate an example of modelling a large grassy area on campus. This technique is one component of a larger set of tools which allow users to capture and work with complex outdoor geometry interactively, and to view this geometry in new and interesting ways.
[Solid modeling, complex outdoor geometry, Control systems, augmented reality, large outdoor ground feature modelling, Augmented reality, mobile augmented reality system, Geometry, mobile computing, user physical presence, Wearable computers, Fingers, Lakes, User interfaces, Cameras, Bread Crumbs, on-campus large grassy area, Mobile computing]
Stereo augmentation of simulation results on a projection wall by combining two basic ARVIKA systems
Proceedings. International Symposium on Mixed and Augmented Reality
None
2002
The article describes the potential for cost-reduction by using augmented reality (AR) in the automotive industry. AR allows for the evaluation of computer-generated simulations with physically crashed cars. The augmented simulation results are displayed on a projection wall in stereo by simply combining two basic ARVIKA systems.
[cost reduction, Visualization, computer generated simulation evaluation, automotive industry, Computational modeling, Computer simulation, automobiles, Vehicle crash testing, augmented reality, Computer crashes, digital simulation, Augmented reality, stereo augmentation, Automotive engineering, physically crashed cars, ARVIKA systems, Virtual reality, stereo image processing, Animation, projection wall, engineering graphics, Cams, video signal processing]
Marker-less vision based tracking for mobile augmented reality
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
In this paper an object recognition and tracking approach for the mobile, marker-less and PDA-based augmented reality system AR-PDA is described. For object recognition and localization 2D features are extracted from images and compared with a priori known 3D models. The system consists of a 2D graph matching, 3D hypothesis generation and validation and an additional texture based validation step.
[object recognition, Shape, 3D hypothesis generation, localization 2D features, augmented reality, object detection, marker-less augmented reality, tracking approach, mobile computing, Computer graphics, Production, notebook computers, mobile augmented reality, Intelligent systems, rendering (computer graphics), Object recognition, image matching, Augmented reality, Layout, AR-PDA, image extraction, markerless vision based tracking, PDA-based augmented reality, computer vision, Cameras, Feature extraction, texture based validation step, 3D models, Mobile computing, 2D graph matching]
Displaying digital documents on real paper surface with arbitrary shape
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
In this paper, we propose a system that displays digital components on real paper surface with arbitrary shape, so that the viewer can feel as if the digital document images are printed on the real paper surface. Such displaying of the digital documents is realized by rendering the document images on the arbitrary shaped surface via a projector. We apply a holography between a source image plane and a projector image plane to render the images on the surface. For adapting the arbitrary shape of the surface, we divide the shaped surface into many small rectangular regions, and generate warp images of each region by calculating this holography of the plane of each divided region. By protecting the warp image on the real surface, the image can be observed as if the image is printed on the surface. Since the system always compute the holography of each divided region, the image can be aligned onto the surface even the surface moves.
[Shape, digital components, paper surface, warp images, real surface, image segmentation, projector, arbitrary shape, Books, image rendering, rendering (computer graphics), rectangular regions, document image processing, Cultural differences, Computer science, image plane, Computer displays, digital documents, Image generation, Cathode ray tubes, Rendering (computer graphics), Cameras, holography, Liquid crystal displays, digital images]
An object-oriented software architecture for 3D mixed reality applications
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
This paper presents a new software architecture for 3D mixed reality applications, named Tinmith-evo5. Currently there are a limited number of existing toolkits for the development of 3D mixed reality applications, each optimized for particular feature but at the detriment of others. Complex interactive user interfaces and applications require extensive supporting infrastructure, and can be hampered by inadequate support. The Tinmith-evo5 architecture is optimised to develop mobile augmented reality and other interactive 3D applications on portable platforms with limited resources. This architecture is implemented in C++ with an object-oriented data flow design, an object store based on the Unix file system model, and uses other ideas from existing previous work.
[Unix, Tinmith-evo5, object-oriented software, Programming, augmented reality, user interfaces, portable platforms, software architecture, optimisation, Software architecture, Wearable computers, interactive user interfaces, Virtual reality, Computer architecture, 3D mixed reality applications, object-oriented data flow, Hardware, software tools, file system model, mobile augmented reality, data flow design, interactive 3D applications, C++ language, Application software, Augmented reality, data flow computing, User interfaces, infrastructure support, Rendering (computer graphics), software development tools, Unix file system]
Implementation of an augmented reality system on a PDA
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
We present a client/server implementation for running demanding mobile AR application on a PDA device. The system incorporates various data compression methods to make it run as fast as possible on a wide range of communication networks, from GSM to WLAN.
[Wireless LAN, Displays, augmented reality, PDA, Image coding, mobile computing, Software architecture, augmented reality system, handheld displays, Hardware, notebook computers, camera, GSM, client-server systems, data compression, communication networks, client-server implementation, WLAN, Decoding, Augmented reality, personal digital assistant, Cameras, Rendering (computer graphics), wireless LAN, mobile AR application]
Authoring of a mixed reality assembly instructor for hierarchical structures
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
Mixed reality is a very useful and powerful instrument for the visualization of processes, including the assembly process. A Mixed Reality based step-by-step furniture assembly application is introduced. On the one hand context related actions are given to the user to install elements. On the other hand an intuitive way for authors to create new MR based assembly instructions is provided. Our goal is to provide a powerful, flexible and easy-to-use authoring wizard for assembly experts, allowing them to author their new assembly instructor for hierarchical structures. This minimizes the costs for the creation of new mixed reality assembly instructors.
[Visualization, Assembly systems, Costs, Switches, Manuals, authoring tool, assembly experts, augmented reality, 2D representation program, user interfaces, hierarchical structures, furniture assembly application, authoring systems, Industrial training, assembly instructor, Virtual reality, data structures, hypermedia markup languages, Instruments, Computerized monitoring, authoring wizard, assembly process, Augmented reality, process visualization, MR based assembly instructions, mixed reality, computer aided instruction]
Consistent illumination within optical see-through augmented environments
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
We present techniques which create a consistent illumination between real and virtual objects inside an application specific optical see-through display: the virtual showcase. We use projectors and cameras to capture reflectance information from diffuse real objects and to illuminate them under new synthetic lighting conditions. Matching direct and indirect lighting effects, such as shading, shadows, reflections and color bleeding can be approximated at interactive rates in such a controlled mixed environment.
[shading, reflections, Displays, augmented reality, augmented environments, controlled mixed environment, optical see-through environments, Lighting, computer displays, lighting effects, diffuse real objects, Hardware, image colour analysis, virtual objects, color bleeding, Optical reflection, rendering (computer graphics), object-oriented programming, reflectance information, illumination, three-dimensional displays, optical see-through display, lighting, Augmented reality, hidden feature removal, Reflectivity, virtual showcase, synthetic lighting conditions, Cameras, Rendering (computer graphics), shadows, Acceleration, Hemorrhaging, interactive rates]
User interaction in mixed reality interactive storytelling
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
In this paper, we describe a mixed reality system based on a "magic mirror" model, in which the user's image is captured in real time by a video camera, extracted from his/her background and mixed with a 3D graphic model of a virtual image including the synthetic characters taking part in the story. The resulting image is projected on a large screen facing the user, who sees his/her own image embedded in the virtual stage with the synthetic actors. The graphic component of the mixed reality world is based on a game engine, Unreal Tournament 2003. This engine not only performs graphic rendering and character animation but incorporates a new version of our previously described storytelling engine. A single 2D camera facing the user analyses the image in real-time by segmenting the user's contours.
[Real time systems, game engine, graphic rendering, entertainment, interactive storytelling, user image, augmented reality, user interaction, character animation, user interfaces, Engines, cameras, computer animation, Virtual reality, edge detection, Mirrors, rendering (computer graphics), synthetic characters, real time, Unreal Tournament 2003, synthetic actors, video camera, virtual stage, 3D graphic model, Graphics, Image segmentation, Image analysis, mixed reality, storytelling engine, virtual image, Cameras, Rendering (computer graphics), Animation, user contour, graphic component, magic mirror model]
Industrial augmented reality (IAR): challenges in design and commercialization of killer apps
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
false
[Wireless communication, Thyristors, Communication industry, Computer industry, Data engineering, Production facilities, Augmented reality, Commercialization, Mobile computing, Image reconstruction]
Results of a study on software architectures for augmented reality systems
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
Research prototypes in AR usually do not emphasize software architecture. Nevertheless, their architecture is not arbitrary, but results from specific needs. Architectural approaches embodying research contributions are of particular value for reuse at component and architectural levels. We have conducted a study of existing AR software architectures for the ARVIKA project by W. Friedrich et al (2001). This has resulted in a catalog of important desired quality attributes for AR systems, a reference architecture for comparison of AR architectures, and a catalog of architectural approaches used in current AR systems. We believe this lays the foundation for further research in AR software architectures.
[Software prototyping, object-oriented programming, software prototyping, research prototypes, software architectures, augmented reality, component levels, Wire, ARVIKA, research contributions, Augmented reality, Information systems, software architecture, Image analysis, augmented reality systems, architectural approaches, Software architecture, quality attributes, Computer architecture, architectural levels, AR systems, Assembly, Context modeling, Bars]
ID CAM: a smart camera for scene capturing and ID recognition
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
An ID recognition system is described that uses optical beacons and a high-speed image sensor. The ID sensor captures a scene like an ordinary camera and recognizes the ID of a beacon emitted over a long distance. The ID recognition system has three features. The system is robust to changes in the optical environment, e.g. complete darkness, spotlights, and sunlight. It can recognize up to 255 multiple optical beacons simultaneously. Furthermore, it can recognize beacons even over a long distance, e.g. 40 m indoors and 20 m outdoors. Implementation and evaluation of this ID recognition system showed that a mobile augmented reality system can be achieved by combining this ID recognition system with a PDA and a wireless network.
[CADCAM, Image recognition, augmented reality, PDA, scene capturing, mobile computing, Smart cameras, notebook computers, camera, mobile augmented reality, wireless network, Computer aided manufacturing, video cameras, image sensor, optical environment, Intelligent sensors, complete darkness, Image sensors, ID recognition, sunlight, Stimulated emission, Layout, ID CAM, optical beacons, spotlights, Optical sensors, wireless LAN, High speed optical techniques, image recognition]
Evaluating label placement for augmented reality view management
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
View management, a relatively new area of research in Augmented Reality (AR) applications, is about the spatial layout of 2D virtual annotations in the view plane. This paper represents the first study in an actual AR application of a specific view management task: evaluating the placement of 2D virtual labels that identify information about real counterparts. Here, we objectively evaluated four different placement algorithms, including a novel algorithm for placement based on identifying existing clusters. The evaluation included both a statistical analysis of traditional metrics (e.g. counting overlaps) and an empirical user study guided by principles from human cognition. The numerical analysis of the three real-time algorithms revealed that our new cluster-based method recorded the best average placement accuracy while requiring only relatively moderate computation time. Measures of objective readability from the user study demonstrated that in practice, human subjects were able to read labels fastest with the algorithms that most quickly prevented overlap, even if placement wasn't ideal.
[Laboratories, empirical user study, Humans, augmented reality, Cognition, 2D virtual annotations, placement accuracy, computation time, spatial layout, Clustering algorithms, Virtual reality, algorithm theory, numerical analysis, label placement, view plane, Testing, Statistical analysis, cognition, Two dimensional displays, 2D virtual labels, counting overlaps, human cognition, Augmented reality, three real-time algorithms, cluster-based method, view management, Numerical analysis, pattern clustering, clusters identification, statistical analysis, computational complexity]
Immersive evaluation of virtualized soccer match at real stadium model
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
This paper presents a novel observation system for immersive soccer match taken by multiple video cameras at a real stadium. The user sees the soccer field model in front of his/her eyes from the viewpoint through head-mounted display, while the images of players and a soccer ball are also rendered onto the display. For geometric registration between the soccer field model in the real world and the dynamic soccer scene in the rendered images, the viewpoint position of the user is computed by using only natural feature lines in the HMD camera image. Since it is difficult to strongly calibrate the HMD camera and the multiple cameras that capture the real soccer scene, we employ projective geometry for the registration and the rendering. For demonstrating the efficacy of the proposed system, video images of soccer matches taken at real stadium are rendered onto the HMD camera images of a tabletop stadium model. This is a completely new challenge to apply augmented reality to a dynamic event in a large-space.
[images of players, multiple cameras, image registration, Watches, immersive evaluation, HMD camera, Displays, augmented reality, projective geometry, tabletop stadium model, real stadium model, observation system, dynamic soccer scene, Image reconstruction, virtualized soccer match, TV broadcasting, real world, feature extraction, multiple video cameras, Virtual reality, camera image, head mounted display, rendering (computer graphics), calibration, video images, natural feature lines, video cameras, three-dimensional displays, helmet mounted displays, rendering, Augmented reality, rendered images, soccer field model, soccer matches, Layout, Character generation, computer vision, soccer ball, Cameras, Rendering (computer graphics), geometric registration, sport]
A LCD cube transporting high dynamic range light environments
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
In the MR world, it needs to overlay virtual objects onto real scene with harmonizing the photometric consistency. We developed a new illumination device "LCD cube" for keeping photometric consistency. In the demonstration, we plan to present the method to display high dynamic range light environments by "LCD cube" for image-based lighting. In addition, we performed light reproduction in real-time.
[Charge coupled devices, virtual object overlay, illumination device, liquid crystal displays, Dynamic range, light reproduction, augmented reality, lighting, Light sources, Degradation, computer graphics, Layout, mixed reality, Lighting, photometric consistency, image-based lighting, Liquid crystal displays, light environments, Mirrors, Photometry, LCD cube, Charge-coupled image sensors]
Interactive mediated reality
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
Mediated reality describes the concept of filtering or vision of reality, typically using a head-worn video mixing display. In this paper, we propose a generalized concept and new tools for interactively mediated reality. We present also our first prototype system for painting, grabbing and gluing together real and virtual elements.
[Brushes, head-worn display, Video sharing, graphical user interfaces, video mixing display, Displays, augmented reality, helmet mounted displays, reality vision, Information filtering, interactive mediated reality, Augmented reality, Painting, Geometry, glue tool, computer graphics, painting tool, grab tool, prototype system, Computer graphics, Information filters, human computer interaction, reality filtering, Virtual prototyping]
A real-time tracker for markerless augmented reality
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
Augmented reality has now progressed to the point where real-time applications are required and being considered. At the same time it is important that synthetic elements are rendered and aligned in the scene in an accurate and visually acceptable way. In order to address these issues a real-time, robust and efficient 3D model-based tracking algorithm is proposed for a 'video see through' monocular vision system. The tracking of objects in the scene amounts to calculating the pose between the camera and the objects. Virtual objects can then be projected into the scene using the pose. Here, non-linear pose computation is formulated by means of a virtual visual servoing approach. In this context, the derivation of point-to-curve interaction matrices is given for different features including lines, circles, cylinders and spheres. A local moving edge tracker is used in order to provide real-time tracking of points normal to the object contours. A method is proposed for combining local position uncertainty and global pose uncertainty in an efficient and accurate way by propagating uncertainty. Robustness is obtained by integrating an M-estimator into the visual control law via an iteratively re-weighted least squares implementation. The method presented in this paper has been validated on several complex image sequences including outdoor environments. Results show the method to be robust to occlusion, changes in illumination and mistracking.
[Real time systems, tracking algorithm, object contours, Uncertainty, optical tracking, point-to-curve interaction, augmented reality, Visual servoing, synthetic elements, least squares implementation, servomechanisms, real-time applications, visual control law, Robustness, markerless augmented reality, edge detection, virtual objects, nonlinear pose computation, local position uncertainty, rendering (computer graphics), image sequences, least squares approximations, global pose uncertainty, 3D model, complex image sequences, Augmented reality, Least squares methods, uncertainty propagation, outdoor environments, Robust control, real-time tracker, interaction matrices, Machine vision, monocular vision system, Layout, occlusion, computer vision, moving edge tracker, stereo image processing, Cameras, virtual visual serving approach]
Stereo depth assessment experiment for microscope-based surgery
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
We present experimental data on the use of autostereoscopic displays as complementary visualization aids to the surgical stereo microscope for augmented reality surgery. Five experts in the use of the microscope, and five non-experts, performed a depth experiment to assess stereo cues as provided by two autostereoscopic displays (DTI 2015XLS Virtual Window and SHARP micro-optic twin), the surgical microscope and the "naked" eye.
[Visualization, Biomedical optical imaging, surgical stereo microscope, Control systems, augmented reality, autostereoscopic displays, surgical microscope, Optical microscopy, Surgery, computer displays, naked eye, microscope-based surgery, stereo depth assessment, optical microscopes, DTI 2015XLS Virtual Window, Augmented reality, SHARP microoptic twin, Diffusion tensor imaging, Computer displays, stereo cues, stereo image processing, Optical crosstalk, Liquid crystal displays, augmented reality surgery, medical computing, experimental data, visualization aids, surgery]
Augmented reality kanji learning
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
ARToolKit programmers are familiar with the kanji symbols supplied with the distribution. Most of them do not know what these kanji symbols mean. We propose a piece of educational software that uses collaborative augmented reality (AR) to teach users the meaning of kanji symbols. The application is laid out as a two player augmented reality computer game. The novelty of our approach is that we do not use regular workstations or laptops to host the AR (augmented reality) application. Instead we use fully autonomous PDAs, running the application together with an optical marker-based tracking module that makes this application not only available for a broad audience but also optimally mobile.
[AR application, Portable computers, entertainment, augmented reality, ARToolKit programmers, collaborative AR, PDA, optical marker-based tracking, computer games, Libraries, Workstations, notebook computers, Personal digital assistants, courseware, laptops, kanji symbols, Collaborative software, kanji learning, tracking module, Application software, Augmented reality, Programming profession, workstations, broad audience, educational software, AR computer game, Cameras, Collaborative work]
Evaluation of calibration procedures for optical see-through head-mounted displays
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
Optical see-through head-mounted displays (HMDs) are less commonly used because they are difficult to accurately calibrate. In this article, we report a user study to compare the accuracy of 4 variants of the SPAAM calibration method. Among the 4 variants, Stylus-marker calibration, where the user aligns a crosshair projected in the HMD with a tracked stylus tip, achieved the most accurate result. A decomposition and analysis of the calibration matrices from the trials is performed and the characteristics of the computed calibration matrices are examined. A physiological engineering point of view is also discussed to explain why calibrating optical see-through HMD is so difficult for users.
[Tracking, optical see-through displays, Laboratories, optical tracking, human factors, augmented reality, decomposition, Stylus-marker calibration, physiological engineering, Computer graphics, tracked stylus tip, Mirrors, Head, head-mounted displays, user study, SPAAM calibration method, helmet mounted displays, calibration matrices, Calibration, Matrix decomposition, Augmented reality, Computer displays, computer vision, Cameras, crosshair alignment]
A coded visual marker for video tracking system based on structured image analysis
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
This paper proposes a new kind of visual marker for a wearable computer based augmented reality (AR) system. Various marker systems have already been developed for using with an AR system. However, the improvement is needed to use these marker systems on a wearable AR system in terms of the amount of information or the width of the recognizable area. The main feature of our visual marker is a large amount of information that it can possess. Our marker consists of 32 bits data area with error detection code. This feature enables a wearable AR system to be used in various ways.
[Thyristors, recognition algorithm, error detection code, optical tracking, augmented reality, Information science, Wearable computers, Space technology, data area, structured image analysis, video tracking system, marker system, Parity check codes, AR system, edge detection, image colour analysis, coded visual marker, Labeling, wearable computer, data compression, video coding, Augmented reality, wearable computers, Image analysis, Computer errors, Pixel]
The CREATE project: mixed reality for design, education, and cultural heritage with a constructivist approach
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
The global scope of the CREATE project is to develop a mixed-reality framework that enables highly interactive real-time construction and manipulation of photo-realistic, virtual worlds based on real data sources. This framework will be tested and applied to cultural heritage content in an educational context, as well as to the design and review of architectural/urban planning settings. The evaluation of the project is based on a human-centered, constructivist approach to working and learning, with special attention paid to the evaluation of the resulting mixed reality experience. Through this approach, participants in an activity "construct" their own knowledge by testing ideas and concepts based on their prior knowledge and experience, applying these to a new situation, and integrating the new knowledge gained with pre-existing intellectual constructs. CREATE project uses a high degree of interactivity, and includes provision for other senses (haptics and sound). The application developed in CREATE are designed to run on different platforms, and the targeted running systems are SGI and PC driven, with immersive stereo-displays such as a workbench, a ReaCTor (CAVE-like environment), and a wide projection screen.
[virtual reality, CREATE project, education, architectural planning, haptic interfaces, CAVE-like environment, Displays, intellectual constructs, urban planning, ReaCTor, haptic interface, interactive real-time construction, humanities, Vegetation mapping, mixed reality experience, Virtual reality, Cities and towns, educational context, photo-realistic virtual worlds, workbench, Testing, projection screen, stereo displays, Educational institutions, Haptic interfaces, constructivist approach, cultural heritage content, Cultural differences, realistic images, town and country planning, real data sources, Geometry, mixed-reality framework, running systems, Layout, human-centered approach]
Multiview paraperspective projection model for diminished reality
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
This paper introduces a "diminished reality" technique for removing an object or collection of objects and replacing it with an appropriate background image. Diminished reality can be considered an important part of many mixed and augmented reality applications. Our target application is the use of augmented reality (AR) to revamp procedures in industrial plants. An object or a region of interest is delineated on a single reference image. A paraperspective projection model is used to generate the correct background from multiple calibrated views of the scene. We propose methods to deal with approximately planar backgrounds with different orientations. We also propose a multi-resolution approach to deal with non-planar backgrounds. Different sets of experimental results demonstrate the success and limits of the algorithms. Results on real data from water treatment and power plants show the usefulness of this method for industrial applications.
[multiview paraperspective, Pipelines, diminished reality, Medical services, multiple calibrated views, computational geometry, augmented reality, Image reconstruction, background image, paraperspective projection model, water treatment, rendering (computer graphics), Power generation, industrial plants, image processing, Biomedical equipment, rendering, realistic images, target application, planar backgrounds, Augmented reality, Computer science, power plants, Layout, object removal, object replacement, Rendering (computer graphics), industrial applications, Industrial plants, solid modelling, reference image]
Using augmented reality for visualizing complex graphs in three dimensions
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
In this paper we explore the effect of using augmented reality (AR) for three-dimensional graph link analysis. Two experiments were conducted. The first was designed to compare a tangible AR interface to a desktop-based interface. Different modes of viewing network graphs were presented using a variety of interfaces. The results of the first experiment show that a tangible AR interface is well suited to link analysis. The second experiment was designed to test the effect of stereographic viewing on graph comprehension. The results show that stereographic viewing has little effect on comprehension and performance. These experiments add support to the work of Ware and Frank, whose studies showed that depth and motion cues provide huge gains in spatial comprehension and accuracy in link analysis.
[desktop-based interface, Visualization, complex graph visualization, Circuits, Humans, Performance gain, augmented reality, graph comprehension, user interfaces, data visualisation, graph link analysis, Three dimensional displays, motion cues, Logic devices, Testing, three-dimensional displays, network graphs, depth cues, Augmented reality, stereographic viewing, AR interface, spatial comprehension, Kinetic theory, Motion analysis, three dimension]
Resolving multiple occluded layers in augmented reality
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
A useful function of augmented reality (AR) systems is their ability to visualize occluded infrastructure directly in a user's view of the environment. This is especially important for our application context, which utilizes mobile AR for navigation and other operations in an urban environment. A key problem in the AR field is how to best depict occluded objects in such a way that the viewer can correctly infer the depth relationships between different physical and virtual objects. Showing a single occluded object with no depth context presents an ambiguous picture to the user. But showing all occluded objects in the environments leads to the "Superman's X-ray vision" problem, in which the user sees too much information to make sense of the depth relationships of objects. Our efforts differ qualitatively from previous work in AR occlusion, because our application domain involves far-field occluded objects, which are tens of meters distant from the user. Previous work has focused on near-field occluded objects, which are within or just beyond arm's reach, and which use different perceptual cues. We designed and evaluated a number of sets of display attributes. We then conducted a user study to determine which representations best express occlusion relationships among far-field objects. We identify a drawing style and opacity settings that enable the user to accurately interpret three layers of occluded objects, even in the absence of perspective constraints.
[Visualization, visualization, Laboratories, occluded objects, Context awareness, Human factors, Switches, Displays, augmented reality, object detection, navigation, data visualisation, computer displays, perspective constraints, Virtual reality, mobile AR, virtual objects, AR occlusion, AR systems, multiple occluded layers, drawing style, Navigation, Augmented reality, hidden feature removal, opacity settings, Computer science, occluded infrastructure]
Live mixed-reality 3D video in soccer stadium
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
This paper proposes a method to realize a 3D video display system that can capture video from multiple cameras, reconstruct 3D models and transmit 3D video data in real time. We represent a target object with a simplified 3D model consisting of a single plane and a 2D texture extracted from multiple cameras. This 3D model is simple enough to be transmitted via a network. We have developed a prototype system that can capture multiple videos, reconstruct 3D models, transmit the models via a network, and display 3D video in real time. A 3D video of a typical soccer scene that includes a dozen players was processed at 26 frames per second.
[Real time systems, virtual reality, multiple cameras, 3D video display system, real time system, Data engineering, 3D video data, target object, Image reconstruction, Intelligent robots, 3D reconstruction, Space technology, soccer stadium, Prototypes, Virtual reality, Three dimensional displays, soccer scene, visual information, 2D texture, image media, three-dimensional displays, 3D model, image reconstruction, video capture, prototype system, Layout, mixed reality, computer vision, image representation, Cameras, sport]
Personal positioning based on walking locomotion analysis with self-contained sensors and a wearable camera
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
In this paper, we propose a method of personal positioning for a wearable augmented reality (AR) system that allows a user to freely move around indoors and outdoors. The user is equipped with self-contained sensors, a wearable camera, an inertial head tracker and display. The method is based on sensor fusion of estimates for relative displacement caused by human walking locomotion and estimates for absolute position and orientation within a Kalman filtering framework. The former is based on intensive analysis of human walking behavior using self-contained sensors. The latter is based on image matching of video frames from a wearable camera with an image database that was prepared beforehand.
[human walking behavior, image registration, Humans, Sensor fusion, Displays, augmented reality, sensor fusion, Kalman filtering framework, cameras, absolute position, relative displacement, self-contained sensors, motion estimation, pedometer, AR system, Kalman filters, position measurement, Wearable sensors, personal positioning, Legged locomotion, video frames, Head, orientation, Filtering, image database, human walking analysis, inertial head display, wearable computing, image matching, walking locomotion analysis, Augmented reality, wearable computers, inertial head tracker, wearable camera, Cameras]
A wearable mixed reality with an on-board projector
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
One of the methods achieving mixed reality (MR) displays is the texture projection method using projectors. Another kind of emerging information environment is a wearable information device, which realizes ubiquitous computing. It is very promising to integrate these technologies. Using this kind of fusion system, two or more users can get the same MR environments without using HMD at the same moment. In this demonstration, we propose a wearable MR system with an on-board projector and introduce some applications with this system.
[augmented reality, ubiquitous computing, wearable mixed reality, Wearable computers, Physics computing, Fingers, Prototypes, texture projection, wearable information device, Virtual reality, MR displays, fusion system, projectors, helmet mounted displays, Application software, Identity management systems, on-board projector, wearable computers, Computer displays, MR environments, information environment, wearable MR system, Cameras, head mounted displays, Optical sensors]
SenseShapes: using statistical geometry for object selection in a multimodal augmented reality
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
We introduce a set of statistical geometric tools designed to identify the objects being manipulated through speech and gesture in a multimodal augmented reality system. SenseShapes are volumetric regions of interest that can be attached to parts of the user's body to provide valuable information about the user's interaction with objects. To assist in object selection, we generate a rich set of statistical data and dynamically choose which data to consider based on the current situation.
[statistical geometry, speech identification, haptic interfaces, computational geometry, speech-based user interfaces, augmented reality, user interaction, speech recognition, object selection, Monitoring, multimodal augmented reality, Head, Buildings, user body, SenseShapes, Augmented reality, Microphones, Geometry, Computer science, geometric tools, audio user interfaces, gesture identification, statistical data, Fusion power generation, Speech recognition, volumetric regions, Optical sensors]
Textured shadow
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
We demonstrate a real-time interactive system illuminating your shadows cast on the floor. A user occluding the projector light may create undesirable shadow in front-projector-based systems. We overcome and turn this problem to our advantage with an elaborately designed optics including multi-projection techniques. The systems will make you experience an exciting space of interaction performing an illusion on your shadow.
[Real time systems, Art, shadow illuminating, textured shadow, multiprojection techniques, augmented reality, optics design, illusion, Information science, spatially augmented reality, Optical design, gesture recognition, interaction space, projector light, Optical design techniques, Image converters, lighting, light effects, Augmented reality, image texture, Casting, computer graphics, Interactive systems, Cameras, real-time interactive system, front-projector-based systems]
BlueTrak - a wireless six degrees of freedom motion tracking system
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
We resent six degrees of freedom tracking system, which is wireless and scalable concerning the tracking volume and the number of devices being tracked. This is achieved by the modular design of the system consisting of two different types of modules: an arbitrary number of tracked user modules and a number of fixed reference modules. It provides a flexible setup for head tracking in virtual and augmented reality environments and for various other applications such as motion capture and analysis. The system combines inertial sensor data with ultrasonic ranging measurements to determine orientation and absolute position.
[six degrees freedom, Bluetooth, virtual reality, Tracking, optical tracking, head tracking, augmented reality, Sensor systems, ultrasonic ranging measurements, BlueTrak, motion analysis, tracked user modules, tracking volume, Ultrasonic variables measurement, absolute position, motion estimation, orientation determination, Universal Serial Bus, motion tracking system, wireless tracking system, inertial sensor data, flexible setup, position measurement, modular design, Pulse measurements, Augmented reality, image sensors, Collaboration, computer vision, Position measurement, motion capture, Motion analysis, fixed reference modules]
Predicting accuracy in pose estimation for marker-based tracking
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
Tracking is a necessity for interactive virtual environments. Marker-based tracking solutions involve the placement of fiducials in a rigid configuration on the object(s) to be tracked, called a tracking probe. The realization that tracking performance is linked to probe performance necessitates investigation into the design of tracking probes for proponents of marker-based tracking. A challenge involved with probe design is predicting the accuracy of a tracking probe. We present a method for predicting the accuracy of a tracking probe based upon a first-order propagation of the errors associated with the markers on the probe. Results for two sample tracking probes show excellent agreement between measured and predicted errors.
[Estimation error, Target tracking, Virtual environment, Error analysis, placement of fiducials, Heuristic algorithms, augmented reality, object detection, Topology, probe design, interactive virtual environments, Augmented reality, Accuracy, marker-based tracking, accuracy prediction, target tracking, pose estimation, first-order propagation, tracking performance, probe performance, error prediction, Probes, tracking probe]
An augmented virtuality approach to 3D videoconferencing
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
This paper describes the concept, prototypical implementation, and usability evaluation of an augmented virtuality (AV) based videoconferencing (VC) system: "cAR/PE!". We present a first solution which allows three participants at different locations to communicate over a network in an environment simulating a traditional face-to-face meeting. Integrated into the AV environment are live video streams of the participants spatially arranged around a virtual table, a large virtual presentation screen for 2D display and application sharing, and 3D geometry (models) within the room and on top of the table.
[Solid modeling, Virtual colonoscopy, virtual reality, AV environment, face-to-face meeting, application sharing, multimedia systems, augmented reality, 3D geometry models, teleconferencing, VC system, usability evaluation, groupware, augmented virtuality, AV based videoconferencing, virtual presentation screen, Virtual prototyping, cAR/PE!, 3D videoconferencing, Augmented virtuality, communication network, Video sharing, prototypical implementation, 2D display, Two dimensional displays, Geometry, Teleconferencing, distant computer supported collaborative work, Streaming media, video streams, CSCW, virtual table, Usability]
A city-planning system based on augmented reality with a tangible interface
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
This demonstration shows a city-planning system based on augmented reality with tangible user interface. Miniature models, illustrations and graphical computer displays have been used for the comparison and consideration in city-planning process. Augmented reality technology enables users to consider city plans more effectively and easily. One important issue of the augmented reality environment is how user can manipulate 3D structures that are displayed as virtual objects. It has to be intuitive and easy so that it may not disturb user's thought. We propose a new direct manipulation method based on the concept called tangible user interface. User holds a transparent cup upside down and can pick up, move or delete a virtual object by using it.
[Production systems, Shape, city-planning system, Computational modeling, Computer simulation, miniature models, Urban planning, 3D structures, haptic interfaces, augmented reality, tangible interface, 3D manipulation, Augmented reality, town and country planning, user interface, Computer displays, computer graphics, manipulation method, computer displays, User interfaces, Cities and towns, Cameras, virtual objects, city-planning process]
Tinmith - mobile outdoor augmented reality modelling demonstration
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
This paper outlines some of the capabilities of the Tinmith-Metro modeling system, based on mobile outdoor augmented reality technology. This system implements a new user interface based on tracked pinch gloves and a series of techniques named construction at a distance for the capture and creation of 3D geometry. The user controls the modeling process using hand and head motions, with modelling accuracy guided by the requirements of the user. Using Tinmith-Metro, users are able to model outdoor geometry representing buildings and natural features in an intuitive fashion.
[Visualization, Avatars, Laboratories, augmented reality modelling demonstration, computational geometry, Tinmith-Metro, augmented reality, modeling system, intuitive fashion, Information geometry, Information science, gesture recognition, Wearable computers, Fingers, Virtual reality, 3D geometry capture, mobile augmented reality, data gloves, outdoor augmented reality, 3D geometry creation, Augmented reality, user interface, modeling process, User interfaces, tracked pinch gloves, outdoor geometry, natural features]
3D audio augmented reality: implementation and experiments
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
Augmented reality (AR) presentations may be visual or auditory. Auditory presentation has the potential to provide hands-free and visually non-obstructing cues. Recently, we have developed a 3D audio wearable system that can be used to provide alerts and informational cues to a mobile user in such a manner as to appear to emanate from specific locations in the user's environment. In order to study registration errors in 3D audio AR representations, we conducted a perceptual training experiment in which visual and auditory cues were presented to observers. The results of this experiment suggest that perceived registration errors may be reduced through head movement and through training presentations that include both visual and auditory cues.
[3D audio augmented reality, auditory cues, augmented reality, audio AR representations, user modelling, Engines, informational cues, user environment, Feedback, 3D audio wearable system, Three dimensional displays, training presentations, auditory presentation, Navigation, audio-visual systems, head movement, three-dimensional displays, nonobstructing cues, Magnetic heads, visual cues, Application software, Augmented reality, Auditory displays, perceptual training experiment, Global Positioning System, registration errors, mobile user, Position measurement]
A high performance AR system for medical applications
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
We report on a new single PC based stereoscopic video-see-through AR system which we developed for medical applications. Recent advances in graphics hardware, memory bandwidth, and computing power of standard PCs made it possible that this system outperforms an earlier version which included 3 networked SGI workstations. We designed and implemented a new AR software platform. It is component based and - in conjunction with XML configuration files - provides efficiency, modularity, and extensibility for fast and robust prototyping of AR applications. The system has a compelling real-time performance with 30 frames/second, displaying stereoscopic augmented video views with XGA resolution.
[AR visualization, prototyping, Medical services, networked SGI workstations, augmented reality, stereoscopic augmented video views, software architecture, modularity, Bandwidth, computing power, graphics hardware, head mounted display, Hardware, Computer networks, Robustness, AR system, Workstations, Biomedical equipment, object-oriented programming, video cameras, AR software platform, extensibility, Graphics, memory bandwidth, real-time performance, medical applications, stereoscopic video-see-through AR, XML, XML configuration files, Personal communication networks, XGA resolution, medical computing]
Optical camouflage using retro-reflective projection technology
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
This paper describes a kind of active camouflage system named optical camouflage. Optical camouflage uses the retro-reflective projection technology, a projection-based augmented-reality system composed of a projector with a small iris and a retro-reflective screen. The object that needs to be made transparent is painted or covered with retro-reflective material. Then a projector projects the background image on it making the masking object virtually transparent.
[active camouflage system, Humans, Displays, augmented reality, masking object, retroreflective projection technology, Haptic interfaces, image matching, retroreflective screen, Mercury (metals), optical projectors, retroreflective material, Iris, background image, transparent object, virtual transparency, Virtual reality, optical camouflage, projection-based augmented-reality system, Optical variables control, Cameras, active vision, Optical materials, Optical refraction]
Collaborative work with volumetric data using augmented reality
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
The augmented reality videoconferencing system is a novel remote collaboration tool combining a desktop-based AR system and a videoconferencing module. The novelty of our system is the combination of these tools i.e. superimposing AR applications on live video background displaying the conference parties' real environment, thus merging the advantages of videoconferencing (natural face-to-face communication) and AR (interaction with distributed virtual objects using tangible physical artifacts). We demonstrate the system's collaborative features with a volume rendering application that allows users to display and examine volumetric data simultaneously and to highlight or explore slices of the volume by manipulating an optical marker as a cutting plane interaction device.
[Biomedical optical imaging, volume rendering application, augmented reality, Data mining, AR applications, Videoconference, teleconferencing, face-to-face communication, optical marker, data visualisation, groupware, videoconferencing system, Computer Supported Collaborative Work, software tools, rendering (computer graphics), collaborative work, interaction device, tangible physical artifacts, Collaborative tools, distributed virtual objects, live video background, videoconferencing module, desktop-based AR system, Application software, Augmented reality, volumetric data, Teleconferencing, Computer displays, remote collaboration tool, Collaboration, Collaborative work, collaborative features]
Miniaturization, calibration &amp; accuracy evaluation of a hybrid self-tracker
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
We have previously presented a prototype of a novel vision/inertial hybrid self-tracker intended for AR, wearable computing and mobile robotics applications. In this paper we describe a new prototype of the system which has been greatly reduced in size, weight, power consumption and cost, while simultaneously improved in performance through careful calibration. We describe the calibration approach in detail and present results to show the high accuracy levels achieved for the camera calibration and for the integrated tracking system.
[Energy consumption, system prototype, optical tracking, Sensor fusion, Sensor phenomena and characterization, helmet mounted display, augmented reality, sensor fusion, integrated tracking system, System performance, Prototypes, Smart cameras, Virtual reality, calibration, inertial hybrid self-tracker, Charge coupled devices, miniaturization, Calibration, mobile robotics, wearable computing, wearable computers, systems analysis, computer vision, accuracy evaluation, vision hybrid self-tracker, Lenses]
Fiducial-less 3-D object tracking in AR systems based on the integration of top-down and bottom-up approaches and automatic database addition
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
We propose a novel fiducial-less 3-D object tracking method. Our method consists of three components: 1) bottom-up approach (BUA), 2) top-down approach (TDA), and 3) automatic database addition (ADA). An experimental result shows an accuracy and robustness of our method.
[Computer vision, Measurement errors, Parameter estimation, Costs, bottom-up approaches, automatic database addition, top-down approaches, augmented reality, Spatial databases, object detection, History, image matching, Augmented reality, Image databases, Detectors, fiducialless 3D object tracking, parameter estimation, Robustness, AR systems]
Towards a usable stereoscopic augmented reality interface for the manipulation of virtual cursors
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
The combination of augmented reality (AR) systems and stereoscopic display devices has created a powerful tool with which to "supplement" our view. One application for such systems is for the augmented teleoperation of unmanned vehicles, deployed in remote or hazardous environments. Research in this area has highlighted the need for accurate 3D measurement techniques - such as through the use of virtual cursors. This paper describes our work in the development of an AR interface designed to assist the accurate selection of position in 3D space. We describe some preliminary experimental work using virtual cursors before discussing how we believe depth cues can be utilized to allow a user to make a more informed judgment of depth in unprepared environments. It is expected that the guidelines outlined in this report will be used as a benchmark for the development of further 3D ARA cursors.
[Computer interfaces, graphical user interfaces, augmented reality interface, 3D ARA cursors, augmented reality, remote environments, Vehicles, Guidelines, Measurement techniques, stereoscopic display devices, Three dimensional displays, Surface texture, AR systems, depth cues, Augmented reality, benchmark, augmented teleoperation, 3D space, Computer displays, virtual cursors, Layout, unmanned vehicles, 3D measurement techniques, AR interface, stereo image processing, Mice, stereoscopic augmented reality]
Photorealistic rendering for augmented reality using environment illumination
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
Mixing 3D computer-generated images with real-scene images seamlessly in augmented reality has many desirable and wide areas of applications such as entertainment, cinematography, design visualization and medical trainings. The challenging task is to make virtual objects blend harmoniously into the real scene and appear as if they are like real. Apart from constructing detailed geometric 3D model representation and obtaining accurate surface properties for virtual objects, adopting real scene lighting information to render virtual objects is another important factor to achieve photorealistic rendering. Such a factor not only improves visual complexity of virtual objects, but also determines the consistency of illumination between the virtual objects and the surrounding real objects in the scene. Conventional rendering techniques such as ray tracing, and radiosity require intensive computation and data preparation to solve the lighting transport equation. Hence, they are less practical for rendering virtual objects in augmented reality, which demands a real-time performance. This work explores an image-based and hardware-based approach to improve photorealism for rendering synthetic objects in augmented reality. It uses a recent technique of image-based lighting, environment illumination maps, and a simple yet practical multi-pass rendering framework for augmented reality.
[Visualization, Solid modeling, photorealistic rendering, entertainment, ray tracing, image-based approach, geometric 3D model, augmented reality, lighting transport equation, cinematography, intensive computation, Lighting, computer displays, 3D computer-generated images, image-based lighting, Ray tracing, virtual objects, lighting information, rendering (computer graphics), real-scene images, Biomedical imaging, hardware-based approach, environment illumination maps, visual complexity, Cinematography, medical trainings, image construction, Application software, lighting, realistic images, Augmented reality, data preparation, radiosity, real-time performance, synthetic objects, multipass rendering, photorealism, Layout, image representation, Rendering (computer graphics), surface properties, design visualization]
Graphic shadow: augmenting your shadow on the floor
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
This paper proposes real-time interactive systems illustrating your shadows cast on the floor, which we name "graphic shadow." They will make you experience an exciting space of interaction performing an illusion on your own shadows.
[Real time systems, pictorial shadows, graphic shadow, Image converters, Humans, augmented reality, graphical illusion, lighting, Light sources, Graphics, cameras, textured shadows, Information science, computer graphics, floors, Space technology, Interactive systems, Layout, Cameras, human computer interaction, real-time interactive systems]
The great buddha project: modeling cultural heritage for VR systems through observation
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
false
[Graphics, Geometry, Solid modeling, Computer vision, Iterative closest point algorithm, Layout, Merging, Virtual reality, Hardware, Cultural differences]
Image overlay on optical see-through displays for vehicle navigation
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
In this paper, we propose a method for image overlay on the front glass of a vehicle to navigate a driver to a desired destination. By overlaying the navigation information on the front glass, the driver need not gaze at the console panel. Therefore, accidents caused by gazing at console panel could be reduced. To overlay the image accurately on the target object through the front glass, both the vehicle's position/orientation and the driver's position/information are estimated by vision-based tracking and measuring angular velocities of the vehicle's wheels. Experimental results show the validity of the proposed method.
[optical see-through displays, image registration, Wheels, Glass, driver information, helmet mounted display, Displays, augmented reality, vehicle navigation, front glass, driver position, angular velocities measurement, target object, navigation, console panel, road vehicles, desired destination, Velocity measurement, Target tracking, Navigation, Vehicle driving, image overlay, vehicle orientation, navigation information, vehicle wheels, vehicle position, car navigation systems, virtual image, computer vision, Position measurement, Angular velocity, vision-based tracking, Accidents]
The effects of shadow representation of virtual objects in augmented reality
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
In this paper, we describe the effects of shadow representation of virtual objects in augmented reality. Optical consistency is important in order to create realistic augmented reality environments. We focus on providing accurate shadows and made two assumptions about the effects of shadow representation of virtual objects. First, that the shadow of virtual objects provides a stronger connection between the real world and virtual objects and so increases virtual object presence. Second, that the shadow of virtual objects provides depth cues and so makes three-dimensional perceptions easier for the users of the interface. We report on two experiments that show that these assumptions are correct. We also find that users report that a characteristic shadow shape provides more virtual object presence in spite of incorrect virtual light direction.
[virtual reality, Shape, optical consistency, Geometrical optics, augmented reality, user interfaces, Light sources, real world, Lighting, virtual objects, rendering (computer graphics), Material properties, three-dimensional perceptions, lighting, virtual light direction, Augmented reality, shadow representation, user interface, Shadow mapping, object presence, Layout, image representation, virtual environment, Rendering (computer graphics), Cameras]
Capturing water and sound waves to interact with virtual nature
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
To improve the interaction of people with virtually generated environments we need first to break the barriers that prevent the user from getting an experience as close to reality as possible. The main problem is to give the user the sensation that his/her presence affects the virtual world and then to let the user perceive that the actions he/she takes on the real world can change the virtual one in a smooth natural way in order to achieve virtual biofeedback. Interacting with virtual nature can transport us out from reality. We developed an interactive application with two interfaces on which the user can pretend to be the wind and interact with a virtual pond. Our application makes the user believe and feel that he/she is modifying a 3D virtual pond by the interaction with a small water receptacle in the real world using innovative wave-sensing device. The user, by speaking to a microphone, can also interact with a virtual tree by making it move according to his/her wishes. The physics for the tree are calculated to present the user's action as a wind force making this an entertaining experience.
[virtual reality, virtual tree, user sensation, sound wave, ambient media, water wave, Virtual reality, Cities and towns, Hardware, microphone, virtual nature, Virtual environment, wave-sensing device, virtual reality hardware, immersive systems, tangible user interface, user action, realistic images, Physics, virtual biofeedback, Microphones, Computer science, human-computer interaction, wave capture, Keyboards, 3D virtual pond, User interfaces, interactive application, human computer interaction, Biological control systems, water receptacle]
Robot vision-based registration utilizing bird's-eye view with user's view
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
This paper describes new vision-based registration methods utilizing not only cameras on a user's head-mounted display but also a bird's-eye view camera that observes the user from an objective viewpoint. Two new methods, the line constraint method (LCM) and global error minimization method (GEM), are proposed. The former method reduces the number of unknown parameters concerning the user's viewpoint by restricting it to be on the line of sight from the bird's-eye view. The other method minimizes the sum of errors, which is the sum of the distance between the fiducials on the view and the calculated positions of them based on the current viewing parameters, for both the user's view and the bird's-eye view. The methods proposed here reduce the number of points that should be observed from the user's viewpoint for registration, thus improving the stability. In addition to theoretical discussions, this paper demonstrates the effectiveness of our methods by experiments in comparison with methods that use only a user's view camera or a bird's-eye view camera.
[Minimization methods, Robust stability, Laboratories, image registration, head-mounted display, line constraint method, vision-based registration, Displays, robot vision, augmented reality, viewing parameters, Augmented reality, bird eye view camera, cameras, Space technology, Magnetic sensors, fiducials, user view camera, user viewpoint, Position measurement, Cameras, Robustness, global error minimization method, unknown parameters]
Augmented reality live-action compositing
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
This report describes a system that performs live-action compositing of physical and virtual objects to a panoramic background image in real-time at interactive rates. A static camera is directed towards a 40 cm/sup 3/ miniature stage, whose backdrop has been colored in chromatic green. Users can add virtual objects and manipulate their parameters within the scene by using a proxy device that consists of a small rod attached to a fiducial marker. Our system runs on commodity hardware such as a notebook equipped with a firewire video camera. The necessary chroma-keying and adaptive difference-matting algorithms have been implemented on a GPU using fragment shading.
[Real time systems, fiducial marker, optical tracking, augmented reality, fragment shading, notebook computer, firewire video camera, GPU, difference-matting algorithms, background image, chromatic green, Firewire, proxy device, physical objects, live-action compositing, Virtual reality, commodity hardware, Hardware, notebook computers, virtual objects, rendering (computer graphics), Contracts, Multimedia systems, video cameras, chroma keying, Augmented reality, programmable shading, Layout, Streaming media, Cameras, interactive rates, adaptive difference matting]
3D reconstruction of stereo images for interaction between real and virtual worlds
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
Mixed reality is different from the virtual reality in that users can feel immersed in a space which is composed of not only virtual but also real objects. Thus, it is essential to realize seamless integration and interaction of the virtual and real worlds. We need depth information of the real scene to synthesize the real and virtual objects. We propose a two-stage algorithm to find smooth and precise disparity vector fields with sharp object boundaries in a stereo image pair for depth estimation. Hierarchical region-dividing disparity estimation increases the efficiency and the reliability of the estimation process, and a shape-adaptive window provides high reliability of the fields around the object boundary region. At the second stage, the vector fields are regularized with a energy model which produces smooth fields while preserving their discontinuities resulting from the object boundaries. The vector fields are used to reconstruct 3D surface of the real scene. Simulation results show that the proposed algorithm provides accurate and spatially correlated disparity vector fields in various kinds of images, and synthesized 3D models produce natural space where the virtual objects interact with the real world as if they are in the same world.
[3D surface, virtual reality, Displays, Image reconstruction, cameras, real objects, Surface reconstruction, natural space, Ultrasonic variables measurement, 3D reconstruction, two-stage algorithm, Virtual reality, region-dividing disparity estimation, energy model, Stereo vision, virtual objects, estimation process, stereo camera, fields while, Stereo image processing, depth estimation, image reconstruction, object boundary region, stereo images, Geometry, real-virtual interaction, seamless integration, Layout, mixed reality, stereo image pair, simulation results, shape-adaptive window, stereo image processing, Cameras, disparity vector fields, human computer interaction, computational complexity, synthesized 3D models]
Computer vision based head tracking from re-configurable 2D markers for AR
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
This paper presents a computer vision based head tracking system for augmented reality. A camera is attached to a head mounted display and used to track markers in the user's field of view. These markers are movable on a table and used as interface with virtual objects. Furthermore, they are used as landmarks to track the user's viewpoint and viewing direction by a homography based camera pose estimation algorithm. By integrating this computer vision tracker with a commercially available InterSense tracker, we take the advantages of the small jitter of the former one without losing the tracking speed of the later one. For static and slow head motion the system has less than 0.3mm RMS of position jitter and 0.165 degrees RMS of orientation jitter.
[Laboratories, optical tracking, head tracking, reconfigurable 2D markers, Jitter, augmented reality, Delay, virtual object, pose estimation algorithm, user viewfield, viewing direction, gesture recognition, head mounted display, camera, Computer vision, orientation jitter, helmet mounted displays, head motion, Magnetic heads, Augmented reality, user interface, Computer displays, InterSense tracker, Layout, position jitter, computer vision, Cameras, homography, tracking speed, Optical sensors]
Fully automated and stable registration for augmented reality applications
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
We present a fully automated approach to camera registration for augmented reality systems. It relies on purely passive vision techniques to solve the initialization and real-time tracking problems, given a rough CAD model of parts of the real scene. It does not require a controlled environment, for example placing markers. It handles arbitrarily complex models, occlusions, large camera displacements and drastic aspect changes. This is made possible by two major contributions: the first one is a fast recognition method that detects the known part of the scene, registers the camera with respect to it, and initializes a real-time tracker, which is the second contribution. Our tracker eliminates drift and jitter by merging the information from preceding frames in a traditional recursive tracking fashion with that of a very limited number of key-frames created off-line. In the rare instances where it fails, for example because of large occlusion, it detects the failure and reinvokes the initialization procedure. We present experimental results on several different kinds of objects and scenes.
[fast recognition method, large camera displacements, controlled environment, Merging, image registration, Jitter, drastic aspect changes, augmented reality, initialization procedure, cameras, real-time tracking, merging the information, Automatic control, augmented reality applications, Robustness, automated registration, Target tracking, CAD model, Spatial databases, Augmented reality, hidden feature removal, stable registration, recursive tracking, Image databases, Layout, passive vision techniques, computer vision, occlusions, Cameras, camera registration, complex models, failure detection]
Robust visual tracking for non-instrumental augmented reality
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
This paper presents a robust and flexible framework for augmented reality which does not require instrumenting either the environment or the workpiece. A model-based visual tracking system is combined with rate gyroscopes to produce a system which can track the rapid camera rotations generated by a head-mounted camera, even if images are substantially degraded by motion blur. This tracking yields estimates of head position at video field rate (50Hz) which are used to align computer-generated graphics on an optical see-through display. Nonlinear optimisation is used for the calibration of display parameters which include a model of optical distortion. Rendered visuals are pre-distorted to correct the optical distortion of the display.
[display parameters, Tracking, image registration, augmented reality, model-based visual tracking, Degradation, cameras, flexible framework, Robustness, Gyroscopes, edge detection, camera rotations, rendering (computer graphics), motion blur, video field rate, head position, nonlinear optimisation, rate gyroscopes, Instruments, computer-generated graphics, Optical distortion, optical see-through display, helmet mounted displays, robot vision, Yield estimation, Augmented reality, Computer displays, visual rendering, Cameras, head-mounted camera, optical distortion, noninstrumental augmented reality]
MIXED FANTASY: exhibition of entertainment research for mixed reality
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
false
[Industrial training, Computer science, Laboratories, Collaboration, Virtual reality, Computer industry, Computer science education, Distributed algorithms, Logic arrays, Convergence]
Model-based tracking with stereovision for AR
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
This demo shows a robust model-based tracker using stereovision. The combined use of a 3D model with stereoscopic analysis allows accurate pose estimation in the presence of partial occlusions by non rigid objects like the hands of the user. Furthermore, using a second camera improves the stability of tracking and also simplifies the algorithm.
[marker-less tracking, stereovision, visual tracking, optical tracking, augmented reality, tracking stability, cameras, Feedback loop, stereoscopic analysis, pose estimation, model-based tracking, Robustness, Karhunen-Loeve transforms, camera, position measurement, algorithm, Target tracking, partial occlusions, Stability, 3D model, hidden feature removal, Computer science, Image segmentation, Image motion analysis, Optical feedback, stereo image processing, Cameras]
Telepresence system using high-resolution omnidirectional movies and a reactive display
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
This paper describes a novel telepresence system that uses high-resolution movies and a reactive display system with a treadmill. In this system, users can walk through a virtualized environment by actually walking on a treadmill. According to walking motion which is detected by using 3-D position sensors put on both legs, the virtualized environment captured by an omnidirectional multi-camera system is projected on a multi-screen display.
[motion detection, Displays, augmented reality, sensor fusion, Sensor systems, multiscreen display, cameras, computer displays, immersive screen, Automatic control, Motion pictures, Motion detection, position measurement, Legged locomotion, omnidirectional movies, omnidirectional multi-camera, 3D position sensors, Leg, virtualized environment, Graphics, reactive display, Belts, high-resolution movies, Personal communication networks, walking motion, telepresence system, treadmill]
A wearable augmented reality system with personal positioning based on walking locomotion analysis
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
In this paper, we present a wearable augmented reality (AR) system with personal positioning based on walking locomotion analysis that allows a user to freely mover around indoors and outdoors. The user is equipped with self-contained sensors, a wearable camera, an inertial head tracker and display. The system is based on the sensor fusion of estimates for relative displacement caused by human walking locomotion and estimates for absolute position and orientation within a Kalman filtering framework. The former is based on intensive analysis of human walking behavior using self-contained sensors. The latter is based on image matching of video frames from a wearable camera with an image database that was prepared beforehand.
[human walking behavior, Humans, Sensor fusion, Displays, augmented reality, sensor fusion, Kalman filtering framework, cameras, gesture recognition, human walking locomotion, absolute position, relative displacement, self-contained sensors, intensive analysis, Kalman filters, position measurement, Wearable sensors, personal positioning, Legged locomotion, video frames, Head, Filtering, image database, inertial head display, image matching, walking locomotion analysis, Augmented reality, wearable computers, inertial head tracker, wearable camera, Cameras, wearable augmented reality]
Augmented reality for programming industrial robots
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
Existing practice for programming robots involves teaching it a sequence of waypoints in addition to process-related events, which defines the complete robot path. The programming process is time consuming, error prone and, in most cases, requires several iterations before the program quality is acceptable. By introducing augmented reality technologies in this programming process, the operator gets instant real-time, visual feedback of a simulated process in relation to the real object, resulting in reduced programming time and increased quality of the resulting robot program. This paper presents a demonstrator of a standalone augmented reality pilot system allowing an operator to program robot waypoints and process specific events related to paint applications. During the programming sequence, the system presents visual feedback of the paint result for the operator, allowing him to inspect the process result before the robot has performed the actual task.
[error prone, Design automation, simulated process, real object, augmented reality, computer-animated design, robot programming, robot path, visual feedback, real-time feedback, Service robots, Feedback, industrial robots, programming robots, time consuming, AR pilot system, process specific events, robot waypoints, Educational robots, path planning, programming sequence, Application software, programming time, paint applications, Augmented reality, Robot programming, process-related events, program quality, Robot kinematics, programming process, robot program, Cameras, process inspection, Paints]
ARWin - a desktop augmented reality Window Manager
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
We present ARWin, a single user 3D augmented reality desktop. We explain our design considerations and system architecture and discuss a variety of applications and interaction techniques designed to take advantage of this new platform.
[interaction techniques, graphical user interfaces, graphical user interface, desktop augmented reality, augmented reality, Window Manager, Application software, desktop computer, Augmented reality, user 3D augmented reality, Computer science, ARWin, Computer displays, Keyboards, system architecture, Computer architecture, Computer graphics, Cameras, Mice, Clocks]
A mirror metaphor interaction system: touching remote real objects in an augmented reality environment
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
We propose a real-world-oriented interface called the "mirror metaphor interaction system". The display shows a mirror image from a camera facing a user, and the user can "touch" objects without making direct contact with the display. The "touched" object displays a menu or works directly. Objects can be placed in a remote room as well as in the user's room, and can also be moved around in a room. The user can therefore control equipment or interact with objects anywhere through the display of the system by combining images from local and remote places translucently. Our demonstration shows how the interface makes it easy to establish contact with movable objects in a remote room by "touching" them in the display.
[mirror metaphor interaction system, Control equipment, Displays, augmented reality, Sensor systems, remote room, equipment control, gesture recognition, user room, translucent self-image, real-world-oriented interface, Infrared sensors, Mirrors, mirror image, image combinations, Ubiquitous computing, augmented reality environment, Augmented reality, movable objects, translucent combination, Home computing, computer graphics, object interaction, Character generation, nondirect contact, Cameras, remote real objects]
A step forward in manual welding: demonstration of augmented reality helmet
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
A new welding helmet for the manual welding process has been developed. The welders working conditions are improved by augmenting the visual information before and during welding. The image is improved by providing a better view of the working area. An online quality assistant is available during welding, suggesting the correction of the guns position or pointing out welding errors, by analyzing the electrical welding parameters. An assembly advisor will suggest the assembly sequence, by displaying the type and the position of the following piece into the actual ensemble. In addition, an available online documentation of the welding process gives an opportunity to reduce the effort of post process quality assurance which often uses expensive X-ray investigations.
[Assembly systems, online documentation, Welding, manual welding, Glass, augmented reality, online quality assistant, user interfaces, assembly sequence, computer based training, Protection, visual information, electrical welding parameters, welding errors, welding, Computer aided manufacturing, assembly advisor, Documentation, x-ray investigations, helmet mounted displays, welding process, Augmented reality, Employee welfare, Robotic assembly, augmented reality helmet, computer vision, actual ensemble, Cameras, guns position]
Real-time localization and mapping with wearable active vision
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
We present a general method for real-time, vision-only single-camera simultaneous localisation and mapping (SLAM) - an algorithm which is applicable to the localisation of any camera moving through a scene - and study its application to the localisation of a wearable robot with active vision. Starting from very sparse initial scene knowledge, a map of natural point features spanning a section of a room is generated on-the-fly as the motion of the camera is simultaneously estimated in full 3D. Naturally this permits the annotation of the scene with rigidly-registered graphics, but further it permits automatic control of the robot's active camera: for instance, fixation on a particular object can be maintained during extended periods of arbitrary user motion, then shifted at will to another object which has potentially been out of the field of view. This kind of functionality is the key to the understanding or "management" of a workspace which the robot needs to have in order to assist its wearer usefully in tasks. We believe that the techniques and technology developed are of particular immediate value in scenarios of remote collaboration, where a remote expert is able to annotate, through the robot, the environment the wearer is working in.
[wearable robot, active camera, user motion, augmented reality, object detection, wearable active vision, scene knowledge, augmentation, single-camera localisation, remote collaboration, cameras, Simultaneous localization and mapping, natural point map, Robot sensing systems, active vision, registered graphics, visual sensor, position control, Wearable sensors, section spanning, algorithm, SLAM, Collaborative software, Motion estimation, Robot vision systems, simultaneous localisation, vision-only localisation, scene annotation, automatic, wearable visual robot, real-time localization, Graphics, image sensors, Magnetic sensors, remote expert, Layout, Cameras]
All-around display for video avatar in real world
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
This paper describes the methodology and prototype for an all-around display system for video avatar presentation in the real world. This system enables us to reconstruct a video avatar which users can look at from all around, by spinning a flat panel display which has a small viewing angle and changing the image on the display panel depending on the display's orientation.
[flat panel display, display orientation, virtual reality, Virtual environment, Avatars, Two dimensional displays, Humans, viewing angle, prototype, Image reconstruction, video avatar presentation, Information science, real world, all-around display, Prototypes, computer displays, stereo image processing, Motion pictures, display panel, telepresence, Spinning, Flat panel displays, video signal processing, methodology]
IP network designer: interface for IP network simulation
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
In this demonstration, we present IP network designer: interfaces for IP network simulation. The IP network designer consists of two subsystems: the IP network design workbench and a 3D simulator. IP network design workbench is intended to support a collaborative design and simulation of an IP network by a group of network designers and their customers. This system is based on a tangible user interface platform called "sensetable" and allows users to directly manipulate network topologies. Users can control parameters of nodes and links using physical pucks on the sensing table and simultaneously see the simulation results projected onto the table. 3D simulator provides a 3D view of simulation results. Users can see traffic packets flow as if they are inside the network. This system allows users to understand network behavior intuitively.
[parameter control, digital simulation, user interfaces, tangible user interfaces, sensing table, Network servers, Network topology, network behavior, Computer architecture, groupware, Traffic control, network designer, Communication system traffic control, IP networks, network simulation, network parameters, Computational modeling, network design workbench, physical pucks, tangible user interface, network topology, node parameters, 3D simulator, IP network, sensetable, User interfaces, network topologies, traffic packet flow, Collaborative work, Liquid crystal displays, collaborative design, network designers]
The augmented composer project: the music table
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
The music table enables the composition of musical patterns by arranging cards on a tabletop. An overhead camera allows the computer to track the movements and positions of the cards and to provide immediate feedback in the form of music and on-screen computer generated images. Musical structure is experienced as a tangible space enriched with physical and visual cues about the music produced.
[Tracking, Laboratories, augmented reality, music composition, computer generated images, tracking, cameras, Information science, music, Feedback, musical patterns, software tools, Instruments, tabletop, visual cues, Multiple signal classification, musical structure, augmented composer project, music table, Augmented reality, computer graphics, Image generation, Cameras, physical cues, Timing]
Hybrid indoor and outdoor tracking for mobile 3D mixed reality
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
false
[Costs, Navigation, Wearable computers, Firewire, Virtual reality, Cameras, Application software, Augmented reality, Mobile computing, Global Positioning System]
Herding sheep: live system for distributed augmented reality
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
In the past, architectures of augmented reality systems have been widely different and tailored to specific tasks. In this paper, we use the example of the SHEEP game to show how the structural flexibility of DWARF, our component-based distributed wearable augmented reality framework, facilitates a rapid prototyping and online development process for building, debugging and altering a complex, distributed, highly interactive AR system. The SHEEP system was designed to test and demonstrate the potential of tangible user interfaces which dynamically visualize, manipulate and control complex operations of many inter-dependent processes. SHEEP allows the users more freedom of action and forms of interaction and collaboration, following the tool metaphor that bundles software with hardware in units that are easily understandable to the user. We describe how we developed SHEEP, showing the combined evolution of framework and application, as well as the progress from rapid prototype to final demonstration system. The dynamic aspects of DWARF facilitated testing and allowed us to rapidly evaluate new technologies. SHEEP has been shown successfully at various occasions. We describe our experiences with these demos.
[System testing, Visualization, operation visualization, software prototyping, graphical user interfaces, prototyping, hardware-software codesign, Control systems, augmented reality, component-based augmented reality, user interfaces, demonstration system, AR altering, Prototypes, computer games, groupware, distributed wearable augmented reality, AR building, middleware, tool metaphor, Collaborative software, Collaborative tools, DWARF, Buildings, Debugging, Augmented reality, operation manipulation, SHEEP game, structural flexibility, User interfaces, interactive AR system, combined evolution, online development, AR debugging, operation control]
BLADESHIPS: an interactive attraction in mixed reality
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
Our purpose is to construct a new type of interactive attraction which detects the user's hand actions as triggers in a multi-participated mixed environment. "BLADESHIPS" is a game in which players compete with each other controlling virtually expressed belt-shaped flying objects, which are called "ships\
[interactive attraction, computer game, Displays, augmented reality, Digital cameras, user interfaces, games of skill, image scanners, cameras, Design engineering, 3D scanner, computer games, Virtual reality, BLADESHIPS, multiparticipated mixed environment, Marine vehicles, Material properties, user hand actions, Magnetic heads, game control, Electromagnetic measurements, high-resolution camera, mixed reality, Character generation, Rendering (computer graphics), player competition, real environment, hit real/virtual obstacles, belt-shaped flying objects]
Real-time workspace localisation and mapping for wearable robot
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
This demo showcases breakthrough results in the general field real-time simultaneous localization and mapping (SLAM) using vision and in particular its vital role in enabling a wearable robot to assists its user. In our approach, a wearable active vision system ("wearable robot") is mounted at the shoulder. As the wearer moves around his environment, typically browsing a workspace in which a task must be completed, the robot acquires images continuously and generates a map of natural visual features on-the-fly while estimating its ego-motion.
[wearable robot, real-time localisation, wearable active vision, user interfaces, cameras, Simultaneous localization and mapping, visual features, active vision, real-time mapping, SLAM, Collaborative software, Robot vision systems, ego-motion estimating, robot vision, image acquisition, Robotics and automation, workspace localisation, real-time localization, Graphics, Robot kinematics, Layout, real-time systems, Cameras, Collaborative work, Biomedical monitoring]
Case studies in application of augmented reality in future media production
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
In this application-based poster, we describe three case studies about potential application of augmented reality (AR) in the broadcasting and entertainment industry. The poster covers the potential impact on BBC's principal objectives to "entertain, educate and inform" in a variety of environments such as broadcast studios, classrooms and in the home.
[application-based poster, Computer aided software engineering, BBC, educational computing, entertainment, Broadcast technology, Displays, augmented reality, case studies, Multimedia communication, multimedia computing, Research and development, Production, Broadcasting, media production, Textile industry, entertainment industry, news information, broadcasting industry, science education, broadcast studios, Augmented reality, broadcasting, home, principal objectives, Animation, classrooms]
A wearable augmented reality system for navigation using positioning infrastructures and a pedometer
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
This paper describes a wearable augmented reality system using positioning infrastructures and a pedometer. To realize augmented reality systems, the position and orientation of user's viewpoint should be obtained in real time. The proposed system measures the orientation of user's viewpoint by an inertial sensor and the user's position using positioning infrastructures in environments and a pedometer. The system specifies the user's position using the position ID received from RFID tags or IrDA markers which are the components of positioning infrastructures. When the user goes away from them, the user's position is alternatively estimated by using a pedometer. We have developed a navigation system using the proposed techniques and have proven the feasibility of the system with experiments.
[positioning infrastructures, augmented reality, Sensor systems, navigation, IrDA markers, Wearable computers, augmented reality system, pedometer, Hardware, position measurement, Wearable sensors, real time, user position, Navigation, inertial sensor, RFID tags, Augmented reality, wearable computers, image sensors, Computer displays, Image databases, Layout, user viewpoint, Position measurement, wearable augmented reality, position ID]
DART: the Designer's Augmented Reality Toolkit
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
This demonstration highlights the Designer's Augmented Reality Toolkit (DART), a system that allows users to easily create augmented reality (AR) experiences. Over the past year our research has focused on the creation of this toolkit that can be used by technologists, designers, and students alike to rapidly prototype AR applications. Current approaches to AR development involve extensive programming and content creation as well as knowledge of technical topics involving cameras, trackers, and 3D geometry. The result is that it is very difficult even for technologists to create AR experiences. Our goal was to eliminate these obstacles that prevent such users from being able to experiment with AR. The DART system is based on the Macromedia Director multimedia-programming environment, the de facto standard for multimedia content creation. DART uses the familiar Director paradigms of a score, sprites and behaviors to allow a user to visually create complex AR applications. DART also provides low-level support for the management of trackers, sensors, and camera via a Director plug-in Xtra. This demonstration will show the wide range of AR and other types of multimedia applications that can be created with DART, and visitors will have the opportunity to use DART to create their own experiences.
[AR application, trackers, multimedia applications, Designer Augmented Reality Toolkit, multimedia-programming environment, augmented reality, application prototyping, multimedia computing, content creation, cameras, Macromedia Director, Prototypes, AR development, software tools, Testing, low-level support, Multimedia systems, 3D geometry, DART system, Director plug-in Xtra, multimedia content, extensive programming, Augmented reality, Programming profession, Geometry, Human computer interaction, Director paradigms, Cameras, Sprites (computer), visual programming, programming environments]
Inertial and magnetic sensing of human movement near ferromagnetic materials
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
This paper describes a Kalman filter design to estimate orientation of human body segments by fusing gyroscope, accelerometer and magnetometer signals. Ferromagnetic materials near the sensor disturb the local magnetic field and therefore the orientation estimation. The magnetic disturbance can be detected by looking at the total magnetic density and a magnetic disturbance vector can be calculated. Results show the capability of this filter to correct for magnetic disturbances.
[Magnetic flux, magnetometers, Kalman filter design, magnetic field, Magnetometers, magnetic disturbance vector, gyroscope fusing, magnetic sensing, magnetic density, Filters, gesture recognition, magnetometer signals, magnetic disturbances, accelerometer, magnetic sensors, Magnetic materials, Gyroscopes, Kalman filters, ferromagnetic materials, human body segments, Signal design, Accelerometers, orientation estimation, inertial sensors, gyroscopes, image motion analysis, Magnetic field measurement, Magnetic separation, Magnetic sensors, human movement]
An occlusion capable optical see-through head mount display for supporting co-located collaboration
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
An ideal augmented reality (AR) display for multi-user co-located collaboration should have following three features: 1) any virtual object should be able to be shown at any arbitrary position, e.g. a user can see a virtual object in front of other users' faces. 2) Correct occlusion of virtual and real objects should be supported. 3) The real world should be naturally and clearly visible, which is important for face-to-face conversation. We have been developing an optical see-through display, ELMO (Enhanced see-through display using an LCD panel for Mutual Occlusion), that satisfies these three requirements. While previous prototype systems were not practical due to their size and weight, we have come up with an improved optics design which has reduced size and is lightweight enough to wear. In this paper, the characteristics of typical multi-user three-dimensional displays are summarized and the design details of the latest optics are then described. Finally, a collaborative AR application employing the new display and its user experience are explained.
[arbitrary position, Laboratories, Humans, AR display, augmented reality, user interfaces, multiuser collaboration, user modelling, virtual object, optics design, lightweight, collaboration support, Optical design, face-to-face conversation, head mount display, computer displays, computer games, groupware, Three dimensional displays, Face, optical see-through display, three-dimensional displays, helmet mounted displays, ELMO, Augmented reality, hidden feature removal, prototype systems, collaborative AR application, Layout, co-located collaboration, Collaboration, occlusion, reduced size, typical multiuser displays, Collaborative work, Liquid crystal displays, Enhanced see-through display using an LCD panel for Mutual Occlusion]
Jellyfish party: blowing soap bubbles in mixed reality space
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
This paper describes a mixed reality installation named Jellyfish Party, for enjoying playing with soap bubbles. A special feature of this installation is the use of a spirometer sensor to measure the amount and speed of expelled air used to blow virtual soap bubbles.
[Head, virtual reality, Tracking, virtual soap bubble, Switches, Educational institutions, Simple object access protocol, mixed reality space, Fluid flow measurement, sensors, Character generation, computer games, Virtual reality, Games, Velocity measurement, jellyfish party, spirometer sensor]
Real-time augmented face
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
This real-time augmented reality demonstration relies on our tracking algorithm described in V. Lepetit et al (2003). This algorithm considers natural feature points, and then does not require engineering of the environment. It merges the information from preceding frames in traditional recursive tracking fashion with that provided by a very limited number of reference frames. This combination results in a system that does not suffer from jitter and drift, and can deal with drastic changes. The tracker recovers the full 3D pose of the tracked object, allowing insertion of 3D virtual objects for augmented reality applications.
[tracking algorithm, Portable computers, Robust stability, Humans, Glass, Jitter, augmented reality, real-time augmented face, reference frames, tracking, Augmented reality, information merging, recursive tracking, Firewire, real-time systems, face recognition, Cameras, Hardware, natural feature points, Face, 3D pose, solid modelling, augmented reality demonstration, 3D virtual objects]
WireAR - legacy applications in augmented reality
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
Current augmented reality (AR) applications require that the application software be written to support a specific AR interface set up. WireAR was developed to enable output from any OpenGL application to be viewed in an AR fashion. This enables the output from any legacy graphical or scientific visualization applications to be viewed in a collaborative AR setting. This demonstration shows how the output of standard desktop visualization programs can be embedded into an augmented reality experience.
[Computer interfaces, Visualization, application software, graphical applications, augmented reality, user interfaces, scientific applications, AR applications, collaborative AR, visualization programs, groupware, Computer Supported Collaborative Work, legacy applications, WireAR, OpenGL, Application software, Augmented reality, desktop programs, Geometry, Layout, AR interface, User interfaces, Chromium, Rendering (computer graphics), Collaborative work, visualization applications]
A tracker alignment framework for augmented reality
The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.
None
2003
To achieve accurate registration, the transformations which locate the tracking system components with respect to the environment must be known. These transformations relate the base of the tracking system to the virtual world and the tracking system's sensor to the graphics display. In this paper we present a unified, general calibration method for calculating these transformations. A user is asked to align the display with objects in the real world. Using this method, the sensor to display and tracker base to world transformations can be determined with as few as three measurements.
[Tracking, graphics display, image registration, optical tracking, Optical distortion, Displays, augmented reality, tracker alignment framework, Magnetic heads, Sensor systems, Calibration, virtual world, calibration method, Augmented reality, Graphics, Magnetic field measurement, accurate registration, computer vision, transformations, tracking system components, Distortion measurement, tracking system sensor, motion capture]
Message from the General Chairs
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
Presents the welcome message from the conference proceedings.
[]
Message from the Program Chairs
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
Presents the welcome message from the conference proceedings.
[]
Organizing Committee
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
Provides a listing of current committee members.
[]
Program Committee
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
Provides a listing of current committee members.
[]
The transcendent Greek [keynote speech abstract]
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
Summary form only given, as follows. Ever wish you were better at getting a date? Or just remembering names? Have trouble getting a fair shake at your annual job review? Are you the last one to hear about the corporate reorg? Computers are now becoming socially aware, and that means we can begin to augment our social reality. I will describe a series of machine perception tools that sense social signals and map social networks, and then use AR interfaces that may someday help you get a date, get a job, and get a raise.
[]
Augmenting This ... Augmented That: Maximizing Human Performance
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
false
[Humans]
OSGAR: a scene graph with uncertain transformations
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
An important problem for augmented reality is registration error. No system can be perfectly tracked, calibrated or modeled. As a result, the overlaid graphics are not aligned perfectly with objects in the physical world. This can be distracting, annoying or confusing. In this paper, we propose a method for mitigating the effects of registration errors that enables application developers to build dynamically adaptive AR displays. Our solution is implemented in a programming toolkit called OSGAR. Built upon OpenSceneGraph (OSG), OSGAR statistically characterizes registration errors, monitors those errors and, when a set of criteria are met, dynamically adapts the display to mitigate the effects of the errors. Because the architecture is based on a scene graph, it provides a simple, familiar and intuitive environment for application developers. We describe the components of OSGAR, discuss how several proposed methods for error registration can be implemented, and illustrate its use through a set of examples.
[error registration, Laboratories, Switches, Displays, Educational institutions, augmented reality, Calibration, programming toolkit, OpenSceneGraph, Augmented reality, uncertain transformations, scene graph, Graphics, computer graphics, OSGAR, Layout, Virtual reality, adaptive AR display, Computer errors, software tools, overlaid graphics]
A compact optical see-through head-worn display with occlusion support
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
We are proposing an optical see-through head-worn display that is capable of mutual occlusions. Mutual occlusion is an attribute of an augmented reality display where real objects can occlude virtual objects and virtual objects can occlude real objects. For a user to achieve the perception of indifference between the real and the virtual images superimposed on the real environment, mutual occlusion is a strongly desired attribute for certain applications. This paper presents a breakthrough in display hardware from a mobility (i.e. compactness), resolution, and a switching speed based criteria. Specifically, we focus on the research that is related to virtual objects being able to occlude real objects. The core of the system is a spatial light modulator (SLM) and polarization-based optics which allow us to block or pass certain parts of a scene which is viewed through the head-worn display. An objective lens images the scene onto the SLM and the modulated image is mapped back to the original scene via an eyepiece. We are combining computer generated imagery with the modulated version of the scene to form the final image a user would see.
[display hardware, Humans, augmented reality, polarization-based optics, optics, computer generated imagery, real objects, head mounted display, Hardware, virtual objects, Optical modulation, spatial light modulator, image processing, Educational institutions, helmet mounted displays, compact optical see-through head-worn display, augmented reality display, Augmented reality, spatial light modulators, Computer displays, computer graphics, occlusion support, mutual occlusions, Layout, optical system design, occlusion, Image generation, virtual images, Optical sensors, Spatial resolution]
Projected augmentation - augmented reality using rotatable video projectors
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
In this paper, we propose a new way of augmenting our environment with information without making the user carry any devices. We propose the use of video projection to display the augmentation on the objects directly. We use a projector that can be rotated and in other ways controlled remotely by a computer, to follow objects carrying a marker. The main contribution of this paper is a system that keeps the augmentation displayed in the correct place while the object or the projector moves. We describe the hardware and software design of our system, the way certain functions such as following the marker or keeping it in focus are implemented and how to calibrate the multitude of parameters of all the subsystems.
[projected augmentation, Video sharing, hardware-software design, augmented reality, Application software, image processing equipment, Augmented reality, Cables, Computer displays, Software design, video projection, Position measurement, Cameras, Hardware, Books, video signal processing, calibration, rotatable video projectors]
Sensor fusion and occlusion refinement for tablet-based AR
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
This paper presents a set of technologies which enable robust, accurate, high resolution augmentation of live video, delivered via a tablet PC to which a video camera has been attached. By combining several technologies, this is achieved without the use of contrived markers in the environment: An outside-in tracker observes the tablet to generate robust, low-accuracy pose estimates. An inside-out tracker running on the tablet observes the video feed from the tablet-mounted camera and provides high-accuracy pose estimates by tracking natural features in the environment. Information from both of these trackers is combined in an extended Kalman filter. Finally, to maximise the quality of the augmented imagery, boundaries where the real world occludes the virtual imagery are identified and another tracker is used to refine the boundaries between real and virtual imagery so that their synthesis is as convincing as possible.
[augmented imagery, tablet-based AR, Sensor fusion, Jitter, Displays, augmented reality, sensor fusion, virtual imagery, tracking, Delay, cameras, tablet-mounted camera, Robustness, Kalman filters, Personal digital assistants, video signal processing, occlusion refinement, video camera, Calibration, tablet PC, Augmented reality, hidden feature removal, outside-in tracker, extended Kalman filter, Cameras, Feeds, inside-out tracker, live video]
Combining edge and texture information for real-time accurate 3D camera tracking
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
We present an effective way to combine the information provided by edges and by feature points for the purpose of robust real-time 3-D tracking. This lets our tracker handle both textured and untextured objects. As it can exploit more of the image information, it is more stable and less prone to drift that purely edge or feature-based ones. We start with a feature-point based tracker we developed in earlier work and integrate the ability to take edge-information into account. Achieving optimal performance in the presence of cluttered or textured backgrounds, however, is far from trivial because of the many spurious edges that bedevil typical edge-detectors. We overcome this difficulty by proposing a method for handling multiple hypotheses for potential edge-locations that is similar in speed to approaches that consider only single hypotheses and therefore much faster than conventional multiple-hypothesis ones. This results in a real-time 3-D tracking algorithm that exploits both texture and edge information without being sensitive to misleading background information and that does not drift over time.
[Information resources, 3D camera tracking, texture information, image information, Video sequences, Jitter, Data mining, tracking, Augmented reality, image texture, feature-point based tracker, textured background, edge-detectors, Degradation, real-time 3D tracking, edge information, Layout, real-time systems, cluttered background, Cameras, Feature extraction, Robustness, edge detection, edge-locations]
Handling uncertain sensor data in vision-based camera tracking
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
A hybrid approach for real-time markerless tracking is presented. Robust and accurate tracking is obtained from the coupling of camera and inertial sensor data. Unlike previous approaches, we use sensor information only when the image-based system fails to track the camera. In addition, sensor errors are measured and taken into account at each step of our algorithm. Finally, we address the camera/sensor synchronization problem and propose a method to resynchronize these two devices online. We demonstrate our method in two example sequences that illustrate the behavior and benefits of the new tracking method.
[sensor synchronization, real-time markerless tracking, optical tracking, sensor fusion, uncertain sensor data, Sensor systems, vision-based camera tracking, cameras, image-based system, sensor errors, camera synchronization, Robustness, inertial sensor data, Computational efficiency, sensor information, Computer vision, Augmented reality, Global Positioning System, synchronisation, Image sensors, Magnetic sensors, Layout, real-time systems, computer vision, Cameras, data handling]
Display-relative calibration for optical see-through head-mounted displays
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
Optical see-through head-mounted displays (OSTHMDs) have many advantages in augmented reality application, but their utility in practical applications has been limited by the complexity of calibration. Because the human subject is an inseparable part of the eye-display system, previous methods for OSTHMD calibration have required extensive manual data collection using either instrumentation or manual point correspondences and are highly dependent on operator skill. This paper describes display-relative calibration (DRC) for OSTHMDs, a new two phase calibration method that minimizes the human element in the calibration process and ensures reliable calibration. Phase I of the calibration captures the parameters of the display system relative to a normalized reference frame and is performed in a jig with no human factors issues. The second phase optimizes the display for a specific user and the placement of the display on the head. Several phase II alternatives provide flexibility in a variety of applications including applications involving untrained users.
[Head, Instruments, optical see-through head-mounted displays, Laboratories, Human factors, Glass, eye-display system, OSTHMD, helmet mounted displays, augmented reality, Calibration, Application software, data collection, Augmented reality, phase calibration, Computer displays, Computer graphics, display-relative calibration, display system, calibration]
A marker calibration method utilizing a priori knowledge on marker arrangement
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
This paper describes a calibration method of markers which are used for registration in MR applications. There have been many vision-based approaches proposed as registration methods in MR. When multiple markers are utilized in a vision-based method, it is necessary that the geometric information of the marker arrangement such as their positions and orientations be known in advance. In this paper, we propose a hybrid method combining the "bundle adjustment method," which is a photogrammetric technique that calculates the geometric information from a set of images, with some constraints on the marker arrangement which are obtained a priori (e.g. the multiple markers are located on a single plane). After considering marker arrangements seen in many MR systems, we summarize some constraints seen in these arrangements. Then, we explain the basic framework of this method as well as a solution method under some practical constraints. We further describe several experiments and their results in order to show the effectiveness of this method.
[Fuses, Laboratories, image registration, photogrammetry, photogrammetric technique, augmented reality, vision-based method, a priori knowledge, Virtual reality, calibration, Computer vision, registration methods, Target tracking, marker arrangement, Calibration, bundle adjustment, marker calibration, geometric information, MR systems, Magnetic sensors, Layout, mixed reality, computer vision, Computer errors, Concrete]
Embedding imperceptible patterns into projected images for simultaneous acquisition and display
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
We introduce a method to imperceptibly embed arbitrary binary patterns into ordinary color images displayed by unmodified off-the-shelf digital light processing (DLP) projectors. The encoded images are visible only to cameras synchronized with the projectors and exposed for a short interval, while the original images appear only minimally degraded to the human eye. To achieve this goal, we analyze and exploit the micro-mirror modulation pattern used by the projection technology to generate intensity levels for each pixel and color channel. Our real-time embedding process maps the user's original color image values to the nearest values whose camera-perceived intensities are the ones desired by the binary image to be embedded. The color differences caused by this mapping process are compensated by error-diffusion dithering. The non-intrusive nature of our approach allows simultaneous (immersive) display and acquisition under controlled lighting conditions, as defined on a pixel level by the binary patterns. We therefore introduce structured light techniques into human-inhabited mixed and augmented reality environments, where they previously often were too intrusive.
[arbitrary binary patterns, projection technology, binary image, error-diffusion dithering, Humans, Displays, augmented reality, imperceptible patterns, digital light processing, Degradation, cameras, computer displays, Virtual reality, Hardware, image colour analysis, Optical modulation, real-time embedding process, Color, Lighting control, structured light techniques, lighting, Augmented reality, controlled lighting, mixed reality, color channel, projected images, micro-mirror modulation pattern, Cameras, color images, encoded images, image coding, DLP projector]
Scene modelling, recognition and tracking with invariant image features
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
We present a complete system architecture for fully automated markerless augmented reality (AR). The system constructs a sparse metric model of the real-world environment, provides interactive means for specifying the pose of a virtual object, and performs model-based camera tracking with visually pleasing augmentation results. Our approach does not require camera pre-calibration, prior knowledge of scene geometry, manual initialization of the tracker or placement of special markers. Robust tracking in the presence of occlusions and scene changes is achieved by using highly distinctive natural features to establish image correspondences.
[Image recognition, robust tracking, optical tracking, augmented reality, sparse metric model, virtual object, cameras, automated markerless augmented reality, invariant image features, Computer architecture, scene recognition, Robustness, scene tracking, scene changes, real-world environment, Calibration, image correspondences, Power system modeling, Augmented reality, model-based camera tracking, Computer science, Geometry, Layout, system architecture, occlusions, Cameras, scene modelling, image recognition, solid modelling, natural features]
A method for designing marker-based tracking probes
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
Many tracking systems utilize collections of fiducial markers arranged in rigid configurations, called tracking probes, to determine the pose of objects within an environment. In this paper, we present a technique for designing tracking probes called the viewpoints algorithm. The algorithm is generally applicable to tracking systems that use at least three fiduciary marks to determine the pose of an object. The algorithm is used to create an integrated, head-mounted display tracking probe. The predicted accuracy of this probe was 0.032 /spl plusmn/ 0.02 degrees in orientation and 0.09 /spl plusmn/ 0.07 mm in position. The measured accuracy of the probe was 0.028 /spl plusmn/ 0.01 degrees in orientation and 0.11 /spl plusmn/ 0.01 mm in position. These results translate to a predicted, static positional overlay error of a virtual object presented at 1m of less than 0.5 mm. The algorithm is part of a larger framework for designing tracking probes based upon performance goals and environmental constraints.
[Algorithm design and analysis, virtual reality, Tracking, Design methodology, Educational institutions, Displays, helmet mounted displays, object detection, integrated head-mounted display, Augmented reality, virtual object, Photonics, Computer science, Accuracy, environmental constraint, target tracking, viewpoint algorithm, static positional overlay error, tracking system, Probes, marker-based tracking probes, fiducial markers]
Collaborative mixed reality visualization of an archaeological excavation
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
We present VITA (visual interaction tool for archaeology), an experimental collaborative mixed reality system for offsite visualization of an archaeological dig. Our system allows multiple users to visualize the dig site in a mixed reality environment in which tracked, see-through, head-worn displays are combined with a multi-user, multi-touch, projected table surface, a large screen display, and tracked hand-held displays. We focus on augmenting existing archaeological analysis methods with new ways to organize, visualize, and combine the standard 2D information available from an excavation (drawings, pictures, and notes) with textured, laser range-scanned 3D models of objects and the site itself. Users can combine speech, touch, and 3D hand gestures to interact multimodally with the environment. Preliminary user tests were conducted with archaeology researchers and students, and their feedback is presented here.
[Visualization, head-worn display, large screen displays, Large screen displays, Laser feedback, VITA, augmented reality, Information analysis, 3D hand gestures, object texture, data visualisation, Virtual reality, archaeological dig, archaeology, Laser modes, experimental collaborative mixed reality system, Testing, projected table surface, Collaborative tools, helmet mounted displays, visual interaction tool for archaeology, collaborative mixed reality visualization, tracked hand-held display, archaeological analysis, Collaboration, archaeological excavation, Speech, laser range-scanned 3D model, see-through display, large screen display]
Agents that talk and hit back: animated agents in augmented reality
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
AR puppet is a hierarchical animation framework for augmented reality agents, which is a research area combining augmented reality (AR), sentient computing and autonomous animated agents into a single coherent human-computer interface paradigm. While sentient computing systems use the physical environment as an input channel, AR outputs virtual information superimposed on real world objects. To enhance man-machine communication with more natural and efficient information presentation, this framework adds animated agents to AR applications that make autonomous decisions based on their perception of the real environment. These agents are able to turn physical objects into interactive, responsive entities collaborating with both anthropomorphic and non-anthropomorphic virtual characters, extending AR with a previously unexplored output modality. AR puppet explores the requirements for context-aware animated agents concerning visualization, appearance, behavior, in addition to associated technologies and application areas. A demo application with a virtual repairman collaborating with an augmented LEGO/spl reg/ robot illustrates our concepts.
[Computer interfaces, Humans, man-machine communication, augmented reality, Anthropomorphism, AR puppet, animation framework, computer animation, sentient computing, physical environment, Virtual reality, interactive systems, nonanthropomorphic virtual character, Autonomous agents, Man machine systems, robots, real world object, context-aware animated agents, virtual repairman, virtual information, Application software, augmented LEGO/spl reg/ robot, software agents, Augmented reality, anthropomorphic virtual character, Collaboration, Animation, human computer interaction, autonomous animated agents, human-computer interface]
Outdoor see-through vision utilizing surveillance cameras
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
This paper presents a new outdoor mixed-reality system designed for people who carry a camera-attached small handy device in an outdoor scene where a number of surveillance cameras are embedded. We propose a new functionality in outdoor mixed reality that the handy device can display live status of invisible areas hidden by some structures such as buildings, walls, etc. The function is implemented on a camera-attached, small handy subnotebook PC (HPC). The videos of the invisible areas are taken by surveillance cameras and they are precisely overlapped on the video of HPC camera, hence a user can notice objects in the invisible areas and see directly what the objects do. We utilize surveillance cameras for two purposes. (1) They obtain videos of invisible areas. The videos are trimmed and warped so as to impose them into the video of the HPC camera. (2) They are also used for updating textures of calibration markers in order to handle possible texture changes in real outdoor world. We have implemented a preliminary system with four surveillance cameras and proved that our system can visualize invisible areas in real time.
[Real time systems, Visualization, calibration markers, Displays, augmented reality, subnotebook PC, Videos, HPC camera, computer displays, Virtual reality, notebook computers, surveillance, video signal processing, calibration, mixed-reality system, Buildings, video cameras, outdoor mixed reality, Calibration, outdoor see-through vision, outdoor scene, surveillance cameras, handy device, Surveillance, Layout, computer vision, Cameras]
Augmented reality working planes: a foundation for action and construction at a distance
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
This paper introduces the concept of augmented reality working planes to action and construction at a distance for mobile outdoor augmented reality systems. We have based our new AR working planes technique on CAD working planes, but by using coordinate systems relative to the body they can be specified and used much more intuitively than on a desktop system. We demonstrate in this paper how AR working planes can be used for the display of information, the manipulation of existing 3D objects, and the creation of new geometry. AR working planes are particularly well suited to supporting 3D modelling in mobile outdoor AR systems. This modelling task is a particularly difficult problem, because the environment is typically at a scale much larger than the user, and direct manipulation techniques can not be used.
[coordinate system, desktop system, Laboratories, Humans, CAD, computational geometry, augmented reality, mobile outdoor AR system, 3D objects, Augmented reality, Geometry, Information science, information display, augmented reality working planes, Wearable computers, augmented reality system, 3D modelling, Virtual reality, Lakes, Australia, CAD working planes, Mobile computing, solid modelling]
Immersive authoring of tangible augmented reality applications
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
In this paper, we suggest a new approach for authoring tangible augmented reality applications, called 'immersive authoring.' The approach allows the user to carry out the authoring tasks within the AR application being built, so that the development and testing of the application can be done concurrently throughout the development process. We describe the functionalities and the interaction design for the proposed authoring system that are specifically targeted for intuitive specification of scenes and various object behaviors. Several cases of applications developed using the authoring system are presented. A small pilot user study was conducted to compare the proposed method to a non-immersive approach, and the results have shown that the users generally found it easier and faster to carry out authoring tasks in the immersive environment.
[immersive authoring, Educational programs, AR application, Authoring systems, immersive environment, Humans, augmented reality, tangible augmented reality, Application software, authoring tasks, Augmented reality, Programming profession, authoring systems, authoring system, Software libraries, Layout, Virtual reality, interaction design, Testing]
Online estimation of trifocal tensors for augmenting live video
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
We propose a method to augment live video based on the tracking of natural features, and the online estimation of the trinocular geometry. Previous without-marker approaches require the computation of camera pose to render virtual objects. The strength of our proposed method is that it doesn 7 require tracking of camera pose, and exploits the usual advantages of marker-based approaches for a fast implementation. A 3-view AR system is used to demonstrate our approach. It consists of an uncalibrated camera that moves freely inside the scene of interest, and of three reference frames taken at the time of system initialization. As the camera is moving, image features taken from an initial triplet set are tracked throughout the video sequence. And the trifocal tensor associated with each frame is estimated online. With this tensor, the square pattern that was visible in the reference frames is transferred to the video. This invisible pattern is then used by the ARToolkit to embed virtual objects.
[trinocular geometry, augmented reality, ARToolkit, live video augmentation, 3-view AR system, tracking, trifocal tensors, cameras, Information geometry, motion estimation, uncalibrated camera, virtual objects, video signal processing, image sequences, Video sequences, marker-based approach, video sequence, online estimation, Calibration, Information technology, Augmented reality, Tensile stress, Councils, Layout, Streaming media, Cameras]
Ubiquitous tracking for augmented reality
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
Augmented reality (AR) provides a natural interface to the "calm" pervasive technology anticipated in large-scale ubiquitous computing environments. However, the range of classic AR applications has been limited by the scope, range and cost of sensors used for tracking. Hybrid tracking approaches can go some way to extending this range. We propose an approach, called ubiquitous tracking, in which data from widespread and diverse heterogeneous tracking sensors is automatically and dynamically fused, and then transparently provided to applications. A formal model represents spatial relationships between objects as a graph attributed with quality-of-service parameters. This paper presents a software implementation, in which a dynamic data flow network of distributed software components is thereby constructed in response to queries and optimisation criteria specified by applications. This implementation is demonstrated using a small laboratory example, and larger setups modelled in a simulation environment.
[large-scale ubiquitous computing, Target tracking, object-oriented programming, ubiquitous tracking, Subspace constraints, Sensor fusion, Ubiquitous computing, augmented reality, sensor fusion, quality of service, Application software, ubiquitous computing, Middleware, tracking, Augmented reality, pervasive technology, Space technology, distributed software component, Cameras, data flow network, Large-scale systems, heterogeneous tracking sensors, quality-of-service]
A head tracking method using bird's-eye view camera and gyroscope
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
This paper describes a new head tracking method utilizing a gyroscope mounted on a head-mounted display (HMD) and a bird's-eye view camera that observes the HMD from a fixed third-person viewpoint. Furthermore, we propose an extension of this method to hybrid registration, combining it with user's view cameras. The HMD is equipped with a gyroscope and a marker. The gyroscope measures the orientation of the user s view camera so that the number of pose parameters to be solved can be reduced. The other parameters are to be estimated by the bird's-eye view camera that observes the marker. This method is an improvement over the conventional outside-in-style vision-based tracker method, which only uses visual information. Hence, it can be thought of as an alternative to a physical head tracker such as a magnetic sensor and to an inside-out-style vision-based tracker. In addition to theoretical discussions, this paper demonstrates the effectiveness of our methods by experiments. We also propose methods for calibrating the gyroscope and the marker on HMD, which are essential in implementing the tracking method.
[Target tracking, vision-based tracker, Laboratories, image registration, head-mounted display, magnetic sensor, Displays, helmet mounted displays, Magnetic heads, head tracking method, gyroscopes, tracking, Augmented reality, bird eye view camera, gyroscope, cameras, hybrid registration, fixed third-person viewpoint, Magnetic sensors, Space technology, Layout, computer vision, Cameras, Gyroscopes]
FlightTracker: a novel optical/inertial tracker for cockpit enhanced vision
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
One of the earliest fielded augmented reality applications was enhanced vision for pilots, in which a display projected on the pilot's visor provides geo-spatially registered information to help the pilot navigate, avoid obstacles, maintain situational awareness in reduced visibility, and interact with avionics instruments without looking down. This requires exceptionally robust and accurate head-tracking, for which there is not a sufficient solution yet available. In this paper, we apply miniature MEMS sensors to cockpit helmet-tracking for enhanced/synthetic vision by implementing algorithms for differential inertial tracking between helmet-mounted and aircraft-mounted inertial sensors, and novel optical drift correction techniques. By fusing low-rate inside-out and outside-in optical measurements with high-rate inertial data, we achieve millimeter position accuracy and milliradian angular accuracy, low-latency and high robustness using small and inexpensive sensors.
[optical-inertial tracker, optical tracking, FlightTracker, Aerospace electronics, Radar tracking, High temperature superconductors, augmented reality, avionics instruments, cockpit enhanced vision, Weapons, helmet-mounted inertial sensors, aerospace computing, Robustness, Target tracking, Two dimensional displays, cockpit helmet-tracking, geo-spatially registered information, optical drift correction, helmet mounted displays, avionics, Computer displays, sensors, computer vision, synthetic vision, miniature MEMS sensors, aircraft-mounted inertial sensors, Optical sensors, Aircraft, aircraft displays]
Automatic determination of text readability over textured backgrounds for augmented reality systems
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
This paper describes a pattern recognition approach to determine readability of text labels in augmented reality systems. In many augmented reality applications, one of the ways in which information is presented to the user is to place a text label over the area of interest. However, if this information is placed over very busy and textured backgrounds, this can affect the readability of the text. The goal of this work was to identify methods of quantitatively describing conditions under which such text would be readable or unreadable. We used texture properties and other visual features to determine if a text placed on a particular background would be readable or not. Based on these features, a supervised classifier was built that was trained using data collected front human subjects' judgment of text readability. Using a rather small training set of about 400 human evaluations over 50 heterogeneous textures the system is able to achieve a correct classification rate of over 85%.
[text analysis, Humans, Interference, augmented reality, Pattern recognition, Application software, Augmented reality, textured background, Computer science, Graphics, supervised classifier, Information science, automatic text readability determination, augmented reality system, Layout, Labeling, pattern recognition]
Interactive tools for virtual x-ray vision in mobile augmented reality
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
This paper presents a set of interactive tools designed to give users virtual x-ray vision. These tools address a common problem in depicting occluded infrastructure: either too much information is displayed, confusing users, or too little information is displayed, depriving users of important depth cues. Four tools are presented: the tunnel tool and room selector tool directly augment the user's view of the environment, allowing them to explore the scene in direct, first person view. The room in miniature tool allows the user to select and interact with a room from a third person perspective, allowing users to view the contents of the room from points of view that would normally be difficult or impossible to achieve. The room slicer tool aids users in exploring volumetric data displayed within the room in miniature tool. Used together, the tools presented in this paper can be used to achieve the virtual x-ray vision effect. We test our prototype system in a far-field mobile augmented reality setup, visualizing the interiors of a small set of buildings on the UCSB campus.
[System testing, Data security, tunnel tool, Buildings, room slicer tool, augmented reality, room selector tool, Augmented reality, X-ray imaging, room in miniature tool, Wireless networks, Layout, Data visualization, Prototypes, computer vision, interactive tools, Solids, virtual x-ray vision, mobile augmented reality, Floors]
Recording and reproducing high order surround auditory scenes for mixed and augmented reality
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
Virtual reality systems are largely based on computer graphics and vision technologies. However, sound also plays an important role in human's interaction with the surrounding environment, especially for the visually impaired people. In this paper, we develop the theory of recording and reproducing real-world surround auditory scenes in high orders using specially designed microphone and loudspeaker arrays. It is complementary to vision-based technologies in creating mixed and augmented realities. Design examples and simulations are presented.
[Computational modeling, Laboratories, augmented reality, Microphone arrays, Augmented reality, high order surround auditory scene, virtual reality system, microphone array, vision technology, Loudspeakers, Computer displays, computer graphics, loudspeaker array, audio signal processing, Layout, mixed reality, Virtual reality, Computer graphics, computer vision, Robustness]
Video see-through AR on consumer cell-phones
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
We present a first running video see-through augmented reality system on a consumer cell-phone. It supports the detection and differentiation of different markers, and correct integration of rendered 3D graphics into the live video stream via a weak perspective projection camera model and an OpenGL rendering pipeline.
[Image edge detection, Pipelines, Optical distortion, augmented reality, consumer cell phones, Augmented reality, OpenGL rendering pipeline, Graphics, augmented reality system, rendered 3D graphics, Streaming media, Rendering (computer graphics), Cameras, live video stream, weak perspective projection camera model, Distortion measurement, Personal digital assistants, rendering (computer graphics), video signal processing, video see-through AR, cellular radio]
Rendering of highly polygonal augmented reality applications on a scalable PC-cluster architecture
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
In the automobile industry virtual prototypes are often used within the product development process. Here computer models of cars which are still under development are generated and analyzed to reduce the time and costs for building and testing real prototypes. In this context the technology of augmented reality offers a new perspective. In this paper we describe a system architecture for a PC-cluster consisting of AR nodes for the image processing and VR nodes for the parallel rendering. The AR nodes analyze the live video coming from two cameras and calculate the tracking data for the VR nodes. According to the tracking data the VR nodes render the 3D objects in parallel. The rendered images are sent back to the AR nodes where they are mixed with the life video. The proposed scalable architecture provides a very high image quality and rendering performance. This allows the superimposing of highly polygonal 3D models at high frame rates and excellent image quality.
[workstation clusters, image processing, Costs, scalable PC-cluster architecture, virtual prototyping, augmented reality, automobile industry, Application software, Automobiles, Augmented reality, Image quality, mechanical engineering computing, automobile industry virtual prototypes, parallel rendering, system architecture, Virtual reality, Computer architecture, Rendering (computer graphics), Product development, highly polygonal augmented reality, Virtual prototyping, rendering (computer graphics), video signal processing]
An augmented reality system for treating psychological disorders: application to phobia to cockroaches
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
Augmented reality has been used in many fields, but it has not been used to treat psychological disorders. Augmented reality presents several advantages respect to: the traditional treatment of psychological disorders and virtual reality treatments. In this paper we present the first augmented reality system for the treatment of phobia to cockroaches. Our system has been developed using ARToolkit software. It has been tested with one patient and the results have been very satisfactory. At first of the exposure session the patient was not able to approach to a real cockroach and after the exposure session using our augmented reality system, the patient was able to approach to a real cockroach, to interact with it and to kill it by herself. This first result is very encouraging and it demonstrates that augmented reality exposure is effective for the treatment of this kind of phobias.
[virtual reality, Instruments, Psychology, Medical treatment, augmented reality, Augmented reality, ARToolkit software, Animals, psychology, augmented reality system, Virtual reality, Streaming media, psychological disorders, Cameras, Universal Serial Bus, cockroach phobia, patient diagnosis, Testing]
Projection matrix decomposition in AR - a study with Access3D
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
Access3D is an AR application developed for industrial maintenance. An important feature of Access3D is that, in addition to visualization, it allows the user to inquire further information, directly from the AR view through the hyperlinks. In Access3D, we use VRML for visualization, the advantage is the direct database inquiry through the hyperlinks. On the other hand, VRML requires explicit camera parameters for the virtual camera modeling. Explicit camera calibration is often obtained from projection matrix computation (PMC) followed by projection matrix decomposition (PMD). The PMD is known to be numerically bistable, this is a problem of great importance and research interest in computer vision. Different methods, including data normalization have been introduced for finding numerically stable algorithms. Our analysis shows that even if data normalization is applied to the PMC step, a non-favorite distribution of the 3D points may still cause error in the results and numerical instability. This paper provides a look at the numerical instability of PMD from a different point of view.
[AR application, projection matrix computation, Transmission line matrix methods, augmented reality, numerical instability, matrix decomposition, numerical stability, virtual camera modeling, data visualisation, data visualization, Matrix converters, Access3D, virtual reality languages, Computer vision, camera calibration, projection matrix decomposition, Spatial databases, Calibration, Matrix decomposition, data normalization, VRML, Data visualization, computer vision, Computer errors, Cameras, High definition video, industrial maintenance]
Accuracy in optical tracking with fiducial markers: an accuracy function for ARToolKit
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
Optical tracking with fiducial markers is commonly used in augmented reality (AR) systems - AR systems that rely on the ARToolKit (Kato and Billinghurst, 1999) are prominent examples. The information obtained by the tracking subsystem are widely used in AR, e.g. in order to calculate how virtual objects should be located and oriented. The results of extensive accuracy experiments with single markers are reported and made operational by the definition of an accuracy function. The results show a specific distribution of tracking accuracy dependent on distance as well as angle between camera and marker. This insight is applicable for designing the set-up of AR applications in general that rely on optical tracking.
[augmented reality systems, Optical design, optical tracking, Cameras, augmented reality, ARToolKit, virtual objects, Augmented reality, Pattern matching, fiducial markers, Testing, Lenses]
A texture based time delay compensation method for augmented reality
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
One of the key problems in augmented reality systems is registration, that is to say the synchronization of real and virtual world. Augmented reality uses a lot of different sensors in order to estimate camera or operator's point of view. These sensors could provide samples faster than mixing of virtual and real information could be displayed. We expose here a way to fake into account samples that are generated during the mixing process. This method is using a post-rendering technique involving a texture to perform this task. We expose the errors reduction obtained by performing such technique with a simulation test-bench implementing our proposal.
[Performance evaluation, sensor data, Delay effects, texture based time delay compensation, image registration, Switches, Displays, augmented reality, Augmented reality, postrendering technique, cameras, augmented reality systems, error reduction, Layout, post-rendering techniques, Chromium, Rendering (computer graphics), Cameras, registration problem, rendering (computer graphics), pseudo-correction, sensors data, Testing]
3D modeling of wide area outdoor environments by integrating omnidirectional range and color images
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
This paper describes a method for modeling wide area outdoor environments by integrating omnidirectional range and color images. The proposed method effectively reconstructs the 3D models of outdoor environments by using omnidirectional laser rangefinder and omnidirectional multi-camera system (OMS). In this paper, we also give experimental results of 3D wide area reconstruction using the data acquired at 50 points in our campus.
[omnidirectional multicamera system, Image resolution, Density measurement, Shape measurement, Color, 3D model reconstruction, Sensor systems, omnidirectional range, image reconstruction, Image reconstruction, cameras, wide area outdoor environments, omnidirectional laser rangefinder, laser ranging, Position measurement, Laser modes, Cameras, color images, image colour analysis, 3D wide area reconstruction, Rotation measurement, solid modelling]
Real-time markerless human body tracking using colored voxels and 3D blobs
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
This paper presents a robust method for real-time visual human body tracking. We perform a hierarchical 3D reconstruction from multiple camera views as a basis for tracking. Blobs attached to a kinematic model are then used to reliably track individual body parts with both volume and color information. We describe how the blob-model is dynamically adjusted to accommodate different body configurations. In tests, our system has proved robust in presence of noisy data and self-occlusions.
[Real time systems, real-time visual human body tracking, Costs, Humans, optical tracking, multiple camera view, image reconstruction, Image reconstruction, Augmented reality, cameras, 3D blobs, real-time markerless human body tracking, hierarchical 3D reconstruction, Layout, real-time systems, blob model, Kinematics, kinematic model, Cameras, Robustness, Three dimensional displays, colored voxels, image colour analysis]
Implementing MR-based interaction techniques for manipulating 2D visualizations in 3D information space
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
A 3D information space is a three-dimensional visualization that contains 2D visualizations and puts them in a semantic context. In this paper, we sketch how mixed reality (MR) can be exploited as a technology in order to realize better interaction in a 3D information space and, as a result, to develop new interactive visualization techniques. Here, the real space is used as a metaphor for interacting with the 3D information space.
[virtual reality, 3D visualization, Navigation, Liver, Lamps, MR-based interaction technique, Image restoration, Filters, Space technology, mixed reality, Data visualization, data visualisation, Virtual reality, 2D visualizations, Mice, interactive visualization, Lenses, 3D information space]
Augmented standardized patients now virtually a reality
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
Standardized patients (SPs), individuals who realistically portray patients, are widely used in medical education to teach and assess communication skills, eliciting a history, performing a physical exam, and other important clinical skills. One limitation is that each SP can only portray a limited set of physical symptoms. Finding SPs with the abnormalities students need to encounter is typically not feasible. This project augments the SP by permitting the learner to hear abnormal heart and lung sounds in a normal SP.
[Heart, medical diagnostic computing, Computational modeling, Medical simulation, Computer simulation, medical education, biomedical education, lung sounds, augmented reality, History, patient care, Augmented reality, abnormal heart, augmented standardized patients, Lungs, Education, Stethoscope, clinical skills, computer aided instruction, physical exam, Medical diagnostic imaging, communication skills]
Invisible marker tracking for AR
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
We introduce a novel tracking system based on invisible markers which are created/drawn with an IR fluorescent pen. The tracking system consists of one scene camera, one IR camera, and one half mirror. The two cameras are positioned in each side of half mirror so that their optical centers coincide with each other. We track the invisible markers using the IR camera and visualize AR in the view of the scene camera. Thus, it works as a robust marker-less tracking system. Experimental results are given to demonstrate the viability of the proposed system.
[Visualization, Computer vision, optical tracking, Ink, augmented reality, Augmented reality, IR fluorescent pen, Geometry, cameras, Filters, infrared imaging, Layout, Cameras, invisible marker tracking, Robustness, Mirrors, markerless tracking system]
An augmented reality based simulation of obstetric forceps delivery
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
During the process of human childbirth, obstetric forceps delivery is a justified alternative to Caesarean section when vaginal delivery proves difficult or impossible. Currently, training of forceps interventions is done on a real case due to the lack of realistic dummy models. This paper presents a basic augmented reality implementation of a forceps delivery which provides a platform for both training of forceps placement and manipulation for junior obstetricians as well as the assessment of any mechanical effects these actions may have on the fetus, and the fetal head and skull in particular.
[human childbirth, Caesarean section, Blades, realistic dummy model, Humans, augmented reality, Magnetic heads, digital simulation, Face detection, Augmented reality, Optical devices, Image segmentation, junior obstetricians, Optical polarization, computer based training, augmented reality based simulation, Fetus, Skull, obstetric forceps delivery, forceps placement training, obstetrics, vaginal delivery, medical computing]
Easing the transition between multiple trackers
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
Augmented reality research currently aims at extending the working range of applications by combining multiple trackers in adjacent areas. The transition between two such devices leads to discontinuities in the trajectory of a tracked object. The result are "jumps" in visual augmentations shown to the user. We present a three step unsupervised learning algorithm that determines the working areas of involved trackers and the area they overlap, and permanently observes the tracked object's position with regard to these areas in order to enable a smooth interpolation within the overlapping area from one tracker's readings to another's. We have tested the algorithm's performance in an experimental setup. The results show that the method is feasible and only adds a negligible overhead to AR systems.
[smooth interpolation, Machine learning algorithms, Area measurement, augmented reality, multiple trackers, Calibration, tracking, Augmented reality, Unsupervised learning, unsupervised learning, Interpolation, visual augmentation, interpolation, Neural networks, Machine learning, Trajectory, Testing]
Virtual reality and augmented reality in digestive surgery
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
Medical image processing led to a major improvement of patient care: the 3D modeling of patients from their CT-scan or MRI provides an improved surgical planning and simulation allows to train the surgical gesture before carrying it out. These two preoperative steps can be used intra-operatively with the development of augmented reality (AR). In this paper, we present the tools we developed to provide our first prototypal AR guiding system for abdominal surgery.
[digestive surgery, Visualization, virtual reality, Medical simulation, augmented reality, patient care, Augmented reality, Abdomen, Pathology, 3D modeling, Magnetic resonance imaging, Surgery, surgical simulation, Virtual reality, surgical planning, AR guiding system, Liver neoplasms, medical image processing, Biomedical imaging, surgery]
Designing backpacks for high fidelity mobile outdoor augmented reality
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
This paper presents the design for our latest backpack to support mobile outdoor augmented reality, and how it evolved from lessons learned with our previous designs. We present a number of novel features which help to reduce size and weight, improve reliability and ease of configuration, and reduce CPU usage on laptop computers.
[Portable computers, backpack design, Fasteners, augmented reality, Augmented reality, Global Positioning System, Cables, laptop computers, laptop computer, Design engineering, mobile computing, Optical design, high fidelity mobile outdoor augmented reality, Wearable computers, Australia, Joining processes]
Making tracking technology accessible in a rapid prototyping environment
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
In this paper we present an approach for exposing tracking technology in an accessible and flexible way to users of a rapid prototyping system for mixed (MR) and augmented reality (AR). Our system provides a tracking framework that alleviates the need for a high level of expertise while also presenting a model of the technology that allows for flexible modification of tracking configurations, the ability to quickly change an application from one type of tracking technology to another, and the creation of synthetic trackers for playback of prerecorded data, data fusion from multiple trackers, and wizard-of-oz applications.
[rapid prototyping system, data fusion, System testing, Object oriented modeling, software prototyping, rapid prototyping environment, Switches, Educational institutions, augmented reality, sensor fusion, multiple trackers, Application software, tracking, Augmented reality, Computer languages, mixed reality, Prototypes, tracking technology, Cameras, Hardware, tracking configuration, wizard-of-oz application]
An AR workbench for experimenting with attentive user interfaces
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
We present a workbench to build, evaluate and iteratively develop user interfaces using augmented reality, eye tracking, and visual programming. To test our system, we have developed an attentive user interface (AUI) for automotive environments, which coordinates its activities based on the context and visual attention of the user. Development of AUI requires interdisciplinary teams like psychologists, human-factor engineers, designers and computer scientists to work together. The main problem of interdisciplinary communication is a lack of common language and different notion of the system. We have developed a workbench, which facilitates the communication between the team members and enhances the comprehension of the system by visualizing users' attention and system reactions.
[Context, Computer interfaces, Visualization, System testing, Psychology, augmented reality, user interfaces, interdisciplinary communication, automotive environments, tracking, Augmented reality, Automotive engineering, attentive user interfaces, team working, Design engineering, Prototypes, data visualisation, eye tracking, User interfaces, visual programming]
Augmented reality in support of interaction for location-aware applications
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
There has been an increased interest in both the augmented reality (AR) and ubiquitous computing (Ubicomp) research communities to integrate these two technologies. In an attempt to introduce visual interaction into location-aware applications we have developed a prototype that lets users experience a Ubicomp environment visually. Some system issues we came across in accomplishing this task are described.
[Pervasive computing, Visualization, Laboratories, Ubiquitous computing, augmented reality, ubiquitous computing, Yarn, Augmented reality, Programming profession, Design engineering, mobile computing, Physics computing, Prototypes, location-aware applications, visual interaction]
Annotation-based assistance system for unmanned helicopter with wearable augmented reality environment
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
In this paper, we introduce an annotation-based assistance system for an unmanned helicopter with a wearable augmented reality environment. In this system, an operator controls the helicopter remotely while watching an annotated view from the helicopter through a head mounted display (HMD) with a laptop PC in a backpack. Annotations assist the operation indicating some conditions of the helicopter and a name of buildings nearby. The position and the attitude of the helicopter is measured by GPS and a gyroscope, and sent to the operator's PC via a wireless LAN.
[Wireless LAN, unmanned helicopter, Helicopters, Control systems, augmented reality, aircraft control, GPS, laptop computers, remotely operated vehicles, helicopters, laptop PC, aerospace computing, annotation-based assistance system, head mounted display, Gyroscopes, Head, remote control, helmet mounted displays, Augmented reality, Global Positioning System, gyroscope, wearable computers, telecontrol, wearable augmented reality environment, Image generation, Cameras, wireless LAN, Payloads]
The AR apprenticeship: replication and omnidirectional viewing of subtle movements
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
We propose an AR system that learns from the expert by tracking his movements while he/she uses a simulator or performs a real (often complicated) task. This information is reproduced for demonstration to students in an enhanced simulator. By simultaneous visualization and comparison of the experts and students performance, direct feedback is provided. We propose real-time recording of instruments' tracking data of expert's actions for latter replication in an AR system for teaching purposes. The movements are recorded relative to an existing physical simulator. By this means the student can view the subtle movements from any direction repeatedly without time limits of the expert in order to learn and improve his own performance.
[Visualization, omnidirectional viewing, Target tracking, Head, expert systems, movement tracking, Instruments, Biological system modeling, Computational modeling, Medical simulation, augmented reality, digital simulation, teaching, real-time recording, tracking, Feedback, real-time systems, Imaging phantoms, Cameras, AR system, computer aided instruction]
Rapid development of expressive AR applications
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
We describe our approach of developing AR applications with expressive visual representations and realistic object behavior. We combine concepts from high-level scene graphs, physical simulation libraries and a Java binding for ARToolkit with the ideas of iterative prototyping and show that rapid development of expressive AR applications using Java is possible.
[Java, high-level scene graphs, software prototyping, graph theory, augmented reality, ARToolkit, realistic object behavior, expressive AR applications, Application software, Augmented reality, Physics, software libraries, Graphics, physical simulation libraries, expressive visual representations, Layout, Prototypes, data visualisation, Java language, Rendering (computer graphics), Libraries, iterative prototyping, Testing]
The effect that haptically perceiving a projection augmented model has on the perception of size
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
This paper reports on a study that investigated the effect touching a projection augmented model, and interacting with it using a spatially-coincident device, has on the perception of size. It was found that touching increased the accuracy of size estimates, however interaction using a spatially-coincident device did not.
[Solid modeling, Shape, visual perception, haptic interfaces, spatially-coincident device, augmented reality, Haptic interfaces, projection augmented model, Augmented reality, Information systems, Computer displays, Physics computing, size perception, Feedback, Computer applications, haptic perception, Mice]
A multi-camera 6-DOF pose tracker
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
Most of the work in head-pose tracking has concentrated on single-camera systems with a relatively small field of view which have limited accuracy because features are only observed in a single viewing direction. We present a multicamera pose tracker that handles an arbitrary configuration of cameras rigidly fixed to the observer's head. By using multiple cameras, we increase the robustness and accuracy by which a 6-DOF pose is tracked. However, in a multicamera rig setting, earlier methods for determining the unknown pose from three world-to-camera correspondences are no longer applicable. We present a RANSAC (M. Fischler and R. Bolles, 1981) based method that handles multicamera rigs by using a fast nonlinear minimization step in each RANSAC round.
[Minimization methods, world-to-camera correspondences, Educational institutions, Calibration, Noise measurement, Machinery, tracking, Equations, head-pose tracking, cameras, RANSAC, Algorithms, Gaussian noise, multicamera rigs, fast nonlinear minimization, computer vision, motion estimation, Cameras, Robustness, minimisation, multicamera 6-DOF pose tracker]
Virtual training for welding
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
A mixed reality system has been created for simulating gas metal arc welding (GMAW) welding. This simulation system is intended for use in training human welders. The system is comprised of a real welding torch attached to a force feedback device, a head-mounted display, a 6 DOF tracking system for both the torch and the user's head, and external audio speakers. The welding simulation is based on empirical results from detailed analysis of a series of test welds. The simulation runs in real-time, using a neural network to determine the quality and shape of the created weld based on the orientation and speed of the welding torch. The welding process and resulting weld bead are displayed in a virtual environment. Weld quality and recorded process values can be displayed after welding for review.
[virtual training, external audio speakers, force feedback, Shape, Welding, Humans, head-mounted display, production engineering computing, augmented reality, mixed reality system, digital simulation, tracking, neural network, force feedback device, Analytical models, 6-DOF tracking system, computer based training, Virtual reality, Testing, Virtual environment, gas metal arc welding simulation, Force feedback, welding torch, helmet mounted displays, Auditory displays, real-time simulation, welding training, Neural networks, arc welding, real-time systems, neural nets]
Level of detail interfaces
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
We present the level of detail interface based on the marriage of level of detail geometry and an adaptable user interface. Level of detail interfaces allow applications to paramaterize their display of data and interface widgets with respect to distance from the camera, to best take advantage of diminished screen space in a 3D environment.
[Navigation, graphical user interfaces, computational geometry, level of detail interfaces, Application software, level of detail geometry, Graphics, Computer science, adaptable user interface, Computational geometry, Computer displays, User interfaces, Three dimensional displays, Lenses, Bars]
Vision-based augmented reality for pilot guidance in airport runways and taxiways
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
This paper describes our on-going efforts to develop an augmented reality system for enhanced pilot situational awareness in airport runways and taxiways. The system consists of a sensing component based on computer vision and an information component based on high-fidelity graphic model databases. Vision algorithms are used for a variety of guidance and warning tasks. A necessary requirement is for vision algorithms to have a real-time response.
[pilot situational awareness, pilot guidance, high-fidelity graphic model databases, visual databases, Airports, augmented reality, Spatial databases, taxiways, Visual databases, Data mining, Character recognition, Optical character recognition software, real-time response, Augmented reality, airports, Graphics, Image databases, aircraft landing guidance, real-time systems, computer vision, Feature extraction, vision-based augmented reality, airport runways]
Developing AR Applications with ARToolKit
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
false
[]
Advanced Visual Tracking
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
false
[]
Spatial Augmented Reality
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
false
[Augmented reality]
Extreme MR: Going Beyond Reality to Create Extreme Multi-Modal Mixed Reality for Entertainment, Training and Education
Third IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2004
false
[Virtual reality]
Message from the General Chairs
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
Presents the welcome message from the conference proceedings.
[]
Message from the Program Chairs
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
Presents the welcome message from the conference proceedings.
[]
Organizing Committee
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
Provides a listing of current committee members.
[]
Steering Committee and Area Chairs
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
Provides a listing of current committee members.
[]
Program Committee
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
Provides a listing of current committee members.
[]
Workshop [Industrial Augmented Reality]
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
Augmented Reality has matured from a pure research field into actual industrial applications. There are still many research questions to solve, and solutions are discussed every year at the ISMAR conference series. However, making systems work reliably in an industrial setting usually requires more effort, knowledge, and ideas than can be discussed by academia at a purely scientific conference. The goal of this workshop is to bring together people from industry and academia who use augmented reality technologies in real industrial settings aimed at producing a commercial benefit. The workshop provides a platform to jointly discuss the &#x0201C;devil in the detail&#x0201D; and to identify ways to make the leap from research-based demonstrations to fully integrated systems. It is organized as a series of talks intended for a broad audience interested in the current application of augmented reality technology in industrial settings.
[]
Mixed Reality Research: The European Dimension
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
false
[Video coding, Image processing, Digital images, Bit rate, Collaboration, Virtual reality, Proposals, Research and development]
The Art of Technology and the Future of MR: Integrating an Artistic Approach to Transform the Next Generation of Mixed and Augmented Reality
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
false
[Technological innovation, Art, Cellular phones, Subspace constraints, Laboratories, Virtual reality, Turning, Augmented reality, Commercialization, Convergence]
The art of nurturing citizen scientists through mixed reality
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
Modern society requires the public to have an increased knowledge of science beyond the basics, with average people needing to understand concepts in science, technology, engineering and mathematics that rapidly evolve throughout their lifetimes. Our future depends upon closing the gaps between citizen and scientist to create a "citizen scientist". Mixed reality innovations provide the magic to spark a lifetime of learning for community learning centers by continuously providing new content and reinventing the free-choice learning experience that helps citizens make informed choices in an increasingly complex world. This paper explores a mixed reality experiential learning landscape that expands our ability to provide dynamic content structures for venues to engage the user's physical environment and interactive imagination by incorporating the conventions of story, play and game employed in competing leisure time activities. This is intended to reverse the standard decline of attendance by teenagers and young adults.
[Knowledge engineering, Technological innovation, virtual reality, Computational modeling, Computer simulation, Subspace constraints, Laboratories, Mathematics, Environmental economics, Computer science, mixed reality experiential learning landscape, humanities, Virtual reality, community learning centers, citizen scientists, interactive imagination, computer aided instruction]
Enabling view-dependent stereoscopic projection in real environments
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
We show how view-dependent image-based and geometric warping, radiometric compensation, and multi-focal projection enable a view-dependent stereoscopic visualization on ordinary (geometrically complex, colored and textured) surfaces within everyday environments. Special display configurations for immersive or semi-immersive AR/VR applications that require permanent and artificial projection canvases might become unnecessary. We demonstrate several ad-hoc visualization examples in a real architectural and museum application context.
[Visualization, radiometric compensation, view-dependent stereoscopic projection, Two dimensional displays, view-dependent stereoscopic visualization, Optical distortion, Geometrical optics, augmented reality, real environments, view-dependent image-based warping, multifocal projection, Augmented reality, museum, data visualisation, computer displays, Virtual reality, stereo image processing, Radiometry, geometric warping, Surface texture, Nonlinear optics, Water resources]
Simulation-based design and rapid prototyping of a parallax-free, orthoscopic video see-through head-mounted display
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
We built a video see-through head-mounted display with zero eye offset from commercial components and a mount fabricated via rapid prototyping. The orthoscopic HMD's layout was created and optimized with a software simulator. We describe simulator and HMD design, we show the HMD in use and demonstrate zero parallax.
[medical AR, Biomedical optical imaging, Avatars, software prototyping, orthoscopic HMD layout, Displays, augmented reality, digital simulation, zero parallax, Virtual prototyping, Mirrors, medical image processing, Eyes, simulation-based design, Computational modeling, minimally invasive surgery, rapid prototyping, Optical distortion, video cameras, helmet mounted displays, Optical devices, software simulator, zero eye offset, Cameras, surgery, parallax-free orthoscopic video see-through head-mounted display]
A polarized head-mounted projective display
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
The lack of image brightness is a common problem in optical see-through head-mounted displays (OST-HMD) where a beamsplitter is required to combine views from HMD image source and the direct-view of a real world scene. This problem is further aggregated in a head-mounted projective display (HMPD) due to the fact that light passes through the beamsplitter multiple times. In this paper, we present a novel design of an ultra-bright polarized head-mounted projective display (p-HMPD). The image brightness observed by a viewer theoretically is four-times brighter than existing designs. We further demonstrate a design with currently available technology that leads to a display in which the observed image is significantly brighter than existing designs. Finally, experimental results from a bench setup are presented.
[image processing, Biomedical optical imaging, optical see-through head-mounted displays, ultra-bright polarized head-mounted projective display, Brightness, image brightness, Optical computing, polarisation, helmet mounted displays, beamsplitter, Augmented reality, Optical polarization, Computer displays, brightness, Layout, Optical attenuators, Prototypes, Collaboration, optical beam splitters]
Synchronizing 3D movements for quantitative comparison and simultaneous visualization of actions
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
In our poster presentation at ISMAR '04, we proposed the idea of an AR training solution including capture and 3D replays of subtle movements. The crucial part missing for realizing such a training system was an appropriate way of synchronizing trajectories of similar movements with varying speed in order to simultaneously visualize the motion of experts and trainees, and to study trainees' performances quantitatively. In this paper we review the research from different communities on synchronization problems of similar complexity. We give a detailed description of the two most applicable algorithms. We then present results using our AR based forceps delivery training system and therefore evaluate both methods for synchronization of experts' and trainees' 3D movements. We also introduce the first concepts of an online synchronization system allowing the trainee to follow movements of an expert and the experts to annotate 3D trajectories for initiation of actions such as display of timely information. A video demonstration provides an overview of the work and a visual idea of what users of the proposed system could observe through their video-see through HMD.
[Visualization, Medical simulation, medical education, augmented reality, digital simulation, 3D movement synchronization, AR birth simulator, 3D trajectories, data visualisation, computer based training, quantitative comparison, Three dimensional displays, AR based forceps delivery training system, medical image processing, Biomedical imaging, Target tracking, Head, Instruments, biomedical education, helmet mounted displays, Augmented reality, synchronisation, virtual image, Imaging phantoms, Cameras, video-see through HMD, online synchronization system]
Authoring and user interaction for the production of wave field synthesis content in an augmented reality system
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
Wave field synthesis (WFS) enables the accurate reproduction of a sound field for a large listening area with correct characteristics for each listener position. An exact perspective on the synthesized wave field is provided for every listener. Therefore, WFS-technology is ideally suited to be combined with augmented reality systems, where every user perceives his own visual perspective of a given scene. This paper presents a concept for authoring and user interaction for the production of wave field synthesis content in an augmented reality system. Also, the implementation of a prototype WFS-AR system based on ARToolkit is explained.
[Production systems, Circuits, acoustic waves, wave field synthesis content production, augmented reality, user interaction, ARToolkit, authoring systems, acoustic field, augmented reality system, Integral equations, Prototypes, Hardware, Workstations, sound field, signal synthesis, visual perception, sound reproduction, Augmented reality, audio user interfaces, Surface waves, audio signal processing, Layout, User interfaces, visual perspective]
ARVino - outdoor augmented reality visualisation of viticulture GIS data
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
This paper describes the combination of two technologies, augmented reality and GIS, to provide a new way to visualise viticulture GIS data using outdoor mobile computers. Viticulturists use GIS to assist with accurately understanding the parameters that affect their yields and quality of the grapes from different vineyards. The ability to view this data in the field digitally would be advantageous to the viticulturist. This paper describes the ARVino system; an AR platform that was built for visualising 3D data outdoors using a movable tripod-based computer. We describe the user interface, some problems that were encountered, and how the visualisation and interface were evaluated through an expert review.
[Geographic Information Systems, Portable computers, Pipelines, agricultural products, Displays, augmented reality, geographic information systems, user interfaces, vineyards, viticulture GIS data, outdoor mobile computers, agriculture, outdoor augmented reality visualisation, mobile computing, ARVino, Wearable computers, data visualisation, movable tripod-based computer, 3D data visualisation, Virtual environment, Augmented reality, user interface, Data visualization, grape quality, Australia, Mobile computing]
Experimental evaluation of an augmented reality visualization for directing a car driver's attention
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
With recent advances of head-up display technology in cars, augmented reality becomes interesting in supporting the driving task to guide a driver's attention. We have set up an experiment to compare two different approaches to inform the driver about dangerous situations around the car. One approach used AR to visualize the source of danger in the driver's frame of reference while the other one presented information in an egocentric frame of reference. Both approaches were evaluated in user tests.
[Visualization, Navigation, Eyes, Roads, Taxonomy, automobiles, augmented reality visualization, Displays, augmented reality, road safety, Augmented reality, head-up display technology, Vehicles, experimental evaluation, head-up displays, car driver attention redirection, driver information systems, data visualisation, Automotive components, Testing]
Adaptive line tracking with multiple hypotheses for augmented reality
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
We present a real-time model-based line tracking approach with adaptive learning of image edge features that can handle partial occlusion and illumination changes. A CAD (VRML) model of the object to track is needed. First, the visible edges of the model with respect to the camera pose estimate are sorted out by a visibility test performed on standard graphics hardware. For every sample point of every projected visible 3D model line a search for gradient maxima in the image is then carried out in a direction perpendicular to that line. Multiple hypotheses of these maxima are considered as putative matches. The camera pose is updated by minimizing the distances between the projection of all sample points of the visible 3D model lines and the most likely matches found in the image. The state of every edge's visual properties is updated after each successful camera pose estimation. We evaluated the algorithm and showed the improvements compared to other tracking approaches.
[Performance evaluation, computer graphic equipment, augmented reality, tracking, adaptive learning, image sampling, edge visual property, partial occlusion, feature extraction, Lighting, graphics hardware, Hardware, Robustness, Gyroscopes, edge detection, State estimation, VRML model, Testing, virtual reality languages, CAD model, CAD, video cameras, image matching, Augmented reality, hidden feature removal, image edge feature, Graphics, real-time visible 3D model-based line tracking approach, image gradient maxima search, image illumination, Cameras, camera pose estimate, solid modelling]
A balanced approach to 3D tracking from image streams
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
Estimation of camera pose is an integral part of augmented reality systems. Vision-based methods offer a flexible and accurate method for this estimation. Current vision based methods rely on markers to reduce the computation and increase robustness of the pose estimation. However, this limits the algorithm's applicability while being expensive since the markers also require maintenance. Alternatively, reconstructed scene features can be used for pose estimation but this can lead to a loss of accuracy. To avoid this we propose a two-stage balanced tracking method which does not require any visual markers in the scene. The first stage of our method is based on the sequential recovery of structure from motion which allows the system to learn the scene from a few frames in which the markers are visible. In the next stage, the learned features are used for camera tracking. The system ensures greater accuracy and reduces error drift due to its use of the HEIV estimator which is provably unbiased to the first degree. We also make use of a novel method for the detection and removal of outliers which are unavoidable in such systems. The experiments show the superiority of our method when compared to a nonlinear method based on Levenberg-Marquardt minimization.
[Real time systems, augmented reality, tracking, outlier detection, Image reconstruction, outlier removal, vision-based method, two-stage balanced tracking method, augmented reality system, feature extraction, Levenberg-Marquardt minimization method, Robustness, video streaming, 3D tracking, Computer vision, Minimization methods, camera tracking, video cameras, image stream, image reconstruction, Augmented reality, image motion analysis, Layout, camera pose estimation, Streaming media, Cameras, Rendering (computer graphics), HEIV estimator, visual marking, scene feature reconstruction]
Face to face collaborative AR on mobile phones
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
Mobile phones are an ideal platform for augmented reality. In this paper we describe how they also can be used to support face to face collaborative AR applications. We have created a custom port of the ARToolKit library to the Symbian mobile phone operating system and then developed a sample collaborative AR game based on this. We describe the game in detail and user feedback from people who have played it. We also provide general design guidelines that could be useful for others who are developing mobile phone collaborative AR applications.
[Collaborative software, graphical user interfaces, face to face collaborative AR, Symbian mobile phone operating system, mobile phone collaborative AR application, augmented reality, Mobile handsets, Application software, Augmented reality, Guidelines, mobile computing, Operating systems, Collaboration, computer games, groupware, collaborative AR game, User interfaces, Collaborative work, Libraries, user feedback, ARToolKit library, mobile handsets]
Evaluation of mixed-space collaboration
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
Recently augmented reality (AR) technology has been used to develop the next generation collaborative interfaces. First results have shown the value of using AR for co-located tasks based on egocentric viewpoints. In contrast, virtual reality (VR) seems to offer interesting advantages for immersive collaborative experiences with egocentric viewpoints. In this paper we focus on a new area: a mixed collaboration between AR and VR environments. We present a new conceptual model of transitional interfaces that allow users to move between AR and VR viewpoints. We then describe the results of a quantitative evaluation with an AR exocentric viewpoint and a VR egocentric viewpoint for a navigational task. We also conducted a second experiment on the impact of the relationship between the interaction and visualization space in mixed collaboration. Results of these studies can provide a better understanding of how to design interfaces for multispace and transitional collaboration.
[Visualization, virtual reality, Navigation, Virtual environment, graphical user interfaces, Switches, augmented reality, augmented reality technology, visualization space, Augmented reality, egocentric viewpoint, collaborative interface, mixed-space collaboration, Space technology, Collaboration, data visualisation, Virtual reality, groupware, Collaborative work, Books]
Enhanced eyes for better gaze-awareness in collaborative mixed reality
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
The concept of "enhanced eyes" to restore gaze awareness in a collaborative mixed-reality space is proposed. Three "enhanced eyes" schemes are described: controlling the highlight in the eyes to aid awareness of eye contact, deforming the eyelids to enhance eye motion, and adjusting the rotation angle of the eyeballs to improve perception of gaze direction. The effectiveness of the schemes has been confirmed by subjective evaluations. The "enhanced eyes" emulate the natural appearance of the face and are effective not only to indicate gaze direction but also to create the feeling of gaze.
[Communication effectiveness, virtual reality, Eyes, visual perception, enhanced eyes scheme, eye motion, collaborative mixed-reality space, Augmented reality, eye, Computer displays, Space technology, Collaboration, Character generation, Virtual reality, Eyelids, groupware, gaze awareness restoration, gaze direction, eye contact, eyelid deformation, Motion control]
Augmented foam: a tangible augmented reality for product design
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
Computer aided design applications have become designers' inevitable tools for expressing and simulating innovative ideas and concepts. However, replacing traditional materials and mock-ups with 3D CAD systems, designers are faced with the intangibility problem, unable to physically interact with test products in early stages of design process. As a touchable and graspable interface based on 3D CAD data, we propose augmented foam, which applies augmented reality technologies to physical blue foams. Using augmented foam, a blue foam mock-up is overlaid with a 3D virtual object, which is rendered with the same CAD model used for mock-up production. We presented a method to correct occlusions of the virtual products by user's hand. Augmented foam was tested for a mug design and a cleaning robot design. Designers were able to inspect and evaluate the design alternatives interactively and efficiently.
[Process design, Materials testing, System testing, Design automation, graphical user interfaces, blue foam mock-up, virtual product, CAD/CAM, augmented reality, tangible augmented reality, foams, cleaning robot design, mock-up production, virtual manufacturing, rendering (computer graphics), computer aided design, CAD model, Computational modeling, Computer simulation, 3D CAD system interface, mug design, Product design, Application software, Augmented reality, hidden feature removal, product design, occlusion, Computer applications, augmented foam, 3D virtual object rendering]
Immersive mixed-reality configuration of hybrid user interfaces
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
Information in hybrid user interfaces can be spread over a variety of different, but complementary, displays, with which users interact through a potentially equally varied range of interaction devices. Since the exact configuration of these displays and devices may not be known in advance, it is desirable for users to be able to reconfigure at runtime the dataflow between interaction devices and objects on the displays. To make this possible, we present the design and implementation of a prototype mixed reality system that allows users to immersively reconfigure a running hybrid user interface.
[Visualization, dataflow visualization, virtual reality, graphical user interfaces, display object, Control systems, Augmented reality, Computer science, Computer displays, Runtime, Prototypes, data visualisation, mixed-reality configuration, Virtual reality, User interfaces, display devices, hybrid user interface, Three dimensional displays, interactive devices, interaction device]
AR Karaoke: acting in your favorite scenes
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
In this paper we present a concept for augmented reality entertainment, called AR Karaoke, where users perform their favorite dramatic scenes with virtual actors. AR Karaoke is the acting equivalent of traditional Karaoke, where the goal is to facilitate an acting experience for the user that is entertaining for both the user and audience. Prototype implementations were created to evaluate various user interfaces and design approach reveal guidelines that are relevant to the design of mixed reality applications in the domains of gaming, performance, and entertainment.
[Software prototyping, augmented reality entertainment, gaming, graphical user interfaces, entertainment, Displays, Educational institutions, augmented reality, Application software, Augmented reality, user interface, favorite dramatic scene, AR Karaoke, DVD, humanities, Layout, Prototypes, computer games, Virtual reality, User interfaces, mixed reality application design, virtual actor]
Localisation and interaction for augmented maps
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
Paper-based cartographic maps provide highly detailed information visualisation with unrivalled fidelity and information density. Moreover, the physical properties of paper afford simple interactions for browsing a map or focusing on individual details, managing concurrent access for multiple users and general malleability. However, printed maps are static displays and while computer-based map displays can support dynamic information, they lack the nice properties of real maps identified above. We address these shortcomings by presenting a system to augment printed maps with digital graphical information and user interface components. These augmentations complement the properties of the printed information in that they are dynamic, permit layer selection and provide complex computer mediated interactions with geographically embedded information and user interface controls. Two methods are presented which exploit the benefits of using tangible artifacts for such interactions.
[Computer interfaces, graphical user interfaces, optical tracking, Surfaces, augmented reality, computer-based map display, digital graphical information, spatially augmented reality, augmented map interaction, user interface component, data visualisation, Tangible User Interfaces, Projection displays, Personal digital assistants, information visualisation, Embedded computing, projection display, Inspection, computer mediated interaction, Optical Tracking, Spatially Augmented Reality, cartography, map browsing, printed map augmentation system, Augmented reality, Computer displays, geographically embedded information, Data visualization, User interfaces, paper-based cartographic map, Spatial resolution, user interface control]
Lens model selection for a markerless AR tracking system
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
This paper describes a visual markerless real-time tracking system for augmented reality applications. The system uses a firewire camera with a fisheye lens mounted at 10 fps. Visual tracking of 3D scene points is performed simultaneously with 3D camera pose estimation without any prior scene knowledge. All visual-geometric data is acquired using a structure-from-motion approach. The lens selection was driven by research results that show the superiority of a fisheye lens to a standard perspective lens for this approach. 2D features in the hemispherical image are tracked using a 2D point tracker. Based on the feature tracks, 3D camera ego-motion and 3D features are estimated.
[Real time systems, 3D camera pose estimation, augmented reality, tracking, Wearable computers, Firewire, feature extraction, Robustness, structure-from-motion approach, photographic lenses, visual-geometric data, Robot vision systems, visual markerless real-time AR tracking system, 2D point tracker, video cameras, 2D hemispherical image feature, Augmented reality, visual 3D scene point tracking, 3D camera ego-motion, 3D feature estimation, fisheye lens model selection, Layout, real-time systems, Streaming media, firewire camera, Cameras, Lenses]
Augmenting deformable objects in real-time
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
We present a real-time system that can draw virtual patterns or images on deforming real objects by estimating both the deformations and the shading parameters. We show that this is what is required to render the virtual elements so that they blend convincingly with the surrounding real textures. The whole process of uncompressing the video stream, measuring the deformations, estimating the lighting parameters, and realistically augmenting the input image takes about 100 ms on a 2.8 GHz PC. It is fully automated and does not require any manual initialization or engineering of the scene. It is also robust to large deformations, lighting changes, motion blur, specularities, and occlusions. It can therefore be demonstrated live on a simple laptop.
[Real time systems, Computer vision, Parameter estimation, Laboratories, augmenting deformable object, augmented reality, hidden feature removal, image texture, virtual element, Layout, Lighting, real-time systems, Streaming media, Rendering (computer graphics), Robustness, video streaming, video stream, Probes, real-time system]
A hybrid and linear registration method utilizing inclination constraint
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
This paper describes a new hybrid vision-based registration method with an inclination sensor. In the method, a camera is tracked by solving linear equations under inclination constraint. Linear operations are faster than the nonlinear optimization process, but the output does not usually satisfy the orthonormality constraint. On the contrary, the proposed method calculates the camera position and azimuth directly, thus the result satisfies the orthonormality constraint. Many hybrid approaches using inertia sensors have been proposed for AR/MR. However, such methods still depend on vision-based methods in initialization processes. On the other hand, the proposed method is totally hybrid in that the inclination measured by the sensor is always incorporated in pose calculation process as well as vision information. The method can be used in initialization process of conventional hybrid methods as well as it can be used as an independent registration method. The proposed method can be applied to not only inside-out-style camera tracking but also outside-in-style object tracking.
[linear registration method, outside-in-style object tracking, Image edge detection, Laboratories, image registration, Optimization methods, Humans, inside-out-style camera tracking, hybrid vision-based registration method, independent registration method, augmented reality, nonlinear optimization process, inclination sensor, image sensors, cameras, Azimuth, orthonormality constraint, Virtual reality, Position measurement, Cameras, Robustness, Gyroscopes]
Encoded LED system for optical trackers
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
Since introducing a hybrid vision-inertial tracker using passive fiducial markers, we have encountered several applications in which the use of encoded LEDs would be preferable to paper fiducials. This includes small-scale applications requiring higher precision, and applications where the size of the target, or the range of viewing angles, or operation in the dark is important. We present a novel technique to encode LEDs without any need for synchronization between the LEDs and cameras. By using amplitude modulation codes instead of blinking binary codes, the LED is always on, and can therefore be tracked after it has been decoded at an arbitrary or even non-periodic frame rate with no missed data.
[optical tracker, passive fiducial marker, Target tracking, binary codes, blinking binary codes, optical tracking, hybrid vision-inertial tracker, Pulse width modulation, Sensor fusion, Light emitting diodes, Decoding, Batteries, encoding, encoded LED system, light emitting diodes, cameras, Wires, Smart cameras, Binary codes, synchronization, small-scale application, Optical sensors]
Calibration errors in augmented reality: a practical study
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
This paper confronts some theoretical camera models to reality and evaluates the suitability of these models for effective augmented reality (AR). It analyses what level of accuracy can be expected in real situations using a particular camera model and how robust the results are against realistic calibration errors. An experimental protocol is used that consists of taking images of a particular scene from different quality cameras mounted on a 4DOF micro-controlled device. The scene is made of a calibration target and three markers placed at different distances of the target. This protocol enables us to consider assessment criteria specific to AR as alignment error and visual impression, in addition to the classical camera positioning error.
[Protocols, Target tracking, realistic calibration error, 4DOF micro-controlled device, video cameras, augmented reality, Calibration, Augmented reality, Volume measurement, Layout, Cameras, Imaging phantoms, Robustness, calibration, image resolution, Biomedical imaging, camera model]
Novel view generation from multiple omni-directional videos
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
Recently, generation of novel views from images acquired by multiple cameras has been investigated in the fields of virtual and mixed reality. Most conventional methods need some assumptions about the scene such as a static scene and limited positions of objects. In this paper, we propose a new method for generating novel view images of a dynamic scene with a wide view, which does not depend on the scene. The images acquired from omni-directional cameras are first divided into static regions and dynamic regions. The novel view images are then generated by applying a morphing technique to static regions and by computing visual hulls for dynamic regions in realtime. In experiments, we show that a prototype system can generate novel view images in real-time from live video streams.
[Real time systems, image morphing technique, image morphing, Videos, Image segmentation, Information science, visual hulls, Layout, Prototypes, real-time systems, Image generation, Streaming media, multiple omni-directional videos, Cameras, Rendering (computer graphics), video streams, video streaming, real-time system]
A pipeline for rapidly incorporating real objects into a mixed environment
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
A method is presented to rapidly incorporate real objects into virtual environments using laser scanned 3D models with color-based marker tracking. Both the real objects and their geometric models are put into a mixed environment (ME). In the ME, users can manipulate the scanned, articulated real objects, such as tools, parts, and physical correlates to complex computer-aided design (CAD) models. Our aim is to allow engineering teams to effectively conduct hands-on assembly design verification. This task would be simulated at a high degree of fidelity, and would benefit from the natural interaction afforded by a ME with many specific real objects.
[Solid modeling, Design automation, virtual reality, Pipelines, NASA, Laser feedback, CAD, mixed environment, color-based marker tracking, computer-aided design, laser scanned 3D model, Augmented reality, Design engineering, Laser noise, hands-on assembly design verification, virtual environment, Laser modes, image colour analysis, Assembly, solid modelling, CAD models]
Augmented reality techniques in games
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
As a consequence of technical difficulties such as unreliable tracking, many AR applications get stuck in the "how to implement" phase rather than progressing to the "what to show" phase driven by information visualization needs rather than basic technology. In contrast, most of today's computer games are set in a fairly realistic 3D environment, and unlike many AR applications, game interfaces undergo extensive usability testing. This creates the interesting situation that games can be perfect simulators of AR applications, because they are able to show perfectly registered "simulated AR" overlays on top of a real-time environment. This work examines how some visualization and interaction techniques used in games can be useful for real AR applications.
[Computer interfaces, Visualization, AR application, graphical user interfaces, Displays, augmented reality, Application software, augmented reality techniques, Augmented reality, realistic 3D environment, usability testing, real-time environment, Keyboards, computer games, real-time systems, Games, game interfaces, Cameras, Mice, Usability]
The synthetic character Ritchie: first steps towards a virtual companion for mixed reality
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
Unlike most existing work on traversable interfaces, we focus on the use of synthetic characters to accompany the user in mixed reality (MR) applications. We examine virtual companions as a promising means to design smooth transitions between different worlds and to avoid orientation problems. We propose a taxonomy to describe the spatial relationship between character and user which has an important impact on the style of interaction. To flexibly transfer user and character into different spaces, we have created a platform that supports the design of interfaces derived from the proposed taxonomy as well as transitions between them.
[virtual reality, Taxonomy, Humans, Switches, Displays, user interfaces, traversable interfaces, Augmented reality, Geometry, mixed reality application, Space technology, synthetic character Ritchie, design interfaces, Virtual reality, User interfaces, Cities and towns]
The SQUASH 1000 tangible user interface system
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
A hand-held object whose pose can be determined automatically is useful for augmented reality (AR) and other applications needing human-computer interaction. Some existing such input devices use active (ex. LED lighting) or passive markers to be recognized in a video image by computer vision. Markers are typically mounted onto flat objects which are not ergonomic, or can only have limited number of sides due to the small library and inter-marker confusion rate of the marker system used. A system is presented based on the ARTag fiducial marker system where objects of arbitrary shape can be covered with many small markers, the pose of the object is recovered automatically and can be used as an input device. Other tangible user interface systems require specialized hardware, whereas this approach needs only a printer, a video camera or Webcam, and a large garden vegetable. The utility of this system both for measuring pose and for 3D modeling is shown.
[Computer vision, LED lamps, Image recognition, Shape, video cameras, Light emitting diodes, augmented reality, video camera, Webcam, Application software, Augmented reality, 3D modeling, SQUASH 1000 tangible user interface system, human-computer interaction, video image, Ergonomics, computer vision, User interfaces, ARTag fiducial marker system, Libraries, human computer interaction]
Autocalibration of an electronic compass for augmented reality
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
Electronic compass is often used to provide the absolute heading reference for tracking the user's head and hands in virtual reality (VR) and augmented reality (AR), especially for outdoor AR applications. However, compass is vulnerable to environment magnetism disturbance. Existing compass calibration methods require complex steps and true heading reference which is often impossible to be obtained in outdoor AR applications, and is useful only when compass is in horizontal plane. An autocalibration method without the need of heading reference and redundant sensors is proposed in this paper. First the compass error model based on physical principle is presented, then the algorithm to calculate the compensation coefficients with a set of sample measurements of the sensors in the compass is described. Because the influence of the environmental disturbance has been effectively compensated, the calibrated compass can provided accurate heading even when it is under large tilt attitude.
[Accelerometers, virtual reality, Magnetometers, Control systems, augmented reality, Magnetic heads, Calibration, Table lookup, compasses, Augmented reality, sensors, Magnetic sensors, Virtual reality, electronic compass autocalibration, absolute heading reference, calibration, Gravity]
A mediated reality environment using a loose and sketchy rendering technique
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
In this poster, we present sketchy-ar-us, a modified, realtime version of the Loose and Sketchy algorithm used to render graphics in an AR environment. The primary challenge was to modify the original algorithm to produce a NPR effect at interactive frame rate. Our algorithm renders moderately complex scenes at multiple frames per second. Equipped with a handheld visor, visitors can see the real environment overlaid with virtual objects with both the real and virtual content rendered in a non-photorealistic style.
[Algorithm design and analysis, Real time systems, sketchy-ar-us realtime version, Filtering, rendering technique, mediated reality environment, augmented reality, interactive frame rate, Augmented reality, Image segmentation, Filters, Layout, Computer graphics, Rendering (computer graphics), Cameras, Loose and Sketchy algorithm, rendering (computer graphics)]
Reality tooning: fast non-photorealism for augmented video streams
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
Recently, we have proposed a novel approach to generating augmented video streams. The output images are a non-photorealistic reproduction of the augmented environment. Special stylization methods are applied to both the background camera image and the virtual objects. This way, the graphical foreground and the real background images are rendered in a similar style, so that they are less distinguishable from each other. Here, we present a new algorithm for the cartoon-like stylization of augmented reality images, which uses a novel post-processing filter for cartoon-like color segmentation and high-contrast silhouettes. In order to make a fast post-processing of rendered images possible, the programmability of modern graphics hardware is exploited. The system is capable of generating a stylized augmented video stream of high visual quality at real-time frame rates.
[Real time systems, augmented reality images, computer graphic equipment, Pipelines, augmented reality, virtual object, Filters, reality tooning, image segmentation, nonphotorealistic reproduction, cartoon-like color segmentation, graphics hardware, video streaming, image colour analysis, rendering (computer graphics), background camera image, Image edge detection, Color, realistic images, Augmented reality, rendered images, Graphics, augmented video streams, Streaming media, Rendering (computer graphics), Cameras, special stylization method]
Camera-marker alignment framework and comparison with hand-eye calibration for augmented reality applications
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
An integral part of every augmented reality system is the calibration between camera and camera-mounted tracking markers. Accuracy and robustness of the AR overlay process is greatly influenced by the quality of this step. In order to meet the very high precision requirements of medical skill training applications, we have set up a calibration environment based on direct sensing of LED markers. A simulation framework has been developed to predict and study the achievable accuracy of the backprojection needed for the scene augmentation process. We demonstrate that the simulation is in good agreement with experimental results. Even if a slight improvement of the precision has been observed compared to well-known hand-eye calibration methods, the subpixel accuracy required by our application cannot be achieved even when using commercial tracking systems providing marker positions within very low error limits.
[Infrared detectors, Computer vision, hand-eye calibration, Medical simulation, Laboratories, biomedical education, medical skill training application, augmented reality, Calibration, camera-marker alignment framework, subpixel accuracy, Application software, LED marker, Augmented reality, light emitting diodes, cameras, Firewire, Cameras, Robustness, calibration, medical computing]
Projected augmentation II - a scalable architecture for multi projector based AR-systems based on 'Projected Applications'
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
Augmenting or annotating physical objects using steerable video projector and camera combinations works well in the range of the projection system. However, since these systems are usually mounted in fixed locations, users have to stay within a certain radius around that location. By using several of these AR-Projection systems, we can make the application roam these systems and thus follow the user beyond these limits. Using this method, we furthermore remove the need to hold the augmented objects in a suitable direction towards the projector and can reduce the possibility of shadowing the projection by the user. We describe an architecture based on our 'Projected Applications' which coordinates the systems used to project the augmentations and keeps the projected applications' states in sync between the projection systems.
[Computer interfaces, projected augmentation, multiprojector based AR-system, Containers, Control systems, Turning, augmented reality, Application software, Security, Shadow mapping, optical projectors, cameras, Current measurement, video projector, User interfaces, display devices, Cameras, camera]
Annotating user-viewed objects for wearable AR systems
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
By realizing augmented reality on wearable computers, it is possible to overlay annotations on the real world based on the user's current position and orientation. However, it is difficult for the user to understand links between annotations and real objects intuitively when the scene is complicated or many annotations are overlaid at the same time. This paper describes a view management method which emphasizes user-viewed real objects and their annotations using 3D models of the real scene. The proposed method highlights the objects viewed by the user. In addition, when the viewed object is occluded by other real objects, the object is complemented by using an image, which is made from 3D models, on the overlaid image.
[user-viewed objects, Buildings, Project management, computational geometry, wearable AR system, 3D model, augmented reality, Augmented reality, hidden feature removal, Image sensors, wearable computers, Wearable computers, Layout, Prototypes, occlusion, Image generation, Virtual reality, Position measurement, view management method, wearable computer, solid modelling]
Specular reflection elimination for projection-based augmented reality
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
In projection-based augmented reality, specular reflection may distract the users. This paper demonstrates that the specular reflection can be eliminated by redundantly illuminating the projection surface using multiple overlapping projectors mounted at different locations. Our initial system using two projectors is presented. The system automatically determines the pixels which include specular reflection in one projector, blank the light falling on the pixel and boost the other projector's output so that the added light is projected onto the pixel consistently.
[multiple overlapping projector, Virtual environment, projection-based augmented reality, projection surface, Optimization methods, computational geometry, augmented reality, Calibration, lighting, Augmented reality, Image reconstruction, optical projectors, specular reflection elimination, Geometry, cameras, Surface reconstruction, Cameras, Visual effects, Optical reflection]
ARpm: an augmented reality interface for polygonal modeling
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
We present a prototype system called ARpm, (augmented reality for polygonal modeling). Created as a front-end to a commercially available 3D animation program (3D Studio Max), it does not require it's modification through inaccessible software code. Users look through a head mounted display (HMD) and the object being modeled appears as a 3D augmentation in the real world. This object can be manipulated using a tangible interface of marker panels that correspond to the modeling tools in 3D Studio Max, and a wireless pointer device. Unlike virtual reality which brings the user into the virtual world, this system places the 3D model in the user's world while allowing the use of a large range of complex 3D polygonal modeling tools provided by 3D Studio Max.
[3D animation program, graphical user interfaces, augmented reality interface, augmented reality, helmet mounted displays, 3D Studio Max, Augmented reality, computer animation, polygonal modeling, wireless pointer device, ARpm prototype system, head mounted display, 3D augmentation, solid modelling]
AR storyboard: an augmented reality based interactive storyboard authoring tool
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
In early stages of production, storyboards are used for visually describing the story and the script. In this paper, an augmented reality based storyboard-authoring tool is introduced. Proposed tool is easy-to-use, and provides intuitive interface for scene composition and camera pose/motion control. Using AR Storyboard, non-experienced users may compose 3D scenes for a Storyboard using interfaces in his/her real environments.
[Pediatrics, graphical user interfaces, interactive storyboard authoring tool, augmented reality, Augmented reality, authoring systems, Computer science, cameras, Layout, Production, Aging, Cameras, Animation, Rendering (computer graphics), Motion control, AR storyboard]
Shading and shadowing of architecture in mixed reality
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
We propose a simple method to express shading and shadowing of virtual objects in mixed reality especially appropriate for static architecture models in outdoor scenes. We create the shadows of the virtual objects in a fast and efficient way using a set of pre-rendered basis images and shadowing planes. The proposed method is limited in interactivity but can operate in near real-time.
[virtual reality, pre-rendered basis image, outdoor scene, architecture shading, virtual object, Shadow mapping, Light sources, Geometry, Jacobian matrices, shadowing plane, Layout, mixed reality, Lighting, Virtual reality, Image generation, Rendering (computer graphics), static architecture model, rendering (computer graphics), Assembly, solid modelling]
A hybrid image-based and model-based telepresence system using two-pass video projection onto a 3D scene model
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
A telepresence system is presented that has the advantage of both model-based and image-based approaches, namely, free viewpoint control and real-time color update with live video. A remote place is presented as a virtual environment by using live video projection captured by a head-worn camera onto the static 3D geometry. The observer can then observe the remote place in cooperation with the remote camera man, and give him a set of 3D instructions by a mouse.
[Real time systems, real-time color update, natural scenes, virtual reality, Buffer storage, free viewpoint control, computational geometry, two-pass live video projection, Prototypes, image-based telepresence, 3D scene model, rendering (computer graphics), Virtual environment, model-based telepresence system, image texture, optical projectors, Geometry, Layout, virtual environment, Cameras, Rendering (computer graphics), Mice, Optical sensors, static 3D geometry, solid modelling]
Real-time rendering of realistic trees in mixed reality
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
Mixed reality applications put very high demands on both the visual realism and the rendering times of computer graphics elements that are to be perceived as part of the physical scene. This paper presents novel techniques to render photorealistic trees in real-time mixed reality. Animation of the tree branches leads to a realistic effect of the tree swaying in the wind. To enhance the effect of blending the tree into a video texture, we present three levels of real-time filtering of the tree and its shadow, which has a great impact on the perceived realism.
[real-time filtering, virtual reality, Filtering, realistic tree, visual realism, Application software, realistic images, Augmented reality, image texture, Shadow mapping, computer animation, computer graphics, Tree graphs, tree blending, Layout, mixed reality, video texture, Virtual reality, Computer graphics, Rendering (computer graphics), Animation, rendering (computer graphics), real-time rendering, photorealistic tree animation]
A step towards a multimodal AR interface: a new handheld device for 3D interaction
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
In this paper, we describe the AR Mask, a novel handheld augmented reality (AR) input and display device that not only provides support for a full range of traditional interaction techniques, but also facilitates new metaphors. Unlike other AR display and input technologies, our device consolidates input and output within a single piece of hardware. We demonstrate how this design provides a closed control loop between the user's input modalities and natural sensory receptors.
[natural sensory receptor, Switches, Grasping, Context awareness, Displays, augmented reality, user interfaces, user input modality, Augmented reality, AR Mask, Handheld computers, multimodal augmented reality interface, handheld device, Collaboration, Virtual reality, Speech, Hardware, interactive devices, 3D interaction]
Spatial measurements for medical augmented reality
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
This paper presents a set of augmented reality (AR) based interaction techniques for spatial analysis of medical datasets. Computer-aided medical planning tools such as our virtual liver surgery planning system require precise and intuitive interaction for the quantitative inspection of anatomical and pathological structures. We argue that AR is a superior tool compared to desktop 2D or 3D visualization for performing such analysis, because it allows true direct manipulation of 3D virtual objects in space, while rendering the medical data in the familiar context of the user's own body.
[Data analysis, 3D visualization, Oncological surgery, Liver, Inspection, augmented reality, Path planning, Augmented reality, medical dataset rendering, Volume measurement, data visualisation, computer-aided medical planning tool, quantitative inspection, medical augmented reality, Distance measurement, Performance analysis, spatial measurements, rendering (computer graphics), medical computing, 3D virtual object, Biomedical imaging, surgery, virtual liver surgery planning system]
Product associated displays in a shopping scenario
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
In this paper, we introduce the concept of product associated displays -PADs - as a way of providing visual feedback to users interacting with physical objects in an instrumented environment. PADs are projected public displays created at locations that can be intuitively associated with the objects they show information about. The concept is illustrated in a shopping scenario.
[Visualization, Instruments, product associated display, Humans, user interaction, user interfaces, visual feedback, Augmented reality, projected public display, Computer displays, Feedback, computer displays, shopping scenario, Cameras, interactive devices, instrumented environment, Radiofrequency identification]
Butterfly effect: an augmented reality puzzle game
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
Butterfly effect is a 3D puzzle game using augmented reality. The key motivation was to create a game that leverages the structure of the physical world during gameplay without requiring the computer to have a detailed model of the space. The butterflies are virtual, but the space the player navigates is physical. The player travels her environment, collecting the butterflies. For butterflies that are out of reach, the player can rotate the virtual world in 90 degree chunks about an arbitrary axis to bring them to an accessible location.
[Navigation, Laboratories, Scattering, Humans, augmented reality, virtual world, Augmented reality, 3D puzzle game, Space technology, Physics computing, computer games, butterfly effect, Games, Telephone sets, Space exploration, gameplay]
Driving view simulation synthesizing virtual geometry and real images in an experimental mixed-reality traffic space
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
We propose an efficient and effective image generation system for an experimental mixed-reality traffic space. Our enhanced traffic/driving simulation system represents the view through a hybrid that combines virtual geometry with real images to realize high photo-reality with little human cost. Images for datasets are captured from the real world, and the view for the simulation system is created by synthesizing image datasets - with a conventional driving simulator.
[virtual geometry, Solid modeling, mixed-reality traffic space, virtual reality, image generation system, real image, Humans, computational geometry, rendering, traffic engineering computing, realistic images, Image reconstruction, Geometry, Microscopy, Road vehicles, data visualisation, driving view simulation, Virtual reality, Image generation, Traffic control, Cameras, rendering (computer graphics), image dataset synthesis]
Design and implementation of a widget set for steerable projector-camera units
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2005
We describe the design and implementation of graphical interaction widgets for use with a steerable projector-camera unit. The design of our widgets is adapted to provide the right visual cues when projected and they are controlled by the user's hand. The widgets' input regions are arranged in an ergonomic way and they use a simple but robust computer vision technique for interaction.
[visual cue, Computer vision, Image resolution, Instruments, graphical user interfaces, graphical interaction widget, ergonomics, Ubiquitous computing, computer vision technique, optical projectors, Computer science, cameras, Ergonomics, Feedback, computer vision, steerable projector-camera unit, Cameras, Robustness, interactive devices, Informatics]
Proceedings - ISMAR 2006 - International Symposium on Mixed and Augmented Reality
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
false
[]
Message from the General Chairs
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
Presents the welcome message from the conference proceedings.
[]
Message from the Program Chairs
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
Presents the welcome message from the conference proceedings.
[]
Workshop - Industrial augmented reality
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
false
[]
Tutorials - MAR Tutorial 1 (half day) & ISMAR Tutorial 2 (half day)
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
Provides an abstract for each of the presentations and a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.
[]
Keynote speeches - The poor man's palace: special effects in the real world / Do we have six brains?
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
Provides an abstract for each of the keynote presentations and may include a brief professional biography of each
[]
Quantification of visual capabilities using augmented reality displays
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
In order to be able to perceive and recognize objects or surface properties of objects, one must be able to resolve the features. These perceptual tasks can be difficult for both graphical representations and real objects in augmented reality (AR) displays. This paper presents the results of objective measurements and two user studies. The first evaluation explores visual acuity and contrast sensitivity; the second explores color perception. Both experiments test users' capabilities with their natural vision against their capabilities using commercially-available AR displays. The limited graphical resolution, reduced brightness, and uncontrollable visual context of the merged environment demonstrably reduce users' visual capabilities. The paper concludes by discussing the implications for display design and AR applications, as well as outlining possible extensions to the current studies.
[Brightness, Humans, commercially-available AR displays, Retina, augmented reality, color perception, Augmented reality, graphical resolution, Computer displays, visual capabilities quantification, augmented reality displays, graphical representations, Computer graphics, computer vision, face recognition, Three dimensional displays, Space exploration, image colour analysis, Optical sensors, image resolution, Testing]
Effective control of a car driver's attention for visual and acoustic guidance towards the direction of imminent dangers
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
In cars, augmented reality is becoming an interesting means to enhance active safety in the driving task. Guiding a driver's attention to an imminent danger somewhere around the car is a potential application. In a research project with the automotive industry, we are exploring different approaches towards alerting drivers to such dangers. First results were presented last year. We have extended two of these approaches. One uses AR to visualize the source of danger in the driver's frame of reference while the other one presents information in a bird's eye schematic map. Our extensions were the incorporation of a real head-up display, improved visual perception and acoustic support. Both schemes were evaluated both with and without 3D encoded sound. This paper reports on a user test in which 24 participants provided objective and subjective measurements. The results indicate that the AR-based three-dimensional presentation scheme with and without sound support systematically outperforms the bird's eye schematic map.
[Visualization, Acoustic testing, visual perception, acoustic guidance, head-up display, Displays, augmented reality, Augmented reality, Automotive engineering, Visual perception, Vehicle safety, driver information systems, Animation, Acoustic measurements, acoustic support, Automotive components, visual guidance]
User evaluations on form factors of tangible magic lenses
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
Magic Lens is a small inset window embedded in a large context display, which provides an alternative view to the region of interest selected from the context view. This metaphor is used for 3D visualization in our Augmented Virtual Environment infrastructure, SCAPE (Stereoscopic Collaboration in Augmented and Projective Environments), which is composed of an immersive room display for a high level of detail, life-size virtual world and a workbench display for simplified god-like view to the world. A tangible Magic Lens is used on the workbench display to allow direct and intuitive selection of continuous levels of detail, bridging the gap between the two extreme levels of detail in SCAPE. This paper presents our first step to the user evaluations of tangible Magic Lens. We conducted two sets of user evaluations, one mainly testing the lens aspect ratio, and another for the lens size. For both of the tests, two types of tasks are conducted: information gathering and relating the detailed information with the context. We found that the aspect ratio of a lens plays more important role in user preference for smaller lenses than for larger ones. Meanwhile, the size of a lens is the most important factor that affects the user performance in the two types of tasks.
[Visualization, immersive room display, 3D visualization, Navigation, Virtual environment, graphical user interfaces, augmented virtual environment infrastructure, form factor, three-dimensional displays, augmented reality, user evaluation, Augmented reality, small inset window, Optical design, data visualisation, tangible magic lens, stereoscopic collaboration, Collaborative work, Three dimensional displays, large context display, Lenses, Graphical user interfaces, Testing]
Evaluation of three input techniques for selection and annotation of physical objects through an augmented reality view
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
This paper presents results from a study into the usability issues of two tasks (selection and annotation of a physical object) for users operating mobile augmented reality systems. The study compared the following three different modes of cursor manipulation: a handheld mouse, a head cursor, and an image-plane vision-tracked device. The selection task was evaluated based on number of mouse button clicks, completion time, and a subjective survey. The annotation task was evaluated based on accuracy of the annotation, completion time, and a subjective survey.
[human factors, augmented reality, user interfaces, mobile computing, Wearable computers, handheld mouse, Virtual reality, Computer graphics, usability issue, interactive devices, wearable computer, image-plane vision-tracked device, Head, human factor, physical object selection, cursor manipulation, input device, user interface technology, Augmented reality, mobile augmented reality system, wearable computers, head cursor, physical object annotation, User interfaces, Collaborative work, Cameras, Mice, Usability]
Automatic online walls detection for immediate use in AR tasks
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
This paper proposes a method to automatically detect and reconstruct planar surfaces for immediate use in AR tasks. Traditional methods for plane detection are typically based on the comparison of transfer errors of a homography, which make them sensitive to the choice of a discrimination threshold. We propose a very different approach: the image is divided into a grid and rectangles that belong to the same planar surface are clustered around the local maxima of a Hough transform. As a result, we simultaneously get clusters of coplanar rectangles and the image of their intersection line with a reference plane, which easily leads to their 3D position and orientation. Results are shown on both synthetic and real data.
[coplanar rectangle, Buildings, augmented reality, discrimination threshold, Surface roughness, object detection, Rough surfaces, image reconstruction, Hough transform, Image reconstruction, Augmented reality, automatic online wall detection, Surface reconstruction, planar surface reconstruction, pattern clustering, Layout, Hough transforms, Object detection, Cameras, plane detection, Detection algorithms]
Predicting and estimating the accuracy of n-occular optical tracking systems
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
Marker-based optical tracking systems are widely used in augmented reality, medical navigation and industrial applications. We propose a model for the prediction of the target registration error (TRE) in these kinds of tracking systems by estimating the fiducial location error (FLE) from two-dimensional errors on the image plane and propagating that error to a given point of interest. We have designed a set of experiments in order to estimate the actual parameters of the model for any given tracking system. We present the results of a study which we used to demonstrate the effect of different sources of error. The method is applied to real applications to show the usefulness for any kind of augmented reality system. We also present a set of tools that can be used to visualize the accuracy at design time.
[Estimation error, Visualization, Computer vision, Target tracking, Biomedical optical imaging, n-occular marker-based optical tracking system, image plane error, image registration, optical tracking, object pose estimation, augmented reality, Covariance matrix, Augmented reality, target registration error prediction model, fiducial location error estimation, augmented reality system, Optical propagation, pose estimation, target tracking, Cameras, Biomedical imaging]
Online camera pose estimation in partially known and dynamic scenes
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
One of the key requirements of augmented reality systems is a robust real-time camera pose estimation. In this paper we present a robust approach, which does neither depend on offline pre-processing steps nor on pre-knowledge of the entire target scene. The connection between the real and the virtual world is made by a given CAD model of one object in the scene. However, the model is only needed for initialization. A line model is created out of the object rendered from a given camera pose and registrated onto the image gradient for finding the initial pose. In the tracking phase, the camera is not restricted to the modeled part of the scene anymore. The scene structure is recovered automatically during tracking. Point features are detected in the images and tracked from frame to frame using a brightness invariant template matching algorithm. Several template patches are extracted from different levels of an image pyramid and are used to make the 2D feature tracking capable for large changes in scale. Occlusion is detected already on the 2D feature tracking level. The features' 3D locations are roughly initialized by linear triangulation and then refined recursively over time using techniques of the Extended Kalman Filter framework. A quality manager handles the influence of a feature on the estimation of the camera pose. As structure and pose recovery are always performed under uncertainty, statistical methods for estimating and propagating uncertainty have been incorporated consequently into both processes. Finally, validation results on synthetic as well as on real video sequences are presented.
[Real time systems, Uncertainty, Brightness, image registration, online robust real-time camera pose estimation, object rendering, augmented reality, image template patch extraction, object detection, virtual world, 2D feature tracking, tracking, brightness invariant template matching algorithm, dynamic scene, cameras, augmented reality system, feature extraction, statistical method, pose estimation, point feature detection, Robustness, Kalman filters, gradient methods, rendering (computer graphics), Quality management, Computer vision, CAD model, CAD, image pyramid, linear triangulation, nonlinear filters, partially known scene, image matching, Augmented reality, hidden feature removal, image gradient registration, Layout, real-time systems, extended Kalman filter, Cameras, Rendering (computer graphics), statistical analysis, occlusion detection]
An all-in-one solution to geometric and photometric calibration
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
We propose a fully automated approach to calibrating multiple cameras whose fields of view may not all overlap. Our technique only requires waving an arbitrary textured planar pattern in front of the cameras, which is the only manual intervention that is required. The pattern is then automatically detected in the frames where it is visible and used to simultaneously recover geometric and photometric camera calibration parameters. In other words, even a novice user can use our system to extract all the information required to add virtual 3D objects into the scene and light them convincingly. This makes it ideal for Augmented Reality applications and we distribute the code under a GPL license.
[multiple cameras, geometric-photometric calibration, virtual 3D objects, image registration, photometric camera calibration parameters, Manuals, Licenses, augmented reality, Calibration, Data mining, Augmented reality, Geometry, cameras, textured planar pattern, Layout, augmented reality applications, Cameras, Robustness, Photometry]
A registration evaluation system using an industrial robot
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
This paper describes an evaluation system using an industrial robot, constructed for the purpose of evaluating registration technology for Mixed Reality. In this evaluation system, the tip of the robot arm plays the role of the user's head, where a head- mounted display is mounted. By using an industrial robot, we can obtain the ground truth of the camera pose with a high level of accuracy and robustness. Additionally, we have the ability to play back the same specified operations repeatedly under identical conditions. In addition to the system implementation, we propose evaluation methods for motion robustness, relative orientation robustness, relative distance robustness, jitter, and an overall evaluation. We verify the validity of this system through some experiments.
[motion robustness, Head, registration evaluation system, Robot vision systems, image registration, head tracking, Jitter, Displays, augmented reality, relative orientation robustness, Service robots, Fixtures, industrial robots, mixed reality, Virtual reality, computer vision, Chromium, Cameras, Robustness, relative distance robustness, robot arm, industrial robot]
Performance analysis of an outdoor augmented reality tracking system that relies upon a few mobile beacons
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
We describe and evaluate a new tracking concept for outdoor Augmented Reality. A few mobile beacons added to the environment correct errors in head-worn inertial and GPS sensors. We evaluate the accuracy through detailed simulation of many error sources. The most important parameters are the errors in measuring the beacon and user's head positions, and the geometric configuration of the beacons around the point to augment. Using Monte Carlo simulations, we identify combinations of beacon configurations and error parameters that meet a specified goal of 1 m net error at 100 m range.
[outdoor augmented reality tracking system, mobile beacons, Military computing, tracking concept, GPS sensors, Laboratories, augmented reality, Sensor systems, Augmented reality, Global Positioning System, head-worn inertial sensors, Computer displays, Monte Carlo methods, geometric configuration, Monte Carlo simulations, head positions, target tracking, Computer errors, Infrared sensors, Unmanned aerial vehicles, Performance analysis, military computing]
Spatial relationship patterns: elements of reusable tracking and calibration systems
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
With tracking setups becoming increasingly complex, it gets more difficult to find suitable algorithms for tracking, calibration and sensor fusion. A large number of solutions exists in the literature for various combinations of sensors, however, no development methodology is available for systematic analysis of tracking setups. When modeling a system as a spatial relationship graph (SRG), which describes coordinate systems and known transformations, all algorithms used for tracking and calibration correspond to certain patterns in the graph. This paper introduces a formal model for representing such spatial relationship patterns and presents a small catalog of patterns frequently used in augmented reality systems. We also describe an algorithm to identify patterns in SRGs at runtime for automatic construction of data flows networks for tracking and calibration.
[spatial relationship graph pattern, Instruments, Buildings, Software algorithms, image fusion, Sensor fusion, data flow graphs, augmented reality, sensor fusion, Sensor systems, Calibration, tracking, Augmented reality, Runtime, reusable tracking, calibration system, augmented reality system, pose estimation, Cameras, pattern identification algorithm, data flow network, Data flow computing, calibration]
A mobile markerless AR system for maintenance and repair
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
We present a solution for AR based repair guidance. This solution covers software as well as hardware related issues. In particular we developed a markerless CAD based tracking system which can deal with different illumination conditions during the tracking stage, partial occlusions and rapid motion. The system is also able to automatically recover from occasional tracking failures. On the hardware side the system is based on an off the shelf notebook, a wireless mobile setup consisting of a wide-angle video camera and an analog video transmission system. This setup has been tested with a monocular full-color video-see-through HMD and additionally with a monochrome optical-see-through HMD. Our system underwent several extensive test series under real industrial conditions and proved to be useful for different maintenance and repair scenarios.
[Computer vision, System testing, partial occlusions, Tracking, markerless CAD based tracking system, Computer aided manufacturing, CAD, maintenance engineering, computerised instrumentation, augmented reality, monochrome optical-see-through HMD, monocular full-color video-see-through HMD, Augmented reality, Manufacturing industries, mobile markerless AR system, Lighting, Computer applications, Cameras, Hardware, rapid motion, maintenance, repair guidance]
Going out: robust model-based tracking for outdoor augmented reality
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
This paper presents a model-based hybrid tracking system for outdoor augmented reality in urban environments enabling accurate, realtime overlays for a handheld device. The system combines several well-known approaches to provide a robust experience that surpasses each of the individual components alone: an edge-based tracker for accurate localisation, gyroscope measurements to deal with fast motions, measurements of gravity and magnetic field to avoid drift, and a back store of reference frames with online frame selection to re-initialize automatically after dynamic occlusions or failures. A novel edge-based tracker dispenses with the conventional edge model, and uses instead a coarse, but textured, 3D model. This yields several advantages: scale-based detail culling is automatic, appearance-based edge signatures can be used to improve matching and the models needed are more commonly available. The accuracy and robustness of the resulting system is demonstrated with comparisons to map-based ground truth data.
[Tracking, urban environment, gyroscope measurement, 3D texture model, Sensor fusion, augmented reality, tracking, Information systems, handheld device, accurate localisation, Detectors, appearance-based edge signature, Robustness, edge detection, outdoor augmented reality, Image edge detection, gravity field measurement, robust model-based hybrid tracking system, magnetic field measurement, online frame selection, image matching, edge-based tracker, appearance based-line detection, Augmented reality, hidden feature removal, image texture, Global Positioning System, point-based image matching, Magnetic field measurement, dynamic occlusion, scale-based detail culling, Magnetic sensors, solid modelling]
LightSense: enabling spatially aware handheld interaction devices
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
The vision of spatially aware handheld interaction devices has been hard to realize. The difficulties in solving the general tracking problem for small devices have been addressed by several research groups and examples of issues are performance, hardware availability and platform independency. We present LightSense, an approach that employs commercially available components to achieve robust tracking of cell phone LEDs, without any modifications to the device. Cell phones can thus be promoted to interaction and display devices in ubiquitous installations of systems such as the ones we present here. This could enable a new generation of spatially aware handheld interaction devices that would unobtrusively empower and assist us in our everyday tasks.
[Pervasive computing, Computer interfaces, Displays, Light emitting diodes, Ubiquitous computing, LED displays, LightSense, tracking, Augmented reality, spatially aware handheld interaction device, tracking problem, robust cell phone LED tracking, Handheld computers, Cellular phones, Virtual reality, Cameras, human computer interaction, interactive devices, cellular radio]
Interactive laser-projection for programming industrial robots
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
A method for intuitive and efficient programming of industrial robots based on Augmented Reality (AR) is presented, in which tool trajectories and target coordinates are interactively visualized and manipulated in the robot's environment by means of laser projection. The virtual information relevant for programming, such as trajectories and target coordinates, is projected into the robot's environment and can be manipulated interactively. For an intuitive and efficient user input to the system, spatial interaction techniques have been developed, which enable the user to virtually draw the desired motion paths for processing a work piece surface, directly onto the respective object. The discussed method has been implemented in an integrated AR-user interface and has been initially evaluated in an experimental programming scenario. The obtained results indicate that it enables significantly faster and easier programming of processing tasks compared to currently available shop-floor programming methods.
[Optical mixing, virtual information, Displays, augmented reality, robot programming, user interfaces, industrial robot programming, Robotics and automation, Augmented reality, Robot programming, Service robots, Robot kinematics, industrial robots, Data visualization, data visualisation, target coordinate, tool trajectory, User interfaces, motion path, Trajectory, interactive laser-projection, interactive visualization, AR-based user interface]
Photometric inconsistency on a mixed-reality face
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
A mixed-reality face (MR face) is a mosaic face with real and virtual facial parts, presented by overlaying a virtual facial part on a real face using mixed-reality techniques. An MR face is an effective means to improve communication in mixed-reality space by restoring the eye expressions lost when wearing HMDs. Photometric registration between the real and virtual parts is important because our eyes are very sensitive, even to small changes in human faces. However, efforts to achieve perfect 'physical' photometric registration on an MR face are not feasible in an ordinary MR space. Therefore, it is essential to clarify the sensitivity of our eyes to the photometric inconsistencies on an MR face, and to concentrate on resolving them. In this paper, we first present the results of a systematic experiment that evaluated our sensitivity to the photometric inconsistencies on an MR face. Then, a technique to resolve the inconsistency and an experimental system to demonstrate the effectiveness of an MR face are described.
[virtual reality, Eyes, Face recognition, image registration, Humans, Color, Displays, Extraterrestrial measurements, mosaic face, photometric registration, Reflectivity, Space technology, image segmentation, photometric inconsistency, mixed-reality face, Virtual reality, face recognition, Photometry, virtual facial part]
Texture overlay for virtual clothing based on PCA of silhouettes
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
In this paper, we propose a method for overlaying an arbitrary texture image onto a surface of a plain T-shirt worn by a user. For overlaying arbitrary textures onto the surface of the T-shirt, we need to know the deformation of the surface. For estimating the deformation of the surface from the input images, we use a two-phase process: learning and searching. In the learning phase, the system learns the relationship between the deformation of the surface and the silhouette of the T-shirt region in the image. A database of a number of training images in which a person wearing a T-shirt with markers moves through a variety of positions is used for this learning. Using the database, the system can learn the relationship between the shape of the silhouette and the surface deformation that is provided by the 2D positions of the markers on the surface of the T-shirt. In the searching phase, the silhouette of the user's T-shirt is extracted from the input image, and then, a search for a similar silhouette in the database is conducted in the subspace of the silhouette, which is computed using a PCA of the database. By using the proposed method for estimating the deformation of the surface of the T-shirt, we perform experiments for overlaying virtual clothing.
[texture overlay, Shape, estimation theory, Biological system modeling, Motion estimation, Clothing, Humans, surface deformation estimation, visual databases, augmented reality, Augmented reality, image texture, virtual clothing, PCA, arbitrary texture image, Image databases, feature extraction, Surface fitting, clothing, learning phase, Surface texture, 2D positions, principal component analysis, Principal component analysis]
Ubiquitous animated agents for augmented reality
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
Most of today's Augmented Reality (AR) systems operate as passive information browsers relying on a finite and deterministic world model and a predefined hardware and software infrastructure. We propose an AR framework that dynamically and proactively exploits hitherto unknown applications and hardware devices, and adapts the appearance of the user interface to persistently stored and accumulated user preferences. Our framework explores proactive computing, multi-user interface adaptation, and user interface migration. We employ mobile and autonomous agents embodied by real and virtual objects as an interface and interaction metaphor, where agent bodies are able to opportunistically migrate between multiple AR applications and computing platforms to best match the needs of the current application context. We present two pilot applications to illustrate design concepts.
[Pervasive computing, Computer interfaces, passive information browser, user interface migration, augmented reality, interaction metaphor, user interfaces, Application software, Augmented reality, computer animation, proactive computing, autonomous agent, mobile computing, mobile agent, mobile agents, Computer graphics, Virtual reality, ubiquitous animated agent, User interfaces, Animation, user preference, Hardware, Autonomous agents, multiuser interface adaptation]
Visualizing and navigating complex situated hypermedia in augmented and virtual reality
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
We present a set of techniques that enable mobile users to visualize and navigate complex hypermedia structures embedded in the real world, through augmented reality or virtual reality. Situating hypermedia in the 3D physical environment makes it possible to represent information about users' surroundings in context. However, it requires addressing a new set of problems beyond those of visualizing hypermedia on a 2D display: Nodes and links can potentially be distributed across large distances, and may be occluded by other objects, both real and virtual. Our techniques address these issues by enabling mobile users to select and manipulate portions of the hypermedia structure by tilting, lifting and shifting them, to view more clearly links and nodes that would otherwise be occluded or ambiguously connected.
[Visualization, virtual reality, Navigation, Buildings, Two dimensional displays, complex hypermedia structures, augmented reality, Augmented reality, Computer science, Computer displays, Virtual reality, User interfaces, Chromium, mobile users, complex situated hypermedia]
Using aerial photographs for improved mobile AR annotation
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
We present a mobile augmented reality system for outdoor annotation of the real world. To reduce user burden, we use aerial photographs in addition to the wearable system's usual data sources (position, orientation, camera and user input). This allows the user to accurately annotate 3D features with only a few simple interactions from a single position by aligning features in both their first-person viewpoint and in the aerial view. We examine three types of aerial photograph features - corners, edges, and regions - that are suitable for a wide variety of useful mobile augmented reality applications, and are easily visible on aerial photographs. By using aerial photographs in combination with wearable augmented reality, we are able to achieve much higher accuracy 3D annotation positions than was previously possible from a single user location.
[Eyes, Avatars, Laboratories, human factors, wearable system, augmented reality, 3D feature annotation positions, Application software, user experience, Augmented reality, wearable computers, mobile computing, mobile AR annotation, aerial photograph features, augmented reality system, Layout, Computer graphics, Chromium, Cameras, Feeds, photography, outdoor annotation]
Implementation of god-like interaction techniques for supporting collaboration between outdoor AR and indoor tabletop users
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
This paper presents a new interaction metaphor we have termed "god-like interaction". This is a metaphor for improved communication of situational and navigational information between outdoor users, equipped with mobile augmented reality systems, and indoor users, equipped with tabletop projector display systems. Physical objects are captured by a series of cameras viewing a table surface indoors, the data is sent over a wireless network, and is then reconstructed at a real-world location for outdoor augmented reality users. Our novel god-like interaction metaphor allows users to communicate information using physical props as well as natural gestures. We have constructed a system that implements our god-like interaction metaphor as well as a series of novel applications to facilitate collaboration between indoor and outdoor users. We have extended a well-known video based rendering algorithm to make it suitable for use on outdoor wireless networks of limited bandwidth. This paper also describes the limitations and lessons learned during the design and construction of the hardware that supports this research.
[Navigation, indoor tabletop users, navigational information, Mobile communication, Displays, augmented reality, tabletop projector display systems, god-like interaction techniques, user interfaces, Augmented reality, video based rendering algorithm, cameras, Surface reconstruction, mobile computing, augmented reality systems, Wireless networks, Collaboration, Bandwidth, Cameras, Hardware, wireless network]
A 2D-3D integrated interface for mobile robot control using omnidirectional images and 3D geometric models
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
This paper proposes a novel visualization and interaction technique for remote surveillance using both 2D and 3D scene data acquired by a mobile robot equipped with an omnidirectional camera and an omnidirectional laser range sensor. In a normal situation, telepresence with an egocentric-view is provided using high resolution omnidirectional live video on a hemispherical screen. As depth information of the remote environment is acquired, additional 3D information can be overlaid onto the 2D video image such as passable area and roughness of the terrain in a manner of video see-through augmented reality. A few functions to interact with the 3D environment through the 2D live video are provided, such as path-drawing and path-preview. Path-drawing function allows to plan a robot's path by simply specifying 3D points on the path on screen. Path- preview function provides a realistic image sequence seen from the planned path using a texture-mapped 3D geometric model in a manner of virtualized reality. In addition, a miniaturized 3D model is overlaid on the screen providing an exocentric view, which is a common technique in virtual reality. In this way, our technique allows an operator to recognize the remote place and navigate the robot intuitively by seamlessly using a variety of mixed reality techniques on a spectrum of Milgram's real-virtual continuum.
[Solid modeling, Robot control, augmented reality, realistic image sequence, mobile robots, egocentric-view, Mobile robots, hemispherical screen, data visualisation, Virtual reality, 3D geometric, Laser modes, virtualized reality, data visualization, image sequences, interaction technique, remote surveillance, Robot vision systems, 2D-3D integrated interface, robot vision, mobile robot control, image sensors, real-virtual continuum, Surveillance, Layout, Data visualization, Cameras, omnidirectional images, path-drawing function]
The universal media book: tracking and augmenting moving surfaces with projected information
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
We explore the integration of projected imagery with a physical book that acts as a tangible interface to multimedia data. Using a camera and projector pair, a tracking framework is presented wherein the 3D position of planar pages are monitored as they are turned back and forth by a user, and data is correctly warped and projected onto each page at interactive rates. The book pages are blank, so traditional approaches to tracking physical features on the display surface do not apply. Instead, in each frame, feature points are independently extracted from the camera and projector images, and matched in order to recover the geometry of the pages in motion. The book can be loaded with multimedia content, including images, videos, and volumetric datasets (in which case a page can be removed from the book and used to navigate through a virtual 3D volume).
[Computer interfaces, virtual 3D volume, augmented book, mixed-reality interface, augmented reality, moving surface augmentation, user interfaces, Data mining, multimedia computing, volumetric visualization, tracking, tangible interface, Videos, feature extraction, data visualisation, universal media book, Books, Computer vision, multimedia data, image matching, projected imagery, image motion analysis, Geometry, Computer displays, Layout, moving surface tracking, Cameras, Liquid crystal displays]
"Move the couch where?" : developing an augmented reality multimodal interface
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
This paper describes an augmented reality (AR) multimodal interface that uses speech and paddle gestures for interaction. The application allows users to intuitively arrange virtual furniture in a virtual room using a combination of speech and gestures from a real paddle. Unlike other multimodal AR applications, the multimodal fusion is based on the combination of time-based and semantic techniques to disambiguate a users speech and gesture input. We describe our AR multimodal interface architecture and discuss how the multimodal inputs are semantically integrated into a single interpretation by considering the input time stamps, the object properties, and the user context.
[Computer interfaces, multimodal fusion, Virtual environment, speech-based user interfaces, augmented reality, augmented reality multimodal interface, Application software, Augmented reality, speech gesture, gesture recognition, speech recognition, Space technology, Virtual reality, time-based technique, User interfaces, Speech, Mice, semantic technique, paddle gesture, Graphical user interfaces]
High-fidelity visuo-haptic interaction with virtual objects in multi-modal AR systems
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
The driving force of our research is the precise combination of real and - possibly indistinguishable - virtual objects in an interactive augmented reality environment. This requires real-time, multimodal simulation, as well as stable and accurate overlay of the computer-generated objects. This paper describes several methods to improve accuracy and stability of our hybrid augmented reality system. In a comparison of two approaches to hybrid head pose refinement, we show that Quasi-Newton method enables high performance optimization for image space error minimization. Moreover, a 3D landmark refinement step is proposed, which significantly improves quality and robustness of the overlay process. The enhanced system is demonstrated in an interactive AR environment, which provides accurate haptic feedback from real and virtual deformable objects. Finally, the effect of landmark occlusion on tracking stability during user interaction is also analyzed.
[interactive augmented reality, haptic interfaces, augmented reality, user interaction, Optimization, tracking stability, high-fidelity visuo-haptic interaction, computer-generated objects, Robustness, virtual objects, multimodal simulation, image space error minimization, Newton method, landmark occlusion, image processing, Head, Minimization methods, Stability, Computational modeling, Computer simulation, virtual deformable objects, Haptic interfaces, Augmented reality, Quasi-Newton method, multimodal AR systems, Computer errors, haptic feedback, 3D landmark refinement step]
An event architecture for distributed interactive multisensory rendering
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
We describe an architecture for coping with latency and asynchrony of multisensory events in interactive virtual environments. We propose to decompose multisensory interactions into a series of discrete, perceptually significant events, and structure the application architecture within this event-based context. We analyze the sources of latency, and develop a framework for event prediction and scheduling. Our framework decouples synchronization from latency, and uses prediction to reduce latency when possible. We evaluate the performance of the architecture using vision-based motion sensing and multisensory rendering using haptics, sounds, and graphics. The architecture makes it easy to achieve good performance using commodity off-the-shelf hardware.
[event architecture, Virtual environment, augmented reality, Haptic interfaces, interactive virtual environments, Delay, Augmented reality, Graphics, distributed interactive multisensory rendering, software architecture, Computer displays, vision-based motion sensing, event-based context, Rendering (computer graphics), Computer networks, commodity off-the-shelf hardware, Trajectory, Frequency synchronization, rendering (computer graphics)]
Enhanced visual realism by incorporating camera image effects
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
In video see-through augmented reality (AR), virtual objects are overlaid over digital video images. One particular problem of this image mixing process is that the visual appearance of the computer graphics differs strongly from the real background image. The reason for this is that typical AR systems use fast but simple real-time rendering techniques for displaying virtual objects. In this paper, methods for reducing the impact of three effects which make virtual and real objects easily distinguishable are presented. The first effect is camera image noise, which is contained in the data delivered by the image sensor used for capturing the real scene. The second effect considered is edge aliasing, which makes distinguishing virtual objects from real objects simple. Finally, we consider motion blur, which is caused by the temporal integration of color intensities in the image sensor during fast movements of the camera or observed objects. In this paper, we present a system for generating a realistic simulation of image noise based on a new camera calibration step. Additionally, a rendering algorithm is introduced, which performs a smooth blending between the camera image and virtual objects at their boundary in order to reduce aliasing. Lastly, a rendering method is presented, which produces motion blur according to the current camera movement. The implementation of the new rendering techniques utilizes the programmability of modern graphics processing units (GPUs) and delivers real-time frame rates.
[Real time systems, camera image effects, image registration, augmented reality, Noise generators, Calibration, digital video images, graphics processing units, Augmented reality, Image sensors, image enhancement, Layout, Computer graphics, enhanced visual realism, Cameras, Rendering (computer graphics), image noise, virtual objects, Colored noise]
Interactive context-driven visualization tools for augmented reality
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
In this article we present an interaction tool, based on the Magic Lenses technique, that allows a 3D scene to be affected dynamically given contextual information, for example, to support information filtering. We show how elements of a scene graph are grouped by context in addition to hierarchically, and, how this enables us to locally modify their rendering styles. This research has two major contributions, the use of context sensitivity with 3D Magic Lenses in a scene graph and the implementation of multiple volumetric 3D Magic Lenses for Augmented Reality setups. We have developed our tool for the Studierstube framework which allows us doing rapid prototyping of Virtual and Augmented Reality applications. Some application directions are shown throughout the paper. We compare our work with other methods, highlight strengths and weaknesses and finally discuss research directions for our work.
[Visualization, Liver, Studierstube framework, magic lenses technique, augmented reality, virtual prototyping, information filtering, Information filtering, virtual rapid prototyping, Augmented reality, Layout, data visualisation, Computer graphics, Virtual reality, User interfaces, rendering styles, Rendering (computer graphics), 3D scene, Lenses, interactive context-driven visualization tools]
Augmented reality based on estimation of defocusing and motion blurring from captured images
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
Photometric registration is as important as geometric registration to generate a seamless augmented reality scene. Especially the difference in image quality between a real image and virtual objects caused by defocusing and motion blurring in capturing a real scene image easily exhibits the seam between real and virtual worlds. To avoid this problem in video see-through augmented reality, it is necessary to simulate the optical system of camera when virtual objects are rendered. This paper proposes an image composition method for video see-through augmented reality, which is based on defocusing and motion blurring estimation from the captured real image and rendering of virtual objects with blur effects. In experiments, the effectiveness of the proposed method is confirmed by comparing a real image with virtual objects rendered by the proposed method.
[image registration, virtual object rendering, augmented reality, photometry, photometric registration, optical camera system, real scene, Degradation, Information science, motion estimation, motion blurring estimation, Photometry, video see-through augmented reality, Motion estimation, video cameras, image composition, Augmented reality, Image quality, image quality, Layout, Cameras, Rendering (computer graphics), defocusing estimation, geometric registration, Lenses]
Notational-based prototyping of mixed interactions
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
Development of mixed reality systems is almost always following an ad-hoc process. The development cycle often turns out to be highly expensive and time consuming. This paper presents a new prototyping approach: a combination of model-based design and simulation based prototyping.
[Software testing, System testing, notational-based prototyping, software prototyping, Buildings, augmented reality, mixed interactions, user interfaces, Application software, Prototypes, XML, Virtual reality, Chromium, model-based design, mixed reality systems, simulation based prototyping, Virtual prototyping, Software tools, ad-hoc process]
Transitional interface: concept, issues and framework
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
Transitional Interfaces have emerged as a new way to interact and collaborate between different interactive spaces such as reality, virtual reality and augmented reality environments. In this paper we explore this concept further. We introduce a descriptive model of the concept, its collaborative aspect and how it can be generalized to describe natural and continuous transitions between contexts (e.g. across space, scale, viewpoints, and representation).
[Legged locomotion, collaborative transitional interface, virtual reality, Navigation, Switches, augmented reality, user interfaces, Augmented reality, Interpolation, Chemistry, interactive space, Collaboration, Virtual reality, groupware, Collaborative work, continuous perceptual transition, Context modeling]
Visualization of sensor data using mobile phone augmented reality
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
We have developed a prototype system for visual inspection of hidden structures using a mobile phone wireless ZigBee sensor network. Data collected from an embedded wireless sensor matrix is used to synthesize graphics in real-time. Combining this with augmented reality technology on a mobile phone yields a novel approach to on-site inspection of a broad range of elements and their current internal states.
[ZigBee, sensor data visualization, Inspection, augmented reality, Mobile handsets, Sensor systems, Augmented reality, mobile phone augmented reality, Graphics, Wireless sensor networks, mobile computing, visual inspection, ZigBee sensor network, Data visualization, Prototypes, data visualisation, embedded wireless sensor matrix, Network synthesis, mobile handsets]
Mobile augmented reality interaction techniques for authoring situated media on-site
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
We present a set of mobile augmented reality interaction techniques for authoring situated media: multimedia and hypermedia that are embedded within the physical environment. Our techniques are designed for use with a tracked hand-held tablet display with an attached camera, and rely on "freezing" the frame for later editing.
[hand-held tablet display, Scattering, hypermedia, augmented reality, Augmented reality, Organizing, Computer science, Mars, mobile computing, multimedia, authoring situated media, Layout, physical environment, Virtual reality, interactive systems, Cameras, Books, mobile augmented reality interaction technique, Mobile computing]
Mobile pointing and input system using active marker
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
We present a mobile pointing and input system for mobile phones with minimal hardware parts that allows users who wear an eyeglass display to view and interact with a large virtual image. To use the mobile phone as a pointing and input device, a fiducial marker-based tracking system developed for augmented reality is adapted for our system. The brightness and shape of active marker could be changed according to the state of operating environment and/or the user's intentions in order to increase the detection rate of the marker.
[fiducial marker-based tracking system, Shape, Brightness, input device, Displays, augmented reality, Mobile handsets, Augmented reality, mobile computing, input system, eyeglass display, mobile phone, virtual image, mobile pointing, Hardware, interactive devices, active marker]
Mixed reality pre-visualization and camera-work authoring in filmmaking
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
In this paper, we introduce the outline of "The MR-PreViz Project" performed in Japan. In the pre-production process of filmmaking, Pre Viz, pre-visualizing the desired scene by CGI, is used as a new technique. As its advanced approach, we propose MR-PreViz that utilized mixed reality technology in current PreViz. MR-PreViz makes it possible to merge the real background and the computer-generated humans and creatures in open set or at outdoor location. The user can consider the camerawork and camera blocking efficiently by using MR-PreViz. This paper introduces the outline of the MR-PreViz project, the design of hardware configuration, camera-work authoring method and the results of prototyping.
[mixed reality previsualization, virtual reality, Digital cameras, filmmaking, Application software, computer-generated humans, cinematography, cameras, camera blocking, Layout, Character generation, Prototypes, data visualisation, Virtual reality, Image generation, Motion pictures, Animation, MR-PreViz Project, camera-work authoring, High definition video]
Viewpoint stabilization for live collaborative video augmentations
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
We present a method for stabilizing live video from a moving camera for the purpose of a tele-meeting, in which a participant with an AR view onto a shared canvas collaborates with a remote user. The AR view is established without markers and using no other tracking equipment than a head-worn camera. The remote user is allowed to directly annotate the local user's view in real time on a desktop or tablet PC. The planar homographies between the reference frame and the other following frames are maintained. In effect, both the local and remote participants can annotate the physical meeting space, the local AR user through physical interaction, the remote user through our stabilized video. When tracking is lost, the remote user can still continue annotating on a frozen video frame. We tested several small demo applications with this new form of transient AR collaboration that can be established easily, on a per need basis, and without complicated equipment or calibration requirements.
[Target tracking, Eyes, Video sharing, Laboratories, viewpoint stabilization, live collaborative video augmentations, telemeeting, augmented reality, Application software, head-worn camera, teleconferencing, cameras, Layout, video frame, Collaboration, local AR user, computer vision, Streaming media, remote user, Cameras, Collaborative work, video streaming]
Support system for guitar playing using augmented reality display
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
Learning to play the guitar is difficult. We proposed a system that assists people learning to play the guitar using augmented reality. This system shows a learner how to correctly hold the strings by overlaying a virtual hand model and lines onto a real guitar. The player learning to play the guitar can easily understand the required position by overlapping their hand on a visual guide. An important issue for this system to address is the accurate registration between the visual guide and the guitar, therefore we need to track the pose and the position of the guitar. We also proposed a method to track the guitar with a visual marker and natural features of the guitar. Since we used marker information and edge information as natural features, the system could continually track the guitar. Accordingly, our system can constantly display visual guides at the required position to enable a player to learn to play the guitar in a natural manner.
[Instruments, Image edge detection, Image processing, image registration, support system, augmented reality, augmented reality display, Augmented reality, Computer science, Computer displays, virtual lines, edge information, virtual hand model, feature extraction, visual marker, Keyboards, computer displays, Chromium, visual guide, Universal Serial Bus, Cameras, edge detection, guitar playing, natural features]
Using backlight intensity for device identification
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
This paper presents a method for identifying handheld devices (e.g. smart phones and pocket PCs) to facilitate the use of these devices as tangible interfaces for desktop augmented reality systems. The proposed system leverages the ability of these handheld devices to programmatically control their backlight intensity to display a binary code. The codes produced are non-intrusive, require no specialized hardware, and can be generated with most handheld devices. This technique is shown to accurately and robustly identify up to 16 different devices in under 500 msec and is easily expandable to 256 or more devices.
[binary codes, backlight intensity, Control systems, Displays, augmented reality, user interfaces, Object recognition, Augmented reality, Handheld computers, Binary codes, device identification, desktop augmented reality systems, Hardware, Robustness, handheld devices, Personal communication networks, Smart phones]
User experience and acceptance of a mixed reality system in a naturalistic setting - a case study
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
This poster presents a qualitative user study investigating user experience and acceptance of an MR application designed to give instructions on how to start up a diathermy apparatus. The study was conducted in a naturalistic setting on site at a hospital with actual users and their equipment. The analysis of the results has indicated that although there are several ergonomic issues to be solved, the acceptance of an MR system in this user group is high. As a result of the study, the MR system has been redesigned to better fit the ergonomic needs of this user group.
[Assembly systems, virtual reality, mixed reality system, diathermy apparatus, naturalistic setting, Augmented reality, Computer displays, Hospitals, Ergonomics, Industry applications, Virtual reality, Cameras, Usability, Software tools, ergonomic issues]
Tag detection algorithm for improving the instability problem of an augmented reality
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
Detection technology is a requirement for an Augmented Reality system. One of the problems with detection technology is the instability problem, which occurs when an obstacle occludes a tag while detecting the tag, and the augmented object suddenly disappears. We have proposed a corner detection algorithm to solve this instability problem. The key feature is that if the tag can recognize its position using its four corner cells despite the obstacle being present, then it can maintain its augmented object. We defined the corner case for all types of cases where the instability problem occurs in ARToolkit or ARTag. We have adapted our proposed algorithm to the corner case in ARToolkit, ARTag and ColorCode vision systems and have compared their false detection rates.
[Image recognition, Error analysis, tag detection algorithm, Laboratories, Data gloves, augmented reality, object detection, Augmented reality, Computer science, instability problem, Machine vision, Object detection, Cameras, corner detection, edge detection, Detection algorithms]
Augmented reality as a comparison tool in automotive industry
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
Augmented reality (AR) can be used in the automotive industry to compare real parts of a car with their associated construction data. The real parts have to be checked whether they correspond to the latest version of the design and whether they have been manufactured with appropriate precision. With AR, CAD construction data can be superimposed on the real parts striving for maximum correspondence. The real and the virtual part should both be visible at the same time and in the same place. Therefore, for this kind of overlay, a special method of augmentation is needed. We present and discuss some visualization schemes.
[automotive industry, Shape, associated construction data, CAD, production engineering computing, Muscles, augmented reality, automobile industry, Wire, Augmented reality, Automotive engineering, Construction industry, Layout, Data visualization, Prototypes, Bones, program visualisation, CAD construction, visualization schemes]
Robust gloves for 3D interaction in mobile outdoor AR environments
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
This paper describes the design of hand-worn gloves for interacting with mobile outdoor augmented reality systems. Most existing systems rely on more traditional 2D input devices such as mice and keyboards. Since augmented reality information is typically registered in 3D to the environment, user interfaces need to be designed that are capable of supporting the more complex operations possible. This paper describes how we used metallic thread and adhesive fabric to add conduction sensing to a standard set of gloves which can survive harsh treatment; how Bluetooth and MSP430 microcontrollers are used to build a small circuit that is wireless and highly portable; and how AR-ToolKit is used for 3D tracking of fiducial markers on the thumbs. While we have previously demonstrated this technology with a number of our previous systems, this paper explains the various techniques we use in the implementation.
[Bluetooth, Microcontrollers, Circuits, augmented reality, user interfaces, MSP430 microcontrollers, mobile outdoor augmented reality systems, Yarn, Augmented reality, wearable computers, mobile computing, Keyboards, User interfaces, Robustness, Mice, Fabrics, robust gloves, 3D interaction]
Texture generation over the marker area
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
In this paper, we present a method for generating a texture for hiding a marker in augmented reality applications. The texture is generated using the neighbourhood of the detected marker area in the image, which enables photorealistic results. The method presented here shows clear potential for real time use.
[photorealistic rendering, Multimedia systems, image inpainting, augmented reality, Application software, Augmented reality, image texture, Information systems, detected marker area, Virtual reality, Image generation, Computer graphics, Chromium, Filling, Automatic voltage control, texture generation, rendering (computer graphics)]
Markerless augmented reality for cubic panorama sequences
2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2006
This paper presents a system for introducing augmented reality (AR) enhancements into an image-based cubic panorama sequence. Panoramic cameras, such as the Point Gray Research Ladybug allow rapid capture and generation of panoramic sequences for users to navigate and view. Our AR system provides the ability for authors to add virtual content into the panoramic sequences. First, a user manually selects a planar region over which to add the content. Then the system automatically finds a matching planar region in all the panoramas, allowing the virtual content to propagate. No preconditioning of the imaged scene through the addition of physical markers is necessary. Instead, 3-D position information is obtained by matching interest-point features across the panoramic sequence. This paper presents an application of augmented reality algorithms to the unique case of pre-captured panoramic sequences.
[Computer vision, virtual contents, Navigation, Image processing, 3D position information, augmented reality, Application software, image matching, Augmented reality, image-based cubic panorama sequence, image enhancement, Councils, Layout, Computer graphics, Chromium, Cameras, Point Gray Research Ladybug, image sequences]
Experiences with Handheld Augmented Reality
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
In this paper, we present Studierstube ES, a framework for the development of handheld Augmented Reality. The applications run self-contained on handheld computers and smartphones with Windows CE. A detailed description of the performance critical tracking and rendering components are given. We also report on the implementation of a client-server architecture for multi-user applications, and a game engine for location based museum games that has been built on top of this infrastructure. Details on two games that were created, permanently deployed and evaluated in two Austrian museums illustrate the practical value of the framework and lessons learned from using it.
[Computer interfaces, augmented reality games, Application software, wearable computing, cultural heritage, Augmented reality, Engines, Mobile augmented reality, Handheld computers, Wearable computers, Games, Personal communication networks, Personal digital assistants, Smart phones]
Laser Pointer Tracking in Projector-Augmented Architectural Environments
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
We present a system that employs a custom-built pan-tilt-zoom camera for laser pointer tracking in arbitrary real environments. Once placed in a room, it carries out a fully automatic self-registration, registrations of projectors, and sampling of surface parameters, such as geometry and reflectivity. After these steps, it can be used for tracking a laser spot on the surface as well as an LED marker in 3D space, using inter-playing fish-eye context and controllable detail cameras. The captured surface information can be used for masking out areas that are problematic for laser pointer tracking, and for guiding geometric and radiometric image correction techniques that enable a projector-based augmentation on arbitrary surfaces. We describe a distributed software framework that couples laser pointer tracking for interaction, projector-based AR as well as video see-through AR for visualizations, with the domain specific functionality of existing desktop tools for architectural planning, simulation and building surveying.
[laser pointer tracking, building surveying, architectural planning, Geometrical optics, augmented reality, Computer Graphics, radiometric image correction techniques, Surface emitting lasers, data visualisation, projector-augmented architectural environments, distributed software framework, Image Processing and Computer Vision, Automatic control, Radiometry, I.3.3 [Computer Graphics, Computer Applications, Light emitting diodes, Optical control, optical projectors, Reflectivity, image sensors, fully automatic self-registration, computer vision, Cameras, Sampling methods, pan-tilt-zoom camera, Software tools]
Urban Sketcher: Mixed Reality on Site for Urban Planning and Architecture
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
Urban sketcher, a mixed reality application, is designed to encourage and improve communication on urban design among stakeholders. A mix of multimodal input devices enhances collaborative interaction in real-time, while visual feedback is given to all participants on a projected live video augmentation from urban sketcher. Sketching, modifying the scene on site, in the space of the video augmentation supports the exchange of information with interactive visual support. Urban Sketcher is instrumental for developing visions of future urban spaces by augmenting the real environment with sketches, facades, buildings, green spaces or skylines.
[Urban planning, augmented reality, visual feedback, live video augmentation, urban planning, urban sketcher, collaborative interaction, Mixed reality, natural multimodal interaction, collaboration, Virtual reality, computer vision, video signal processing, architecture]
Augmented Reality-based factory planning - an application tailored to industrial needs
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
Over the past years, a variety of augmented reality (AR)-based applications were created, to support industrial processes. Although these first demonstrator applications or prototypes cover all parts of the industrial product process (design, planning and production, service and maintenance), only a few of them actually turned into established and applied solutions. In the automotive industry, one successful field for AR-based applications is factory design and planning, metaio and Volkswagen Group Research have concerted their research work to continuously enhance a first prototype in order to create a serviceable and accepted software tool, which is tailored to the needs of AR-based factory planning processes. This paper presents the developing process of this application, the requirements analysis and the realization as a complete system. A concrete example in industry describes the usage of the system for factory planning and an outlook on future work in this area closes the document.
[Process design, Software prototyping, augmented reality-based factory planning, Process planning, H.5.1 [Information Interfaces and Presentation, production engineering computing, augmented reality, Production facilities, automobile industry, Application software, factory planning process, Augmented reality, Automotive engineering, software tool, Production planning, Prototypes, industrial needs, Computer-Aided Engineering, Computer industry, production planning, Information Interfaces and Presentation, Physical Science and Engineering]
Face-to-Face Tabletop Remote Collaboration in Mixed Reality
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
This paper proposes a novel remote face-to-face mixed reality (MR) system that enables two people in distant places to share MR space. Challenging issues to realize such an MR system include capturing, sending, and rendering each user's appearance in real time. We developed a method to represent user's upper body and hands on the table as a single deformed-billboard. An MR Othello game is implemented as a test bed of the remote face-to-face MR system. Users can play the tabletop game as if their opponent were sitting across from the table, despite being physically separated. By detecting and sending the status of each real game board to the other site, both users feel that they are sharing tabletop objects.
[Real time systems, Othello game, Real-Time Modeling and Rendering, Shape, Tele-Immersion, mixed reality system, rendering, user interfaces, Application software, Information systems, Teleconferencing, face-to-face tabletop remote collaboration, computer graphics, Collaboration, Virtual reality, Computer graphics, computer vision, Rendering (computer graphics), Cameras, Billboard, Shared Mixed-Reality]
Visual Hints for Tangible Gestures in Augmented Reality
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
Tangible augmented reality (AR) systems imbue physical objects with the ability to act and respond in new ways. In particular, physical objects and gestures made with them gain meaning that does not exist outside the tangible AR environment. The existence of this new set of possible actions and outcomes is not always apparent, making it necessary to learn new movements or gestures. Addressing this opportunity, we present visual hints, which are graphical representations in AR of potential actions and their consequences in the augmented physical world. Visual hints enable discovery, learning, and completion of gestures and manipulation in tangible AR. Here, we discuss our investigation of a variety of representations of visual hints and methods for activating them. We then describe a specific implementation that supports gestures developed for a tangible AR user interface to an electronic field guide for botanists, and present results from a pilot study.
[visual hints, Multimedia systems, Documentation, Displays, augmented reality, user interfaces, History, Augmented reality, user interface, Radio access networks, gestures, Tangible augmented reality, graphical representations, Morphology, Virtual reality, computer vision, User interfaces, Chromium, tangible gestures, electronic field guide]
A 3D Flexible and Tangible Magic Lens in Augmented Reality
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
The Magic Lens concept is a focus and context technique which facilitates the visualization of complex and dense data. In this paper, we propose a new type of 3D tangible Magic Lens in the form of a flexible sheet. We describe new interaction techniques associated with this tool, and demonstrate how it can be applied in different AR applications.
[interaction techniques, Filtering, H.5.2 [Information Systems, Glass, Context awareness, Information Systems, augmented reality, user interfaces, 3D tangible magic lens, Augmented reality, Information systems, Graphics, Data visualization, data visualisation, User interfaces, Hardware, dense data visualization, Lenses, flexible sheet]
AR-Jig: A Handheld Tangible User Interface for Modification of 3D Digital Form via 2D Physical Curve
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
We introduce AR-Jig, a new handheld tangible user interface for 3D digital modeling in augmented reality (AR) space. AR-Jig has a pin array that displays a 2D physical curve coincident with a contour of a digitally displayed 3D form. It supports physical interaction with a portion of a 3D digital representation, allowing 3D forms to be directly touched and modified. Traditional tangible user interfaces physically embody all the data; in contrast, this project leaves the majority of the data in the digital domain but gives physicality to any portion of the larger digital dataset via a handheld tool. This tangible intersection enables the flexible manipulation of digital artifacts, both physically and virtually. Through an informal test by end-users and interviews with professionals, we confirmed the potential of the AR-Jig concept while identifying the improvements necessary to make AR-Jig a practical tool for 3D digital design.
[3D digital representation, actuated interface, Two dimensional displays, Laboratories, pin array display, Humans, AR-Jig, 3D digital form, augmented reality, 2D physical curve, user interfaces, Augmented reality, handheld tangible user interface, Space technology, Prototypes, handheld tool, User interfaces, Imaging phantoms, Three dimensional displays, Pins, digital artifacts, digital modeling]
Semi-automatic Annotations in Unknown Environments
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
Unknown environments pose a particular challenge for augmented reality applications because the 3D models required for tracking, rendering and interaction are not available ahead of time. Consequently, authoring of AR content must take place on-line. This work describes a set of techniques to simplify the online authoring of annotations in unknown environments using a simultaneous localisation and mapping (SLAM) system. The point-based SLAM system is extended to specifically track and estimate high-level features indicated by the user. The automatic estimation of these complex landmarks by the system relieves the user from the burden of manually specifying the full 3D pose of annotations while improving accuracy. These properties are especially interesting for remote collaboration applications where either user interfaces on handhelds or camera control by the remote expert are limited.
[simultaneous localisation and mapping system, H.5.1 [Information Systems, Shape measurement, Humans, Information Systems, augmented reality, rendering, Application software, tracking, Augmented reality, remote collaboration, Geometry, Simultaneous localization and mapping, semiautomatic annotations, Layout, computer vision, Image Processing and Computer Vision, User interfaces, Cameras, Collaborative work, 3D pose]
Automatic Reconstruction of Wide-Area Fiducial Marker Models
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
We present an approach towards automatic reconstruction of large assemblies of fiducial markers scattered throughout a wide indoor area, using a computer vision based reconstruction approach. The data is acquired from a video stream captured with a monoscopic camera. The system is capable of creating markers models that are significantly larger in physical area and number of markers than with previous approaches.
[Computer vision, Pipelines, Humans, Scattering, automatic reconstruction, augmented reality, image reconstruction, Image reconstruction, Augmented reality, monoscopic camera, computer vision, Streaming media, Cameras, Hardware, video streaming, video stream, wide-area fiducial marker models, Assembly]
Automatic contour model creation out of polygonal CAD models for markerless Augmented Reality
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
We present a solution for automatic extraction of contour models out of polygonal CAD models. Such contour models can be used for markerless initialization and tracking purposes. To create a contour model we synthesize different views of the object using its CAD model. During the view generation we alternate the camera pose as well as light conditions in order to extract the most stable contours in terms of illumination invariance and view independence. We project the extracted 2D edges back into the 3D space and accumulate for every 3D point statistics over different views describing its visibility and stability under different illumination conditions. After filtering out all 3D points with probability below a certain threshold value we use the Euclidean Minimum Spanning Tree algorithm to get the connected contours out of the 3D point cloud. The result is a B-Spline or VRML representation of the most stable contours.
[euclidean minimum spanning tree algorithm, Costs, splines (mathematics), Clouds, computational geometry, 3D point cloud, augmented reality, B-spline contour model, industrial polygonal CAD model, I.4.8 [Image processing and computer vision, automatic contour extraction model, Image processing and computer vision, Lighting, illumination invariance, Filtering algorithms, markerless augmented reality, 2D edge extraction, Stability, trees (mathematics), CAD, Probability, Statistics, Augmented reality, Layout, VRML representation, Cameras, solid modelling]
Semi-Autonomous Generation of Appearance-based Edge Models from Image Sequences
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
Many of the robust visual tracking techniques utilized by augmented reality applications rely on 3D models and information extracted from images. Models enhanced with image information make it possible to initialize tracking and detect poor registration. Unfortunately, generating 3D CAD models and registering them to image information can be a time consuming operation. Regularly the process requires multiple trips between the site being modeled and the workstation used to create the model. The system presented in this work eliminates the need for a separately generated 3D model by utilizing modern structure-from-motion techniques to extract the model and associated image information directly from an image sequence. The technique can be implemented on any handheld device instrumented with a camera and network connection. The process of creating the model requires minimal user interaction in the form of a few cues to identify planar regions on the object of interest. In addition the system selects a set of keyframes for each region to capture viewpoint based appearance changes. This work also presents a robust tracking framework to take advantage of these new edge models. Performance of both the modeling technique and the tracking system are verified on several different objects.
[robust visual tracking techniques, Image edge detection, Instruments, image registration, appearance-based edge models, CAD, Image sequences, Data mining, semi-autonomous generation, poor registration, Image reconstruction, Augmented reality, 3D CAD models, Handheld computers, Cameras, Rendering (computer graphics), Robustness, edge detection, image sequences]
Evaluating Display Types for AR Selection and Annotation
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
This paper evaluates different display devices for selection or annotation tasks in augmented reality (AR). We compare three different display types - a head mounted display and two hand held displays. The first hand held display is configured as a magic lens where the user sees the augmented space directly behind the display. The second hand held display is configured to be used at waist level (as one would commonly hold a tablet computer) but the view is still of the scene in front of the user. Making a selection or annotation in AR requires two distinct tasks by the user. First, the user must find the real (or virtual) object they want to mark. Second, the user must move a cursor to the object's location. We test and compare our three representative displays with respect to both tasks. We found that using a hand held display in the magic lens configuration was faster for cursor movement than either of the other two displays. There was no significant difference among the displays regarding the amount of time it took users to search for either physical or virtual objects.
[Head, Eyes, augmented space, augmented reality, helmet mounted displays, Application software, Augmented reality, Computer Graphics, Computer displays, computer graphics, H.5.2 [Information Interfaces and Presentation, Layout, computer vision, Cameras, head mounted display, annotation tasks, Information Interfaces and Presentation, Personal digital assistants, representative displays, Lenses, Testing]
An Evaluation of Graphical Context as a Means for Ameliorating the Effects of Registration Error
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
An ongoing research problem in Augmented Reality (AR) is to improve tracking and display technology in order to minimize registration errors. However, perfect registration is not always necessary for users to understand the intent of an augmentation. This paper describes the results of an experiment to evaluate the effects of registration error in a Lego block placement task and the effectiveness of graphical context at ameliorating these effects. Three types of registration error were compared: no error, fixed error and random error. These three errors were evaluated with no context present and some graphical context present. The results of this experiment indicated that adding graphical context to a scene in which some registration error is present can allow a person to effectively operate in such an environment, in this case completing the Lego block placement task with a reduced number of errors made and in a shorter amount of time.
[Visualization, Uncertainty, Adaptive systems, Multimedia systems, image registration, augmented reality, augmented environments, Augmented reality, Programming profession, Graphics, communicative intent, Computer displays, human-computer interaction, Layout, computer displays, registration error, Back, Lego block placement task, human computer interaction]
An Industrial Augmented Reality Solution For Discrepancy Check
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
Construction companies employ CAD software during the planning phase, but what is finally built often does not match the original plan. The procedure of validating the model is called "discrepancy check". The system proposed here allows the user to easily obtain an augmentation in order to find differences between the planned 3D model and the built items. The main difference to previous body of work in this field is the emphasis on usability and acceptance of the solution. While standard image-based solutions use markers or rely on a "perfect" 3D model to find the pose of the camera, our software uses anchor-plates. Anchor-Plates are rectangular structures installed on walls and ceiling in the majority of industrial edifices. We are using them as landmarks because they are the most reliable components often used as reference coordinates by constructors. Furthermore, for real industrial applications, they are the most suitable solutions in terms of general applicability. Unfortunately, they have not been designed with computer vision applications in mind. On the contrary, they are often made or painted in such way that they are not easily popping out. They are therefore difficult targets to segment and to track. This paper proposes a solution to extract and match them to their 3D counterparts. We created a software that uses the detected structures for pose estimation and image augmentation. The software has been successfully employed to find discrepancies in several rooms of two industrial plants.
[Segmentation, production engineering computing, augmented reality, Multimedia Information Systems, discrepancy check, Software standards, anchor-plates, Computer vision, Target tracking, CAD software, Applications, User Interfaces, CAD, Application software, construction industry, Augmented reality, construction companies, Construction industry, H.5.1 [Multimedia Information Systems, Image segmentation, computer vision, Cameras, industrial augmented reality solution, Industrial plants, Usability]
Visualization of Spatial Sensor Data in the Context of Automotive Environment Perception Systems
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
Spatial sensor systems in cars are gaining more and more importance. Such sensor systems are the foundation of future safety systems, such as automatic emergency brakes, as well as for interactive driver assistance systems. We have developed a system that can visualize such spatial sensor data. Two environments are supported: a laboratory setup for off-line experience and a car setup that enables live experience of spatially aligned laser scanner and video data in real traffic. We have used two visualization devices, a video see-through LCD flat panel (TFT) and an optical see-through head-mounted display (HMD) in both setups. For the laboratory setup, a back-projection table has been integrated as well. To present data in correct spatial alignment, we have installed tracking systems in both environments. Visualization schemes for spatial sensor data and for geometric models that outline recognized objects have been developed. We report on our system and discuss experiences from the development and realization phases. The system is not intended to be used as a component of real driver assistance systems. Rather, it can bridge the gap between human machine interface (HMI) designers and sensing engineers during the development phase. Furthermore, it can be both a debugging tool for the realization of environmental perception systems and an experimental platform for the design of presentation schemes for upcoming driver assistance systems.
[object recognition, Geometrical optics, Sensor systems, user interfaces, human machine interface, road safety, Models and Principles, Automotive engineering, H.5.2 [Information Interfaces and Presentation, driver information systems, data visualisation, Safety, spatial sensor data visualization, interactive driver assistance systems, Thin film transistors, see-through LCD flat panel, Optical devices, Data visualization, automatic emergency brakes, Liquid crystal displays, Information Interfaces and Presentation, automotive environment perception systems, Optical sensors, Flat panel displays, optical see-through head-mounted display, debugging tool]
Laparoscopic Virtual Mirror for Understanding Vessel Structure Evaluation Study by Twelve Surgeons
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
In this paper we present the evaluation of a virtual mirror used as a navigational tool within a medical augmented reality (AR) system for laparoscopy. 12 surgeons of our clinical partner participated in an experiment to evaluate whether laparoscope augmentation extended by a virtual mirror is useful for improved perception of complex structures. Such complex structures are encountered for instance in laparoscopic resection of tumor affected liver tissue. The blood vessels supplying the tumor have to be cut and closed before tumorous tissue can be removed. A laparoscopic camera and an optical tracking system allow for the visualization of visualized medical volumetric data registered with the real anatomy. Previously injected contrast agent provides an accentuation of blood vessels within the visualization. For evaluating the suitability of a virtual mirror to support the mentioned procedure, we designed a phantom consisting of wooden branches simulating the structure of blood vessel trees. Quantitative results of the experiment show the advantage of a mirror in certain cases, when blood vessels cannot be directly seen from the camera point of view due to self-occlusion of the structure. Results of a questionnaire filled out by the surgeons after the experiments confirm the acceptance of AR technology for particular medical procedures.
[medical volumetric data, Biomedical optical imaging, navigated surgery, medical augmented reality system, augmented reality, user interaction, navigational tool, Laparoscopes, medical visualization, Surgery, Mirrors, tumours, Liver neoplasms, laparoscopy, Navigation, liver, Blood vessels, blood vessels, laparoscopic virtual mirror, liver tissue, Augmented reality, vessel structure evaluation, Data visualization, tumor, Cameras, medical computing]
Contextual Anatomic Mimesis Hybrid In-Situ Visualization Method for Improving Multi-Sensory Depth Perception in Medical Augmented Reality
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
The need to improve medical diagnosis and reduce invasive surgery is dependent upon seeing into a living human system. The use of diverse types of medical imaging and endoscopic instruments has provided significant breakthroughs, but not without limiting the surgeon's natural, intuitive and direct 3D perception into the human body. This paper presents a method for the use of augmented reality (AR) for the convergence of improved perception of 3D medical imaging data (mimesis) in context to the patient's own anatomy (in-situ) incorporating the physician's intuitive multi- sensory interaction and integrating direct manipulation with endoscopic instruments. Transparency of the video images recorded by the color cameras of a video see-through, stereoscopic head- mounted-display (HMD) is adjusted according to the position and line of sight of the observer, the shape of the patient's skin and the location of the instrument. The modified video image of the real scene is then blended with the previously rendered virtual anatomy. The effectiveness has been demonstrated in a series of experiments at the Chirurgische Klinik in Munich, Germany with cadaver and in-vivo studies. The results can be applied for designing medical AR training and educational applications.
[Visualization, Humans, H.5.1 [Information Interfaces and Presentation, augmented reality, invasive surgery, Medical diagnosis, Computer Graphics, 3D medical imaging data, Surgery, data visualisation, contextual anatomic mimesis hybrid in-situ visualization method, endoscopes, video signal processing, stereoscopic head- mounted-display, medical image processing, Biomedical imaging, medical diagnostic computing, Instruments, Life and Medical Sciences, medical diagnosis, helmet mounted displays, multi-sensory depth perception, endoscopic instruments, Anatomy, Surges, Augmented reality, computer vision, medical augmented reality, Information Interfaces and Presentation, Medical diagnostic imaging, medical imaging]
Feature Tracking for Mobile Augmented Reality Using Video Coder Motion Vectors
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
We propose a novel, low-complexity, tracking scheme that uses motion vectors directly from a video coder. We compare our tracking algorithm against ground truth data, and show that we can achieve a high level of accuracy, even though the motion vectors are rate-distortion optimized and do not represent true motion. We develop a framework for tracking in video sequences with various GOP structures. Such a scheme would find applications in the context of Mobile Augmented Reality. The proposed feature tracking algorithm can significantly reduce the required rate of feature extraction and matching.
[feature tracking, Target tracking, Video sequences, Displays, augmented reality, Mobile handsets, Spatial databases, Data mining, Augmented reality, video coder, Image databases, motion vectors, feature extraction, Feature extraction, mobile augmented reality, Personal digital assistants, feature matching, mobile handsets]
Real-Time Object Tracking for Augmented Reality Combining Graph Cuts and Optical Flow
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
We present an efficient and accurate object tracking algorithm based on the concept of graph cut segmentation. The ability to track visible objects in real-time provides an invaluable tool for the implementation of markerless Augmented Reality. Once an object has been detected, it's location in future frames can be used to position virtual content, and thus annotate the environment. Unlike many object tracking algorithms, our approach does not rely on a preexisting 3D model or any other information about the object or its environment. It takes, as input, a set of pixels representing an object in an initial frame and uses a combination of optical flow and graph cut segmentation to determine the corresponding pixels in each future frame. Experiments show that our algorithm robustly tracks objects of disparate shapes and sizes over hundreds of frames, and can even handle difficult cases where an object contains many of the same colors as its background. We further show how this technology can be applied to practical AR applications.
[graph cut segmentation, Navigation, Shape, Image edge detection, graph theory, augmented reality, optical flow, Augmented reality, I.4.8 [Image Processing and Computer Vision, Image motion analysis, Image segmentation, image segmentation, Object detection, Image Processing and Computer Vision, Cameras, Robustness, Information Interfaces and Presentation, real-time object tracking, Pixel, image sequences]
Ninja on a Plane: Automatic Discovery of Physical Planes for Augmented Reality Using Visual SLAM
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
Most work in visual augmented reality (AR) employs predefined markers or models that simplify the algorithms needed for sensor positioning and augmentation but at the cost of imposing restrictions on the areas of operation and on interactivity. This paper presents a simple game in which an AR agent has to navigate using real planar surfaces on objects that are dynamically added to an unprepared environment. An extended Kalman filter (EKF) simultaneous localisation and mapping (SLAM) framework with automatic plane discovery is used to enable the player to interactively build a structured map of the game environment using a single, agile camera. By using SLAM, we are able to achieve real-time interactivity and maintain rigorous estimates of the system's uncertainty, which enables the effects of high quality estimates to be propagated to other features (points and planes) even if they are outside the camera's current field of view.
[Real time systems, sensor positioning, Uncertainty, H.5.1 [Information Systems, simultaneous localisation and mapping, visual SLAM, Information Systems, augmented reality, Sensor systems, physical planes automatic discovery, Augmented reality, Simultaneous localization and mapping, Interactive systems, SLAM (robots), Layout, extended Kalman filter, Image Processing and Computer Vision, Cameras, Surface fitting, Robustness, Kalman filters]
A Method for Predicting Marker Tracking Error
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
Many augmented reality (AR) applications use marker-based vision tracking systems to recover camera pose by detecting one or more planar landmarks. However, most of these systems do not interactively quantify the accuracy of the pose they calculate. Instead, the accuracy of these systems is either ignored, assumed to be a fixed value, or determined using error tables (constructed in an off-line ground-truthed process) along with a run-time interpolation scheme. The validity of these approaches are questionable as errors are strongly dependent on the intrinsic and extrinsic camera parameters and scene geometry. In this paper we present an algorithm for predicting the statistics of marker tracker error in real-time. Based on the scaled spherical simplex unscented transform (SSSUT), the algorithm is applied to the augmented reality toolkit plus (ARToolKitPlus). The results are validated using precision off-line photogrammetric techniques.
[Error analysis, vision tracking systems, marker tracking error prediction, augmented reality, ARToolKit, tracking, Augmented reality, augmented reality toolkit plus, Geometry, Interpolation, scaled spherical simplex unscented transform, Accuracy, Runtime, unscented transforms, Machine vision, Layout, computer vision, Cameras, Prediction algorithms, run-time interpolation scheme]
Initialisation for Visual Tracking in Urban Environments
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
Outdoor augmented reality systems often rely on GPS to cover large environments. Visual tracking approaches can provide more accurate location estimates but typically require a manual initialisation procedure. This paper describes the combination of both techniques to create an accurate localisation system that does not require any additional input for (re-)initialisation. The 2D GPS position together with average user height is used as an initial estimate for the visual tracking. The large gap in available GPS accuracy versus required accuracy for initialisation is overcome through a search procedure that tries to minimise search time by improving the likelihood of finding the correct estimate early. Re-initialisation of the visual tracking system after catastrophic failures is further improved by modelling the GPS error with a Gaussian process to provide a better estimate of the current location, thereby decreasing search time.
[H.5.1 [Information Systems, Information Systems, augmented reality, Sensor systems, visual tracking initialisation, GPS, Augmented reality, Global Positioning System, Satellites, augmented reality systems, urban environments, Gaussian process, Magnetic sensors, Image Processing and Computer Vision, Position measurement, Cameras, Robustness, Error correction, Wearable sensors]
Hear-Through and Mic-Through Augmented Reality: Using Bone Conduction to Display Spatialized Audio
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
We present a novel approach for mixing real and computer-generated audio for augmented reality (AR) applications. Analogous to optical-see-through and video-see-through techniques in the visual domain, we present Hear-Through and Mic-Through audio AR. Hear-Through AR uses a bone-conduction headset to deliver computer-generated audio, while leaving the ear canals free to receive audio from the surrounding environment. Mic-Through AR allows audio signals captured from ear-worn microphones to be mixed with computer-generated audio in the computer, and delivered to the user over headphones. We present preliminary results from an empirical user study conducted to compare a bone-conduction device, headphones, and a speaker array. The results show that subjects achieved the best accuracy using an array of speakers physically located around the listener when stationary sounds were played, but that there was no difference in accuracy between the speaker array and the bone-conduction device for sounds that were moving, and that both devices outperformed standard headphones for moving sounds.
[hear-through augmented reality, Optical mixing, headphones, Optical computing, augmented reality, Application software, bone conduction headset, Augmented reality, Auditory displays, Headphones, Loudspeakers, Computer displays, computer-generated audio, microphones, bone conduction, Ear, mic-through augmented reality, Bones, speaker array, audio]
Human-Centered Development of an AR Handheld Display
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
In this paper we present the process of designing and developing a fully functional and interactive AR handheld device. Based on a human-centered approach we describe the results of our exploration; from the contextual usage to the different rapid prototyped models and the evaluation activities. We also discuss some recommendations for how to use similar methods for developing future new AR physical interfaces.
[user interface management systems, Design methodology, software prototyping, B.4.2 [Input/Output and data communications, Humans, Displays, augmented reality, Augmented reality, Guidelines, human-centered development, Information interfaces and presentation, Design engineering, Handheld computers, Ergonomics, AR handheld display, rapid prototyped models, Prototypes, contextual usage, Usability, Input/Output and data communications]
Dynamic Adaptation of Projected Imperceptible Codes
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
In this paper we present an innovative adaptive imperceptible pattern projection technique that takes into account parameters of human visual perception. A coded image is temporally integrated into the projected image, which is invisible to the human eye but can be reconstructed by a synchronized camera. The embedded code is dynamically adjusted on the fly to guarantee its imperceptibility and to adapt it to the current camera pose. Linked with real-time flash keying, for instance, this enables in-shot optical tracking using a dynamic multi-resolution marker technique. A sample prototype has been realized that demonstrates the application of our method in the context of augmentations in television studios.
[TV, visual perception, pattern projection technique, Humans, INFORMATION INTERFACES AND PRESENTATION, IMAGE PROCESSING AND COMPUTER VISION, Multiresolution analysis, Image reconstruction, Visual perception, Image coding, Image analysis, coded image, dynamic adaptation, H.5.1 [INFORMATION INTERFACES AND PRESENTATION, Prototypes, Cameras, projected imperceptible codes, real-time flash keying, multi-resolution marker technique, image coding, human visual perception, High speed optical techniques, image resolution]
Interactive Focus and Context Visualization for Augmented Reality
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
In this article we present interactive focus and context (F+C) visualizations for augmented reality (AR) applications. We demonstrate how F+C visualizations are used to affect the user's perception of hidden objects by presenting contextual information in the area of augmentation. We carefully overlay synthetic data on top of the real world imagery by taking into account the information that is about to be occluded. Furthermore, we present operations to control the amount of augmented information. Additionally, we developed an interaction tool, based on the magic lens technique, which allows for interactive separation of focus from context. We integrated our work into a rendering framework developed on top of the Studierstube augmented reality system. We finally show examples to demonstrate how our work benefits AR.
[context visualization, Object overlay and spatial layout techniques, magic lens technique, rendering framework, Data structures, augmented reality, contextual information, Augmented reality, X-ray imaging, Computer displays, augmented reality system, interaction techniques for MR/AR, Layout, Focusing, Data visualization, data visualisation, mediated and diminished reality, Computer graphics, computer vision, Rendering (computer graphics), Needles, rendering (computer graphics), real-time rendering, interactive focus]
A Two-by-Two Mixed Reality System That Merges Real and Virtual Worlds in Both Audio and Visual Senses
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
There have been many implementations of virtual reality, using audio and visual senses. However, implementations of mixed reality (MR) have thus far only dealt with the visual sense. We have developed an MR system that merges real and virtual worlds in both the audio and visual senses, wherein the geometric consistency of the audio sense was fully coordinated with the visual sense. We tried two approaches for merging real and virtual worlds in the audio sense, using open-air and closed-air headphones.
[Real time systems, Open-Air Headphones, Closed-Air Headphones, virtual reality, two-by-two mixed reality system, Merging, Humans, audio sense, Audio and Visual Senses, Auditory displays, Headphones, Computer displays, Geometric Consistency, Layout, Virtual reality, Chromium, geometric consistency, Optical sensors, Mixed Reality, closed-air headphones]
A Wide Field-of-view Head Mounted Projective Display using Hyperbolic Half-silvered Mirrors
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
The development of a wide field-of-view (FOV) head mounted display (HMD) has been a technological challenge for decades. Previous HMDs tackled this problem using multiple display units (tiling) or multiple curved mirrors. The former approach tends to be expensive and heavy, whereas the latter approach tends to suffer from image distortion and a small exit pupil. In order to provide a wide FOV image with a large exit pupil, the present paper proposes a novel head mounted projective display (HMPD) using a hyperbolic half-silvered mirror, rather than a conventional planar mirror. The first bench-top prototype has successfully shown wide field-of-view projection capability.
[Head, Eyes, Augmented Reality, Optical distortion, tiling, Optical imaging, augmented reality, helmet mounted displays, wide field-of-view head mounted projective display, Augmented reality, multiple curved mirrors, computer graphics, Head Mounted Projective Display, Optical design, Hyperbolic Mirror, Wide Field-of-view, Virtual reality, computer vision, multiple display units, Three dimensional displays, Mirrors, Optical refraction, hyperbolic half-silvered mirrors, Virtual Reality]
A System Architecture for Ubiquitous Tracking Environments
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
Ubiquitous tracking setups, covering large tracking areas with many heterogeneous sensors of varying accuracy, require dedicated middleware to facilitate development of stationary and mobile applications by providing a simple interface and encapsulating the details of sensing, calibration and sensor fusion. In this paper we present a centrally coordinated peer-to-peer architecture for ubiquitous tracking, where a server computes optimal data flow configurations for sensor and application clients, which are directly exchanging tracking data with low latency using a light-weight data flow framework. The server's decisions are inferred from an actively maintained central spatial relationship graph (SRG) using spatial relationship patterns. The system is compared to a previous Ubitrack implementation using the highly distributed DWARF middleware. It exhibits significantly better performance in a reference scenario.
[Sensor fusion, augmented reality, ubiquitous tracking environments, ubiquitous computing, I.4.8 [Image Processing and Computer Vision, software architecture, heterogeneous sensors, Runtime, Virtual reality, Image Processing and Computer Vision, spatial relationship patterns, central spatial relationship graph, middleware, Pervasive computing, peer-to-peer computing, Peer to peer computing, Calibration, Application software, Middleware, Intelligent sensors, system architecture, coordinated peer-to-peer architecture, Information Interfaces and Presentation, Optical sensors, DWARF middleware]
Measurement of absolute latency for video see through augmented reality
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
Latency is a key property of video see through AR systems since users' performance is strongly related to it. However, there is no standard way of latency measurement of an AR system in the literature. We have created a stable and comparable way of estimating the latency in a video see through AR system. The latency is estimated by encoding the time in the image and decoding the time after camera feedback. We have encoded the time as a translation of a circle in the image. The cross ratio has been used as an image feature that is preserved in a projective transformation. The encoding allows for a simple but accurate way of decoding. We show that this way of encoding has an adequate accuracy for latency measurements. As the method allows for a series of automatic measurements we propose to visualize the measurements in a histogram. This histogram reveals meaningful information about the system other than the mean value and standard deviation of the latency. The method has been tested on four different AR systems that use different camera technology, resolution and frame rates.
[Visualization, Information Interfaces And Presentation, absolute latency, augmented reality, Encoding, Decoding, video coding, video see through augmented reality, Delay, Augmented reality, Histograms, Image coding, camera feedback, Measurement standards, Feedback, Cameras, image coding, H.5.2 [Information Interfaces And Presentation]
Precise Geometric Registration by Blur Estimation for Vision-based Augmented Reality
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
This paper proposes an accurate geometric registration method by estimating blur effects from a degraded image with image markers for augmented reality. A small and inexpensive camera used in augmented reality systems sometimes captures degraded images because its focus and/or iris are fixed. This degradation of a captured image affects the accuracy of the detected positions of feature points in the image. The proposed method improves the accuracy of the estimated camera position and posture by estimating blur effects from the captured image, and by correcting the detected positions of feature points through the results. The effectiveness of the method is confirmed through experiments of corner estimation from simulated images and extrinsic camera parameter estimation from real images.
[Computer vision, Motion estimation, image registration, augmented reality, Augmented reality, point spread function, Degradation, image sensors, Iris, image markers, Layout, blur effects, Focusing, computer vision, Cameras, Robustness, Motion detection, geometric registration, blur estimation, vision-based augmented reality]
Parallel Tracking and Mapping for Small AR Workspaces
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
This paper presents a method of estimating camera pose in an unknown scene. While this has previously been attempted by adapting SLAM algorithms developed for robotic exploration, we propose a system specifically designed to track a hand-held camera in a small AR workspace. We propose to split tracking and mapping into two separate tasks, processed in parallel threads on a dual-core computer: one thread deals with the task of robustly tracking erratic hand-held motion, while the other produces a 3D map of point features from previously observed video frames. This allows the use of computationally expensive batch optimisation techniques not usually associated with real-time operation: The result is a system that produces detailed maps with thousands of landmarks which can be tracked at frame-rate, with an accuracy and robustness rivalling that of state-of-the-art model-based systems.
[Algorithm design and analysis, hand-held camera, parallel mapping, Tracking, Robot vision systems, augmented reality, robot vision, robotic exploration, Yarn, Concurrent computing, Simultaneous localization and mapping, Handheld computers, SLAM (robots), batch optimisation techniques, Layout, SLAM algorithms, Cameras, Robustness, parallel tracking]
Deformable Surface Augmentation in Spite of Self-Occlusions
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
The augmentation problem for images of a deforming surface has been studied since recently. The surface is usually assumed not to be self-occluding. Two dimensional deformation estimation in the presence of self-occlusions is very challenging. This paper proposes a specific framework explicitly modeling self-occlusions for augmented reality applications. The basic idea is to detect self-occlusions as warp shrinkage areas. Deformations are initially estimated via direct non-rigid image registration. Temporal smoothness is then used to refine the warps and the image are augmented. Experimental results on several challenging datasets show that our approach convincingly augments self-occluded surfaces. Associated videos are available on the first author's Web homepage.
[Deformable models, Image registration, deformable surface augmentation, Video sequences, image registration, 2D self-occlusion modeling, warp shrinkage area, augmented reality, image augmentation, augmented reality application, Augmented reality, hidden feature removal, Cameras, deformation, Erbium, direct nonrigid image registration]
A Fast Initialization Method for Edge-based Registration Using an Inclination Constraint
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
We propose a hybrid camera pose estimation method using an inclination sensor value and correspondence-free line segments. In this method, possible azimuths of the camera pose are hypothesized by a voting method under an inclination constraint. Then some camera positions for each possible azimuth are calculated based on the detected line segments that affirmatively voted for the azimuth. Finally, the most consistent one is selected as the camera pose out of the multiple sets of the camera positions and azimuths. Unlike many other tracking methods, our method does not use past information but rather estimates the camera pose using only present information. This feature is useful for an initialization measure of registration in augmented reality (AR) systems. This paper describes the details of the method and shows its effectiveness with experiments in which the method is actually used in an AR application.
[Image edge detection, Laboratories, image registration, Humans, augmented reality, edge-based registration, hybrid camera pose estimation method, image sensors, Image segmentation, correspondence-free, Azimuth, inclination constraint, line segments, Space technology, Voting, Layout, pose estimation, inclination sensor value, Cameras, initialization, Robustness, edge detection, fast initialization method]
Retexturing in the Presence of Complex Illumination and Occlusions
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
We present a nonrigid registration technique that achieves spatial, photometric, and visibility accuracy. It lets us photo-realistically augment 3D deformable surfaces under complex illumination conditions and in spite of severe occlusions. There are many approaches that address some of these issues but very few that simultaneously handle all of them as we do. We use triangulated meshes to model the geometry and introduce explicit visibility maps as well as separate illumination parameters for each mesh vertex. We cast our registration problem in an expectation maximization framework that allows robust and fully automated operation. It provides explicit illumination and occlusion models that can be used for rendering purposes.
[Occlusion, Solid modeling, Shape, Augmented Reality, augmented reality, mesh vertex, expectation maximization framework, Shadows, Fingers, Lighting, Robustness, Surface texture, Deformable models, nonrigid registration technique, rendering, retexturing, lighting, Augmented reality, computer graphics, Image databases, computer vision, occlusions, expectation-maximisation algorithm, complex illumination, Principal component analysis]
Initializing Markerless Tracking Using a Simple Hand Gesture
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
We introduce a technique to establish a coordinate system for augmented reality (AR) on tabletop environments. A user's hand is tracked and the fingertips on the outstretched hand are detected, providing a camera pose estimation relative to the hand. As a user places the hand on the surface of a tabletop environment, the hand's coordinate system is propagated to the environment, detecting distinctive image features in the scene. The features are tracked fast and robustly using optical flow. In this way, a new tabletop AR environment is set up without having to carry a marker or a sophisticated tracking system to the environment itself. We also demonstrate a proof-of-concept application for establishing a tabletop AR environment and recognizing a scene when detecting its features.
[Computer vision, Eyes, markerless tracking, augmented reality, Augmented reality, hand gesture, Image motion analysis, Computer Graphics, gesture recognition, Wearable computers, Layout, Optical propagation, feature extraction, camera pose estimation, Computer graphics, pose estimation, distinctive image features detection, I.3.7 [Computer Graphics, Image Processing and Computer Vision, Cameras, Personal digital assistants]
Visually Elegant and Robust Semi-Fiducials for Geometric Registration in Mixed Reality
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
This paper describes a novel image-based geometric registration method using visually elegant and robust semi-fiducial markers in mixed reality (MR). Most traditional visual markers stand out against the background, i.e., they can be recognized and identified easily. Here, we try to construct new visual markers for MR registration, which achieve a good balance between visual elegance and robustness. As the first step, we propose "two-tone colored markers". These markers have the following characteristics: 1) color similar to that of the background object, 2) placed at the corners of real objects in an inconspicuous manner. This paper describes the registration method using two- tone colored markers and a few experiments.
[virtual reality, Semi-fiducials, Stability, Multimedia systems, Brightness, image registration, Information systems, Image sensors, computer graphics, Geometric registration, Visual marker, Layout, mixed reality, Mixed reality, Virtual reality, Chromium, Cameras, Robustness, geometric registration, image colour analysis, two-tone colored markers]
Webtag: A World Wide Internet Based AR System
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
Webtag is a marker based system where every marker contains an internationally unique identifier which links to 2D or 3D content from a user's Website. A two stage design combines the basic 10-bit ID (tier-1 ID) of the current ARTag system which is used for pose tracking when the marker is far away, and a smaller and dense array of extra bits allow an additional 32 bits (tier-2 ID) to be recognized when the camera is close. A user brings his/her AR device close to the marker such that the tier-2 ID is read, and then the IDs are mapped to a web address from which the augmenting model, image, or animation is loaded. Thereafter the content is rendered relative to the tier-1 ID which can be seen at greater distances. This allows a user anywhere on internet to access augmented content printed in magazines, seen on posters, etc. Webtag is a prototype system that may allow large scale acceptance of AR by the public.
[Computer vision, self-identifying patterns, augmented reality, Uniform resource locators, pose tracking, computer graphics, Cellular phones, Councils, Webtag, marker detection, Intrusion detection, computer vision, Cameras, Animation, Robustness, Internet, Web sites, fiducial markers]
Mosaicing a Wide Geometric Field of View for Effective Interaction in Augmented Reality
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
In this paper, we have addressed the usability issue of augmented reality systems. In order to overcome the lack of view into the interaction space in nominal AR systems, we proposed to create and use a mosaiced image as a cost effective and practical solution. One of the obvious problem is that even if the view span is increased by the mosaicing, because of the fixed and limited resolution/size of the display(either HMD or upright display), the content must be viewed in smaller scale. Yang et al. has found out that increasing the geometric field of view can be tolerated to some degree without much degradation in task performance [3]. In our pilot experiment, a comparison of our mosaiced AR display to the nominal set up improved task performance when the task involved manipulation of many tangible props in a relatively wide area. Our future work is to conduct a more formal usability study and address other AR usability issues.
[Head, Transmission line matrix methods, Tracking, Augmented Reality, video cameras, Spatial Awareness, Displays, augmented reality, helmet mounted displays, Haptic interfaces, augmented reality display resolution, Augmented reality, Equations, Computer science, image mosaicing, augmented reality system, image segmentation, Virtual reality, usability issue, Mosaicing, Cameras, head mounted display, human computer interaction]
Adaptive Augmented Reality Using Context Markup and Style Maps
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
Augmented reality (AR) enables users to visualize synthetic information overlaid on top of real imagery. Such visualization may be achieved by tools that distort, filter or enhance the explored information. However, little to no work has focused on the separation of style definitions and their mapping to scene objects. We target this separation based on a context rich scenegraph. Our research allows the definition of visualization styles independent on the data to be visualized.
[Object overlay and spatial layout techniques, Data structures, Displays, augmented reality, Information filtering, realistic images, context markup, Augmented reality, visualize synthetic information, scenegraph, interaction techniques for MR/AR, Layout, Data visualization, data visualisation, Computer graphics, style maps, Rendering (computer graphics), Information filters, adaptive augmented reality, real-time rendering, Lenses]
A High-level Event System for Augmented Reality
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
3D graphics systems increasingly rely on sophisticated event systems derived from collision detection mechanisms, which support the discretisation of Physics as well as high-level programming and scripting. By contrast, augmented reality systems have not yet adopted this approach. We describe the development of a high-level event system on top of the ARToolkit environment incorporating the ODE Physics engine. We first define a typology of events encompassing interactions between virtual objects as well as interactions involving markers. We then describe how these events can be recognised in real-time from elementary collisions detected by the ODE Physics engine. We conclude by discussing examples of high-level event recognitions and how they can support the development of applications.
[Event detection, Multimedia systems, high-level event system, Augmented Reality, augmented reality, ARToolKit, Application software, Augmented reality, Engines, Graphics, Event Systems, scripting, computer graphics, augmented reality systems, Physics computing, high-level programming, Virtual reality, computer vision, Chromium, Sampling methods, 3D graphics systems]
Accelerating Template-Based Matching on the GPU for AR Applications
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
Recently researchers have shown that it is possible to use GPU hardware for image processing and computer vision algorithms. We have been exploring how to use GPU hardware to improve marker- based tracking for AR Applications. In this paper we describe our findings and explored issues in the context of a standard fiducial tracking pipeline. We demonstrate the implementation of a template matching process on the GPU and the performance improvement gained in comparison to a traditional CPU implementation.
[image processing, Computer vision, Costs, Image processing, Pipelines, augmented reality, fiducial tracking pipeline, Computing Methodologies, Application software, AR applications, image matching, computer graphics, template-based matching, marker-based tracking, computer vision, GPU hardware, Hardware, Libraries, Robustness, Acceleration, I.4.8 [Computing Methodologies, Pattern matching]
ARMO: Augmented Reality based Reconfigurable MOck-up
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
Rapid prototyping allows for evaluation of design product in a short period of time. Designers or CEOs may perceive the size and appearance of the product by touching the automatically constructed physical objects produced through rapid prototyping. In this paper, we introduce 'augmented reality based reconfigurable mock-up', which enables interactive changes of shapes of products as well as colors, textures, and user interfaces. The shapes of the physical objects are reconfigurable by assembling different parts to the main body of the mock-up. Augmented reality technologies were used to alter the appearance of the mock-up by rendering the 3D models, colors, and interfaces accordingly. Developed AR-based reconfigurable mock-up is expected to be used for realistic design evaluation and CEO presentations.
[Process design, design evaluation, Design automation, Shape, software prototyping, rapid prototyping, Augmented Reality, augmented reality, Product design, mock-up, Augmented reality, computer graphics, interactive changes, reconfigurable mock-up, Prototypes, Computer graphics, computer vision, User interfaces, Rendering (computer graphics), Assembly]
Overlay what Humanoid Robot Perceives and Thinks to the Real-world by Mixed Reality System
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
One of the problems in developing a humanoid robot is caused by the fact that intermediate results, such as what the robot perceives the environment, and how it plans its moving path are hard to be observed online in the physical environment. What developers can see is only the behavior. Therefore, they usually investigate logged data afterwards, to analyze how well each component worked, or which component was wrong in the total system. In this paper, we present a novel environment for robot development, in which intermediate results of the system are overlaid on physical space using mixed reality technology. Real-time observation enables the developers to see intuitively, in what situation the specific intermediate results are generated, and to understand how results of a component affected the total system. This feature makes the development efficient and precise. This environment also gives a human-robot interface that shows the robot internal state intuitively, not only in development, but also in operation.
[virtual reality, human-robot interface, Tracking, RNA, Humanoid robots, Humans, humanoid robot, Independent component analysis, mixed reality system, man-machine systems, Intelligent robots, Orbital robotics, Humanoid robot, Space technology, Virtual reality, Robot sensing systems, humanoid robots, control engineering computing, robot development]
Vesp'R - Transforming Handheld Augmented Reality
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
This paper presents first results of an interaction design study performed for a novel handheld interaction system. Human factors of mid-size, self-containing wearable computer systems are explored.
[wearable computer systems, human factors, Human factors, Fatigue, Displays, augmented reality, Handheld AR, Augmented reality, wearable computers, Ergonomics, Wearable computers, Cellular phones, Computer graphics, spatial user interface, Cameras, handheld interaction system, Personal digital assistants]
Reliving Museum Visiting Experiences on-and-off the Spot
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
We have developed a GUI tool and a mobile MR system for reliving experiences at museums. The GUI tool was developed to provide services based on content made from activity logs and to enable effective analysis of visitors' behaviors using the logs. In addition, by running the GUI tool on a mobile MR system, users can browse visitors' activities with a sensation of realism on the spot. This paper describes the GUI tool and a pilot user study that was conducted to evaluate it.
[Multimedia systems, reliving experience, Control systems, mixed reality system, museum visiting experiences, Information technology, GUI tool, GIS, wearable mixed reality system, computer graphics, mobile computing, humanities, Layout, Data visualization, Virtual reality, Bicycles, Chromium, Mice, on-and-off the spot, museum guide, mobile MR system, Graphical user interfaces]
Identifying differences between CAD and physical mock-ups using AR
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
Since the last ten years product development in automotive industry is changing radically. Most physical mock-ups have vanished and are now replaced by digital ones. But they are still needed for final evaluations or issues, which cannot be adequately simulated. During their production, deviations from the CAD model may be made. Since digital and real mock-up must match for the further product development, the transfer of differences between physical and digital mock-up to the CAD format is a crucial issue. In this paper an augmented reality (AR) based tool-chain is presented, which allows matching the CAD data with real mock-ups and documents the differences between them. Essential functions like measurement and online construction are provided, allowing the end-users to create information in AR space and feeding them back into the CAD model.
[Solid modeling, Design automation, Switches, augmented reality, tracking, AR based tool-chain, mixed reality application, Surface emitting lasers, data visualisation, product development, Laser modes, CAD model, CAD, Light emitting diodes, Extraterrestrial measurements, rendering, automobile industry, automotive industry product development, Augmented reality, visualization system, product design, physical mock-ups, machine tools, Cameras, Product development]
A Framework for Tangible User Interfaces within Projector-based Mixed Reality
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
This paper proposes a framework named TIPMR, for designing tangible user interfaces (TUIs) within projector-based mixed reality applications. The framework divides the target application into three parts: GUI-based application, TUI and an assistant. The assistant is employed as an adapter to translate between TUI operations and general GUI commands like mouse or keyboard events. This architecture makes it easier to focus on designing GUI-based applications and TUI separately. We built a tourist guidance system with two different tangible interaction modes based on TIPMR to demonstrate its usefulness and efficiency.
[Computer interfaces, virtual reality, graphical user interfaces, GUI commands, graphical user interface, tangible user interface, Electronic mail, user interfaces, Application software, tangible user interfaces, projector-based mixed reality, Keyboards, Virtual reality, Computer architecture, User interfaces, Mice, Joining processes, extended MCRpd model, Graphical user interfaces]
Visualizing Occluded Physical Objects in Unfamiliar Outdoor Augmented Reality Environments
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
This paper describes techniques to allow both the visualization of hidden objects, and removal of real objects, for a mobile augmented reality user. A gesture based technique is also described which allows the user to select when to view occluded objects. By using the real-time modeling techniques of the Tinmith system, the user is able to create the required geometry to allow image based rendering techniques to be used to render corrected images on the user's display. The images of the hidden objects are captured by a mobile robot platform that is controlled by the mobile user.
[Visualization, Solid modeling, Wearable Computers, augmented reality, Outdoor Augmented Reality, Mobile robots, hidden objects visualization, Wearable computers, data visualisation, Computer graphics, 3D Modeling, rendering (computer graphics), unfamiliar outdoor augmented reality environments, Tinmith system, Telepresence, Navigation, real-time modeling techniques, Image-based Rendering, Augmented reality, Computer displays, occluded physical objects, computer vision, mobile robot platform, Rendering (computer graphics), gesture based technique, Mobile computing]
Efficient Extraction of Robust Image Features on Mobile Devices
2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
None
2007
Recent convergence of imaging sensors and general purpose processors on mobile phones creates an opportunity for a new class of augmented reality applications. Robust image feature extraction is a crucial enabler of this type of systems. In this article, we discuss an efficient mobile phone implementation of a state-of-the-art algorithm for computing robust image features called SURF. We implement several improvements to the basic algorithm that significantly improve its performance and reduce its memory footprint making the use of this algorithm on the mobile phone practical. Our prototype implementation has been applied to several practical applications such as image search, object recognition and augmented reality applications.
[object recognition, mobile phones, Sensor phenomena and characterization, augmented reality, Mobile handsets, Application software, Augmented reality, Convergence, Image sensors, robust image feature extraction, computer graphics, Image matching, feature extraction, computer vision, Cameras, Feature extraction, memory footprint, Robustness, image search, mobile handsets]
Corporate supporter
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
The conference organizers greatly appreciate the support of the various corporate sponsors listed.
[]
Messages
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
Presents the introductory welcome message from the conference proceedings.
[]
From the program cochairs
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
Presents the introductory welcome message from the conference proceedings.
[]
IEEE Visualization and Graphics Technical Committee (VGTC)
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
Provides a listing of current committee members.
[]
Technical Committee on Wearable Information Systems (TCWIS)
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
Provides a listing of current committee members.
[]
Task Force on Human Centered Computing (TFHCC)
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
false
[]
Conference committee
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
Provides a listing of current committee members.
[]
Using the marginalised particle filter for real-time visual-inertial sensor fusion
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
The use of a particle filter (PF) for camera pose estimation is an ongoing topic in the robotics and computer vision community, especially since the FastSLAM algorithm has been utilised for simultaneous localisation and mapping (SLAM) applications with a single camera. The major problem in this context consists in the poor proposal distribution of the camera pose particles obtained from the weak motion model of a camera moved freely in 3D space. While the FastSLAM 2.0 extension is one possibility to improve the proposal distribution, this paper addresses the question of how to use measurements from low-cost inertial sensors (gyroscopes and accelerometers) to compensate for the missing control information. However, the integration of inertial data requires the additional estimation of sensor biases, velocities and potentially accelerations, resulting in a state dimension, which is not manageable by a standard PF. Therefore, the contribution of this paper consists in developing a real-time capable sensor fusion strategy based upon the marginalised particle filter (MPF) framework. The performance of the proposed strategy is evaluated in combination with a marker-based tracking system and results from a comparison with previous visual-inertial fusion strategies based upon the extended Kalman filter (EKF), the standard PF and the MPF are presented.
[inertial data, Noise, particle filtering (numerical methods), sensor fusion, Proposals, (marginalised) particle filter, real-time, (extended) Kalman filter, Image processing and computer vision, pose estimation, Computer graphics, Probability and statistics, Mathematical model, Kalman filters, real-time visual-inertial sensor fusion, G.3 [Probability and statistics, inertial sensors, robot vision, marginalised particle filter, gyroscopes, marker-based tracking system, Atmospheric measurements, camera pose estimation, extended Kalman filter, Cameras, Particle measurements, accelerometers, nonlinear filtering, Acceleration, Artificial intelligence, Fast-SLAM]
Dynamic gyroscope fusion in Ubiquitous Tracking environments
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
Ubiquitous tracking (Ubitrack) setups, consisting of many previously unknown sensors, offer many possibilities to perform sensor fusion in order to increase robustness and accuracy. In particular, the dynamic combination of mobile and stationary trackers enables the creation of new wide-area tracking concepts. In this work, we present a setup in which a gyroscope is dynamically fused with three different mobile and stationary sensors, based on the concepts of spatial relationship graphs (SRGs) and patterns. For this, we contribute new patterns that, based on well-known algorithms, enable the transformation of rotation velocity and the fusion with different absolute trackers. The usefulness of the approach is shown in a system that automatically reconfigures the SRG based on course tracking data, and, depending on the structure of this SRG, automatically selects a suitable fusion algorithm.
[Tracking, spatial relationship graphs, Augmented Reality, H.5.1 [Information Interfaces and Presentation, augmented reality, sensor fusion, Calibration, ubiquitous tracking environments, Inertial Sensors, gyroscopes, tracking, Sensor Fusion, Computer Graphics, Quaternions, Cameras, Information Interfaces and Presentation, Gyroscopes, Sensors, dynamic gyroscope fusion, Rotation measurement]
Relative pose calibration of a spherical camera and an IMU
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
This paper is concerned with the problem of estimating the relative translation and orientation of an inertial measurement unit and a spherical camera, which are rigidly connected. The key is to realize that this problem is in fact an instance of a standard problem within the area of system identification, referred to as a gray-box problem. We propose a new algorithm for estimating the relative translation and orientation, which does not require any additional hardware, except a piece of paper with a checkerboard pattern on it. The experimental results show that the method works well in practice.
[gray-box problem, spherical camera, Predictive models, pose calibration, inertial sensors, Calibration, cameras, image sensors, inertial measurement unit, Quaternions, Measurement uncertainty, pose estimation, Position measurement, Cameras, Optical sensors, system identification, calibration]
A differential GPS carrier phase technique for precision outdoor AR tracking
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
This paper presents a differential GPS carrier phase technique for 3D outdoor position tracking in mobile augmented reality (AR) applications. It has good positioning accuracy, low drift and jitter, and low computation requirement. It eliminates the resolution of integer ambiguities. The position from an initial point is tracked by accumulating the displacement in each time step, which is determined using Differential Single Difference. Preliminary results using low cost GPS receivers show that the position error is 10 cm, and the drift is 0.001 ms-1, which can be compensated using linear models. Stable and accurate augmentations in outdoor scenes are demonstrated.
[Phase measurement, differential GPS carrier phase, integer ambiguities, Global positioning system, Receivers, Jitter, H.5.2 User Interfaces: Input devices, Mobile communication, augmented reality, differential carrier phase, augmented, tracking, differential single difference, Global Positioning System, Satellites, Accuracy, mobile computing, precision outdoor AR tracking, Position tracking, H.5.1 Multimedia Information Systems: Artificial, mobile augmented reality, outdoor augmented reality, 3D outdoor position tracking, and virtual realities]
Optical free-form surfaces in off-axis head-worn display design
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
Head-worn displays are becoming a viable visual display option for mobile augmented reality. In this paper, we highlight the challenges and some recent progress towards compact and lightweight head-worn displays tending towards the eyeglass form factor. Single and dual-element eyeglass display designs that leverage the advances in free-form optics fabrication technology will be presented. There are three new contributions in this paper: first, we present a single-element eyeglass display design and the fabricated mirror prototype; second, we present our second generation dual-element eyeglass display design with the revised opto-mechanical packaging; third, we present our initial studies on the field of view limit with an 8 mm pupil for the dual-element design.
[dual-element eyeglass display design, mirrors, free-form, free-form optics fabrication technology, augmented reality, helmet mounted displays, Optics, radial basis functions, single-element eyeglass display design, optical design techniques, optical free-form surface, optical system design, Numerical Analysis, fabricated mirror prototype, Hardware, opto-mechanical packaging, mobile augmented reality, B.4.2 [Hardware, optical fabrication, lightweight off-axis head-worn display design, optical glass]
An optical see-through head mounted display with addressable focal planes
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
Most existing stereoscopic head mounted displays (HMDs), presenting a pair of stereoscopic images at a fixed focal distance, lack the ability to correctly render the naturally coupled accommodation and convergence cues. Psychophysical studies have shown that such displays may cause many adverse consequences such as visual fatigue, diplopic vision, degraded oculomotor response, and depth perception errors. In this paper, we present a see-through HMD with addressable focal planes utilizing a novel active optical element - a liquid lens. The element, with a varying optical power from -5 to 20 diopters, is able to address the focal distance of the HMD from infinity to the near point of the eye. A monocular prototype was built from off-the-shelf elements and experimental results are presented to validate the proposed designs. We also describe both subjective and objective measurements of the accommodation responses of the viewer to the focal distances presented by the prototype.
[Display hardware, liquid lens, active optical element, usability studies and experiments, Performance of Systems, helmet mounted displays, stereoscopic image, Computer Graphics, addressable focal plane, focal planes, stereo image processing, accommodation, head mounted display, I.3.1 [Computer Graphics, Information Interfaces and Presentation, optical see-through head mounted display]
Photometric registration by adaptive high dynamic range image generation for augmented reality
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
This paper describes photometric registration for augmented reality (AR) using a high-dynamic-range (HDR) image. In photorealistic AR, estimating the lighting environment of virtual objects is difficult because of low dynamic range cameras. In order to overcome this problem, we propose a method that estimates the lighting environment from an HDR image and renders virtual objects using an HDR environment map. Virtual objects are overlaid in real-time by adjusting the dynamic range of the rendered image with tone mapping according to the exposure time of the camera. The HDR image is generated from multiple images captured with various exposure times. We have found through experimentation that the updating rate is improved by effectively limiting the dynamic range, depending on the exposure time. We have verified the effect of limiting the dynamic range on the reality of virtual objects.
[image registration, Estimation, virtual object rendering, Dynamic range, tone mapping, augmented reality, photometry, photometric registration, H.5.1 [Information interfaces and presentation, Light sources, adaptive high dynamic range image generation, Reflectivity, Information interfaces and presentation, Lighting, Image generation, Cameras]
Compositing for small cameras
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
To achieve a realistic integration of virtual and real imagery in video see-through augmented reality, the rendered images should have a similar appearance and quality to those captured by the video camera. This paper describes a compositing method which models the artefacts produced by a small low-cost camera, and adds these effects to an ideal pinhole image produced by conventional rendering methods. We attempt to model and simulate each step of the imaging process, including distortions, chromatic aberrations, blur, Bayer masking, noise and colour-space compression, all while requiring only an RGBA image and an estimate of camera velocity as inputs.
[Bayer masking, real imagery, Noise, aberrations, augmented reality, image distortions, virtual imagery, image blur, Graphics, small cameras, Image color analysis, chromatic aberrations, RGBA image, Image generation, colour-space compression, Cameras, compositing, image rendering, rendering (computer graphics), video signal processing, noise compression, Pixel, Lenses]
OutlinAR: an assisted interactive model building system with reduced computational effort
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
This paper presents a system that allows online building of 3D wireframe models through a combination of user interaction and automated methods from a handheld camera-mouse. Crucially, the model being built is used to concurrently compute camera pose, permitting extendable tracking while enabling the user to edit the model interactively. In contrast to other model building methods that are either off-line and/or automated but computationally intensive, the aim here is to have a system that has low computational requirements and that enables the user to define what is relevant (and what is not) at the time the model is being built. OutlinAR hardware is also developed which simply consists of the combination of a camera with a wide field of view lens and a wheeled computer mouse.
[Solid modeling, Visualization, Computational modeling, Buildings, optical tracking, augmented reality, user interaction, tracking, Augmented reality, camera pose, handheld camera-mouse, 3D wireframe model, interactive systems, online building, Cameras, Three dimensional displays, human computer interaction, assisted interactive model building system, solid modelling, OutlinAR]
Fast annotation and modeling with a single-point laser range finder
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
This paper presents methodology for integrating a small, single-point laser range finder into a wearable augmented reality system. We first present a way of creating object-aligned annotations with very little user effort. Second, we describe techniques to segment and pop-up foreground objects. Finally, we introduce a method using the laser range finder to incrementally build 3D panoramas from a fixed observerpsilas location. To build a 3D panorama semi-automatically, we track the systempsilas orientation and use the sparse range data acquired as the user looks around in conjunction with real-time image processing to construct geometry around the userpsilas position. Using full 3D panoramic geometry, it is possible for new virtual objects to be placed in the scene with proper lighting and occlusion by real world objects, which increases the expressivity of the AR experience.
[Solid modeling, real-time image processing, augmented reality, object-aligned annotations, I.4.8 [Image Processing and Computer Vision, 3D panoramas, Computer Graphics, Image color analysis, laser ranging, object segmentation, Lasers, image segmentation, pop-up foreground objects, Image Processing and Computer Vision, Cameras, Three dimensional displays, wearable augmented reality system, Laser applications, Pixel, single-point laser range finder]
User evaluation of see-through vision for mobile outdoor augmented reality
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
We have developed a system built on our mobile AR platform that provides users with see-through vision, allowing visualization of occluded objects textured with real-time video information. We present a user study that evaluates the userpsilas ability to view this information and understand the appearance of an outdoor area occluded by a building while using a mobile AR computer. This understanding was compared against a second group of users who watched video footage of the same outdoor area on a regular computer monitor. The comparison found an increased accuracy in locating specific points from the scene for the outdoor AR participants. The outdoor participants also displayed more accurate results, and showed better speed improvement than the indoor group when viewing more than one video simultaneously.
[Occlusion, Visualization, see-through vision, object visualization, Wearable Computers, augmented reality, user evaluation, Outdoor Augmented Reality, Computer Graphics, Accuracy, mobile computing, data visualisation, I.3.6 [Computer Graphics, Three dimensional displays, video signal processing, Telepresence, Buildings, Image-based Rendering, real-time video information, occluded object, Augmented reality, mobile outdoor augmented reality, computer vision, Streaming media, Cameras]
An evaluation of graphical context when the graphics are outside of the task area
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
An ongoing research problem in Augmented Reality (AR) is to improve tracking and display technology in order to minimize registration errors. However, perfect registration is not always necessary for users to understand the intent of an augmentation. This paper describes the results of an experiment to evaluate the effects of graphical context in a Lego block placement task when the graphics are located outside of the task area. Four conditions were compared: fully registered AR; non-registered AR; a heads-up display (HUD) with the graphics always visible in the field of view; and a HUD with the graphics not always visible in the field of view. The results of this experiment indicated that registered AR outperforms both non-registered AR and graphics displayed on a HUD. The results also indicated that non-registered AR does not offer any significant performance advantages over a HUD, but is rated as less intrusive and can keep non-registered graphics from cluttering the task space.
[heads-up display, registration error minimization, Psychology, H.5.1 [Information Interfaces and Presentation, Maintenance engineering, augmented reality, Sensor systems, Augmented reality, Graphics, AR, graphical context, human-computer interaction, Cameras, Lego block placement task, Information Interfaces and Presentation, Sensors]
The effect of registration error on tracking distant augmented objects
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
We conducted a user study of the effect of registration error on performance of tracking distant objects in augmented reality. Categorizing error by types that are often used as specifications, we hoped to derive some insight into the ability of users to tolerate noise, latency, and orientation error. We used measurements from actual systems to derive the parameter settings. We expected all three errors to influence userspsila ability to perform the task correctly and the precision with which they performed the task. We found that high latency had a negative impact on both performance and response time. While noise consistently interacted with the other variables, and orientation error increased user error, the differences between ldquohighrdquo and ldquolowrdquo amounts were smaller than we expected. Results of userspsila subjective rankings of these three categories of error were surprisingly mixed. Users believed noise was the most detrimental, though statistical analysis of performance refuted this belief. We interpret the results and draw insights for system design.
[distant object tracking, Noise, Buildings, image registration, H.5.1 [Information Interfaces and Presentation, augmented reality, object detection, Calibration, Models and Principles, tracking, Vehicles, registration error effect, Graphics, Measurement uncertainty, Distance measurement, Information Interfaces and Presentation, statistical analysis]
Mobile Augmented Reality in industrial applications: Approaches for solution of user-related issues
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
Augmented Reality (AR) uses computer-generated virtual information to enhance the userpsilas information access. While numerous previous studies have demonstrated the large potential of AR to improve industrial processes by enhancing product quality and reducing production times it is still unclear if and how long term usage of such AR technology produces stress and strain. This paper presents an approach to use the analysis of Heart Rate Variability to objectively measure current user strain during different work tasks. Results of a user study comparing strain during an AR supported and a non-AR supported work task in a laboratory setting are presented and discussed.
[Psychology, human factors, INFORMATION INTERFACES AND PRESENTATION, Mobile communication, augmented reality, Electronic mail, Augmented reality, Strain, industrial manufacturing, mobile computing, computer-generated virtual information, product quality, Electrocardiography, H.1.2 [MODELS AND PRINCIPLES, mobile augmented reality, Heart rate variability, heart rate variability analysis, MODELS AND PRINCIPLES]
Supporting order picking with Augmented Reality
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
We report on recent progress in the iterative process of exploring, evaluating and refining Augmented Reality-based methods to support the order picking process. We present our findings from three user studies and from demonstrations at several exhibitions. The resulting setup is a combined visualization to precisely and efficiently guide the user, even if the augmentation is not always in the field of view of the HMD.
[Fading, Visualization, Navigation, order picking process, INFORMATION INTERFACES AND PRESENTATION, Mobile communication, augmented reality, order picking, logistics, user interfaces, iterative process, Augmented reality, user interface, goods dispatch data processing, logistics data processing, H.5.1 [ INFORMATION INTERFACES AND PRESENTATION, data visualisation, Three dimensional displays, Distance measurement, data visualization, HMD]
Virtual redlining for civil engineering in real environments
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
Field workers of utility companies are regularly engaged in outdoor tasks such as network planning and inspection of underground infrastructure. Redlining is the term used for manually annotating either printed paper maps or a 2D geographic information system on a notebook computer taken to the field. Either of these approaches requires finding the physical location to be annotated on the physical or digital map. In this paper, we describe a mobile Augmented Reality (AR) system capable of supporting virtual redlining. The AR visualization delivered by the system is constructed from data directly extracted from a GIS used in day-to-day production by utility companies. We also report on encouraging trials and interviews performed with professional field workers from the utility sector.
[Solid modeling, AR visualization, microcomputer applications, H.5.1 [Information Interfaces and Presentation, Companies, redlining, augmented reality, geographic information systems, notebook computer, 2D geographic information system, data visualisation, Three dimensional displays, public utilities, virtual redlining, mobile augmented reality, Geographic information systems, digital map, underground infrastructure visualization, civil engineering computing, Augmented reality, underground infrastructure inspection, civil engineering, Data visualization, engineering graphics, Information Interfaces and Presentation, Planning, network planning, Augmented Reality system]
The design of a mixed-reality book: Is it still a real book?
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
In this paper we present the results of our long term development of a mixed reality book. Most previous work in the area has focused on the technology of augmented reality books, such as providing registration using fiducial markers. In this work, however, we focused on exploring the design and development process of this type of application in a broader sense. We studied the semantics of a mixed reality book, the design space and the user experience with this type of interface.
[augmented reality books, Visualization, augmented reality, user experience, mixed-reality book design, H.5.1 [Information interfaces and presentation, Augmented reality, Information interfaces and presentation, USA Councils, Layout, Virtual reality, Three dimensional displays, Books]
An Augmented Reality museum guide
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
Recent years have seen advances in many enabling augmented reality technologies. Furthermore, much research has been carried out on how augmented reality can be used to enhance existing applications. This paper describes our experiences with an AR-museum guide that combines some of the latest technologies. Amongst other technologies, markerless tracking, hybrid tracking, and an ultra-mobile-PC were used. Like existing audio guides, the AR-guide can be used by any museum visitor, during a six-month exhibition on Islamic art. We provide a detailed description of the museumpsilas motivation for using AR, of our experiences in developing the system, and the initial results of user surveys. Taking this information into account, we can derive possible system improvements.
[exhibition, art, ultra-mobile-PC, markerless tracking, Information Systems, Mobile communication, augmented reality, Augmented reality, multimedia in the museum, Computer Graphics, mobile computing, 1.3.6 [Computer Graphics, Computer graphics, exhibitions, Islamic art, Animation, Cameras, Robustness, museum guide, hybrid tracking, Interviews]
Collocated AAR: Augmenting After Action Review with Mixed Reality
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
This paper proposes collocated after action review (AAR) of training experiences. Through mixed reality (MR), collocated AAR allows users to review past training experiences in situ with the userpsilas current, real-world experience. MR enables a user-controlled egocentric viewpoint, a visual overlay of virtual information, and playback of recorded training experiences collocated with the userpsilas current experience. Collocated AAR presents novel challenges for MR, such as collocating time, interactions, and visualizations of previous and current experiences. We created a collocated AAR system for anesthesia education, the augmented anesthesia machine visualization and interactive debriefing system (AAMVID). The system was evaluated in two studies by students (n=19) and educators (n=3). The results demonstrate how collocated AAR systems such as AAMVID can: (1) effectively direct student attention and interaction during AAR and (2) provide novel visualizations of aggregate student performance and insight into student understanding for educators.
[Visualization, Computer Applications, biomedical education, virtual information, augmented reality, After Action Review, J.3 [Computer Applications, Training, collocated after action review, Anesthesia Machine, mixed reality, Data visualization, computer based training, data visualisation, Virtual reality, Anesthesia, Book reviews, User Studies, user-controlled egocentric, medical computing, Mixed Reality, Lenses, augmented anesthesia machine visualization and interactive debriefing system]
Multiple 3D Object tracking for augmented reality
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
We present a method that is able to track several 3D objects simultaneously, robustly, and accurately in real-time. While many applications need to consider more than one object in practice, the existing methods for single object tracking do not scale well with the number of objects, and a proper way to deal with several objects is required. Our method combines object detection and tracking: Frame-to-frame tracking is less computationally demanding but is prone to fail, while detection is more robust but slower. We show how to combine them to take the advantages of the two approaches, and demonstrate our method on several real sequences.
[frame-to-frame tracking, Solid modeling, Target tracking, Tracking, multiple 3D object tracking, Augmented Reality, augmented reality, object detection, tracking, Object detection, computer vision, Feature extraction, Three dimensional displays, Robustness, Computer Vision]
Robust and unobtrusive marker tracking on mobile phones
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
Marker tracking has revolutionized augmented reality about a decade ago. However, this revolution came at the expense of visual clutter. In this paper, we propose several new marker techniques, which are less obtrusive than the usual black and white squares. Furthermore, we report methods that allow tracking beyond the visibility of these markers further improving robustness. All presented techniques are implemented in a single tracking library, are highly efficient in their memory and CPU usage and run at interactive frame rates on mobile phones.
[Tracking, visual clutter, mobile phones, H.5.1 [Information Interfaces and Presentation, augmented reality, Mobile handsets, Augmented reality, mobile computing, mobile phone marker tracking, Image Processing and Computer Vision, marker tracking, Cameras, Feature extraction, Robustness, Information Interfaces and Presentation, pixel flow, Pixel, mobile handsets]
Pose tracking from natural features on mobile phones
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
In this paper we present two techniques for natural feature tracking in real-time on mobile phones. We achieve interactive frame rates of up to 20 Hz for natural feature tracking from textured planar targets on current-generation phones. We use an approach based on heavily modified state-of-the-art feature descriptors, namely SIFT and Ferns. While SIFT is known to be a strong, but computationally expensive feature descriptor, Ferns classification is fast, but requires large amounts of memory. This renders both original designs unsuitable for mobile phones. We give detailed descriptions on how we modified both approaches to make them suitable for mobile phones. We present evaluations on robustness and performance on various devices and finally discuss their appropriateness for augmented reality applications.
[Real time systems, mobile phones, H.5.1 [Information Interfaces and Presentation, augmented reality, Mobile handsets, mobile computing, feature extraction, Detectors, pose estimation, Image Processing and Computer Vision, SIFT feature descriptor, Ferns feature descriptor, Target tracking, textured planar target, Computational modeling, current-generation phone, image texture, pose tracking, mobile phone, target tracking, natural feature tracking, Feature extraction, Cameras, Information Interfaces and Presentation, mobile handsets, natural features]
In-place Augmented Reality
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
In this paper we present a new vision-based approach for transmitting virtual models for augmented reality (AR). A two dimensional representation of the virtual models is embedded in a printed image. We apply image-processing techniques to interpret the printed image and extract the virtual models, which are then overlaid back on the printed image. The main advantages of our approach are: (1) the image of the embedded virtual models and their behaviors are understandable to a human without using an AR system, and (2) no database or network communication is required to retrieve the models. The latter is useful in scenarios with large numbers of users. We implemented an AR system that demonstrates the feasibility of our approach. Applications in education, advertisement, gaming, and other domains can benefit from our approach, since content providers need only to publish the printed content and all virtual information arrives with it.
[image processing, Solid modeling, Visualization, content transmission, printed content, image-processing techniques, augmented reality, Multimedia Information Systems, Augmented reality, H.5.1 [Multimedia Information Systems, model embedding, Image color analysis, Databases, Image Processing and Computer Vision, Libraries, Augmented Reality content, Pixel, dual perception encoding]
An information layout method for an optical see-through head mounted display focusing on the viewability
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
Accessing information when we are on the move is a key feature if mobile computing environments, and using an optical see-through head mounted display (HMD) is one of the most suitable ways to do this. Although the HMD can display information without interfering with the user's view, when the sight behind the display is too complex or too bright, the information displayed can bee very difficult to see. To solve this problem, we have created a way of laying out information for the optical see-through HMD. The ideal area for displaying information is determined by evaluating the sight image behind the HMD captured by a pantoscopic camera mounted on it. Moreover, if there is no suitable area for displaying information, our method select involves using the sight image around users use to the ideal direction and instructing them to face the direction. Our method displays information to ideal areas.
[Optical imaging, helmet mounted displays, Electronic mail, Augmented reality, Equations, mobile computing, H.5.2 [Information Interfaces and Presentation, Layout, pantoscopic camera, Cameras, Information Interfaces and Presentation, information layout method, Mathematical model, optical see-through head mounted display]
Label segregation by remapping stereoscopic depth in far-field augmented reality
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
This paper describes a novel technique for segregating overlapping labels in stereoscopic see-through displays. The present study investigates the labeling of far-field objects, with distances ranging 100-120 m. At these distances the stereoscopic disparity difference between objects is below 1 arcmin, so labels rendered at the same distance as their corresponding objects appear as if on a flat layer in the display. This flattening is due to limitations of both display and human visual resolution. By remapping labels to pre-determined depth layers on the optical path between the observer and the labeled object, an interlayer disparity ranging from 5 to 20 arcmin can be achieved for 5 overlapping labels. The present study evaluates the impact of such depth separation of superimposed layers, and found that a 5 arcmin interlayer disparity yields a significantly lower response time, over 20% on average, in a visual search task compared to correctly registering labels and objects in depth. Notably the performance does not improve when doubling the interlayer disparity to 10 arcmin and, surprisingly, the performance degrades significantly when again doubling the interlayer disparity to 20 arcmin, approximating the performance in situations with no interlayer disparity. These results confirm that our technique can be used to segregate overlapping labels in the far visual field, without the cost associated with traditional label placement algorithms.
[Visualization, overlapping labels, depth separation, H.5.2 [Information Systems, augmented reality, stereoscopic see-through displays, user interfaces, Clutter, stereoscopic depth remapping, visual resolution, label segregation, far-field augmented reality, information layering, visual clutter, Observers, three-dimensional displays, Information Systems, Optical imaging, Computing Methodologies, Label placement, interlayer disparity, superimposed layers, stereoscopic displays, Layout, stereo image processing, Distance measurement, Time factors, stereoscopic disparity difference]
A practical radiometric compensation method for projector-based augmentation
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
Radiometric compensation has made it possible for a projector to display on ordinary surface with colors and textures. For previous methods, itpsilas necessary to calibrate both the projector and camera at first. The calibration can be time-consuming and needs to be redone once the system settings change. We present a method that simplifies the calibration process. As a result, the system is more practicable for ad-hoc setups.
[Real time systems, surface textures, Image resolution, surface colors, Image Correction, Virtual and Augmented Realities, Transfer functions, practical radiometric compensation, H.5.1 [Information Interfaces and Presentation, Radiometric Compensation, augmented reality, Calibration, image texture, Projector-Camera Systems, Computer Graphics, radiometry, Cameras, Radiometry, Information Interfaces and Presentation, image colour analysis, Pixel, projector-based augmentation]
Relighting with real incident light source
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
This paper presents an approach that can relight captured image with real incident light source. We first captured basis images of objects illuminated with a projector as the basis light source. We then established a pixel level mapping relationship between a real incident light source and the basis light source. Finally we developed a relighting algorithm that can simulate the relit effects with the real light source. The techniques can be applied to mixed reality.
[Image-Based Relighting, Image Processing And Computer Vision, pixel level mapping relationship, Glass, Near-Field Reflectance, Management of Computing and Information Systems, Real Incident Light Acquisition, Augmented reality, Light sources, light sources, Reflectivity, basis light source, Computer Graphics, projector, Cameras, AR/MR, incident light source, Arrays, Pixel, image resolution, K.6.1 [Management of Computing and Information Systems]
Optical tracking using commodity hardware
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
We describe a method for using Nintendo Wii controllers as a stereo vision system to perform 3D tracking or motion capture in real time. Commodity consumer hardware allows a wireless, portable tracker to be created that obtains accurate results for a fraction of the cost of conventional setups. Consequently, tracking becomes viable in situations where cost or space were previously prohibitive. Initial results show an accuracy of plusmn2 mm over a large tracking volume.
[Tracking, optical tracking, H.5.1 [Information Interfaces and Presentation, Light emitting diodes, Calibration, Nintendo Wii controllers, stereo vision system, motion compensation, Accuracy, commodity hardware, Image Processing and Computer Vision, stereo image processing, Cameras, Hardware, Information Interfaces and Presentation, Optical sensors, motion capture, 3D tracking]
Augmented robot agent: Enhancing co-presence of the remote participant
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
In this paper, we present a tele-meeting system which uses an augmented robot agent as the representation of the remote participant. In this system, we augment a 3D volume video of the remote participant over the on-site robot. The robot agent in this system represents a remote user with camera, microphone, and mobility. Using this robot agent, the remote user and the local user can show their appearances and interact with tangible interfaces. This robot agent system can be applied to various tele-meeting applications such as tele-marketing and tele-tutoring. For example, we implemented a tele-marketing system to show the feasibility of the suggested system.
[Real time systems, Visualization, 3D volume video augmentation, Robot vision systems, telerobotics, H.5.1 [Information Interfaces and Presentation, augmented reality, Electronic mail, user interfaces, augmented robot agent, tangible interface, tele-meeting system, Cameras, tele-tutoring, Three dimensional displays, Information Interfaces and Presentation, Robots, tele-marketing]
The haunted book
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
This paper describes an artwork that relies on recent computer vision and augmented reality techniques to animate the illustrations of a poetry book. Because we donpsilat need markers, we can achieve seamless integration of real and virtual elements to create the desired atmosphere. The visualization is done on a computer screen to avoid cumbersome head-mounted displays. The camera is hidden into a desk lamp for easing even more the spectator immersion.
[Computers, Visualization, art, visualization, augmented reality, Electronic mail, artwork, augmented reality techniques, Augmented reality, poetry book, computer displays, haunted book, computer vision, computer screen, Animation, Cameras, Books]
Stepping into the operating theater: ARAV &#x2014; Augmented Reality Aided Vertebroplasty
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
Augmented reality (AR) for preoperative diagnostics and planning, intra operative navigation and postoperative follow-up examination has been a topic of intensive research over the last two decades. However, clinical studies showing AR technology integrated into the real clinical environment and workflow are still rare. The incorporation of an AR system as a standard tool into the real clinical workflow has not been presented so far. This paper reports on the strategies and intermediate results of the ARAV - augmented reality aided vertebroplasty project that has been initiated to make an AR system based on a stereo video see-through head mounted display that is permanently available in the operating room (OR).
[Current transformers, preoperative planning, Life and Medical Sciences, H.5.1 [Information Interfaces and Presentation, augmented reality, preoperative diagnostics, Circuit faults, postoperative follow-up examination, Augmented reality, Fault currents, Computed tomography, Surgery, ARAV, Information Interfaces and Presentation, intraoperative navigation, medical computing, augmented reality aided vertebroplasty, Biomedical imaging, operating room]
Augmented assembly using a mobile phone
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
We present a mobile phone based augmented reality (AR) assembly system that enable users to view complex models on their mobile phones. It is based on a client-server architecture, where complex model information is located on a PC, and a mobile phone with the camera is used as a thin client access device to this information. With this system users are able to see an AR view that provides step by step guidance for a real world assembly task. We also present results from a pilot user study evaluating the system, showing that people felt the interface was intuitive and very helpful in supporting the assembly task.
[Solid modeling, augmented reality view, mobile phone based augmented reality, Maintenance engineering, assembling, augmented reality, Mobile handsets, Servers, assembly task, Augmented reality, augmented assembly system, thin client access device, complex model information, Software, client-server architecture, Assembly, mobile handsets]
Haptically extended augmented prototyping
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
This project presents a new display concept, which brings together haptics, augmented and mixed reality and tangible computing within the context of an intuitive conceptual design environment. The project extends the paradigm of augmented prototyping by allowing modelling of virtual geometry on the physical prototype, which can be touched by means of a haptic device. Wireless tracking of the physical prototype is achieved in three different ways by attaching to it a 'Speck', a tracker and Nintendo Wii Remote and it provides continuous tangible interaction. The physical prototype becomes a tangible interface augmented with mixed reality and with a novel 3D haptic design system.
[Tactile &#x00026; Haptic UIs, tangible computing, 3D haptic design system, software prototyping, virtual geometry modelling, Product Design, wireless tracking, Augmented Reality, Glass, haptic interfaces, haptic device, augmented reality, Haptic interfaces, H.5.2 User Interfaces, extended haptic augmented prototyping, Augmented reality, intuitive conceptual design environment, Tangible Computing, Geometry, mixed reality, Prototypes, Three dimensional displays, Mirrors, Mixed Reality]
How to augment the second image? Recovery of the translation scale in image to image registration
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
In this paper, we present an automatic pose estimation (6 DoF) technique to augment images using keyframes pre-registered to a CAD model. State of the art techniques recover the essential matrix (5 DoF) in an automatic manner, but include a manual step to align the image with the CAD reference system because the essential matrix does not provide the scale of the translation. We propose using planar structures to recover this scale automatically and to offer immediate augmentation. These techniques have been implemented in our augmented reality software. Qualitative tests are performed in an industrial environment.
[augmented reality software, Solid modeling, Design automation, Computational modeling, image registration, CAD, augmented reality, planar structures, Augmented reality, automatic pose estimation technique, pose estimation, Streaming media, Three dimensional displays, Distance measurement, CAD reference system, image translation scale]
Generating perceptually-correct shadows for mixed reality
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
When human cannot perceive the inconsistency of artificial shadows which are not physically correct, they are acceptable as ldquoperceptually-correctrdquo shadows. This paper focuses on the simplification of light-source models for generating the perceptually-correct artificial shadows. First, we conducted subjective evaluations to obtain knowledge about the human perception of the shadows. Then the knowledge was applied to control the resolution of the light-source map to generate perceptually-correct artificial shadows. Comparative studies among artificial and real shadows justified perceptually correctness. All experiments were done using still images, not videos. Our research becomes a reference to determine the resolution of light-source map in an MR scene.
[Image resolution, virtual reality, Fluorescence, I.3.7, Augmented reality, light-source models, perceptually-correct artificial shadows, mixed reality, Lighting, Image generation, Virtual reality, Distance measurement, still images, [I.3.7]
Wearable augmented reality system using gaze interaction
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
Undisturbed interaction is essential to provide immersive AR environments. There have been a lot of approaches to interact with VEs (virtual environments) so far, especially in hand metaphor. When the userpsilas hands are being used for hand-based work such as maintenance and repair, necessity of alternative interaction technique has arisen. In recent research, hands-free gaze information is adopted to AR to perform original actions in concurrence with interaction. [3, 4]. There has been little progress on that research, still at a pilot study in a laboratory setting. In this paper, we introduce such a simple WARS (wearable augmented reality system) equipped with an HMD, scene camera, eye tracker. We propose dasiaAgingpsila technique improving traditional dwell-time selection, demonstrate AR gallery - dynamic exhibition space with wearable system.
[Integrated optics, Art, hand-based work, H.5.1 [Information Interfaces and Presentation, augmented reality, scene camera, Aging, motion estimation, head mounted display, virtual environments, HMD, gaze interaction, wearable, Virtual environment, WARS, Maintenance engineering, helmet mounted displays, Augmented reality, VE, wearable computers, AR, alternative interaction technique, Cameras, human computer interaction, Information Interfaces and Presentation, wearable augmented reality system, eye tracker]
ComposAR: An intuitive tool for authoring AR applications
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
This paper introduces ComposAR, a tool to allow a wide audience to author AR and MR applications. It is unique in that it supports both visual programming and interpretive scripting, and an immediate mode for runtime testing. ComposAR is written in Python which means the user interface and runtime behavior can be easily customized and third-party modules can be incorporated into the authoring environment. We describe the design philosophy and the resulting user interface, lessons learned and directions for future research.
[Visualization, program testing, graphical user interfaces, runtime testing, ComposAR, User Interfaces, Programming, authoring tool, augmented reality, Multimedia Information Systems, Augmented reality, authoring systems, user interface, H.5.1 [Multimedia Information Systems, mixed reality, Virtual reality, User interfaces, Three dimensional displays, visual programming, interpretive scripting, Graphical user interfaces, Python]
Fast geometry acquisition for mixed reality applications using motion tracking
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
Mixing real and virtual elements into one environment often involves creating geometry models of physical objects. Traditional approaches include manual modeling by 3D artists or use of dedicated devices. Both approaches require special skills or special hardware and may be costly. We propose a new method for fast semi-automatic 3D geometry acquisition, based upon unconventional use of motion tracking equipment. The proposed method is intended for quick surface prototyping for virtual, augmented and mixed reality applications where quality of visualization of objects is not required or is of low priority.
[Solid modeling, virtual reality, Tracking, Shape, fast semiautomatic 3D geometry acquisition, Radar tracking, augmented reality, path planning, surface prototyping, Laser radar, motion tracking, mixed reality, Measurement by laser beam, Three dimensional displays]
Efficiency of techniques for mixed-space collaborative navigation
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
This paper describes the results of a study conducted to determine the efficiency of visual cues for a collaborative navigation task in a mixed-space environment. The task required a user with an exocentric view of a virtual room to navigate a fully immersed user with an egocentric view to an exit. The study compares natural hand-based gestures, a mouse-based interface and an audio only technique to determine their relative efficiency on task completion times. The results show that visual cue-based collaborative navigation techniques are significantly more efficient than an audio-only technique.
[Computers, visual cue, Visualization, Navigation, audio only technique, Electronic mail, user interfaces, mouse-based interface, Augmented reality, Computer Graphics, mouse controllers (computers), navigation, H.5.2 [Information Interfaces and Presentation, natural hand-based gestures, Collaboration, groupware, Three dimensional displays, Information Interfaces and Presentation, mixed-space collaborative navigation]
3D fiducials for scalable AR visual tracking
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
A new vision and inertial pose estimation system was implemented for real-time handheld augmented reality (AR). A sparse set of 3D cone fiducials are utilized for scalable indoor/outdoor tracking, as opposed to traditional planar patterns. The cones are easy to segment and have a large working volume which makes them more suitable for many applications. The pose estimation system receives measurements from the camera and IMU at 30 Hz and 100 Hz respectively. With a dual-core workstation, all measurements can be processed in real-time to update the pose of virtual graphics within the AR display.
[Real time systems, virtual graphics, scalable AR visual tracking, Augmented Reality, inertial pose estimation system, frequency 30 Hz, augmented reality, Pose Estimation, frequency 100 Hz, Augmented reality, vision pose estimation system, real-time handheld augmented reality, Image segmentation, Image color analysis, 3D fiducials, H.5.2 [Information Interfaces and Presentation, real-time systems, pose estimation, Cameras, Three dimensional displays, Distance measurement, Information Interfaces and Presentation]
Augmented reality in-situ 3D model menu for outdoors
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
We present a design and implementation of an in-situation menu system for loading and visualising 3D models in a physical world context. The menu system uses 3D objects as menu items, and the whole menu is placed within the context of the augmented environment. The use of 3D objects supports the visualisation and placement of 3D models into the augmented world. The menu system employs techniques for the placement of 3D models in two relative coordinate systems: head relative and world relative.
[head relative coordinate system, world relative coordinate system, in-situ 3D model menu, Three-Dimensional Graphics and Realism, graphical user interfaces, outdoors, H.5.2 [User Interfaces, data visualisation, User Interfaces, augmented reality]
ARWeather &#x2014; An Augmented Reality Weather ystem
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
This paper presents the design and development of an ARWeather simulation application, which can simulate various types of precipitation: rain, snow, and hail. We analysed various real occurrences weather types and how they could be simulated in a mobile Augmented Reality system. The Tinmith system is wearable computer system for the development and deployment of the final ARWeather system that allows for autonomous and free movement for the user. The users can move freely inside the simulated weather without limitation.
[weather types, Solid modeling, ARWeather simulation application, Snow, Atmospheric modeling, Computational modeling, wearable computer system, augmented reality, Types of Simulation, Augmented reality, mobile augmented reality system, wearable computers, Rain, mobile computing, Three-Dimensional Graphics and Realism, augmented reality weather system, I.3.7 [Three-Dimensional Graphics and Realism, Tinmith system, ARWeather system, simulated weather, Meteorology]
Perception thresholds for augmented reality navigation schemes in large distances
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
Because the resolution of see-through displays is lower than the resolution of the human eye, perception of AR schemes is complicated in large distances. To discover, how design issues of perception correlate with presentation in large distances, we developed three different variants of arrow-based route guidance systems. With a large-scale head-up display having a large focal depth, we tested the variants under different conditions on perception and interpretability.
[Visualization, Shape, Navigation, Humans, human factors, head-up display, augmented reality, H.1.2 [Models and Principles, user interfaces, augmented reality navigation, Models and Principles, Augmented reality, user interface, head-up displays, arrow-based route guidance system, Distance measurement, Information Interfaces and Presentation, perception thresholds, see-through display, Pixel]
Trends in augmented reality tracking, interaction and display: A review of ten years of ISMAR
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
Although Augmented Reality technology was first developed over forty years ago, there has been little survey work giving an overview of recent research in the field. This paper reviews the ten-year development of the work presented at the ISMAR conference and its predecessors with a particular focus on tracking, interaction and display research. It provides a roadmap for future augmented reality research which will be of great value to this relatively young field, and also for helping researchers decide which topics should be explored when they are beginning their own studies in the area.
[Solid modeling, Visualization, virtual reality, H5.1. [Information System, Tracking, Computational modeling, Collaboration, augmented reality, Three dimensional displays, Sensors, ISMAR, Information System]
Tutorials and workshop
2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality
None
2008
Provides an abstract for each of the presentations and a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.
[]
Mixed and augmented reality: &#x201C;Scary and wondrous&#x201D;
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
Summary form only given. Mixed and augmented reality is the nature of the supporting infrastructure.Its current incarnation is cloud computing versus microcontrollers distributed throughout the environment.Ubiquity has always been a catchword of the distributed processing enthusiasts, and each new generation has pushed the idea beyond the horizons of the previous generation. Imagine an environment where most physical objects know where they are, what they are, and can (in principle) network with any other object. With this infrastructure, reality becomes its own database.Multiple consensual virtual environments are possible, each oriented to the needs of its constituency. If we also have open standards, then bottom-up social networks and even bottom-up advertising become possible.Then the physical world becomes much more like a software construct. The possibilities are both scary and wondrous.
[Cloud computing, Microcontrollers, Object oriented databases, Virtual environment, Social network services, software construct, distributed processing, augmented reality, Augmented reality, open standard, Distributed processing, microcontroller, Image databases, Web services, physical object, bottom-up social network, mixed reality, social networking (online), Software standards, cloud computing, Advertising, microcontrollers, multiple consensual virtual environment]
From the symposium general chairs
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
Mixed and Augmented Reality melts the boundaries between visceral physical reality and a dynamic virtual reality to enhance human performance and heighten experiences. It extends the power of our imagination to create and extends the efficacy of communication, and collaboration. Our symposium has expanded to represent and serve a large and diverse community. It is designed to address research challenges in science, business, professional trades, the arts, media and the humanities.
[]
From the Science and Technology program chairs
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
While ISMAR is expanding and reaching out to new communities, we proudly continue to present to you the best research papers in the field of Mixed and Augmented Reality. The technical program of ISMAR 2009, in the tradition of the proceedings of seven previous ISMAR, two ISAR, two ISMR, and two IWAR meetings, this year takes the form of a Science and Technology (S&#x00026;T) track. This program is comprised of 24 papers, 28 posters, as well as an array of keynote talks, demonstrations, tutorials, workshops and the tracking competition. All of these elements of the program are the result of dedicated hard work by members of the conference committee and additional volunteers, and we would like to recognize their efforts.
[]
IEEE Visualization and Graphics Technical Committee (VGTC)
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
Provides a listing of current committee members.
[Computer science, Conferences, Laboratories, Data visualization, Computer graphics, Virtual reality, Educational institutions, Data engineering, Technical activities, Technical Activities Board]
Task Force on Human Centered Computing (TFHCC)
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
false
[Human computer interaction, Computer interfaces, Computer science, Physics computing, Sociology, Software systems, Executive Committee, Cultural differences, Computer Society, Convergence]
Conference committee
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
Provides a listing of current committee members.
[]
Using AR to support cross-organisational collaboration in dynamic tasks
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
This paper presents a study where Augmented Reality (AR) technology has been used as a tool for supporting collaboration between the rescue services, the police and military personnel in a crisis management scenario. There are few studies on how AR systems should be designed to improve cooperation between actors from different organizations while at the same time support individual needs. In the present study an AR system was utilized for supporting joint planning tasks by providing organisation-specific views of a shared working. The study involved a simulated emergency event conducted in close to real settings with representatives from the organisations for which the system is developed. As a baseline, a series of trials without the AR system was carried out. Results show that the users were positive towards the AR system, and would like to use it in real work. They also experience some performance benefits of using the AR system compared to their traditional tools. Finally, the problem of designing for collaborative work as well as the benefits of using an iterative design processes is discussed.
[Crisis management, Terminology, Collaborative tools, cross-organisational collaboration, police personnel, augmented reality, Paper technology, Personnel, rescue services, augmented reality technology, Augmented reality, Command and control systems, Information science, Collaboration, groupware, Collaborative work, emergency services, crisis management, military personnel]
Interference avoidance in multi-user hand-held augmented reality
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
In a multi-user augmented reality application for a shared physical environment, it is possible for users to interfere with each other. For example, in a multi-player game in which each player holds a display whose tracked position and orientation affect the outcome, one player may physically block another player's view or physically contact another player. We explore software techniques intended to avoid such interference. These techniques modify what a user sees or hears, and what interaction capabilities they have, when their display gets too close to another user's display. We present Redirected Motion, an effective, yet nondistracting, interference avoidance technique for hand-held AR, which transforms the 3D space in which the user moves their display, to direct the display away from other displays. We conducted a within-subject, formal user study to evaluate the effectiveness and distraction level of Redirected Motion compared to other interference avoidance techniques. The study is based on an instrumented, two-player, first-person-shooter, augmented reality game, in which each player holds a 6DOF-tracked ultra-mobile computer. Comparison conditions include an unmanipulated control condition and three other software techniques for avoiding interference: dimming the display, playing disturbing sounds, and disabling interaction capabilities. Subjective evaluation indicates that Redirected Motion was unnoticeable, and quantitative analysis shows that the mean distance between users during Redirected Motion was significantly larger than for the comparison conditions.
[Computer interfaces, Solid modeling, Visualization, software technique, disturbing sounds, orientation tracking, Collaborative/competitive augmented reality, user display, augmented reality, mobile computing, multiuser hand-held augmented reality, computer displays, computer games, groupware, shared physical environment, Three dimensional displays, position tracking, multiplayer game, interaction capability, display dimming, instrumented two-player first-person-shooter, Instruments, Interference, augmented reality game, ultramobile computer, computer-supported cooperative play/work (CSCP/CSCW), Augmented reality, redirected motion, 3D space, Computer displays, Collaborative work, unmanipulated control condition, Motion analysis, interference avoidance]
Animatronic Shader Lamps Avatars
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
Applications such as telepresence and training involve the display of real or synthetic humans to multiple viewers. When attempting to render the humans with conventional displays, non-verbal cues such as head pose, gaze direction, body posture, and facial expression are difficult to convey correctly to all viewers. In addition, a framed image of a human conveys only a limited physical sense of presence - primarily through the display's location. While progress continues on articulated robots that mimic humans, the focus has been on the motion and behavior of the robots. We introduce a new approach for robotic avatars of real people: the use of cameras and projectors to capture and map the dynamic motion and appearance of a real person onto a humanoid animatronic model. We call these devices Animatronic Shader Lamps Avatars (SLA).We present a proof-of-concept prototype comprised of a camera, a tracking system, a digital projector, and a life-sized styrofoam head mounted on a pan-tilt unit. The system captures imagery of a moving, talking user and maps the appearance and motion onto the animatronic SLA, delivering a dynamic, real-time representation of the user to multiple viewers.
[humanoid animatronic model, Head, Avatars, Robot vision systems, Humans, Lamps, Displays, Multimedia Information Systems, robotic avatars, Computer Graphics, H.4.3 [Information Systems Applications, computer vision, Animation, Robot sensing systems, Cameras, Rendering (computer graphics), articulated robots, rendering (computer graphics), animatronic shader lamps avatars, avatars, Information Systems Applications]
Augmenting Aerial Earth Maps with dynamic information
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
We introduce methods for augmenting aerial visualizations of Earth (from services like Google Earth or Microsoft Virtual Earth) with dynamic information obtained from videos. Our goal is to make Augmented Aerial Earth Maps that visualize an alive and dynamic scene within a city. We propose different approaches for analyzing videos of cities with pedestrians and cars, under differing conditions and then created augmented Aerial Earth Maps (AEMs) with live and dynamic information. We further extend our visualizations to include analysis of natural phenomenon (specifically clouds) and add this information to the AEMs adding to the visual reality.
[Clouds, aerial visualization, Data mining, Videos, Information analysis, Earth, aerial Earth Maps, dynamic information, Layout, Web and internet services, Data visualization, data visualisation, geography, Cities and towns, Cameras, Google Earth, Microsoft Virtual Earth]
Interaction and presentation techniques for shake menus in tangible augmented reality
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
Menus play an important role in both information presentation and system control. We explore the design space of shake menus, which are intended for use in tangible augmented reality. Shake menus are radial menus displayed centered on a physical object and activated by shaking that object. One important aspect of their design space is the coordinate system used to present menu options. We conducted a within-subjects user study to compare the speed and efficacy of several alternative methods for presenting shake menus in augmented reality (world-referenced, display-referenced, and object-referenced), along with a baseline technique (a linear menu on a clipboard). Our findings suggest tradeoffs amongst speed, efficacy, and flexibility of interaction, and point towards the possible advantages of hybrid approaches that compose together transformations in different coordinate systems. We close by describing qualitative feedback from use and present several illustrative applications of the technique.
[radial menu, 3D interaction technique, shake menu design space, authoring, graphical user interfaces, world-referenced menu, haptic interfaces, augmented reality, display-referenced menu, tangible augmented reality, clipboard, information display, object-referenced menu, physical object, positioning, menus, 3D interactions, coordinate system, shake menus, Augmented reality, linear menu, qualitative feedback, selection, information presentation technique, human computer interaction, 3D user interface]
Influence of visual and haptic delays on stiffness perception in augmented reality
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
Visual delays are unavoidable in augmented reality setups and occur in different steps of the rendering pipeline. In the context of haptic interaction with virtual objects, it has been shown that delayed force feedback can alter the perception of object stiffness. We hypothesize that delays in augmented reality systems can have similar consequences. To test this, we carried out a user study to investigate the effect of visual and haptic delays on the perception of stiffness. The experiment has been performed in an optimized visuo-haptic augmented reality setup, which allows to artificially manipulate delays during visual and haptic rendering. In line with previous results, delays for haptic feedback resulted in decreased perceived stiffness. In contrast, visual delays caused an increase in perceived stiffness. However, the simultaneous occurrence of delays in both sensory channels led to a partial compensation of these effects. This could potentially help to correct stiffness perception of virtual objects in visuo-haptic augmented reality systems.
[force feedback, optimized visuo-haptic augmented reality setup, Laboratories, haptic interfaces, Displays, augmented reality, user evaluation, virtual object, visual delay, haptic, Feedback, haptic delay, rendering (computer graphics), Computer vision, Delay effects, haptic interaction, multimodal, Haptic interfaces, Synchronization, haptic rendering, Augmented reality, rendering pipeline, object stiffness perception, sensory channel, visual rendering, delay, delays, Cameras, Rendering (computer graphics)]
A user study towards understanding stereo perception in head-worn augmented reality displays
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
Properly perceived stereo display is often assumed to be vital in augmented reality (AR) displays used for close distances, echoing the general understanding from the perception literature. However, the accuracy of the perception of stereo in head-worn AR displays has not been studied greatly. We conducted a user study to elicit the precision of stereo perception in AR and its dependency on the size and contrast of the stimulus. We found a strong effect of contrast on the disparity users desired to make a virtual target verge at the distance of a real reference object. We also found that whether the target began behind or in front of the reference in a method of adjustments protocol made a significant difference. The mean disparity in the rendering that users preferred had a strong linear relationship with their IPD. We present our results and infer stereoacuity thresholds.
[Protocols, Eyes, Multimedia systems, Laboratories, Humans, H.5.1 [Information Interfaces and Presentation, Displays, augmented reality, helmet mounted displays, head-worn augmented reality displays, Models and Principles, Augmented reality, Virtual reality, stereo perception understanding, Rendering (computer graphics), Information Interfaces and Presentation, Stereo vision]
Multiple target detection and tracking with guaranteed framerates on mobile phones
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
In this paper we present a novel method for real-time pose estimation and tracking on low-end devices such as mobile phones. The presented system can track multiple known targets in real-time and simultaneously detect new targets for tracking. We present a method to automatically and dynamically balance the quality of detection and tracking to adapt to a variable time budget and ensure a constant frame rate. Results from real data of a mobile phone Augmented Reality system demonstrate the efficiency and robustness of the described approach. The system can track 6 planar targets on a mobile phone simultaneously at framerates of 23 fps.
[Real time systems, Optical filters, multiple target detection, Target tracking, Optical mixing, Pose estimation, constant frame rate, Mobile handsets, object detection, real-time pose estimation, tracking, Augmented reality, mobile computing, augmented reality system, mobile phone, Object detection, pose estimation, Cameras, Robustness, Optical sensors, 6DOF, natural features]
Shape recognition and pose estimation for mobile augmented reality
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
In this paper we present Nestor, a system for real-time recognition and camera pose estimation from planar shapes. The system allows shapes that carry contextual meanings for humans to be used as augmented reality (AR) tracking fiducials. The user can teach the system new shapes at runtime by showing them to the camera. The learned shapes are then maintained by the system in a shape library. Nestor performs shape recognition by analyzing contour structures and generating projective invariant signatures from their concavities. The concavities are further used to extract features for pose estimation and tracking. Pose refinement is carried out by minimizing the reprojection error between sample points on each image contour and its library counterpart. Sample points are matched by evolving an active contour in real time. Our experiments show that the system provides stable and accurate registration, and runs at interactive frame rates on a Nokia N95 mobile phone.
[Real time systems, projective invariant signatures, Shape, Nestor, image registration, Humans, shape dual perception, augmented reality, Active contours, In-Place Augmented Reality, Runtime, contour structures, feature extraction, reprojection error, pose estimation, Libraries, shape recognition, Performance analysis, mobile augmented reality, 3D pose estimation, free-hand sketching, concavities, Augmented reality, handheld AR, Cameras, Feature extraction, pose refinement, geometric projective invariance, Nokia N95 mobile phone, vision-based tracking, mobile handsets]
Wide area localization on mobile phones
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
We present a fast and memory efficient method for localizing a mobile user's 6DOF pose from a single camera image. Our approach registers a view with respect to a sparse 3D point reconstruction. The 3D point dataset is partitioned into pieces based on visibility constraints and occlusion culling, making it scalable and efficient to handle. Starting with a coarse guess, our system only considers features that can be seen from the user's position. Our method is resource efficient, usually requiring only a few megabytes of memory, thereby making it feasible to run on low-end devices such as mobile phones. At the same time it is fast enough to give instant results on this device class.
[Image Processing And Computer Vision, image registration, wide area localization, Mobile handsets, object detection, 3D point dataset, Image reconstruction, Computer System Implementation, pose estimation, camera image, I.2.10 [Artificial Intelligence, mobile user 6DOF pose localization, sparse 3D point reconstruction, Computer vision, mobile radio, Data acquisition, Artificial Intelligence, image reconstruction, occlusion culling, Image segmentation, Image analysis, Pattern Recognition, Layout, mobile phone, Image generation, Cameras, Mobile computing]
Parallel Tracking and Mapping on a camera phone
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
Camera phones are a promising platform for hand-held augmented reality. As their computational resources grow, they are becoming increasingly suitable for visual tracking tasks. At the same time, they still offer considerable challenges: Their cameras offer a narrow field-of-view not best suitable for robust tracking; images are often received at less than 15 Hz; long exposure times result in significant motion blur; and finally, a rolling shutter causes severe smearing effects. This paper describes an attempt to implement a keyframe-based SLAMsystem on a camera phone (specifically, the Apple iPhone 3 G). We describe a series of adaptations to the Parallel Tracking and Mapping system to mitigate the impact of the device's imaging deficiencies. Early results demonstrate a system capable of generating and augmenting small maps, albeit with reduced accuracy and robustness compared to SLAM on a PC.
[Apple iPhone 3G, parallel mapping, Tracking, Laboratories, visual tracking, augmented reality, camera phone, Augmented reality, cameras, hand-held augmented reality, Simultaneous localization and mapping, Computer displays, mobile computing, Layout, Cameras, Robustness, parallel tracking, Acceleration, Books, mobile handsets]
In-place 3D sketching for authoring and augmenting mechanical systems
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
We present a framework for authoring three-dimensional virtual scenes for Augmented Reality (AR) which is based on hand sketching. Sketches consisting of multiple components are used to construct a 3D virtual scene augmented on top of the real drawing. Model structure and properties can be modified by editing the sketch itself and printed content can be combined with hand sketches to form a single scene. Authoring by sketching opens up new forms of interaction that have not been previously explored in Augmented Reality. To demonstrate the technology, we implemented an application that constructs 3D AR scenes of mechanical systems from freehand sketches, and animates the scenes using a physics engine. We provide examples of scenes composed from trihedral solid models, forces, and springs. Finally, we describe how sketch interaction can be used to author complicated physics experiments in a natural way.
[Visualization, mechanical systems augmenting, Psychology, Augmented Reality, computational geometry, augmented reality, mechanical systems authoring, Mechanical systems, visual languages, free hand sketching, In-Place Augmented Reality, three-dimensional virtual scenes, trihedral solid models, visual language, Education, Virtual reality, hand sketching, 3D content authoring, Assembly, interaction by sketching, physical simulation, dual perception, in-place 3D sketching, Educational institutions, image reconstruction, Object detection, Breast, Output feedback, sketch interaction]
Augmenting text document by on-line learning of local arrangement of keypoints
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
We propose a technique for text document tracking over a large range of viewpoints. Since the popular SIFT or SURF descriptors typically fail on such documents, our method considers instead local arrangement of keypoints. We extends locally likely arrangement hashing (LLAH), which is limited to fronto-parallel images: We handle a large range of viewpoints by learning the behavior of keypoint patterns when the camera viewpoint changes. Our method starts tracking a document from a nearly frontal view. Then, it undergoes motion, and new configurations of keypoints appear. The database is incrementally updated to reflect these new observations, allowing the system to detect the document under the new viewpoint. We demonstrate the performance and robustness of our method by comparing it with the original LLAH.
[Computer vision, text analysis, paper based augmented reality, paper-based augmented reality, Multimedia systems, Image processing, LLAH, augmented reality, Augmented reality, Nearest neighbor searches, Image databases, locally likely arrangement hashing, text document augmentation, Virtual reality, pose estimation, paper registration, Cameras, Robustness, on-line learning, Pattern matching, online learning]
Augmented touch without visual obtrusion
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
Visuo-haptic mixed reality consists of adding to a real scene the ability to see and touch virtual objects. It requires the use of see-through display technology for visually mixing real and virtual objects, and haptic devices for adding haptic interaction with the virtual objects. However, haptic devices tend to be bulky items that appear in the field of view of the user. In this work, we propose a novel mixed reality paradigm where it is possible to touch and see virtual objects in combination with a real scene, but without visual obtrusion produced by the haptic device. This mixed reality paradigm relies on the following three technical steps: tracking of the haptic device, visual deletion of the device from the real scene, and background completion using image-based models. We have developed a successful proof-of-concept implementation, where a user can touch virtual objects in the context of a real scene.
[Actuators, Multimedia systems, H.5.1 [Information Interfaces and Presentation, Medical services, haptic interfaces, haptic device, Displays, augmented reality, haptic interaction, Haptic interfaces, visuo-haptic mixed reality, Optical devices, virtual object, real scene, augmented touch, Space technology, Layout, data visualisation, Virtual reality, see-through display technology, Rendering (computer graphics), Information Interfaces and Presentation, rendering (computer graphics), image-based rendering]
Online environment model estimation for augmented reality
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
Augmented reality applications often rely on a detailed environment model to support features such as annotation and occlusion. Usually, such a model is constructed offline, which restricts the generality and mobility of the AR experience. In online SLAM approaches, the fidelity of the model stays at the level of landmark feature maps. In this work we introduce a system which constructs a textured geometric model of the user's environment as it is being explored. First, 3D feature tracks are organized into roughly planar surfaces. Then, image patches in keyframes are assigned to the planes in the scene using stereo analysis. The system runs as a background process and continually updates and improves the model over time. This environment model can then be rendered into new frames to aid in several common but difficult AR tasks such as accurate real-virtual occlusion and annotation placement.
[annotation, real-virtual occlusion, augmented reality, stereo analysis, Surface roughness, Rough surfaces, Augmented reality, Image reconstruction, Computer Graphics, Simultaneous localization and mapping, Image analysis, computer graphics, online SLAM approach, Layout, occlusion, Computer graphics, I.3.7 [Computer Graphics, Image Processing and Computer Vision, stereo image processing, Rendering (computer graphics), Cameras, online environment model estimation]
In situ image-based modeling
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
We present an interactive image-based modelling method for generating 3D models within an augmented reality system. Applying real time camera tracking, and high-level automated image analysis, enables more powerful modelling interactions than have previously been possible. The result is an immersive modelling process which generates accurate three dimensional models of real objects efficiently and effectively. In demonstrating the modelling process on a range of indoor and outdoor scenes, we show the flexibility it offers in enabling augmented reality applications in previously unseen environments.
[Solid modeling, Parameter estimation, Shape, H.5.1 [Information Interfaces and Presentation, augmented reality, immersive modelling process, IMAGE PROCESSING AND COMPUTER VISION, Power system modeling, Augmented reality, Information analysis, interactive image-based modelling method, Geometry, Image analysis, high-level automated image analysis, augmented reality system, Layout, computer vision, in situ image-based modeling, Cameras, Information Interfaces and Presentation, real time camera tracking]
Dynamic seethroughs: Synthesizing hidden views of moving objects
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
This paper presents a method to create an illusion of seeing moving objects through occluding surfaces in a video. This illusion is achieved by transferring information from a camera viewing the occluded area. In typical view interpolation approaches for 3D scenes, some form of correspondence across views is required. For occluded areas, establishing direct correspondence is impossible as information is missing in one of the views. Instead, we use a 2D projective invariant to capture information about occluded objects (which may be moving). Since invariants are quantities that do not change across views, a visually compelling rendering of hidden areas is achieved without the need for explicit correspondences. A piece-wise planar model of the scene allows the entire rendering process to take place without any 3D reconstruction, while still producing visual parallax. Because of the simplicity and robustness of the 2D invariant, we are able to transfer both static backgrounds and moving objects in real time. A complete working system has been implemented that runs live at 5Hz. Applications for this technology include the ability to look through corners at tight intersections for automobile safety, concurrent visualization of a surveillance camera network, and monitoring systems for patients/elderly/children.
[performance issues [real-time approaches, Visualization, camera viewing, piecewise constant techniques, bandwidth 5 Hz, static background, Vehicle dynamics, industrial and military MR/AR applications, illusion creation method, Robustness, automobile safety, personal MR/AR information systems, view interpolation approach, rendering process, surveillance camera networks concurrent visualization, monitoring systems, rendering (computer graphics), video signal processing, real-time rendering, visual parallax production, Automobiles, object overlay and spatial layout techniques, real-time approaches, synthesizing hidden view, piece wise planar model, image motion analysis, Interpolation, Patient monitoring, moving object, 2D invariant robustness, Surveillance, occluded area, Layout, Vehicle safety, dynamic seethrough, Cameras, 2D projective invariant, information capture, vision-based registration and tracking, 3D scene]
Pick-by-Vision: A first stress test
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
In this paper we report on our ongoing studies around the application of augmented reality methods to support the order picking process of logistics applications. Order picking is the gathering of goods out of a prepared range of items following some customer orders. We named the visual support of this order picking process using head-mounted displays ldquopick-by-visionrdquo. This work presents the case study of bringing our previously developed pick-by-vision system from the lab to an experimental factory hall to evaluate it under more realistic conditions. This includes the execution of two user studies. In the first one we compared our pick-by-vision system with and without tracking to picking using a paper list to check picking performance and quality in general. In a second test we had subjects using the pick-by-vision system continuously for two hours to gain in-depth insight into the longer use of our system, checking user strain besides the general performance. Furthermore, we report on the general obstacles of trying to use HMD-based AR in an industrial setup and discuss our observations of user behaviour.
[order processing, Visualization, Production systems, order picking process, goods gathering, head-mounted display, stress test, INFORMATION INTERFACES AND PRESENTATION, Displays, augmented reality, logistics data processing, H.5.1 [ INFORMATION INTERFACES AND PRESENTATION, customer order, HMD, Testing, pick-by-vision system, helmet mounted displays, order picking, Stress, Augmented reality, Employee welfare, AR, logistics application, experimental factory hall, User interfaces, Usability, Logistics]
Real-time in-situ visual feedback of task performance in mixed environments for learning joint psychomotor-cognitive tasks
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
This paper proposes an approach to mixed environment training of manual tasks requiring concurrent use of psychomotor and cognitive skills. To train concurrent use of both skill sets, the learner is provided real-time generated, in-situ presented visual feedback of her performance. This feedback provides reinforcement and correction of psychomotor skills concurrently with guidance in developing cognitive models of the task. The general approach is presented: 1) Sensors placed in the physical environment detect in real-time a learner's manipulation of physical objects. 2) Sensor data is input to models of task performance which output quantitative measures of the learner's performance. 3) Pre-defined rules are applied to transform the learner's performance data into visual feedback presented in realtime and in-situ with the physical objects being manipulated. With guidance from medical education experts, we have applied this approach to a mixed environment for learning clinical breast exams (CBEs). CBE belongs to a class of tasks that require learning multiple cognitive elements and task-specific psychomotor skills. Traditional approaches to learning CBEs and other joint psychomotor-cognitive tasks rely on extensive one-onone training with an expert providing subjective feedback. By integrating real-time visual feedback of learners' quantitatively measured CBE performance, a mixed environment for learning CBEs provides on-demand learning opportunities with more objective, detailed feedback than available with expert observation. The proposed approach applied to learning CBEs was informally evaluated by four expert medical educators and six novice medical students. This evaluation highlights that receiving real-time in-situ visual feedback of their performance provides students an advantage, over traditional approaches to learning CBEs, in developing correct psychomotor and cognitive skills.
[Visualization, medical education, medical students, Psychology, augmented reality, learning, information visualization, task performance, feedback, Education, physical objects, computer based training, psychomotor skills, Virtual reality, Assembly, skills correction, real-time in-situ visual feedback, output quantitative measure, learner manipulation, mixed environment training, clinical breast exam, biomedical education, cognitive skills, Educational institutions, detailed feedback, skills reinforcement, mixed reality, Object detection, Computer applications, Breast, medical computing, Output feedback, patient diagnosis]
Evaluating the benefits of augmented reality for task localization in maintenance of an armored personnel carrier turret
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
We present the design, implementation, and user testing of a prototype augmented reality application to support military mechanics conducting routine maintenance tasks inside an armored vehicle turret. Our prototype uses a tracked head-worn display to augment a mechanic's natural view with text, labels, arrows, and animated sequences designed to facilitate task comprehension, location, and execution. A within-subject controlled user study examined professional military mechanics using our system to complete 18 common tasks under field conditions. These tasks included installing and removing fasteners and indicator lights, and connecting cables, all within the cramped interior of an armored personnel carrier turret. An augmented reality condition was tested against two baseline conditions: an untracked headworn display with text and graphics and a fixed flat panel display representing an improved version of the laptop-based documentation currently employed in practice. The augmented reality condition allowed mechanics to locate tasks more quickly than when using either baseline, and in some instances, resulted in less overall head movement. A qualitative survey showed mechanics found the augmented reality condition intuitive and satisfying for the tested sequence of tasks.
[military mechanics, repair, flat panel displays, maintenance engineering, Fasteners, Control systems, augmented reality, armored personnel carrier turret maintenance, Personnel, Vehicles, laptop computers, computer animation, Prototypes, computer animated sequence, military computing, Testing, tracked head-worn display, task localization, fixed flat panel display, Maintenance, Augmented reality, armored vehicle turret, military vehicles, computer graphics, service, localization augmented reality, Animation, attention, military equipment, laptop-based documentation, Flat panel displays, maintenance, task comprehension]
A dataset and evaluation methodology for template-based tracking algorithms
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
Unlike dense stereo, optical flow or multi-view stereo, template-based tracking lacks benchmark datasets allowing a fair comparison between state-of-the-art algorithms. Until now, in order to evaluate objectively and quantitatively the performance and the robustness of template-based tracking algorithms, mainly synthetically generated image sequences were used. The evaluation is therefore often intrinsically biased. In this paper, we describe the process we carried out to perform the acquisition of real scene image sequences with very precise and accurate ground truth poses using an industrial camera rigidly mounted on the end-effector of a high-precision robotic measurement arm. For the acquisition, we considered most of the critical parameters that influence the tracking results such as: the texture richness and the texture repeatability of the objects to be tracked, the camera motion and speed, and the changes of the object scale in the images and variations of the lighting conditions over time. We designed an evaluation scheme for object detection and interframe tracking algorithms and used the image sequences to apply this scheme to several state-of-the-art algorithms. The image sequences will be made freely available for testing, submitting and evaluating new template-based tracking algorithms, i.e. algorithms that detect or track a planar object in an image sequence given only one image of the object (called the template).
[Performance evaluation, Tracking, ground truth poses, texture richness, augmented reality, high-precision robotic measurement arm, object detection, camera speed, Image sequences, texture repeatability, template-based tracking algorithms, pose estimation, Robustness, image sequences, evaluation methodology, interframe tracking algorithms, Robot vision systems, camera motion, image texture, real scene image sequences, Image motion analysis, Layout, Object detection, Image generation, Cameras, dataset]
Global pose estimation using multi-sensor fusion for outdoor Augmented Reality
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
Outdoor Augmented Reality typically requires tracking in unprepared environments. For global registration, Global Positioning System (GPS) is currently the best sensing technology, but its precision and update rate are not sufficient for high quality tracking. We present a system that uses Kalman filtering for fusion of Differential GPS (DGPS) or Real-Time Kinematic (RTK) based GPS with barometric heights and also for an inertial measurement unit with gyroscopes, magnetometers and accelerometers to improve the transient oscillation. Typically, inertial sensors are subjected to drift and magnetometer measurements are distorted by electro-magnetic fields in the environment. For compensation, we additionally apply a visual orientation tracker which is drift-free through online mapping of the unknown environment. This tracker allows for correction of distortions of the 3-axis magnetic compass, which increases the robustness and accuracy of the pose estimates. We present results of applying this approach in an industrial application scenario.
[Real time systems, magnetometers, Magnetometers, visual tracking, Kalman filter, 3-axis magnetic compass, kalman filter, augmented reality, sensor fusion, GPS, tracking, multi-sensor fusion, Measurement units, Kinematics, pose estimation, accelerometer, real-time kinematic, Gyroscopes, Kalman filters, outdoor augmented reality, barometric heights, Filtering, magnetometer, global pose estimation, gyroscopes, handheld augmented reality, Augmented reality, Global Positioning System, gyroscope, inertial tracking, transient oscillation, Magnetic separation, electromagnetic fields, accelerometers, global registration, differential GPS]
ESM-Blur: Handling &#x00026; rendering blur in 3D tracking and augmentation
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
The contribution of this paper is two-fold. First, we show how to extend the ESM algorithm to handle motion blur in 3D object tracking. ESM is a powerful algorithm for template matching-based tracking, but it can fail under motion blur. We introduce an image formation model that explicitly considers the possibility of blur, and show it results in a generalization of the original ESM algorithm. This allows to converge faster, more accurately and more robustly even under large amount of blur. Our second contribution is an efficient method for rendering the virtual objects under the estimated motion blur. It renders two images of the object under 3D perspective, and warps them to create many intermediate images. By fusing these images we obtain a final image for the virtual objects blurred consistently with the captured image. Because warping is much faster that 3D rendering, we can create realistically blurred images at a very low computational cost.
[Tracking, Image converters, Motion estimation, 3D object tracking, augmented reality, motion blur estimation, object detection, Electronic mail, augmentation, image matching, tracking, image formation model, ESM-blur, motion estimation, blur rendering, Rendering (computer graphics), Cameras, Robustness, Computational efficiency, template matching-based tracking, Detection algorithms, rendering (computer graphics), Mobile computing, ESM algorithm]
Integration of georegistered information on a virtual globe
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
In collaborative augmented reality (AR) missions, much georegistered information is collected and sent to a command and control center. This paper describes the concept and prototypical implementation of a mixed reality (MR) based system that integrates georegistered information from AR systems and other sources on a virtual globe. The application can be used for a command and control center to monitor the field operation where multiple AR users are engaging in a collaborative mission. Google Earth is used to demonstrate the system, which integrates georegistered icons, live video streams from field operators or surveillance cameras, 3D models, and satellite or aerial photos into one MR environment.
[georegistered information integration, H.5.1 [Information Interfaces and Presentation, augmented reality, geographic information systems, virtual globe, collaborative mission, Augmented reality, live video streams, Command and control systems, Earth, surveillance cameras, Satellites, Surveillance, Collaboration, Virtual reality, georegistered icons, Streaming media, Cameras, Information Interfaces and Presentation, Google Earth, Virtual prototyping, 3D models, collaborative augmented reality, mixed reality based system]
Forked! A demonstration of physics realism in augmented reality
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
In making fully immersive augmented reality (AR) applications, real and virtual objects will have to be seen to physically interact together in a realistic and believable way. This paper describes Forked! a system that has been developed to show how physical interactions between real and virtual objects can be simulated realistically and believably through appropriate use of a physics engine. The system allows users control a robotic forklift to manipulate virtual crates in an AR environment. The paper also describes a evaluation experiment in which it is shown that the physical interactions between the forklift and the virtual creates are realistic and believable enough to be comparable with the physical interactions between a forklift and real crates.
[Performance evaluation, Robot control, manipulators, Control systems, augmented reality, Augmented reality, Engines, robotic forklift, Robot kinematics, Physics computing, Layout, Forked!, physics realism, Cameras, virtual crates, Artificial intelligence]
Contextual in-situ visualization for port placement in keyhole surgery: Evaluation of three target applications by two surgeons and eighteen medical trainees
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
Port position in minimally invasive surgeries is chosen to minimize the lesion of tissue and maximize the movability for endoscopic instruments. In this study, we present an evaluation of the potential of a 3D contextual in-situ visualization of the anatomic target region to help surgeons for three different surgical procedures decide where best to create ports and incisions to enable the insertion of a specific set of instruments.
[Visualization, Computer vision, Target tracking, 3D contextual in-situ visualization, Computer Uses in Education, port placement, K.3.1 [Computer Uses in Education, augmented reality, Multimedia Information Systems, Augmented reality, minimally invasive surgeries, Surgical instruments, keyhole surgery, Minimally invasive surgery, data visualisation, Cameras, Imaging phantoms, medical trainees, Lesions, medical computing, Testing, contextual in-situ visualization]
Using optical flow as lightweight SLAM alternative
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
Visual simultaneous localisation and mapping (SLAM) is since the last decades an often addressed problem. Online mapping enables tracking in unknown environments. However, it also suffers from high computational complexity and potential drift. Moreover, in augmented reality applications the map itself is often not needed and the target environment is partially known, e.g. in a few 3D anchor or marker points. In this paper, rather than using SLAM, measurements based on optical flow are introduced. With these measurements, a modified visual-inertial tracking method is derived, which in Monte Carlo simulations reduces the need for 3D points and allows tracking for extended periods of time without any 3D point registrations.
[lightweight SLAM, optical tracking, visual SLAM, Sensor fusion, augmented reality, sensor fusion, Simultaneous localization and mapping, Fluid flow measurement, Monte Carlo methods, Kinematics, online mapping, visual-inertial tracking method, augmented reality applications, Optical variables control, optical variables measurement, image sequences, Optical filters, flow measurement, camera tracking, inertial sensors, Time measurement, optical flow, Image motion analysis, Monte Carlo simulations, simultaneous visual localisation and mapping, Cameras, Optical sensors, computational complexity]
Advanced training methods using an Augmented Reality ultrasound simulator
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
Ultrasound (US) is a medical imaging modality which is extremely difficult to learn as it is user-dependent, has low image quality and requires much knowledge about US physics and human anatomy. For training US we propose an Augmented Reality (AR) ultrasound simulator where the US slice is simulated from a CT volume. The location of the US slice inside the body is visualized using contextual in-situ techniques. We also propose advanced methods how to use an AR simulator for training.
[Visualization, US physics-and-human anatomy, training method, Medical simulation, H.5.1 [Information Interfaces and Presentation, biomedical ultrasonics, augmented reality, teaching, augmented reality ultrasound simulator, Ultrasonic imaging, Computed tomography, computer based training, data visualisation, CT volume, US slice, medical image processing, Biomedical imaging, contextual in-situ visualization, Human anatomy, Biological system modeling, Life and Medical Sciences, Augmented reality, Physics, Image quality, image quality, Information Interfaces and Presentation, medical imaging]
Object recognition and localization while tracking and mapping
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
This paper demonstrates how objects can be recognized, reconstructed, and localized within a 3D map, using observations and matching of SIFT features in keyframes. The keyframes arise as part of a frame-rate process of parallel camera tracking and mapping, in which the keyframe camera poses and 3D map points are refined using bundle adjustment. The object reconstruction process runs independently, and in parallel to, the tracking and mapping processes. Detected objects are automatically labelled on the user's display using predefined annotations. The annotations are also used to highlight areas of interest upon the objects to the user.
[object recognition, keyframes, Laboratories, Displays, object detection, Multimedia Information Systems, Yarn, Image reconstruction, Object localization, Multimedia systems, object-oriented databases, object reconstruction process, Spatial databases, Object recognition, bundle adjustment, H.5.1 [Multimedia Information Systems, Scene Analysis, Image databases, frame-rate process, parallel camera tracking, Object detection, computer vision, Cameras, SIFT features, 3D map points, image recognition, parallel camera mapping]
Streaming mobile augmented reality on mobile phones
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
Continuous recognition and tracking of objects in live video captured on a mobile device enables real-time user interaction. We demonstrate a streaming mobile augmented reality system with 1 second latency. User interest is automatically inferred from camera movements, so the user never has to press a button. Our system is used to identify and track book and CD covers in real time on a phone's viewfinder. Efficient motion estimation is performed at 30 frames per second on a phone, while fast search through a database of 20,000 images is performed on a server.
[Real time systems, object recognition, continuous object tracking, camera movement, book cover identification, multimedia systems, augmented reality, Mobile handsets, Delay, CD cover tracking, mobile computing, phone viewfinder, real-time user interaction, motion estimation, video streaming, Books, video signal processing, book cover tracking, Motion estimation, Augmented reality, continuous object recognition, Image databases, streaming mobile augmented reality, Layout, mobile phone, Streaming media, Cameras, live video]
MoleARlert - an augmented reality game based on Lemmings
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
In this poster, we present our outdoor AR game MoleARlert. The idea behind this research prototype was to exploit the enormous potential of AR in combination with classical game play based on the well-known game Lemmings by Psygnosis. Real players interact with virtual moles on a real playing field. The moles are guided through the hazardous environment to their final goal. The multiplayer game is observed through a stationary video see-through monitor. The team leader directs the real players around the field in order for them to steer the game with special markers and human gestures. Thus, the game requires a lot of action from players and is very entertaining.
[Humans, H.5.1 [Information Interfaces and Presentation, augmented reality, Psygnosis, virtual moles, real playing field, video see-through monitor, gesture recognition, Prototypes, computer displays, computer games, Virtual reality, MoleARlert, multiplayer game, Multimedia systems, Multimedia computing, augmented reality game, Hazards, Lemmings, Augmented reality, Tiles, human gestures, Games, Cameras, Information Interfaces and Presentation, special markers, Personal Computing]
A setup for evaluating detectors and descriptors for visual tracking
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
In many cases, visual tracking is based on detecting, describing, and then matching local features. A variety of algorithms for these steps have been proposed and used in tracking systems, leading to an increased need for independent comparisons. However, existing evaluations are geared towards object recognition and image retrieval, and their results have limited validity for real-time visual tracking. We present a setup for evaluation of detectors and descriptors which is geared towards visual tracking in terms of testbed, candidate algorithms and performance criteria. Most notably, our testbed consists of video streams with several thousand frames naturally affected by noise and motion blur.
[object recognition, Tracking, object detection, descriptor, tracking, candidate algorithm, feature extraction, Detectors, noise, video streaming, Man machine systems, video stream, video signal processing, motion blur, Testing, Image retrieval, real-time visual tracking system, Object recognition, image matching, image motion analysis, Computer science, Layout, real-time systems, Streaming media, image retrieval, Cameras, local feature matching]
Photo-based Industrial Augmented Reality application using a single keyframe registration procedure
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
In the recent years, many industrial augmented reality (IAR) applications are shifting from video to still images to create a mixed view. This new type of application is called photo-based augmented reality. In order to guarantee the success of these applications, a simple and efficient registration method is required. We present a new method to register an image to a CAD model using a single keyframe. This registration is based on sparse 3D information from the model linked to the keyframe during its offline registration. We demonstrate this method in our in-house IAR software for visual inspection and documentation: VID.
[Transmission line matrix methods, Head, CAD model, image registration, Documentation, CAD, production engineering computing, sparse 3D information, Inspection, Displays, augmented reality, still image, Registers, Matrix decomposition, Application software, Augmented reality, photo-based industrial augmented reality, single keyframe registration procedure, industrial engineering, Cameras, solid modelling]
Evaluating the trackability of natural feature-point sets
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
In this work we present a novel idea of evaluating natural feature-point based tracking targets. Our main objective is to evaluate the inherent characteristics of natural feature-point sets with respect to vision-based pose estimation algorithms. Our work attempts to break new ground by concentrating on evaluating complete tracking targets, rather than evaluating tracking methods or single features. This allows deriving indications on how to improve the trackability of natural feature point sets.
[Algorithm design and analysis, Computer vision, Target tracking, Computational modeling, Image processing, Pipelines, Augmented Reality, vision-based pose estimation algorithms, augmented reality, trackability, Tracking Simulation, Runtime, natural feature-point sets, Object detection, pose estimation, Robustness, Karhunen-Loeve transforms, Natural Feature Tracking Target Design]
Robust pose estimation in untextured environments for augmented reality applications
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
We present a robust camera pose estimation approach for stereo images captured in untextured environments. Unlike most of existing registration algorithms which are point-based and make use of intensities of pixels in the neighborhood, our approach imports line segments in registration process. With line segments as primitives, the proposed algorithm is capable to handle untextured images such as scenes captured in man-made environments, as well as the cases when there are large viewpoint changes or illumination changes. Furthermore, since the proposed algorithm is robust to large base-line stereos, there are improvements on the accuracy of 3D points reconstruction. With well-calculated camera pose and object positions in 3D space, we can embed virtual objects into existing scene with higher accuracy for realistic effects. In our experiments, 2D labels are embedded in the 3D scene space to achieve annotation effects as in AR.
[Image registration, image registration, annotation effect, augmented reality, Application software, untextured environments, stereo images, Augmented reality, cameras, Image segmentation, point-based algorithm, line segments, 3D points reconstruction, Layout, image segmentation, Lighting, Computer graphics, pose estimation, stereo image processing, Cameras, Robustness, man-made environments, Stereo vision, robust camera pose estimation]
In-situ refinement techniques for outdoor geo-referenced models using mobile AR
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
We present a set of techniques to perform in-situ refinements on simple geo-referenced models using mobile augmented reality systems. The refinements include affine transformations to the model and surface feature additions, including high detail concave and convex features. The techniques employ pinch gloves and a single-point laser rangefinder augmented with an orientation sensor as input devices. Finished models can be exported for use with other geospatial applications. The proposed techniques are intended to be an effective and elegant approach to enhancing outdoor models using mobile augmented reality.
[Visualization, Clouds, outdoor AR, augmented reality, geospatial application, Earth, mobile computing, laser ranging, Wearable computers, data visualisation, pinch glove, Laser modes, mobile AR, surface convex feature, data visualization, data gloves, geo-referenced models, Buildings, Thumb, Augmented reality, mobile augmented reality system, affine transformation, orientation sensor, surface concave feature, 3D modelling, Solids, outdoor geo-referenced model, Mobile computing, single-point laser rangefinder, solid modelling, in-situ refinement technique]
Temporal calibration in multisensor tracking setups
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
Spatial tracking is one of the most challenging parts of Augmented Reality. Many AR applications rely on the fusion of several tracking systems in order to optimize the overall performance. While the topic of sensor fusion has already seen considerable interest, most results only deal with the integration of particular setups. A crucial part of sensor fusion is the temporal alignment of the sensor signals, as sensors in general are not synchronized. We present a general method to calibrate the temporal offset between different sensors by applying the normalized cross correlation method.
[temporal calibration, ubiquitous tracking, spatial tracking, Sensor fusion, augmented reality, sensor fusion, Time measurement, Calibration, Synchronization, multisensor tracking setup, tracking, sensor signals temporal alignment, Augmented reality, cross correlation method, Operating systems, Signal processing, Cameras, synchronization, Hardware, calibration, Clocks, correlation methods]
Integrating conversational virtual humans and mannequin patient simulators to present mixed reality clinical training experiences
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
We integrated two simulators, a Virtual Patient (VP) system and a physical Human Patient Simulator (HPS) to create a mixed reality conscious sedation training environment. The VP system simulates clinical environments where learners conduct patient interviews. The HPS is a mannequin that models and simulates a wide array of clinical signs. The VP+HPS combination provides new capabilities specially suited for conscious sedation training. Future work will evaluate the efficacy of this MR simulation.
[Drugs, Protocols, virtual reality, Medical simulation, Humans, digital simulation, training, History, virtual humans, computer based training, conscious sedation training environment, Virtual reality, physical human patient simulator, Computational modeling, Computer simulation, biomedical education, conversational virtual humans, virtual patients, patient care, Patient monitoring, virtual patient system, mixed reality, conscious sedation, Computer applications, mannequin patient simulator, human computer interaction]
Mobile augmented reality based 3D snapshots
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
We describe a mobile augmented reality application that is based on 3D snapshotting using multiple photographs. Optical square markers provide the anchor for reconstructed virtual objects in the scene. A novel approach based on pixel flow highly improves tracking performance. This dual tracking approach also allows for a new single-button user interface metaphor for moving virtual objects in the scene. The development of the AR viewer was accompanied by user studies confirming the chosen approach.
[Algorithm design and analysis, Tracking, Jitter, augmented reality, Mobile handsets, Multimedia Information Systems, Image reconstruction, Research and development, dual tracking, mobile computing, optical square markers, tracking performance, pixel flow, mobile augmented reality, single-button user interface, image reconstruction, Augmented reality, H.5.1 [Multimedia Information Systems, 3D snapshots, Layout, virtual object reconstruction, Cameras, AR viewer, Information Interfaces and Presentation, Usability, solid modelling, multiple photographs]
Effects of sizes and shapes of props in tangible augmented reality
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
In this paper, we investigated the effect of relative difference between the virtual object and tangible prop in its size and shape in terms of usability. We have found the obvious fact that in general that manipulation is more efficient with equally sized/shaped prop and virtual objects. In addition, when decoupled, the size difference factor did not matter. While an additional experiment (in progress) is needed to confirm the true effect of shape difference, we posit that prop design should concentrate on representing the critical shape features for a given class of objects the prop is to represent. Humans are adept at recognizing and identifying objects even if there are shown at different scales and in different angles. This ability, called ldquoconstancyrdquo is weaker in the dimension of shapes, e.g. compared to sizes, which is another reason to suspect shape to be a more critical factor for effective prop design. Another significant factor, in the design of props (not treated in this paper) is the prop to virtual object alignment (which can affect the task performance in terms of finding and feeling for a stable grasp). For instance, possible choices for spatially registering a prop to a virtual object (or vice versa) can be about their respective center of gravity, about the chosen surface, etc. Future experiments and prop design method will have to take this into account as well.
[Laboratories, virtual object alignment, augmented reality, Product design, tangible prop design, Haptic interfaces, tangible augmented reality, Augmented reality, Guidelines, Shape control, prop shape, USA Councils, Floppy disks, Usability, Testing, prop size]
A replication study testing the validity of AR simulation in VR for controlled experiments
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
It is extremely challenging to run controlled studies comparing multiple augmented reality (AR) systems. We use an ldquoAR simulationrdquo approach, in which a virtual reality (VR) system is used to simulate multiple AR systems. In order to validate this approach, we carefully replicated a well-known study by Ellis et al. using our simulator, obtaining comparable results.
[System testing, Computational modeling, Methodology and Techniques, Displays, augmented reality, AR simulation system testing, Delay, Augmented reality, virtual reality system, Graphics, controlled experiment, VR system, Three-Dimensional Graphics and Realism, I.3.7 [Three-Dimensional Graphics and Realism, Virtual reality, multiple augmented reality system, Control system synthesis, Hardware, Performance analysis]
Physical-virtual tools for spatial augmented reality user interfaces
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
This paper presents a new user interface methodology for Spatial Augmented Reality systems. The methodology is based on a set of physical tools that are overloaded with logical functions. Visual feedback presents the logical mode of the tool to the user by projecting graphics onto the physical tools. This approach makes the tools malleable in their functionality, with this change conveyed to the user by changing the projected information. Our prototype application implements a two handed technique allowing an industrial designer to digitally airbrush onto an augmented physical model, masking the paint using a virtualized stencil.
[Visualization, physical-virtual tools, Shape, graphical user interfaces, Spraying, User Interfaces, Lamps, logical functions, augmented reality, technical drawing, Spatial Augmented Reality, user interfaces, visual feedback, Augmented reality, Feedback, graphics projection, User interfaces, Large-scale systems, projected information, virtualized stencil, Paints, spatial augmented reality, Graphical user interfaces]
Head-mounted virtual loupe with sight-based activation for surgical applications
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
This work presents the development of an augmented reality magnification system, termed virtual loupe, implemented in a head-mounted display for surgical applications. The system provides a magnified view with a novel control based on tracked sight orientation. The system was evaluated by measuring the completion time of a suturing task performed by surgeons. The magnifying approach implemented proved to be useful by providing global context of the operating field. The sight-based activation was widely accepted by surgeons as a useful functionality to control viewing modalities.
[Visualization, Target tracking, Medical control systems, head-mounted virtual loupe, augmented reality, helmet mounted displays, user interaction, Application software, Augmented reality, medical visualization, Computer displays, augmented reality magnification system, Microscopy, Surgery, Cameras, surgical application, medical computing, sight-based activation, Biomedical imaging, surgery]
Interactive model reconstruction with user guidance
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
ProFORMA, an on-line reconstruction system for textured objects rotated by a user's hand, can be coupled with augmented reality (AR) to allow users to rapidly generate textured 3D models. We demonstrate how the use of an overlaid mesh model and 3D arrow can be used to assist the user in view planning, guiding the user to collect new keyframes from desirable views. The method described is particularly suited for use with AR headsets, providing guidance with minimal user input and allowing in situ modelling using the head-mounted camera (ProFORMA does not require a completely stationary camera).
[Visualization, Uncertainty, minimal user input, augmented reality, Wire, Yarn, on-line reconstruction system, user guidance, interactive devices, overlaid mesh model, ProFORMA, view planning, user hand rotation, textured objects, image reconstruction, Augmented reality, image texture, 3D arrow, interactive model reconstruction, textured 3D model, User interfaces, Cameras, Mice, head-mounted camera, Usability, solid modelling]
Egocentric space-distorting visualizations for rapid environment exploration in mobile mixed reality
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
Throughout the last decade, mobile information browsing has become a widely-adopted practice. Most of today's mobile Internet devices contain facilities to display maps of the user's surroundings with points of interest embedded into the map. Other researchers have already explored complementary, egocentric visualizations of these points of interest using mobile mixed reality. However, it is challenging to display off-screen or occluded points of interest. We have designed and implemented space-distorting visualizations to address these situations. Based on the informal user feedback that we have gathered, we have performed several iterations on our visualizations. We hope that our initial results can inspire other researchers to also investigate space-distorting visualizations for mixed and augmented reality.
[Artificial, Visualization, iterative methods, Uncertainty, augmented reality, Wire, Yarn, Computer Graphics, mobile computing, mobile mixed reality, data visualisation, Virtual reality, Interaction Techniques, mobile Internet devices, augmented and virtual realities, Augmented reality, iterations, rapid environment exploration, H.5.1. [Information Interfaces and Presentation, User interfaces, Cameras, Mice, Information Interfaces and Presentation, Internet, Usability, egocentric space-distorting visualizations]
Multitouch interaction for Tangible User Interfaces
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
We introduce a novel touch-based interaction technique for tangible user interfaces (TUIs) in Augmented Reality (AR) applications. The technique allows for direct access and manipulation of virtual content on a registered tracking target, is robust and lightweight, and can be applied in numerous tracking and interaction scenarios.
[Computer interfaces, Computer vision, Target tracking, H.5.1 [Information Systems, Image sampling, tangible user interface, Information Systems, augmented reality, Computing Methodologies, Electronic mail, user interfaces, Augmented reality, Fingers, target tracking, User interfaces, Cameras, Robustness, multitouch interaction]
Immersive image-based modeling of polyhedral scenes
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
In this paper, we describe a purely image-based system that allows a user to interactively capture the 3D geometry of a polyhedral scene with the aid of its physical presence. A video camera is used as both an interaction and tracking device. The 3D user interface is intuitive to a non-expert and the mouseless control procedure makes the system particularly suitable for mobile devices such as PDAs and mobile phones. The efficiency and accuracy of the method are demonstrated on a polyhedral scene made of two house-like boxes.
[Solid modeling, polyhedral scenes, Head, graphical user interfaces, Image-Based Modeling, mobile phones, Augmented Reality, Wearable Computers, Turning, video camera, Mobile handsets, PDA, Geometry, computer graphics, 3D User Interfaces, Layout, Computer graphics, Construction at a Distance, User interfaces, Cameras, immersive image-based modeling, Mice, video signal processing, 3D user interface]
Real-time representation of inter-reflection for cubic marker
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
This paper proposes a method for rendering an inter-reflection between a marker and a glossy floor in Augmented Reality (AR). At first, a reflectance ratio of the floor is estimated from the reflection on the floor and from the marker box directly. Then the roughness of the floor is estimated based on the sharpness of the reflected marker box image on the floor. Lastly, the marker box reflection is eliminated based on the surrounding colors of the marker box reflection. Rendered images show that natural inter-reflection can be achieved by using the proposed method.
[inter-reflection image rendering, glossy floor estimation, H.5.1 [Information Interfaces and Presentation, augmented reality, Surface roughness, Electronic mail, Rough surfaces, Augmented reality, Light sources, Reflectivity, real-time representation, cubic marker box reflection, Rendering (computer graphics), Cameras, Information Interfaces and Presentation, Optical reflection, Mirrors, rendering (computer graphics), reflectance ratio]
A solution for navigating user-generated content
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
We are interested how to contextualize the digital content of an individual user with the help of content from other users, how to create a positive user experience from that, and what kind of interaction that can encourage between users. As a result the environment can be modelled. Creating a digital representation of the world from photographs is an exciting opportunity, for e.g. as a registering system for Augmented Reality (AR), forming one cornerstone for what Hollerer et al. call Anywhere Augmentation (AA). In this paper we present the user experience of the prototype and its key features. We have implemented a prototype solution for discovering shared digital media via a novel user interface, presenting the spatial relationships of the content.
[User-generated content, data mining, Satellite navigation systems, augmented reality, Digital cameras, user interfaces, content spatial relationship, individual user digital content, Application software, multimedia computing, world digital representation, Augmented reality, Videos, Photography, Prototypes, Virtual reality, discovering shared digital media via user interface, Mirrors, navigating user generated content]
Vision based people tracking for ubiquitous Augmented Reality applications
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
The task of vision based people tracking is a major research problem in the context of surveillance applications or human behavior estimation, but it has had only minimal impact on (Ubiquitous) Augmented Reality applications thus far. Deploying stationary infrastructural cameras within indoor environments for the purpose of Augmented Reality could provide a users' devices with additional functionality that a small device and mobile sensors cannot provide to its user. Therefore people tracking could be expected to become an ubiquitously available infrastructural element in buildings since surveillance cameras are widely used. The use for scenarios indoors or close to buildings is obvious. We present and discuss several different ways where people tracking in real-time could influence the fields of Augmented Reality and further vision based applications.
[Fuses, mobile sensors, Augmented Reality, ubiquitous augmented reality, Sensor fusion, augmented reality, sensor fusion, Sensor systems, ubiquitous computing, tracking, Sensor Fusion, cameras, vision based people tracking, People Tracking, Particle filters, surveillance, human behavior estimation, Target tracking, Augmented reality, Global Positioning System, Image analysis, Surveillance, computer vision, Cameras, stationary infrastructural cameras]
Consistent real-time lighting for virtual objects in augmented reality
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
We present a technique for rendering realistic shadows of virtual objects in a mixed reality environment by recovering the light source distribution of a scene in real-time, through the segmentation and analysis of a known occluding object's shadows. A fiducial marker provides information about the position of the occluding object and the plane of the surface on which shadows are cast, and serves as the origin of a marker coordinate system. A new shadow segmentation approach is carried out on the shadow image and is able to recover geometrical information on multiple faint shadows. Using normalised iterative reinforcement, noise and artifacts can be suppressed in the final shadow map. The scene's light source distribution is then extrapolated using geometrical data from both the occluding object and its cast shadows. Virtual light sources in a game engine are used to mimic real light sources and achieve consistent illumination and increase the realism of the augmented reality scene.
[fiducial marker, game engine, shadow, realistic rendering, augmented reality, geometrical information, Engines, virtual object, Light sources, normalised iterative reinforcement, image segmentation, Lighting, Virtual reality, Probes, rendering (computer graphics), Radiosity, light source distribution, Image edge detection, illumination, shadow image segmentation approach, lighting, marker coordinate system, Augmented reality, Image segmentation, Layout, realistic shadow rendering, consistent real-time lighting, Cameras, geometry]
Tutorials &#x00026; workshops
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
Provides an abstract for each of the tutorial presentations and a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.
[]
Comprehensive tutorials
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
Provides an abstract for each of the tutorial presentations and a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.
[Tutorial, Laboratories, Human factors, Displays, Augmented reality, Research and development]
Experiential learning with Mixed and Augmented Reality
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
false
[Continuing education, Technological innovation, Space technology, Medical simulation, Laboratories, Virtual reality, Internet, Facebook, Augmented reality, Convergence]
Transforming lives: High-performance, high-risk training with mixed reality
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
false
[Continuing education, Technological innovation, Space technology, Medical simulation, Laboratories, Virtual reality, Internet, Facebook, Augmented reality, Convergence]
Industrial workshop: Manufacturing the future
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
false
[]
Mobile Magic Wand: Augmented reality on mobile devices
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
false
[Manufacturing industries, Industrial training, Manufacturing processes, Process planning, Packaging, Mobile handsets, Internet, Augmented reality, Commercialization, Assembly]
Lets go out: Research in outdoor mixed and augmented reality
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
false
[Graphics, Social network services, Physics computing, Cellular phones, Lighting, Cameras, Displays, Large-scale systems, Australia, Augmented reality]
AR 2.0: Social Augmented Reality - social computing meets Augmented Reality
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
false
[Graphics, Social network services, Physics computing, Cellular phones, Lighting, Cameras, Displays, Large-scale systems, Australia, Augmented reality]
Proposal of international voluntary activities on establishing benchmark test schemes for AR/MR geometric registration and tracking methods
2009 8th IEEE International Symposium on Mixed and Augmented Reality
None
2009
This is a proposal to the ISMAR community from Japanese AR/MR researchers for the future progress of AR/MR technology. We hope to expand our activities over the ISMAR community and call for international participants who would take part in a number of voluntary works. At the same time, this paper presents a current view of the outcomes of these activities may have be in due course. We will focus here on the various tracking methods, one of the most active themes at the annual ISMAR symposiums. The main goal of our activities is to build a framework to comprehensively evaluate a variety of existing and future tracking methods. Strictly speaking, our targets should include all the geometric registration methods that merge the real and virtual world seamlessly. They can also be termed as real-time 3D matchmove. After initial registration (calibration) of two spaces is achieved, either object tracking or camera tracking is required when the subject or camera moves. In addition to methods suited to static registration, there exist also methods that focus solely on improving the performance of tracking, without calibration. In this paper, when we use the terms "registration and tracking" or simply "tracking" we will be referring to the general definition of tracking as given above.
[Software testing, benchmark test schemes, Target tracking, geometric registration method, camera tracking, Pipelines, real-time 3D matchmove, tracking method, augmented reality, international voluntary activities, static registration, Calibration, Proposals, tracking, Recruitment, mixed reality, Virtual reality, Benchmark testing, Cameras, ISMAR symposium, object tracking]
From the symposium general chairs
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
Welcome to ISMAR 2010 in Seoul, Korea. This year's ISMAR already marks its 9th occasion and it seems that mixed and augmented reality technology has finally come of age, attracting the due attention from the general public. This is especially due to the popularity of the smart mobile devices, and thus, it is quite fitting that ISMAR is being held in Seoul, one of the most networked and dynamic place in adopting new information technology and their deployments. Especially with recent explosion of smart phones, Korea may provide a fertile ground for the wide deployment of mobile augmented reality technology on smart phones.
[]
From the Science &amp; Technology program chairs
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
Welcome to the ISMAR 2010 Science and Technology (S&amp;T) track, which follows in the proud tradition of several IWAR, ISMR, and ISAR meetings, as well as eight previous ISMAR symposia to bring you research excellence in mixed and augmented reality.
[]
IEEE Visualization and Graphics Technical Committee (VGTC)
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
The IEEE Visualization and Graphics Technical Committee (VGTC) is a formal subcommittee of the Technical Activities Board (TAB) of the IEEE Computer Society. The VGTC provides technical leadership and organizes technical activities in the areas of visualization, computer graphics, virtual and augmented reality, and interaction.
[]
Task force on Human centered Computing (TFHCC)
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
The field of Human-Centered Computing (HCC) has emerged from the convergence of multiple disciplines and research areas that are concerned both with understanding human beings and with the design of computational devices and interfaces. Researchers and designers of human-centered computing include individuals from computer science, sociology, psychology, cognitive science, engineering, graphic design, and industrial design.
[]
Conference committee
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
Provides a listing of current committee members.
[]
International Program Committee and Reviewers
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
Provides a listing of current committee members.
[]
Augmenting reality for medicine, training, presence and telepresence
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
At least since Sutherland's 1968 head-mounted display, augmented reality systems have inspired (and frustrated) generations of developers, users, and enthusiasts. These inspirations have led to decades of effort that yielded major innovations in technologies for 3D capture, 3D displays, tracking, and real-time image generation. Some of these component technologies, such as head-mounted displays, have proven much more difficult to bring to widespread adoption than many of us expected. Others, such as real-time image generation, have become phenomenally successful, with major societal impacts extending far beyond the initial applications. In this talk, I will describe several developments in these areas, and illustrate the resulting systems that exploited them: 1) enhancing 3D scene acquisition by laser scanning, or with structured light, or with multiple acquisition cameras; 2) augmenting a physician's view of her patient with registered internal imagery; 3) augmenting a user's surroundings with projection onto multiple nearby surfaces; 4) augmenting a user's remote presence using a human-sized avatar that mimics appearance, pose, and gestures; 5) augmenting tabletop displays with multi-user autostereoscopic capabilities. The common goal of all these systems is to enrich users' immediate surroundings with computer-generated or -controlled enhancements, which can run the gamut from mere virtual imagery to full-fledged robotic androids. I will also speculate about the possible paths to progress in the coming years. The future is encouraging, as the decreasing cost of the necessary components lowers the barriers to entry and encourages ever-increasing participation in innovation, development and use. Coupled with the rapidly advancing technologies in sensors, cameras, displays, robotics, and networks, this should enable us to accelerate bringing the visions of augmenting reality to daily life.
[telepresence application, internal patient imagery, 3D scene acquisition, augmented reality, helmet mounted displays, virtual imagery, realistic images, real time image generation, telecontrol, augmented reality system, human sized avatar, head mounted display, multiple acquisition camera, medical computing, avatars, solid modelling]
Augmented dreams
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
Many forms of artistic expression might be considered as an augmentation of reality. What are the challenges for Augmented Reality - technologies that enhance physical reality by layering interactive computer-generated content to it - when used as an artistic medium? While the underlying technologies (tracking, displaying and rendering) that drive augmented reality focus on a transparent creation, distribution and access of information the artist should critically examine their implementation. V2_Lab considers the artistic Research and Development (aRt&amp;D) as an important element that can introduce specific qualities in the field of technological innovation and realization. Especially in the interdisciplinary collaboration with engineers and computer scientists, the artistic approach is unconventional in connecting different research fields, leaving behind discipline-specific paradigms.
[artistic research and development, art, interactive computer generated content, V2_Lab, interactive systems, augmented reality, augmented dream]
Perceptual issues in augmented reality revisited
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
This paper provides a classification of perceptual issues in augmented reality, created with a visual processing and interpretation pipeline in mind. We organize issues into ones related to the environment, capturing, augmentation, display, and individual user differences. We also illuminate issues associated with more recent platforms such as handhelds or projector-camera systems. Throughout, we describe current approaches to addressing these problems, and suggest directions for future research.
[Visualization, virtual reality, perceptual issues, visual perception, Object segmentation, augmented reality, Human perception, mobile computing, Image color analysis, Lighting, visual processing, Cameras, Adaptive optics, handheld devices, interpretation pipeline, Lenses]
The effect of out-of-focus blur on visual discomfort when using stereo displays
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
Visual discomfort is a major problem for head-mounted displays and other stereo displays. One effect that is known to reduce visual comfort is double vision, which can occur due to high disparities. Previous studies suggest that adding artificial out-of-focus blur increases the fusional limits, where the left and right image can be fused without double vision. We investigate the effect of adding artificial out-of-focus blur on visual discomfort using two different setups. One uses a stereo monitor and an eye tracker to change the depth of focus based on the gaze of the user. The other one uses a video-see through head mounted display. A study involving 18 subjects showed that the viewing comfort when using blur is significantly higher in both setups for virtual scenes. However we can not confirm without doubt that the higher viewing comfort is only related to an increase of the fusional limits, as many subjects reported that double vision did not occur during the experiment. Results for additional photographed images that have been shown to the subjects were less significant. A first prototype of an AR system extracting a depth map from stereo images and adding artificial out-of-focus blur is presented.
[Visualization, virtual reality, stereo displays, virtual scenes, H.5.1 [Information Interfaces and Presentation, helmet mounted displays, Augmented reality, out-of-focus blur, stereo image processing, stereo monitor, depth map, Cameras, visual discomfort, double vision, Three dimensional displays, Information Interfaces and Presentation, Stereo vision, head mounted displays, Monitoring, Pixel, eye tracker]
Image-based ghostings for single layer occlusions in augmented reality
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
In augmented reality displays, X-Ray visualization techniques make hidden objects visible through combining the physical view with an artificial rendering of the hidden information. An important step in X-Ray visualization is to decide which parts of the physical scene should be kept and which should be replaced by overlays. The combination should provide users with essential perceptual cues to understand the relationship of depth between hidden information and the physical scene. In this paper we present an approach that addresses this decision in unknown environments by analyzing camera images of the physical scene and using the extracted information for occlusion management. Pixels are grouped into perceptually coherent image regions and a set of parameters is determined for each region. The parameters change the X-Ray visualization for either preserving existing structures or generating synthetic structures. Finally, users can customize the overall opacity of foreground regions to adapt the visualization.
[Solid modeling, Visualization, single layer occlusion management, H.5.1 [Information Interfaces and Presentation, image based ghosting, augmented reality, X-ray imaging, cameras, Image color analysis, feature extraction, data visualisation, camera images analysis, Image Processing and Computer Vision, rendering (computer graphics), artificial rendering, Image edge detection, X-Ray visualization technique, Transfer functions, augmented reality display, hidden feature removal, information extraction, Cameras, Information Interfaces and Presentation, Pixel, solid modelling]
An Augmented Reality X-Ray system based on visual saliency
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
In the past, several systems have been presented that enable users to view occluded points of interest using Augmented Reality X-ray visualizations. It is challenging to design a visualization that provides correct occlusions between occluder and occluded objects while maximizing legibility. We have previously published an Augmented Reality X-ray visualization that renders edges of the occluder region over the occluded region to facilitate correct occlusions while providing foreground context. While this approach is simple and works in a wide range of situations, it provides only minimal context of the occluder object. In this paper, we present the background, design, and implementation of our novel visualization technique that aims at providing users with richer context of the occluder object. While our previous visualization only employed one salient feature (edges) to determine which parts of the occluder to display, our novel visualization technique is an initial attempt to explore the design space of employing multiple salient features for this task. The prototype presented in this paper employs three additional salient features: hue, luminosity, and motion. We have conducted two evaluations with human participants to investigate the benefits and limitations of our prototype compared to our previous system. The first evaluation showed that although our novel visualization provides a richer context of the occluder object, it does not impede users to select objects in the occluded area; but, it also indicated problems in our prototype. In the second evaluation, we have investigated these problems through an online survey with systematically varied occluder and occluded scenes, focussing on the qualitative aspects of our visualizations. The results were encouraging, but pointed out that our novel visualization needs a higher level of adaptiveness.
[Context, Visualization, visualization, Image edge detection, augmented reality, augmented reality X-ray, saliency, X-ray imaging, occluder object, evaluation, Image color analysis, visual saliency, Prototypes, data visualisation, augmented reality x-ray visualizations, Rendering (computer graphics)]
Determining the point of minimum error for 6DOF pose uncertainty representation
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
In many augmented reality applications, in particular in the medical and industrial domains, knowledge about tracking errors is important. Most current approaches characterize tracking errors by 6&#x00D7;6 covariance matrices that describe the uncertainty of a 6DOF pose, where the center of rotational error lies in the origin of a target coordinate system. This origin is assumed to coincide with the geometric centroid of a tracking target. In this paper, we show that, in case of a multi-camera fiducial tracking system, the geometric centroid of a body does not necessarily coincide with the point of minimum error. The latter is not fixed to a particular location, but moves, depending on the individual observations. We describe how to compute this point of minimum error given a covariance matrix and verify the validity of the approach using Monte Carlo simulations on a number of scenarios. Looking at the movement of the point of minimum error, we find that it can be located surprisingly far away from its expected position. This is further validated by an experiment using a real camera system.
[tracking error, covariance matrices, Target tracking, Uncertainty, multicamera fiducial tracking system, optical tracking, augmented reality, geometric centroid, Covariance matrix, I.4.8 [Image Processing and Computer Vision, Jacobian matrices, cameras, Monte Carlo methods, pose estimation, target tracking, Image Processing and Computer Vision, measurement uncertainty, Cameras, point of minimum error, Information Interfaces and Presentation, 6D0F pose uncertainty representation, position measurement, Erbium, Monte Carlo simulation]
Point-and-shoot for ubiquitous tagging on mobile phones
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
We propose a novel way to augment a real scene with minimalist user intervention on a mobile phone: The user only has to point the phone camera to the desired location of the augmentation. Our method is valid for vertical or horizontal surfaces only, but this is not a restriction in practice in man-made environments, and avoids to go through any reconstruction of the 3D scene, which is still a delicate process. Our approach is inspired by recent work on perspective patch recognition and we show how to modify it for better performances on mobile phones and how to exploit the phone accelerometers to relax the need for fronto-parallel views. In addition, our implementation allows to share the augmentations and the required data over peer-to-peer communication to build a shared AR space on mobile phones.
[real scene augmentation, user intervention, augmented reality, Mobile handsets, Graphics processing unit, cameras, mobile computing, AR space, Three dimensional displays, phone accelerometer, peer-to-peer communication, Kernel, point-and-shoot, Accelerometers, peer-to-peer computing, patch recognition, image reconstruction, phone camera, mobile phone, 3D scene reconstruction, Cameras, accelerometers, ubiquitous tagging, Pixel, image recognition, mobile handsets]
Foldable augmented maps
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
This paper presents folded surface detection and tracking for augmented maps. For the detection, plane detection is iteratively applied to 2D correspondences between an input image and a reference plane because the folded surface is composed of multiple planes. In order to compute the exact folding line from the detected planes, the intersection line of the planes is computed from their positional relationship. After the detection is done, each plane is individually tracked by frame-by-frame descriptor update. For a natural augmentation on the folded surface, we overlay virtual geographic data on each detected plane. The user can interact with the geographic data by finger pointing because the finger tip of the user is also detected during the tracking. As scenario of use, some interactions on the folded surface are introduced. Experimental results show the accuracy and performance of folded surface detection for evaluating the effectiveness of our approach.
[Solid modeling, intersection line, Image edge detection, Estimation, finger pointing, H.5.1 [Information Interfaces and Presentation, augmented reality, foldable augmented map, object detection, Electronic mail, frame-by-frame descriptor update, Databases, natural augmentation, feature extraction, virtual geographic data, folded surface detection, finger tip detection, Cameras, folding line, Three dimensional displays, Information Interfaces and Presentation]
Management of tracking for industrial AR setups
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
The accuracy of a real time tracking system for industrial AR (IAR) applications often needs to comply with production tolerances. Such a system typically incorporates different off-/online devices so that the overall precision and accuracy cannot be trivially stated. Additionally, tracking needs to be flexible to not interfere with existing working processes and it needs to be operated and maintained free of error by on-site personnel who typically have a quality management (QM) background. For the final validation of such a complex tracking setup, empiric testing alone is either too expensive or lacks generality. This paper demonstrates a new approach to define and verify, deploy and validate, as well as to operate and maintain an IAR tracking infrastructure. We develop our concepts on the basis of an IAR application in the field of QM in the aircraft production process. It integrates a qualitative visual comparison with accurate quantitative measurements of 3D coordinates using a metrological probe. The focus is on the verification, validation, and error free operation. Monte Carlo simulation predicts the error for arbitrary system states. Using a limited set of empiric measurements in the target environment allows us to validate the simulation and thereby validate the application. This combination assures compliance of the IAR application with the required production tolerances. We show that our simulation model yields realistic results, using an in-depth analysis of an optical IR tracking system and a high-precision coordinate measurement machine capable of densely sampling the entire tracking volume. Additionally, it allows for a straightforward derivation of run-time consistency checks for the automatic identification of possible system failures. Also, estimation of the system performance during the planning and definition phases becomes possible, using the elementary accuracy specifications of the involved sensor systems.
[offline device, 3D coordinate measurement, optical tracking, production engineering computing, augmented reality, Electronic mail, real time tracking system, system recovery, industrial AR setup, onsite personnel, metrological probe, Analytical models, Monte Carlo methods, spatial variables measurement, run-time consistency check, Variable speed drives, IAR tracking, Sensors, optical IR tracking system, Computational modeling, system failure, aircraft, Maintenance engineering, Calibration, aircraft production process, production tolerance, quality management, online device, target tracking, personnel, tracking management, Monte Carlo simulation]
Task support system by displaying instructional video onto AR workspace
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
This paper presents an instructional support system based on augmented reality (AR). This system helps a user to work intuitively by overlaying visual information in the same way of a navigation system. In usual AR systems, the contents to be overlaid onto real space are created with 3D Computer Graphics. In most cases, such contents are newly created according to applications. However, there are many 2D videos that show how to take apart or build electric appliances and PCs, how to cook, etc. Therefore, our system employs such existing 2D videos as instructional videos. By transforming an instructional video to display, according to the user's view, and by overlaying the video onto the user's view space, the proposed system intuitively provides the user with visual guidance. In order to avoid the problem that the display of the instructional video and the user's view may be visually confused, we add various visual effects to the instructional video, such as transparency and enhancement of contours. By dividing the instructional video into sections according to the operations to be carried out in order to complete a certain task, we ensure that the user can interactively move to the next step in the instructional video after a certain operation is completed. Therefore, the user can carry on with the task at his/her own pace. In the usability test, users evaluated the use of the instructional video in our system through two tasks: a task involving building blocks and an origami task. As a result, we found that a user's visibility improves when the instructional video is transformed to display according to his/her view. Further, for the evaluation of visual effects, we can classify these effects according to the task and obtain the guideline for the use of our system as an instructional support system for performing various other tasks.
[Real time systems, Visualization, AR workspace, instructional support system, H.5.1 [Information Interfaces and Presentation, instructional video display, augmented reality, Calibration, origami task, task support system, computer graphics, Image color analysis, Streaming media, Cameras, Visual effects, Information Interfaces and Presentation, computer aided instruction, building blocks, 3D Computer Graphics]
Evaluation of the virtual mirror as a navigational aid for augmented reality driven minimally invasive procedures
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
The paper presents a user study investigating perceptual effects of a virtual mirror being used as a tangible interactive tool in a medical Augmented Reality scenario. In particular, we evaluated whether an additional perspective provided by the virtual mirror supports instrument guidance in terms of precision and straightness. 31 participants were asked to navigate an endoscopic instrument in a simulated minimally invasive port setting. Results show a significant higher precision of instrument guidance when a virtual mirror provides additional views of the target region.
[navigational aid, endoscopic instrument, Target tracking, Navigation, virtual mirror, instrument guidance, Instruments, tangible interactive tool, augmented reality, Multimedia Information Systems, H.5.1 [Multimedia Information Systems, Surgery, Cameras, Information Interfaces and Presentation, Mirrors, Biomedical imaging]
Differential Instant Radiosity for mixed reality
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
In this paper we present a novel plausible realistic rendering method for mixed reality systems, which is useful for many real life application scenarios, like architecture, product visualization or edutainment. To allow virtual objects to seamlessly blend into the real environment, the real lighting conditions and the mutual illumination effects between real and virtual objects must be considered, while maintaining interactive frame rates (20-30fps). The most important such effects are indirect illumination and shadows cast between real and virtual objects. Our approach combines Instant Radiosity and Differential Rendering. In contrast to some previous solutions, we only need to render the scene once in order to find the mutual effects of virtual and real scenes. The dynamic real illumination is derived from the image stream of a fish-eye lens camera. We describe a new method to assign virtual point lights to multiple primary light sources, which can be real or virtual. We use imperfect shadow maps for calculating illumination from virtual point lights and have significantly improved their accuracy by taking the surface normal of a shadow caster into account. Temporal coherence is exploited to reduce flickering artifacts. Our results show that the presented method highly improves the illusion in mixed reality applications and significantly diminishes the artificial look of virtual objects superimposed onto real scenes.
[Real time systems, virtual reality, product visualization, virtual point light, mixed reality system, virtual object, Light sources, light sources, temporal coherence, Differential Rendering, Real-time Global Illumination, Lighting, Virtual reality, plausible realistic rendering method, flickering artifacts, differential rendering, rendering (computer graphics), Mixed Reality, differential instant radiosity, image processing, fish eye lens camera, image stream, interactive frame rates, Shadow mapping, brightness, Instant Radiosity, Rendering (computer graphics), Cameras, lighting condition]
Foreground and shadow occlusion handling for outdoor augmented reality
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
Occlusion handling in augmented reality (AR) applications is challenging in synthesizing virtual objects correctly into the real scene with respect to existing foregrounds and shadows. Furthermore, outdoor environment makes the task more difficult due to the unpredictable illumination changes. This paper proposes novel outdoor illumination constraints for resolving the foreground occlusion problem in outdoor environment. The constraints can be also integrated into a probabilistic model of multiple cues for a better segmentation of the foreground. In addition, we introduce an effective method to resolve the shadow occlusion problem by using shadow detection and recasting with a spherical vision camera. We have applied the system in our digital cultural heritage project named Virtual Asuka (VA) and verified the effectiveness of the system.
[shadow occlusion handling, foreground occlusion handling, shadow detection, augmented reality, object detection, Sun, foreground segmentation, Image segmentation, I.3.7 [Computer graphics, spherical vision camera, Image color analysis, Motion segmentation, image segmentation, Lighting, Computer graphics, Image Processing and Computer Vision, Cameras, virtual Asuka, Pixel, outdoor augmented reality, shadow recasting]
The importance of eye-contact for collaboration in AR systems
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
Eye contact is believed to be an important factor in normal human communication and as a result of this a head mounted display (HMD) is often seen as something intrusive and limiting. This can be especially problematic when AR is used in a collaborative setting. The study presented in this paper aims to investigate the effects an HMD-based AR system can have on eye contact behaviour between participants in a collaborative task and thus, in extension, the effects of the HMD on collaboration itself. The focus of the study is on task-oriented collaboration between professionals. The participants worked through three different scenarios alternating between HMDs and regular paper maps with the purpose of managing the crisis response to a simulated major forest fire. Correlations between eye contact between participants and questionnaire items concerning team- and taskwork were analysed, indicating that, for the paper map condition, a high amount of eye contact is associated with low confidence and trust in the artefacts used (i.e. paper map and symbols). The amount of eye-contact in both conditions was very low. It was significantly higher for conditions without HMDs. However, the confidence and trust in the artefacts was generally rated significantly higher with HMDs than without. In conclusion, the decrease in eye contact with HMDs does not seem to have a direct effect on the collaboration in a professional, task-oriented context. This is contrary to popular assumptions and the results are relevant for future design choices for AR systems using HMDs.
[Correlation, Biological system modeling, INFORMATION INTERFACES AND PRESENTATION, augmented reality, helmet mounted displays, human communication, AR systems collaboration, collaborative task, Command and control systems, augmented reality systems, H.5.1 [INFORMATION INTERFACES AND PRESENTATION, groupware, Cameras, Collaborative work, head mounted display, Teamwork, eye contact]
Experiences with an AR evaluation test bed: Presence, performance, and physiological measurement
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
This paper discusses an experiment carried out in an AR test bed called &#x201C;the pit&#x201D;. Inspired by the well-known VR acrophobia study of Meehan et al. [18], the experimental goals were to explore whether VR presence instruments were useful in AR (and to modify them where appropriate), to compare additional measures to these well-researched techniques, and to determine if findings from VR evaluations can be transferred to AR. An experimental protocol appropriate for AR was developed. The initial experimental findings concern varying immersion factors (frame rate) and their effect on feelings of presence, user performance and behavior. Unlike the VR study, which found differing frame rates to affect presence measures, there were few differences in the five frame rate modes in our study as measured by the qualitative and quantitative instruments, which included physiological responses, a custom presence questionnaire, task performance, and user behavior. The AR presence questionnaire indicated users experienced a high feeling of presence in all frame rate modes. Behavior, performance, and interview results indicated the participants felt anxiety in the pit environment. However, the physiological data did not reflect this anxiety due to factors of user experience and experiment design. Efforts to develop a useful AR test bed and to identify results from a large data set has produced a body of knowledge related to AR evaluation that can inform others seeking to create AR experiments.
[Visualization, Evaluation, physiological responses, Instruments, Laboratories, Augmented Reality, augmented reality, task performance, virtual reality acrophobia study, custom presence questionnaire, Atmospheric measurements, frame rate, augmented reality evaluation test bed, immersion factors, Particle measurements, Presence, Software, Physiological Measures, physiological measurement, user behavior]
Effects of a retroreflective screen on depth perception in a head-mounted projection display
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
The perceived depth accuracy is a fundamental performance metrics for an augmented reality display system. Many factors may affect the perceived depth of a head-mounted display (HMD), such as display resolution, interpupillary distance (IPD), conflicting depth cues, stereoacuity, and head tracking error. Besides these typical limiting factors to an HMD-type system, the perceived depth through a head-mounted projection display (HMPD) may be further affected by the usage of a retroreflective screen. In this paper, we will evaluate the perceived depth accuracy of an HMPD system using a perceptual depth matching method. The main factor to be investigated in our study is the position of a retroreflective screen relative to the projection image plane, with the projection image plane placed at different distances to the user. Overall, it is found that the ratio of the judged distance to real distance and the standard deviation of the judged distance increase linearly with the reference object distance. A transition effect from depth underestimation to overestimation has been observed at the reference object distance of around 1.4m. The position of a retroreflective screen only has a significant effect on the depth judgment error around this switching point. The paper also analyzes various effects brought by a retroreflective screen on the depth judgment. The depth cue and the image luminance reduction brought by a retroreflective screen could be the main factors that affect the depth judgment accuracy.
[Legged locomotion, image luminance reduction, augmented reality display system, Optical imaging, augmented reality, helmet mounted displays, Calibration, HEAD-MOUNTED DISPLAYS, retroreflective screen, AUGMENTED REALITY, head mounted projection display, Accuracy, Focusing, computer vision, perceived depth accuracy, interpupillary distance, Cameras, display resolution, Adaptive optics, depth perception]
A practical multi-viewer tabletop autostereoscopic display
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
This paper introduces a multi-user autostereoscopic tabletop display and its associated real-time rendering methods. Tabletop displays that support both multiple viewers and autostereoscopy have been extremely difficult to construct. Our new system is inspired by the &#x201C;Random Hole Display&#x201D; design that modified the pattern of openings in a barrier mounted in front of a flat panel display from thin slits to a dense pattern of tiny, pseudo-randomly placed holes. This allows viewers anywhere in front of the display to see a different subset of the display's native pixels through the random-hole screen. However, a fraction of the visible pixels will be observable by more than a single viewer. Thus the main challenge is handling these &#x201C;conflicting&#x201D; pixels, which ideally must show different colors to each viewer. We introduce several solutions to this problem and describe in detail the current method of choice, a combination of color blending and approximate error diffusion, performing in real time in our GPU-based implementation. The easily reproducible design uses a pattern film barrier affixed to the display by means of a transparent polycarbonate layer spacer. We use a commercial optical tracker for viewers' locations and synthesize the appropriate image (or a stereoscopic image pair) for each viewer. The system supports graceful degradation with increasing number of simultaneous views, and graceful improvement as the number of views decreases.
[multiviewer tabletop, flat panel display, approximate error diffusion, Image resolution, flat panel displays, collaborative display, random hole display design, 3D display, Calibration, autostereoscopic display, GPU based implementation, parallax barrier, Image color analysis, real time rendering method, mixed reality, Rendering (computer graphics), Three dimensional displays, rendering (computer graphics), tabletop display, Pixel, multi-user display, Lenses, random hole barrier]
The City of Sights: Design, construction, and measurement of an Augmented Reality stage set
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
We describe the design and implementation of a physical and virtual model of an imaginary urban scene-the &#x201C;City of Sights&#x201D;- that can serve as a backdrop or &#x201C;stage&#x201D; for a variety of Augmented Reality (AR) research. We argue that the AR research community would benefit from such a standard model dataset which can be used for evaluation of such AR topics as tracking systems, modeling, spatial AR, rendering tests, collaborative AR and user interface design. By openly sharing the digital blueprints and assembly instructions for our models, we allow the proposed set to be physically replicable by anyone and permit customization and experimental changes to the stage design which enable comprehensive exploration of algorithms and methods. Furthermore we provide an accompanying rich dataset consisting of video sequences under varying conditions with ground truth camera pose. We employed three different ground truth acquisition methods to support a broad range of use cases. The goal of our design is to enable and improve the replicability and evaluation of future augmented reality research.
[Solid modeling, Computational modeling, imaginary urban scene, Subspace constraints, ground truth acquisition methods, video sequence, ground truth camera pose, augmented reality, Calibration, user interfaces, augmented reality stage set, digital blueprints sharing, Accuracy, Cameras, Three dimensional displays, collaborative AR design, user interface design, rendering (computer graphics), image sequences, City of Sights, rendering test, solid modelling]
Build your world and play in it: Interacting with surface particles on complex objects
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
We explore interacting with everyday objects by representing content as interactive surface particles. Users can build their own physical world, map virtual content onto their physical construction and play directly with the surface using a stylus. A surface particle representation allows programmed content to be created independent of the display object and to be reused on many surfaces. We demonstrated this idea through a projector-camera system that acquires the object geometry and enables direct interaction through an IR tracked stylus. We present three motivating example applications, each displayed on three example surfaces. We discuss a set of interaction techniques that show possible avenues for structuring interaction on complicated everyday objects, such as Surface Adaptive GUIs for menu selection. Through a preliminary informal evaluation and interviews with end users, we demonstrate the potential of interacting with surface particles and identify improvements necessary to make this interaction practical on everyday surfaces.
[Visualization, virtual reality, virtual content, graphical user interfaces, projector camera system, computational geometry, IR tracked stylus, interactive surface particle, Surface treatment, Information interfaces and presentation, physical world, Games, H5.2 [Information interfaces and presentation, complex object, Three dimensional displays, GUI, Surface texture, Face, object geometry, Sprites (computer), solid modelling]
Positioning, tracking and mapping for outdoor augmentation
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
This paper presents a novel approach for user positioning, robust tracking and online 3D mapping for outdoor augmented reality applications. As coarse user pose obtained from GPS and orientation sensors is not sufficient for augmented reality applications, sub-meter accurate user pose is then estimated by a one-step silhouette matching approach. Silhouette matching of the rendered 3D model and camera data is carried out with shape context descriptors as they are invariant to translation, scale and rotational errors, giving rise to a non-iterative registration approach. Once the user is correctly positioned, further tracking is carried out with camera data alone. Drifts associated with vision based approaches are minimized by combining different feature modalities. Robust visual tracking is maintained by fusing frame-to-frame and model-to-frame feature matches. Frame-to-frame tracking is accomplished with corner matching while edges are used for model-to-frame registration. Results from individual feature tracker are fused using a pose estimate obtained from an extended Kalman filter (EKF) and a weighted M-estimator. In scenarios where dense 3D models of the environment are not available, online 3D incremental mapping and tracking is proposed to track the user in unprepared environments. Incremental mapping prepares the 3D point cloud of the outdoor environment for tracking.
[model to frame feature match, robust tracking, Shape, 3D point cloud, augmented reality, sensor fusion, corner matching, rotational error, GPS, tracking, weighted M-estimator, cameras, vision based approach, pose estimation, outdoor augmented reality application, Robustness, Three dimensional displays, shape recognition, Kalman filters, user positioning, rendering (computer graphics), image sequences, feature tracker, Context, one step silhouette matching approach, Target tracking, shape matching, noniterative registration approach, robust visual tracking, frame to frame tracking, image matching, Augmented reality, Global Positioning System, coarse user pose estimation, orientation sensor, rendered 3D model, extended Kalman filter, camera data, shape context descriptor, Cameras, online 3D incremental mapping, online 3D mapping, solid modelling, 3D mapping]
Towards real time 3D tracking and reconstruction on a GPU using Monte Carlo simulations
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
This paper addresses the problem of camera tracking and 3D reconstruction from image sequences, i.e., the monocular SLAM problem. Traditionally, this problem is solved using non-linear minimization techniques that are very accurate but hardly used in real time. This work presents a highly parallelizable random sampling approach based on Monte Carlo simulations that fits very well on the graphics hardware. The proposed algorithm achieves the same precision as non linear optimization, getting real time performance running on commodity graphics hardware. Both accuracy and performance are evaluated using synthetic data and real video sequences captured with a hand-held camera. Moreover, results are compared with an implementation of Bundle Adjustment showing that the presented method gets similar results in much less time.
[Real time systems, computer graphic equipment, commodity graphics hardware, camera tracking, Estimation, real time 3D tracking, object detection, monocular SLAM problem, coprocessors, bundle adjustment, GPU, Graphics processing unit, nonlinear optimization, image sensors, real time 3D reconstruction, Simultaneous localization and mapping, hand held camera, Monte Carlo simulations, Cameras, Three dimensional displays, minimisation, nonlinear minimization techniques, Kernel, image sequences]
Keyframe-based modeling and tracking of multiple 3D objects
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
We propose a real-time solution for modeling and tracking multiple 3D objects in unknown environments. Our contribution is two-fold: First, we show how to scale with the number of objects. This is done by combining recent techniques for image retrieval and online Structure from Motion, which can be run in parallel. As a result, tracking 40 objects in 3D can be done within 6 to 25 milliseconds per frame, even under difficult conditions for tracking. Second, we propose a method to let the user add new objects very quickly. The user simply has to select in an image a 2D region lying on the object. A 3D primitive is then fitted to the features within this region, and adjusted to create the object 3D model. In practice, this procedure takes less than a minute.
[Solid modeling, 3d object tracking, Target tracking, H.5.1 [Information Interfaces and Presentation, Image reconstruction, image motion analysis, target tracking, Image Processing and Computer Vision, image retrieval, Cameras, Feature extraction, Three dimensional displays, Information Interfaces and Presentation]
Interactive modelling for AR applications
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
We present a method for estimating the 3D shape of an object from a sequence of images captured by a hand-held device. The method is well suited to augmented reality applications in that minimal user interaction is required, and the models generated are of an appropriate form. The method proceeds by segmenting the object in every image as it is captured and using the calculated silhouette to update the current shape estimate. In contrast to previous silhouette-based modelling approaches, however, the segmentation process is informed by a 3D prior based on the previous shape estimate. A voting scheme is also introduced in order to compensate for the inevitable noise in the camera position estimates. The combination of the voting scheme with the closed-loop segmentation process provides a robust and flexible shape estimation method. We demonstrate the approach on a number of scenes where segmentation without a 3D prior would be challenging.
[Solid modeling, 3D shape estimation, Shape, augmented reality, interactive modelling, image sequence, voting scheme, AR applications, Image segmentation, Image color analysis, I.4.6 [Image Processing and Computer Vision, image segmentation, Image Processing and Computer Vision, Cameras, Three dimensional displays, shape recognition, Information Interfaces and Presentation, closed loop segmentation process, Pixel, image sequences, solid modelling]
Advanced self-contained object removal for realizing real-time Diminished Reality in unconstrained environments
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
While Augmented Reality has always been restricted to adding artificial content to the real environment, Diminished Reality allows for removing real world content. Existing approaches however, either require complex setups or are not applicable in real-time. In this paper we present our approach for removing real-world objects from a live video stream of the user's real environment. Our approach is based on a simple setup and neither requires any pre-processing nor any information on the structure and location of the objects to be removed or on their background. Our approach is based on the identification of the objects to be removed combined with an image completion and synthesis algorithm. The performance of our approach is one to two magnitudes better than that of previous work in the area of image completion, providing real-time object cancellation on standard laptop or tablet computers.
[Real time systems, Mediated Reality, object cancellation, Diminished Reality, Pipelines, diminished reality, image synthesis, Augmented Reality, augmented reality, image completion, Image color analysis, Coherence, computer vision, Streaming media, Cameras, video stream, Pixel]
An automatic parallax adjustment method for stereoscopic augmented reality systems
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
This paper presents an automatic parallax adjustment method that considers the border effect to produce more realistic stereo images on a stereoscopic augmented reality system. Three-dimensional (3D) imaging is an emerging method of displaying three-dimensional information and providing an immersive and intuitive experience with augmented reality. However, the protruding parts of displayed stereoscopic images may be blurry and cause viewing discomfort. Furthermore, the border effect may make it difficult for an imaging system to display regions next to screen borders, even with considerable negative parallax. This paper proposes a method of automatically adjusting the parallax of displayed stereo images by analyzing the feature points in regions near screen borders to produce better stereo effects. Experimental results and a subjective assessment of human factor issues indicate that the proposed method makes stereoscopic augmented reality systems significantly more attractive and comfortable to view.
[Visualization, acceptance of MR/AR technology, usability studies and experiments, augmented reality, automatic parallax adjustment method, MR/AR for entertainment, stereo images, Augmented reality, three dimensional imaging, stereoscopic augmented reality system, stereo image processing, negative parallax, Feature extraction, Cameras, Three dimensional displays, Stereo vision]
Painterly rendering with coherence for augmented reality
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
A seamless blending of the real and virtual worlds is key to increased immersion and improved user experiences for augmented reality (AR). Photorealistic and non-photorealistic rendering (NPR) are two ways to achieve this goal. Non-photorealistic rendering creates an abstract version of both the real and virtual world by stylization to make them indistinguishable. We presented a painterly rendering algorithm for AR applications. This algorithm paints composed AR video frames with bump-mapping curly brushstrokes. Tensor fields are created for each frame to define direction for brushstrokes. The anchor point of a brushstroke is tracked or warped from frame to frame. Brushstrokes are also reshaped to provide better temporal coherence. The major difference between our algorithm and existing NPR work in general graphics and AR/VR areas is we use feature points across composed AR video frames to maintain coherence in the rendering.
[AR video frame, augmented reality, bump mapping, painterly rendering, virtual world, Augmented reality, temporal coherence, abstract version, Tensile stress, feature point, Coherence, Streaming media, Non-photorealistic rendering, Rendering (computer graphics), rendering (computer graphics), video signal processing, Pixel, Paints, nonphotorealistic rendering, brushstroke]
k-MART: Authoring tool for mixed reality contents
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
In this poster, we present an authoring tool for mixed reality (MR) contents, called k-MART. The tool is comprehensive to accommodate many different types of mixed reality applications and platforms, being able to mix and match different types of contexts and behaviors for content creation. The content is intuitively modeled as a collection of &#x201C;context-behavior&#x201D; pairs and saved in a declarative manner to be &#x201C;played&#x201D; by a content browser. The tool is designed to minimize user programming and offers many abstractions of functionalities with only important details to be filled in by the user using graphical user interfaces. We believe that our approach has a good potential for the wide dissemination and sharing of MR contents.
[Context, content browser, graphical user interfaces, mixed and augmented reality authoring tool, authoring tool, augmented reality, k-MART, Standards, authoring systems, mixed reality contents, Authoring, Contents, Virtual reality, User interfaces, Three dimensional displays, Software, context-behavior pairs, Behavior, Mixed Reality, Context modeling]
A precise controllable projection system for projected virtual characters and its calibration
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
In this paper we describe a system to project virtual characters that shall live with us in the same environment. In order to project the characters' visual representations onto room surfaces we use a controllable projector.
[Visualization, virtual reality, Conferences, Humans, Control systems, Calibration, optical projectors, project virtual characters, visual representations, controllable projector, Cameras, calibration, Lenses]
Generating vision based Lego augmented reality training and evaluation systems
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
The creation of training applications using Augmented Reality (AR) is still a new field of research. In order to get good training results therefore evaluation should be performed. For the creation of such systems the questions arising are related to the general process of generation, visualization, evaluation and its psychological background. An important aspect of vision based AR is also the robust tracking and initialization of objects for correct augmentation. In this work we present a concept of an entire processing chain, which allows for efficient and automatic generation of such training systems that can also be used for evaluation. We do this in the context of a Lego training system. While explaining the whole process of application generation and usage, we also present a novel approach for robust marker free initialization of colored partly occluded plates and their tracking using one off the shelf monocular camera.
[Solid modeling, Visualization, robust tracking, Lego training system, shelf monocular camera, PATTERN RECOGNITION, vision based Lego augmented reality training, psychological background, INFORMATION INTERFACES AND PRESENTATION, augmented reality, object detection, training, robust marker free initialization, Augmented reality, Training, cameras, Image color analysis, H.5.1 [INFORMATION INTERFACES AND PRESENTATION, computer vision, colored partly occluded plate, Robustness, Cognitive science]
Augmented reality in large environments: Application to aided navigation in urban context
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
This paper addresses the challenging issue of vision-based localization in urban context. It briefly describes our contributions in large environments modeling and accurate camera localization. The efficiency of the resulting system is illustrated through Augmented Reality results on large trajectory of several hundred meters.
[Solid modeling, urban context, vision based localization, augmented reality, Multimedia Information Systems, Image reconstruction, Augmented reality, H.5.1 [Multimedia Information Systems, image sensors, Scene Analysis, Databases, computer vision, camera localization, Cities and towns, Cameras, Three dimensional displays]
Color harmonization for Augmented Reality
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
In this paper we discuss color harmonization for Augmented Reality. Color harmonization is a technique used to adjust the combination of colors in order to follow aesthetic guidelines. We implemented a system which is able to harmonize the combination of the colors in video based AR systems. The presented approach is able to re-color virtual and real-world items, achieving overall more visually pleasant results. In order to allow preservation of certain colors in an AR composition, we furthermore introduce the concept of constraint color harmonization.
[Real time systems, Visualization, aesthetic guidelines, augmented reality, Augmented reality, Histograms, Image color analysis, Cameras, image colour analysis, video based augmented color reality, video signal processing, constraint color harmonization, Pixel]
Extended investigations of user-related issues in mobile industrial AR
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
The potential of Augmented Reality (AR) to support industrial processes has been demonstrated in several studies. While there have been first investigations on user related issues in the long-duration use of mobile AR systems, to date the impact of theses systems on physiological and psychological aspects is not explored extensively. We conducted an extended study in which 19 participants worked 4 hours continuously in an order picking process with and without AR support. Results of the study comparing strain and work efficiency are presented and open issues are discussed.
[Visualization, Target tracking, Error analysis, Navigation, order picking process, mobile industrial AR, Mobile communication, augmented reality, order picking, H.1.2 [Models and Principles, Models and Principles, Strain, psychological aspects, industrial engineering, Information Interfaces and Presentation, user related issues, Heart rate variability, physiological aspects]
Digital Diorama system for museum exhibition
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
In this paper, we proposed the Digital Diorama system to convey background information vividly. The system superimposes computer generated diorama scene reconstructed from related image/video materials on real exhibits. In order to switch and superimpose real exhibits and past photos seamlessly, we implement a matching system for estimating the camera position where photos are taken. By applying this subsystem to 26 past photos about the steam locomotive exhibit, we succeeded in estimating their camera position. Thus, we implement and install a prototype system at estimated position to superimposing virtual scene and real exhibit in the Railway Museum.
[I.3.3 [Computer Graphics, matching system, Materials, museum exhibition, Educational institutions, Electronic mail, camera position, position estimation, locomotives, Videos, steam locomotive, Computer Graphics, museums, Prototypes, Cameras, digital diorama system, Rail transportation]
KHARMA: An open KML/HTML architecture for mobile augmented reality applications
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
Widespread future adoption of augmented reality technology will rely on a broadly accessible standard for authoring and distributing content with, at a minimum, the flexibility and interactivity provided by current web authoring technologies. We introduce KHARMA, an open architecture based on KML for geospatial and relative referencing combined with HTML, JavaScript and CSS technologies for content development and delivery. This architecture uses lightweight representations that decouple infrastructure and tracking sources from authoring and content delivery. Our main contribution is a re-conceptualization of KML that turns HTML content formerly confined to balloons into first-class elements in the scene. We introduce the KARML extension that gives authors increase control over the presentation of HTML content and its spatial relationship to other content.
[Protocols, Augmented Reality, augmented reality, World Wide Web, HTML, Browsers, mobile augmented reality technology, Servers, Augmented reality, authoring systems, JavaScript technology, Global Positioning System, open KML-HTML architecture, Web authoring technologies, software architecture, content development, Authoring, mobile computing, KHARMA, CSS technology, Computer architecture, Internet, hypermedia markup languages]
EXMAR: EXpanded view of mobile augmented reality
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
There have been many studies to minimize the psychological and physical load increase caused by mobile augmented reality systems. In this paper, we propose a new technique called &#x201C;EXMAR&#x201D;, which enables the user to explore his/her surroundings with an expanded field of view, resulting in a decrease of physical movement. Through this novel interaction technique, the user can explore off-screen point of interests with environmental contextual information by simple dragging gestures. To evaluate this initial approach, we conducted a proof of concept usability test under a set of scenarios such as &#x201C;Exploring objects behind the user&#x201D;, &#x201C;Avoiding the invasion of personal space&#x201D; and &#x201C;Walk and type with front-view.&#x201D; Through this initial examination, we found that users can explore off-screen point of interests and grasp the spatial relations without the increase of mental effort. We believe that this preliminary study gives a meaningful indication that employing the interactive field of view can be a useful method to decrease the physical load without any additional mental efforts in a mixed and augmented reality environment.
[Fish-eye lens, Visualization, proof of concept usability test, Augmented Reality, off-screen point of interests, Expanded Field Of View (EFOV), augmented reality, environmental contextual information, Indexes, mobile augmented reality system, mobile computing, Distortion Correction, EXMAR technique, interaction, Three dimensional displays, dragging gestures, expanded field of view, Mixed Reality, interaction technique]
Haptic simulation of breast cancer palpation: A case study of haptic augmented reality
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
Haptic augmented reality (AR) allows to modulate the haptic properties of a real object by providing virtual haptic feedback. We previously developed a haptic AR system wherein the stiffness of a real object can be augmented with the aid of a haptic interface. To demonstrate its potential, this paper presents a case study for medical training of breast cancer palpation. A real breast model made of soft silicone is augmented with a virtual tumor rendered inside. Haptic stimuli for the virtual tumor are generated based on a contact dynamics model identified via real measurements, without the need of geometric information on the breast. A subjective evaluation confirmed the realism and fidelity of our palpation system.
[Solid modeling, soft silicone, Biological system modeling, Force, haptic interfaces, augmented reality, Haptic interfaces, haptic interface, virtual haptic feedback, silicones, virtual tumor, H.5.2 [Information Interfaces and Presentation, breast cancer palpation, Breast, Data models, cancer, Information Interfaces and Presentation, Tumors, haptic augmented reality]
Range-finding projectors: Visualizing range information without sensors
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
We describe sensorless visualizing techniques of 3D range information of object surfaces, in which the range information is projected directly onto the real-world surface by projectors. The proposed system consists of two projectors that merely project still images. Two techniques, namely, contour map projection based on the moire method and pseudo-color map projection based on complementary colors, are shown to be used simultaneously.
[sensorless visualizing techniques, Color, Information Systems, augmented reality, Computing Methodologies, range information visualization, moire method, pseudocolor map projection, Image color analysis, distance measurement, I.4.9 [Computing Methodologies, Data visualization, Cameras, complementary colors, Sensors, Gratings, range-finding projectors, object surfaces, contour map projection, Pixel]
3D discrepancy check via Augmented Reality
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
For many tasks like markerless model-based camera tracking it is essential that the 3D model of a scene accurately represents the real geometry of the scene. It is therefore very important to detect deviations between a 3D model and a scene. We present an innovative approach which is based on the insight that camera tracking can not only be used for Augmented Reality visualization but also to solve the correspondence problem between 3D measurements of a real scene and their corresponding positions in the 3D model. We combine a time-of-flight camera (which acquires depth images in real time) with a custom 2D camera (used for the camera tracking) and developed an analysis-by-synthesis approach to detect deviations between a scene and a 3D model of the scene.
[Solid modeling, Computational modeling, Artificial Intelligence, augmented reality visualization, computational geometry, markerless model based camera tracking, augmented reality, object detection, image sensors, custom 2D camera, scene geometry, Image processing and computer vision, data visualisation, Computer graphics, Cameras, Three dimensional displays, Data models, 3D discrepancy check, I.2.10 [Artificial Intelligence, time-of-flight camera, Mathematical model, Pixel]
Augmentation of check in/out model for remote collaboration with Mixed Reality
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
This paper proposes augmentation of check in/out model for remote collaboration with Mixed Reality (MR). We add a 3D shared and private space into the real workspace by MR technology, and augment the check in/out model to remote collaboration. By our proposal, user can intuitively receive remote partner's work via virtual objects in the shared space and stop sharing information about an object by just moving the object into the private space if the user doesn't want to share it. We implement a system which achieves our proposal and evaluate it.
[Solid modeling, Brushes, Remote Collaboration, Aerospace electronics, augmented reality, remote collaboration, check in-out model, Space technology, mixed reality, Collaboration, Virtual reality, groupware, Three dimensional displays, Mixed Reality, Check In/Out Model]
Sensor Synchronization for AR Applications
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
In this paper, we give a brief introduction to the sensor synchronization problem, highlight how the choice of hardware affects what synchronization methods can be applied, and present our ongoing research on sensor synchronization. Our work is based on estimating timestamps on the host processor, using a method suitable for mobile phones and other low-cost consumer grade devices since it does not require special hardware support. We also describe an experiment to measure sensor synchronization performance using a simple calibration rig. Our initial results show that the estimated timestamps provide a stable synchronization and a clear improvement in synchronization accuracy compared to the direct method of using sample arrival times as timestamps.
[sensor synchronization, AR application, Estimation, stable synchronization, augmented reality, Calibration, Synchronization, synchronisation, cameras, Accuracy, sensors, low cost consumer grade device, mobile phone, Temporal matching, computer vision, Cameras, Universal Serial Bus, Sensor synchronization, hardware affect, Clocks, mobile handsets, host processor]
Light-weight marker hiding for augmented reality
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
In augmented reality, marker-based tracking is the most common method for camera pose estimation. Most of the markers are black and white patterns that are visually obtrusive, but they can be hidden from the video using image inpainting methods. In this paper, we present a computationally efficient approach to achieve this. We use a high-resolution hiding texture, which is captured and generated only once. To capture continuous changes in illumination, reflections and exposure, we also compute a very low-resolution texture at each frame. The coarse and fine textures are combined to obtain a detailed hiding texture which reacts to changing conditions and runs efficiently in mobile phone environments.
[Real time systems, H.5 [Information Interfaces and Presentation, augmented reality, Mobile handsets, Image color analysis, feature extraction, marker based tracking, Image processing and computer vision, Lighting, pose estimation, motion estimation, video signal processing, image resolution, Image resolution, image inpainting method, light weight marker hiding, black white pattern, mobile environment, Augmented reality, image texture, image sensors, computer graphics, camera pose estimation, Information Interfaces and Presentation, data encapsulation, Pixel, high resolution hiding texture]
An immersive e-learning system providing virtual experience
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
This paper introduces immersive e-learning system which provides vivid learning experience using augmented reality(AR) technology. This system gives illusion that participants feel as if they are in foreign environment by synthesizing images of participants, virtual environment, foreign-language speakers in real-time. Furthermore, surrounding virtual environment reacts to the behavior of each participant including student, local teacher, remote teacher. The system has been installed along with 10 scenarios at 14 public elementary schools and conducted during regular class time. This paper presents our motivations for the system development, a detailed design, and its contents.
[virtual experience, software development, Gesture recognition, augmented reality, immersive e-learning system, cultural heritage, distributed and collaborative MR/AR, foreign-language speakers, Image segmentation, real-time systems, Virtual reality, virtual environment, or education and training (primary keyword), MR/AR for art, Cameras, Rendering (computer graphics), Software, Hardware, software engineering, computer aided instruction, educational institutions, public elementary school, illusion system]
Designing and comparing two-handed gestures to confirm links between user controlled objects
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
Systems using two-handed spatial manipulation techniques also require strategies to enable system control tasks. These strategies make it possible to interact with the system comfortably while controlling two hand-held objects simultaneously. The Augmented Chemical Reactions project makes intense use of such two-handed interaction tasks. Users control virtual molecules and subsets that are registered to physical markers and try to combine those by selecting and then confirming a specific bond. When a desired bond has been selected, the user needs a way to confirm that bond without letting an atom go out of position. We developed and investigated two separate methods of confirming a selected bond when both hands are already doing a two-handed symmetric interaction task. The first method is a waiting method and the second method is a back&amp;forth motion gesture. We evaluated the two methods in a user study, showing that the first technique, holding still, outperforms the other technique.
[Visualization, symmetric interaction task, two handed gesture, Jitter, augmented reality, user controlled object, Time measurement, spatial manipulation technique, Chemicals, Augmented reality, chemical reactions, gesture recognition, Three dimensional displays, augmented chemical reaction, Interviews]
Augmented reality for board games
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
We introduce a new type of Augmented Reality games: By using a simple webcam and Computer Vision techniques, we turn a standard real game board pawns into an AR game. We use these objects as a tangible interface, and augment them with visual effects. The game logic can be performed automatically by the computer. This results in a better immersion compared to the original board game alone and provides a different experience than a video game. We demonstrate our approach on Monopoly&#x2122;, but it is very generic and could easily be adapted to any other board game.
[Computers, Computer vision, graphical user interfaces, augmented reality, tangible interface, Augmented reality, webcam, board game, computer games, Games, Detectors, computer vision, Cameras, Three dimensional displays]
PoP-EYE environment: Mixed Reality using 3D Photo Collections
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
The paper is talking about PoP-EYE, a mixed reality environment, using 3D photo collections. This augmentation enhance the quality of the virtual (artificial) object hence providing realistic image-based rendering.
[image processing, Occlusion, 3D photo collection, 3D Photo Collection, augmented reality, artificial augmentation, Image-based lighting, realistic images, virtual object, Light sources, Image segmentation, realistic image based rendering, Lighting, PoP-EYE environment, Virtual reality, Cameras, Rendering (computer graphics), mixed reality environment, Image Quality, Illumination, Image-based rendering, Three dimensional displays, rendering (computer graphics), Mixed Reality]
Video stabilization to a global 3D frame of reference by fusing orientation sensor and image alignment data
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
Estimating the 3D orientation of the camera in a video sequence within a global frame of reference is useful for video stabilization when displaying the video in a virtual 3D environment, as well as for accurate navigation and other applications. This task requires the input of orientation sensors attached to the camera to provide absolute 3D orientation in a geographical frame of reference. However, high-frequency noise in the sensor readings makes it impossible to achieve accurate orientation estimates required for visually stable presentation of video sequences that were acquired with a camera subject to jitter, such as a handheld camera or a vehicle mounted camera. On the other hand, image alignment has proven successful for image stabilization, providing accurate frame-to-frame orientation estimates but drifting over time due to error and bias accumulation and lacking absolute orientation. In this paper we propose a practical method for generating high accuracy estimates of the 3D orientation of the camera within a global frame of reference by fusing orientation estimates from an efficient image-based alignment method, and the estimates from an orientation sensor, overcoming the limitations of the component methods.
[image fusion, video stabilization, Jitter, geographical frame, global 3D frame, Robot sensing systems, Three dimensional displays, 3D orientation estimation, vehicle mounted camera, video signal processing, image sequences, orientation sensor fusion, high frequency noise, virtual 3D environment, Virtual environment, image based alignment method, Video sequences, video cameras, video sequence, image stabilization, Augmented reality, jitter, handheld camera, frame-to-frame orientation, Cameras, solid modelling]
Augmented telepresence using autopilot airship and omni-directional camera
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
This study is concerned with a large-scale telepresence system based on remote control of mobile robot or aerial vehicle. The proposed system provides a user with not only view of remote site but also related information by AR technique. Such systems are referred to as augmented telepresence in this paper. Aerial imagery can capture a wider area at once than image capturing from the ground. However, it is difficult for a user to change position and direction of viewpoint freely because of the difficulty in remote control and limitation of hardware. To overcome these problems, the proposed system uses an autopilot airship to support changing user's viewpoint and employs an omni-directional camera for changing viewing direction easily. This paper describes hardware configuration for aerial imagery, an approach for overlaying virtual objects, and automatic control of the airship, as well as experimental results using a prototype system.
[Real time systems, Parameter estimation, AR technique, telerobotics, H.5.1 [Information Interfaces and Presentation, augmented telepresence, augmented reality, mobile robot, aircraft control, mobile robots, aerial vehicle, Vehicles, cameras, Prototypes, Gyroscopes, control engineering computing, autopilot airship, airships, remote control, Artificial Intelligence, omnidirectional camera, large-scale telepresence system, Global Positioning System, Cameras, aerial imagery, Information Interfaces and Presentation, airship automatic control]
Time-domain augmented reality based on locally adaptive video sampling
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
We propose a new approach for augmented reality, in which real-world scene images are augmented with video fragments manipulated in the time domain. The proposed system aims to display slow-motion video sequences of moving objects instantly without accumulated time lag so that a user can recognize and observe high-speed motion on the spot. Images from a high-speed camera are analyzed to detect regions with important visual features, which are overlaid on a normal-speed video sequence.
[Real time systems, locally adaptive video sampling, slow motion video sequences, Instruction sets, Video sequences, real world scene images, Time domain analysis, Information Systems, augmented reality, Computing Methodologies, time domain augmented reality, video fragments, image sensors, high speed motion, image sampling, I.4.9 [Computing Methodologies, Streaming media, high speed camera, Cameras, Pixel, image sequences]
3DOF tracking accuracy improvement for outdoor Augmented Reality
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
Outdoor Augmented Reality (AR) gained popularity recently due to its potential for location based mobile services. However, most commercially available Global Positioning Systems (GPS), except for the expensive high-end models, do not provide accurate location information that is enough to be used for displaying practically meaningful location based information. In this paper, we present a computer vision based method for improving user's two dimensional location and one-dimensional orientation, the initial values of which are obtained from a GPS and a digital compass. Our method utilizes corner positions of buildings in the map and the vertical edges of the buildings in the captured images. We applied anisotropic diffusion in order to filter noise and preserve edges, and dual vertical edge filters on gray and saturation images. Our method is suitable for mobile services in urban environments where tall buildings degrade GPS signals. In average, our method improved 15.0 meters in position and 2.2 degrees in orientation.
[augmented reality, anisotropic diffusion, two dimensional location, computer vision based method, Accuracy, Anisotropic magnetoresistance, saturation images, GPS signals, LBS, outdoor augmented reality, one-dimensional orientation, gray images, Image edge detection, Buildings, filtering theory, digital compass, Compass, image denoising, location based mobile services, Augmented reality, Global Positioning System, computer vision, outdoor tracking, 3DOF tracking accuracy improvement, global positioning systems, dual vertical edge filters]
Validating Spatial Augmented Reality for interactive rapid prototyping
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
This paper investigates the use of Spatial Augmented Reality in the prototyping of new human-machine interfaces, such as control panels or car dashboards. The prototyping system uses projectors to present the visual appearance of controls onto a mock-up of a product. Finger tracking is employed to allow real-time interactions with the controls. This technology can be used to quickly and inexpensively create and evaluate interface prototypes for devices. In the past, evaluating a prototype involved constructing a physical model of the device with working components such as buttons. We have conducted a user study to compare these two methods of prototyping and to validate the use of spatial augmented reality for rapid iterative interface prototyping. Participants of the study were required to press pairs of buttons in sequence and interaction times were measured. The results indicate that while slower, users can interact naturally with projected control panels.
[Industrial Design, Visualization, real-time interactions, augmented reality, interactive rapid prototyping, Spatial Augmented Reality, man-machine systems, human-machine interfaces, Augmented reality, Automotive engineering, finger tracking, Rapid Prototyping, Presses, Prototypes, Three dimensional displays, Analysis of variance, spatial augmented reality, rapid prototyping (industrial)]
North-centred orientation tracking on mobile phones
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
Magnetic compasses and accelerometers provide absolute orientation measurements within the earth's reference frame. However, sensor output typically suffers from jitter and external disturbances. Conversely, visual tracking provides more stable orientation estimation relative to an unknown initial orientation rather than to true north. We propose a 3-degree-of-freedom orientation tracking approach combining the accuracy and stability of vision tracking with the absolute orientation from inertial and magnetic sensors by estimating the offset between the initial orientation of the vision tracker and true north. We demonstrate that the approach improves absolute orientation estimation on a mobile phone device.
[Visualization, magnetic variables measurement, Tracking, Magnetometers, visual tracking, augmented reality, mobility management (mobile radio), compasses, tracking, Accuracy, mobile computing, magnetic compass, accelerometer, Image Processing and Computer Vision, magnetic sensors, vision tracker, north centred orientation tracking, orientation estimation, H.5.1 [Information Systems, Estimation, magnetic sensor, inertial sensors, Information Systems, Compass, Augmented reality, jitter, mobile phone, computer vision, accelerometers, mobile handsets]
Large area indoor tracking for industrial augmented reality
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
A precise tracking with minimal setup times, minimal changes to the environment and acceptable costs, satisfying industrial demands in large factory buildings is still a challenging task for augmented reality (AR) applications. We present a system to determine the pose for monitor based AR systems in large indoor environments, e.g. 200 &#x00D7; 200 meters and more. An infrared laser detects retroreflective targets and computes a 2D position and orientation based on the information of a preprocessed map of the targets. Based on this information the 6D pose of a video camera attached to a servo motor, that is further mounted on a mobile cart is obtained by identifying the transformation between the laser scanner and the several adjustable views of the camera through a calibration method. The adjustable steps of the servo motor are limited to a discrete number of steps to limit the calibration effort. The positional accuracy of the system is estimated by error propagation and presented.
[industrial augmented reality, Target tracking, indoor tracking, infrared detectors, video cameras, augmented reality, video camera, Production facilities, calibration method, retroreflective target, I.4.8 [Image Processing and Computer Vision, error propagation, Image Processing and Computer Vision, Cameras, Laser modes, Three dimensional displays, 2D position, Information Interfaces and Presentation, laser scanner, Servomotors, calibration, infrared laser]
Camera pose navigation using Augmented Reality
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
We propose an Augmented Reality (AR) system that helps users take a picture from a designated pose, such as the position and camera angle of an earlier photo. Repeat photography is frequently used to observe and document changes in an object. Our system uses AR technology to estimate camera poses in real time. When a user takes a photo, the camera pose is saved as a &#x201C;view bookmark&#x201D;. To support a user in taking a repeat photo, two simple graphics are rendered in an AR viewer on the camera's screen to guide the user to this bookmarked view. The system then uses image adjustment techniques to create an image based on the user's repeat photo that is even closer to the original.
[Solid modeling, Navigation, Augmented Reality, augmented reality, Camera Pose Navigation, Augmented reality, Photography, image sensors, Repeat Photography, camera pose estimation, pose estimation, Rephotography, Cameras, Three dimensional displays, camera pose navigation, photography, image adjustment technique]
SnapAR: Storing snapshots for quick viewpoint switching in hand-held augmented reality
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
Many tasks require a user to move between various locations within an environment to get different perspectives. This can take significant time and effort, especially when the user must switch among those viewpoints repeatedly. We explore augmented reality interaction techniques that involve taking still pictures of a physical scene using a tracked hand-held magic lens and seamlessly switching between augmenting either the live view or one of the still views, without needing to physically revisit the snapshot locations. We describe our optical-marker-tracking-based implementation and how we represent and switch among snapshots. To determine the effectiveness of our techniques, we developed a test application that lets its user view physical and virtual objects from different viewpoints.
[photographic lenses, Visualization, Optical switches, SnapAR, optical tracking, augmented reality interaction technique, augmented reality, tracked handheld magic lens, quick viewpoint switching, handheld augmented reality, snapshot location, Augmented reality, optical marker tracking based implementation, virtual object, viewpoint switching, computer vision, Cameras, Three dimensional displays, Arrays, still picture, physical scene]
A multi-sensor platform for wide-area tracking
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
Indoor tracking scenarios still face challenges in providing continuous tracking support in wide-area workplaces. This is especially the case in Augmented Reality since such augmentations generally require exact full 6DOF pose measurements in order to continuously display 3D graphics from user-related view points. Many single sensor systems have been explored but only few of them have the capability to track reliably in wide-area environments. We introduce a mobile multi-sensor platform to overcome the shortcomings of single sensor systems. The platform is equipped with a detachable optical camera and a rigidly mounted odometric measurement system providing relative positions and orientations with respect to the ground plane. The camera is used for marker-based as well as for marker-less (feature-based) inside-out tracking as part of a hybrid approach. We explain the principle tracking technologies in our competitive/cooperative fusion approach and show possible enhancements to further developments. This inside-out approach scales well with increasing tracking range, as opposed to stationary outside-in tracking.
[indoor tracking, wireless sensor networks, wide area tracking, odometric measurement system, H.5.1 [Information Interfaces and Presentation, Mobile communication, augmented reality, sensor fusion, tracking, cameras, Computer Graphics, Accuracy, mobile computing, optical camera, distance measurement, Robot sensing systems, Three dimensional displays, mobile multisensor platform, Augmented reality, Cameras, 3D graphics, Information Interfaces and Presentation, Optical sensors, principle tracking technology, solid modelling]
Floyd-Warshall all-pair shortest path for accurate multi-marker calibration
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
We propose a novel method to compute the poses of randomly positioned square markers in one world coordinate frame from multiple camera views, by taking the predicted accuracy of the camera pose estimation for each marker into account. The problem of computing the best closed-form solution of the world pose of each marker is modeled as all-pair shortest path problem in graph theory. The computed world poses are further optimized by minimizing the geometric distances in images. Experimental results show that incorporating the predicted accuracy of the pose estimation for each marker yields constant high quality calibration results independent of the order of image sequences compared to cases when this knowledge is not used.
[Tracking, graph theory, Estimation, Closed-form solution, multi-maker calibration, Calibration, Image sequences, visual marker based tracking, Floyd-Warshall, Accuracy, all-pair shortest path, pose estimation, Cameras, square markers, calibration, image sequences]
MTMR: A conceptual interior design framework integrating Mixed Reality with the Multi-Touch tabletop interface
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
This paper introduces a conceptual interior design framework - Multi-Touch Mixed Reality (MTMR), which integrates mixed reality with the multi-touch tabletop interface, to provide an intuitive and efficient interface for collaborative design and an augmented 3D view to users at the same time. Under this framework, multiple designers can carry out design work simultaneously on the top view displayed on the tabletop, while live video of the ongoing design work is captured and augmented by overlaying virtual 3D furniture models to their 2D virtual counterparts, and shown on a vertical screen in front of the tabletop. Meanwhile, the remote client's camera view of the physical room is augmented with the interior design layout in real time, that is, as the designers place, move, and modify the virtual furniture models on the tabletop, the client sees the corresponding life-size 3D virtual furniture models residing, moving, and changing in the physical room through the camera view on his/her screen. By adopting MTMR, which we argue may also apply to other kinds of collaborative work, the designers can expect a good working experience in terms of naturalness and intuitiveness, while the client can be involved in the design process and view the design result without moving around heavy furniture. By presenting MTMR, we hope to provide reliable and precise freehand interactions to mixed reality systems, with multi-touch inputs on tabletop interfaces.
[Solid modeling, augmented 3D view, conceptual interior design framework, augmented reality, user interfaces, virtual furniture models, freehand interactions, cameras, virtual 3D furniture models, multi-touch tabletop interfaces, Layout, 2D virtual counterparts, Mixed reality, Virtual reality, groupware, 3D virtual furniture models, Streaming media, Cameras, Collaborative work, Three dimensional displays, multitouch mixed reality, collaborative design, multitouch tabletop interface]
AR-based visibility evaluation for preserving landscapes of historical buildings
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
Building tall structures behind an aesthetic and historical building tends to destroy the good landscape. To avoid such situations, public agencies must regulate height of buildings and other structures near the landscape target. In order to check the visibility of portions of high, future structures, in this research, a new method using Augmented Reality (AR) was proposed. In this method, a number of virtual rectangular objects with a scale are located on the grid of 3D geographical model. And then, the virtual rulers are shown in an overlapping manner with the actual landscape from multiple viewpoints using the AR technology. The user measures the maximum skyline-preserving height for each rectangular object at a grid point. Using the measured data, the government or public agencies can establish appropriate height regulations for all surrounding areas of the target structures. To verify the proposed method, a system was developed deploying AR Toolkit and was applied to a scenic building. The performance of the system was checked and then, the errors of the obtained data were evaluated. In conclusion, the proposed method was evaluated feasible and effective.
[Solid modeling, Augmented Reality, computational geometry, augmented reality, building height regulation, AR toolkit, building, Accuracy, skyline preserving height, virtual rulers, Prototypes, data visualisation, 3D geographical model, Three dimensional displays, Buildings, structural engineering computing, invisible depth, Augmented reality, preserving landscape, Cameras, scenic building, AR based visibility evaluation, historical building, landscape, solid modelling]
Various tangible devices suitable for mixed reality interactions
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
In this paper, we present various novel tangible devices suitable for interactions in a mixed reality (MR) environment. They are aimed at making the best use of the features of MR, which allows users to touch or handle both virtual and physical objects. Furthermore, we consider usability and intuitiveness as important characteristics of the interface, and thus designed our devices to imitate traditional tools and help users understand their use.
[virtual reality, Shape, tabletop, mixed reality interactions, user interfaces, tool, user interface, Painting, tangible device, Image color analysis, physical objects, tangible devices, Mixed reality, Virtual reality, User interfaces, interaction, virtual objects, Pixel]
Camera motion tracking in a dynamic scene
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
To insert a virtual object into a real image, the position of the object must appear seamlessly as the camera moves. This requires camera tracking with estimations of all internal and external parameters in each frame with an adequate degree of stability to ensure negligible visible drift between the real and virtual elements. In the post production of film, matchmoving software based on SfM is typically used in the camera tracking process. However, most of this type of software fails when attempting to track the camera in a dynamic scene in which a moving foreground object such as a real actor occupies a large part of the background. Therefore, this study proposes a camera tracking system that uses an auxiliary camera to estimate the motion of the main shooting camera and 3D position of background features in a dynamic scene. A novel reconstruction and connection method was developed for feature tracks that are occluded by a foreground object. Experimentation with a 2K sequence demonstrated the feasibility of the proposed method.
[Computer vision, Motion, Camera calibration, Tracking, camera tracking, Calibration, image reconstruction, position estimation, image sequence, Image reconstruction, dynamic scene, cameras, Dynamics, target tracking, motion estimation, Cameras, Three dimensional displays, image sequences]
ARCrowd-a tangible interface for interactive crowd simulation
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
Manipulating a large virtual crowd in an interactive virtual reality environment is a challenging task due to the limitations of the traditional user interface. To address this problem, a tangible interface based on augmented reality (AR) is introduced. Through the tangible AR interface, the user could manipulate the virtual characters directly, or control the crowd behaviors with markers. These markers are used to adjust the environment factors, the decision-making processes of virtual crowds, and their reactions. The AR interface provides more intuitive means of control for the users, promoting the efficiency of human-machine interface.
[Solid modeling, virtual reality, Computational modeling, graphical user interfaces, virtual crowd, crowd behaviors control, Switches, tangible user interface, augmented reality, man-machine systems, tangible interface, Augmented reality, virtual characters, human-machine interface, Fires, decision making, User interfaces, Three dimensional displays]
Origami recognition system using natural feature tracking
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
This paper introduces a system that can recognize different type of paper-folding by users. The system allows users to register and use their desired paper in the interaction, and detect the folding by using Speed Up Robust Feature (SURF) algorithm. The paper also describes a paper-based tower defense game which has been developed as a proof of concept of our method. This method can be considered as the initial step for seamlessly migrating meaningful traditional art of origami into the digital world as a part of the interactive media.
[speed up robust feature algorithm, paper folding, Poles and towers, interactive media, Electronic mail, Registers, paper-based tower defense game, origami recognition system, Games, Detectors, interactive systems, natural feature tracking, Feature extraction, Robustness]
Tutorials and workshops
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
Provides an abstract for each of the presentations and a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.
[]
An intermediate report of TrakMark WG &#x223C;international voluntary activities on establishing benchmark test schemes for AR/MR geometric registration and tracking methods
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
In the study of AR/MR field, tracking and geometric registration methods are very important topics that are actively discussed. Especially, the study on tracking is flourishing and many algorithms are being proposed every year. With this trend in mind, we, the TrakMark WG, had proposed benchmark test schemes for geometric registration and tracking in AR/MR at ISMAR 2009 [1]. This paper is an intermediate report of the TrakMark WG, which describes its activities and the first proposal on benchmarking image sequences.
[Tracking, Benchmark, Geometric Registration]
Feature tracking and object recognition on a hand-held
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
We demonstrate a visual recognition system operating on a hand-held device, with the help of an efficient and robust feature tracking and an object recognition mechanism that can be used for interactive mobile applications. In our recognition system, corner features are detected from captured video frames in a multi-scale image pyramid, and are tracked between consecutive frames efficiently. In order to perform object recognition, local descriptors are calculated on the tracked features, and quantized using a vocabulary tree. For each object, a bag-of-words model is learned from multiple views. The learned objects are recognized by computing the ranking score for the set of features in a single video frame. Our feature tracking algorithm and local descriptors are different than the Lucas-Kanade algorithm in image pyramid or the SIFT descriptor, however improving the efficiency and accuracy. For our implementation on a mobile phone, we used an iPhone 3GS with a 600MHz ARM chip CPU. The video frame is captured from a camera preview screen at a rate of 15 frames per second using the public API. The task of object recognition on a mobile phone runs at around 7 frames per second, including the feature tracking and descriptor calculation.
[object recognition, feature tracking, vocabulary tree, visual recognition system, tracking, SIFT descriptor, vocabulary, feature extraction, handheld device, descriptor calculation, captured video frame, corner feature, video signal processing, 600MHz ARM chip CPU, multiscale image pyramid, interactive mobile application, iPhone 3GS, video cameras, public API, learned object, Lucas-Kanade algorithm, mobile phone, computer vision, bag of words model, image recognition, mobile handsets]
Origami recognition system using natural feature tracking
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
In this demonstration we introduce a system that allows users to register and use their desired paper for the interaction, and recognize the folding during the interaction in real time. This method can be considered as an initial step for seamlessly migrating meaningful traditional art of origami into the digital world and as part of the interactive media.
[origami recognition system, art, feature extraction, interactive media, computer vision, interactive systems, natural feature tracking, real time system, folding process, feature recognition, digital world, image recognition]
Breast cancer palpation system using haptic augmented reality
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
In haptic augmented reality (AR), a stimulus from a real object is augmented by a synthetic haptic signal. We previously developed a haptic AR system wherein the stiffness of a real object can be augmented with the aid of a haptic interface. This demonstration presents a case study of this technology for medical training of breast cancer palpation. A real breast model made of soft silicone is augmented with a virtual tumor rendered inside. Haptic stimuli for the virtual tumor are generated based on a contact dynamics model identified via real measurements. With our palpation system, a user can experience excellent realism comparable to that of a real breast mock-up containing a real tumor.
[breast cancer palpation system, virtual tumor rendered, biomedical education, synthetic haptic signal, haptic interfaces, medical training, augmented reality, haptic interface, virtual tumor, cancer, rendering (computer graphics), tumours, haptic augmented reality]
Demo for differential Instant Radiosity for Mixed Reality
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
This laboratory demo is a showcase for the research results published in our ISMAR 2010 paper [3], where we describe a method to simulate the mutual shading effects between virtual and real objects in Mixed Reality applications. The aim is to provide a plausible illusion so that virtual objects seem to be really there. It combines Instant Radiosity [2] with Differential Rendering [1] to a method suitable for MR applications. The demo consists of two scenarios, a simple one to focus on mutual shading effects and an MR game based on LEGO®.
[virtual reality, mixed reality, differential rendering, rendering (computer graphics), differential instant radiosity, virtual object, mutual shading effect]
A Web Service Platform dedicated to building mixed reality solutions
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
While many attempts have been done towards creating mixed reality platforms for mobile client devices, there have not been any significant efforts at the server/infrastructure side. We demonstrate our Mixed Reality Web Service Platform (MRS-WS) dedicated to enabling rapid creation of mixed reality solutions, those being either desktop or mobile. Focusing on common interfaces and functions across user generated and commercial geo-content, we provide an appealing developer offering, which we are currently evaluating via a closed set of university partners. Our plan is to gradually expand the developer API access to more partners, before deciding if it is ready for fully public developer access.
[virtual reality, application program interfaces, Web services, MRS-WS, commercial geocontent, structural engineering computing, building mixed reality solution, user generated geocontent, mobile client device, API, mixed reality Web service platform]
AR Shooter: An augmented reality shooting game system
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
This paper presents the features and functionalities of AR Shooter, an augmented reality shooting game system which is based on infrared marker tracking. The proposed system consists of two parts: a gun with video cameras and infrared markers composed of LED. When the gun aims on the infrared markers, some monsters will appear on the LCD equipped on the gun. Then, the user can open fire and shoot at the monsters. Figure 1 shows the diagram of the proposed system.
[gun, Infrared Marker, optical tracking, Augmented Reality, liquid crystal displays, video cameras, augmented reality, video camera, LED, infrared marker tracking, augmented reality shooting game system, LCD, light emitting diodes, Shooting Game, computer games, AR shooter]
Foldable augmented maps
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
This demonstration presents folded surface detection and tracking for augmented maps. We model the folded surface as multiple planes. To detect a folded surface, plane detection is iteratively applied to 2D correspondences between an input image and a reference plane. In order to compute the exact folding line from the detected planes, the intersection line of the planes is computed from their positional relationship. After the detection is done, each plane is individually tracked by frame-by-frame descriptor update. For a natural augmentation on the folded surface, we overlay virtual geographic data on each detected plane.
[]
Smart Glasses: An open environment for AR apps
2010 IEEE International Symposium on Mixed and Augmented Reality
None
2010
We present an architecture [fig. 1] and runtime environment for mobile Augmented Reality applications. The architecture is based on a plugin-concept on the device, a set of basic functionalities available for all apps and a cloud-oriented processing approach. As a first running sample app, we show a face recognition service running on a mobile phone, conventional wearable displays and upcoming see-through-goggles. We invite interested 3rd parties to try out the environment, face recognition app and platform.
[goggles, eye protection, mobile phone, face recognition, augmented reality, cloud oriented processing, Internet, mobile augmented reality, mobile handsets]
Art exhibition committee
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
Provides a listing of current committee members.
[]
Conference committee
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
Provides a listing of current committee members.
[]
ISMAR demo chairs
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
We are very happy to present the ISMAR 2011 Demonstrations Program. Demos embody "hello world" - the slogan of this year's ISMAR conference - and have a special place in the consciousness of the community. They provide us with our first glimpse of new ideas, a venue for directly interacting with implemented versions of novel concepts, and an opportunity to engage with the people of this community in the context of their work.
[]
Symposium general chairs
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
Welcome to the Tenth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR 2011).
[]
Multi modal sensory human communication in the Internet society
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
This talk outlines new facilities within human media spaces supporting embodied interaction between humans, animals, and computation both socially and physically, with the aim of novel interactive communication and entertainment. We aim to develop new types of human communications and entertainment environments using all the senses, including touch, taste, and smell, which can increase support for multi-person multi-modal interaction and remote presence. In this talk, we present an alternative ubiquitous computing environment based on an integrated design of real and virtual worlds. We discuss some different research prototype systems for interactive communication, culture, and play.
[]
Science &amp; technology program chairs
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
The S&T Program Chairs are delighted to welcome you to ISMAR 2011, the 10th symposium on Mixed and Augmented Reality! This year's symposium continues a long tradition of ISMAR meetings, a series that itself followed a related series of IWAR, ISMR, and ISAR meetings. This year's Science and Technology (S&T) track comprises a mix of highly selective mixed and augmented reality research and related work. Specifically, in the S&T program this year you will find 26 papers, 27 posters, as well as a stimulating mixture of keynote talks, demonstrations, tutorials, workshops and the tracking competition. All of these elements of the program are the result of dedicated hard work by members of various conference committees and additional volunteers, and we would like to thank all of them for their generous efforts.
[]
S&amp;T program committee
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
Provides a listing of current committee members.
[]
Steering committee
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
Provides a listing of current committee members.
[]
Building your vision with Qualcomm's Mobile Augmented Reality (AR) platform: AR on mobile devices
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
Academic/Research Overview of Mobile Augmented Reality Introduction to the Qualcomm AR Platform &#x2014; Features and Usage Scenarios Developing Mobile AR Applications using Qualcomm's Unity Extension Introduction to Qualcomm's QCAR SDK Native API Cross Platform Development with the Native QCAR SDK
[]
Bi-directional OLED microdisplay for see-through HMD
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
Within the research project &#x201C;iStar &#x2014; Interactive See-Through Augmented-Reality Display&#x201D; the Fraunhofer Gesellschaft developed Augmented Reality goggles comprising a VGA OLED microdisplay with embedded image sensor aimed on gaze-control and see-through head-mounted optics. The active area of the bi-directional microdisplay consists of nested display and image sensor (embedded camera) pixels surrounded by a second image sensor (frame camera) as well as driving and control circuitry (c.f. Table). The display and image sensor systems are electrically independent of one another, simply interacting via synchronization signals. iStar also includes a developer kit integrating Eye-Tracking software, AR system and application demonstrators. The topic of high-contrast See-Through HMDs still forms an important research topic within the AR community, thereby iStar not only offers a light-weight display solution but it also integrates camera sensors into the display to support Eye-Tracking. With the presented demonstrators the tutorial attendees can evaluate the possibilities and the maturity of the developed technologies. The attendees will get an overview to requirements, solution possibilities and open research topics in the field of microdisplays, optics and software development for the realization of interactive see-Through HMDs. Feedback to learned objectives will be evaluated in questioners.
[]
Psychological keys to success in MAR systems
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
Imagine a significant improvement in understanding user response to your application. This tutorial examines psychological considerations in developing and deploying MAR applications to help improve your understanding of key psychological factors in technological development. Successful MAR applications will be those that take advantage of the inherent way our brains process information in this new environment. This panel provides a one half-day session where media-focused psychology is explored at a basic level with numerous illustrations, examples, and techniques to ensure that the MAR application has a strong technology-mind interface. The purpose of this tutorial is to help define the role psychological research and theories play in the successful development and deployment of commercial MAR applications. It contains four major sections of study for session participants. The first three are: 1) cognitive science, 2) psychological design, and 3) narrative transportation theory for applications. There is considerable research in user experience (UX), and there are many lessons to be learned as MAR technology moves into the mainstream. Each section deconstructs core foundational components in the understanding of user-experience (UX) with regard to new interactive technologies. The final section ties the first three sections together and provides practical tips and techniques to help MAR researchers and practitioners take the psychological sciences into their labs, design, and development shops. Throughout this tutorial, attendees will view various examples of each of the topics, as well as be exposed to a variety of similarities and differences in cognitive interpretation of object and environment design. By the end of this session, attendees will have a better understanding of how to design for diverse end-users, as well as how to capture and hold end-user attention for the ultimate purpose of engagement and immersion.
[]
Workshops &amp; tutorials committee
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
Provides a listing of current committee members.
[]
Enabling large-scale outdoor mixed reality and augmented reality
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
While there is significant recent progress in technologies supporting augmented reality for small indoor environments, there is still much work to be done for large outdoor environments. This workshop focuses primarily on research that enables high-quality outdoor Mixed Reality (MR) and Augmented Reality (AR) applications. These research topics include, but are not restricted to: &#x2014; 3D geo-referenced data (images, point clouds, and models) &#x2014; Algorithms for object recognition from large databases of geo-referenced data &#x2014; Algorithms for object tracking in outdoor environment &#x2014; Multi-cue fusion to achieve improved performance of object detection and tracking &#x2014; Novel representation schemes to facilitate large-scale content distribution &#x2014; 3D reasoning to support intelligent augmentation &#x2014; Novel and improved mobile capabilities for data capture (device sensors), processing, and display &#x2014; Applications, experiences, and user interface techniques. The workshop will also showcase existing prototypes of applications enabled by these technologies: mirror worlds, high-fidelity virtual environments, applications of panoramic imagery, and user studies relating to these media types. This workshop aims to bring together academic and industrial researchers and to foster discussion amongst participants on the current state of the art and future directions for technologies that enable large-scale outdoor MR and AR applications. The workshop will start with a session in which position statements and overviews of the state of the art are presented. In the afternoon, we will follow up with discussion sessions and a short closing session.
[]
International workshop on AR/MR registration, tracking and benchmarking (TrakMark2011)
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
In the research fields of Augmented Reality (AR) and Mixed Reality (MR), tracking and registration methods are still one of the most important topics. The tracking research field is highly active, and numerous methods appear on a regular basis. The TrakMark working group (WG) was established 2009 to create a benchmark test that permits objective and accurate evaluation of the tracking methods. This year, the workshop will cover a wide range of topics concerning AR/MR registration, tracking and benchmarking. Key areas include, but are not limited to: &#x2014; Vision-based registration, camera localization &#x2014; Visual SLAM, structure from motion, camera calibration, sensor fusion &#x2014; Natural feature tracking, object tracking, feature detection, feature description &#x2014; Comparison of methods, evaluation of methods, suggestion of new benchmarking scheme &#x2014; Survey of tracking papers.
[]
Visualization in mixed reality environments
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
Mixed and Augmented Reality displays extend the user's perception with computer generated information. This information is typically registered in three-dimensional space, and related to objects and places in the physical world. While individual annotation of objects has historically been a topic of MR research, visualization incorporating multiple related data points or models provides a variety of new research challenges in systems and techniques. For example, photorealistic augmented reality visualization presents data by adapting additionally presented imagery to the real world condition while illustrative visualization techniques aim at enhancing the understanding of augmented scenarios by carefully combining and mediating real and virtual data. Situated visualization techniques present virtual representations of data in relevant locations in the physical scene. A challenge in many of these techniques is the need to correctly communicate the relationships between physical imagery and virtual data.
[]
Indoor positioning and navigation for mobile AR
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
The researchers and developers of mobile AR platforms need to use a common platform for developing experiences regardless of the surroundings of the user. In order to expand the use of AR both indoor and outdoor with and without computer vision techniques, the breadth of options available for positioning users and points of interest needs to expand. Separately, the experts in indoor positioning and navigation are generally not as familiar with AR use scenarios as they are with other domains. Together, positioning and navigation experts, and mobile AR experts, will discuss: &#x2014; What are the indoor positioning and navigation systems best suited for mobile AR? &#x2014; What studies are underway or need to be conducted in order to advance this field?
[]
Authoring solutions for Augmented Reality
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
The motivation of this workshop is to discuss future directions of content authoring in the field of Augmented Reality, as well as to discuss the current state of art on content creation and asset assembly. The workshop will comprise of a paper session where papers, late-breaking results and overviews over state-of-the-art in content authoring for AR are presented. In the afternoon, we will follow up with a discussion on different topics ranging from AR asset creation to content distribution and a closing session. Our goal is to collect ideas and thoughts of research about desired approaches for authoring content for AR, as well as review current and future needs to achieve a high quality content and at the same time scalable approaches for AR.
[]
Robust planar target tracking and pose estimation from a single concavity
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
In this paper we introduce a novel real-time method to track weakly textured planar objects and to simultaneously estimate their 3D pose. The basic idea is to adapt the classic tracking-by-detection approach, which seeks for the object to be tracked independently in each frame, for tracking non-textured objects. In order to robustly estimate the 3D pose of such objects in each frame, we have to tackle three demanding problems. First, we need to find a stable representation of the object which is discriminable against the background and highly repetitive. Second, we have to robustly relocate this representation in every frame, also during considerable viewpoint changes. Finally, we have to estimate the pose from a single, closed object contour. Of course, all demands shall be accommodated at low computational costs and in real-time. To attack the above mentioned problems, we propose to exploit the properties of Maximally Stable Extremal Regions (MSERs) for detecting the required contours in an efficient manner and to apply random ferns as efficient and robust classifier for tracking. To estimate the 3D pose, we construct a perspectively invariant frame on the closed contour which is intrinsically provided by the extracted MSER. In our experiments we obtain robust tracking results with accurate poses on various challenging image sequences at a single requirement: One MSER used for tracking has to have at least one concavity that sufficiently deviates from its convex hull.
[Training, Shape, Image edge detection, Estimation, Detectors, Robustness, Three dimensional displays]
Toward augmenting everything: Detecting and tracking geometrical features on planar objects
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
This paper presents an approach for detecting and tracking various types of planar objects with geometrical features. We combine traditional keypoint detectors with Locally Likely Arrangement Hashing (LLAH) [21] for geometrical feature based keypoint matching. Because the stability of keypoint extraction affects the accuracy of the keypoint matching, we set the criteria of keypoint selection on keypoint response and the distance between keypoints. In order to produce robustness to scale changes, we build a non-uniform image pyramid according to keypoint distribution at each scale. In the experiments, we evaluate the applicability of traditional keypoint detectors with LLAH for the detection. We also compare our approach with SURF and finally demonstrate that it is possible to detect and track different types of textures including colorful pictures, binary fiducial markers and handwritings.
[Databases, Buildings, Robot vision systems, Detectors, Feature extraction, Cameras, Robustness]
Homography-based planar mapping and tracking for mobile phones
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
We present a real-time camera pose tracking and mapping system which uses the assumption of a planar scene to implement a highly efficient mapping algorithm. Our light-weight mapping approach is based on keyframes and plane-induced homographies between them. We solve the planar reconstruction problem of estimating the keyframe poses with an efficient image rectification algorithm. Camera pose tracking uses continuously extended and refined planar point maps and delivers robustly estimated 6DOF poses. We compare system and method with bundle adjustment and monocular SLAM on synthetic and indoor image sequences. We demonstrate large savings in computational effort compared to the monocular SLAM system while the reduction in accuracy remains acceptable.
[Real time systems, mapping, monocular SLAM, Estimation, mobile phone, Cameras, Cost function, Three dimensional displays, Mobile handsets, tracking, plane estimation]
Real-time self-localization from panoramic images on mobile devices
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
Self-localization in large environments is a vital task for accurately registered information visualization in outdoor Augmented Reality (AR) applications. In this work, we present a system for self-localization on mobile phones using a GPS prior and an online-generated panoramic view of the user's environment. The approach is suitable for executing entirely on current generation mobile devices, such as smartphones. Parallel execution of online incremental panorama generation and accurate 6DOF pose estimation using 3D point reconstructions allows for real-time self-localization and registration in large-scale environments. The power of our approach is demonstrated in several experimental evaluations.
[Accuracy, Databases, Cameras, Three dimensional displays, Mobile handsets, Image reconstruction, Global Positioning System]
Information-theoretic database building and querying for mobile augmented reality applications
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
Recently, there has been tremendous interest in the area of mobile Augmented Reality (AR) with applications including navigation, social networking, gaming and education. Current generation mobile phones are equipped with camera, GPS and other sensors, e.g., magnetic compass, accelerometer, gyro in addition to having ever increasing computing/graphics capabilities and memory storage. Mobile AR applications process the output of one or more sensors to augment the real world view with useful information. This paper's focus is on the camera sensor output, and describes the building blocks for a vision-based AR system. We present information-theoretic techniques to build and maintain an image (feature) database based on reference images, and for querying the captured input images against this database. Performance results using standard image sets are provided demonstrating superior recognition performance even with dramatic reductions in feature database size.
[Accuracy, Databases, Object detection, database pruning, Feature extraction, Mobile communication, Entropy, Robustness, Mobile Augmented Reality]
Rapid scene reconstruction on mobile phones from panoramic images
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
Rapid 3D reconstruction of environments has become an active research topic due to the importance of 3D models in a huge number of applications, be it in Augmented Reality (AR), architecture or other commercial areas. In this paper we present a novel system that allows for the generation of a coarse 3D model of the environment within several seconds on mobile smartphones. By using a very fast and flexible algorithm a set of panoramic images is captured to form the basis of wide field-of-view images required for reliable and robust reconstruction. A cheap on-line space carving approach based on Delaunay triangulation is employed to obtain dense, polygonal, textured representations. The use of an intuitive method to capture these images, as well as the efficiency of the reconstruction approach allows for an application on recent mobile phone hardware, giving visually pleasing results almost instantly.
[Solid modeling, Computational modeling, Cameras, Feature extraction, Three dimensional displays, Mobile handsets, Image reconstruction]
The Argon AR Web Browser and standards-based AR application environment
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
A common vision of Augmented Reality (AR) is that of a person immersed in a diverse collection of virtual information, superimposed on their view of the world around them. If such a vision is to become reality, an ecosystem for AR must be created that satisfies at least these properties: multiple sources (or channels of interactive information) must be able to be simultaneously displayed and interacted with, channels must be isolated from each other (for security and stability), channel authors must have the flexibility to design the content and interactivity of their channel, and the application must fluidly integrate with the ever-growing cloud of systems and services that define our digital lives. In this paper, we present the design and implementation of the Argon AR Web Browser and describe our vision of an AR application environment that leverages the WWW ecosystem. We also describe KARML, our extension to KML (the spatial markup language for Google Earth and Maps), that supports the functionality required for mobile AR. We combine KARML with the full range of standard web technologies to create a standards-based web browser for mobile AR. KARML lets users develop 2D and 3D content using existing web technologies and facilitates easy deployment from standard web servers. We highlight a number of projects that have used Argon and point out the ways in which our web-based architecture has made previously impractical AR concepts possible.
[Service oriented architecture, Computer architecture, Mobile communication, augmented reality, Three dimensional displays, HTML, Browsers, Argon, web-based architecture]
Online user survey on current mobile augmented reality applications
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
Augmented reality (AR) as an emerging technology in the mobile computing domain is becoming mature enough to engender publicly available applications for end users. Various commercial applications have recently been emerging in the mobile consumer domain at an increasing pace &#x2014; Layar, Junaio, Google Goggles, and Wikitude are perhaps the most prominent ones. However, the research community lacks an understanding of how well such timely applications have been accepted, what kind of user experiences they have evoked, and what the users perceive as the weaknesses of the various applications overall. During the spring of 2011 we conducted an online survey to study the overall acceptance and user experience of the mobile AR-like consumer applications currently existing on the market. This paper reports the first analyses of the qualitative and quantitative survey data of 90 respondents. We highlight an extensive set of user-oriented issues to be considered in developing the applications further, as well as in directing future user research in AR. The results indicate that the experiences have been inconsistent: generally positive evaluations are overshadowed by mentions of applications' pragmatic uselessness in everyday life and technical unreliability, as well as excessive or limited and irrelevant content.
[Context, Visualization, Image recognition, Mobile communication, augmented reality, Browsers, user experience, Augmented reality, online survey, evaluation, end user application, Cameras, user acceptance]
Tracking-by-synthesis using point features and pyramidal blurring
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
Tracking-by-synthesis is a promising method for markerless vision-based camera tracking, particularly suitable for Augmented Reality applications. In particular, it is drift-free, viewpoint invariant and easy-to-combine with physical sensors such as GPS and inertial sensors. While edge features have been used succesfully within the tracking-by-synthesis framework, point features have, to our knowledge, still never been used. We believe that this is due to the fact that real-time corner detectors are generally weakly repeatable between a camera image and a rendered texture. In this paper, we compare the repeatability of commonly used FAST, Harris and SURF interest point detectors across view synthesis. We show that adding depth blur to the rendered texture can drastically improve the repeatability of FAST and Harris corner detectors (up to 100% in our experiments), which can be very helpful, e.g., to make tracking-by-synthesis running on mobile phones. We propose a method for simulating depth blur on the rendered images using a pre-calibrated depth response curve. In order to fulfil the performance requirements, a pyramidal approach is used based on the well-known MIP mapping technique. We also propose an original method for calibrating the depth response curve, which is suitable for any kind of focus lenses and comes for free in terms of programming effort, once the tracking-by-synthesis algorithm has been implemented.
[Solid modeling, Detectors, Cameras, Calibration, Equations, Lenses]
Evaluating the impact of recovery density on augmented reality tracking
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
Natural feature tracking systems for augmented reality are highly accurate, but can suffer from lost tracking. When registration is lost, the system must be able to re-localize and recover tracking. Likewise, when a camera is new to a scene, it must be able to perform the related task of localization. Localization and re-localization can only be performed at certain points or when viewing particular objects or parts of the scene with a sufficient number and quality of recognizable features to allow for tracking recovery. We explore how the density of such recovery locations/poses influences the time it takes users to resume tracking. We focus our evaluation on two generalized techniques for localization: keyframe-based and model-based. For the keyframe-based approach we assume a constant collection rate for keyframes. We find that at practical collection rates, the task of localization to a previously acquired keyframe that is shown to the user does not become more time-consuming as the interval between keyframes increases. For a localization approach using model data, we consider a grid of points around the model at which localization is guaranteed to succeed. We find that the user interface is crucial to successful localization. Localization can occur quickly if users do not need to orient themselves to marked localization points. When users are forced to mentally register themselves with a map of the scene, localization quickly becomes impractical as the distance to the next localization point increases. We contend that our results will help future designers of localization techniques to better plan for the effects of their proposed solutions.
[Tracking, Buildings, Cities and towns, Time frequency analysis, Cameras, Data models, Augmented reality]
Using egocentric vision to achieve robust inertial body tracking under magnetic disturbances
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
In the context of a smart user assistance system for industrial manipulation tasks it is necessary to capture motions of the upper body and limbs of the worker in order to derive his or her interactions with the task space. While such capturing technology already exists, the novelty of the proposed work results from the strong requirements of the application context: The method should be flexible and use only on-body sensors, work accurately in industrial environments that suffer from severe magnetic disturbances, and enable consistent registration between the user body frame and the task space. Currently available systems cannot provide this. This paper suggests a novel egocentric solution for visual-inertial upper-body motion tracking based on recursive filtering and model-based sensor fusion. Visual detections of the wrists in the images of a chest-mounted camera are used as substitute for the commonly used magnetometer measurements. The on-body sensor network, the motion capturing system, and the required calibration procedure are described and successful operation is shown in a real industrial environment.
[Wrist, Magnetometers, Tracking, Magnetic resonance imaging, Cameras, Sensors, Joints]
Gravity-aware handheld Augmented Reality
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
This paper investigates how different stages in handheld Augmented Reality (AR) applications can benefit from knowing the direction of the gravity measured with inertial sensors. It presents approaches to improve the description and matching of feature points, detection and tracking of planar templates, and the visual quality of the rendering of virtual 3D objects by incorporating the gravity vector. In handheld AR, both the camera and the display are located in the user's hand and therefore can be freely moved. The pose of the camera is generally determined with respect to piecewise planar objects that have a known static orientation with respect to gravity. In the presence of (close to) vertical surfaces, we show how gravity-aligned feature descriptors (GAFD) improve the initialization of tracking algorithms relying on feature point descriptor-based approaches in terms of quality and performance. For (close to) horizontal surfaces, we propose to use the gravity vector to rectify the camera image and detect and describe features in the rectified image. The resulting gravity-rectified feature descriptors (GREFD) provide an improved precision-recall characteristic and enable faster initialization, in particular under steep viewing angles. Gravity-rectified camera images also allow for real-time 6 DoF pose estimation using an edge-based object detection algorithm handling only 4 DoF similarity transforms. Finally, the rendering of virtual 3D objects can be made more realistic and plausible by taking into account the orientation of the gravitational force in addition to the relative pose between the handheld device and a real object.
[Cameras, Feature extraction, Vectors, Three dimensional displays, Mobile handsets, Sensors, Gravity]
Texture-less object tracking with online training using an RGB-D camera
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
We propose a texture-less object detection and 3D tracking method which automatically extracts on the fly the information it needs from color images and the corresponding depth maps. While texture-less 3D tracking is not new, it requires a prior CAD model, and real-time methods for detection still have to be developed for robust tracking. To detect the target, we propose to rely on a fast template-based method, which provides an initial estimate of its 3D pose, and we refine this estimate using the depth and image contours information. We automatically extract a 3D model for the target from the depth information. To this end, we developed methods to enhance the depth map and to stabilize the 3D pose estimation. We demonstrate our method on challenging sequences exhibiting partial occlusions and fast motions.
[Training, Solid modeling, Tracking, Iterative closest point algorithm, US Department of Transportation, Cameras, Three dimensional displays]
KinectFusion: Real-time dense surface mapping and tracking
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
We present a system for accurate real-time mapping of complex and arbitrary indoor scenes in variable lighting conditions, using only a moving low-cost depth camera and commodity graphics hardware. We fuse all of the depth data streamed from a Kinect sensor into a single global implicit surface model of the observed scene in real-time. The current sensor pose is simultaneously obtained by tracking the live depth frame relative to the global model using a coarse-to-fine iterative closest point (ICP) algorithm, which uses all of the observed depth data available. We demonstrate the advantages of tracking against the growing full surface model compared with frame-to-frame tracking, obtaining tracking and mapping results in constant time within room sized scenes with limited drift and high accuracy. We also show both qualitative and quantitative results relating to various aspects of our tracking and mapping system. Modelling of natural scenes, in real-time with only commodity sensor and GPU hardware, promises an exciting step forward in augmented reality (AR), in particular, it allows dense surfaces to be reconstructed in real-time, with a level of detail and robustness beyond any solution yet presented using passive computer vision.
[Real time systems, Tracking, SLAM, GPU, Image reconstruction, Real-Time, Surface reconstruction, AR, Simultaneous localization and mapping, Depth Cameras, Iterative closest point algorithm, Dense Reconstruction, Cameras, Three dimensional displays, Volumetric Representation]
Encumbrance-free telepresence system with real-time 3D capture and display using commodity depth cameras
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
This paper introduces a proof-of-concept telepresence system that offers fully dynamic, real-time 3D scene capture and continuous-viewpoint, head-tracked stereo 3D display without requiring the user to wear any tracking or viewing apparatus. We present a complete software and hardware framework for implementing the system, which is based on an array of commodity Microsoft Kinect&#x2122;color-plus-depth cameras. Novel contributions include an algorithm for merging data between multiple depth cameras and techniques for automatic color calibration and preserving stereo quality even with low rendering rates. Also presented is a solution to the problem of interference that occurs between Kinect cameras with overlapping views. Emphasis is placed on a fully GPU-accelerated data processing and rendering pipeline that can apply hole filling, smoothing, data merger, surface generation, and color correction at rates of up to 100 million triangles/sec on a single PC and graphics board. Also presented is a Kinect-based marker-less tracking system that combines 2D eye recognition with depth information to allow head-tracked stereo views to be rendered for a parallax barrier autostereoscopic display. Our system is affordable and reproducible, offering the opportunity to easily deliver 3D telepresence beyond the researcher's lab.
[object recognition, Smoothing methods, virtual reality, camera calibration, Interference, three-dimensional displays, sensor fusion, teleconferencing, parallel processing, tracking, Graphics processing unit, surface fitting, Image color analysis, computer vision, Cameras, Rendering (computer graphics), Three dimensional displays, color calibration, filtering]
RGB-D camera-based parallel tracking and meshing
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
Compared to standard color cameras, RGB-D cameras are designed to additionally provide the depth of imaged pixels which in turn results in a dense colored 3D point cloud representing the environment from a certain viewpoint. We present a real-time tracking method that performs motion estimation of a consumer RGB-D camera with respect to an unknown environment while at the same time reconstructing this environment as a dense textured mesh. Unlike parallel tracking and mapping performed with a standard color or grey scale camera, tracking with an RGB-D camera allows a correctly scaled camera motion estimation. Therefore, there is no need for measuring the environment by any additional tool or equipping the environment by placing objects in it with known sizes. The tracking can be directly started and does not require any preliminary known and/or constrained camera motion. The colored point clouds obtained from every RGB-D image are used to create textured meshes representing the environment from a certain camera view and the real-time estimated camera motion is used to correctly align these meshes over time in order to combine them into a dense reconstruction of the environment. We quantitatively evaluated the proposed method using real image sequences of a challenging scenario and their corresponding ground truth motion obtained with a mechanical measurement arm. We also compared it to a commonly used state-of-the-art method where only the color information is used. We show the superiority of the proposed tracking in terms of accuracy, robustness and usability. We also demonstrate its usage in several Augmented Reality scenarios where the tracking allows a reliable camera motion estimation and the meshing increases the realism of the augmentations by correctly handling their occlusions.
[Real time systems, Tracking, Image color analysis, Cameras, Feature extraction, Three dimensional displays, Image reconstruction]
Interactive visualization technique for truthful color reproduction in spatial augmented reality applications
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
Spatial augmented reality is especially interesting for the design process of a car, because a lot of virtual content and corresponding real objects are used. One important issue in such a process is that the designer can trust the visualized colors on the real object, because design decisions are made on basis of the projection. In this article, we present an interactive visualization technique which is able to exactly compute the RGB values for the projected image, so that the resulting colors on the real object are equally perceived as the real desired colors. Our approach computes the influences of the ambient light, the material, the pose and the color model of the projector to the resulting colors of the projected RGB values by using a physically-based computation. This information allows us to compute the adjustment for the RGB values for varying projector positions at interactive rates. Since the amount of projectable colors does not only depend on the material and the ambient light, but also on the pose of the projector, our method can be used to interactively adjust the range of projectable colors by moving the projector to arbitrary positions around the real object. The proposed method is evaluated in a number of experiments.
[Visualization, Image color analysis, Materials, Dynamic range, Three dimensional displays, Table lookup, Equations]
Adaptive camera-based color mapping for mixed-reality applications
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
We present a novel adaptive color mapping method for virtual objects in mixed-reality environments. In several mixed-reality applications, added virtual objects should be visually indistinguishable from real objects. Recent mixed-reality methods use global-illumination algorithms to approach this goal. However, simulating the light distribution is not enough for visually plausible images. Since the observing camera has its very own transfer function from real-world radiance values to RGB colors, virtual objects look artificial just because their rendered colors do not match with those of the camera. Our approach combines an on-line camera characterization method with a heuristic to map colors of virtual objects to colors as they would be seen by the observing camera. Previous tone-mapping functions were not designed for use in mixed-reality systems and thus did not take the camera-specific behavior into account. In contrast, our method takes the camera into account and thus can also handle changes of its parameters during runtime. The results show that virtual objects look visually more plausible than by just applying tone-mapping operators.
[Differential Rendering, Visualization, Image color analysis, Lighting, Virtual reality, Color, Cameras, Rendering (computer graphics), Tone Mapping, Color Matching, Mixed Reality]
Image-based clothes transfer
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
Virtual dressing rooms for the fashion industry and digital entertainment applications aim at creating an image or a video of a user in which he or she wears different garments than in the real world. Such images can be displayed, for example, in a magic mirror shopping application or in games and movies. Current solutions involve the error-prone task of body pose tracking. We suggest an approach that allows users who are captured by a set of cameras to be virtually dressed with previously recorded garments in 3D. By using image-based algorithms, we can bypass critical components of other systems, especially tracking based on skeleton models. We rather transfer the appearance of a garment from one user to another by image processing and image-based rendering. Using images of real garments allows for photo-realistic rendering quality with high performance.
[CUDA, visual hull, Runtime, Databases, Clothing, Cameras, Rendering (computer graphics), Feature extraction, augmented reality, virtual dressing room, Three dimensional displays, image-based rendering]
Light factorization for mixed-frequency shadows in augmented reality
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
Integrating animated virtual objects with their surroundings for high-quality augmented reality requires both geometric and radio-metric consistency. We focus on the latter of these problems and present an approach that captures and factorizes external lighting in a manner that allows for realistic relighting of both animated and static virtual objects. Our factorization facilitates a combination of hard and soft shadows, with high-performance, in a manner that is consistent with the surrounding scene lighting.
[Geometry, Computational modeling, Lighting, Vectors, Mathematical model, Equations, Shadow mapping]
Providing guidance for maintenance operations using automatic markerless Augmented Reality system
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
This paper proposes a new real-time Augmented Reality based tool to help in disassembly for maintenance operations. This tool provides workers with augmented instructions to perform maintenance tasks more efficiently. Our prototype is a complete framework characterized by its capability to automatically generate all the necessary data from input based on untextured 3D triangle meshes, without requiring additional user intervention. An automatic offline stage extracts the basic geometric features. These are used during the online stage to compute the camera pose from a monocular image. Thus, we can handle the usual textureless 3D models used in industrial applications. A self-supplied and robust markerless tracking system that combines an edge tracker, a point based tracker and a 3D particle filter has also been designed to continuously update the camera pose. Our framework incorporates an automatic path-planning module. During the offline stage, the assembly/disassembly sequence is automatically deduced from the 3D model geometry. This information is used to generate the disassembly instructions for workers.
[Solid modeling, Image edge detection, Cameras, Feature extraction, Three dimensional displays, Junctions, Assembly]
Augmented reality in the psychomotor phase of a procedural task
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
Procedural tasks are common to many domains, ranging from maintenance and repair, to medicine, to the arts. We describe and evaluate a prototype augmented reality (AR) user interface designed to assist users in the relatively under-explored psychomotor phase of procedural tasks. In this phase, the user begins physical manipulations, and thus alters aspects of the underlying task environment. Our prototype tracks the user and multiple components in a typical maintenance assembly task, and provides dynamic, prescriptive, overlaid instructions on a see-through head-worn display in response to the user's ongoing activity. A user study shows participants were able to complete psychomotor aspects of the assembly task significantly faster and with significantly greater accuracy than when using 3D-graphics-based assistance presented on a stationary LCD. Qualitative questionnaire results indicate that participants overwhelmingly preferred the AR condition, and ranked it as more intuitive than the LCD condition.
[repair, Prototypes, Documentation, workpiece, Maintenance engineering, augmented reality, Three dimensional displays, Combustion, maintenance, Assembly, Engines]
Is there a reality in Industrial Augmented Reality?
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
In the spirit of the seminal article by Brooks [12] that surveys the field of Virtual Reality to evaluate its level of applicability, we study the readiness of Industrial Augmented Reality (IAR). We have been hearing about IAR since Mizell and Caudell [14] first gave a name to AR, but how many applications broke out of the lab to be used by non-developers? In reviewing the literature, we note the amazing progress made in display technology, rendering and tracking. Given these improvements, one might expect AR-based industrial products to flourish. Unfortunately, this is still not the case. In this paper, we provide a comprehensive and up-to-date survey of industrial AR applications. We organize the different applications of IAR over the life-cycle of products, in order to draw some parallels between the different proposed concepts and offer a clear taxonomy for future applications. We also propose and apply a rubric to evaluate existing IAR systems in order to highlight reasons for success and offer guidelines in the hope that it will help IAR become &#x201C;really real&#x201D;.
[Solid modeling, Design automation, Welding, Maintenance engineering, Production facilities, Three dimensional displays, Assembly]
MR in OR: First analysis of AR/VR visualization in 100 intra-operative Freehand SPECT acquisitions
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
For the past two decades, medical Augmented Reality visualization has been researched and prototype systems have been tested in laboratory setups and limited clinical trials. Up to our knowledge, until now, no commercial system incorporating Augmented Reality visualization has been developed and used routinely within the real-life surgical environment. In this paper, we are reporting on observations and analysis concerning the usage of a commercially developed and clinically approved Freehand SPECT system, which incorporates monitor-based Mixed Reality visualization, during real-life surgeries. The workflow-based analysis we present is focused on an atomic sub-task of sentinel lymph node biopsy. We analyzed the usage of the Augmented and Virtual Reality visualization modes by the surgical team, while leaving the staff completely uninfluenced and unbiased in order to capture the natural interaction with the system. We report on our observations in over 100 Freehand SPECT acquisitions within different phases of 52 surgeries.
[Visualization, Target tracking, Surgery, Three dimensional displays, Single photon emission computed tomography, Probes, Image reconstruction]
Out of reach? &#x2014; A novel AR interface approach for motor rehabilitation
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
Mixed reality rehabilitation systems and games are demonstrating potential as innovative adjunctive therapies for health professionals in their treatment of various hand and upper limb motor impairments. Unilateral motor deficits of the arm, for example, are commonly experienced post stroke. Our TheraMem system provides an augmented reality game environment that contributes to this increasingly rich area of research. We present a prototype system which &#x201C;fools the brain&#x201D; by visually amplifying users' hand movements &#x2014; small actual hand movements lead to perceived larger movements. We validate the usability of our system in an empirical study with forty-five non-clinical participants. In addition, we present early qualitative evidence for the utility of our approach and system for stroke recovery and motor rehabilitation. Future uses of the system are considered by way of conclusion.
[Therapy, Tiles, Thumb, Augmented Reality, Games, Cameras, Three dimensional displays, Physical and Motor Rehabilitation, Usability]
User experiences with augmented reality aided navigation on phones
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
We investigate user experiences when using augmented reality (AR) as a new aid to navigation. We integrate AR with other more common interfaces into a handheld navigation system, and we conduct an exploratory study to see where and how people exploit AR. Based on previous work on augmented photographs, we hypothesize that AR is used more to support wayfinding at static locations when users approach a road intersection. In partial contrast to this hypothesis, our results from a user evaluation hint that users will expect to use the system while walking. Further, our results also show that AR is usually exploited shortly before and after road intersections, suggesting that tracking support will be mostly needed in proximity of road intersections.
[Legged locomotion, Navigation, Roads, Mobile communication, Cameras, Software, Augmented reality]
Creating hybrid user interfaces with a 2D multi-touch tabletop and a 3D see-through head-worn display
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
How can multiple different display and interaction devices be used together to create an effective augmented reality environment? We explore the design of several prototype hybrid user interfaces that combine a 2D multi-touch tabletop display with a 3D head-tracked video-see-through display. We describe a simple modeling application and an urban visualization tool in which the information presented on the head-worn display supplements the information displayed on the tabletop, using a variety of approaches to track the head-worn display relative to the tabletop. In all cases, our goal is to allow users who can see only the tabletop to interact effectively with users wearing head-worn displays.
[Solid modeling, Visualization, tabletop interaction, Buildings, Two dimensional displays, Prototypes, User interfaces, hybrid user interface, Three dimensional displays, multi-modal interaction, Augmented reality, urban visualization]
Usability of one handed interaction methods for hand-held projection-based augmented reality
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
With the advent of portable projectors (also embedded in a smart phone), projection based augmented reality (AR) will be an attractive form of AR as the augmentation is made directly in real space (instead of on the video screen). Several interaction methods for &#x201C;Procam&#x201D; systems, also applicable to projection based AR, have been developed, but their comparative usability has not been studied in depth. In this paper, we compare the usability of four representative interaction methods, applied to the menu selection task, for the hand-held projection based AR. The four menu selection methods studied are formed by combinations of two types of cursor control (projector cursor vs. on-device touch screen), and two types of item selection (explicit click vs. crossing). Experimental results have shown that the menu selection task was most efficient, usable and preferred when the projector cursor with the crossing widget was used. Furthermore, the task performance was not statistically different between using the dominant, non-dominant hand and even both hands.
[Projection based, Menu Selection, Thumb, Prototypes, Augmented Reality, Interaction, Educational institutions, Usability, Smart phones, Augmented reality]
3D high dynamic range display system
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
This paper introduces a new high dynamic range (HDR) display system that generates a physical 3D HDR image without using stereoscopic methods. To boost contrast beyond that obtained using either a hardcopy or a projector, we employ a multiprojection system to superimpose images onto a textured solid hardcopy that is output by a 3D printer or a rapid prototyping machine. We introduce two basic techniques for our 3D HDR display. The first technique computes an optimal placement of projectors so that projected images cover the hardcopy's entire surface while maximizing image quality. The second technique allows a user to place the projectors near the computed optimal position by projecting from each projector images that act as visual guides. Through proof-of-concept experiments, we were able to modulate luminance and chrominance with a registration error of less than 3 mm. The physical contrast ratio obtained using our method was approximately 5,000:1, while it was 5:1 in the case of viewing the 3D printout under environmental light and 1,000:1 in the case of using the projectors to project the image on regular screens.
[Image quality, Visualization, Image color analysis, Stereo image processing, Dynamic range, Three dimensional displays, Printers]
Deformable random dot markers
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
We extend planar fiducial markers using random dots [8] to nonrigidly deformable markers. Because the recognition and tracking of random dot markers are based on keypoint matching, we can estimate the deformation of the markers with nonrigid surface detection from keypoint correspondences. First, the initial pose of the markers is computed from a homography with RANSAC as a planar detection. Second, deformations are estimated from the minimization of a cost function for deformable surface fitting. We show augmentation results of 2D surface deformation recovery with several markers.
[Shape, Databases, Robustness, Computational efficiency, Augmented reality, Surface treatment]
Virtual transparency: Introducing parallax view into video see-through AR
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
In this poster, we present the idea of &#x201C;virtual transparency&#x201D; for video see-through AR. In fully synthetic 3D graphics, head-tracked motion parallax has been shown to be a powerful depth cue for understanding the structure of the virtual world. To leverage head-tracked motion parallax in video see-through AR, the view of the virtual and physical world must change together in response to head motion. We present a system for accomplishing this, and discuss the benefits and limitations of our approach.
[Head, Tracking, head tracking, Cameras, augmented reality, Three dimensional displays, motion parallax, Augmented reality, Lenses]
Augmenting 3D urban environment using mobile devices
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
We describe an augmented reality prototype for exploring a 3D urban environment on mobile devices. Our system utilizes the location and orientation sensors on the mobile platform as well as computer vision techniques to register the live view of the device with the 3D urban data. In particular, the system recognizes the buildings in the live video, tracks the camera pose, and augments the video with relevant information about the buildings in the correct perspective. The 3D urban data consist of 3D point clouds and corresponding geo-tagged RGB images of the urban environment. We also discuss the processing steps to make such 3D data scalable and usable by our system.
[Buildings, Mobile communication, Feature extraction, Cameras, Three dimensional displays, Sensors, Servers]
Graph-cut-based 3D model segmentation for articulated object reconstruction
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
The three-dimensional (3D) reconstruction of objects has been well studied in the literature of augmented reality (AR) [1, 2]. Most existing studies have assumed that the to-be-constructed target object is rigid, whereas objects in the real world can be dynamic or deformable. Therefore, AR systems are required to deal with non-rigid objects to be adaptive to environmental changes. In this paper, we address the problem of reconstructing articulated objects as a starting point for modeling deformable objects. An articulated object is composed of partially rigid components linked with joints. After building a mesh model of the object, the model is segmented into the components along their boundaries by a graph-cut-based approach that we propose.
[Tracking, Motion segmentation, Feature extraction, Cameras, Three dimensional displays, Joints, Image reconstruction]
A user study on the Snap-To-Feature interaction method
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
Recent advances in mobile computing and augmented reality (AR) technology have lead to popularization of mobile AR applications. Touch screen input is common in mobile devices, and also widely used in mobile AR applications. However, due to unsteady camera view movement, it can be hard to carry out precise interactions in handheld AR environments, for tasks such as tracing physical objects. In this research, we investigate a Snap-To-Feature interaction method that helps users to perform more accurate touch screen interactions by attracting user input points to image features in the AR scene. A user experiment is performed using the method to trace a physical object, which is typical for modeling real objects within the AR scene. The results shows that the Snap-To-Feature method makes a significant difference in the accuracy of touch screen based AR interaction.
[annotation, Accuracy, Image edge detection, Mobile communication, Cameras, Feature extraction, touch screen interface, Augmented reality]
Outdoor mobile localization from panoramic imagery
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
We describe an end-to-end system for mobile, vision-based localization and tracking in urban environments. Our system uses panoramic imagery which is processed and indexed to provide localization coverage over a large area using few capture points. We utilize a client-server model which allows for remote computation and data storage while maintaining real-time tracking performance. Previous search results are cached and re-used by the mobile client to minimize communication overhead. We evaluate the use of the system for flexible real-time camera tracking in large outdoor spaces.
[Real time systems, Tracking, Urban areas, Cameras, Mobile communication, Servers, Augmented reality]
Evolutionary augmented reality at the Natural History Museum
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
In this paper we describe the development of an augmented reality system designed to provide an exciting new way for the Natural History Museum in London to present evolutionary history to their visitors. The system uses a through-the-lens tracker and infrared LED markers to provide an unobtrusive and robust system that can operate for multiple users across a wide area.
[Films, camera tracking, Augmented Reality application, Cameras, Light emitting diodes, History, Multimedia communication, Optical sensors, Augmented reality]
Adaptive substrate for enhanced spatial augmented reality contrast and resolution
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
This poster presents the concept of combining two display technologies to enhance graphics effects in spatial augmented reality (SAR) environments. This is achieved by using an ePaper surface as an adaptive substrate instead of a white painted surface allowing the development of novel image techniques to improve image quality and object appearance in projector-based SAR environments.
[Image resolution, Image color analysis, Concrete, Substrates, Surface treatment, Augmented reality, Consumer electronics]
Transformative reality: Augmented reality for visual prostheses
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
Visual prostheses such as retinal implants provide bionic vision that is limited in spatial and intensity resolution. This limitation is a fundamental challenge of bionic vision as it severely truncates salient visual information. We propose to address this challenge by performing real time transformations of visual and non-visual sensor data into symbolic representations that are then rendered as low resolution vision; a concept we call Transformative Reality. For example, a depth camera allows the detection of empty ground in cluttered environments that is then visually rendered as bionic vision to enable indoor navigation. Such symbolic representations are similar to virtual content overlays used in Augmented Reality but are registered to the 3D world via the user's sense of touch. Preliminary user trials, where a head mounted display artificially constrains vision to a 25&#x00D7;25 grid of binary dots, suggest that Transformative Reality provides practical and significant improvements over traditional bionic vision in tasks such as indoor navigation, object localisation and people detection.
[Real time systems, Visualization, Image edge detection, Rendering (computer graphics), Cameras, Spatial resolution, Biomedical imaging]
Tightly-coupled robust vision aided inertial navigation algorithm for augmented reality using monocular camera and IMU
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
Odometry component of a camera tracking system for augmented reality applications is described. The system uses a MEMS-type inertial measurement unit (IMU) with 3-axis gyroscopes and accelerometers and a monocular camera to accurately and robustly track the camera motion in 6 degrees of freedom (with correct scale) in arbitrary indoor or outdoor scenes. Tight coupling of IMU and camera is achieved by an error-state extended Kalman filter (EKF) which performs sensor fusion for inertial navigation at a deep level such that each visually tracked feature contributes as an individual measurement as opposed to the more traditional approaches where camera pose estimates are first extracted by means of feature tracking and then used as measurement updates in a filter framework. Robustness, on the other hand, is achieved by using a geometric hypothesize-and-test architecture based on the five-point relative pose estimation method, rather than a Mahalanobis distance type gating mechanism derived from the Kalman filter state prediction, to select the inlier tracks and remove outliers from the raw feature point matches which would otherwise corrupt the filter since tracks are directly used as measurements.
[inertial navigation, MEMS IMU, EKF, Tracking, Current measurement, Cloning, Cameras, Feature extraction, monocular camera, sensor fusion, Vectors, Kalman filters]
Edgel templates for fast planar object detection and pose estimation
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
We describe a method to select edgels and to calculate gradient orientation-based template descriptors for edgel features. An edgel is selected within a grid block based on gradient magnitude; its position and orientation are used to determine a canonical frame where the descriptor is computed based on quantized orientation. The resulting descriptor is efficiently matched using logical operations. We demonstrate the use of the resulting edgel detection and description method for planar object detection and pose estimation.
[Real time systems, Shape, Image edge detection, Estimation, Object detection, Mobile handsets, Augmented reality]
An interactive augmented reality coloring book
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
Creating entertaining and educational books not only requires providing visually stimulating content but also means for students to interact, create, and express themselves. In this paper we present a new type of mixed-reality book experience, which augments an educational coloring book with user-generated three dimensional content. We explore a &#x201C;pop-up book&#x201D; metaphor and describe a process by which children's drawing and coloring is used as input to generate and change the appearance of the book content. Our system is based on natural feature tracking and image processing techniques that can be easily exploited for other AR publishing applications.
[Real time systems, Solid modeling, edutainment, education, 3D texturing, Augmented reality, interactive AR, Mixed Reality book, Image color analysis, Education, Natural Feature Tracking, Three dimensional displays, Books]
Fusing the real and the virtual: A depth-camera based approach to Mixed Reality
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
The seamless integration of the real and the virtual content is the ultimate yet unreached goal of Mixed Reality applications. Among others it requires mutual blocking and lighting between real and virtual objects. In this paper we present our approach of applying a low-cost depth camera, such as Kinect, allowing for an easy acquisition of depth images. However, as the quality of the raw input data is insufficient for this purpose, we apply a series of filter and optimization operations. This allows us to realize mutual real-time lighting and rigid interaction in a dynamic environment. Our approach produces an acceptable quality of images of low-frequency scenes at interactive frame rates on an off-the-shelf desktop computer.
[Real time systems, Smoothing methods, Image edge detection, Augmented Reality, image based lighting, Kinect, Image color analysis, Lighting, Virtual reality, occlusions, Cameras, depth camera, Mixed Reality]
Augmenting magnetic field lines for school experiments
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
We present a system for interactive magnetic field simulation in an AR-setup. The aim of this work is to investigate how AR technology can help to develop a better understanding of the concept of fields and field lines and their relationship to the magnetic forces in typical school experiments. The haptic feedback is provided by real magnets that are optically tracked. In a stereo video see-through head-mounted display, the magnets are augmented with the dynamically computed field lines.
[Visualization, Magnetic separation, Magnetic resonance imaging, Magnetomechanical effects, Magnetic devices, Educational institutions, Augmented reality]
Interactive annotation on mobile phones for real and virtual space registration
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
Registration of real space and virtual information is a fundamental requirement for any augmented reality system. This paper presents an interactive method to quickly create a 3D room model and annotate locations within the room to provide registration anchors for virtual information. The method operates on a mobile phone and uses a visual rotation tracker to obtain orientation tracking for in-situ applications. The simple interaction allows non-expert users to create models of their environment and thus contribute marked-up representations to an online AR platform.
[Solid modeling, Visualization, Target tracking, Computational modeling, visual rotation tracking, Cameras, Three dimensional displays, Mobile handsets, mobile AR, interactive modeling, Annotation]
An empiric evaluation of confirmation methods for optical see-through head-mounted display calibration
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
The calibration of optical see-through head-mounted displays is an important fundament for correct object alignment in augmented reality. Any calibration process for OSTHMDs requires users to align 2D points in screen space with 3D points in the real world and to confirm each alignment. In this poster, we present the results of our empiric evaluation where we compared four confirmation methods: Keyboard, Hand-held, Voice, and Waiting. The Waiting method, designed to reduce head motion during confirmation, showed a significantly higher accuracy than all other methods. Averaging over a time frame for sampling user input before the time of confirmation improved the accuracy of all methods in addition. We conducted a further expert study proving that the results achieved with a video see-through head-mounted display showed valid for optical see-through head-mounted display calibration, too.
[Accuracy, Keyboards, Three dimensional displays, Adaptive optics, Calibration, Electronic mail, Augmented reality]
Augmented reality pipe layout planning in the shipbuilding industry
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
As large ships are never produced in masses, it often occurs that the construction process and production process overlap in time. Many shipbuilding companies have problems with discrepancies between the construction data and the real built ship. The assembly department often has to modify CAD data for a successful installation. We present an augmented reality system, where a user can visualize the construction data of pipes and modify these in the case of misalignment, collisions or any other conflicts. The modified pipe geometry can be stored and further used as input for CNC pipe bending machines. To guarantee an exactly orthogonal passage of the pipes through aligning bolt holes, we integrated an optical measurement tool into the pipe alignment process.
[Solid modeling, Design automation, Three dimensional displays, Calibration, Planning, Augmented reality, Image reconstruction]
Urban canvas: Unfreezing street-view imagery with semantically compressed LIDAR pointclouds
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
Detailed 3D scans of urban environments are increasingly being collected with the goal of bringing more location-aware content to mobile users. This work converts large collections of LIDAR scans and street-view panoramas into a representation that extracts semantically meaningful components of the scene. Compressing this data by an order of magnitude or more enables rich user interactions with mobile applications that have a very good knowledge of the scene around them. These representations are suitable for integrating into physics engines and transmission over mobile networks &#x2014; key components of modern AR entertainment solutions.
[Geometry, Strips, Laser radar, Lighting, Mobile communication, Three dimensional displays, Physics]
Comparing spatial understanding between touch-based and AR-style interaction
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
There are currently two primary ways of viewing location specific information in-situ on hand-held mobile device screens: using a see-through augmented reality interface and using a touch-based interface with panoramas. The two approaches use fundamentally different interaction metaphors: an AR-style of interacting where the user holds up the device and physically moves it to change views of the world, and a touch-based technique where panorama navigation is independent of the physical world. We have investigated how this difference in interaction technique impacts a user's spatial understanding of the mixed reality world. Our study found that AR-style interaction provided better spatial understanding overall, while touch-based interaction changed the experience to have more similar characteristics to interaction in a separate virtual environment.
[Visualization, Green products, Virtual environments, Estimation, Cameras, Analysis of variance, Augmented reality]
Bare-hand-based augmented reality interface on mobile phone
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
This paper proposes an augmented reality interface that provides natural hand-based interaction with virtual objects on mobile phones. Assume that one holds a mobile phone in a hand and sees the other hand through mobile phone's camera. Then, a virtual object is rendered on his/her palm and reacts to hand and finger movements. Since the proposed interface does not require any additional sensors or markers, one freely interacts with the virtual object anytime and anywhere. The proposed interface worked at 5 fps on a mobile phone (Galaxy S2 having a dual-core processor).
[Shape, Thumb, Cameras, Mobile communication, Mobile handsets, MR/AR for entertainment, vision-based registration and tracking, Interaction techniques for MR/AR, Augmented reality]
Visualization of geometric properties of flexible objects for form designing
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
Computer-aided design (CAD) system conventionally have been widely used to support designers for creating, modifying, adding something to or removing something from objects by showing simulated objects on computer screen. These virtual, non-physical, objects have been, however, known as imperfect imitation of reality. The impression of shape is highly related to the second order derivative of geometric feature of the shape. Conventional CAD systems, including AutoCAD, usually have visualization feature of the first derivative (normal) and the second derivative (curvature) of given surfaces. There, however, still have been problems in curvature visualization on the screen. First, it lacks true feeling of physical objects. Second, even if designers were given a physical mock-up object in hand, they wouldn't precisely recognize minute change of curvatures &#x2014; few designers can sense small differences of curvature and most others need a special device to check the curvature. For solving these problem, the authors propose a novel curvature visualization system based on mixed reality technology. The color mapping according to the Gaussian curvature calculated via a time-of-flight camera provides the observers with intuitively understanding the object's curvature information.
[Computers, Visualization, Design automation, Shape, Noise, Observers, Cameras]
&#x201C;Soul Hunter&#x201D;: A novel augmented reality application in theme parks
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
This paper introduces a novel augmented reality shooting game named &#x201C;Soul Hunter&#x201D;, which has been successfully operating in a theme park in China. Soul Hunter adopts an innovative infrared marker scheme to build a mobile augmented reality application in a wide area. It is an extension of the traditional first person game, in which a player is able to fight with virtual ghost through a gunlike device in real environment. This paper describes the challenges of applying augmented reality in theme parks and shares some experiences in solving the problems encountered in practical applications.
[Infrared markers, Lighting, Shooting game, Games, Rendering (computer graphics), Cameras, Augment reality, Augmented reality, Painting, Radiofrequency identification]
Toe input with mobile projector and depth camera
2011 10th IEEE International Symposium on Mixed and Augmented Reality
None
2011
With the miniaturization of mobile projectors, we can provide a larger projection surface for information browsing, despite the small size of the projection devices. Moreover, the opportunities for using location-based information services both indoors and outdoors increase. We can obtain the necessary information via the small LCDs of handheld devices, thanks to the prevalence of cell phones and GPS technology. However, these mobile terminal devices restrict the use of one hand and demand that the user keeps a close watch on the small display. It is necessary to take the devices out of a pocket or bag. To solve these problems, many researchers have focused on wearable projection systems. These systems allow for the provision of hands-free viewing via large projected screens and eliminate the need to take out and hold devices. In this paper, we propose a &#x201C;Toe Input&#x201D; method, which can realize haptic interaction, direct manipulation, and floor projection in wearable projection systems with a slightly larger projection surface. We evaluate the system in terms of accuracy, required time, comfort, and area suitable for input.
[input interface, Accuracy, wearable, Information services, Mobile communication, Educational institutions, Electronic mail, projection, depth camera, Floors, Foot]
BOF
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Tradition is alive and well, but there also should be opportunities for innovation at ISMAR.
[]
Committee
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Provides a listing of current committee members.
[]
Demos
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
We are proud to present the ISMAR 2012 Demonstrations Program. Research in Mixed and Augmented Reality is expanding into broad and diverse horizons. This year's demonstrations touch on a wide variety of topics, from marketing to education to public engagement and beyond; and showcase a multitude of technological implementations, from outdoor feature tracking devices to robotic-assisted camera manipulation to vibration-tolerant HUD displays and many other unique innovations. We are certain that hands-on engagement with the exciting demo selections will spark the enthusiasm and imagination of our diverse community, leading to a memorable conference experience and serving as inspiration for future evolution of the exciting medium of Mixed and Augmented Reality.
[]
Doctoral Consortium
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
We are very happy to present the inaugural ISMAR Doctoral Consortium (DC). The goal of the DC is to create an opportunity for doctoral students to facilitate their research ideas, present their current progress and future plans, and receive constructive criticism and insights related to their future work and career perspectives. A total of seven doctoral students have been selected to the DC from the world, and their research projects in the area of AR, VR and HCI are scheduled to present all day on Monday 5th November from 10am. All ISMAR attendees are welcome to come and interaction with the students as well as foster the future event of DC.
[]
General chairs
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Welcome to the Eleventh IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR 2012)! We are excited that this year's symposium is being held on the campus of the Georgia Institute of Technology, at Georgia Tech's Hotel and Conference Center. Located in the center of Altanta, Georgia, USA, Georgia Tech is one of the top public research universities in the United States, and a nationally ranked leader in many of the academic disciplines that form the heart of ISMAR: Computer Science, Engineering, HCI, Robotics &amp; Computer Vision, and Digital Media. Atlanta is one of the largest cities in the United States, often considered the &#x201C;capital of the South&#x201D; and played a central role in the American Civil War and the Civil Rights movement. With a diverse and young population, Atlanta is a dynamic city with many cultural and historic attractions, restaurants, shopping, sports and outdoor activities. Atlanta is served by Hartsfield-Jackson International Airport, the worlds busiest airport.
[]
A new era of Human Computer Interaction
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
For most researchers outside of the field, Human Computer Interaction (or HCI) is the study and evaluation of interactive systems and techniques. While this is an important part of our discipline, nowadays HCI is as much about &#x201C;building&#x201D; the underlying technologies and systems as it is studying their use. In this talk I will demonstrate why it is an exciting time to be a computer science researcher in this discipline. You can play with the newest technologies, such as exotic cameras, displays and sensing hardware; readily embrace approaches outside of your discipline (e.g. within computer vision, machine learning, signal processing, or computer graphics); and even invent technologies and algorithms along the way. However, ultimately, you'll build working systems that are grounded by real-world problems that have direct impact on users.
[computer science researcher, interactive systems, sensing hardware, human computer interaction, exotic cameras, real-world problems, HCI]
Preface
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
We are delighted to welcome you to ISMAR 2012, the 11th symposium on Mixed and Augmented Reality! This year&#x00E2; &#x20AC;&#x2122;s symposium continues a long tradition of ISMAR meetings, a series that itself followed a related series of IWAR, ISMR, and ISAR meetings.
[]
[Program Committee]
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
The following topics are dealt with: mixed reality; and augmented reality.
[mixed reality, augmented reality]
[Steering Committee]
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Provides a listing of current committee members.
[]
Tracking competition
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
This is the fifth year that ISMAR is organizing a tracking competition. It is encouraging to see how computing, imaging and new algorithms are coming together to enable increasingly complex tasks on mobile platforms.
[]
Tutorial 1: Adaptive augmented reality (A2R): Where AR meets user's interest
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Adaptive behavior is one of the main challenges in building computerized systems, especially in the case of systems which are delivering information to the end users. Indeed, since the information overload has become the main drawback for the future development of such systems (cf. Big Data challenge), there is a huge movement in the research community to develop concepts for better adaptation of the form and size of information that will be delivered to a user (usually taking different forms of the personalization). However, the main effort has been dedicated to the contextualization of the user's task in order to determine what is the best way to tailor/adapt the presentation of information to the user, neglecting the role of the user's internal context, expressed as the user's (short-term) interest. The same is valid for the AR systems. In this tutorial we present novel results in modeling users' interest in the context of AR systems and demonstrate some practical results in realizing such an approach in a multisensor AR system based on the usage of the see-through AR glasses. Due to the need for continuously adapt the AR content to the user's interest, such models are facing many challenges in sensing the user's behavior (using acoustic-, video-, gesture- and bio-sensors), interpreting it as an interest and deciding in real-time what kind of the adaptation to perform. We argue that this lead to a new class of AR system that we coined as adaptive AR (AR) systems. This work has been partially realized within the scope of the FP7 ICT research project ARtSENSE (www.artsense.eu), that is developing new AR concepts for improving personalized museum's experience. The tutorial will present practical results from applying the approach in three cultural heritage institutions in Europe (Paris, Madrid and Liverpool).
[]
Tutorial 2: Integrating and using panoramas and photographic images in AR experiences
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Some AR browsers and other mobile phone apps (e.g. Argon, Photosynth, 360 Cities, Tourwrist) allow the user to create, display or interact with panoramas and other forms of historical and contemporary imagery. These technologies open up exciting possibilities for cultural heritage, entertainment and other uses in location-based experiences. Full panoramas or historical photographs merged into the visual field can provide the user with a perspective on a place as it looked in the past or might look in a possible future. We propose to offer the participants in this tutorial an introduction to the technical issues involved in creating and integrating such imagery into an AR/MR application. We will also provide relevant historical background regarding panoramas and consider issues of aesthetics and user experience design.
[]
Tutorial 3: AR mobile game development: Getting started
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
This tutorial is a half-day project based tutorial to demonstrate how to create an AR mobile game prototype from game design to art, animation and technical production. Tools such as Unity, Maya and Vuforia will be used in this tutorial. Standard game development topics which can be applied to all digital game projects such as the game design process, pre-production planning, 3D modeling, rigging and animation techniques, game engine workflow as well as the unique elements of AR mobile game development will be covered in this tutorial.
[]
Tutorials
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
It is our great pleasure to present the ISMAR Tutorials. We proudly host four tutorials that provide sharing of knowledge from seasoned researchers. Our tutorials cover a wide range of topics including evaluation techniques, game design, adaptive augmented reality, and panoramas. Through these exciting tutorials we hope to expand the minds of ISMAR 2012 attendees and help to foster the next generation of Mixed and Augmented Reality researchers, practitioners, and artists.
[]
Workshop
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
It is our great pleasure to present this year's ISMAR Workshops. These events provide a chance to thoroughly examine specific research areas in the exciting field of Mixed and Augmented Reality.
[]
Workshop 1: 2nd IEEE ISMAR workshop on authoring solutions for augmented reality
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
The motivation of this workshop is to discuss future direction of content authoring in the field of Augmented Reality, as well as to discuss the current state of art on content creation and content authoring for augmented reality. The workshop will comprise of a paper session where authoring papers, late-breaking results and overviews over state-of-the-art are presented. In the afternoon, we will follow up with discussion sessions on different topics ranging from content creation and authoring to content distribution for AR and a short closing session.
[]
Workshop 3: IEEE ISMAR 2012 workshop on tracking methods and applications (TMA)
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
The focus of this workshop is on presenting, discussing and demonstrating recent tracking methods and applications that work well in practice and that show some superiority over state-of-the-art methods. Rather than focusing on pure novelty, this workshop encourages presentations that concentrate on complete systems and integrated approaches. The TMA workshop looks at pose tracking from an end-to-end point of view.
[]
Wide-area scene mapping for mobile visual tracking
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
We propose a system for easily preparing arbitrary wide-area environments for subsequent real-time tracking with a handheld device. Our system evaluation shows that minimal user effort is required to initialize a camera tracking session in an unprepared environment. We combine panoramas captured using a handheld omnidirectional camera from several viewpoints to create a point cloud model. After the offline modeling step, live camera pose tracking is initialized by feature point matching, and continuously updated by aligning the point cloud model to the camera image. Given a reconstruction made with less than five minutes of video, we achieve below 25 cm translational error and 0.5 degrees rotational error for over 80% of images tested. In contrast to camera-based simultaneous localization and mapping (SLAM) systems, our methods are suitable for handheld use in large outdoor spaces.
[subsequent real-time tracking, point cloud model, handheld omnidirectional camera, feature point matching, augmented reality, Mobile handsets, Servers, tracking, Image reconstruction, Computer System Implementation, Accuracy, wide-area scene mapping, handheld device, offline modeling step, pose estimation, Image Processing and Computer Vision, Real-time systems, I.2.10 [Artificial Intelligence, camera tracking session, wide-area augmented reality, video signal processing, system evaluation, Artificial Intelligence, image reconstruction, image matching, mobile visual tracking, live camera pose tracking, Pattern Recognition, panorama, Streaming media, Cameras]
Live tracking and mapping from both general and rotation-only camera motion
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
We present an approach to real-time tracking and mapping that supports any type of camera motion in 3D environments, that is, general (parallax-inducing) as well as rotation-only (degenerate) motions. Our approach effectively generalizes both a panorama mapping and tracking system and a keyframe-based Simultaneous Localization and Mapping (SLAM) system, behaving like one or the other depending on the camera movement. It seamlessly switches between the two and is thus able to track and map through arbitrary sequences of general and rotation-only camera movements. Key elements of our approach are to design each system component such that it is compatible with both panoramic data and Structure-from-Motion data, and the use of the `Geometric Robust Information Criterion' to decide whether the transformation between a given pair of frames can best be modeled with an essential matrix E, or with a homography H. Further key features are that no separate initialization step is needed, that the reconstruction is unbiased, and that the system continues to collect and map data after tracking failure, thus creating separate tracks which are later merged if they overlap. The latter is in contrast to most existing tracking and mapping systems, which suspend tracking and mapping, thus discarding valuable data, while trying to relocalize the camera with respect to the initial map. We tested our system on a variety of video sequences, successfully tracking through different camera motions and fully automatically building panoramas as well as 3D structures.
[Tracking, live tracking, Merging, geometric robust information criterion, automatically building panoramas, 3D environments, keyframe-based simultaneous localization and mapping system, rotation-only camera motion, cameras, live mapping, real-time tracking, Simultaneous localization and mapping, structure-from-motion data, video sequences, Cameras, Real-time systems, Robustness, Data models, real-time mapping, homography H, image sequences]
Kinectrack: Agile 6-DoF tracking using a projected dot pattern
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
We present Kinectrack, a new six degree-of-freedom (6-DoF) tracker which allows real-time and low-cost pose estimation using only commodity hardware. We decouple the dot pattern emitter and IR camera of the Kinect. Keeping the camera fixed and moving the IR emitter in the environment, we recover the 6-DoF pose of the emitter by matching the observed dot pattern in the field-of-view of the camera to a pre-captured reference image. We propose a novel matching technique to obtain dot pattern correspondences efficiently in wide- and adaptive-baseline scenarios. We also propose an auto-calibration method to obtain the camera intrinsics and dot pattern reference image. The performance of Kinectrack is evaluated and the rotational and translational accuracy of the system is measured relative to ground truth for both planar and multi-planar scene geometry. Our system can simultaneously recover the 6-DoF pose of the device and also recover piecewise planar 3D scene structure, and can be used as a low-cost method for tracking a device without any on-board computation, with small size and only simple electronics.
[on-board computation, projected dot pattern, camera intrinsics, adaptive-baseline scenarios, auto-calibration method, agile 6-DoF tracking, augmented reality, Kinectrack, Calibration, Table lookup, multiplanar scene geometry, six degree-of-freedom, Accuracy, Runtime, Cameras, IR camera, rotational accuracy, Robustness, translational accuracy, Pattern matching, dot pattern emitter, 6-DoF pose, piecewise planar 3D scene structure]
Dense multibody motion estimation and reconstruction from a handheld camera
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Existing approaches to camera tracking and reconstruction from a single handheld camera for Augmented Reality (AR) focus on the reconstruction of static scenes. However, most real world scenarios are dynamic and contain multiple independently moving rigid objects. This paper addresses the problem of simultaneous segmentation, motion estimation and dense 3D reconstruction of dynamic scenes. We propose a dense solution to all three elements of this problem: depth estimation, motion label assignment and rigid transformation estimation directly from the raw video by optimizing a single cost function using a hill-climbing approach. We do not require prior knowledge of the number of objects present in the scene - the number of independent motion models and their parameters are automatically estimated. The resulting inference method combines the best techniques in discrete and continuous optimization: a state of the art variational approach is used to estimate the dense depth maps while the motion segmentation is achieved using discrete graph-cut based optimization. For the rigid motion estimation of the independently moving objects we propose a novel tracking approach designed to cope with the small fields of view they induce and agile motion. Our experimental results on real sequences show how accurate segmentations and dense depth maps can be obtained in a completely automated way and used in marker-free AR applications.
[natural scenes, hill-climbing approach, Tracking, graph theory, augmented reality, Optimization, Image reconstruction, rigid transformation estimation, cameras, optimisation, image segmentation, variational techniques, motion estimation, agile motion, object tracking, discrete graph-cut based optimization, discrete-continuous optimization, video signal processing, cost function optimization, image sequences, dynamic scene motion estimation, dynamic scene segmentation, raw video, independently moving rigid objects, marker-free AR applications, inference method, field-of-view, Motion estimation, variational approach, handheld camera tracking, dynamic scene dense 3D reconstruction, Estimation, static scene reconstruction, depth estimation, robot vision, image reconstruction, inference mechanisms, motion label assignment, Motion segmentation, Cameras, dense multibody rigid motion estimation, dense depth map estimation]
Distributed visual processing for augmented reality
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Recent advances have made augmented reality on smartphones possible but these applications are still constrained by the limited computational power available. This paper presents a system which combines smartphones with networked infrastructure and fixed sensors and shows how these elements can be combined to deliver real-time augmented reality. A key feature of this framework is the asymmetric nature of the distributed computing environment. Smartphones have high bandwidth video cameras but limited computational ability. Our system connects multiple smartphones through relatively low bandwidth network links to a server with large computational resources connected to fixed sensors that observe the environment. By contrast to other systems that use preprocessed static models or markers, our system has the ability to rapidly build dynamic models of the environment on the fly at frame rate. We achieve this by processing data from a Microsoft Kinect, to build a trackable point cloud model of each frame. The smartphones process their video camera data on-board to extract their own set of compact and efficient feature descriptors which are sent via WiFi to a server. The server runs computationally intensive algorithms including feature matching, pose estimation and occlusion testing for each smartphone. Our system demonstrates real-time performance for two smartphones.
[Visualization, point cloud model, server, pattern matching, smartphone, augmented reality, Servers, bandwidth network link, distributed computing environment, feature descriptor extraction, feature extraction, data visualisation, pose estimation, distributed visual processing, Microsoft Kinect, Sensors, dynamic model, computational ability, networked infrastructure, fixed sensors, video cameras, occlusion testing, smart phones, Augmented reality, Hip, preprocessed static model, video camera data, Cameras, computational power, wireless LAN, real-time augmented reality, WiFi, feature matching, Smart phones]
LDB: An ultra-fast feature for scalable Augmented Reality on mobile devices
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
The efficiency, robustness and distinctiveness of a feature descriptor are critical to the user experience and scalability of a mobile Augmented Reality (AR) system. However, existing descriptors are either too compute-expensive to achieve real-time performance on a mobile device such as a smartphone or tablet, or not sufficiently robust and distinctive to identify correct matches from a large database. As a result, current mobile AR systems still only have limited capabilities, which greatly restrict their deployment in practice. In this paper, we propose a highly efficient, robust and distinctive binary descriptor, called Local Difference Binary (LDB). LDB directly computes a binary string for an image patch using simple intensity and gradient difference tests on pairwise grid cells within the patch. A multiple gridding strategy is applied to capture the distinct patterns of the patch at different spatial granularities. Experimental results demonstrate that LDB is extremely fast to compute and to match against a large database due to its high robustness and distinctiveness. Comparing to the state-of-the-art binary descriptor BRIEF, primarily designed for speed, LDB has similar computational efficiency, while achieves a greater accuracy and 5x faster matching speed when matching over a large database with 1.7M+ descriptors.
[Performance evaluation, object recognition, Correlation, smartphone, augmented reality, LDB, tracking, tablet, Image coding, mobile computing, BRIEF, gradient difference tests, local difference binary, Lighting, mobile AR systems, Abstracts, Virtual reality, multiple gridding strategy, Robustness, notebook computers, gradient methods, ultra-fast feature, image patch, scalable augmented reality, smart phones, mobile augmented reality system, Augmented reality, mobile devices, binary feature descriptor]
VRCodes: Unobtrusive and active visual codes for interaction by exploiting rolling shutter
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
We show a new visible tagging solution for active displays which allows a rolling-shutter camera to detect active tags from a relatively large distance in a robust manner. Current planar markers are visually obtrusive for the human viewer. In order for them to be read from afar and embed more information, they must be shown larger thus occupying valuable physical space on the design. We present a new active visual tag which utilizes all dimensions of color, time and space while remaining unobtrusive to the human eye and decodable using a 15fps rolling-shutter camera. The design exploits the flicker fusion-frequency threshold of the human visual system, which due to the effect of metamerism, can not resolve metamer pairs alternating beyond 120Hz. Yet, concurrently, it is decodable using a 15fps rolling-shutter camera due to the effective line-scan speed of 15&#x00D7;400 lines per second. We show an off-the-shelf rolling-shutter camera can resolve the metamers flickering on a television from a distance over 4 meters. We use intelligent binary coding to encode digital positioning and show potential applications such as large screen interaction. We analyze the use of codes for locking and tracking encoded targets. We also analyze the constraints and performance of the sampling system, and discuss several plausible application scenarios.
[Visualization, human visual system, digital positioning, planar marker, Tracking, Humans, augmented reality, active display, metamers flickering, unobtrusive code, Image color analysis, active visual codes, sampling system, active visual tag, Colored noise, VRCodes, Augmented Reality [Tracking, visible tagging solution, Decoding, Metamerism, flicker fusion-frequency threshold, computer vision, intelligent binary coding, Cameras, Solids, rolling shutter, image coding, large screen interaction]
Representative feature descriptor sets for robust handheld camera localization
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
We present a method to automatically determine a set of feature descriptors that describes an object such that it can be localized under a variety of viewpoints. Based on a set of synthetically generated views, local image features are detected, described and aggregated in a database. Our proposed method evaluates matches between these database features to eventually find a set of the most representative descriptors from the database. Using this scalable offline process, the localization success rate is significantly increased without adding computational load to the runtime method. Moreover, if camera localization is performed with respect to objects at a known gravity orientation, we propose to create multiple reference descriptor sets for different angles between the camera's principal axis and the gravity vector. This approach is particularly suited for handheld devices with built-in inertial sensors and enables matching against a reference dataset only containing the information relevant for camera poses that are consistent with the measured gravity. Comprehensive evaluations of the proposed methods using a large quantity of real camera images, a variety of objects, different cameras and different kinds of feature descriptors confirm that our approaches outperform standard feature descriptor-based methods.
[scalable offline process, reference dataset, visual databases, Mobile handsets, set theory, localization success rate, gravity orientation, cameras, Databases, gravity vector, measured gravity, local image features, Sensors, comprehensive evaluations, camera poses, real camera images, Gravity, image processing, principal axis, feature descriptor-based methods, representative descriptors, Vectors, multiple reference descriptor sets, runtime method, realistic images, representative feature descriptor sets, vectors, computational load, Cameras, Feature extraction, handheld devices, database features, robust handheld camera localization, built-in inertial sensors, computational complexity]
Multi-sensor navigation algorithm using monocular camera, IMU and GPS for large scale augmented reality
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Camera tracking system for augmented reality applications that can operate both indoors and outdoors is described. The system uses a monocular camera, a MEMS-type inertial measurement unit (IMU) with 3-axis gyroscopes and accelerometers, and GPS unit to accurately and robustly track the camera motion in 6 degrees of freedom (with correct scale) in arbitrary indoor or outdoor scenes. IMU and camera fusion is performed in a tightly coupled manner by an error-state extended Kalman filter (EKF) such that each visually tracked feature contributes as an individual measurement as opposed to the more traditional approaches where camera pose estimates are first extracted by means of feature tracking and then used as measurement updates in a filter framework. Robustness in feature tracking and hence in visual measurement generation is achieved by IMU aided feature matching and a two-point relative pose estimation method, to remove outliers from the raw feature point matches. Landmark matching to contain long-term drift in orientation via on the fly user generated geo-tiepoint mechanism is described.
[feature tracking, MEMS IMU, camera tracking system, error-state EKF, visually tracked feature, microsensors, large scale augmented reality application, augmented reality, sensor fusion, MEMS-type inertial measurement unit, 3-axis gyroscopes, GPS, multisensor navigation algorithm, indoor scenes, cameras, fly user generated geo-tiepoint mechanism, pose estimation, monocular camera, outdoor scenes, landmark matching, GPS unit, raw feature point matching, Kalman filters, Mathematical model, visual measurement generation, filter framework, inertial navigation, EKF, individual measurement, Vectors, nonlinear filters, camera motion, gyroscopes, image matching, Global Positioning System, measurement updates, Current measurement, two-point relative pose estimation method, Measurement uncertainty, camera pose estimation, target tracking, Cameras, Feature extraction, accelerometers, MEMS-type IMU, IMU aided feature matching, error-state extended Kalman filter, long-term drift]
Optical outside-in tracking using unmodified mobile phones
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Marker-based optical outside-in tracking is a mature and robust technology used by many AR, VR and motion capture applications. However, in small environments the tracking cameras are often difficult to install. An example scenario are ergonomic studies in car manufacturing, where the motion of a worker needs to be tracked in small spaces such as the trunk of a car. In this paper, we describe how to extend the tracking volume in small, cluttered environments using small and flexible wireless cameras in form of unmodified mobile phones that can quickly be installed. Since those small cameras are not synchronized with the main tracking cameras, we describe several modifications to the tracking algorithms, such as inter-frame interpolation, the replacement of the least-squares adjustment by a Kalman filter and the integration of rolling-shutter compensation. To support the quick setup of additional cameras while the tracking system is running, the system is extended by an on-line calibration technique that determines the extrinsic camera parameters without requiring a dedicated calibration step.
[unmodified mobile phone, tracking algorithm, Kalman filter, Mobile handsets, cluttered environment, I.4.8 [Image Processing and Computer Vision, cameras, mobile computing, tracking volume, Image Processing and Computer Vision, object tracking, marker-based optical outside-in tracking, rolling-shutter compensation, tracking system, on-line calibration technique, Kalman filters, calibration, least-squares adjustment, Target tracking, least squares approximations, mobile radio, wireless camera, Calibration, Synchronization, motion capture application, tracking camera, image motion analysis, AR, interpolation, camera parameter, Cameras, compensation, Information Interfaces and Presentation, VR, interframe interpolation]
Real-time surface light-field capture for augmentation of planar specular surfaces
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
A single hand-held camera provides an easily accessible but potentially extremely powerful setup for augmented reality. Capabilities which previously required expensive and complicated infrastructure have gradually become possible from a live monocular video feed, such as accurate camera tracking and, most recently, dense 3D scene reconstruction. A new frontier is to work towards recovering the reflectance properties of general surfaces and the lighting configuration in a scene without the need for probes, omni-directional cameras or specialised light-field cameras. Specular lighting phenomena cause effects in a video stream which can lead current tracking and reconstruction algorithms to fail. However, the potential exists to measure and use these effects to estimate deeper physical details about an environment, enabling advanced scene understanding and more convincing AR. In this paper we present an algorithm for real-time surface light-field capture from a single hand-held camera, which is able to capture dense illumination information for general specular surfaces. Our system incorporates a guidance mechanism to help the user interactively during capture. We then split the light-field into its diffuse and specular components, and show that the specular component can be used for estimation of an environment map. This enables the convincing placement of an augmentation on a specular surface such as a shiny book, with realistic synthesized shadow, reflection and occlusion of specularities as the viewpoint changes. Our method currently works for planar scenes, but the surface light-field representation makes it ideal for future combination with dense 3D reconstruction methods.
[lighting configuration, augmented reality, specialised light-field cameras, tracking algorithms, GPU, Light sources, cameras, Image color analysis, Lighting, planar specular surfaces augmentation, real-time surface light-field capture, surface light-field representation, Real-time systems, live monocular video, Surface texture, viewpoint changes, specular components, single hand-held camera, Light-Fields, Illumination Estimation, SLAM, reflectance properties, Surface treatment, Real-Time, reconstruction algorithms, AR, 3D reconstruction methods, image representation, 3D scene reconstruction, Cameras, general specular surfaces]
High-quality reflections, refractions, and caustics in Augmented Reality and their contribution to visual coherence
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
In this paper we present a novel high-quality rendering system for Augmented Reality (AR). We study ray-tracing based rendering techniques in AR with the goal of achieving real-time performance and improving visual quality as well as visual coherence between real and virtual objects in a final composited image. A number of realistic and physically correct rendering effects are demonstrated, that have not been presented in real-time AR environments before. Examples are high-quality specular effects such as caustics, refraction, reflection, together with a depth of field effect and anti-aliasing. We present a new GPU implementation of photon mapping and its application for the calculation of caustics in environments where real and virtual objects are combined. The composited image is produced on-the-fly without the need of any preprocessing step. A main contribution of our work is the achievement of interactive rendering speed for high-quality ray-tracing algorithms in AR setups. Finally we performed an evaluation to study how users perceive visual quality and visual coherence with different realistic rendering effects. The results of our user study show that in 40.1% cases users mistakenly judged virtual objects as real ones. Moreover we show that high-quality rendering positively affects the perceived visual coherence.
[Visualization, ray tracing, H.5.1 [Information Interfaces and Presentation, augmented reality, refraction effect, reflection effect, GPU, Photonics, ray-tracing based rendering technique, Computer Graphics, high-quality rendering system, Lighting, photon mapping, Ray tracing, interactive rendering speed, Kernel, rendering (computer graphics), visual quality, graphics processing units, antialiasing, caustics effect, depth of field effect, Rendering (computer graphics), Cameras, Information Interfaces and Presentation, visual coherence]
Instant indirect illumination for dynamic mixed reality scenes
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
For seamless integration of virtual content into real scenes, realizing mutual global lighting effects between both worlds belongs to the most important and challenging goals. Therefore, plenty of global illumination approaches exist, which mostly share the same restriction: the real scene is approximated by a static model, which was built in advance and thus has to remain static. In our paper, we propose an image-space global illumination approach, based on reflective shadow maps, combined with the use of an RGB-D camera, to simulate first bounce diffuse indirect illumination without any pre-computations. Our approach supports indirect illumination in both directions (real to virtual and vice versa) and runs in real-time. Furthermore, it does not require advanced shader properties, since we developed an implementation making efficient usage of the Z-Buffer algorithm for calculating indirect illumination.
[static model, diffuse indirect illumination, Augmented Reality, image based lighting, instant indirect illumination, augmented reality, Lighting, Virtual reality, kinect, Real-time systems, dynamic mixed reality scene, depth camera, Mixed Reality, image processing, Image resolution, Filtering, Z-Buffer algorithm, mutual global lighting effect, occlusions, reflective shadow maps, Cameras, Rendering (computer graphics), image-space global illumination, RGB-D camera, real-time global illumination]
Real-time photometric registration from arbitrary geometry
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Visually coherent rendering for augmented reality is concerned with seamlessly blending the virtual world and the real world in real-time. One challenge in achieving this is the correct handling of lighting. We are interested in applying real-world light to virtual objects, and compute the interaction of light between virtual and real. This implies the measurement of the real-world lighting, also known as photometric registration. So far, photometric registration has mainly been done through capturing images with artificial light probes, such as mirror balls or planar markers, or by using high dynamic range cameras with fish-eye lenses. In this paper, we present a novel non-invasive system, using arbitrary scene geometry as a light probe for photometric registration, and a general AR rendering pipeline supporting real-time global illumination techniques. Based on state of the art real-time geometric reconstruction, we show how to robustly extract data for photometric registration to compute a realistic representation of the real-world diffuse lighting. Our approach estimates the light from observations of the reconstructed model and is based on spherical harmonics, enabling plausible illumination such as soft shadows, in a mixed virtual-real rendering pipeline.
[fish eye lenses, image registration, mirror balls, H.5.1 [Information Interfaces and Presentation, noninvasive system, augmented reality, virtual world, cameras, spherical harmonics, Computer Graphics, Lighting, real time geometric reconstruction, visually coherent rendering, Image Processing and Computer Vision, realistic representation, Real-time systems, Mathematical model, rendering (computer graphics), real world diffuse lighting, real time photometric registration, mixed virtual real rendering pipeline, Estimation, arbitrary scene geometry, reconstructed model, artificial light probes, planar markers, Geometry, real time global illumination, Rendering (computer graphics), Cameras, dynamic range cameras, Information Interfaces and Presentation]
Reduction of contradictory partial occlusion in mixed reality by using characteristics of transparency perception
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
One of the challenges in mixed reality (MR) applications is handling contradictory occlusions between real and virtual objects. The previous studies have tried to solve the occlusion problem by extracting the foreground region from the real image. However, real-time occlusion handling is still difficult since it takes too much computational cost to precisely segment foreground regions in a complex scene. In this study, therefore, we proposed an alternative solution to the occlusion problem that does not require precise foreground-background segmentation. In our method, a virtual object is blended with a real scene so that the virtual object can be perceived as being behind the foreground region. For this purpose, we first investigated characteristics of human transparency perception in a psychophysical experiment. Then we made a blending algorithm applicable to real scenes based on the results of the experiment.
[Humans, contradictory occlusions, Augmented Reality, Predictive models, augmented reality, contradictory partial occlusion reduction, real-time occlusion handling, real scenes, real objects, image segmentation, psychophysical experiment, Virtual reality, transparency perception, Real-time systems, virtual objects, Mathematical model, foreground-background segmentation, Mixed Reality, real image, computational cost, Observers, realistic images, Equations, blending algorithm, mixed reality applications, real-time systems, foreground regions, human transparency perception]
PixMix: A real-time approach to high-quality Diminished Reality
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Diminished Reality (DR) allows to remove objects from a video stream while preseving a frame to frame coherence. Some approaches apply a pseudo-DR, allowing for the removal of objects only, while their background can be observed by a second camera. Most real DR approaches are highly computational expensive, not even allowing for interactive rates and/or apply significant restrictions regarding the uniformity of the background, or allow linear camera movements or even a static camera only. In this paper we will present a real-time capable Diminished Reality approach for high-quality image manipulation. Our approach achieves a significantly better performance and image quality for almost planar but non-trivial image backgrounds. Our Diminished Reality pipeline provides coherent video streams even for nonlinear camera movements due to the integration of homography based object tracking.
[Visualization, homography-based object tracking integration, pseudoDR, H.5.1 [Information Interfaces and Presentation, Computing Methodologies, coherent video stream preservation, object background removal, high-quality image manipulation, high-quality diminished reality pipeline, cameras, Image segmentation, nonlinear camera movements, Coherence, Streaming media, Cameras, Cost function, object tracking, Real-time systems, Information Interfaces and Presentation, PixMix approach, planar nontrivial image backgrounds, video signal processing, image sequences]
A non-photorealistic rendering framework with temporal coherence for augmented reality
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Many augmented reality (AR) applications require a seamless blending of real and virtual content as key to increased immersion and improved user experiences. Photorealistic and non-photorealistic rendering (NPR) are two ways to achieve this goal. Compared with photorealistic rendering, NPR stylizes both the real and virtual content and makes them indistinguishable. Maintaining temporal coherence is a key challenge in NPR. We propose a NPR framework with support for temporal coherence by leveraging model-space information. Our systems targets painterly rendering styles of NPR. There are three major steps in this rendering framework for creating coherent results: tensor field creation, brush anchor placement, and brush stroke reshaping. To achieve temporal coherence for the final rendered results, we propose a new projection-based surface sampling algorithm which generates anchor points on model surfaces. The 2D projections of these samples are uniformly distributed in image space for optimal brush stroke placement. We also propose a general method for averaging various properties of brush stroke textures, such as their skeletons and colors, to further improve the temporal coherence. We apply these methods to both static and animated models to create a painterly rendering style for AR. Compared with existing image space algorithms our method renders AR with NPR effects with a high degree of coherence.
[Algorithm design and analysis, brush stroke texture, static model, Solid modeling, Brushes, H.5.1 [Information Interfaces and Presentation, augmented reality, projection-based surface sampling algorithm, anchor point, temporal coherence, model-space information, NPR, Computer Graphics, animated model, rendering (computer graphics), nonphotorealistic rendering, sampling methods, Computational modeling, brush anchor placement, 2D projection, tensor field creation, Tensile stress, Coherence, brush stroke reshaping, Rendering (computer graphics), Information Interfaces and Presentation]
Subtle cueing for visual search in augmented reality
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Visual search in augmented reality environments is an important task that can be facilitated through different cueing methods. Current cueing methods rely on explicit cueing, which can potentially reduce visual search performance. In comparison, this paper proposes a subtle cueing method that improves visual search performance while being clutter-neutral. Two empirical user studies were conducted to evaluate our subtle cueing method in outdoor scenes. The results show that subtle cueing functions well within a narrow Feature Congestion range, and could be a feasible alternative to explicit cueing.
[Visualization, Protocols, clutter, subtle cueing method, Humans, feature congestion range, Educational institutions, augmented reality, augmented reality environment, explicit cueing, visual search performance, outdoor scene, Clutter, Augmented reality, clutter-neutral, Outdoor scenes, Subtle visual cueing, Visual search, Erbium]
Interactive 4D overview and detail visualization in augmented reality
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
In this paper we present an approach for visualizing time-oriented data of dynamic scenes in an on-site AR view. Visualizations of time-oriented data have special challenges compared to the visualization of arbitrary virtual objects. Usually, the 4D data occludes a large part of the real scene. Additionally, the data sets from different points in time may occlude each other. Thus, it is important to design adequate visualization techniques that provide a comprehensible visualization. In this paper we introduce a visualization concept that uses overview and detail techniques to present 4D data in different detail levels. These levels provide at first an overview of the 4D scene, at second information about the 4D change of a single object and at third detailed information about object appearance and geometry for specific points in time. Combining the three levels of detail with interactive transitions such as magic lenses or distorted viewing techniques enables the user to understand the relationship between them. Finally we show how to apply this concept for construction site documentation and monitoring.
[Visualization, Overview and Detail, augmented reality, Augmented Reality Visualization, Time-Oriented Visualization, detail technique, Image color analysis, object appearance, data visualisation, Abstracts, comprehensible visualization, object geometry, overview technique, time-oriented data visualization, Context, document handling, interactive 4D overview, construction industry, civil engineering computing, construction site documentation, Geometry, Data visualization, arbitrary virtual object, construction site monitoring, Rendering (computer graphics), detail visualization]
Image-driven view management for augmented reality browsers
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
In this paper, we introduce a novel view management technique for placing labels in Augmented Reality systems. A common issue in many Augmented Reality applications is the absence of knowledge of the real environment, limiting the efficient representation and optimal layout of the digital information augmented onto the real world. To overcome this problem, we introduce an image-based approach, which combines a visual saliency algorithm with edge analysis to identify potentially important image regions and geometric constraints for placing labels. Our proposed solution also includes adaptive rendering techniques that allow a designer to control the appearance of depth cues. We describe the results obtained from a user study considering different scenarios, which we performed for validating our approach. Our technique will provide special benefits to Augmented Reality browsers that usually lack scene knowledge, but also to many other applications in the domain of Augmented Reality such as cultural heritage and maintenance applications.
[Visualization, image-driven view management, image-based approach, H.5.1 [Information Interfaces and Presentation, geometric constraint, augmented reality, scene knowledge, edge analysis, Image color analysis, augmented reality system, edge detection, Labeling, rendering (computer graphics), visual saliency algorithm, Image edge detection, image region, adaptive rendering technique, Linear programming, augmented reality browser, view management technique, cultural heritage, Augmented reality, labels, maintenance application, digital information, Layout, depth cue appearance, geometry, Information Interfaces and Presentation]
Tablet versus phone: Depth perception in handheld augmented reality
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Augmented Reality (AR) applications on mobile devices like smartphones and tablet computers have become increasingly popular. In this paper, for the first time in the AR domain, we present: (1) the influence of different handheld displays and (2) the exocentric depth perception. Unlike egocentric depth perception, exocentric depth perception has not been investigated in AR. We have selected a suitable vision-based tracking method for our user studies based on a set of evaluations. Then we have investigated the effect of display size and resolution through two user studies. One study investigated the effect of different displays on egocentric depth perception. The other study investigated the effect of displays on exocentric and ordinal depth perception. Interestingly, we noticed depth compression is less when using a mobile phone, while participants subjectively preferred a tablet. A similar effect was also noticed in exocentric depth perception. The tablet provided significantly better ordinal depth perception and faster response time than the mobile phone. In both of the studies, we found no effect of the AR X-ray visualization on depth perception. Both egocentric and exocentric distances were underestimated.
[Visualization, augmented reality X-ray visualization, Augmented Reality, Mobile communication, augmented reality, handheld augmented reality application, Mobile handsets, ordinal depth perception, tablet computers, X-ray Visualization, exocentric depth perception, Depth Perception, handheld displays, smartphones, notebook computers, vision based tracking method, Target tracking, Estimation, User Evaluation, egocentric depth perception, mobile device, Handheld Displays, mobile phone, Outdoor Environment, Cameras, mobile handsets]
A hand-held AR magic lens with user-perspective rendering
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
In this paper we present a user study evaluating the benefits of geometrically correct user-perspective rendering using an Augmented Reality (AR) magic lens. In simulation we compared a user-perspective magic lens against the common device-perspective magic lens on both phone-sized and tablet-sized displays. Our results indicate that a tablet-sized display allows for significantly faster performance of a selection task and that a user-perspective lens has benefits over a device-perspective lens for a selection task. Based on these promising results, we created a proof-of-concept prototype, engineered with current off-the-shelf devices and software. To our knowledge, this is the first geometrically correct user-perspective magic lens.
[Performance evaluation, phone-sized display, Visualization, lenses, user study, MR simulation, user-perspective rendering, augmented reality, hand-held AR magic lens, prototype, off-the-shelf device, magic lens, tablet-sized display, Games, Virtual reality, geometrically correct user-perspective magic lens, Cameras, Sensors, rendering (computer graphics), Lenses, User-perspective view]
3D referencing techniques for physical objects in shared augmented reality
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
We introduce an augmented reality referencing technique for shared environments that is designed to improve the accuracy with which one user can point out a real physical object to another user. Our technique, GARDEN (Gesturing in an Augmented Reality Depth-mapped ENvironment), is intended for use in otherwise unmodeled environments in which objects in the environment, and the hand of the user performing a selection, are interactively observed by a depth camera, and users wear tracked see-through displays. We present the results of a user study that compares GARDEN against existing augmented reality referencing techniques, as well as the use of a physical laser pointer. GARDEN performed significantly more accurately than all the comparison techniques when the participating users have sufficiently different views of the scene, and significantly more accurately than one of these techniques when the participating users have similar perspectives.
[shared environments, Collaborative mixed/augmented reality, 3D referencing techniques, augmented reality, Augmented reality, wear tracked see-through displays, Laser theory, Accuracy, physical laser pointer, augmented reality referencing technique, physical objects, shared augmented reality, unmodeled environments, GARDEN, gesturing in an augmented reality depth-mapped environment, Streaming media, display devices, Cameras, Animation, depth camera, referencing technique]
Quick viewpoint switching for manipulating virtual objects in hand-held augmented reality using stored snapshots
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Magic-lens style augmented reality applications allow users to control camera pose easily by manipulating a portable hand-held device and provide immediate visual feedback. However, strategic vantage points must often be revisited repeatedly, adding time and error and taxing memory. We describe a new approach that allows users to take snapshots of augmented scenes that can be virtually revisited at later times. The system stores still images of scenes along with camera poses, so that augmentations remain dynamic and interactive. Users can manipulate virtual objects while viewing snapshots, instead of moving to real-world views. We present a study comparing performance in snapshot and live mode conditions in a task in which a virtual object must be aligned with two pairs of physical objects. Proper alignment requires sequentially visiting two viewpoints. Participants completed the alignment task significantly faster and more accurately using snapshots than when using the live mode. Moreover, participants preferred manipulating virtual objects using snapshots to the live mode.
[Legged locomotion, live mode condition, Visualization, stored snapshots, Switches, augmented reality, user interfaces, visual feedback, quick viewpoint switching, virtual object manipulation, virtual object, Augmented reality, virtual travel, hand-held augmented reality, viewpoint alignment task, Handheld computers, magic-lens style augmented reality, physical object, augmented scene, Cameras, snapshot condition, Arrays, Feeds]
Using children's developmental psychology to guide augmented-reality design and usability
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Augmented reality (AR) designers have great potential to enrich children's lives through AR experiences in education and entertainment. A significant difficulty in designing for children is that tremendous physical and cognitive development occurs across the first 10 years of life, and the changes in children's capabilities and limitations impact how these users respond to AR designs. Currently, little is known about how developmental changes relate to AR designs, or what AR designs are effective for young children. In this work, we focus on children 6-9 years old, presenting several concepts from developmental psychology and discussing how these relate to AR designs. Specifically, we investigate children's skills in the categories of motor abilities, spatial cognition, attention, logic and memory, and we discuss the relationship of these skills to current and hypothetical AR designs. Through this work, we intend to strengthen the field's understanding of AR usability and design, resulting in the generation of effective AR experiences for young users.
[Performance evaluation, education, AR designs, memory, entertainment, Psychology, human factors, Augmented Reality, motor abilities, augmented reality, spatial cognition, children skills, Guidelines, augmented-reality usability, psychology, Children, Mixed Reality, children developmental psychology, augmented-reality design, physical development, Muscles, Interaction Design, Augmented reality, Games, software reusability, cognitive development, Cameras, attention, AR experiences, logic]
Learning task structure from video examples for workflow tracking and authoring
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
We present a robust real-time capable and simple framework for segmenting video sequences and live-streams of manual workflows into the comprising single tasks. Using classifiers trained on these segments we can follow a user that is performing the workflow in real-time as well as learn task variants from additional video examples. Our proposed method neither requires object detection nor high-level features. Instead we propose a novel measure derived from image distance that evaluates image properties jointly without prior segmentation. Our method can cope with repetitive and free-hand activities and the results are in many cases comparable or equal to manual task segmentation. One important application of our method is the automatic creation of a step-by-step task documentation from a video demonstration. The entire process to automatically create a fully functional augmented reality manual will be explained in detail and results are shown.
[authoring, image classification, Manuals, augmented reality, video example, Training, live-streams, image distance, image segmentation, task documentation, classifier, Robustness, video segmentation, video signal processing, image sequences, workflow tracking, image properties, manual task segmentation, video sequence, learning task structure, task variant, Augmented reality, Image segmentation, Motion segmentation, Current measurement, video demonstration]
Mobile augmented reality for cultural heritage: A technology acceptance study
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
We have developed a mobile augmented reality application with historical photographs and information about a historical street. We follow a design science research methodology and use an extended version of the technology acceptance model (TAM) to study the acceptance of this application. A prototype has been developed in accordance with general principles for usability design, and two surveys have been conducted. A web survey with 200 participants that watched a short video demonstration of the application to validate the adapted acceptance model, and a street survey, where 42 participants got the opportunity to try the application in a live setting before answering a similar questionnaire and provide more concrete feedback. The results show that both perceived usefulness and perceived enjoyment has a direct impact on the intention to use mobile augmented reality applications with historical pictures and information. Further a number of practical recommendations for the development and deployment of such systems are provided.
[Adaptation models, TAM, Mobile communication, augmented reality, history, Multimedia Information Systems, Cultural differences, cultural heritage, Augmented reality, perceived usefulness, H.5.1 [Multimedia Information Systems, design science research methodology, mobile computing, usability design, Prototypes, historical photograph, historical street, Cities and towns, Cameras, mobile augmented reality, technology acceptance model, perceived enjoyment, user centred design]
Augmented reality during angiography: Integration of a virtual mirror for improved 2D/3D visualization
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Visualizing 2D and 3D anatomical information simultaneously within an X-ray image is challenging. Multiple monitors are required in the operating room to enable visualization of anatomical data by the surgeon. Consequently, this results in an interruption during the operation for proper assessment of information. In this paper, we introduce an interactive visualization of the 3D data from new perspectives including visualizing a virtual mirror from the same viewpoint as the X-ray source. The main contribution is our development of a complete angiographic visualization system that displays simultaneous 2D X-ray and 3D anatomical information in a common monitor for the surgeon in the operating room. No previous works have conceived the integration of the virtual mirror into the projection geometry of a C-arm fluoroscope. In total, 24 participants were asked to assess the benefits of the angiographic virtual mirror with different colour-depth encodings. The results of our feasibility study show a clear improvement when deciphering the true positions of aneurysms in X-ray. Lastly, color depth encoding improves correspondence between the 3D vasculature displayed in the virtual mirror to their projection images in X-ray.
[Visualization, virtual mirror, multiple monitors, Angiography, Augmented Reality, Aneurysm, surgeon, 3D vasculature, augmented reality, X-ray image, X-ray aneurysms, X-ray source, 3D data interactive visualization, X-ray imaging, deciphering, Image color analysis, data visualisation, Virtual Mirror, simultaneous 2D X-ray anatomical information displays, C-arm fluoroscope projection geometry, image colour analysis, X-ray, Mirrors, medical image processing, operating room, colour-depth encoding, 2D/3D Registration, three-dimensional displays, Encoding, diagnostic radiography, 2D anatomical information visualization, simultaneous 3D anatomical information displays, 3D anatomical information visualization, Data visualization, color depth encoding, Rendering (computer graphics), image coding, surgery, angiographic visualization system]
Making Pretense Visible and Graspable: An augmented reality approach to promote pretend play
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Children with autism are often found to lack facility for pretend play. It is believed that this deficit is linked to linguistic, social and creativity competencies in autism. We observe that both Augmented Reality (AR) and pretend play involve processing of information that is coupled with real scenes while not necessarily being directly perceived. This research therefore examines the potential of using AR technologies to promote pretend play behaviors in children with autism. As an initial outcome, we present the design and implementation of an AR system that aims to enhance the comprehension and flexibility of object substitution during pretend play.
[Airplanes, handicapped aids, creativity competency, Augmented Reality, pretend play, pretend play behavior, Educational institutions, augmented reality, autistic children, Cotton, Augmented reality, autism, linguistic competency, Bridges, Autism, AR technology, social competency, Mirrors]
An interactive Augmented Reality system: A prototype for industrial maintenance training applications
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
In this paper, we present an innovative Augmented Reality prototype designed for industrial education and training applications. The system uses an Optical See-Through HMD integrating a calibrated camera and a laser pointer to interactively augment an industrial object with virtual sequences designed to train a user for specific maintenance tasks. The training leverages user interactions by simply pointing on a specific object component. The architecture of our prototype involves two main vision-based modules : camera localization and user-interaction handling. The first module includes markerless trackers for camera localization, which can deal with partial occlusions and specular reflections on the metallic object surfaces. In the second module, we developed fast image processing methods for red laser dot tracking. By combining these processing elements, the proposed system is able to interactively augment in real time an industrial object making the learning process more interesting and intuitive.
[calibrated camera, MR/AR applications [industrial and military MR/AR applications, industrial maintenance training, maintenance engineering, production engineering computing, MR/AR applications, augmented reality, user-interaction handling, object component, industrial and military MR/AR applications, industrial education, Training, cameras, Prototypes, computer based training, Laser modes, Real-time systems, Sensors, image processing methods, vision-based modules, image processing, laser pointer, partial occlusions, metallic object surfaces, helmet mounted displays, Augmented reality, maintenance tasks, virtual sequences, interactive augmented reality system, industrial object, interaction techniques for MR/AR, target tracking, camera localization, specular reflections, laser beam applications, Cameras, red laser dot tracking, human computer interaction, optical see-through HMD, industrial training, Laser applications]
Superman-like X-ray vision: Towards brain-computer interfaces for medical augmented reality
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
This paper describes first steps towards a Superman-like X-ray vision where a brain-computer interface (BCI) device and a gaze-tracker are used to allow the user controlling the augmented reality (AR) visualization. A BCI device is integrated into two medical AR systems. To assess the potential of this technology first feedback from medical doctors is gathered. While in this pilot study not the full range of available signals but only electromyographic signals are used, the medical doctors provided very positive feedback on the use of BCI for medical AR.
[Visualization, Superman-like X-ray vision, brain-computer interfaces, AR visualization, brain-computer interface, gaze-tracker, H.5.1 [Information Interfaces and Presentation, augmented reality, electromyographic signal, Augmented reality, X-ray imaging, BCI device, medical signal processing, Surgery, data visualisation, electromyography, object tracking, medical augmented reality, medical AR system, Electromyography, Information Interfaces and Presentation, visual evoked potentials, Monitoring, Biomedical imaging]
Integrating 3D object detection, modelling and tracking on a mobile phone
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
This paper presents a complete system on a camera phone that integrates a texture less 3D object detector together with in-situ modelling and tracking. The result is a suite intended for AR applications on the move where objects can be captured, tracked and with the automated detection providing a bridge to either initialize tracking or resume it after measurement loss. The object detector training is online and in-situ and benefits from tight integration with the modelling and tracking processes.
[object capturing, Solid modeling, Tracking, mobile camera phone, Image edge detection, Computational modeling, measurement loss, automatic texture-less 3D object detector, augmented reality, object detection, AR applications, image texture, cameras, online object detector training, 3D object detection integration, Constellation diagram, in-situ 3D object modelling integration, in-situ 3D object tracking integration, Detectors, Object detection, object tracking, mobile handsets]
Hybrid virtual-physical entities
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Mixed reality (MR) combines virtual elements with the real, physical world. Virtual elements can be more dynamic - easily moving around or changing appearance. Physical elements are more static but improve immersion and presence. The combination of the two creates dynamic, immersive MR environments. However, individual entities in MR environments, e.g. an object or a character, are typically either purely physical or purely virtual. We propose using both virtual and physical elements in a hybrid virtual-physical entity. We implemented this idea with a hybrid virtual human and ran several user studies. These studies helped us develop guidelines for implementing hybrid entities.
[Legged locomotion, Computing Milieux, virtual reality, Avatars, Humans, hybrid virtual-physical entity, Computer Graphics, Animatronics, virtual element, mixed reality, real physical world, I.3.7 [Computer Graphics, hybrid virtual human, MR environment, Face, physical element]
ClonAR: Rapid redesign of real-world objects
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
ClonAR enables users to rapidly clone and edit real-world objects. First, real-world objects can be scanned using KinectFusion. Second, users can edit the scanned objects in our visuo-haptic Augmented Reality environment. Our whole pipeline does not use mesh representation of objects, but rather Signed Distance Fields, which are the output of KinectFusion. We directly render Signed Distance Fields haptically and visually. We do direct haptic rendering of Signed Distance Fields, which is faster and more flexible than rendering meshes. Visual rendering is performed by our custom-built raymarcher, which facilitates realistic illumination effects like ambient occlusions and soft shadows. Our prototype demonstrates the whole pipeline. We further present several results of redesigned real-world objects.
[Artificial, ambient occlusion, haptic interfaces, Human factors, augmented reality, USA Councils, Lighting, Real-time systems, soft shadow, rendering (computer graphics), signed distance fields, KinectFusion, ClonAR, Cloning, augmented and virtual realities, Information Systems, Haptic interfaces, haptic rendering, Augmented reality, realistic illumination effect, visual rendering, custom-built raymarcher, H.5.1. [Information Interfaces and Presentation, Rendering (computer graphics), real-world object redesign, visuo-haptic augmented reality, Information Interfaces and Presentation, Haptic I/O]
Relationship between features of augmented reality and user memorization
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
The objective of this study is to investigate the relationship between the features of augmented reality (AR) and human memorization ability. The basis of this relation is derived from the following features. The AR feature is that AR can provide information associated with specific locations in the real world. The feature of human memory is that humans can easily memorize information if the information is visually associated with specific locations. To investigate this relation, we conduct a pilot user study in which blocks are picked from some drawers. As a result, significant differences are found between a situation in which visual information is displayed at the location of each drawer in the real world and that in which textual information is displayed at an unrelated location.
[Visualization, textual information, Humans, Psychology, augmented reality, H.1.2 [Models and Principles, Models and Principles, Augmented reality, Accuracy, human memorization ability, real world, data visualisation, Abstracts, Information Interfaces and Presentation, human memory, Assembly, AR feature, user memorization, visual information]
Occlusion capable optical see-through head-mounted display using freeform optics
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Most state-of-the-art optical see-through head-mounted display (OST-HMD) lacks mutual occlusion capability between computer-rendered and real objects so that the virtual view through an OST-HMD appears &#x201C;ghost-like&#x201D;, floating in the real world. In this paper, we demonstrated a light-weight, compact OST-HMD with mutual occlusion capability by exploring a highly innovative optical approach based on emerging freeform optical design and fabrication technologies. Our approach enabled us to achieve an occlusion-capable OST-HMD system with a very compelling form factor and high optical performance. The proposed display technology is designed for highly efficient liquid crystal on silicon (LCoS) type spatial light modulator (SLM) and bright Organic LED (OLED) microdisplay, which is capable of working in both indoor and outdoor environments. Our current design offered a 1280&#x00D7;1024 color resolution with a field of view (FOV) of 40 degrees and lightweight optics about 30 grams per eye.
[Microdisplays, Biomedical optical imaging, microdisplays, organic light emitting diodes, Augmented Reality, computerised instrumentation, occlusion-capable optical see-through head-mounted display, bright organic LED icrodisplay, lightweight optics, ghost-like, Optical design, Optical device fabrication, indoor environments, fabrication technologies, compact OST-HMD, liquid crystal displays, Optical imaging, helmet mounted displays, liquid crystal on silicon type spatial light modulator, Head Mounted Display, freeform optics, hidden feature removal, Mutual Occlusion, outdoor environments, Optical See-through, LCoS type SLM, computer graphics, freeform optical design, mutual occlusion capability, occlusion-capable OST-HMD system, Adaptive optics, bright OLED microdisplay]
Recreating the parallax effect associated with Fishtank VR in a Real-Time telepresence system using head-tracking and a robotic camera
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
This project aims to simulate the effect of Fishtank virtual reality in a telepresence system by using head-tracking and real-time control of a robotic camera. Despite the use of the term telepresence, the focus is here not on two-way communication between persons in separate spaces, but in creating a strong sense of physical presence by making the observed scene seem to be an extension of the room. The observer can thus interact with a 3D stage as though the screen were a window into the remote space. The research carried out here is unprecedented in that, to the authors' knowledge, nobody has employed one of the fundamental aspects of the Fishtank concept, namely the alignment of viewer-screen distance to camera-stage distance, in conjunction with a head-coupled camera system. The implementation is described followed by a brief discussion of its limitations and suggestions for future developments.
[Head, virtual reality, Fishtank VR, Tracking, camera-stage distance, Robot vision systems, telerobotics, head-tracking control, Fishtank concept, parallax effect, robotic camera, cameras, Cameras, Real-time systems, position control, realtime telepresence system, viewer-screen distance, control engineering computing, head-coupled camera system]
Fractal marker fields: No more scale limitations for fiduciary markers
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
One limitation of existing fiduciary markers is that the camera motion is tightly limited: the marker (one of the markers) must be visible and it must be observed at a proper scale. This paper introduces a fractal structure of markers similar to matrix codes (such as QR-code or Data Matrix): the Fractal Marker Field. The FMF allows for embedding markers of a virtually unlimited number of scales. At the same time, for each of the scales it guarantees a constant density of markers at that scale over the whole marker field's surface. The Fractal Marker Field can provide unprecedented freedom of motion to camera-based augmented reality applications.
[fractal marker field, embedding marker, fractal structure, data matrix, camera-based augmented reality, marker constant density, augmented reality, Fractals, Calibration, camera motion, matrix code, Augmented reality, fractals, image motion analysis, cameras, Image color analysis, QR-code, Cameras, marker field surface, Reliability, fiduciary marker, FMF, Manganese]
Distance-based modeling and manipulation techniques using ultrasonic gloves
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
We present a set of distance-based interaction techniques for modeling and manipulation, enabled by a new input device called the ultrasonic gloves. The ultrasonic gloves are built upon the original design of the pinch glove device for virtual reality systems with a tilt sensor and a pair of ultrasonic transducers in the palms of the gloves. The transducers are distance-ranging sensors that allow the user to specify a range of distances by natural gestures such as facing the palms towards each other or towards other surfaces. The user is able to create virtual models of physical objects by specifying their dimensions with hand gestures. We combine the reported distance with the tilt orientation data to construct virtual models. We also map the distance data to create a set of affine transformation techniques, including relative and fixed scaling, translation, and rotation. Our techniques can be generalized to different sensor technologies.
[Solid modeling, pinch glove device, modeling, distance-ranging sensor, distance-based techniques, ultrasonic gloves, rotation technique, augmented reality, Acoustics, user interfaces, translation technique, Ultrasonic transducers, hand gesture, tilt sensor, Ultrasonic variables measurement, distance-based manipulation technique, relative scaling technique, tilt orientation data, manipulation, data gloves, sensor technology, distance-based interaction technique, Thumb, Augmented reality, virtual reality system, sensors, distance-based modeling technique, affine transformation technique, fixed scaling technique, virtual model, ultrasonic transducer, solid modelling]
A GPGPU accelerated descriptor for mobile devices
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
We present a modified upright SURF feature descriptor for mobile phone GPUs. Our implementation called uSURF-ES is multiple times faster than a comparable CPU variant on the same device. Our results proof the feasibility of modern mobile graphics accelerators for GPGPU tasks especially for the detection phase in natural feature tracking used in Augmented Reality applications.
[mobile graphics accelerators, Graphics processing units, SURF feature descriptor, INFORMATION INTERFACES AND PRESENTATION, Mobile communication, Mobile handsets, Encoding, mobile phone GPU, IMAGE PROCESSING AND COMPUTER VISION, graphics processing units, Augmented reality, uSURF-ES, computer graphics, mobile computing, H.5.1 [INFORMATION INTERFACES AND PRESENTATION, GPGPU accelerated descriptor, mobile devices, natural feature tracking, augmented reality applications, Feature extraction, detection phase, Androids, mobile handsets]
Using mixed reality to map human exercise demonstrations to a robot exercise coach
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Obesity is a growing health problem in the United States, especially among children. Indicators show that the rate of obesity for children age 12-19 years old has risen from 5% percent to 18% over the last ten years. To deal with the obesity epidemic, a number of technology interventions, including the use of robotics and virtual reality games, have arisen to motivate youth to become physically active. The difficulty though lies in providing a tool for health professionals to embed established clinical health protocols into these technologies. As such, in this paper we present a mixed reality system that translates physical demonstrations of various exercise protocols into movements for a robotic agent. This is accomplished by mapping real-time data from an RGB-D sensor to a robotic exercise coach. Details of the system are discussed and results from evaluation with 20 human subjects are provided.
[Pediatrics, Humans, health problem, robot kinematics, augmented reality, united states, clinical health protocols, mixed reality system, mobile robots, robot exercise coach, RGB-D sensor, computer games, Robot sensing systems, 1.2.9: Artificial Intelligence&#x2014;Robotics, humanoid robots, protocols, virtual reality games, Joints, physical demonstration translation, health professionals, Obesity, robotic agent, human exercise demonstration mapping, sensors, Robot kinematics, exercise protocols, obesity epidemic rate]
AR marker hiding based on image inpainting and reflection of illumination changes
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
This paper proposes a new method of diminished reality which removes AR markers from a user's view image in real time. To achieve natural marker hiding, assuming that an area around a marker is locally planar, the marker area in the first frame is inpainted using the rectified image to achieve high-quality inpainting. The unique inpainted texture is overlaid on the marker region in each frame according to camera motion for geometric consistency. Both global and local luminance changes around the marker are separately detected and reflected to the inpainted texture for photometric consistency.
[Measurement, inpainted texture reflection, high-quality inpainting, diminished reality, image rectification, H.5.1 [Information Interfaces and Presentation, augmented reality, photometry, cameras, inpainted texture detection, natural marker hiding, Lighting, local luminance changes, Real-time systems, geometric consistency, marker region, global luminance changes, marker area, camera motion, Indexes, lighting, Augmented reality, image texture, image motion analysis, Interpolation, image inpainting-based AR marker hiding, photometric consistency, Cameras, Information Interfaces and Presentation, illumination changes reflection, real time view image, data encapsulation]
Interface design for an inexpensive hands-free collaborative videoconferencing system
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
In this paper an interaction framework for AR enhanced video conferencing is presented. The goal is to provide a cheap and portable system based on a combination of commodity Kinect cameras and regular computer screens. These conditions necessitate the use of contact free interaction methods. The interaction framework presented in this paper is specifically suited for remotely presenting, sharing and annotating visual data such as images, presentation slides and 3D objects. In the proposed system all data is represented by freely manipulable 3D objects which are augmented into the camera views. These representations are integrated into a differentiated ownership scheme, allowing for operations such as spatially managed data sharing. The suitability of different interaction paradigms with regards to this usage scenario is examined. Furthermore, occlusion and collision management between virtual objects and real obstacles is enabled by integrating basic models of the environment.
[Visualization, presentation slides, computer screens, collision management, augmented reality, user interfaces, portable system, telecommunication computing, teleconferencing, contact free interaction methods, H.4.3 [Information Systems Applications, Real-time systems, virtual objects, video communication, remote visual data sharing, interface design, Information Systems Applications, Kinect cameras, hands-free collaborative videoconferencing system, images, remote visual data presentation, 3D objects, Augmented reality, remote visual data annotation, Teleconferencing, AR enhanced video conferencing, Collaboration, occlusion, Cameras]
Texture-less planar object detection and pose estimation using Depth-Assisted Rectification of Contours
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
This paper presents a method named Depth-Assisted Rectification of Contours (DARC) for detection and pose estimation of texture-less planar objects using RGB-D cameras. It consists in matching contours extracted from the current image to previously acquired template contours. In order to achieve invariance to rotation, scale and perspective distortions, a rectified representation of the contours is obtained using the available depth information. DARC requires only a single RGB-D image of the planar objects in order to estimate their pose, opposed to some existing approaches that need to capture a number of views of the target object. It also does not require to generate warped versions of the templates, which is commonly needed by existing object detection techniques. It is shown that the DARC method runs in real-time and its detection and pose estimation quality are suitable for augmented reality applications.
[Shape, Transforms, augmented reality, object detection, scale distortion, texture-less objects, perspective distortion, feature extraction, pose estimation, Real-time systems, template contour, RGB-D cameras, rotation distortion, depth information, Pose estimation, Estimation, contours matching, image matching, Augmented reality, image texture, DARC, Object detection, Cameras, texture-less planar object detection, RGB-D camera, depth-assisted rectification of contours]
Development of a ubiquitous learning system for dexterous hand operation
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
It is not easy to learn the dexterous finger movements of a skilled person due to flexibility and complexity. Therefore, a lot of training and effort is needed for a novice in order to acquire proficiency. The role model and feedback information of experienced persons is important for learning skilled movements. In this paper, we evaluate a ubiquitous learning system for dexterous hand operation using the finger motion capture data measured by the Hand-MoCap system. Through this system, a novice can learn under the guidance of the virtual 3D hand reconstructed using the master's motion capture data. We quantitatively evaluate the effectiveness of the system.
[virtual reality, virtual 3D hand, Thumb, dexterous finger movement, ubiquitous learning system, Educational institutions, training, ubiquitous computing, Dexterous finger movement, Augmented reality, Learning systems, Training, skilled movement learning, finger motion capture data, Hand-MoCap system, computer aided instruction, dexterous hand operation, motion capture, Spatial resolution]
A waist-mounted ProCam system for remote collaboration
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
We propose a waist-mounted projector-camera (ProCam) system for asymmetric remote collaboration. A wearable camera is often used to transmit a worker's situation to a remote instructor, however 3D structure of the worker's environment is not always available and the instructor has a minimal flexibility in changing the camera's viewpoint. A stationary 3D measurement system is also commonly used for remote collaboration, however a narrow measurement area and occlusion from a worker's body can be a severe problem. Our waist-mounted ProCam system reconstructs worker's environment in real-time without occlusion from the worker's body. The remote instructor can give instructions simply by drawing annotations on the reconstructed environment on screen, and they are properly projected in front of the worker. Structured-light based reconstruction, vision-based localization, and visual annotation projection, are processed synchronously with a camera and modified shutter glasses so that both the camera and the worker observe only the information they need. Experimental results show that users prefer our system to a stationary ProCam system.
[measurement systems, Solid modeling, Area measurement, H.5.1 [Information Interfaces and Presentation, Glass, augmented reality, Electronic mail, remote instructor, remote collaboration, cameras, stationary 3D measurement system, structured-light based reconstruction, augmented reality system, groupware, camera viewpoint, projector-camera system, worker environment, Educational institutions, vision-based localization, waist-mounted ProCam system, visual annotation projection, Collaboration, wearable camera, Cameras, Information Interfaces and Presentation, computer aided instruction, solid modelling]
Alice's adventures in an immersive mixed reality environment
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Immersive mixed reality environments offer new possibilities to reproduce reality or embodied presence with constructing elaborate fantasy worlds and provide to the user an intense and seemingly real experience. Mixed reality gives possibilities to create deeply dimensional narratives and simulations that put the user in the center of the action. In this work we describe an interactive mixed reality installation named Alice, consisting of six separate stages based on the narrative &#x201C;Alice's Adventures in Wonderland&#x201D;. To be able to achieve the intended experience we have to build complex and heterogeneous distributed system, composed of sensors, actuators, virtual reality, application components and variety of processing components that manage the flow of context information between the sensors/actuators and applications.
[virtual reality, application components, distributed processing, distributed system, deeply dimensional narratives, immersive mixed reality environment, Immersive mixed reality, cultural computing, humanities, sensors, actuators, Abstracts, Alices adventures in wonderland, Argon, Robots, elaborate fantasy worlds]
Lighty: A painting interface for room illumination by robotic light array
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
We propose an AR-based painting interface that enables users to design an illumination distribution for a real room using an array of computer-controlled lights. Users specify an illumination distribution of the room by painting on the image obtained by a camera mounted in the room. The painting result is overlaid on the camera image as contour lines of the target illumination intensity. The system runs an optimization interactively to calculate light parameters to deliver the requested illumination condition. In this implementation, we used actuated lights that can change the lighting direction to generate the requested illumination condition more accurately and efficiently than static lights. We built a miniature-scale experimental environment and ran a user study to compare our method with a standard direct manipulation method using widgets. The results showed that the users preferred our method for informal light control.
[direct manipulation method, augmented reality, user interfaces, AR-based painting interface, computer-controlled lights, target illumination intensity, Computer Graphics, illumination distribution, I.3.6 [Computer Graphics, robots, camera, control engineering computing, contour lines, miniature-scale experimental environment, informal light control, Computing Methodologies, Lighty, Augmented reality, image sensors, robotic light array, static lights, lighting control, room illumination, Numerical Analysis, Information Interfaces and Presentation]
Augmented prototyping of 3D rigid curved surfaces
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
This paper presents an application of Augmented Reality (AR) in Rapid Prototyping (RP) of non-textured rigid curved surfaces. By enhancing the prototypes with AR, evaluation of its design and aesthetic concepts in real-time becomes easier, saving time and production costs. In our application, no fiducial markers are required and the CAD model used to build the prototype is applied in an edge-based tracking system specially designed to deal with curved shapes. Results from a pilot user study comparing the use of a 3D software and the proposed application are also presented.
[Solid modeling, Visualization, Shape, CAD model, 3D rigid curved surfaces, rapid prototyping, computational geometry, augmented reality, Augmented reality, non-textured curved surfaces, product design, curved shapes, Prototypes, model-based tracking, Software, edge-based tracking system, Surface texture, nontextured rigid curved surface, augmented prototyping]
Digital map based pose improvement for outdoor Augmented Reality
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
With popularization of smart phones, needs for location based services (LBS), which is one of the most promising Augmented Reality applications, increased rapidly. However, accuracy of most commercially available Global Positioning Systems (GPS) is below levels for providing practically meaningful location based information. Especially when there are high building structures nearby, GPS location measurements are known to be erroneous and deviant. In this paper, we present a computer vision based method for improving user's position and orientation for outdoor Augmented Reality with initial values obtained from a GPS and a digital compass. Given a digital map, our goal was to determine corresponding buildings visible in the camera image and improve the user location and orientation. In average, our method improved (14.4m, 3.3m) in position and 2.8 degrees in orientation. Our method is suitable for mobile services in urban environments where tall buildings degrade GPS signals.
[GPS signal, structural engineering, location based information, urban environment, mobile service, user orientation, augmented reality, GPS, cameras, user location, Accuracy, mobile computing, camera image, LBS, outdoor augmented reality, digital map, user position, building structure, Image edge detection, Buildings, location based service, digital compass, smart phone, smart phones, tall building, cartography, GPS location measurement, Compass, Augmented reality, Global Positioning System, pose improvement, computer vision, depth map, Cameras, outdoor tracking]
Supervised classification for customized intraoperative augmented reality visualization
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
In this paper, we present a fusion algorithm supplemented with appropriate visualization by selecting relevant information from different modalities in mixed and augmented reality (AR). This encompasses a learning based method upon relevance of information, defined by an expert, which ultimately enables confident interventional decisions based on mixed reality (MR) images. The performance of our developed fusion and tailored visualization techniques was evaluated by employing X-ray/optical images during surgery and validated qualitatively using a 5-point Likert scale. Our observations indicated that the proposed technique provided semantic contextual information about underlying pixels and in general was preferred over the traditional pixel-wise linear alpha-blending method.
[Visualization, Biomedical optical imaging, interventional decision, image classification, augmented reality, sensor fusion, X-ray image, X-ray imaging, supervised classification, 5-point Likert scale, Surgery, data visualisation, information relevance, X-ray, learning (artificial intelligence), medical image processing, pixel-wise linear alpha-blending method, Optical mixing, MR, Optical imaging, diagnostic radiography, customized intraoperative augmented reality visualization, Medical Augmented Reality, CamC, Augmented reality, AR, mixed reality, learning based method, Relevant Information, optical image, fusion algorithm, Fusion, biomedical optical imaging, surgery]
Why should my students use AR? A comparative review of the educational impacts of augmented-reality
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Augmented reality is increasingly reaching young users such as elementary-school and high-school children, as their parents and teachers become aware of the technology and its potential for education. Although research has shown that AR systems have the potential to improve student learning, the educator community does not clearly understand the educational impact of AR, nor the factors which impact the educational effectiveness of AR. In this poster, we analyse 32 publications that have previously compared learning effects of AR vs non-AR applications. We identify a list of positive and negative impacts of AR on student learning, and identify potential underlying causes for these effects. Our vision is that educational initiatives will exploit these factors, in order to realize the full potential of AR to enrich learner's lives.
[Computers, student learning, Solid modeling, Augmented Reality, Human factors, Media, augmented reality, educational impacts, augmented-reality, AR learning effects, educator community, high-school children, Augmented reality, Education, educational effectiveness, teachers, educational initiatives, Machine learning, elementary-school children, computer aided instruction, Children, AR systems, Human Factors, parents]
Effect of eye and body movement on augmented reality in the manufacturing domain
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Understanding the eye and body movements required to perform large vehicle assembly tasks is important for developing a mobile support system for mechanics, and tracking user movements with regard to the surrounding environment is critical for designing a wearable Augmented Reality (AR) systems. This poster summarizes a study measuring the eye and body movements of mechanics performing assembly activities in a live manufacturing environment. It reviews our quantitative analysis of eye movements and qualitative analysis of body movements and describes the implications of this data in terms of the feasibility and potential utility of using a mobile AR application to support manufacturing. We found that the mechanics' eye movements ranged over a slightly larger field than the eye movements reported in previous research because of constraints imposed by some body positions required in manufacturing tasks.
[Tracking, production, human factors, manufacturing processes, saccadic eye movement, Mobile communication, augmented reality, user movements tracking, biomechanics, Vehicles, mobile computing, augmented reality system, mobile support system, manufacturing tasks, manufacturing, gaze shift, live manufacturing environment, eye movements quantitative analysis, Assembly, heads-up display, computer aided manufacturing, assembling, eye movements mechanics, Augmented reality, eye, manufacturing domain, body movements qualitative analysis, eye tracking, Cameras, mobile AR application]
Generation of virtual display surfaces for in-vehicle contextual augmented reality
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
In-vehicle contextual augmented reality (I-CAR) has the potential to provide novel visual feedback to drivers for an enhanced driving experience. To enable I-CAR, we present a parametrized road trench model (RTM) for dynamically extracting display surfaces from a driver's point of view that is adaptable to constantly changing road curvature and intersections. We use computer vision algorithms to analyze and extract road features that are used to estimate the parameters of the RTM. GPS coordinates are used to quickly compute lighting parameters for shading and shadows. Novel driver-based applications that use the RTM are presented.
[Roads, augmented reality, RTM model, GPS coordinates, visual feedback, in-vehicle contextual augmented reality, RTM parameter estimation, Vehicles, driver information systems, feature extraction, Lighting, lighting parameter, parameter estimation, road feature extraction, road intersection, road traffic, Image edge detection, road curvature, I-CAR, lighting, Sun, Global Positioning System, driving experience, driver-based application, computer vision, Cameras, road trench model, computer vision algorithm, virtual display surface generation]
Uniform Marker Fields: Camera localization by orientable De Bruijn tori
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
In various applications, a wider area needs to be covered by fiduciary markers but a large marker cannot be used because only a fraction of the area is to be viewed by the camera. Such an area can be covered by a number of small markers with unique identifiers. However, with the camera freely moving in the scene and with occluders present, it is difficult to ensure that at least one of the individual markers is completely visible, unless the markers are small and numerous. In that case, the markers are not recognizable from larger distances. In this paper we introduce the concept of Marker Fields which overcome this limitation. The Marker Field covers a large-scale planar (or non-planar) area and it is composed of mutually overlapping partial markers. We propose a particular arrangement of the Marker Field: a Uniform Checker-Board Marker Field, which is a black- and-white checkerboard whose square modules are defined by aperiodic 4-orientable binary n2-window arrays (De Bruijn tori). We propose a genetic algorithm for construction of 4-orientable n2window arrays. We used a supercomputer to synthesize large 4-orientable 42window arrays and offer them publicly for downloading. We prototyped an algorithm for detection of the checkerboard marker fields and measured its performance. When processing input video from a cellphone camera, the algorithm visits only about 5 % of image pixels for reliable detection and the processing time is about 1 ms on a mid-range PC processor. The Uniform Marker Field increases freedom of camera movement, especially with occluders present in the scene. The detection algorithm is efficient and real-time marker field detection will be feasible on ultramobile devices.
[Algorithm design and analysis, supercomputer, fiducial marker, Image edge detection, graph theory, Supercomputers, object detection, genetic algorithms, aperiodic 4-orientable binary n2-window arrays, black-and-white checkerboard, Genetic algorithms, genetic algorithm, camera localization, Cameras, Feature extraction, ultramobile device, orientable De Bruijn tori, large-scale planar area, Reliability, uniform checker-board marker field]
SLAM using both points and planes for hand-held 3D sensors
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
We present a simultaneous localization and mapping (SLAM) algorithm for a hand-held 3D sensor that uses both points and planes as primitives. Our algorithm uses any combination of three point/plane primitives (3 planes, 2 planes and 1 point, 1 plane and 2 points, and 3 points) in a RANSAC framework to efficiently compute the sensor pose. As the number of planes is significantly smaller than the number of points in typical 3D scenes, our RANSAC algorithm prefers primitive combinations involving more planes than points. In contrast to existing approaches that mainly use points for registration, our algorithm has the following advantages: (1) it enables faster correspondence search and registration due to the smaller number of plane primitives; (2) it produces plane-based 3D models that are more compact than point-based ones; and (3) being a global registration algorithm, our approach does not suffer from local minima or any initialization problems. Our experiments demonstrate real-time, interactive 3D reconstruction of office spaces using a hand-held Kinect sensor.
[plane-based 3D models, three plane primitives, image registration, SLAM algorithm, point-based 3D models, correspondence search, I.4.8 [Image Processing and Computer Vision, office spaces, Simultaneous localization and mapping, hand-held 3D sensors, Image Processing and Computer Vision, Real-time systems, Three dimensional displays, RANSAC framework, hand-held Kinect sensor, global registration algorithm, three point primitives, Color, image reconstruction, sensor pose, 3D scenes, interactive 3D reconstruction, image sensors, Current measurement, SLAM (robots), RANSAC algorithm, Cameras, simultaneous localization and mapping algorithm, solid modelling]
Depth perception control by hiding displayed images based on car vibration for monocular head-up display
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
We have developed a novel depth perception control method for a monocular head-up display (HUD) in a car. However, it is difficult to achieve an accurate depth perception in the real world because of car vibration. To resolve this problem, we focus on a property that people complement hidden images by previous continuous observed images. We hide the image on the HUD when the car is vibrated. We aim to point at the accurate depth position by using HUD images with having users compliment the hidden image positions based on the continuous images before car vibration. We developed a car which detects big vibration by an acceleration sensor and is equipped with our monocular HUD. Our method pointed at the depth position within a 3.4 [m] error, which was 2 times more accurate than the previous method does.
[continuous observed image, Image resolution, Navigation, depth control, automobiles, augmented reality, Augmented reality, Vehicles, displayed image hiding, monocular head-up display, Vibrations, head-up displays, navigation, sensors, depth perception control, HUD, Position measurement, car vibration, position control, Acceleration, acceleration sensor, image coding]
Touch-n-Paste: Direct texture transfer interaction in AR environments
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Our Touch-n-Paste allows a user to touch one part of an object, copy and move its texture, and paste it onto another object, directly with his or her hand, in an augmented reality environment. To transfer texture appropriately from one part of an object to another, two texture images are generated by the Least Square Conformal Map (LSCM) technique. Two regions in the texture images corresponding to source and target areas of interest are then obtained using cross-boundary brushes. Target texel values are sampled from corresponding source texels by Moving Least Squares (MLS), and are finally mapped onto the target object.
[Solid modeling, Brushes, LSCM technique, tactile sensors, H.5.1 [Information Interfaces and Presentation, moving least squares, augmented reality, Electronic mail, direct texture transfer interaction, least square conformal map technique, Computer Graphics, Image color analysis, MLS, Computer graphics, source texels, least squares approximations, Clothing, touch-n-paste, Educational institutions, AR environments, augmented reality environment, image texture, target texel values, target areas of interest, Information Interfaces and Presentation, cross-boundary brushes]
Subjective evaluations on perceptual depth of stereo image and effective field of view of a wide-view head mounted projective display with a semi-transparent retro-reflective screen
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
We report two user studies on a wearable hyperboloidal head mounted projective display (HHMPD) with a semi-transparent retro-reflective screen. First experiment revealed that a virtual image is perceived at a similar distance as the real image only when the observation distance is within 2.5m with monocular vision, whereas its threshold is further than 3m with stereo (binocular) vision. Second experiment revealed that users are able to identify visual stimuli in the periphery of the visual field up to &#x00B1;50 degrees in horizontal, while paying attention to a real object in frontal direction.
[Visualization, Shape, subjective evaluations, monocular vision, Electronic mail, Models and Principles, semitransparent retro-reflective screen, Input / Output and Data Communications, visual stimuli, stereo binocular vision, HHMPD, perceived virtual stereo image, Head, perceptual depth, field-of-view, wearable wide-view hyperboloidal head mounted projective display, real image, Educational institutions, helmet mounted displays, Magnetic heads, observation distance, wearable computers, computer vision, stereo image processing, frontal direction, B.4.2 [Input / Output and Data Communications, Biomedical monitoring, visual field periphery]
A component-based approach towards mobile distributed and collaborative PTAM
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Having numerous sensors on-board, smartphones have rapidly become a very attractive platform for augmented reality applications. Although the computational resources of mobile devices grow, they still cannot match commonly available desktop hardware, which results in downscaled versions of well known computer vision techniques that sacrifice accuracy for speed. We propose a component-based approach towards mobile augmented reality applications, where components can be configured and distributed at runtime, resulting in a performance increase by offloading CPU intensive tasks to a server in the network. By sharing distributed components between multiple users, collaborative AR applications can easily be developed. In this poster, we present a component-based implementation of the Parallel Tracking And Mapping (PTAM) algorithm, enabling to distribute components to achieve a mobile, distributed version of the original PTAM algorithm, as well as a collaborative scenario.
[mobile augmented reality applications, object-oriented programming, component-based approach, CPU intensive tasks, parallel tracking and mapping algorithm, Mobile communication, augmented reality, Mobile handsets, smart phones, Servers, desktop hardware, Augmented reality, mobile distributed PTAM, Runtime, mobile computing, sensors, collaborative PTAM, Collaboration, groupware, mobile devices, Cameras, smartphones]
BurnAR: Feel the heat
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Augmented Reality systems that run interactively and in real time, using high quality graphical displays and sensational cues, can create the illusion of virtual objects appearing to be real. This paper presents the design and implementation of BurnAR, a demonstration which enables users to experience the illusion of seeing their own hands burning, which we achieve by overlaying virtual flames and smoke on their hands. Surprisingly, some users reported an involuntary warming sensation of their hands.
[Artificial, virtual object illusion, Human factors, augmented reality, user interfaces, graphical display, flames, Image color analysis, USA Councils, augmented reality system, virtual flame, BurnAR, virtual smoke, augmented and virtual realities, smoke, Information Systems, user experience, Augmented reality, Image segmentation, H.5.1. [Information Interfaces and Presentation, Streaming media, Cameras, Information Interfaces and Presentation, sensational cue, involuntary warming sensation]
Toward a practical wall see-through system for drivers: How simple can it be?
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
This study specifically examines wall see-through visualization for drivers at blind corners to prevent crossing collisions. We believe that realizing the desired effect with the simplest visualization is a key to building practical systems, although previous studies mainly targeted rich visualization as if the wall were actually transparent. We compared several visualization levels using qualitative and quantitative measures based on performance of the driver's collision estimation and the meaning assignment to visual stimuli. The results revealed that displaying only the direction of the obscured vehicle by a small circle is sufficient for collision estimation, although it was perceived as less informative. We also obtained a preliminary result indicating that the meaning assignment performance is significantly lower in a peripheral region of the driver's view. Although both collision estimation and meaning assignment performance are necessary for building an effective system, these results clarify that future studies must specifically examine the meaning assignment performance of the stimuli.
[Visualization, meaning assignment performance, wall see-through visualization, Buildings, driver collision estimation, advanced driver assistance systems, Estimation, peripheral vision, active safety, qualitative measure, Augmented reality, rich visualization, Vehicles, Videos, visual stimuli, quantitative measure, blind corner driving, driver information systems, data visualisation, crossing collision prevention, x-ray vision, wall see-through, Accidents]
A general approach for closed-loop registration in AR
2012 IEEE International Symposium on Mixed and Augmented Reality
None
2012
Tracking and augmentation are usually handled in independent consecutive stages in augmented reality (AR). The result is that the real-virtual registration is &#x201C;open loop&#x201D;-inaccurate tracking leads to misregistration that is seen by the users but not the system. We propose a general approach to &#x201C;close the loop&#x201D; in the displayed appearance by using the visual feedback of registration for tracking. Specifically, a model-based method is introduced to simultaneously track and augment real objects in a closed-loop fashion, where the model is comprised of the combination of the real object to be tracked and the virtual object to be rendered. This method is applicable to paradigms including video-based AR, projector-based AR, and diminished reality.
[model-based method, Solid modeling, image registration, diminished reality, real object, augmented reality, visual feedback, augmentation, tracking, Optimization, virtual object, feedback, real-virtual registration, Mathematical model, rendering (computer graphics), Computational modeling, projector-based AR, Educational institutions, closed-loop registration, rendering, Augmented reality, Equations, closed loop systems, video-based AR, Closed-loop registration]
General chair
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Welcome to the Twelfth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR 2013).
[Laboratories, Europe, Media, Cities and towns, Educational institutions, Australia, Augmented reality]
Program chairs
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
We are delighted to welcome you to ISMAR 2013, the 12th symposium on Mixed and Augmented Reality! This year's symposium continues a long tradition of ISMAR meetings, a series that itself followed a related series of IWAR, ISMR, and ISAR meetings.
[]
Tutorial chairs
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
It is our great pleasure to present the ISMAR Tutorials. We proudly host two tutorials that provide sharing of knowledge from seasoned researchers. Our tutorials cover open standards and development using HTML and instantAR. Through these exciting tutorials we hope to expand the minds of ISMAR 2013 attendees and help to foster the next generation of Mixed and Augmented Reality researchers, practitioners, and artists.
[]
Workshop chairs
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
It is our pleasure to present the workshops associated with ISMAR 2013. These events provide a chance to thoroughly examine specific research areas in the exciting field of Mixed and Augmented Reality.
[]
Demo chairs
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
We are delighted to present the 12th edition of the demonstration program of the IEEE International Symposium on Mixed and Augmented Reality. The ISMAR demonstration program provide hands-on experience to the community of the most recent technical and applied advancement in Augmented Reality.
[]
Doctoral chairs
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
We are proud to present the 2nd ISMAR Doctoral Consortium (DC). We are continuing to build on the success of the 1st DC at ISMAR 2012, where DC students received invaluable inputs from DC mentors to help improve their research. The goal of the DC is to create an opportunity for PhD students to present their research, discuss their current progresses and future plans, and receive constructive criticism and guidances regarding their current work, future work, and career perspectives.
[]
Wip chairs
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
New this year to ISMAR 2013, we are proud to present the Works In Progress (WIP) Program. Augmented Reality is rapidly growing into many new areas, so the WIP is a platform to present the field's latest, emerging results to the larger community before the work has reached its final form. This year, the program includes bread and butter AR technologies such as remote collaboration interfaces, fiducial marker design, and perceptual studies, to even loftier applications like AR interactions aboard the International Space Station. Passive haptics, bare-handed gesture interfaces, and realistic rendering round out the offerings. So come to the WIP sessions to hear about active AR research and find the spark of inspiration!
[]
IEEE Visualization &amp; Graphics Technical Committee (VGTC)
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
The IEEE Visualization and Graphics Technical Committee (VGTC) is a formal subcommittee of the Technical Activities Board (TAB) of the IEEE Computer Society. The VGTC provides technical leadership and organizes technical activities in the areas of visualization, computer graphics, virtual and augmented reality, and interaction.
[]
[Conference officers]
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Provides a listing of current committee members and society officers.
[]
Adaptive ghosted views for Augmented Reality
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
In Augmented Reality (AR), ghosted views allow a viewer to explore hidden structure within the real-world environment. A body of previous work has explored which features are suitable to support the structural interplay between occluding and occluded elements. However, the dynamics of AR environments pose serious challenges to the presentation of ghosted views. While a model of the real world may help determine distinctive structural features, changes in appearance or illumination detriment the composition of occluding and occluded structure. In this paper, we present an approach that considers the information value of the scene before and after generating the ghosted view. Hereby, a contrast adjustment of preserved occluding features is calculated, which adaptively varies their visual saliency within the ghosted view visualization. This allows us to not only preserve important features, but to also support their prominence after revealing occluded structure, thus achieving a positive effect on the perception of ghosted views.
[Visualization, Solid modeling, occluding-occluded elements structural interplay, H.5.1 [Information Interfaces and Presentation, Color, structural features, augmented reality, adaptive ghosted view visualization, Augmented reality, Three-dimensional displays, Image color analysis, visual saliency, data visualisation, Rendering (computer graphics), Information Interfaces and Presentation, AR environment dynamics, occluding features contrast adjustment]
Diminished reality using appearance and 3D geometry of internet photo collections
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
This paper presents a new system level framework for Diminished Reality, leveraging for the first time both the appearance and 3D information provided by large photo collections on the Internet. Recent computer vision techniques have made it possible to automatically reconstruct 3-D structure-from-motion points from large and unordered photo collections. Using these point clouds and a prior provided by GPS, reasonably accurate 6 degree of freedom camera poses can be obtained, thus allowing localization. Once the camera (and hence the user) is correctly localized, photos depicting scenes visible from the user's viewpoint can be used to remove unwanted objects indicated by the user in the video sequences. Existing methods based on texture synthesis bring undesirable artifacts and video inconsistency when the background is heterogeneous; the task is rendered even harder for these methods when the background contains complex structures. On the other hand, methods based on plane warping fail when the background has arbitrary shape. Unlike these methods, our algorithm copes with these problems by making use of internet photos, registering them in 3D space and obtaining the 3D scene structure in an offline process. We carefully design the various components during the online phase so as to meet both speed and quality requirements of the task. Experiments on real data collected demonstrate the superiority of our system.
[Correlation, virtual reality, complex structures, diminished reality, GPS, Image reconstruction, 3D structure-from-motion points reconstruction, cameras, Three-dimensional displays, Image Processing and Computer Vision, system level framework, arbitrary shape, video signal processing, image sequences, 6 degree of freedom camera poses, point clouds, Internet photo collections, computer vision techniques, H.5.1 [Information Systems, video inconsistency, 3D geometry, Information Systems, Computing Methodologies, image reconstruction, image texture, Global Positioning System, plane warping, 3D space, digital photography, 3D information, Internet photos, texture synthesis, computer vision, video sequences, Cameras, Feature extraction, Internet, 3D scene structure]
Approximated user-perspective rendering in tablet-based augmented reality
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
This study addresses the problem of geometric consistency between displayed images and real scenes in augmented reality using a video see-through hand-held display or tablet. To solve this problem, we present approximated user-perspective images rendered by homography transformation of camera images. Homography approximation has major advantages not only in terms of computational costs, but also in the quality of image rendering. However, it can lead to an inconsistency between the real image and virtual objects. This study also introduces a variety of rendering methods for virtual objects and discusses the differences between them. We implemented two prototypes and designed three types of user studies on matching tasks between real scenes and displayed images. We have confirmed that the proposed method works in real time on an off-the-shelf tablet. Our pilot tests show the potential to improve users' visibility, even in real environments, by using our method.
[tablet based augmented reality, tablet-based AR, augmented reality, user interfaces, Approximation methods, real scenes, video see-through, video see-through handheld display, camera images, Three-dimensional displays, user perspective images, Prototypes, notebook computers, geometric consistency, virtual objects, image rendering, rendering (computer graphics), image processing, User-perspective rendering, approximated user perspective rendering, Calibration, homography approximation, Augmented reality, computational costs, Cameras, Rendering (computer graphics), homography transformation]
Computational augmented reality eyeglasses
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
In this paper we discuss the design of an optical see-through head-worn display supporting a wide field of view, selective occlusion, and multiple simultaneous focal depths that can be constructed in a compact eyeglasses-like form factor. Building on recent developments in multilayer desktop 3D displays, our approach requires no reflective, refractive, or diffractive components, but instead relies on a set of optimized patterns to produce a focused image when displayed on a stack of spatial light modulators positioned closer than the eye accommodation distance. We extend existing multilayer display ray constraint and optimization formulations while also purposing the spatial light modulators both as a display and as a selective occlusion mask. We verify the design on an experimental prototype and discuss challenges to building a practical display.
[optical see-through head-worn display, Optical diffraction, focused image, three-dimensional displays, Nonhomogeneous media, Optical imaging, augmented reality, helmet mounted displays, computational augmented reality eyeglasses, multilayer desktop 3D displays, Optimization, spatial light modulators, eye, optimisation, compact eyeglasses-like form factor, optimization formulations, Modulation, multilayer display ray constraint, selective occlusion mask, Optical refraction, focal depths, Lenses, eye accommodation distance]
Improving procedural task performance with Augmented Reality annotations
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
This paper presents results of a study measuring user performance in a procedural task using Spatial Augmented Reality (SAR). The task required participants to press sequences of buttons on two control panel designs in the correct order. Instructions for the task were shown either on a computer monitor, or projected directly onto the control panels. This work was motivated by discrepancies between the expectations from AR proponents and experimental findings. AR is often promoted as a way of improving user performance and understanding. With notable exceptions however, experimental results do not confirm these expectations. Reasons cited for results include limitations of current display technologies and misregistration caused by tracking and calibration errors. Our experiment utilizes SAR to remove these effects. Our results show that augmented annotations lead to significantly faster task completion speed, fewer errors, and reduced head movement, when compared to monitor based instructions. Subjectively, our results show augmented annotations are preferred by users.
[augmented reality annotations, User Interfaces, augmented reality, Spatial Augmented Reality, Augmented reality, Presses, augmented annotations, Atmospheric measurements, User Study, Pressing, Particle measurements, procedural task performance improvement, computer monitor, Monitoring, Assembly, spatial augmented reality, user performance measurement]
Through the looking glass: Pretend play for children with autism
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Lack of spontaneous pretend play is an early diagnostic indicator of autism spectrum conditions (ASC) along with impaired communication and social interaction. In a previous ISMAR poster [2] we proposed an Augmented Reality (AR) system to encourage pretend play, based on an analogy between imaginative interpretation of physical objects (pretense) and the superimposition of virtual content on the physical world in AR. This paper reports an empirical experiment evaluating that proposal, involving children between the ages of 4 and 7 who have been diagnosed with ASC. Results find significantly more pretend play, and higher engagement, using the AR system by comparison to a non-augmented condition. We also discuss usability issues and design implications for AR systems that aim to support children with ASC and other pervasive developmental disorders.
[Airplanes, Visualization, handicapped aids, diagnostic indicator, impaired communication, Augmented Reality, pretend play, Materials, Educational institutions, augmented reality, pervasive developmental disorder, autism spectrum condition, autism, Vehicles, medical disorders, Autism, ASC, spontaneous pretend play, children, Fires, social interaction]
Effects of an in-car augmented reality system on improving safety of younger and older drivers
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
We designed and tested the effects of an in-car augmented reality system (ARS) on younger and older drivers, with and without a secondary distraction task. When potential danger is detected, the ARS alerts the driver by progressively indicating the time to collision to the lead vehicle as well as merging vehicles from side lanes by an AR display that overlaps with the lead or merging vehicles. We tested the ARS with younger (18-30) and older (65-75) drivers in a high-fidelity driving simulator. Results showed that the ARS could significantly reduce collisions caused by hazard events such as sudden slowing of the lead vehicle or merging of vehicles from sides lanes. Consistent with previous results, older drivers, despite age-related decline in cognitive and motor abilities, could leverage their driving experience to avoid forward collisions with the lead vehicle as much as younger drivers. However, older drivers were poorer in avoiding collisions caused by sudden merging events than younger drivers. The ARS was found to be most useful in helping older adults to avoid collision caused by sudden hazard events, especially with the presence of a distraction task. The ARS was also more effective for older than younger drivers to encourage a safe driving distance with the lead vehicle. Interestingly, there seemed to be differential effects of the ARS on the general driving behavior of younger and older drivers. While older drivers in general became more careful and safer in how they drive with the ARS, younger drivers seemed to rely on the ARS to alert them to potential hazard events without adopting safer driving behavior.
[distraction task, Visualization, high-fidelity driving simulator, accident prevention, Merging, motor ability, older driver, augmented reality, Hazards, traffic engineering computing, younger driver, Augmented reality, Human-Computer Interaction, Vehicles, Human-Centered Computing [Human-Computer Interaction, in-car augmented reality system, forward collision, human computer interaction, hazard event, cognitive ability, driving behavior, Analysis of variance, Accidents]
Going with the flow: Modifying self-motion perception with computer-mediated optic flow
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
One major benefit of wearable computers is that users can naturally move and explore computer-mediated realities. However, researchers often observe that users' space and motion perception severely differ in such environments compared to the real world, an effect that is often attributed to slight discrepancies in sensory cues, for instance, caused by tracking inaccuracy or system latency. This is particularly true for virtual reality (VR), but such conflicts are also inherent to augmented reality (AR) technologies. Although, head-worn displays will become more and more available soon, the effects on motion perception have rarely been studied, and techniques to modify self-motion in AR environments have not been leveraged so far. In this paper we introduce the concept of computer-mediated optic flow, and analyze its effects on self-motion perception in AR environments. First, we introduce different techniques to modify optic flow patterns and velocity. We present a psychophysical experiment which reveals differences in self-motion perception with a video see-through head-worn display compared to the real-world viewing condition. We show that computer-mediated optic flow has the potential to make a user perceive self-motion as faster or slower than it actually is, and we discuss its potential for future AR setups.
[Visualization, virtual reality, H.5.1 [Information Interfaces and Presentation, computer-mediated realities, sensory cues, augmented reality, Computer Graphics, psychophysical experiment, video see-through head-worn display, self-motion perception, optic flow patterns, real-world viewing condition, video signal processing, image sequences, Legged locomotion, computer-mediated optic flow, Observers, Optical imaging, helmet mounted displays, AR environments, image motion analysis, wearable computers, Optical feedback, users space, Cameras, Information Interfaces and Presentation, Optical sensors, VR]
Mobile interactive hologram verification
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Verification of paper documents is an important part of checking a person's identity, authorization for access or simply establishing a trusted currency. Many documents such as passports or paper bills include holograms or other view-dependent elements that are difficult to forge and therefore are used to verify the genuineness of that document. View-dependent elements change their appearance based both on viewing direction and dominant light sources, thus it requires special knowledge and training to accurately distinguish original elements from forgeries. We present an interactive application for mobile devices that integrates the recognition of the documents with the interactive verification of view-dependent elements. The system recognizes and tracks the paper document, provides user guidance for view alignment and presents a stored image of the element's appearance depending on the current view of the document also recording user decisions. We describe how to model and capture the underlying spatially varying BRDF representation of view-dependent elements. Furthermore, we evaluate this approach within a user study and demonstrate that such a setup captures images that are recognizable and that can be correctly verified.
[view alignment, mobile interactive hologram verification, Mobile communication, Mobile handsets, document genuineness, Security, BRDF representation, Light sources, light sources, trusted currency, authorization, mobile computing, viewing direction, person identity checking, user guidance, interactive systems, paper bills, mobile radio, Inspection, document image processing, Light emitting diodes, view-dependent elements, passports, paper documents verification, forgeries, interactive application, mobile devices, Cameras, holography, documents recognition]
MonoFusion: Real-time 3D reconstruction of small scenes with a single web camera
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
MonoFusion allows a user to build dense 3D reconstructions of their environment in real-time, utilizing only a single, off-the-shelf web camera as the input sensor. The camera could be one already available in a tablet, phone, or a standalone device. No additional input hardware is required. This removes the need for power intensive active sensors that do not work robustly in natural outdoor lighting. Using the input stream of the camera we first estimate the 6DoF camera pose using a sparse tracking method. These poses are then used for efficient dense stereo matching between the input frame and a key frame (extracted previously). The resulting dense depth maps are directly fused into a voxel-based implicit model (using a computationally inexpensive method) and surfaces are extracted per frame. The system is able to recover from tracking failures as well as filter out geometrically inconsistent noise from the 3D reconstruction. Our method is both simple to implement and efficient, making such systems even more accessible. This paper details the algorithmic components that make up our system and a GPU implementation of our approach. Qualitative results demonstrate high quality reconstructions even visually comparable to active depth sensor-based systems such as KinectFusion.
[off-the-shelf Web camera, Image reconstruction, Optimization, cameras, Three-dimensional displays, 6DoF camera pose, MonoFusion, pose estimation, Real-time systems, Robustness, voxel-based implicit model, natural outdoor lighting, small scenes, single Web camera, image reconstruction, graphics processing units, sparse tracking method, image sensors, real-time systems, target tracking, Streaming media, power intensive active sensors, real-time 3D reconstruction, Cameras, GPU implementation]
Simultaneous 3D tracking and reconstruction on a mobile phone
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
A novel framework for joint monocular 3D tracking and reconstruction is described that can handle untextured objects, occlusions, motion blur, changing background and imperfect lighting, and that can run at frame rate on a mobile phone. The method runs in parallel (i) level set based pose estimation and (ii) continuous max flow based shape optimisation. By avoiding a global computation of distance transforms typically used in level set methods, tracking rates here exceed 100Hz and 20Hz on a desktop and mobile phone, respectively, without needing a GPU. Tracking ambiguities are reduced by augmenting orientation information from the phone's inertial sensor. Reconstruction involves probabilistic integration of the 2D image statistics from keyframes into a 3D volume. Per-voxel posteriors are used instead of the standard likelihoods, giving increased accuracy and robustness. Shape coherency and compactness is then imposed using a total variational approach solved using globally optimal continuous max flow.
[Shape, shape optimisation, total variational approach, Mobile handsets, level set based pose estimation, tracking, 3D volume, Image reconstruction, Optimization, Three-dimensional displays, mobile computing, 3D reconstruction, pose estimation, variational techniques, 2D image statistics, motion blur, optimal continuous max flow, probabilistic integration, inertial sensor, probability, image reconstruction, shape coherency, monocular 3D tracking, image motion analysis, mobile phone, Cameras, Rendering (computer graphics), tracking ambiguities, per-voxel posteriors]
Interactive syntactic modeling with a single-point laser range finder and camera
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
In-situ 3D Modeling becomes increasingly prominent in current Augmented Reality research, particularly for mobile scenarios. However, real-time performance and qualitative modeling remain highly challenging. In this work, we propose a new interactive 3D modeling approach for indoor environments, combining an assistive user interface and constrained reconstruction with a device consisting of a single-point laser range finder and a camera. Using our system, a user pans around capturing a panorama of the environment, while simultaneously measuring the distance to a single point per frame. An automatic detection process estimates planes from these sparse 3D measurements. The user can highlight specific geometric features in the environment, such as 2- or 3-way corners, with simple gestures, adding more 3D points to the estimation. The segmented planes are refined using a constrained optimization, enforcing orthogonality and parallel constraints as well as minimizing the number of planes used in the reconstruction. Finally a volumetric space-carving approach determines the geometry of the environment. Our reconstruction approach can output highly accurate models built only from simple, clean geometry. To examine the quantitative performance of our approach, we run evaluations on both synthetic and real data.
[Solid modeling, mobile scenarios, H.5.1 [Information Interfaces and Presentation, augmented reality, interactive syntactic modeling, gestures, reconstruction approach, 3-way corners, Three-dimensional displays, laser ranging, image segmentation, volumetric space-carving approach, 3D points, interactive systems, Laser modes, orthogonality constraints, indoor environments, camera, constrained optimization, single-point laser range finder, segmented planes, parallel constraints, Computational modeling, sparse 3D measurements, Artificial Intelligence, assistive user interface, interactive 3D modeling, image reconstruction, automatic detection process, 2-way corners, Geometry, real-time performance, panorama, Measurement by laser beam, geometric features, Cameras, geometry, Information Interfaces and Presentation, solid modelling]
Real-time modeling and tracking manual workflows from first-person vision
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Recognizing previously observed actions in video sequences can lead to Augmented Reality manuals that (1) automatically follow the progress of the user and (2) can be created from video examples of the workflow. Modeling is challenging, as the environment is susceptible to change drastically due to user interaction and camera motion may not provide sufficient translation to robustly estimate geometry. We propose a piecewise homographic transform that projects the given video material onto a series of distinct planar subsets of the scene. These subsets are selected by segmenting the largest image region that is consistent with a homographic model and contains a given region of interest. We are then able to model the state of the environment and user actions using simple 2D region descriptors. The model elegantly handles estimation errors due to incomplete observation and is robust towards occlusions, e.g., due to the user's hands. We demonstrate the effectiveness of our approach quantitatively and compare it to the current state of the art. Further, we show how we apply the approach to visualize automatically assessed correctness criteria during run-time.
[Visualization, manual workflow tracking, Tracking, piecewise homographic transform, Manuals, Optical distortion, transforms, Optical imaging, augmented reality, user interaction, camera motion, tracking, image motion analysis, first-person vision, Image segmentation, 2D region descriptors, real-time modeling, video sequences, Cameras, augmented reality manuals, video signal processing, homographic model, image sequences]
Delta Light Propagation Volumes for mixed reality
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Indirect illumination is an important visual cue which has traditionally been neglected in mixed reality applications. We present Delta Light Propagation Volumes, a novel volumetric relighting method for real-time mixed reality applications which allows to simulate the effect of first bounce indirect illumination of synthetic objects onto a real geometry and vice versa. Inspired by Radiance Transfer Fields, we modify Light Propagation Volumes in such a way as to propagate the change in illumination caused by the introduction of a synthetic object into a real scene. This method combines real and virtual light in one representation, provides improved temporal coherence for indirect light compared to previous solutions and implicitly includes smooth shadows.
[visual cue, radiance transfer field, augmented reality, first bounce indirect illumination, volumetric relighting method, Image reconstruction, Light sources, Geometry, Surface reconstruction, Real-time Global Illumination, indirect light, real-time mixed reality, Lighting, Virtual reality, Cameras, rendering (computer graphics), Mixed Reality, delta light propagation volume]
Differential Irradiance Caching for fast high-quality light transport between virtual and real worlds
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Fast and realistic synthesis of real videos with computer generated content has been a challenging problem in computer graphics. It involves computationally expensive light transport calculations. We present a novel and efficient algorithm for diffuse light transport calculation between virtual and real worlds called Differential Irradiance Caching. Our algorithm produces a high-quality result while preserving interactivity and allowing dynamic geometry, materials, lighting, and camera movement. The problem of expensive differential irradiance evaluation is solved by exploiting the spatial coherence in indirect illumination using irradiance caching. We enable multiple bounces of global illumination by using Monte Carlo integration in GPU ray-tracing to evaluate differential irradiance at irradiance cache records in one pass. The combination of ray-tracing and rasterization is used in an extended irradiance cache splatting algorithm to provide a fast GPU-based solution of indirect illumination. Limited information stored in the irradiance splat buffer causes errors for pixels on edges in case of depth of field rendering. We propose a solution to this problem using a reprojection technique to access the irradiance splat buffer. A novel cache miss detection technique is introduced which allows for a linear irradiance cache data structure. We demonstrate the integration of differential irradiance caching into a rendering framework for Mixed Reality applications capable of simulating complex global illumination effects.
[Graphics processing units, ray tracing, cache miss detection technique, augmented reality, cache storage, virtual world, irradiance cache records, Computer Graphics, Monte Carlo methods, real world, GPU ray-tracing, reprojection technique, field rendering, Lighting, linear irradiance cache data structure, Virtual reality, I.3.7 [Computer Graphics, Ray tracing, Real-time systems, data structures, rasterization, Mathematical model, diffuse light transport calculation, irradiance splat buffer, rendering (computer graphics), complex global illumination effects, indirect illumination, irradiance cache splatting algorithm, computer generated content, rendering framework, differential irradiance caching, GPU-based solution, high-quality light transport, differential irradiance evaluation, computer graphics, spatial coherence, Monte Carlo integration, mixed reality applications, Rendering (computer graphics), Information Interfaces and Presentation]
3D High Dynamic Range dense visual SLAM and its application to real-time object re-lighting
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Acquiring High Dynamic Range (HDR) light-fields from several images with different exposures (sensor integration periods) has been widely considered for static camera positions. In this paper a new approach is proposed that enables 3D HDR environment maps to be acquired directly from a dynamic set of images in real-time. In particular a method will be proposed to use an RGB-D camera as a dynamic light-field sensor, based on a dense real-time 3D tracking and mapping approach, that avoids the need for a light-probe or the observation of reflective surfaces. The 6dof pose and dense scene structure will be estimated simultaneously with the observed dynamic range so as to compute the radiance map of the scene and fuse a stream of low dynamic range images (LDR) into an HDR image. This will then be used to create an arbitrary number of virtual omni-directional light-probes that will be placed at the positions where virtual augmented objects will be rendered. In addition, a solution is provided for the problem of automatic shutter variations in visual SLAM. Augmented reality results are provided which demonstrate real-time 3D HDR mapping, virtual light-probe synthesis and light source detection for rendering reflective objects with shadows seamlessly with the real video stream in real-time.
[Solid modeling, dynamic light-field sensor, augmented reality, MR/AR for entertainment, photo-realistic rendering, Light sources, pose structure, virtual augmented object, automatic shutter variation, Three-dimensional displays, Lighting, low dynamic range image, 3D HDR mapping, real-time object relighting, object tracking, 3D high dynamic range dense visual SLAM, dense real-time 3D tracking, Real-time systems, rendering (computer graphics), virtual light-probe synthesis, virtual omni-directional light-probe, Real-time rendering, real video stream, reflective surface, dense scene structure, static camera position, rendering, HDR light-field, radiance map, sensor integration period, 3D HDR environment maps, SLAM (robots), Rendering (computer graphics), Cameras, vision-based registration and tracking, RGB-D camera, light source detection]
User friendly SLAM initialization
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
The development of new Simultaneous Localization and Mapping (SLAM) techniques is quickly advancing in research communities and rapidly transitioning into commercial products. Creating accurate and high-quality SLAM maps relies on a robust initialization process. However, the robustness and usability of SLAM initialization for end-users has often been disregarded. This paper presents and evaluates a novel tracking system for 6DOF pose tracking between a single keyframe and the current camera frame, without any prior scene knowledge. Our system is particularly suitable for SLAM initialization, since it allows 6DOF pose tracking in the intermediate frames before a wide-enough baseline between two keyframes has formed. We investigate how our tracking system can be used to interactively guide users in performing an optimal motion for SLAM initialization. However, our findings from a pilot study indicate that the need for such motion can be completely hidden from the user and outsourced to our tracking system. Results from a second user study show that letting our tracking system create a SLAM map as soon as possible is a viable and usable solution. Our work provides important insight for SLAM systems, showing how our novel tracking system can be integrated with a user interface to support fast, robust and user-friendly SLAM initialization.
[Tracking, SLAM, Augmented Reality, Radar tracking, augmented reality, tracking, SLAM initialization, Simultaneous localization and mapping, Three-dimensional displays, Accuracy, SLAM (robots), single keyframe, Cameras, intermediate frames, Robustness, robust initialization process, tracking system, simultaneous localization and mapping techniques, 6DOF pose tracking, camera frame]
Behaviour-aware sensor fusion: Continuously inferring the alignment of coordinate systems from user behaviour
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Within mobile mixed reality experiences, we would like to engage the user's head and hands for interaction. However, this requires the use of multiple tracking systems. These must be aligned, both as part of initial system setup and to counteract inter-tracking system drift that can accumulate over time. Traditional approaches to alignment use obtrusive procedures that introduce explicit constraints between the different tracking systems. These can be highly disruptive for the user's experience. In this paper, we propose another type of information which can be exploited to effect alignment: the behaviour of the user. The crucial insight is that user behaviours - such as selection through pointing - introduce implicit constraints between tracking systems. These constraints can be used as the user continually interacts with the system to infer alignment without the need for disruptive procedures. We call this concept behaviour-aware sensor fusion. We introduce two different interaction techniques-the redirected pointing technique and the yaw fix technique - to illustrate this concept. Pilot experiments show that behaviour-aware sensor fusion can increase ease of use and speed of interaction in exemplar mixed-reality interaction tasks.
[virtual reality, Tracking, head-mounted display, yaw fix technique, Mobile communication, sensor fusion, selection tasks, augmented reality, user interfaces, tracking, behaviour-aware sensor fusion, mobile computing, Abstracts, redirected pointing technique, interaction techniques, exemplar mixed-reality interaction tasks, selection through pointing, Calibration, Indexes, 3D user interaction, mobile mixed reality experiences, inter-tracking system drift, user behaviour, Heating, obtrusive procedures, Animation, Mobile virtual reality, coordinate systems]
Real-time RGB-D camera relocalization
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
We introduce an efficient camera relocalization approach which can be easily integrated into real-time 3D reconstruction methods, such as KinectFusion. Our approach makes use of compact encoding of whole image frames which enables both online harvesting of keyframes in tracking mode, and fast retrieval of pose proposals when tracking is lost. The encoding scheme is based on randomized ferns and simple binary feature tests. Each fern generates a small block code, and the concatenation of codes yields a compact representation of each camera frame. Based on those representations we introduce an efficient frame dissimilarity measure which is defined via the block-wise hamming distance (BlockHD). We illustrate how BlockHDs between a query frame and a large set of keyframes can be simultaneously evaluated by traversing the nodes of the ferns and counting image co-occurrences in corresponding code tables. In tracking mode, this mechanism allows us to consider every frame/pose pair as a potential keyframe. A new keyframe is added only if it is sufficiently dissimilar from all previously stored keyframes. For tracking recovery, camera poses are retrieved that correspond to the keyframes with smallest BlockHDs. The pose proposals are then used to reinitialize the tracking algorithm. Harvesting of keyframes and pose retrieval are computationally efficient with only small impact on the run-time performance of the 3D reconstruction. Integrating our relocalization method into KinectFusion allows seamless continuation of mapping even when tracking is frequently lost. Additionally, we demonstrate how marker-free augmented reality, in particular, can benefit from this integration by enabling a smoother and continuous AR experience.
[Pipelines, augmented reality, tracking mode, Proposals, block code, cameras, real-time RGB-D camera relocalization, compact representation, Three-dimensional displays, compact encoding, block-wise hamming distance, Iterative closest point algorithm, pose retrieval, binary feature tests, pose estimation, pose proposal retrieval, BlockHD, object tracking, Real-time systems, query frame, stored keyframes, code tables, KinectFusion, online harvesting, marker-free augmented reality, frame dissimilarity measure, code concatenation, Encoding, image reconstruction, image cooccurrence, tracking recovery, concatenated codes, randomized ferns, image frames, real-time 3D reconstruction method, real-time systems, image retrieval, Cameras, encoding scheme]
Single-shot extrinsic calibration of a generically configured RGB-D camera rig from scene constraints
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
With the increasing use of commodity RGB-D cameras for computer vision, robotics, mixed and augmented reality and other areas, it is of significant practical interest to calibrate the relative pose between a depth (D) camera and an RGB camera in these types of setups. In this paper, we propose a new single-shot, correspondence-free method to extrinsically calibrate a generically configured RGB-D camera rig. We formulate the extrinsic calibration problem as one of geometric 2D-3D registration which exploits scene constraints to achieve single-shot extrinsic calibration. Our method first reconstructs sparse point clouds from a single-view 2D image. These sparse point clouds are then registered with dense point clouds from the depth camera. Finally, we directly optimize the warping quality by evaluating scene constraints in 3D point clouds. Our single-shot extrinsic calibration method does not require correspondences across multiple color images or across different modalities and it is more flexible than existing methods. The scene constraints can be very simple and we demonstrate that a scene containing three sheets of paper is sufficient to obtain reliable calibration and with a lower geometric error than existing methods.
[sparse point clouds, single-shot extrinsic calibration method, image registration, robotics, 3D point cloud, INFORMATION INTERFACES AND PRESENTATION, augmented reality, warping quality, commodity RGB-D cameras, extrinsic calibration problem, cameras, Three-dimensional displays, Image color analysis, geometric error, scene constraints, RGB-D camera rig, image colour analysis, depth camera, calibration, dense point clouds, Robot vision systems, geometric 2D-3D registration, Color, Educational institutions, Calibration, IMAGE PROCESSING AND COMPUTER VISION, computer graphics, I.4.1 [IMAGE PROCESSING AND COMPUTER VISION, mixed reality, computer vision, RGB camera, Cameras, color images, single-view 2D image]
A camera-based calibration for automotive augmented reality Head-Up-Displays
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Using Head-up-Displays (HUD) for Augmented Reality requires to have an accurate internal model of the image generation process, so that 3D content can be visualized perspectively correct from the viewpoint of the user. We present a generic and cost-effective camera-based calibration for an automotive HUD which uses the windshield as a combiner. Our proposed calibration model encompasses the view-independent spatial geometry, i.e. the exact location, orientation and scaling of the virtual plane, and a view-dependent image warping transformation for correcting the distortions caused by the optics and the irregularly curved windshield. View-dependency is achieved by extending the classical polynomial distortion model for cameras and projectors to a generic five-variate mapping with the head position of the viewer as additional input. The calibration involves the capturing of an image sequence from varying viewpoints, while displaying a known target pattern on the HUD. The accurate registration of the camera path is retrieved with state-of-the-art vision-based tracking. As all necessary data is acquired directly from the images, no external tracking equipment needs to be installed. After calibration, the HUD can be used together with a head-tracker to form a head-coupled display which ensures a perspectively correct rendering of any 3D object in vehicle coordinates from a large range of possible viewpoints. We evaluate the accuracy of our model quantitatively and qualitatively.
[Solid modeling, H.5.1 [Information Interfaces and Presentation, head-up-displays, augmented reality, Image reconstruction, Vehicles, cameras, head-up displays, Three-dimensional displays, driver information systems, automotive augmented reality, automotive engineering, Image Processing and Computer Vision, view-independent spatial geometry, automotive HUD, image warping transformation, calibration, rendering (computer graphics), Automotive components, image sequences, 3D content, polynomials, rendering, Calibration, image sequence, polynomial distortion, 3D object, image generation process, Numerical Analysis, Cameras, camera-based calibration, geometry, Information Interfaces and Presentation]
Image-guided simulation of heterogeneous tissue deformation for augmented reality during hepatic surgery
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
This paper presents a method for real-time augmentation of vascular network and tumors during minimally invasive liver surgery. Internal structures computed from pre-operative CT scans can be overlaid onto the laparoscopic view for surgery guidance. Compared to state-of-the-art methods, our method uses a real-time biomechanical model to compute a volumetric displacement field from partial three-dimensional liver surface motion. This permits to properly handle the motion of internal structures even in the case of anisotropic or heterogeneous tissues, as it is the case for the liver and many anatomical structures. Real-time augmentation results are presented on in vivo and phantom data and illustrate the benefits of such an approach for minimally invasive surgery.
[anisotropic tissue, image-guided simulation, Liver, H.5.1 [Information Interfaces and Presentation, augmented reality, surgery guidance, preoperative CT scans, phantoms, state-of-the-art method, Computer Graphics, Three-dimensional displays, Surgery, laparoscopic view, hepatic surgery, minimally invasive liver surgery, tumours, medical image processing, Deformable models, Biological system modeling, Computational modeling, minimally invasive surgery, liver, real-time augmentation, partial three-dimensional liver surface motion, internal structures, Biomechanics, computerised tomography, heterogeneous tissue deformation, tumors, real-time biomechanical model, real-time systems, phantom data, anatomical structures, Information Interfaces and Presentation, volumetric displacement field, vascular network, solid modelling, surgery]
Robust monocular SLAM in dynamic environments
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
We present a novel real-time monocular SLAM system which can robustly work in dynamic environments. Different to the traditional methods, our system allows parts of the scene to be dynamic or the whole scene to gradually change. The key contribution is that we propose a novel online keyframe representation and updating method to adaptively model the dynamic environments, where the appearance or structure changes can be effectively detected and handled. We reliably detect the changed features by projecting them from the keyframes to current frame for appearance and structure comparison. The appearance change due to occlusions also can be reliably detected and handled. The keyframes with large changed areas will be replaced by newly selected frames. In addition, we propose a novel prior-based adaptive RANSAC algorithm (PARSAC) to efficiently remove outliers even when the inlier ratio is rather low, so that the camera pose can be reliably estimated even in very challenging situations. Experimental results demonstrate that the proposed system can robustly work in dynamic environments and outperforms the state-of-the-art SLAM systems (e.g. PTAM).
[Solid modeling, iterative methods, robot vision, PARSAC, prior-based adaptive RANSAC algorithm, I.4.8 [Image Processing and Computer Vision, dynamic environments, Three-dimensional displays, Simultaneous localization and mapping, SLAM (robots), robust monocular SLAM, Image Processing and Computer Vision, online keyframe representation method, Cameras, Feature extraction, Robustness, Real-time systems, Information Interfaces and Presentation]
Augmented Reality binoculars
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
In this paper we present an augmented reality binocular system to allow long range high precision augmentation of live telescopic imagery with aerial and terrain based synthetic objects, vehicles, people and effects. The inserted objects must appear stable in the display and must not jitter and drift as the user pans around and examines the scene with the binoculars. The design of the system is based on using two different cameras with wide field of view, and narrow field of view lenses enclosed in a binocular shaped shell. Using the wide field of view gives us context and enables us to recover the 3D location and orientation of the binoculars much more robustly, whereas the narrow field of view is used for the actual augmentation as well as to increase precision in tracking. We present our navigation algorithm that uses the two cameras in combination with an IMU and GPS in an Extended Kalman Filter (EKF) and provides jitter free, robust and real-time pose estimation for precise augmentation. We have demonstrated successful use of our system as part of a live simulated training system for observer training, in which fixed and rotary wing aircrafts, ground vehicles, and weapon effects are combined with real world scenes.
[Visualization, Tracking, image fusion, 3D orientation, binocular shaped shell, augmented reality, sensor fusion, GPS, Training, live telescopic imagery, monocular wide and narrow field of view camera, pose estimation, Kalman filters, navigation algorithm, high precision augmentation, inertial navigation, EKF, Calibration, 3D location, Augmented reality, Global Positioning System, IMU, extended Kalman filter, Cameras, augmented reality binocular system]
Handling pure camera rotation in keyframe-based SLAM
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Handling degenerate rotation-only camera motion is a challenge for keyframe-based simultaneous localization and mapping with six degrees of freedom. Existing systems usually filter corresponding keyframe candidates, resulting in mapping starvation and tracking failure. We propose to employ these otherwise discarded keyframes to build up local panorama maps registered in the 3D map. Thus, the system is able to maintain tracking during rotational camera motions. Additionally, we seek to actively associate panoramic and 3D map data for improved 3D mapping through the triangulation of more new 3D map features. We demonstrate the efficacy of our approach in several evaluations that show how the combined system handles rotation only camera motion while creating larger and denser maps compared to a standard SLAM system.
[Tracking, rotation only camera motion, keyframe-based SLAM, monocular visual SLAM, general and rotation-only camera motion, History, panorama maps, image motion analysis, keyframe-based simultaneous mapping, Three-dimensional displays, Simultaneous localization and mapping, SLAM (robots), mobile phone, keyframe-based simultaneous localization, Cameras, Feature extraction, Robustness, hybrid 6DOF and panoramic mapping and tracking, 3D mapping]
See-through window vs. magic mirror: A comparison in supporting visual-motor tasks
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
There are two alternative display metaphors for Augmented Reality (AR) screens: a see-through window or a magic mirror. Commonly used by task-support AR applications, the see-through display has not been compared with the mirror display in terms of user's task performance, even though the &#x201C;mirror&#x201D; hardware is more accessible to general users. We conducted a novel experiment to compare participants' performance when following object rotation cues with the two display metaphors. Results show that participants' overall performance under the mirror view was comparable to the see-through view, which indicates that the augmented mirror display may be a promising alternative to the window display for AR applications which guide moderately complex three-dimensional manipulations with physical objects.
[display metaphor, mirrors, Augmented Reality, visual-motor tasks, display metaphors, Educational institutions, augmented reality, visual-motor, Augmented reality, magic mirror, augmented reality screens, AR screens, Three-dimensional displays, augmented mirror display, see-through window, three-dimensional manipulations, physical objects, computer displays, task-support AR applications, Cameras, Mirrors, Face, see-through display, Monitoring]
AR in the library: A pilot study of multi-target acquisition usability
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Libraries use call numbers to organize their books and enable patrons to locate them. To keep the books in order, library workers conduct a time-consuming and tedious task called &#x201C;shelf-reading.&#x201D; Workers look at the call numbers on the spines of each book in the library, one at a time, to make sure they are in the correct places. ShelvAR is an augmented reality shelf-reading system for smart phones that reduces time spent, increases accuracy, and produces an inventory of the books on their shelves as a byproduct. Shelf-reading requires rapid acquisition of many targets (books). Unlike many target acquisition tasks considered in the AR literature, the user is not trying to select a single target from among many. Instead, the user is trying to scan all of the targets, and must be able to easily double-check that none were missed. Our goal is to explore the usability of augmented reality applications for this type of &#x201C;multiple target acquisition&#x201D; task. We present the results of a pilot study on the effectiveness of ShelvAR. We demonstrate that individuals with no library experience are just as fast and accurate, when using ShelvAR, as experienced library workers at the shelf-reading task.
[augmented reality shelf-reading system, multitarget acquisition usability, Manuals, target acquisition tasks, libraries, augmented reality, smart phones, library automation, Augmented reality, Accuracy, books organization, ShelvAR, call numbers, books inventory, augmented reality applications, Cameras, Libraries, multiple target acquisition, Usability, library workers, Lenses]
Interaction techniques for HMD-HHD hybrid AR systems
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Most mobile Augmented Reality (AR) systems use either a head mounted display (HMD) or a handheld display (HHD) as a hardware platform. As mobile devices become more affordable, it becomes more common that users own more than one mobile device and use them together. In this research, we investigate Hybrid AR systems that use both HMD and HHD for AR visualization and interaction. In addition to a simple approach of using HMD as a display and HHD as an input device (e.g. a touch pad or a pointer), we further explore novel interaction techniques that can take advantage of having both HMD and HHD closely integrated into one AR system, such as cross-device information sharing, situation adaptive visualization management, and multi-layered visualization.
[Visualization, AR visualization, AR interaction, mobile augmented reality systems, HMD-HHD hybrid AR system, Mobile communication, augmented reality, Mobile handsets, handheld display, Mobile augmented reality, Three-dimensional displays, mobile computing, Wearable computers, data visualisation, multilayered visualization, head mounted display, situation adaptive visualization management, Hardware, wearable computer, interaction techniques, helmet mounted displays, Augmented reality, headmounted display, mobile devices, cross-device information sharing, human computer interaction]
Giving mobile devices a SIXTH sense: Introducing the SIXTH middleware for Augmented Reality applications
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
With the increasing availability of sensors within smartphones and within the world at large, a question arises about how this sensor data can be leveraged by Augmented Reality (AR) devices. AR devices have traditionally been limited by the capability of a given device's unique set of sensors. Connecting sensors from multiple devices using a Sensor Web could address this problem. Through leveraging this SensorWeb existing AR environments could be improved and new scenarios made possible, with devices that previously could not have being used as part of an AR environment. This paper proposes the use of SIXTH: a middleware designed to generate a Sensor Web, which allows a device to leverage heterogeneous external sensors within its environment to help facilitate the creation of richer AR experiences. This paper will present a worst case scenario, in which the device chosen will be a see-through, Android-based Head Mounted Display that has no access to sensors. This device is transformed into an AR device through the creation of a Sensor Web allowing it to sense its environment facilitated through the use of SIXTH.
[AR devices, sensor data, OSGi, augmented reality, SIXTH middleware, Android Augmented Reality, Android (operating system), mobile computing, Sensor Web, connecting sensors, heterogeneous external sensors, Android-based head mounted display, augmented reality applications, SensorWeb, Libraries, smartphones, middleware, augmented reality devices, Educational institutions, helmet mounted displays, AR environments, smart phones, Middleware, Augmented reality, mobile devices, Internet, AR experiences, Smart phones]
Third person perspective augmented reality for high accuracy applications
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
We proposed a 6 degrees of freedom augmentation system aimed at meeting the high accuracy requirements of engineering tasks. A stationary panoramic video camera captures a stream that is augmented by a portable computer. A handheld tablet device located in the same area broadcasts its instantaneous orientation, and receives the augmented view in the corresponding orientation, in real time. The panoramic camera can also be moved to other locations and simultaneously tracked by the system, providing 6 degrees of freedom augmentation. This gives the user a third person perspective augmentation, which is very precise and potentially more accurate than handheld augmentation.
[Solid modeling, Portable computers, Design automation, engineering tasks, Tracking, stationary panoramic video camera, handheld tablet device, 3D model, augmented reality, third person perspective, accuracy, engineering, tracking, Augmented reality, third person perspective augmented reality, Accuracy, 6 degrees of freedom augmentation system, portable computer, Cameras, panoramic camera]
Poor man's SimulCam: Real-time and effortless matchmoving
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
In this article, we propose an instant matchmoving solution for green screen. It uses a recent technique of planar uniform marker fields. Marker fields are an extension of planar markers used in augmented reality, offering better reliability and performance suitable for our task: tolerance to occlusion, speed of detection, and use of arbitrary low-contrast colors. We show that marker fields of shades of green (or blue or other color) can be used to obtain an instant and effortless camera pose estimation. We provide exemplar applications of the presented technique: virtual camera/simulcam and live storyboarding or shot prototyping. The matchmoving technique based on marker fields of shades of green is very computationally efficient - our measurements show that the matchmoving preview and living storyboard editing and recording can be easily done on today's ultramobile devices. Our technique is thus available to anyone at low cost and with easy setup, opening space for new levels of filmmakers' creative expression.
[Green Screen, speed of detection, reliability, augmented reality, effortless matchmoving, cameras, Image color analysis, pose estimation, Real-time systems, Movie Production, Film Tricks, storyboarding, Estimation, Color, MatchMoving, Camera Pose Estimation, virtual camera, real-time matchmoving, SimulCam, Chroma Key, Green products, real-time systems, occlusion, camera pose estimation, Cameras, shot prototyping, Reliability, planar uniform marker fields, Storyboard]
Content first - A concept for industrial augmented reality maintenance applications using mobile devices
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Although AR has a long history in the area of maintenance and service-support in industry, there still is a lack of lightweight, yet practical solutions for handheld AR systems in everyday workflows. Attempts to support complex maintenance tasks with AR still miss reliable tracking techniques, simple ways to be integrated into existing maintenance environments, and practical authoring solutions, which minimize costs for specialized content generation. We present a general, customisable application framework, allowing to employ AR and VR techniques in order to support technicians in their daily tasks. In contrast to other systems, we do not aim to replace existing support systems such as traditional manuals. Instead we integrate well-known AR- and novel presentation techniques with existing instruction media. To this end practical authoring solutions are crucial and hence we present an application development system based on web-standards such as HTML,CSS and X3D.
[Solid modeling, application development system, maintenance engineering, X3D, augmented reality, HTML, Multimedia Information Systems, VR techniques, Three-dimensional displays, Web standards, Context, CSS, maintenance environments, User Interfaces, Documentation, Maintenance engineering, Media, industrial augmented reality maintenance applications, customisable application framework, Augmented reality, H.5.1 [Multimedia Information Systems, specialized content generation, complex maintenance tasks, handheld AR systems, mobile devices, tracking techniques, service support, mobile handsets]
Augmented Reality driving supported by Vehicular Ad Hoc Networking
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
The confined space of a car and the configuration of its controls and displays towards the driver, offer significant advantages for Augmented Reality (AR) systems in terms of the immersion level provided to the user. In addition, the inherent mobility and virtually unlimited power autonomy transform cars into perfect mobile computing platforms. However, the limited network connectivity that is currently available in automobiles leads to the design of Advanced Driver Assistance Systems (ADAS) that create AR objects based only on the information generated by on-board sensors, stored maps and databases, and eventually high-latency online content for Internet-enabled vehicles. By combining the new paradigm of Vehicular Ad Hoc Networking (VANET) with AR human machine interfaces, we show that it is possible to design novel cooperative ADAS, that base the creation of AR content on the information collected from neighbouring vehicles or roadside infrastructures. We provide a prototype implementation of a visual AR system that can significantly improve the driving experience.
[Visualization, AR human machine interfaces, visual AR system, Network Architecture and Design, Glass, vehicular ad hoc networking, augmented reality, Acoustics, user interfaces, ADAS, Multimedia Information Systems, Vehicles, immersion level, mobile computing, driver information systems, mobile computing platforms, virtually unlimited power autonomy, C.6.1 [Network Architecture and Design, vehicular ad hoc networks, Applications, advanced driver assistance systems, VANET, Augmented reality, AR objects, H.5.1 [Multimedia Information Systems, Internet-enabled vehicles, &#x2014;Wireless communication, Vehicular ad hoc networks, Streaming media, Internet, augmented reality driving]
Acceleration methods for radiance transfer in photorealistic augmented reality
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Radiance transfer computation from unknown real-world environments is an intrinsic task in probe-less photometric registration for photorealistic augmented reality, which affects both the accuracy of the real-world light estimation and the quality of the rendering. We discuss acceleration methods that can reduce the overall ray-tracing costs for computing the radiance transfer for photometric registration in order to free up resources for more advanced augmented reality lighting. We also present evaluation metrics for a systematic evaluation.
[ray-tracing costs reduction, ray tracing, photorealistic augmented reality, H.5.1 [Information Interfaces and Presentation, augmented reality lighting, augmented reality, Vectors, radiance transfer, photometric registration, Augmented reality, Geometry, Computer Graphics, brightness, Lighting, Image Processing and Computer Vision, Cameras, Rendering (computer graphics), Information Interfaces and Presentation, Acceleration, acceleration methods, rendering (computer graphics)]
Passive Deformable Haptic glove to support 3D interactions in mobile augmented reality environments
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
We present a passive deformable haptic (PDH) glove to enhance mobile immersive augmented reality manipulation with a sense of computer-captured touch, responding to objects in the physical environment. We extend our existing pinch glove design with a Digital Foam sensor, placed under the palm of the hand. The novel glove input device supports a range of touch-activated, precise, direct manipulation modeling techniques with tactile feedback including hole-punching, trench cutting, and chamfer creation. The PDH glove helps improve a user's task performance time, decrease error rate and erroneous hand movements, and reduce fatigue.
[Nails, tactile sensors, Augmented Reality, Pinch Gloves, Materials, Mobile communication, augmented reality, trench cutting, passive deformable haptic glove, computer-captured touch, mobile computing, mobile augmented reality environments, Digital Foam sensor, Input Device, mobile immersive augmented reality manipulation, data gloves, 3D interactions, PDH glove, direct manipulation modeling techniques, Thumb, pinch glove design, Haptic interfaces, Augmented reality, glove input device, hole-punching, Interaction Technique, tactile feedback, chamfer creation, Passive Haptics]
Diminished reality considering background structures
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
This paper proposes a new diminished reality method for 3D scenes considering background structures. Most conventional methods using image inpainting assumes that the background around a target object is almost planar. In this study, approximating the background structure by the combination of local planes, perspective distortion of texture is corrected and searching area is limited for improving the quality of image inpainting. The temporal coherence of texture is preserved using the estimated structures and camera pose estimated by Visual-SLAM.
[H.5.1 [Information Interfaces and Presentation, Search problems, augmented reality, background structures, texture perspective distortion, cameras, Three-dimensional displays, pose estimation, local planes, Real-time systems, texture temporal coherence, image inpainting, visual-SLAM, Computing Methodologies, Vectors, diminished reality method, image reconstruction, structure estimation, image texture, 3D scenes, Image segmentation, camera pose estimation, Coherence, Cameras, Information Interfaces and Presentation]
Study of augmented gesture communication cues and view sharing in remote collaboration
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
In this research, we explore how different types of augmented gesture communication cues can be used under different view sharing techniques in a remote collaboration system. In a pilot study, we compared four conditions: (1) Pointers on Still Image, (2) Pointers on Live Video, (3) Annotation on Still Image, and (4) Annotation on Live Video. Through this study, we found three results. First, users collaborate more efficiently using annotation cues than pointer cues for communicating object position and orientation information. Second, live video becomes more important when quick feedback is needed. Third, the type of gesture cue has more influence on performance and user preference than the type of view sharing method.
[Visualization, Portable computers, view sharing techniques, Augmented Reality, remote collaboration system, augmented reality, live video pointers, object position, gesture recognition, user preference, video communication, video signal processing, augmented gesture communication cues, live video annotation, annotation cues, pointer cues, still image annotation, Video Conferencing, Augmented reality, still image pointers, quick feedback, Collaboration, orientation information, Streaming media, Speech, video conferencing]
An outdoor ground truth evaluation dataset for sensor-aided visual handheld camera localization
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
We introduce the first publicly available test dataset for outdoor handheld camera localization comprising over 45,000 real camera images of an urban environment captured under natural camera motions and different illumination settings. For all these images the dataset not only contains readings of the sensors attached to the camera, but also ground truth information on the geometry and texture of the environment and the full 6DoF ground truth camera pose. This poster describes the extensive process of creating this comprehensive dataset that we have made available to the public. We hope this not only enables researchers to objectively evaluate their camera localization and tracking algorithms and frameworks on realistic data but also stimulates further research.
[Solid modeling, Visualization, urban environment, Mobile handsets, Electronic mail, 6DoF ground truth camera pose, image texture, outdoor ground truth evaluation dataset, cameras, camera images, sensor-aided visual handheld camera localization, Three-dimensional displays, pose estimation, Cameras, Robot sensing systems, geometry]
Fast and automatic city-scale environment modeling for an accurate 6DOF vehicle localization
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
To provide high quality augmented reality service in a car navigation system, accurate 6DoF localization is required. To ensure such accuracy, most of current vision-based solutions rely on an off-line large scale modeling of the environment. Nevertheless, while existing solutions require expensive equipments and/or a prohibitive computation time, we propose in this paper a complete framework that automatically builds an accurate city scale database using only a standard camera, a GPS and Geographic Information System (GIS). As illustrated in the experiments, only few minutes are required to model large scale environments. The resulting databases can then be used during a localization algorithm for high quality Augmented Reality experiences.
[Solid modeling, augmented reality, geographic information systems, GPS, I.4.8 [Image Processing and Computer Vision, Three-dimensional displays, Accuracy, Databases, Image Processing and Computer Vision, geographic information system, vision-based solutions, camera, car navigation system, Buildings, automobiles, 6DOF vehicle localization, traffic engineering computing, Global Positioning System, GIS, automatic city-scale environment modeling, computer vision, off-line large scale modeling, Cameras, city scale database, augmented reality service]
Are HMDs the better HUDs?
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Head-mounted displays (HMDs) have the potential to overcome some of the technological limitations of currently existing automotive head-up displays (HUDs), such as the limited field of view and the restrictive boundaries of the windshield. In an initial study we evaluated the use of HMDs in cars by means of a typical HUD visualization, using a HUD as baseline output technology. We found no significant differences in terms of driving performance, physical uneasiness or visual distraction. User statements revealed several advantages and drawbacks of the different output technologies apart from technological maturity and ergonomics. These results will hopefully inspire researchers as well as application developers and even might lead us to novel HMD visualization approaches.
[Visualization, ergonomics, HUD visualization, head-mounted display, technological maturity, visual distraction, Vehicles, Automotive engineering, technological limitations, head-up displays, data visualisation, automotive head-up displays, Mirrors, automotive components, physical uneasiness, head-mounted displays, automobiles, head-up display, Educational institutions, helmet mounted displays, traffic engineering computing, field of view, HMD visualization, Augmented reality, driving performance, mixed reality, Data visualization, windshield, baseline output technology]
In-situ interactive modeling using a single-point laser rangefinder coupled with a new hybrid orientation tracker
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
We present a method for in situ modeling of polygonal scenes, using a laser rangefinder, an IMU and a camera. The main contributions of this work are a well-founded calibration procedure, a new hybrid, driftless orientation tracking method and an easy-to-use interface based on natural interactions.
[Solid modeling, I.2.10 [Vision and Scene Understanding, orientation tracker, Calibration, Laser beams, cameras, IMU, Three-dimensional displays, laser ranging, in-situ interactive modeling, Measurement by laser beam, target tracking, interactive systems, Vision and Scene Understanding, Cameras, Laser modes, polygonal scenes, camera, single-point laser rangefinder, solid modelling]
Subtle cueing for visual search in head-tracked head worn displays
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Goal-oriented visual search in augmented reality can be facilitated by using visual cues to call attention to a target. However, traditional use of explicit cues can degrade visual search performance due to scene distortion, occlusion and addition of visual clutter. In contrast, Subtle Cueing has been previously proposed as an alter-native to explicit cueing, but little is known about how well it works for head-tracked head worn displays (HWDs). We investigated the effect of Subtle Cueing for head-tracked head worn displays, using visual search research methods in simulated augmented reality environments. Our user study found that Subtle Cueing improves visual search performance, and serves as a feasible cueing mechanism for AR environments using HWDs.
[Visualization, Head, clutter, visual clutter, Attention, Mobile communication, augmented reality, helmet mounted displays, Magnetic heads, scene distortion, Clutter, Augmented reality, subtle cueing, Subtle visual cueing, Visual search, occlusion, query formulation, head-tracked head worn displays, Erbium, goal-oriented visual search]
Photo-shoot localization of a mobile camera based on registered frame data of virtualized reality models
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
This paper presents a study of a method for estimating the position and orientation of a photo-shoot in indoor environments for augmented reality applications. Our proposed localization method is based on registered frame data of virtualized reality models, which are photos with known photo-shoot positions and orientations, and depth data. Because registered frame data are secondary product of modeling process, additional works are not necessary to create registered frame data especially for the localization. In the method, a photo taken by a mobile camera is compared to registered frame data for the localization. Since registered frame data are linked with photo-shoot position, orientation, and depth data, 3D coordinates of each pixel on the photo of registered frame data is available. We conducted experiments with employing five techniques of the estimation for comparative evaluations.
[Solid modeling, Image edge detection, Estimation, H.5.1 [Information Interfaces and Presentation, virtualized reality models, 3D coordinates, Mobile communication, augmented reality, position estimation, cameras, mobile camera, photo-shoot localization, pose estimation, Image Processing and Computer Vision, registered frame data, Cameras, Feature extraction, Data models, Information Interfaces and Presentation]
In-situ lighting and reflectance estimations for indoor AR systems
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
We introduce an in-situ lighting and reflectance estimation method that does not require specific light probes and/or preliminary scanning. Our method uses images taken from multiple viewpoints while data accumulation and lighting and reflectance estimations run in the background of the primary AR system. As a result, our method requires little in the way of manipulations for image collection because it consists primarily of image processing and optimization. When used, lighting directions and initial optimization values are estimated via image processing. Eventually, the full parameters are obtained by optimization of the differences between real images. This system uses current best parameters because the parameter estimation and input image updates are run independently.
[indoor AR systems, image updates, augmented reality, optimization values, Electronic mail, light probes, Optimization, Light sources, lighting estimations, Information interfaces and presentation, optimisation, Image color analysis, Lighting, Image Processing and Computer Vision, parameter estimation, lighting directions, real images, reflectance estimation method, image processing, image collection, Estimation, Educational institutions, data accumulation, H.5.1 [Information interfaces and presentation, viewpoints, in-situ lighting]
Kinect for interactive AR anatomy learning
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Education of anatomy is a challenging but crucial element in educating medical professionals, but also for general education of pupils. Our research group has previously developed a prototype of an Augmented Reality (AR) magic mirror which allows intuitive visualization of realistic anatomical information on the user. However, the current overlay is imprecise as the magic mirror depends on the skeleton output from Kinect. These imprecisions affect the quality of education and learning. Hence, together with clinicians we have defined bone landmarks which users can touch easily on their body while standing in front of the sensor. We demonstrate that these landmarks allow the proper deformation of medical data within the magic mirror and onto the human body, resulting in a more precise augmentation.
[AR magic mirror, intuitive visualization, Augmented Reality, augmented reality, Kinect, anatomy education, learning quality, realistic anatomical information, Computed tomography, Education, data visualisation, Bones, skeleton output, Mirrors, interactive AR anatomy learning, Biomedical imaging, biomedical education, bone, human body, medical data deformation, Augmented reality, Anatomy Learning, education quality, bone landmarks, sensor, computer aided instruction, medical computing]
Interactive exploration of augmented aerial scenes with free-viewpoint image generation from pre-rendered images
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
This study proposes a framework to photorealistically synthesize virtual objects and virtualized real-world. We combine the offline rendering of virtual objects and the free-viewpoint image generation to take advantage of the higher quality of offline rendering without the computational cost of online computer graphics (CG) rendering; i.e., it incurs only the cost of the online computation for the free-viewpoint image generation. In addition, the generation of structured viewpoints (e.g., at every grid point) reduces the computational costs required to online process.
[Solid modeling, virtualized real-world, virtual reality, interactive exploration, free-viewpoint image generation, online computer graphics rendering, geophysical image processing, offline rendering, structured viewpoints generation, augmented aerial scenes, CG rendering, prerendered images, Three-dimensional displays, Lighting, Image generation, photorealistic synthesis, Rendering (computer graphics), Cameras, Computational efficiency, virtual objects, rendering (computer graphics), grid point]
Towards intelligent view management: A study of manual text placement tendencies in mobile environments using video see-through displays
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
When viewing content in a see-through head mounted display (HMD), displaying readable information is still difficult when text is overlayed onto a changing background or lighted surface. Moving text or content to a more appropriate place on the screen through automation or intelligent algorithms is one viable solution to this kind of issue. However, many of these algorithms fail to act as a human would when placing text in a more appropriate location in real time. In order to improve these text and view management algorithms, we report the results and analysis of an experiment designed to evaluate user tendencies when placing virtual text in the real world through an HMD. In the conducted experiment, 20 users manually overlayed text in real time onto 4 different videos taken from the first-person perspective of a pedestrian. We find that users have a tendency to place overlayed text in locations near the center of the viewing field, gravitating towards a point just below the horizon. Common locations for text overlay such as walls, shaded areas, and pavement are classified and discussed.
[Algorithm design and analysis, manual text placement, mobile environments, Visualization, Head, see-through head mounted display, Automation, Heads Up Display, Roads, helmet mounted displays, intelligent view management, Head Mounted Display, Augmented reality, View Management, mobile computing, virtual text, video see-through displays, Streaming media, Real-time systems, pedestrian, Text Overlay, HMD]
Psychophysical exploration of stereoscopic pseudo-transparency
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
We report an experiment related to perceiving (virtual) objects in the vicinity of (real) surfaces when using stereoscopic augmented reality displays. In particular, our goal was to explore the effect of various visual surface features on both perception of object location and perception of surface transparency. Surface features were manipulated using random dot patterns on a simulated real object surface, by manipulating dot size, dot density, and whether or not objects placed behind the surface were partially occluded by the surface.
[Visualization, visual surface features, Human factors, augmented reality, Three-dimensional displays, stereoscopic pseudo-transparency, transparency perception, Surface texture, virtual objects, object location perception, pseudo-transparency, Stereo image processing, three-dimensional displays, Educational institutions, surface transparency perception, surface features, Augmented reality, Surface treatment, random dot patterns, dot size manipulation, dot density manipulation, stereo image processing, real object surface, stereoscopic augmented reality display, stereoscopic augmented reality, psychophysical exploration]
User awareness of tracking uncertainties in AR navigation scenarios
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Current Augmented Reality navigation applications for pedestrians usually do not visualize tracking errors. However, tracking uncertainties can accumulate so that the user is presented with a distorted impression of navigation accuracy. To increase the awareness of users about potential imperfections of the tracking at a given time, we alter the visualization of the navigation system. We developed four visualization and error visualization concepts and used a controlled Mixed Reality environment to conduct a pilot study. We found that, while error visualization has the potential to improve AR navigation systems, it is difficult to find suitable visualizations, which are correctly understood among the users.
[Visualization, Target tracking, AR navigation systems, Navigation, error visualization, AR navigation scenarios, navigation accuracy, H.5.1 [Information Interfaces and Presentation, Color, pedestrians, tracking errors, augmented reality, augmented reality navigation applications, Augmented reality, Accuracy, navigation, tracking uncertainties, user awareness, mixed reality environment, Information Interfaces and Presentation]
Augmenting markerless complex 3D objects by combining geometrical and color edge information
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
This paper presents a method to address the issue of augmenting a markerless 3D object with a complex shape. It relies on a model-based tracker which takes advantage of GPU acceleration and 3D rendering in order to handle the complete 3D model, whose sharp edges are efficiently extracted. In the pose estimation step, we propose to robustly combine geometrical and color edge-based features in the nonlinear minimization process, and to integrate multiple-hypotheses in the geometrical edge-based registration phase. Our tracking method shows promising results for augmented reality applications, with a Kinect-based reconstructed 3D model.
[Solid modeling, markerless complex 3D object augmentation, nonlinear minimization process, image registration, augmented reality, pose estimation step, geometrical information, GPU acceleration, Three-dimensional displays, geometrical feature, Image color analysis, feature extraction, 3D visual tracking, model-based tracker, pose estimation, color edge-based feature, augmented reality applications, object tracking, model-based tracking, Robustness, geometrical edge-based registration phase, edge detection, image colour analysis, rendering (computer graphics), sharp edge extraction, Image edge detection, Computational modeling, 3D rendering, tracking method, 3D model, image reconstruction, graphics processing units, Kinect-based reconstructed 3D model, Augmented reality, color edge information, solid modelling]
KITE: Platform for mobile Augmented Reality gaming and interaction using magnetic tracking and depth sensing
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
In this paper, we describe the KITE, a mobile Augmented Reality (AR) platform that uses a magnetic tracker and a depth sensor for games and interaction development that is typically only available on a desktop system. We have achieved this using off-the-shelf hardware and efficient software that can be easily assembled and executed. We demonstrate four possible modalities based on hand input to provide a platform that game and interaction designers can use to explore new possibilities for gaming in AR.
[AR platform, desktop system, interaction development, magnetic tracker, Mobile communication, augmented reality, magnetic tracking, KITE, Augmented reality, gaming platform, mobile computing, Transmitters, depth sensor, computer games, Games, Cameras, depth sensing, Hardware, Software, mobile augmented reality gaming]
Panoramic mapping on a mobile phone GPU
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Creating panoramic images in real-time is an expensive operation for mobile devices. Mapping of individual pixels into the panoramic image is the main focus of this paper, since it is one of the most time consuming parts. The pixel-mapping process is transferred from the Central Processing Unit (CPU) to the Graphics Processing Unit (GPU). The independence of pixels being projected allows OpenGL shaders to perform this operation very efficiently. We propose a shader-based mapping approach and confront it with an existing solution. The application is implemented for Android phones and works fluently on current generation devices.
[image processing, OpenGL shaders, Brightness, Graphics processing units, H.5.1 [Information Interfaces and Presentation, pixel-mapping process, shader-based mapping approach, CPU, Mobile handsets, smart phones, central processing unit, panoramic mapping, mobile phone GPU, graphics processing units, Augmented reality, mobile device, mobile computing, graphics processing unit, Cameras, Real-time systems, Android phones, Information Interfaces and Presentation, Central Processing Unit, panoramic image creation]
Design of an AR marker for cylindrical surface
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
This paper proposes an augmented reality marker that can be robustly detected even on a cylindrical surface. The marker enables the surface normal estimation of a cylindrical object to realize the presentation of appropriate virtual information on the object. Conventional markers have difficulty detecting and obtaining accurate surface normal in the presence of occlusion or distortion of the marker in the image. Furthermore, it is difficult to identify a feature on a cylindrical object on which to position a marker. These problems are resolved by relying on the characteristic that a line parallel to the central axis of the cylinder maintains linearity. In addition, surface normal is calculated by estimating the object's shape by using transformation matrices.
[surface normal estimation, Estimation, Cylindrical surface, virtual information, augmented reality, Vectors, Marker, Multimedia communication, Augmented reality, matrix algebra, augmented reality marker, cylindrical surface, transformation matrices, Cameras, Silicon, AR marker]
Geometric registration for zoomable camera using epipolar constraint and pre-calibrated intrinsic camera parameter change
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
In general, video see-through based augmented reality (AR) cannot change the magnification of camera zooming parameter due to the difficulty of dealing with changes in intrinsic camera parameters. To realize the usage of camera zooming in AR, we propose a novel simultaneous intrinsic and extrinsic camera parameter estimation method based on an energy minimization framework. Our method is composed of the online and offline stages. An intrinsic camera parameter change depending on the zoom values is calibrated in the offline stage. Intrinsic and extrinsic camera parameters are then estimated based on the energy minimization framework in the online stage. In our method, two energy terms are added to the conventional marker-based camera parameter estimation method. One is reprojection errors based on the epipolar constraint. The other is the constraint of continuity of zoom values. By using a novel energy function, our method can estimate accurate intrinsic and extrinsic camera parameters. In an experiment, we confirmed that the proposed method can achieve accurate camera parameter estimation during camera zooming.
[Parameter estimation, augmented reality, energy terms, Electronic mail, Multimedia Information Systems, cameras, zoomable camera, marker-based camera parameter estimation method, reprojection errors, zoom values continuity constraint, Image Processing and Computer Vision, parameter estimation, calibration, Computer vision, energy minimization framework, Estimation, epipolar constraint, intrinsic camera parameter estimation method, Calibration, Augmented reality, H.5.1 [Multimedia Information Systems, AR, energy function, Pattern Recognition, precalibrated intrinsic camera parameter change, Cameras, geometric registration, extrinsic camera parameter estimation method]
Further stabilization of a microlens-array-based fiducial marker
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Fiducial markers (AR/visual markers) are still useful tools in many AR/MR applications. But conventional markers have two fundamental problems in orientation estimation. One is degradation of orientation accuracy in frontal observation. The other is &#x201C;pose ambiguity&#x201D; where the estimated orientation repeats switching between two values. We previously developed a novel marker &#x201C;ArrayMark&#x201D; which uses a microlens array and solved the former problem. This time we propose a practical solution to the latter problem by improving the ArrayMark. We attach an additional reference point to detect the occurrence of invalid estimation, and modify the orientation of the marker by inverting the zenith-angle of the visual-line. This marker enables stable pose estimation from a one-shot image without using any filtering techniques. The method is applicable to conventional markers, too. We demonstrated the availability of this improvement to the pose-ambiguity problem.
[Visualization, stabilization, visual-line, zenith-angle, performance issues, augmented reality, Accuracy, pose ambiguity, filtering techniques, visual marker, pose estimation, AR marker, orientation estimation, position and orientation tracking technology, Estimation, filtering theory, ArrayMark, reference point, pose-ambiguity problem, Augmented reality, Microoptics, microlenses, microlens-array-based fiducial marker, orientation accuracy, Cameras, vision-based registration and tracking, Lenses]
User attention oriented augmented reality on documents with document dependent dynamic overlay
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
When we read a document (any kind of, scientific papers, novels, etc.), we often encounter a situation that the information from the reading document is too less to comprehend what the author(s) would like to convey. In this paper, we demonstrate how the combination of a wearable eye tracker, a see-through head-mounted display (HMD) and an image based document retrieval engine enhances people's reading experiences. By using our proposed system, the reader can get supportive information in the see-through HMD when he wants. A wearable eye tracker and a document retrieval engine are used to detect which line in the document the reader is reading. We propose a method to detect the reader's attention on a word in a reading document, in order to present information at a preferable moment. Furthermore, we also propose a method to project a point of the document to a point of the HMD screen, by calculating the pose of the reading document in the camera image. This projection enables the system to overlay the information dynamically in an augmented view on the reading line. The results from the user study and the experiments show the potential of the proposed system in a practical use case.
[Visualization, Dictionaries, Tracking, image based document retrieval engine, user attention oriented augmented reality, information retrieval, augmented reality, helmet mounted displays, Calibration, Augmented reality, Engines, H.5.2 [INFORMATION INTERFACES AND PRESENTATION (e.g., wearable eye tracker, see-through head-mounted display, camera image, Feature extraction, INFORMATION INTERFACES AND PRESENTATION (e.g., HCI), document dependent dynamic overlay, HMD]
Representational systems with tangible and graphical elements
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
This research centres on the development of a number of prototype interactive systems, each of which uses a tangible means of representation and manipulation of musical elements in musical composition. Data gathered through collaborative prototyping and user studies is analysed using grounded theory methods. The resultant contribution to knowledge includes theory, design criteria and guidelines specific to tangible representations of music. This knowledge will be useful for future design of systems that use tangible representations, particularly for making music. The prototypes themselves also serve as a form of knowledge and as creative works.
[Tangible Interface, Augmented Reality, Creative Support Tools, Qualitative Research, Music Composition, Grounded Theory]
Visuo-haptic augmented reality runtime environment for medical training
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
During the last decade, Visuo-Haptic Augmented Reality (VHAR) systems have emerged that enable users to see and touch digital information that is embedded in the real world. They pose unique problems to developers, including the need for precise augmentations, accurate colocation of haptic devices, and efficient concurrent processing of multiple, realtime sensor inputs to achieve low latency. We think that this complexity is one of the main reasons, why VHAR technology has only been used in few user interface research projects. The proposed project's main objective is to pioneer the development of a widely applicable VHAR runtime environment, which meets the requirements of realtime, low latency operation with precise co-location, haptic interaction with deformable bodies, and realistic rendering, while reducing the overall cost and complexity for developers. A further objective is to evaluate the benefits of VHAR user interfaces with a focus on medical training applications, so that creators of future medical simulators or other haptic applications recognize the potential of VHAR.
[physically-based simulation, mixed reality, medical training, augmented reality, haptic interaction, dataflow architectures]
Visual analytics in Augmented Reality
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
In the last decade, Augmented Reality has become more mature and is widely adopted on mobile devices. Exploring the available information of a user's environment is one of the key applications. However, current mobile Augmented Reality interfaces are very limited compared to the recently emerging big data exploration tools for desktop computers. Our vision is to bring powerful Visual Analytic tools to mobile Augmented Reality.
[mixed reality, context-based, visualisation]
Unified Visual Perception Model for context-aware wearable AR
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
We propose Unified Visual Perception Model (UVPM), which imitates the human visual perception process, for the stable object recognition necessarily required for augmented reality (AR) in the field. The proposed model is designed based on the theoretical bases in the field of cognitive informatics, brain research and psychological science. The proposed model consists of Working Memory (WM) in charge of low-level processing (in a bottomup manner), Long-Term Memory (LTM) and Short-Term Memory (STM), which are in charge of high-level processing (in a top-down manner). WM and LTM/STM are mutually complementary to increase recognition accuracies. By implementing the initial prototype of each boxes of the model, we could know that the proposed model works for stable object recognition. The proposed model is available to support context-aware AR with the optical see-through HMD.
[Image Processing and Computer Vision, H.5.1 [Information Interfaces and Representation (HCI), Information Interfaces and Representation (HCI)]
Management and manipulation of text in dynamic mixed reality workspaces
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Viewing and interacting with text based content safely and easily while mobile has been an issue with see-through displays for many years. For example, in order to effectively use optical see through Head Mounted Displays (HMDs) in constantly changing dynamic environments, variables like lighting conditions, human or vehicular obstructions in a user's path, and scene variation must be dealt with effectively. My PhD research focuses on answering the following questions: 1) What are appropriate methods to intelligently move digital content such as e-mail, SMS messeges, and news articles, throughout the real world? 2) Once a user stops moving, in what way should dynamics of the current workspace change when migrated to a new static environment? 3) Lastly, how can users manipulate mobile content using the fewest number of interactions possible? My strategy for developing solutions to these problems primarily involves automatic or semi-automatic movement of digital content throughout the real world using camera tracking. I have already developed an intelligent text management system that actively manages movement of text in a user's field of view while mobile [11]. I am optimizing and expanding on this type of management system, developing appropriate interaction methodology, and conducting experiments to verify effectiveness, usability, and safety when used with an HMD in various environments.
[Scene Analysis, Heads Up Display, Wearable Display, Content Stabilization, Text Placement, View Management]
Improved outdoor augmented reality through &#x201C;Globalization&#x201D;
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Despite the major interest in live tracking and mapping (e.g., SLAM), the field of augmented reality has yet to truly make use of the rich data provided from large-scale reconstructions generated by structure from motion. This dissertation focuses on extensible tracking and mapping for large-scale reconstructions that enables SfM and SLAM to operate cooperatively to mutually enhance the performance. We describe a multi-user, collaborative augmented reality system that will collectively extend and enhance reconstructions of urban environments at city-scales. Contrary to current outdoor augmented reality systems, this system is capable of continuous tracking through areas previously modeled as well as new, undiscovered areas. Further, we describe a new process called globalization that propagates new visual information back to the global model. Globalization allows for continuous updating of the 3D models with visual data from live users, providing data to fill coverage gaps that are common in 3D reconstructions and to provide the most current view of an environment as it changes over time. The proposed research is a crucial step toward enabling users to augment urban environments with location-specific information at any location in the world for a truly global augmented reality.
[]
Filling the gaps: Hybrid vision and inertial tracking
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Existing head-tracking systems all suffer from various limitations, such as latency, cost, accuracy, or drift. I propose to address these limitations by using depth cameras and existing 3D reconstruction algorithms to simultaneously localize the camera position and build a map of the environment, providing stable and drift-free tracking. This method is enabled by the recent proliferation of light-weight, inexpensive depth cameras. Because these cameras have a relatively slow frame rate, I combine this technique with a low-latency inertial measurement unit to estimate movement between frames. Using the generated environment model, I further propose a collision avoidance system for use with real walking.
[Computer Graphics, I.3.7 [Computer Graphics, Image Processing and Computer Vision]
3D interactions with a passive deformable haptic glove
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
This paper explores enhancing mobile immersive augmented reality manipulations by providing a sense of computer-captured touch through the use of a passive deformable haptic glove that responds to objects in the physical environment. The glove extends our existing pinch glove design with a Digital Foam sensor that is placed under the palm of the hand. The novel glove input device supports a range of touch-activated, precise, direct manipulation modeling techniques with tactile feedback including hole cutting, trench cutting, and chamfer creation. A user evaluation study comparing an image plane approach to our passive deformable haptic glove showed that the glove improves a user's task performance time, decreases error rate and erroneous hand movements, and reduces fatigue.
[Interaction Technique, Augmented Reality, Pinch Gloves, Input Device, Passive Haptics]
Ego- and Exocentric interaction for mobile AR conferencing
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
In this research we explore how a handheld display (HHD) can be used to provide input into an Augmented Reality (AR) conferencing application shown on a head mounted display (HMD). Although AR has successfully been used for many collaborative applications, there has been little research on using HHD and HMD together to enhance remote conferencing. This research investigates two different HHD interfaces and methods for supporting file sharing in an AR conferencing application. A formal evaluation compared four different conditions and found that an Exocentric view and using Visual cues for requesting content produced the best performance. The results were used to create a set of basic design guidelines for future research and application development.
[hand held display, Augmented Reality, mobile, head mounted display, conferencing]
CARMa: Content augmented reality marker
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
The current marker-based augmented reality (AR) rendering has demonstrated good results for online and special purpose applications such as computer-assisted tasks and virtual training. However, it fails to deliver a solution for off-line and generic applications such as augmented books, newspapers, and scientific articles. These applications feature too many markers that imposes a serious challenge on the recognition module. This paper proposes a novel design for augmented reality markers. The proposed marker design employs multi-view orthographic projection to derive dense depth maps and relies on splats rendering for visualisation. The main objective is to interpret the marker rather than recognising it. The proposed marker design stores six depth map projections of the 3D model along with their coloured textures in the marker.
[Marker-based Augmented Reality, Content-based Marker]
Psychophysical exploration of stereoscopic pseudo-transparency
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
We report a two part experiment related to perceiving (virtual) objects in the vicinity of (real) surfaces when using stereoscopic augmented reality displays. In particular, our goal was to explore the effect of various visual surface features on both perception of object location and perception of surface transparency. Surface features were manipulated using random dot patterns on a simulated real object surface, by manipulating dot size, dot density, and whether or not objects placed behind the surface were partially occluded by the surface.
[Human factors, transparency perception, stereoscopic augmented reality pseudo-transparency]
Adapting ray tracing to Spatial Augmented Reality
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Ray tracing is an elegant and intuitive image generation method. The introduction of GPU-accelerated ray tracing and corresponding software frameworks makes this rendering technique a viable option for Augmented Reality applications. Spatial Augmented Reality employs projectors to illuminate physical models and is used in fields that require photorealism, such as design and prototyping. Ray tracing can be used to great effect in this Augmented Reality environment to create scenes of high visual fidelity. However, the peculiarities of SAR systems require that core ray tracing algorithms be adapted to this new rendering environment. This paper highlights the problems involved in using ray tracing in a SAR environment and provides solutions to overcome them. In particular, the following issues are addressed: ray generation, hybrid rendering and view-dependent rendering.
[Augmented Reality, Spatial AR, Ray tracing]
Augmented reality image generation with virtualized real objects using view-dependent texture and geometry
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Augmented reality (AR) images with virtualized real objects can be used for various applications. However, such AR image generation requires hand-crafted 3D models of that objects, which are usually not available. This paper proposes a view-dependent texture (VDT)- and view-dependent geometry (VDG)-based method for generating high quality AR images, which uses 3D models automatically reconstructed from multiple images. Since the quality of reconstructed 3D models is usually insufficient, the proposed method inflates the objects in the depth map as VDG to repair chipped object boundaries and assigns a color to each pixel based on VDT to reproduce the detail of the objects. Background pixel exposure due to inflation is suppressed by the use of the foreground region extracted from the input images. Our experimental results have demonstrated that the proposed method can successfully reduce above visual artifacts.
[view-dependent geometry, AR with virtualized real objects, view-dependent texture]
View management for driver assistance in an HMD
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Head-mounted displays (HMDs) have the potential to overcome some of the technological limitations of currently existing automotive head-up displays (HUDs), such as the limited field of view and the restrictive boundaries of the windshield. However, in a formative study, we identified other, partially known problems with HMDs regarding content stability and occlusion. As a counter-measure we propose a novel layout mechanism for HMD visualization, which, on the one hand, benefits from the unique characteristics of HMDs and, on the other, combines the advantages of head-stabilized and cockpit-stabilized content. By subdividing the HMD's field of view into different slots to which the content is dynamically assigned depending on the user's head rotation, we ensure that the driver's vision is effectively augmented in every possible direction.
[driver assistance, view management, mixed reality, head-mounted display, head-up display]
SIXTH middleware for sensor web enabled AR applications
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
We increasingly live in a world where sensors have become truly ubiquitous in nature. Many of these sensors are an integral part of devices such as smartphones, which contain sufficient sensors to allow for their use as Augmented Reality (AR) devices. This AR experience is limited by the precision and functionality of an individual device's sensors and the its capacity to process the sensor data into a useable form. This paper discuss the current work on a mobile version of the SIXTH middleware which allows for creation of Sensor Web enabled AR applications. This paper discusses current work on mobile SIXTH, which involves the creation of a sensor web between different Android and non-Android devices. This has led to several small demonstrators which are discussed in this work in progress paper. Future work on the project will be outline the aims of the project to allow for the integration of additional devices so as to explore new abilities such as leveraging additional proprieties of those devices.
[Sensor Web, OSGi, OSGi Android Augmented Reality, Android Augmented Reality]
Motion capturing empowered interaction with a virtual agent in an Augmented Reality environment
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
We present an Augmented Reality (AR) system where we immerse the user's whole body in the virtual scene using a motion capturing (MoCap) suit. The goal is to allow for seamless interaction with the virtual content within the AR environment. We describe an evaluation study of a prototype application featuring an interactive scenario with a virtual agent. The scenario contains two conditions: in one, the agent has access to the full tracking data of the MoCap suit and therefore is aware of the exact actions of the user, while in the second condition, the agent does not get this information. We then report and discuss the differences we were able to detect regarding the users' perception of the interaction with the agent and give future research directions.
[Augmented Reality, Virtual Agent, Natural Interaction, Motion Capturing, Full Body Interaction]
Blur with depth: A depth cue method based on blur effect in augmented reality
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
In this paper, a depth cue method based on blur effect in augmented reality is proposed. Different from previous approaches, the proposed method offers an algorithm which estimates the blur effect in the whole scene based on the spatial information in the real world and the intrinsic parameters of the camera. We implemented a prototype using the proposed method and conducted two user tests on how the users might perceive the blur effect rendered by different blurring methods. The test settings are introduced and the results are discussed. The test results show that our blur estimation method is acceptable for moving virtual objects. We also find that the users might prefer a stronger contrast of blur than the blur consistent to the background.
[depth cue, blur, thin lens model, user perception, Augmented reality]
A pilot study for Augmented Reality supported procedure guidance to operate payload racks on-board the International Space Station
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
We present our current state in developing and testing of Augmented Reality supported spaceflight procedures for intra-vehicular payload activities. Our vision is to support the ground team and the flight crew to author and operate easily AR guidelines without programming and AR knowledge. For visualization of the procedural instructions using an HMD, 3D registered visual aids are overlaid onto the payload model operated by additional voice control. Embedded informational resources (e.g., images and videos) are provided through a mobile tangible user interface. In a pilot study that was performed at the ESA European Astronaut Centre by application domain experts, we evaluated the performance, workload and acceptance by comparing our AR system with the conventional method of displaying PDF documents of the procedure.
[usability, aerospace, Augmented reality, HCI]
Comparing pointing and drawing for remote collaboration
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
In this research, we explore using pointing and drawing in a remote collaboration system. Our application allows a local user with a tablet to communicate with a remote expert on a desktop computer. We compared performance in four conditions: (1) Pointers on Still Image, (2) Pointers on Live Video, (3) Annotation on Still Image, and (4) Annotation on Live Video. We found that using drawing annotations would require fewer inputs on an expert side, and would require less cognitive load on the local worker side. In a follow-on study we compared the conditions (2) and (4) using a more complicated task. We found that pointing input requires good verbal communication to be effective and that drawing annotations need to be erased after completing each step of a task.
[Augmented Reality, Video conferencing]
Region-based tracking using sequences of relevance measures
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
We present the preliminary results of our proposal: a region-based detection and tracking method of arbitrary shapes. The method is designed to be robust against orientation and scale changes and also occlusions. In this work, we study the effectiveness of sequence of shape descriptors for matching purpose. We detect and track surfaces by matching the sequences of descriptor so called relevance measures with their correspondences in the database. First, we extract stable shapes as the detection target using Maximally Stable Extreme Region (MSER) method. The keypoints on the stable shapes are then extracted by simplifying the outline of the stable regions. The relevance measures that are composed by three keypoints are then computed and the sequences of them are composed as descriptors. During runtime, the sequences of relevance measures are extracted from the captured image and are matched with those in the database. When a particular region is matched with one in the database, the orientation of the region is then estimated and virtual annotations can be superimposed. We apply this approach in an interactive task support system that helps users for creating paper craft objects.
[Artificial, augmented, virtual realities]
Consider your clutter: Perception of virtual object motion in AR
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Background motion and visual clutter are present in almost all augmented reality applications. However, there is minimal prior work that has investigated the effects that background motion and clutter (e.g., a busy city street) can have on the perception of virtual object motion in augmented reality. To investigate these issues, we conducted an experiment in which participants' perceptions of changes in overlaid virtual object velocity were evaluated on a black background and a high clutter/motion background. Results offer insights into the impact that background clutter and motion has on perception in augmented reality.
[user study, Augmented Reality, perception]
Bare hand natural interaction with augmented objects
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
In this work in progress we address the problem of interacting with augmented objects. A bare hand tracking technique is developed, which allied to gesture recognition heuristics, enables interaction with augmented objects in an intuitive way. The tracking algorithm uses a flock of features approach that tracks both hands in real time. The interaction occurs by the execution of grasp and release gestures. Physics simulation and photorealistic rendering are added to the pipeline. This way, the tool provides more coherent feedback in order to make the virtual objects look and respond more likely real ones. The pipeline was tested through specific tasks, designed to analyze its performance regarding the easiness of use, precision and response time.
[augmented objects, bare hand tracking, virtual grasp]
Using a HHD with a HMD for mobile AR interaction
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Mobile Augmented Reality (AR) applications are typically deployed either on head mounted displays (HMD) or handheld displays (HHD). This paper explores novel interaction techniques for a combined HHD-HMD hybrid system that builds on the strengths of each type of device. We use the HMD for viewing AR content and a touch screen HHD for interacting with the content. A prototype system was developed and a user study was conducted comparing four interaction techniques for selection tasks.
[gesture interaction, head-mounted display, Augmented reality, wearable computer, handheld display]
A projected augmented reality system for remote collaboration
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
This paper describes an AR system for remote collaboration using a captured 3D model of the local user's scene. In the system a remote user can manipulate the scene independently of the view of the local user and add AR annotations that appear projected into the real world. Results from a pilot study and the design of a further full study are presented.
[Kinect Fusion, Remote collaboration, Augmented Reality, Projection]
Towards object based manipulation in remote guidance
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
This paper presents a method for using object based manipulation and spatial augmented reality for the purpose of remote guidance. Previous remote guidance methods have typically not made use of any semantic information about the physical properties of the environment and require the helper and worker to provide context. Our new prototype system introduces a level of abstraction to the remote expert, allowing them to directly specify the object movements required of a local worker. We use 3D tracking to create a hidden virtual reality scene, mirroring the real world, with which the remote expert interacts while viewing a camera feed of the physical workspace. The intended manipulations are then rendered to the local worker using Spatial Augmented Reality (SAR). We report on the implementation of a functional prototype that demonstrates an instance of this approach. We anticipate that techniques such as the one we present will allow more efficient collaborative remote guidance in a range of physical tasks.
[physical workspace, Visualization, 3D CHI, collaborative remote guidance, object based manipulation, SAR, camera feed, Multi touch interaction, augmented reality, Object Manipulation, hidden virtual reality scene, Three-dimensional displays, Prototypes, spatial augmented reality, 3D tracking, physical property, Spatially Augmented Reality, Augmented reality, hidden feature removal, prototype system, remote expert, Cameras, human computer interaction, Remote Guidance, Feeds, Australia, semantic information]
Tangible interaction techniques to support asynchronous collaboration
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Industrial uses of Augmented Reality (AR) are growing, however their uses are consistently fashioned with an emphasis on consumption, delivering additional information to the worker to assist them in the completion of their job. A promising alternative is to allow user data creation during the actual process by the worker performing their duties. This not only allows spatially located annotations to be produced, it also allows an AR scene to be developed in-situ and in real-time. Tangible markers offer a physical interface while also creating physical containers to allow for fluent interactions. This form factor allows both attached and detached annotations, whilst allowing the creation of an AR scene during the process. This annotated scene will allow asynchronous collaboration to be conducted between multiple stakeholders, both locally and remotely. In this paper we discuss our reasoning behind such an approach, and present the current work on our prototype created to test and validate our proposition.
[tangible markers, physical containers, Augmented Reality, haptic interfaces, Mobile communication, augmented reality, asynchronous collaboration, fluent interactions, tangible interaction techniques, Wearable computers, AR scene, Prototypes, physical interface, Context, job completion, Tangible Interaction, Spatial Annotations, Augmented reality, attached annotations, detached annotations, Collaboration, user data creation, human computer interaction, Asynchronous Collaboration, Australia]
Markerless 3D gesture-based interaction for handheld Augmented Reality interfaces
2013 IEEE International Symposium on Mixed and Augmented Reality
None
2013
Conventional 2D touch-based interaction methods for handheld Augmented Reality (AR) cannot provide intuitive 3D interaction due to a lack of natural gesture input with real-time depth information. The goal of this research is to develop a natural interaction technique for manipulating virtual objects in 3D space on handheld AR devices. We present a novel method that is based on identifying the positions and movements of the user's fingertips, and mapping these gestures onto corresponding manipulations of the virtual objects in the AR scene. We conducted a user study to evaluate this method by comparing it with a common touch-based interface under different AR scenarios. The results indicate that although our method takes longer time, it is more natural and enjoyable to use.
[touch-based interface, handheld AR devices, 3D interaction technique, 2D touch-based interaction methods, markerless 3D gesture-based interaction, Thumb, handheld augmented reality interfaces, Educational institutions, augmented reality, Mobile handsets, real-time depth information, user fingertip position identification, fingertip detection, handheld augmented reality, Augmented reality, virtual object manipulation, natural gesture interaction, 3D, Three-dimensional displays, user fingertip movement identification, gesture recognition, Cameras, gesture mapping, natural interaction technique]
Message from the general chairs
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from the science &amp; technology paper chairs
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from the poster chairs
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from the demo chairs
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from the doctoral consortium chairs
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
IEEE Visualization and Graphics Technical Committee (VGTC)
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Provides a listing of current committee members and society officers.
[]
Symposium committee
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Provides a listing of current committee members and society officers.
[]
Steering committee
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Provides a listing of current committee members and society officers.
[]
Keynote address: The role of augmented reality displays for guiding intra-cardiac interventions
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Summary form only given. Many inter-cardiac interventions are performed either via open-heart surgery, or using minimally invasive approaches, where instrumentation is introduced into the cardiac chambers via the vascular system or heart wall. While many of the latter procedures are often employed under x-ray guidance, for some of these xray imaging is not appropriate, and ultrasound is the preferred intra-operative imaging modality. Two such procedures involves the repair of a mitral-valve leafet, and the replacement of aortic valves. Both employ instruments introduced into the heart via the apex. For the mitral procedure, the standard of care for this procedure employs a 3D Trans-esophageal echo (TEE) probe as guidance, but using primarily its bi-plane mode, with full 3D only being used sporadically. In spite of the clinical success of this procedure, many problems are encountered during the navigation of the instrument to the site of the therapy. To overcome these difficulties, we have developed a guidance platform that tracks the US probe and instrument, and augments the US mages with virtual elements representing the instrument and target, to optimise the navigation process. Results of using this approach on animal studies have demonstrated increased performance in multiple metrics, including total tool distance from ideal pathway, total navigation time, and total tool path lengths, by factors of 3,4, and 5 respectively, as well as a 40 fold reduction in the number of times an instrument intruded into potentially unsafe zones in the heart.
[bi-plane mode, mitral-valve leafet, biomedical ultrasonics, augmented reality, TEE probe, augmented reality display, cardiology, ultrasound, 3D trans-esophageal echo, imaging modality, X-ray guidance, open-heart surgery, mitral procedure, minimally invasive approach, medical image processing, intra-cardiac intervention guidance, surgery]
Keynote Address: Seeing Anew: Paradigm shifting across the virtuality continuum
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
As an original pioneer of Virtual Reality, Tom Furness, founder of the international network of HIT Labs, presents both the legacy and future of Mixed and Augmented Reality through innovations that unlock and link minds. Inventing some of the original inspirations that led to the Google Glass and Oculus Rift, his keynote will inspire making creative leaps to realize the promise of research. Since the 1960's, his career has been about how to achieve radical innovation through helping others to &#x201C;see anew.&#x201D; In inspiring a paradigm shift in thinking about the virtuality continuum, his work and global network of labs has shifted the thinking from being about function to purpose; perception to performance; from capabilities to possibilities. &#x201C;The virtuality continuum for me has been really about really bridging between the wonders of the real world and the infnite possibilities of the human imagination.&#x201D; As Mixed and Augmented Reality innovation begins to transform everyday life, Dr. Furness presents the future role of trans disciplinary research and curriculum in melting the boundaries between Science, Technology, Media, Art, Humanities and Design to teach the next generation of innovators. He shares his future venture of the Virtual World Society that is creating a collaborative initiative of experiential learning, using virtual, mixed and augmented reality as a platform to empower youth to envision and create a better world for tomorrow.
[]
AR-IVI &#x2014; Implementation of In-Vehicle Augmented Reality
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
In the last three years, a number of automotive Augmented Reality (AR) concepts and demonstrators have been presented, all looking for an interpretation of what AR in a car may look like. In October 2013, Mercedes-Benz exhibited to a public audience the AR In-Vehicle Infotainment (AR-IVI) system aimed at defining an overall in-vehicle electric/electronic (E/E) architecture for augmented reality rather than showing specific use cases. In this paper, we explain the requirements and design decisions that lead to the system-design, and we share the challenges and experiences in developing the AR-IVI system in the prototype vehicle. Based on our experiences, we give an outlook on future software and E/E architectural challenges of in-vehicle augmented reality.
[Roads, H.5.1 [Information Interfaces and Presentation, AR-IVI system, augmented reality, electronic engineering computing, Augmented reality, Vehicles, Automotive engineering, Global Positioning System, in-vehicle augmented reality, Computer architecture, automotive electronics, Mercedes-Benz, Information Interfaces and Presentation, Sensors, in-vehicle electric-electronic architecture, AR in-vehicle infotainment system]
Thermal touch: Thermography-enabled everywhere touch interfaces for mobile augmented reality applications
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
We present an approach that makes any real object a true touch interface for mobile Augmented Reality applications. Using infrared thermography, we detect residual heat resulting from a warm fingertip touching the colder surface of an object. This approach can clearly distinguish if a surface has actually been touched, or if a finger only approached it without any physical contact, and hence significantly less heat transfer. Once a touch has been detected in the thermal image, we determine the corresponding 3D position on the touched object based on visual object tracking using a visible light camera. Finally the 3D position of the touch is used by human machine interfaces for Augmented Reality providing natural means to interact with real and virtual objects. The emergence of wearable computers and head-mounted displays desires for alternatives to a touch screen, which is the primary user interface in handheld Augmented Reality applications. Voice control and touchpads provide a useful alternative to interact with wearables for certain tasks, but particularly common interaction tasks in Augmented Reality require to accurately select or define 3D points on real surfaces. We propose to enable this kind of interaction by simply touching the respective surface with a fingertip. Based on tests with a variety of different materials and different users, we show that our method enables intuitive interaction for mobile Augmented Reality with most common objects.
[residual heat detection, mobile augmented reality application, touchpads, Materials, visible light camera, augmented reality, warm fingertip, user interfaces, Multimedia Information Systems, Three-dimensional displays, mobile computing, voice control, visual object tracking, object tracking, infrared thermography, head-mounted displays, User Interfaces, thermography-enabled everywhere touch interfaces, Augmented reality, Temperature measurement, wearable computers, Heating, H.5.2 [User Interfaces, thermal touch, User interfaces, Cameras, heat transfer]
AR-mentor: Augmented reality based mentoring system
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
AR-Mentor is a wearable real time Augmented Reality (AR) mentoring system that is configured to assist in maintenance and repair tasks of complex machinery, such as vehicles, appliances, and industrial machinery. The system combines a wearable Optical-See-Through (OST) display device with high precision 6-Degree-Of-Freedom (DOF) pose tracking and a virtual personal assistant (VPA) with natural language, verbal conversational interaction, providing guidance to the user in the form of visual, audio and locational cues. The system is designed to be heads-up and hands-free allowing the user to freely move about the maintenance or training environment and receive globally aligned and context aware visual and audio instructions (animations, symbolic icons, text, multimedia content, speech). The user can interact with the system, ask questions and get clarifications and specific guidance for the task at hand. A pilot application with AR-Mentor was successfully built to instruct a novice to perform an advanced 33-step maintenance task on a training vehicle. The initial live training tests demonstrate that AR-Mentor is able to help and serve as an assistant to an instructor, freeing him/her to cover more students and to focus on higher-order teaching.
[visual cue, Visualization, verbal conversational interaction, VPA, wearable realtime augmented reality, machinery repair task, augmented reality, user interaction, OST display device, Virtual Personal Assistant, Mentoring System, Wearable Technology, Vehicles, Training, higher-order teaching, Three-dimensional displays, Databases, user guidance, computer displays, wearable optical-see-through display device, AR-mentor system, locational cue, virtual personal assistant, machinery maintenance task, Maintenance engineering, Optical See Through Glasses, pose tracking, augmented reality based mentoring system, audio cue, Speech, computer aided instruction, natural language]
Towards Augmented Reality user interfaces in 3D media production
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
The idea of using Augmented Reality (AR) user interfaces (UIs) to create 3D media content, such as 3D models for movies and games has been repeatedly suggested over the last decade. Even though the concept is intuitively compelling and recent technological advances have made such an application increasingly feasible, very little progress has been made towards an actual real-world application of AR in professional media production. To this day, no immersive 3D UI has been commonly used by professionals for 3D computer graphics (CG) content creation. In this paper, we are first to publish a requirements analysis for our target application in the professional domain. Based on a survey that we conducted with media professionals, the analysis of professional 3D CG software, and professional training tutorials, we identify these requirements and put them into the context of AR UIs. From these findings, we derive several interaction design principles that aim to address the challenges of real-world application of AR to the production pipeline. We implemented these in our own prototype system while receiving feedback from media professionals. The insights gained in the survey, requirements analysis, and user interface design are relevant for research and development aimed at creating production methods for 3D media production.
[Solid modeling, professional media production, Navigation, professional 3D CG software, Augmented Reality, professional training tutorials, Media, augmented reality, Immersive Authoring, user interfaces, CG content creation, requirements analysis, Three-dimensional displays, UI, Collaboration, Production, 3D media production, Software, augmented reality user interfaces, 3D models, 3D media content, solid modelling, 3D computer graphics]
Interactive near-field illumination for photorealistic augmented reality on mobile devices
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Mobile devices become more and more important today, especially for augmented reality (AR) applications in which the camera of the mobile device acts like a window into the mixed reality world. Up to now, no photorealistic augmentation is possible since the computational power of the mobile devices is still too weak. Even a streaming solution from a stationary PC would cause a latency that affects user interactions considerably. Therefore, we introduce a differential illumination method that allows for a consistent illumination of the inserted virtual objects on mobile devices, avoiding a delay. The necessary computation effort is shared between a stationary PC and the mobile devices to make use of the capacities available on both sides. The method is designed such that only a minimum amount of data has to be transferred asynchronously between the stationary PC and one or multiple mobile devices. This allows for an interactive illumination of virtual objects with a consistent appearance under both temporally and spatially varying real illumination conditions. To describe the complex near-field illumination in an indoor scenario, multiple HDR video cameras are used to capture the illumination from multiple directions. In this way, sources of illumination can be considered that are not directly visible to the mobile device because of occlusions and the limited field of view of built-in cameras.
[AR application, photorealistic augmented reality, interactive near-field illumination, Information Interfaces and Representation, augmented reality, Mobile handsets, illumination condition, illumination source, lighting, Image reconstruction, Light sources, mobile device, differential illumination method, Computer Graphics, mobile computing, mixed reality, Lighting, I.3.7 [Computer Graphics, Streaming media, Cameras, Rendering (computer graphics), virtual objects, necessary computation effort]
Delta Voxel Cone Tracing
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Mixed reality applications which must provide visual coherence between synthetic and real objects need relighting solutions for both: synthetic objects have to match lighting conditions of their real counterparts, while real surfaces need to account for the change in illumination introduced by the presence of an additional synthetic object. In this paper we present a novel relighting solution called Delta Voxel Cone Tracing to compute both direct shadows and first bounce mutual indirect illumination. We introduce a voxelized, pre-filtered representation of the combined real and synthetic surfaces together with the extracted illumination difference due to the augmentation. In a final gathering step this representation is cone-traced and superimposed onto both types of surfaces, adding additional light from indirect bounces and synthetic shadows from anti-radiance present in the volume. The algorithm computes results at interactive rates, is temporally coherent and to our knowledge provides the first real-time rasterizer solution for mutual diffuse, glossy and perfect specular indirect reflections between synthetic and real surfaces in mixed reality.
[augmented reality, mutual diffuse reflections, Surface roughness, glossy reflections, Rough surfaces, lighting, relighting solutions, Image reconstruction, Light sources, Delta Radiance Fields, Geometry, mixed reality application, Real-time Global Illumination, direct shadows, Lighting, Virtual reality, delta voxel cone tracing, Relighting, lighting conditions, visual coherence, indirect reflections, Mixed Reality, Voxel Cone Tracing, first bounce mutual indirect illumination]
P-HRTF: Efficient personalized HRTF computation for high-fidelity spatial sound
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Accurate rendering of 3D spatial audio for interactive virtual auditory displays requires the use of personalized head-related transfer functions (HRTFs). We present a new approach to compute personalized HRTFs for any individual using a method that combines state-of-the-art image-based 3D modeling with an efficient numerical simulation pipeline. Our 3D modeling framework enables capture of the listener's head and torso using consumer-grade digital cameras to estimate a high-resolution non-parametric surface representation of the head, including the extended vicinity of the listener's ear. We leverage sparse structure from motion and dense surface reconstruction techniques to generate a 3D mesh. This mesh is used as input to a numeric sound propagation solver, which uses acoustic reciprocity and Kirchhoff surface integral representation to efficiently compute an individual's personalized HRTF. The overall computation takes tens of minutes on multi-core desktop machine. We have used our approach to compute the personalized HRTFs of few individuals, and we present our preliminary evaluation here. To the best of our knowledge, this is the first commodity technique that can be used to compute personalized HRTFs in a lab or home setting.
[Solid modeling, Pipelines, P-HRTF, 3D spatial audio rendering, nonparametric surface representation, 3D mesh generation, Three-dimensional displays, acoustic reciprocity, numerical analysis, Numerical models, rendering (computer graphics), high-fidelity spatial sound, numerical simulation pipeline, numeric sound propagation solver, interactive virtual auditory displays, multiprocessing systems, Computational modeling, image reconstruction, sparse structure from motion technique, commodity technique, image motion analysis, Kirchhoff surface integral representation, multicore desktop machine, personalized head-related transfer functions, Ear, image representation, image-based 3D modeling, dense surface reconstruction technique, solid modelling]
Visibility-based blending for real-time applications
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
There are many situations in which virtual objects are presented half-transparently on a background in real time applications. In such cases, we often want to show the object with constant visibility. However, using the conventional alpha blending, visibility of a blended object substantially varies depending on colors, textures, and structures of the background scene. To overcome this problem, we present a framework for blending images based on a subjective metric of visibility. In our method, a blending parameter is locally and adaptively optimized so that visibility of each location achieves the targeted level. To predict visibility of an object blended by an arbitrary parameter, we utilize one of the error visibility metrics that have been developed for image quality assessment. In this study, we demonstrated that the metric we used can linearly predict visibility of a blended pattern on various texture images, and showed that the proposed blending methods can work in practical situations assuming augmented reality.
[image processing, Visualization, blended object visibility, Computational modeling, blending parameter, Transforms, Predictive models, augmented reality, alpha blending, error visibility metrics, Human visual system model, image blending, Sensitivity, Image color analysis, Blending, image quality assessment, Visibility, virtual objects, blending, rendering (computer graphics), visibility-based blending]
Grasp-Shell vs gesture-speech: A comparison of direct and indirect natural interaction techniques in augmented reality
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
In order for natural interaction in Augmented Reality (AR) to become widely adopted, the techniques used need to be shown to support precise interaction, and the gestures used proven to be easy to understand and perform. Recent research has explored free-hand gesture interaction with AR interfaces, but there have been few formal evaluations conducted with such systems. In this paper we introduce and evaluate two natural interaction techniques: the free-hand gesture based Grasp-Shell, which provides direct physical manipulation of virtual content; and the multi-modal Gesture-Speech, which combines speech and gesture for indirect natural interaction. These techniques support object selection, 6 degree of freedom movement, uniform scaling, as well as physics-based interaction such as pushing and flinging. We conducted a study evaluating and comparing Grasp-Shell and Gesture-Speech for fundamental manipulation tasks. The results show that Grasp-Shell outperforms Gesture-Speech in both efficiency and user preference for translation and rotation tasks, while Gesture-Speech is better for uniform scaling. They could be good complementary interaction methods in a physics-enabled AR environment, as this combination potentially provides both control and interactivity in one interface. We conclude by discussing implications and future directions of this research.
[Shape, augmented reality, user interfaces, pushing interaction, Three-dimensional displays, rotation task, object selection, free-hand gesture interaction, Kinematics, user preference, natural interaction, multi-modal interface, Thumb, Grasping, physics-based interaction, Augmented reality, AR, natural interaction techniques, translation task, Speech, Cameras, gesture-speech technique, virtual content manipulation, flinging interaction, grasp-shell technique]
Improving co-presence with augmented visual communication cues for sharing experience through video conference
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Video conferencing is becoming more widely used in areas other than face-to-face conversation, such as sharing real world experience with remote friends or family. In this paper we explore how adding augmented visual communication cues can improve the experience of sharing remote task space and collaborating together. We developed a prototype system that allows users to share live video view of their task space taken on a Head Mounted Display (HMD) or Handheld Display (HHD), and communicate through not only voice but also using augmented pointer or annotations drawn on the shared view. To explore the effect of having such an interface for remote collaboration, we conducted a user study comparing three video-conferencing conditions with different combination of communication cues: (1) voice only, (2) voice + pointer, and (3) voice + annotation. The participants used our remote collaboration system to share a parallel experience of puzzle solving in the user study, and we found that adding augmented visual cues significantly improved the sense of being together. The pointer was the most preferred additional cue by users for parallel experience, and there were different states of the users' behavior found in remote collaboration.
[Visualization, Tracking, HHD, Augmented Reality, augmented reality, telecommunication computing, teleconferencing, handheld display, live video sharing, voice plus annotation cue, Visual communication, face-to-face conversation, remote task space sharing, Prototypes, head mounted display, voice only cue, video communication, HMD, parallel experience, augmented visual communication cue, voice plus pointer cue, helmet mounted displays, video conference, Video Conferencing, Collaboration, Streaming media, Cameras, user behavior]
A study of depth perception in hand-held augmented reality using autostereoscopic displays
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Displaying three-dimensional content on a flat display is bound to reduce the impression of depth, particularly for mobile video see-trough augmented reality. Several applications in this domain can benefit from accurate depth perception, especially if there are contradictory depth cues, like occlusion in a x-ray visualization. The use of stereoscopy for this effect is already prevalent in head-mounted displays, but there is little research on the applicability for hand-held augmented reality. We have implemented such a prototype using an off-the-shelf smartphone equipped with a stereo camera and an autostereoscopic display. We designed and conducted an extensive user study to explore the effects of stereoscopic hand-held augmented reality on depth perception. The results show that in this scenario depth judgment is mostly influenced by monoscopic depth cues, but our system can improve positioning accuracy in challenging scenes.
[stereo camera, Visualization, Stereo image processing, Autostereoscopy, user study, augmented reality, autostereoscopic displays, X-ray visualization, handheld augmented reality, Augmented reality, depth cue, Accuracy, Three-dimensional displays, three-dimensional content, mobile video see-trough augmented reality, computer displays, occlusion, mobile devices, Cameras, Hardware, depth perception]
Measurements of live actor motion in mixed reality interaction
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
This paper presents a method for measuring the magnitude and impact of errors in mixed reality interactions. We define the errors as measurements of hand placement accuracy and consistency within bimanual movement of an interactive virtual object. First, a study is presented which illustrates the amount of variability between the hands and the mean distance of the hands from the surfaces of a common virtual object. The results allow a discussion of the most significant factors which should be considered in the frame of developing realistic mixed reality interaction systems. The degree of error was found to be independent of interaction speed, whilst the size of virtual object and the position of the hands are significant. Second, a further study illustrates how perceptible these errors are to a third person viewer of the interaction (e.g. an audience member). We found that interaction errors arising from the overestimation of an object surface affected the visual credibility for the viewer considerably more than an underestimation of the object. This work is presented within the application of a real-time Interactive Virtual Television Studio, which offers convincing realtime interaction for live TV production. We believe the results and methodology presented here could also be applied for designing, implementing and assessing interaction quality in many other Mixed Reality applications.
[Visualization, Observers, mixed reality interaction system, augmented reality, Standards, Videos, live actor motion measurement, Interactive Virtual Studios, hand placement accuracy, bimanual movement, third person interaction viewer, Measurement uncertainty, Interaction Framework, Performance Measurement, Virtual reality, interactive virtual television studio, Mixed Reality, interactive virtual object]
Improved registration for vehicular AR using auto-harmonization
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
This paper describes the design, development and testing of an AR system that was developed for aerospace and ground vehicles to meet stringent accuracy and robustness requirements. The system uses an optical see-through HMD, and thus requires extremely low latency, high tracking accuracy and precision alignment and calibration of all subsystems in order to avoid mis-registration and &#x201C;swim&#x201D;. The paper focuses on the optical/inertial hybrid tracking system and describes novel solutions to the challenges with the optics, algorithms, synchronization, and alignment with the vehicle and HMD systems. A system accuracy analysis is presented with simulation results to predict the registration accuracy. Finally, a car test is used to create a through-the-eyepiece video demonstrating well-registered augmentations of the road and nearby structures while driving.
[image registration, augmented reality, subsystem precision alignment, car test, AR system development, Vehicles, AR system testing, Accuracy, AR system design, through-the-eyepiece video, object tracking, optical-inertial hybrid tracking system, hybrid tracking, optical see-through HMDsubsystems, subsystem calibration, video signal processing, calibration, see-through HMD, registration accuracy prediction, Optical imaging, helmet mounted displays, Calibration, vehicular AR registration improvement, inertial, Augmented reality, subsystem tracking accuracy, Cameras, optical see-through HMD, registration, Optical sensors, subsystem latency, Aircraft, autoharmonization]
Real-time illumination estimation from faces for coherent rendering
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
We present a method for estimating the real-world lighting conditions within a scene in real-time. The estimation is based on the visual appearance of a human face in the real scene captured in a single image of a monocular camera. In hardware setups featuring a user-facing camera, an image of the user's face can be acquired at any time. The limited range in variations between different human faces makes it possible to analyze their appearance offline, and to apply the results to new faces. Our approach uses radiance transfer functions - learned offline from a dataset of images of faces under different known illuminations - for particular points on the human face. Based on these functions, we recover the most plausible real-world lighting conditions for measured reflections in a face, represented by a function depending on incident light angle using Spherical Harmonics. The pose of the camera relative to the face is determined by means of optical tracking, and virtual 3D content is rendered and overlaid onto the real scene with a fixed spatial relationship to the face. By applying the estimated lighting conditions to the rendering of the virtual content, the augmented scene is shaded coherently with regard to the real and virtual parts of the scene. We show with different examples under a variety of lighting conditions, that our approach provides plausible results, which considerably enhance the visual realism in real-time Augmented Reality applications.
[Solid modeling, virtual content, Estimation, optical tracking, visual realism, augmented reality, coherent rendering, lighting, Geometry, spherical harmonics, Three-dimensional displays, realtime illumination estimation, incident light angle, Lighting, visual appearance, augmented reality applications, Cameras, Rendering (computer graphics), monocular camera, user-facing camera, rendering (computer graphics), radiance transfer functions, lighting condition]
Comprehensive workspace calibration for visuo-haptic augmented reality
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Visuo-haptic augmented reality systems enable users to see and touch digital information that is embedded in the real world. Precise co-location of computer graphics and the haptic stylus is necessary to provide a realistic user experience. PHANToM haptic devices are often used in such systems to provide haptic feedback. They consist of two interlinked joints, whose angles define the position of the haptic stylus and three sensors at the gimbal to sense its orientation. Previous work has focused on calibration procedures that align the haptic workspace within a global reference coordinate system and developing algorithms that compensate the non-linear position error, caused by inaccuracies in the joint angle sensors. In this paper, we present an improved workspace calibration that additionally compensates for errors in the gimbal sensors. This enables us to also align the orientation of the haptic stylus with high precision. To reduce the required time for calibration and to increase the sampling coverage, we utilize time-delay estimation to temporally align external sensor readings. This enables users to continuously move the haptic stylus during the calibration process, as opposed to commonly used point and hold processes. We conducted an evaluation of the calibration procedure for visuo-haptic augmented reality setups with two different PHANToMs and two different optical trackers. Our results show a significant improvement of orientation alignment for both setups over the previous state of the art calibration procedure. Improved position and orientation accuracy results in higher fidelity visual and haptic augmentations, which is crucial for fine-motor tasks in areas including medical training simulators, assembly planning tools, or rapid prototyping applications. A user friendly calibration procedure is essential for real-world applications of VHAR.
[Artificial, Visualization, optical tracking, haptic interfaces, augmented reality, VHAR, visual augmentations, sampling coverage, Phantoms, visuo-haptic augmented reality systems, point and hold processes, Sensors, global reference coordinate system, position accuracy, comprehensive workspace calibration, Joints, calibration, haptic stylus, assembly planning tools, Target tracking, error compensation, sampling methods, joint angle sensors, augmented and virtual realities, haptic augmentations, realistic user experience, Calibration, Haptic interfaces, touch digital information, haptic workspace, improved workspace calibration, computer graphics, orientation accuracy, user friendly calibration procedure, nonlinear position error, H.5.1. [Information Interfaces and Presentation, PHANToM haptic devices, gimbal sensors, calibration process, haptic feedback, calibration procedures, time-delay estimation, optical trackers, Information Interfaces and Presentation, Haptic I/O, rapid prototyping applications, medical training simulators]
Recognition and reconstruction of transparent objects for augmented reality
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Dealing with real transparent objects for AR is challenging due to their lack of texture and visual features as well as the drastic changes in appearance as the background, illumination and camera pose change. The few existing methods for glass object detection usually require a carefully controlled environment, specialized illumination hardware or ignore information from different viewpoints. In this work, we explore the use of a learning approach for classifying transparent objects from multiple images with the aim of both discovering such objects and building a 3D reconstruction to support convincing augmentations. We extract, classify and group small image patches using a fast graph-based segmentation and employ a probabilistic formulation for aggregating spatially consistent glass regions. We demonstrate our approach via analysis of the performance of glass region detection and example 3D reconstructions that allow virtual objects to interact with them.
[object recognition, object discovery, graph-based segmentation, image classification, graph theory, Augmented Reality, Glass, augmented reality, Image reconstruction, transparent object reconstruction, spatially-consistent glass region aggregation, Three-dimensional displays, 3D reconstruction, object appearance, image segmentation, visual features, texture features, transparent object recognition, edge detection, virtual objects, Distortion measurement, learning (artificial intelligence), transparent object classification, multiple images, learning approach, image patch classification, Image edge detection, glass region detection, image reconstruction, glass object detection, image texture, Image segmentation, Glass detection, probabilistic formulation, image patch extraction, 3D Reconstruction, image patch grouping, Cameras, Computer Vision]
Pixel-wise closed-loop registration in video-based augmented reality
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
In Augmented Reality (AR), visible misregistration can be caused by many inherent error sources, such as errors in tracking, calibration, and modeling. In this paper we present a novel pixel-wise closed-loop registration framework that can automatically detect and correct registration errors using a reference model comprised of the real scene model and the desired virtual augmentations. Registration errors are corrected in both global world space via camera pose refinement, and local screen space via pixel-wise corrections, resulting in spatially accurate and visually coherent registration. Specifically we present a registration-enforcing model-based tracking approach that weights important image regions while refining the camera pose estimates (from any conventional tracking method) to achieve better registration, even in the case of modeling errors. To deal with remaining errors, which can be rigid or non-rigid, we compute the optical flow between the camera image and the real model image rendered with the refined pose, enabling direct screen-space pixel-wise corrections to misregistration. The estimated flow field can be applied to improve registration in two distinct ways: (1) forward warping of modeled on-real-object-surface augmentations (e.g., object re-texturing) into the camera image, leading to surface details that are not present in the virtual object; and (2) backward warping of the camera image into the real scene model, preserving the full use of the dense geometry buffer (depth in particular) provided by the combined real-virtual model for registration, leading to pixel accurate real-virtual occlusion. We discuss the trade-offs between, and different use cases of, forward and backward warping with model-based tracking in terms of specific properties for registration. We demonstrate the efficacy of our approach with both simulated and real data.
[tracking error, Solid modeling, image registration, H.5.1 [Information Interfaces and Presentation, backward warping, augmented reality, virtual augmentation, dense geometry buffer, Three-dimensional displays, registration-enforcing model-based tracking approach, pose estimation, Image Processing and Computer Vision, object tracking, Mathematical model, rendering (computer graphics), video signal processing, calibration error, real-virtual occlusion, Computational modeling, rendering, modeling error, AR, camera pose estimation, direct screen-space pixel-wise corrections, pixel-wise closed-loop registration framework, forward warping, Cameras, video-based augmented reality, Information Interfaces and Presentation, Integrated circuit modeling]
Semi-dense visual odometry for AR on a smartphone
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
We present a direct monocular visual odometry system which runs in real-time on a smartphone. Being a direct method, it tracks and maps on the images themselves instead of extracted features such as keypoints. New images are tracked using direct image alignment, while geometry is represented in the form of a semi-dense depth map. Depth is estimated by filtering over many small-baseline, pixel-wise stereo comparisons. This leads to significantly less outliers and allows to map and use all image regions with sufficient gradient, including edges. We show how a simple world model for AR applications can be derived from semi-dense depth maps, and demonstrate the practical applicability in the context of an AR application in which simulated objects can collide with real geometry.
[Tracking, augmented reality, image tracking, Mapping, AR applications, direct image alignment, semidense depth map, Accuracy, Simultaneous localization and mapping, feature extraction, Semi-Dense, pixel-wise stereo comparison, object tracking, Real-time systems, Image resolution, direct monocular visual odometry system, filtering theory, depth estimation, smart phone, Direct Visual Odometry, smart phones, AR, NEON, 3D Reconstruction, stereo image processing, Cameras, Feature extraction, semidense visual odometry, Mobile Devices]
Sticky projections &#x2014; A new approach to interactive shader lamp tracking
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Shader lamps can augment physical objects with projected virtual replications using a camera-projector system, provided that the physical and virtual object are well registered. Precise registration and tracking has been a cumbersome and intrusive process in the past. In this paper, we present a new method for tracking arbitrarily shaped physical objects interactively. In contrast to previous approaches our system is mobile and makes solely use of the projection of the virtual replication to track the physical object and &#x201C;stick&#x201D; the projection to it. Our method consists of two stages, a fast pose initialization based on structured light patterns and a non-intrusive frame-by-frame tracking based on features detected in the projection. In the initialization phase a dense point cloud of the physical object is reconstructed and precisely matched to the virtual model to perfectly overlay the projection. During the tracking phase, a radiometrically corrected virtual camera view based on the current pose prediction is rendered and compared to the captured image. Matched features are triangulated providing a sparse set of surface points that is robustly aligned to the virtual model. The alignment transformation serves as an input for the new pose prediction. Quantitative experiments show that our approach can robustly track complex objects at interactive rates.
[virtual reality, Tracking, Graphics processing units, Mobile communication, virtual object, Three-dimensional displays, alignment transformation, Iterative closest point algorithm, interactive shader lamp tracking, pose prediction, pose estimation, camera-projector system, object tracking, sticky projections approach, rendering (computer graphics), fast pose initialization, initialization phase, radiometrically corrected virtual camera view, virtual replication, dense point cloud, rendering, Cameras, Feature extraction, nonintrusive frame-by-frame tracking, virtual model]
Dense planar SLAM
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Using higher-level entities during mapping has the potential to improve camera localisation performance and give substantial perception capabilities to real-time 3D SLAM systems. We present an efficient new real-time approach which densely maps an environment using bounded planes and surfels extracted from depth images (like those produced by RGB-D sensors or dense multi-view stereo reconstruction). Our method offers the every-pixel descriptive power of the latest dense SLAM approaches, but takes advantage directly of the planarity of many parts of real-world scenes via a data-driven process to directly regularize planar regions and represent their accurate extent efficiently using an occupancy approach with on-line compression. Large areas can be mapped efficiently and with useful semantic planar structure which enables intuitive and useful AR applications such as using any wall or other planar surface in a scene to display a user's content.
[Computing methodologies [Scene understanding, RGB-D sensors, Noise, augmented reality, AR applications, Scene understanding, Simultaneous localization and mapping, Three-dimensional displays, dense planar SLAM, feature extraction, Image Processing and Computer Vision, 3D SLAM system, Real-time systems, Reconstruction, simultaneous localization and planning, image reconstruction, Indexes, bounded planes, dense multiview stereo reconstruction, SLAM (robots), red-green-blue-depth sensor, stereo image processing, surfel extraction, Cameras, Information Interfaces and Presentation]
Real-time deformation, registration and tracking of solids based on physical simulation
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
This paper proposes a novel approach to registering deformations of 3D non-rigid objects for Augmented Reality applications. Our prototype is able to handle different types of objects in real-time regardless of their geometry and appearance (with and without texture) with the support of an RGB-D camera. During an automatic offline stage, the model is processed in order to extract the data that serves as input for a physics-based simulation. Using its output, the deformations of the model are estimated by considering the simulated behaviour as a constraint. Furthermore, our framework incorporates a tracking method based on templates in order to detect the object in the scene and continuously update the camera pose without any user intervention. Therefore, it is a complete solution that extends from tracking to deformation formulation for either textured or untextured objects regardless of their geometrical shape. Our proposal focuses on providing a correct visual with a low computational cost. Experiments with real and synthetic data demonstrate the visual accuracy and the performance of our approach.
[Deformable models, Visualization, Shape, Computational modeling, solid tracking, red-green-blue-depth camera, augmented reality, physics-based simulation, IMAGE PROCESSING AND COMPUTER VISION, augmented reality application, camera pose, cameras, visual accuracy, solid registration, I.2.10, Three-dimensional displays, I.4.8 [IMAGE PROCESSING AND COMPUTER VISION, SIMULATION AND MODELING, solid deformation, Cameras, 3D nonrigid objects, Real-time systems, RGB-D camera, solid modelling]
Performance and sensitivity analysis of INDICA: INteraction-Free DIsplay CAlibration for Optical See-Through Head-Mounted Displays
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
An issue in AR applications with Optical See-Through Head-Mounted Display (OST-HMD) is to correctly project 3D information to the current viewpoint of the user. Manual calibration methods give the projection as a black box which explains observed 2D-3D relationships well (Fig. 1). Recently, we have proposed an INteraction-free DIsplay CAlibration method (INDICA) for OST-HMD, utilizing camera-based eye tracking [7]. It reformulates the projection in two ways: a black box with an actual eye model (Recycle Setup), and a combination of an explicit display model and an eye model (Full Setup). Although we have shown the former performs more stably than a repeated SPAAM calibration, we could not yet prove whether the same holds for the Full Setup. More importantly, it is still unclear how the error in the calibration parameters affects the final results. Thus, the users can not know how accurately they need to estimate each parameter in practice. We provide: (1) the fact that the Full Setup performs as accurately as the Recycle Setup under a marker-based display calibration, (2) an error sensitivity analysis for both SPAAM and INDICA over the on-/offline parameters, and (3) an investigation of the theoretical sensitivity on an OST-HMD justified by the real measurements.
[explicit display model, Correlation, H.5.1 [Information Interfaces and Presentation, augmented reality, AR applications, black box model, Three-dimensional displays, Accuracy, Recycling, calibration, OST-HMD, optical see-through head-mounted displays, eye model, helmet mounted displays, INDICA method, 2D-3D relationship, Calibration, repeated SPAAM calibration, interaction-free display calibration, recycle setup, Sensitivity, camera-based eye tracking, 3D information, user viewpoint, Cameras, Information Interfaces and Presentation]
Analysing the effects of a wide field of view augmented reality display on search performance in divided attention tasks
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
A wide field of view augmented reality display is a special type of head-worn device that enables users to view augmentations in the peripheral visual field. However, the actual effects of a wide field of view display on the perception of augmentations have not been widely studied. To improve our understanding of this type of display when conducting divided attention search tasks, we conducted an in depth experiment testing two view management methods, in-view and in-situ labelling. With in-view labelling, search target annotations appear on the display border with a corresponding leader line, whereas in-situ annotations appear without a leader line, as if they are affixed to the referenced objects in the environment. Results show that target discovery rates consistently drop with in-view labelling and increase with in-situ labelling as display angle approaches 100 degrees of field of view. Past this point, the performances of the two view management methods begin to converge, suggesting equivalent discovery rates at approximately 130 degrees of field of view. Results also indicate that users exhibited lower discovery rates for targets appearing in peripheral vision, and that there is little impact of field of view on response time and mental workload.
[Visualization, Shape, search performance, mental workload, Search problems, augmented reality, helmet mounted displays, divided attention search tasks, peripheral vision, user interfaces, head-worn device, Augmented reality, in-situ labelling method, Sensitivity, Green products, in-view method, see-through head-mounted displays, Labeling, wide field-of-view augmented reality display, information display methods, peripheral visual field]
SmartColor: Real-time color correction and contrast for optical see-through head-mounted displays
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Users of optical see-through head-mounted displays (OHMD) perceive color as a blend of the display color and the background. Color-blending is a major usability challenge as it leads to loss of color encodings and poor text legibility. Color correction aims at mitigating color blending by producing an alternative color which, when blended with the background, more closely approaches the color originally intended. To date, approaches to color correction do not yield optimal results or do not work in real-time. This paper makes two contributions. First, we present QuickCorrection, a realtime color correction algorithm based on display profiles. We describe the algorithm, measure its accuracy and analyze two implementations for the OpenGL graphics pipeline. Second, we present SmartColor, a middleware for color management of userinterface components in OHMD. SmartColor uses color correction to provide three management strategies: correction, contrast, and show-up-on-contrast. Correction determines the alternate color which best preserves the original color. Contrast determines the color which best warranties text legibility while preserving as much of the original hue. Show-up-on-contrast makes a component visible when a related component does not have enough contrast to be legible. We describe the SmartColor's architecture and illustrate the color strategies for various types of display content.
[Algorithm design and analysis, user interface components, Transparency, Head-Mounted Displays, correction management strategy, color perception, realtime color correction, Correction, show-up-on-contrast management strategy, Accuracy, text legibility, contrast management strategy, Image color analysis, color blending, alternative color production, OHMD, SmartColor middleware, Real-time systems, image colour analysis, display content, middleware, optical see-through head-mounted displays, Color, realtime color contrast, helmet mounted displays, SmartColor algorithm, Contrast, Graphics, Color Blending, Adaptive optics, OpenGL graphics pipeline, QuickCorrection algorithm, color encodings, See-through Displays]
Minimizing latency for augmented reality displays: Frames considered harmful
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
We present initial results from a new image generation approach for low-latency displays such as those needed in head-worn AR devices. Avoiding the usual video interfaces, such as HDMI, we favor direct control of the internal display technology. We illustrate our new approach with a bench-top optical see-through AR proof-of-concept prototype that uses a Digital Light Processing (DLPTM) projector whose Digital Micromirror Device (DMD) imaging chip is directly controlled by a computer, similar to the way random access memory is controlled. We show that a perceptually-continuous-tone dynamic gray-scale image can be efficiently composed from a very rapid succession of binary (partial) images, each calculated from the continuous-tone image generated with the most recent tracking data. As the DMD projects only a binary image at any moment, it cannot instantly display this latest continuous-tone image, and conventional decomposition of a continuous-tone image into binary time-division-multiplexed values would induce just the latency we seek to avoid. Instead, our approach maintains an estimate of the image the user currently perceives, and at every opportunity allowed by the control circuitry, sets each binary DMD pixel to the value that will reduce the difference between that user-perceived image and the newly generated image from the latest tracking data. The resulting displayed binary image is &#x201C;neither here nor there,&#x201D; but always approaches the moving target that is the constantly changing desired image, even when that image changes every 50μs. We compare our experimental results with imagery from a conventional DLP projector with similar internal speed, and demonstrate that AR overlays on a moving object are more effective with this kind of low-latency display device than with displays of similar speed that use a conventional video interface.
[perceptually-continuous-tone dynamic gray-scale image, H.5.1 [Information Interfaces and Presentation, Gray-scale, augmented reality, Image color analysis, head-worn AR devices, low-latency displays, continuous-tone image, Mirrors, random access memory, internal display technology, DMD imaging chip, digital light processing projector, Optical imaging, helmet mounted displays, digital micromirror device, augmented reality displays, Image generation, Streaming media, Rendering (computer graphics), binary time-division-multiplexed value, Information Interfaces and Presentation, video interfaces, bench-top optical see-through AR, DLP projector]
Creating automatically aligned consensus realities for AR videoconferencing
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
This paper presents an AR videoconferencing approach merging two remote rooms into a shared workspace. Such bilateral AR telepresence inherently suffers from breaks in immersion stemming from the different physical layouts of participating spaces. As a remedy, we develop an automatic alignment scheme which ensures that participants share a maximum of common features in their physical surroundings. The system optimizes alignment with regard to initial user position, free shared floor space, camera positioning and other factors. Thus we can reduce discrepancies between different room and furniture layouts without actually modifying the rooms themselves. A description and discussion of our alignment scheme is given along with an exemplary implementation on real-world datasets.
[user position, free shared floor space, Avatars, Computational modeling, automatic alignment scheme, augmented reality, AR videoconferencing, teleconferencing, immersion stemming, Optimization, bilateral AR telepresence, Teleconferencing, Three-dimensional displays, H.4.3 [Information Systems Applications, Cameras, Observability, camera positioning, Information Systems Applications]
FLARE: Fast layout for augmented reality applications
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Creating a layout for an augmented reality (AR) application which embeds virtual objects in a physical environment is difficult as it must adapt to any physical space. We propose a rule-based framework for generating object layouts for AR applications. Under our framework, the developer of an AR application specifies a set of rules (constraints) which enforce self-consistency (rules regarding the inter-relationships of application components) and scene-consistency (application components are consistent with the physical environment they are placed in). When a user enters a new environment, we create, in real-time, a layout for the application, which is consistent with the defined constraints (as much as possible). We find the optimal configurations for each object by solving a constraint-satisfaction problem. Our stochastic move making algorithm is domain-aware, and allows us to efficiently converge to a solution for most rule-sets. In the paper we demonstrate several augmented reality applications that automatically adapt to different rooms and changing circumstances in each room.
[Algorithm design and analysis, scene-consistency rule, augmented reality, Probability and Statistics, object layout generation, Proposals, AR applications, Augmented reality, rule-based framework, FLARE, Layout, knowledge based systems, F.4.1 [Mathematical Logic, Games, constraint-satisfaction problem, Cameras, Cost function, Mathematical Logic, virtual objects, self-consistency rule, fast layout for augmented reality applications]
Presence and discernability in conventional and non-photorealistic immersive augmented reality
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Non-photorealistic rendering (NPR) has been shown as a powerful way to enhance both visual coherence and immersion in augmented reality (AR). However, it has only been evaluated in idealized pre-rendered scenarios with handheld AR devices. In this paper we investigate the use of NPR in an immersive, stereoscopic, wide field-of-view head-mounted video see-through AR display. This is a demanding scenario, which introduces many real-world effects including latency, tracking failures, optical artifacts and mismatches in lighting. We present the AR-Rift, a low-cost video see-through AR system using an Oculus Rift and consumer webcams. We investigate the themes of consistency and immersion as measures of psychophysical non-mediation. An experiment measures discernability and presence in three visual modes: conventional (unprocessed video and graphics), stylized (edge-enhancement) and virtualized (edge-enhancement and color extraction). The stylized mode results in chance-level discernability judgments, indicating successful integration of virtual content to form a visually coherent scene. Conventional and virutalized rendering bias judgments towards correct or incorrect respectively. Presence as it may apply to immersive AR, and which, measured both behaviorally and subjectively, is seen to be similarly high over all three conditions.
[Visualization, Tracking, edge enhancement, H.5.1 [Information Interfaces and PresentationMultimedia Information Systems, augmented reality, I.3.7, field-of-view head-mounted video see-through AR display, NPR, Accuracy, image enhancement, feature extraction, consumer Web camera, Information Interfaces and PresentationMultimedia Information Systems, Real-time systems, image colour analysis, rendering (computer graphics), nonphotorealistic rendering, Oculus Rift, Image edge detection, AR immersion, visually coherent scene, immersive augmented reality, color extraction, Rendering (computer graphics), Cameras, chance-level discernability judgment, visual coherence, AR-Rift system]
WeARHand: Head-worn, RGB-D camera-based, bare-hand user interface with visually enhanced depth perception
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
We introduce WeARHand, which allows a user to manipulate virtual 3D objects with a bare hand in a wearable augmented reality (AR) environment. Our method uses no environmentally tethered tracking devices and localizes a pair of near-range and far-range RGB-D cameras mounted on a head-worn display and a moving bare hand in 3D space by exploiting depth input data. Depth perception is enhanced through egocentric visual feedback, including a semi-transparent proxy hand. We implement a virtual hand interaction technique and feedback approaches, and evaluate their performance and usability. The proposed method can apply to many 3D interaction scenarios using hands in a wearable AR environment, such as AR information browsing, maintenance, design, and games.
[Visualization, WeARHand, head-worn display, red-green-blue-depth camera, Augmented Reality, augmented reality, user interfaces, Hand Interaction, AR games, AR maintenance, cameras, Three-dimensional displays, Image color analysis, egocentric visual feedback, Wearable Computing, semitransparent proxy hand, Image resolution, helmet mounted displays, Virtual 3D Object Manipulation, tethered tracking devices, Equations, bare-hand user interface, 3D User Interfaces, AR information browsing, Cameras, Rendering (computer graphics), wearable augmented reality, AR environment, depth perception, AR design, RGB-D camera]
Single view augmentation of 3D elastic objects
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
This paper proposes an efficient method to capture and augment highly elastic objects from a single view. 3D shape recovery from a monocular video sequence is an underconstrained problem and many approaches have been proposed to enforce constraints and resolve the ambiguities. State-of-the art solutions enforce smoothness or geometric constraints, consider specific deformation properties such as inextensibility or ressort to shading constraints. However, few of them can handle properly large elastic deformations. We propose in this paper a real-time method which makes use of a mechanical model and is able to handle highly elastic objects. Our method is formulated as a energy minimization problem accounting for a non-linear elastic model constrained by external image points acquired from a monocular camera. This method prevents us from formulating restrictive assumptions and specific constraint terms in the minimization. The only parameter involved in the method is the Young's modulus where we show in experiments that a rough estimate of its value is sufficient to obtain a good reconstruction. Our method is compared to existing techniques with experiments conducted on computer-generated and real data that show the effectiveness of our approach. Experiments in the context of minimally invasive liver surgery are also provided.
[Deformable models, 3D shape recovery, Shape, Computational modeling, H.5.1 [Information Interfaces and Presentation, image reconstruction, Strain, elastic deformation, Young modulus, Young's modulus, Computer Graphics, Three-dimensional displays, 3D elastic object, shading constraint, deformation property, Information Interfaces and Presentation, minimally invasive liver surgery, minimisation, video signal processing, single view augmentation, image sequences, energy minimization problem, monocular video sequence]
Improved interventional X-ray appearance
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Depth cues are an essential part of navigation and device positioning tasks during clinical interventions. Yet, many minimally-invasive procedures, such as catheterizations, are usually performed under X-ray guidance only depicting a 2D projection of the anatomy, which lacks depth information. Previous attempts to integrate pre-operative 3D data of the patient by registering these to intra-operative data have led to virtual 3D renderings independent of the original X-ray appearance and planar 2D color overlays (e.g. roadmaps). A major drawback associated to these solutions is the trade-off between X-ray attenuation values that is completely neglected during 3D renderings, and depth perception not being incorporated into the 2D roadmaps. This paper presents a novel technique for enhancing depth perception of interventional X-ray images preserving the original attenuation appearance. Starting from patient-specific pre-operative 3D data, our method relies on GPU ray casting to compute a colored depth map, which assigns a predefined color to the first incidence of gradient magnitude value above a predefined threshold along the ray. The colored depth map values are carefully integrated into the X-Ray image while maintaining its original grey-scale intensities. The presented method was tested and analysed for three relevant clinical scenarios covering different anatomical aspects and targeting different levels of interventional expertise. Results demonstrate that improving depth perception of X-ray images has the potential to lead to safer and more efficient clinical interventions.
[virtual 3D renderings, planar 2D color overlays, colored depth map, Medical image visualization, attenuation appearance, X-ray imaging, clinical intervention, depth cue, Three-dimensional displays, Image color analysis, minimally-invasive procedure, X-ray guidance, GPU ray casting, rendering (computer graphics), medical image processing, transfer function, depth information, catheterization, Encoding, X-ray attenuation value, X-ray appearance, image-guided interventions, graphics processing units, interventional x-ray appearance, image representation, graphics processing unit, Rendering (computer graphics), depth perception, Medical diagnostic imaging, grey-scale intensity]
Computer-Assisted Laparoscopic myomectomy by augmenting the uterus with pre-operative MRI data
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
An active research objective in Computer Assisted Intervention (CAI) is to develop guidance systems to aid surgical teams in laparoscopic Minimal Invasive Surgery (MIS) using Augmented Reality (AR). This involves registering and fusing additional data from other modalities and overlaying it onto the laparoscopic video in realtime. We present the first AR-based image guidance system for assisted myoma localisation in uterine laparosurgery. This involves a framework for semi-automatically registering a pre-operative Magnetic Resonance Image (MRI) to the laparoscopic video with a deformable model. Although there has been several previous works involving other organs, this is the first to tackle the uterus. Furthermore, whereas previous works perform registration between one or two laparoscopic images (which come from a stereo laparoscope) we show how to solve the problem using many images (e.g. 20 or more), and show that this can dramatically improve registration. Also unlike previous works, we show how to integrate occluding contours as registration cues. These cues provide powerful registration constraints and should be used wherever possible. We present retrospective qualitative results on a patient with two myomas and quantitative semi-synthetic results. Our multi-image framework is quite general and could be adapted to improve registration in other organs with other modalities such as CT.
[Deformable models, Solid modeling, biomedical MRI, magnetic resonance imaging, AR-based image guidance system, image registration, Transforms, augmented reality, computer-assisted laparoscopic myomectomy, CAI, preoperative MRI data, laparoscopic minimal invasive surgery, laparoscopic video, Laparoscopes, AR, Three-dimensional displays, Magnetic resonance imaging, Surgery, MIS, deformable model, computer assisted intervention, video signal processing, medical image processing, surgery]
[Poster] MOBIL: A moments based local binary descriptor
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
In this paper, we propose an efficient, and fast binary descriptor, called MOBIL (MOments based BInary differences for Local description), which compares not just the intensity, but also sub-regions geometric proprieties by employing moments. This approach offers high distinctiveness against affine transformations and appearance changes. The experimental evaluation shows that MOBIL achieves a quite good performance in term of low computation complexity and high recognition rate compared to state-of-the-art real-time local descriptors.
[Computers, Algorithm design and analysis, object recognition, Computer vision, Image recognition, local binary descriptors, moments based binary differences for local description, Augmented reality, computation complexity, moments, MOBIL descriptor, feature extraction, geometric propriety, computer vision, Approximation algorithms, Robustness, recognition rate]
[Poster] Ongoing development of a user-centered, AR testbed in industry
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Augmented reality (AR) is a live, direct or indirect, view of a physical, real-world environment whose elements are augmented by computer-generated sensory input such as sound, video, graphics or GPS data [1]. AR technology has seen the advent of smaller form factors, more powerful processors, higher resolution cameras, and distributed computation; the advent of technology like Google Glass and Epson Moverio BT-200 have generated renewed interest in the commercial domains. This new eyewear also extends the earlier capabilities of optical-/video-see through glasses with Bluetooth, Wifi, and 3G connectivity to remote databases. While there are as yet many significant technical hurdles for industry-specific AR systems that must be surmounted to ensure efficient operation, there are still numerous user-centric issues that still need to be addressed to enable the desired safety and efficiency potential of the technology itself. The issues themselves are plentiful, including ergonomic issues (size, weight of the augmented reality hardware, when dealing with glasses), user interface requirements (font sizes, lighting conditions impacting the legibility of text or the rendering of the digital content, interaction with tablets or glasses), and physiological issues (eye fatigue, user perception due to latency of content rendering, increased user workload). Our contribution in this paper is the presentation of an ongoing development of a user-centric, industrial testbed devoted to the requirements gathering, development, and assessment of AR technologies and presentation of two sample use cases. Our testbed is designed to investigate usability research questions related to AR, and this contribution constitutes some of our work in progress in developing this vision.
[Bluetooth, optical-video-see through glasses, Wi-Fi, Buildings, Glass, Inspection, Maintenance engineering, computer-generated sensory input, augmented reality, Google Glass, user-centered AR testbed, Engines, Augmented reality, physiological issues, AR technology, Epson Moverio BT-200, user interface requirements, Wireless Fidelity, interactive devices, ergonomic issue, Usability, 3G connectivity]
[Poster] Visualization of solar radiation data in augmented reality
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
We present an AR application for visualizing solar radiation data on facades of buildings, generated from LiDAR data and climatic observations. Data can be visualized using colored surfaces and glyphs. A user study revealed the proposed AR visualizations were easy to use, which can lead to leverage the potential benefits of AR visualizations: to detect errors in the simulated data, to give support to the installation of photovoltaic equipment and to raise public awareness of the use of facades for power production.
[AR application, colored surface, light detection and ranging, augmented reality, building integrated photovoltaics, LiDAR data, Three-dimensional displays, climatic observation, Image color analysis, data visualisation, power production, building facade, Solar radiation, photovoltaic potential, solar power, solar radiation data visualization, scientific data visualization, Buildings, user study, solar radiation, power engineering computing, Augmented reality, glyph, Data visualization, photovoltaic equipment]
[Poster] Visual-lnertial 6-DOF localization for a wearable immersive VR/AR system
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
We present a real-time visual-inertial localization approach directly integrable in a wearable immersive system for simulation and training. In this context, while CAVE systems typically require complex and expensive set-up, our approach relies on visual and inertial information provided by consumer monocular camera and Inertial Measurement Unit, embedded in a wearable stereoscopic HMD. 6-DOF localization is achieved through image registration with respect to a 3D map of descriptors of the training room and robust tracking of visual features. We propose a novel efficient and robust pipeline based on state-of-the-art image-based localization and sensor fusion approaches, which makes use of robust orientation information from FMU, to cope with camera fast motion and limit motion jitters. The proposed system runs at 30 fps on a standard PC and requires very limited set-up for its intended application.
[virtual reality, inertial information, image fusion, image registration, wearable immersive VR-AR system, augmented reality, degrees-of-freedom, Augmented reality, visual-lnertial 6-DOF localization, inertial measurement unit, sensor fusion approach, CAVE systems, feature extraction, image-based localization, consumer monocular camera, visual feature tracking, visual information]
[Poster] Augmentation of live excavation work for subsurface utilities engineering
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
The virtual excavation is a well-known augmentation technique that was proposed for city road environments. It can be used for planning excavation work by augmenting the road surface with a virtual excavation revealing subsurface utility pipes. In this paper, we proposed an extension of the virtual excavation technique for live augmentation of excavation work sites. Our miniaturized setup, consisting of a sandbox and a Kinect device, was used to simulate dynamic terrain topography capture. We hypothesized that the virtual excavation could be used live on the ground being excavated, which could facilitate the excavator operator's work. Our results show that the technique can indeed be adapted to dynamic terrain topography, but turns out to occlude terrain in a potentially hazardous way. Potential solutions include the use of virtual paint markings instead of a virtual excavation.
[virtual reality, Roads, live excavation work, subsurface utilities engineering, subsurface utility pipes, subsurface utilities, Surface topography, civil engineering computing, Augmented reality, Surface treatment, virtual paint markings, Data visualization, infrastructure, virtual excavation, city road environment, Real-time systems, public utilities, dynamic terrain topography capture]
[Poster] Exploring social augmentation concepts for public speaking using peripheral feedback and real-time behavior analysis
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Non-verbal and unconscious behavior plays an important role for efficient human-to-human communication but are often undervalued when training people to become better communicators. This is particularly true for public speakers who need not only behave according to a social etiquette but do so while generating enthusiasm and interest for dozens if not hundreds of other persons. In this paper we propose the concept of social augmentation using wearable computing with the goal of giving users the ability to continuously monitor their performance as a communicator. To this end we explore interaction modalities and feedback mechanisms which would lend themselves to this task.
[social augmentation concept, human-to-human communication, Public speaking, wearable computing, wearable computers, Prototypes, peripheral feedback, Signal processing, Cameras, Real-time systems, social sciences computing, Sensors, public speaking, realtime behavior analysis, Biomedical monitoring]
[Poster] Using augmented reality to support information exchange of teams in the security domain
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
For operational units in the security domain that work together in teams it is important to quickly and adequately exchange context-related information. This extended abstract investigates the potential of augmented reality (AR) techniques to facilitate information exchange and situational awareness of teams from the security domain. First, different scenarios from the security domain that have been elicited using an end-user oriented design approach are described. Second, a usability study is briefly presented based on an experiment with experts from operational security units. The results of the study show that the scenarios are well-defined and the AR environment can successfully support information exchange in teams operating in the security domain.
[Forensics, AR techniques, augmented reality, team situational awareness, Security, Augmented reality, team working, Three-dimensional displays, context-related information exchange, end-user oriented design approach, security of data, information exchange, usability, security domain, Abstracts, Usability, Information exchange]
[Poster] HMD Video see though AR with unfixed cameras vergence
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Stereoscopic video see though AR systems permit accurate marker video based registration. To guarantee accurate registration, cameras are normally rigidly blocked while the user could require changing their vergence. We propose a solution working with lightweight hardware that, without the need for a new calibration of the cameras relative pose after each vergence adjustment, guarantees registration accuracy using pre-determined calibration data.
[Stereo image processing, image registration, vergence adjustment, Fasteners, helmet mounted display, pose calibration, augmented reality, helmet mounted displays, Calibration, marker video based registration, Augmented reality, Video See Though, cameras, unfixed cameras vergence, registration accuracy, Vergence, HMD video see though AR, Surgery, stereoscopic video, stereo image processing, Cameras, Hardware, video signal processing, HMD]
[Poster] Towards user perspective augmented reality for public displays
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
We work towards ad-hoc augmentation of public displays on handheld devices, supporting user perspective rendering of display content. Our prototype system only requires access to a screencast of the public display, which can be easily provided through common streaming platforms and is otherwise self-contained. Hence, it easily scales to multiple users.
[Visualization, streaming platform, augmented reality, screencast, Augmented reality, Three-dimensional displays, user perspective augmented reality, Handheld computers, user perspective rendering, Cameras, Rendering (computer graphics), handheld devices, Face, rendering (computer graphics), public displays]
[Poster] Contextually panned and zoomed augmented reality interactions using COTS heads up displays
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Consumer of the shelf heads up displays with onboard cameras and processing power have recently become available. Evaluations of a naive implementation of video-see-through augmented reality suggest that their small display and off-axis camera presents usability problems. We panned and zoomed a composited video feed on the Google Glass device to center the augmented reality context within the display and to give the appearance of a fixed distance to the content. We pilot tested both the panned and zoomed display against a naive implementation and found that users preferred the view-stabilized version.
[Google, Image resolution, Head, Mixed/Augmented Reality, view-stabilized display, Glass, augmented reality, Augmented reality, COTS heads up displays, Displays and Imagers, video-see-through augmented reality, computer displays, Google Glass device, composite video feed, commecial-off-the-shelf, Cameras, Feeds, contextually panned and zoomed augmented reality interaction, Computer Vision, panned and zoomed display, Interaction Techniques]
[Poster] View management for webized mobile AR contents
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Information presentation techniques are important in augmented reality applications as they are in the traditional desktop user interface (WIMP) and web user interface [6]. This paper introduces view management for a web-based augmented reality mobile platform. We use a webized mobile AR browser called Insight that provides separation of the application logic, including the tracking engine and AR content, so that the view management logic and contents are easy to reuse. In addition, the view management is able to accommodate in-situ context of an AR application.
[layout management, Mobile communication, augmented reality, user interfaces, mobile computing, Semantics, tracking engine, information presentation techniques, Insight, Real-time systems, mobile AR, WIMP, desktop user interface, AR content, webized mobile AR contents, Browsers, web architecture, Augmented reality, application logic, view management, Layout, Content, Web user interface, User interfaces, Internet]
[Poster] Non-parametric camera-based calibration of optical see-through glasses for augmented reality applications
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
This work describes a camera-based method for the calibration of optical See-Through Glasses (STGs). A new calibration technique is introduced for calibrating every single display pixel of the STGs in order to overcome the disadvantages of a parametric model. A non-parametric model compared to the parametric one has the advantage that it can also map arbitrary distortions. The new generation of STGs using waveguide-based displays [5] will have higher arbitrary distortions due to the characteristics of their optics. First tests show better accuracies than in previous works. By using cameras which are placed behind the displays of the STGs, no error prone user interaction is necessary. It is shown that a high accuracy tracking device is not necessary for a good calibration. A camera mounted rigidly on the STGs is used to find the relations between the system components. Furthermore, this work elaborates on the necessity of a second subsequent calibration step which adapts the STGs to a specific user. First tests prove the theory that this subsequent step is necessary.
[nonparametric model, calibration technique, H.5.1 [Information Interfaces and Presentation, Optical distortion, STG, augmented reality, user interaction, Calibration, user interfaces, camera-based method, Augmented reality, Accuracy, Three-dimensional displays, nonparametric camera-based calibration, optical see-through glass, computer displays, Image Processing and Computer Vision, Cameras, Adaptive optics, Information Interfaces and Presentation, waveguide-based display, optical glass]
[Poster] Towards mobile augmented reality for the elderly
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Mobility and independence are key aspects for self-determined living in today's world and demographic change presents the challenge to retain these aspects for the aging population. Augmented Reality (AR) user interfaces might support the elderly, for example, when navigating as pedestrians or by explaining how devices and mobility aids work and how they are maintained. This poster reports on the results of practical field tests with elderly subjects testing handheld AR applications. The main finding is that common handheld AR user interfaces are not suited for the elderly because they require the user to hold up the device so the back-facing camera captures the object or environment related to which digital information shall be presented. Tablet computers are too heavy and they do not provide sufficient grip to hold them over a long period of time. One possible alternative is using head-mounted displays (HMD). We present the promising results of a user test evaluating whether elderly people can deal with AR interfaces on a lightweight HMD. We conclude with an outlook to improved handheld AR user interfaces that do not require continuously holding up the device, which we hope are better suited for the elderly.alternative
[Navigation, head-mounted displays, elderly, Tablet computers, Manuals, augmented reality, helmet mounted displays, user interfaces, tablet computers, handheld AR applications, Augmented reality, mobile computing, AR user interfaces, Senior citizens, User interfaces, Cameras, mobile augmented reality, HMD]
[Poster] An augmented and virtual reality system for training autistic children
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Autism or Autism Spectrum disorder (ASD) is a pervasive development disorder causing impairment in thinking, feeling, hearing, speaking and social interaction. For this reason, children suffering from autism need to follow special training in order to increase their ability to learn new skills and knowledge. These children have propensity to be attracted by the technology devices especially virtual animations. The interest of this research work is to explore and study the use of Augmented and Virtual Reality (AR/VR) system for training the children with ASD based on Applied Behavior Analysis (ABA) techniques. This system assists in teaching children about new pictures or objects along with the associated keyword or matching sentence in an immersive way with fast interaction. The preliminary prototype demonstrates satisfactory performance of the proposed AR/VR system working in laboratory conditions.
[Psychology, H.5.1 [Information Interfaces and Presentation, augmented reality, virtual reality system, Training, autistic children training, Autism, computer animation, AR-VR system, augmented reality system, virtual animation, computer based training, Virtual reality, Variable speed drives, Cameras, ABA technique, Information Interfaces and Presentation, autism spectrum disorder, Monitoring, applied behavior analysis]
[Poster] Device vs. user perspective rendering in google glass AR applications
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
According to Gartner's 2013 Hype Cycle for Emerging Technologies, Augmented Reality will reach its Plateu of Productivity before Wearable User Interfaces. In this work, device and user-perspective rendering are compared regarding their applicability to AR-based solutions for Google Glass. The conducted experiment measured and evaluated the advantages and drawbacks on each method and also got positive and negative feedbacks given by users. The tests showed that users preferred the device-perspective approach.
[Google, device rendering, Hype Cycle for Emerging Technologies, H.5.1 [Information Interfaces and Presentation, Glass, Google Glass AR application, augmented reality, Calibration, device-perspective approach, Augmented reality, user perspective rendering, wearable user interfaces, Rendering (computer graphics), Cameras, interactive devices, Information Interfaces and Presentation, Optical sensors, rendering (computer graphics)]
[Poster] Combining multi-touch and device movement in mobile augmented reality manipulations
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Three input modalities for manipulation techniques in Mobile Augmented Reality have been compared. The first one employs only multi-touch input. The second modality uses the movements of the device. Finally, the third one is a hybrid approach based on a combination of the two previous modalities. A user evaluation (N=12) on a 6 DOF docking task suggests that combining multi-touch input and device movement offers the best results in terms of task completion time and efficiency. Nonetheless, using solely the device is more intuitive and performs worse only in large rotations. Given that mobile devices are increasingly supporting movement tracking, the presented results encourage the addition of device movement as an input modality.
[Performance evaluation, movement tracking, Thumb, Grasping, Mobile communication, augmented reality, user evaluation, user interfaces, Mobile Augmented Reality, Augmented reality, device movement input, input modality, Three-dimensional displays, mobile computing, multitouch input, task completion time, Manipulation, manipulation techniques, mobile devices, Multi-touch, mobile augmented reality]
[Poster] Turbidity-based aerial perspective rendering for mixed reality
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
In outdoor Mixed Reality (MR), objects distant from the observer suffer from an effect called aerial perspective that fades the color of the objects and blends it to the environmental light color. The aerial perspective can be modeled using a physics-based approach; however, handling the changing and unpredictable environmental illumination is demanding. We present a turbidity-based method for rendering a virtual object with aerial perspective effect in a MR application. The proposed method first estimates the turbidity by matching luminance distributions of sky models and a captured omnidirectional sky image. Then the obtained turbidity is used to render the virtual object with aerial perspective.
[Solid modeling, or education and training, omnidirectional sky image, Atmospheric modeling, MR, Scattering, Observers, augmented reality, cultural heritage, virtual object, Photorealistic rendering, turbidity-based aerial perspective, mixed reality, environmental light color, luminance distribution, Virtual reality, MR/AR for art, Rendering (computer graphics), environmental illumination, Mathematical model, rendering (computer graphics), physics-based approach]
[Poster] Representing degradation of real objects using augmented reality
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Much research in augmented reality (AR) technology attempts to match the textures of virtual objects with real world. However, the textures of real objects must also be rendered consistent with virtual information. This paper proposes a method for representing the degradation of real objects in virtual time. Real-world depth information, used to build three-dimensional models of real objects, is captured by a RGB-D camera. The degradation of real objects is then represented by superimposing the degradation texture onto the real object.
[RGB-D Camera, Solid modeling, Visualization, real-world depth information, Shape, red-green-blue-depth camera, Augmented Reality, augmented reality, three-dimensional model, Augmented reality, Degradation, real object degradation representation, AR technology, image representation, Cameras, Real-time systems, virtual objects, RGB-D camera]
[Poster] Indirect augmented reality considering real-world illumination change
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Indirect augmented reality (IAR) utilizes pre-captured omnidirectional images and offline superimposition of virtual objects for achieving high-quality geometric and photometric registration. Meanwhile, IAR causes inconsistency between the real world and the pre-captured image. This paper describes the first-ever study focusing on the temporal inconsistency issue in IAR. We propose a novel IAR system which reflects real-world illumination change by selecting an appropriate image from a set of images pre-captured under various illumination. Results of a public experiment show that the proposed system can improve the realism in IAR.
[indirect augmented reality, IAR system, Clouds, image registration, H.5.1 [Information Interfaces and Presentation, virtual objects superimposition, Mobile communication, augmented reality, Mobile handsets, real-world illumination change, lighting, photometric registration, Augmented reality, pre-captured omnidirectional image, Histograms, Databases, Lighting, geometric registration, Information Interfaces and Presentation]
[Poster] Augmented reality binoculars on the move
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
In this paper, we expand our previous work on augmented reality (AR) binoculars to support wider range of user motion - up to thousand square meters compared to only a few square meters as before. We present our latest improvements and additions to our pose estimation pipeline and demonstrate stable registration of objects on the real world scenery while the binoculars are undergoing significant amount of parallax-inducing translation.
[augmented reality binoculars, inertial navigation, Google, EKF, object registration, image registration, Materials, AR binoculars, user motion, augmented reality, sensor fusion, Augmented reality, Global Positioning System, Training, Earth, Accuracy, pose estimation, interactive devices, parallax-inducing translation]
[Poster] Motion detection based ghosted views for occlusion handling in augmented reality
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
This work presents an improvement to the scene analysis pipeline of a visualization technique called Ghosting. Computer vision and image processing techniques are used to extract natural features, from each video frame. These features will guide the assignment of transparency to pixels, in order to give the ghosting effect, while blending the virtual object into the real scene. Video sequences were obtained from traditional RGB cameras. The main contribution of this work is the inclusion of a motion detection technique to the scene feature analysis step. This procedure leads to a better perception of the augmented scene because the proper ghosting effect is achieved when a moving natural salient object, that catches users attention, passes in front of an augmented one.
[Visualization, ghosting technique, Pipelines, H.5.1 [Information Interfaces and Presentation Systems, image processing techniques, augmented reality, object detection, Information Interfaces and Presentation Systems, feature extraction, moving natural salient object, Image Processing and Computer Vision, Motion detection, video signal processing, image sequences, Computer vision, Augmented reality, scene analysis pipeline, image motion analysis, scene feature analysis, Image analysis, visualization technique, computer vision, motion detection based ghosted view, video sequences, Feature extraction, occlusion handling]
[Poster] QR code alteration for augmented reality interactions
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
QR code, for its recognition robustness and data capacity, has been often used for Augmented Reality applications as well as for other commercial applications. However, it is difficult to enable tangible interactions through which users may change 3D models or animations. It is because QR codes are automatically generated by the rules, and are not easily modifiable. Our goal was to enable QR code based Augmented Reality interactions. By analysis and through experiments, we discovered that some parts of a QR code can be altered to change the text string that the QR code represents. In this paper, we introduced a prototype for QR code based Augmented Reality interactions, which allows for Rubik's cube style rolling interactions.
[Dinosaurs, QR code alteration, Solid modeling, tangible interactions, Laboratories, data capacity, Interaction, Augmented Reality, Media, augmented reality, augmented reality interactions, cube style rolling interactions, Augmented reality, recognition robustness, Three-dimensional displays, bar codes, Prototypes, animations, QR code, 3D models]
[Poster] Interactive deformation of real objects
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
This paper presents a method for interactive deformation of a real object. Our method uses a predefined 3D model of a target object for tracking and deformation. A camera pose relative to the target object is estimated using 3D model-based tracking. Object region in camera image is obtained by projecting the 3D model onto image plane with the estimated camera pose, and a texture map is extracted from the object region and mapped to the 3D model. Then a texture-mapped model is rendered based on a mesh deformed by user via Laplacian operation. Experimental results demonstrate that our method provides user interactions with 3D real objects on real scenes, not augmented virtual contents.
[Deformable models, 3D model-based tracking, Solid modeling, Target tracking, interactive deformation, Computational modeling, user interaction, image texture, object deformation, texture map, image plane, Computer Graphics, Three-dimensional displays, Laplacian operation, camera pose estimation, pose estimation, Information Interface and Presentation, I.3.6 [Computer Graphics, Cameras, object tracking, Real-time systems, solid modelling]
[Poster] Contact-view: A magic-lens paradigm designed to solve the dual-view problem
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Typically handheld AR systems utilize a single back-facing camera and the screen in order to implement device transparency. This creates the dual-view problem a consequence of virtual transparency which does not match true transparency - what the user would see looking through a transparent glass pane. The dual-view problem affects usability of handheld AR systems and is commonly addressed though user-perspective rendering solutions. Whilst such approach produces promising results, the complexity of implementing user-perspective rendering and the fact it does not solve all sources that produce the dual-view problem, means it only ever addresses part of the problem. This paper seeks to create a more complete solution for the dual-view problem that will be applicable to readily available handheld-device. We pursue this goal by designing, implementing and evaluating a novel interaction paradigm we call `contact-view'. By utilizing the back and front-facing camera and the environment base-plane texture - predefined or incrementally created on the fly, we enable placing the device directly on top of the base-plane. As long as the position of the phone in relation to the base-plane is known, appropriate segment of the occluded base-plane can be rendered on the device screen, result of which is transparency in which dual-view problem is eliminated.
[device transparency, Switches, Glass, augmented reality, transparent glass pane, magic-lens, user interfaces, augmented reality systems, virtual transparency, Prototypes, magic-lens paradigm, rendering (computer graphics), Context, tabletop, Observers, user-perspective rendering, AR, handheld AR systems, contact-view paradigm, Cameras, Rendering (computer graphics), dual-view problem, interaction paradigm, mobile handsets, occluded base-plane]
[Poster] Utilizing contact-view as an augmented reality authoring method for printed document annotation
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
In Augmented Reality (AR) the real world is enhanced by superimposed digital information commonly visualized through augmented annotations. The visualized data comes from many different data sources. One increasingly important source of data is user generated content. Unfortunately, AR tools that support user generated content are not common hence the majority of augmented data within AR applications is not generated utilizing AR technology. In this paper we discuss the main reasons for this and evaluate how the contact-view paradigm could enhance annotation authoring process within the class of tabletop size AR workspaces. This evaluation is based on a prototype that allows musicians to annotate a music score manuscript utilizing freehand drawing on top of device screen. Experimentation showed the potential of contact-view paradigm as an annotation authoring method that performs well in single and collaborative multi-user situations.
[authoring, User-generated content, augmented reality, magic-lens, annotation authoring process, multiuser situation, Prototypes, freehand drawing, augmented annotation, contact-view, Context, document handling, annotation, augmented reality authoring method, mobile, Augmented reality, contact-view method, AR, AR technology, Collaboration, collaboration, contact-view paradigm, Rendering (computer graphics), Cameras, printed document annotation, handheld, tbaletop]
[Poster] A preliminary study on altering surface softness perception using augmented color and deformation
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Choosing the appropriate soft/hard material is important for designing a product such as sofa or bed, but typically limited by the number of physical materials that the designer owns. Pseudo-haptic feedback is an alternative way that enables designer to roughly simulate material properties (e.g., softness, hardness) by only generating the visual illusion. However, the current technique is limited within video see-through augmented reality, in which the user interact in a real space while looking at a virtual space. This paper explores the possibility to realize pseudo-haptic feedback for touching objects in spatial augmented reality. We investigate and compare effects of visually superimposing a projection graphics onto the surface of a touched object and the fingernail/finger for changing the surface tactile perception. The potential of our method is discussed through a preliminary user study.
[Visualization, Force measurement, augmented deformation, pseudohaptic feedback, Force, Materials, Color, haptic interfaces, augmented reality, surface softness perception, pseudo-haptics, Augmented reality, perception, product design, Image color analysis, Spatial augmented reality, surface tactile perception, material hardness property, projection graphics, material softness property, augmented color]
[Poster] Social panoramas using wearable computers
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
In this paper we describe the concept of Social Panoramas that combine panorama images, Mixed Reality, and wearable computers to support remote collaboration. We have developed a prototype that allows panorama images to be explored in real time between a Google Glass user and a remote tablet user. This uses a variety of cues for supporting awareness, and enabling pointing and drawing. We conducted a study to explore if these cues can increase Social Presence. The results suggest that increased interaction does not increase Social Presence, but tools with a higher perceived usability show an improved sense of presence.
[Head-mounted Display, Glass, augmented reality, user interfaces, remote collaboration, mobile computing, social presence, Wearable computers, Prototypes, Google Glass user, panorama images, Mixed Reality, image processing, Head, Remote Collaboration, remote tablet user, Mobile Computing, social panorama concept, wearable computers, Panorama, mixed reality, Collaboration, Cameras, perceived usability, Usability]
[Poster] A Mobile Augmented reality system to assist auto mechanics
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Ground-breaking technologies and innovative design of upcoming vehicles introduce complex maintenance procedures for auto mechanics. In order to present these procedures in an intuitive manner, the Mobile Augmented Reality Technical Assistance (MARTA) project was initiated. The goal was to create an Augmented Reality-aided application running on a tablet computer, which shows maintenance instructions superimposed on a live video feed of the car. Robust image-based tracking of specular surfaces using both edge and texture features as well as the software framework are the most important aspects of the project, which are presented here. The resulting application is deployed and used productively to support maintenance of the Volkswagen XL1 vehicle across the world.
[Solid modeling, maintenance engineering, production engineering computing, augmented reality, Three-dimensional displays, mobile computing, edge feature, feature extraction, augmented reality-aided application, object tracking, Volkswagen XL1 vehicle, Image edge detection, Computational modeling, specular surface tracking, Tablet computers, Maintenance engineering, texture feature, automobile industry, image texture, mobile augmented reality system, MARTA, robust image-based tracking, Cameras, auto mechanics, tablet computer, maintenance instructions, mobile augmented reality technical assistance]
[Poster] Smartwatch-aided handheld augmented reality
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
We propose a novel method for interaction of humans with real objects in their surrounding combining Visual Search and Augmented Reality (AR). This method is based on utilizing a smartwatch tethered to a smartphone, and it is designed to provide a more user-friendly experience compared to approaches based only on a handheld device, such as a smartphone or a tablet computer. The smart-watch has a built-in camera, which enables scanning objects without the need to take the smartphone out of the pocket. An image captured by the watch is sent wirelessly to the phone that performs Visual Search and subsequently informs the smartwatch whether digital information related to the object is available or not. We thereby distinguish between three cases. If no information is available or the object recognition failed, the user is notified accordingly. If there is digital information available that can be presented using the smartwatch display and/or audio output, it is presented there. The third case is that the recognized object has digital information related to it, which would be beneficial to see in an Augmented Reality view spatially registered with the object in realtime. Then the smartwatch informs the user that this option exists and encourages using the smartphone to experience the Augmented Reality view. Thereby, the user only needs to take the phone out of the pocket in case Augmented Reality content is available, and when the content is of interest for the user.
[object recognition, Visualization, Watches, Search problems, augmented reality, smart phone, user interfaces, Augmented reality, human interaction, AR, Three-dimensional displays, mobile computing, digital information, visual search, user-friendly experience, Prototypes, smartwatch-aided handheld augmented reality, Cameras, tablet computer]
[Poster] View independence in remote collaboration using AR
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
This poster presents an Augmented Reality (AR) system for remote collaboration that allows a remote user to navigate a local user's scene, independently from their viewpoint. This is achieved by using a 3D scan and reconstruction of the user's environment. The remote user can place virtual objects in the scene that the local user views through a head mounted display (HMD), helping them place real objects. A user study tested how the amount of remote view independence affected collaboration. We found that increased view independence led to faster task completion, more user confiden), helping them place real objects. A user study tested how the amount of remote view independence affected collaboration. We found that increased view independence led to faster task completion, more user confidence, and a decrease in verbal communication, but object placement accuracy remained unchanged.
[Solid modeling, Remote Collaboration, user confidence, Augmented Reality, verbal communication, augmented reality, helmet mounted displays, user interfaces, view independence, 3D scan, remote collaboration, virtual object, object placement accuracy, AR, Three-dimensional displays, Accuracy, task completion, Collaboration, Streaming media, Cameras, head mounted display, user navigation, Usability, HMD]
[Poster] Local optimization for natural feature tracking targets
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
In this work, we present an approach for optimizing targets for natural feature-based pose tracking such as used in Augmented Reality applications. Our contribution is an approach for locally optimizing a given tracking target instead of applying global optimizations, such as proposed in the literature. The local optimization together with visualized trackability rating leads to a tool to create high quality tracking targets.
[Target tracking, Image edge detection, natural feature tracking target, Natural features, Pipelines, visualized trackability rating, augmented reality, local optimization, augmented reality application, Optimization, Pose tracking, Image segmentation, Histograms, pose estimation, target tracking, Cameras, object tracking, natural feature-based pose tracking]
[Poster] Interacting with your own hands in a fully immersive MR system
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
This poster introduces a fully immersive Mixed Reality system we have recently developed, where the user is free to walk inside a virtual scenario while wearing a HMD. The novelty of the system lies in the fact that users can see and use their real hands - by means of a Kinect-like camera mounted on the HMD - in order to naturally interact with the virtual objects. Our working hypothesis are that the introduction of the photorealistic capture of users' hands in a coherently rendered virtual scenario induces in them a strong feeling of presence and embodiment without the need of using a synthetic 3D modelled avatar as a representation of the self. We also argue that the users' ability of grasping and manipulating virtual objects using their own hands not only provides an intuitive interaction experience, but also improves self-perception as well as the perception of the environment.
[Industries, virtual scenario, Avatars, fully immersive MR system, Virtual environments, augmented reality, user interaction, hand gestures, user interfaces, Training, synthetic 3D modelled avatar, Three-dimensional displays, fully immersive mixed reality system, intuitive interaction experience, Kinect-like camera, helmet mounted device, Cameras, virtual objects, Mixed Reality, HMD, photorealistic capture, avatars, natural interaction]
[Poster] Touch gestures for improved 3D object manipulation in mobile augmented reality
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
This work presents three techniques for 3D manipulation on mobile touch devices, taking the specifics of mobile AR scenes into account. We compare the common direct manipulation technique with two indirect techniques, which utilize only the thumbs to perform the transformations. The evaluation of the manipulation variants is conducted in a mixed reality (MR) environment which takes advantage of the controlled conditions of a full virtual reality (VR) system. A study with 18 participants shows that the two-thumb method tops the other techniques. It performs better with respect to the total manipulation time and total number of gestures.
[Performance evaluation, mobile touch devices, Tracking, touch gesture, Thumb, direct manipulation technique, Mobile communication, augmented reality, two-thumb method, Mobile handsets, Three-dimensional displays, mobile computing, VR system, gesture recognition, full virtual reality system, H.5.2 [Information Interfaces and Presentation, Virtual reality, 3D object manipulation, mixed reality environment, Information Interfaces and Presentation, MR environment, mobile augmented reality]
[Poster] The posture angle threshold between airplane and window frame metaphors
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Integrating different mental models and the corresponding interaction techniques for navigation and object manipulation tasks is a topic of concern for providing user interfaces for the wide variety of potential users. With a user controlled, egocentric hand-held device we aim at integrating the so far identified three major interaction techniques based on the metaphors of a steering wheel, a toy airplane and a window frame. Here, specifically the toy airplane and the window frame contradict each other, leaving the issue at which postural angle of the hand-held device the mental model of the users changes.
[egocentric hand-held device, Airplanes, Navigation, Wheels, Virtual environments, toy airplane, user mental model, airplane metaphor, user interfaces, Augmented reality, user interface, Information interfaces and presentation, H.5.m [Information interfaces and presentation, mobile computing, window frame metaphor, postural angle, posture angle threshold, object manipulation task, Cameras, Cognitive science, navigation task, steering wheel]
[Poster] A reconstructive see-through display
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
The two most common display technologies used in augmented reality head-mounted displays are optical see-through and video see-through. In this paper I demonstrate a third alternative: reconstructive see-through. By using a commodity depth camera to construct a dense 3D model of the world and rendering this to the user, distracting latency and position discrepancies between real and virtual objects can be reduced.
[Solid modeling, H.5.1 [Information Systems, Computational modeling, dense 3D model, head-mounted display, Jitter, optical see-through display, Information Systems, augmented reality, helmet mounted displays, video see-through display, Three-dimensional displays, Image color analysis, reconstructive see-through display, Image Processing and Computer Vision, Cameras, Rendering (computer graphics), commodity depth camera, virtual objects, solid modelling]
[DEMO] High volume offline image recognition
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
We show a prototype of an offline image recognition engine, running on a tablet with Intel®Atom&#x2122; processor, searching within less than 250ms through thousands (5000&#x002B;) of images. Moreover, the prototype still offers the advanced capabilities of recognising real world 3D objects, until now reserved only for cloud solutions. Until now image search within large collections of images could be performed only in the cloud, requiring mobile devices to have Internet connectivity. However, for many use cases the connectivity requirement is impractical, e.g. many museums have no network coverage, or do not want their visitors incurring expensive roaming charges. Existing commercial solutions are very limited in terms of searched collections sizes, often imposing a maximum limit of &#x003C;100 reference images. Moreover, adding images typically affects the recognition speed and increases RAM requirements.
[]
[DEMO] Insight: Webized mobile AR and real-life use cases
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
This demonstration shows a novel approach for Webizing mobile augmented reality, which uses HTML as its content structure, and its real-life use cases. Insight is a mobile AR Web browser that executes HTML5-based AR applications. By extending physical objects and places with a uniform resource identifier (URI), we could build objects of interest for mobile AR application as document object model (DOM) elements and control their behavior and user interactions through DOM events in standard HTML documents. A new CSS media type is defined to augment virtual objects to the physical objects. In this model, we introduce the concept of PLACE, which is the model of a physical space in which the user can be located. With this approach, mobile AR applications can be seamlessly developed as common HTML documents under the current Web eco-system. The advantages of the webized mobile AR, which is able to utilize all kind of Web resources without reworking, and its high productivity due to the seamless development of AR contents in HTML documents are shown with real-life use cases in various domains, such as shopping, entertainment, education, and manufacturing.
[authoring environment, real life, content structure, Web architecture, World Wide Web, mobile AR]
[DEMO] MRI design review system: A mixed reality interactive design review system for architecture, serious games and engineering using game engines, standard software, a tablet computer and natural interfaces
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Experience and control your design using natural interfaces! Most of todays conventional design review systems require special programming skills for preparation and high-capacity hard- and software for demonstration. Interacting with 3D data sometimes can be complicated. Today we face five major problem fields using design review systems: Interaction with 3D data, navigation in 3D space, controlling design alternatives, design presentation using less extensive hardware, content development without special software and programming skills. Developments also targeting these issues by using different methods are presented e.g. by LANCELLE, SETTGAST and FELLNER (2008). They developed DAVE &#x2014; Definitely Affordable Virtual Environment at Graz University of Technology. This immersive cage-based system today is used in evaluating the design of the new main railway station in Vienna, Austria. Also SHIRATUDDIN and THABET (2011) utilized the Torque 3D game engine to develop a Virtual Design Review System. Finally DUNSTON et. al. (2011) designed an Immersive Virtual Reality Mock-Up for Design Review of Hospital Patient Rooms. These and other research work was based on standard 3D game engines by using a conventional cave or power wall for presentation and physical immersion. The edddison MRI Design Review System is an easy to use mixed reality interface for design evaluation and presentation. It integrates a wide range of hardware input systems including a special 3D-printed tangible user interface, desktop computers, tablets and touch screens. On the software side it offers plug-ins for standard 3D software including Autodesk Navisworks and Showcase, Unity3D, Trimbles, SketchUp, Web GL and others. The edddison MRI Design Review System enables laymen to create their own interactive 3D content. It is a solution which makes the creation and presentation of interactive 3D applications as simple as preparing a powerpoint presentation. Without any programming skills you can easily manipulate 3D models within standard software applications. Control, change or adapt your design easily and interact with 3D models by natural interfaces and standard handheld devices. Navigate in 3D space using only your tablet computer. Complex buildings can be experienced by means of 2D floor plans and a touchscreen. System requirements are reduced by using standard software applications such as SketchUp or Unity3D. The edddison MRI Design Review System also makes it easy to present different design stages without extensive hard- and software on all common mobile platforms. Actual application areas are Architectural Design, Digital Prototyping, Industrial Simulation, Serious Games and Product Presentation. Currently, the system has two major use-cases: one setup will show the WebGL demo running on an iPad or an Android tablet computer. Using a WebGL/HTML5 cloud solution MRI Design Review System is able to reach the masses. The second demo is a SketchUp file controlled by optical tracking and 3D printed tangible objects also using a touchscreen or a handheld device. The edddison MRI Design Review System extends the range of existing design review systems with an easy-to-use hard- and software. Herein it simplifies the whole design process by an evolutionary, iterative approach, combined with a bunch of user-friendly intuitive interfaces.
[Web GL, User-Friendly Virtual Construction Kit, 3D-Visualization, SketchUp, Easy to use Mixed-Reality Interface (MRI), Building Information Modeling (BIM), Unity3D, Autodesk Showcase, Trimble, Autodesk Navisworks, Digital Prototyping, Serious Games]
[DEMO] On the use of augmented reality techniques in a telerehabilitation environment for wheelchair users' training
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
This work's purpose is to investigate the use of Augmented Reality techniques on telerehabilitation, applied to wheelchair users training. In this scenario, using a computer with unconventional devices, the user will be connected to a remote training space and will be able to issue commands, in order to accomplish the execution of training exercises. The telerehabilitation environment should reproduce the main challenges faced by wheelchair users in their daily activities.
[Augmented Reality, Wheelchair Training, Telerehabilitation]
[DEMO] Tracking texture-less, shiny objects with descriptor fields
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Our demo demonstrates the method we published at CVPR this year for tracking specular and poorly textured objects, and lets the visitors experiment with it and with their own patterns. Our approach only requires a standard monocular camera (no need for a depth sensor), and can be easily integrated within existing systems to improve their robustness and accuracy.
[Robust tracking, Dense Descriptors, Specular objects]
[DEMO] Comprehensive workspace calibration for visuo-haptic augmented reality
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Visuo-haptic augmented reality systems enable users to see and touch digital information that is embedded in the real world. Precise colocation of computer graphics and the haptic stylus is necessary to provide a realistic user experience. PHANToM haptic devices are often used in such systems to provide haptic feedback. They consist of two interlinked joints, whose angles define the position of the haptic stylus and three sensors at the gimbal to sense its orientation. Previous work has focused on a calibration procedures that align the haptic workspace within a global reference coordinate system and an algorithms that compensate the non-linear position error, which is caused by inaccuracies in the joint angle sensors. In our science and technology paper &#x201C;Comprehensive Workspace Calibration for Visuo-Haptic Augmented Reality&#x201D; [1], we present an improved workspace calibration that additionally compensates for errors in the gimbal sensors. This enables us to also align the orientation of the haptic stylus with high precision. To reduce the required time for calibration and to increase the sampling coverage, we utilize time-delay estimation to temporally align external sensor readings. This enables users to continuously move the haptic stylus during the calibration process, as opposed to commonly used point and hold processes. This demonstration showcases the complete workspace calibration procedure as described in our paper including a mixed reality demo scenario, that allows users to experience the calibrated workspace. Additionally, we demonstrate an early stage of our proposed future work in improved user guidance during the calibration procedure using visual guides.
[Artificial, H.5.1. [Information Interfaces and Presentation, augmented and virtual realities, Information Interfaces and Presentation, Haptic I/O]
[DEMO] Exploring multimodal interaction techniques for a mixed reality digital surface
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Quest &#x2014; XRoads is a multimodal and multimedia mixed reality version of the traditional role-play tabletop game Quest: Zeit der Helden. The original game concept is augmented with virtual content to permit a novel gaming experience. The demonstration consists of a turn-based skirmish, where players have to collaborate, control heroes or villains and use their abilities via speech, gesture, touch as well as tangible interactions.
[Multimodal, Gesture, Tangible, User Experience, Speech, Tabletop, Mixed Reality]
[DEMO] The collaborative design platform &#x2014; A protocol for a mixed reality installation for improved incorporation of laypeople in architecture
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Presentations and discussions between architects and clients during the early stages of design usually involve sketches, paper and models, with digital information in the form of simulations and analyses used to assess variants and underpin arguments. Laypeople, however, are not used to reading plans or models and find it difficult to relate digital representations to the real world. Immersive environments represent an alternative approach but are laborious and costly to produce, particularly in the early design phases where information and ideas are still vague. Our project shows, how linking analogue design tools and digital VR representation has given rise to a new interactive presentation platform that bridges the gap between analogue design methods and digital architectural presentation. The prototypical platform creates a direct connection between a physical volumetric model and interactive digital content using a large-format multi-touch table as a work surface combined with real-time 3D scanning. Coupling the 3D data from the scanned model with the 3D digital environment model makes it possible to compute design relevant simulations and analyses. These are displayed in real-time on the working model to help architects assess and substantiate their design decisions. Combining this with a 5-sided projection installation based on the concepts Carolina Cruz Neiras CAVE Automatic Virtual Environment (CAVE)1 offers an entirely new means of presentation and interaction. The design (physical working model), the surroundings (GIS data) and the simulations and analyses are presented stereoscopically in real-time in the virtual environment. While the architect can work as usual, the observer is presented with an entirely new mode of viewing. Different ideas and scenarios can be tried out spontaneously and new ideas can be developed and viewed directly in three dimensions. The client is involved more directly in the process and can contribute own ideas and changes, and then see these in user-centred stereoscopic 3D. By varying system parameters, the model can be walked through at life size.
[Computer Systems Organization, Computer Applications, Information Systems, C.2.2 [Computer Systems Organization]
[DEMO] Towards user perspective augmented reality for public displays
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
We demonstrate ad-hoc augmentation of public displays on handheld devices, supporting user perspective rendering of display content. Our prototype system only requires access to a screencast of the public display, which can be easily provided through common streaming platforms and is otherwise self-contained. Hence, it easily scales to multiple users.
[user perspective rendering, augmented reality, public displays]
[DEMO] Adventurous Dreaming Highflying Dragon: A full body game for children with Attention Deficit Hyperactivity Disorder (ADHD)
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Adventurous Dreaming Highflying Dragon is a full body-driven, game prototype for children ages 6&#x2013;8 with a diagnosis of Attention Deficit Hyperactivity Disorder (ADHD). The current prototype incorporates research evidence showing that physical activity can help improve ADHD-related symptoms. Physical activity is integrated with cognitively challenging tasks that may improve brain activity by encouraging goal planning and dedication. The current prototype is includes three mini-games, each of which teaches skills with real-life use potential. Players role-play as a young dragon in the story and virtual world as they repeat virtual tasks several times to gain mastery over real-life skills. Each activity is focused on a specific strength/weakness reported in children with ADHD, with game mechanics targeting ADHD diagnosis categories: specific hyperactivity, impulsivity and inattention. This demonstration is based on the findings published in Hashemian, Y., &amp; Gotsis, M. 2013, November. In Proceedings of the 4th Conference on Wireless Health (p. 12). ACM. http://dl.acm.org/citation.cfm?id&#x003D;2534101
[Kinect, ADHD/ADD, Motor skills, Games, Attention, Prototyping, Executive Function]
[Demo] Measurement of perceptual tolerance for inconsistencies within mixed reality scenes
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
This demonstration is a live example of the experiment presented in [1], namely a method of assessing the visual credibility of a scene where a real person interacts with a virtual object in realtime. Inconsistencies created by actor's incorrect estimation of the virtual object are measured through a series of videos, each containing a defined visual error and rated against interaction credibility on a scale of 1&#x2013;5 by conference delegates.
[Virtual Studios, Interaction Framework, Performance Measurement, Mixed Reality]
[DEMO] INDICA : Interaction-free display calibration for optical see-through head-mounted displays based on 3D eye localization
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
A correct spatial registration of Optical See-Through Head-Mounted Displays (OST-HMD) w.r.t. a user's eye(s) is an essential problem for any AR application using the such HMDs (Fig. 1). Maintaining the correct registration demands frequent (re)calibrations for the end-users whenever they move the HMD on their head. Thus, a calibration technique should be simple and accurate for the universal, long-run use of the displays. This demonstration showcases INDICA, an automatic OST-HMD calibration approach presented in our previous work[1] and ISMAR 2014 paper [2]. The method calibrates the display to the user's current eyeball position by combining online eye-position tracking with offline parameters. Visitors of our demonstration can try our both manual calibration and our interaction-free calibration on a customized OST-HMD.
[Optical see-though head-mounted display, eye tracking, calibration, HMD]
[DEMO] Integrating highly dynamic RESTful linked data APIs in a virtual reality environment
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
We demonstrate a Virtual Reality information system that shows the applicability of REST in highly dynamic environments as well as the advantages of Linked Data for on-the-fly data integration. We integrate a motion detection sensor application to remote control an avatar in the Virtual Reality. In the Virtual Reality, information about the user is integrated and visualised. Moreover, the user can interact with the visualised information.
[]
[Demo] Real-time illumination estimation from faces for coherent rendering
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
We showcase a method for estimating the real-world lighting conditions within a scene based on the visual appearance of the user's face captured in a single image of a monocular user-facing RGB camera. The implementation is based on our ISMAR 2014 paper [8]. The light reflected from the face towards the camera is measured and the most plausible real-world lighting condition explaining the measurement is estimated in real-time based on knowledge acquired in a one-time pre-processing of a set of images of different faces under known illumination. The estimated illumination is instantly used for the rendering of the virtual objects.
[Illumination Estimation, Demo, Augmented Reality, Precomputed Radiance Transfer, Machine Learning, Real-Time, Monocular, Computer Graphics, ISMAR 2014, Coherent Rendering, Inverse Lighting, Spherical Harmonics, Computer Vision]
[DEMO] Towards augmented reality user interfaces in 3D media production
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
For this demo, we present an Augmented Reality (AR) User Interface (UI) for the 3D design software Autodesk Maya, aimed at professional media creation. A user wears a head-mounted display (HMD) and thin cotton gloves which allow him to interact with virtual 3D models in the work area. Additional viewers can see the video stream on a projector and thus share the users view. Both head and hand positions are tracked from the HMD video stream, and an inertial measurement unit (IMU) and conductive materials on the gloves allow interaction with virtual objects. This system is built using Autodesk Maya &#x2014; a professional 3D software package commonly used in the media industry &#x2014; and aims to fulfill the requirements of professional 3D design work which we identified in our paper of the same title. While still an early prototype, it was already tested with media professionals to evaluate our approach.
[]
[Demo] Thermal touch: Thermography-enabled everywhere touch interfaces for mobile augmented reality applications
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
We present an approach that makes any real object a touch interface for mobile Augmented Reality (AR) applications. Using infrared thermography, we detect residual heat resulting from a warm fingertip touching the colder surface of an object. This approach can clearly distinguish if a surface has actually been touched, or if a finger only approached it without any physical contact, and hence significantly less heat transfer. Once a touch has been detected in the thermal image, we determine the corresponding 3D position on the touched object based on object tracking using a visible light camera. This 3D position then enables user interfaces to naturally interact with real objects and associated digital information. The emergence of wearable computers and head-mounted displays desires for alternatives to a touch screen, which is the primary user interface in handheld Augmented Reality applications. Voice control and touchpads provide a useful alternative to interact with wearables for certain tasks, but particularly common interaction tasks in Augmented Reality require to accurately select or define 3D points on real surfaces. We propose to enable this kind of interaction by simply touching the respective surface with a fingertip. In this demonstration, which is based on our ISMAR 2014 paper [2], we show that our method enables intuitive interaction for mobile Augmented Reality with common objects.
[]
[DEMO] Device vs. user-perspective rendering in AR applications for monocular optical see-through head-mounted displays
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
This demonstration allows visitors to use AR applications for monocular optical see-through head-mounted displays with two forms of visualization. One is the device-perspective approach, in which the user sees the virtual content registered with the camera image at the display. The other is the user-perspective method, in which the display is used as a de facto optical see-through device and the virtual content is registered with the real world.
[optical see-through head-mounted displays, user-perspective rendering, augmented reality, device-perspective rendering]
[DEMO] RGB-D-T camera system for AR display of temperature change
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
The anomalies of power equipment can be founded using temperature changes compared to its normal state. In this paper we present a system for visualizing temperature changes in a scene using a thermal 3D model. Our approach is based on two precomputed 3D models of the target scene achieved with a RGB-D camera coupled with the thermal camera. The first model contains the RGB information, while the second one contains the thermal information. For comparing the status of the temperature between the model and the current time, we accurately estimate the pose of the camera by finding keypoint correspondences between the current view and the RGB 3D model. Knowing the pose of the camera, we are then able to compare the thermal 3D model with the current status of the temperature from any viewpoint.
[temperature, thermal camera. Viewpoint Generative Learning, RGB-D camera]
[DEMO] Markerless augmented reality solution for industrial manufacturing
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
We present a comprehensive augmented reality solution to efficiently perform different verification pocedures during industrial assembly-line manufacturing using CAD data from product lifecycle management (PLM) systems. The demonstration focuses on a variety of industrial use-cases that have to go through different control processes, assisted by augmented reality tools. Thus, the experience has to be precise, robust to natural movements and easy to realise for an assembly-line technician.
[]
[DEMO] &#x201C;It's a Pirate's Life&#x201D; AR game
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
We present &#x201C;It's a Pirate's Life&#x201D; demonstration, an Augmented Reality (AR) game which makes use of real-time 3D reconstruction and tracking using an Intel® RealSense&#x2122; camera system embedded in a tablet to build a dynamic game world. Players can play as a pirate ship captain searching for gold on a virtual sea overlaid on the real-world. Real-world objects become part of the play space; islands in the tropical seas which you have to navigate your ships around while avoiding cannon balls to find the treasure. Players control the wind, and hence, direction of sail by moving the tablet around the play space to guide the virtual ship in the real and virtual environment to the pirate gold.
[SLAM, Augmented Reality, Real-time 3D reconstruction, Marker-less tracking, Depth cameras, GPU, Gaming]
[DEMO] QubeAR: Cube style QR code AR interaction
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
QR code, for its recognition robustness and data capacity, has been often used for Augmented Reality applications as well as for other commercial applications. However, it is difficult to enable tangible interactions through which users may change 3D models or animations. It is because QR codes are automatically generated by the rules, and are not easily modifiable. Our goal was to enable QR code based Augmented Reality interactions. By analysis and through experiments, we discovered that some parts of a QR code can be altered to change the text string that the QR code represents. In this demo, we introduced a prototype for QR code based Augmented Reality interactions, which allows for Rubik's cube style rolling interactions.
[Interaction, Augmented Reality, QR code]
[DEMO] G-SIAR: Gesture-speech interface for augmented reality
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
We demonstrate an Augmented Reality (AR) system that utilizes a combination of direct free hand interaction and indirect multimodal gesture and speech interface. A three-dimensional (3D) design sandbox application, featuring online object creation, has been developed to illustrate the use case of our system that supports dual interaction techniques.
[Augmented reality, natural interaction, multimodal interface]
[DEMO] Dense planar SLAM
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Using higher-level entities during mapping has the potential to improve camera localisation performance and give substantial perception capabilities to real-time 3D SLAM systems. We present an efficient new real-time approach which densely maps an environment using bounded planes and surfels extracted from depth images (like those produced by RGB-D sensors or dense multi-view stereo reconstruction). Our method offers the every-pixel descriptive power of the latest dense SLAM approaches, but takes advantage directly of the planarity of many parts of real-world scenes via a data-driven process to directly regularize planar regions and represent their accurate extent efficiently using an occupancy approach with on-line compression. Large areas can be mapped efficiently and with useful semantic planar structure which enables intuitive and useful AR applications such as using any wall or other planar surface in a scene to display a user's content.
[Reconstruction, Scene understanding, Computing methodologies [Scene understanding, Image Processing and Computer Vision, Information Interfaces and Presentation]
[Demo] On-site augmented collaborative architecture visualization
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
The early design phase for a new building is a crucial stage in the design process of architects. It has to be ensured that the building fits into the future environment. The Collaborative Design Platform targets this issue by integrating modern digital means with well known traditional concepts. Well-used styrofoam blocks are still cut by hand but are now tracked, placed and visualized in 3D by use of a tabletop platform and a TV screen showing an arbitrary view of the scenery. With this demonstration, we get one step further and provide an interactive visualization at the proposed building site, further enhancing collaboration between different audiences. Mobile phones and tablet devices are used to visualize marker-less registered virtual building structures and immediately show changes made to the models in the Collaborative Design laboratory. This way, architects can get a direct impression about how a building will integrate within the environment and residents can get an early impression about future plans.
[Information interfaces and presentation, H.5.m [Information interfaces and presentation]
[DEMO] A complete interior design solution with diminished reality
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
We demonstrate an interior design solution with advanced features, most importantly, a diminished reality feature. The diminished reality functionality takes 3D indoor structures into account and adapts to the lighting of the environment. We present two demonstrations on tablet and laptop. The iPad version utilizes touch screen for selecting removed objects from the image. It allows users to build modular furniture and it casts shadows of the virtual furniture for realistic visualization result. On laptop PC we demonstrate real time diminished reality for indoor AR with several options. Our demonstrations are optimized for interior design and indoor AR.
[AR, indoor AR, realtime, interior design, object removal, diminished reality, user interaction, illumination adaptation, iPad, touch screen, tablet PC, Augmented reality]
[Demo] Smartwatch-aided handheld augmented reality
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
We demonstrate a novel method for interaction of humans with real objects in their surrounding combining Visual Search and Augmented Reality (AR). This method is based on utilizing a smart-watch tethered to a smartphone, and it is designed to provide a more user-friendly experience compared to approaches based only on a handheld device, such as a smartphone or a tablet computer. The smartwatch has a built-in camera, which enables scanning objects without the need to take the smartphone out of the pocket. An image captured by the watch is sent wirelessly to the phone that performs Visual Search and subsequently informs the smartwatch whether digital information related to the object is available or not. As described in our ISMAR 2014 poster [6], we distinguish between three cases. If no information is available or the object recognition failed, the user is notified accordingly. If there is digital information available that can be presented using the smartwatch display and/or audio output, it is presented there. The third case is that the recognized object has digital information related to it, which would be beneficial to see in an Augmented Reality view spatially registered with the object in real-time. Then the smartwatch informs the user that this option exists and encourages using the smartphone to experience the Augmented Reality view. Thereby, the user only needs to take the phone out of the pocket in case Augmented Reality content is available, and when the content is of interest for the user.
[]
[Demo] A mobile augmented reality system for portion estimation
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Accurate assessment of nutrition information is an important part in the prevention and treatment of a multitude of diseases, but remains a challenging task. We present a novel mobile augmented reality application, which assists users in the nutrition assessment of their meals. The user sketches the 3D form of the food and selects the food type. The corresponding nutrition information is automatically computed.
[Computer Applications, H.5.1 [Information Interfaces and Presentation, Image Processing and Computer Vision, Information Interfaces and Presentation]
[Demo] Placing information near to the gaze of the user
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Gaze tracking facilities have yet mainly been used in general for marketing or the disabled and, more specifically, in Augmented Reality, for interaction with control triggers, such as buttons. We go one step further and use the line of sight of the user to attach information. While any information may not conceal the view of the user, we displace the information by an angular degree and provide means for the user to capture the information by looking at it. With such an apporach we see a potential for faster resuming times of the original task for which a required information needs to be accessed. The demonstration shows a comparably complex primary task assisted by our gaze-mounted information and illustrates the inherent differences for information access w.r.t. conventional methods, such as listing action items at a fix position in space or on a screen.
[Information interfaces and presentation, H.5.m [Information interfaces and presentation]
[DEMO] Fast vision-based multiplanar scene modeling in unprepared environments
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
In this demonstration, we present a general purpose Augmented Reality (AR) system that allows to add easily 3D computer generated (CG) objects into real man-made environments. Our system goes to a very intuitive and easy in situ 3D structure recovery of planar piecewise scenes without using powerful hardware nor commodity sensors. The user simply has to move the camera (translation of the camera is mandatory) and take two different pictures of the scene, and our approach obtains a rough planar piecewise representation of the environment suitable to conduct multi-planar tracking for visual model-based augmented reality and to augment it with virtual objects coherently. Polyhedral representations of scenes are very convenient for manmade environments indoor (e.g., offices, rooms, classrooms) and outdoor (e.g., facades, floor), hence we focus the potential applications of our system to augment simple rooms or urban scenes with virtual imagery.
[geometric coherence, structure from motion, scene-based modeling, Augmented reality, multiplanar structure]
[DEMO] Mobile augmented reality &#x2014; 3D object selection and reconstruction with an RGBD sensor and scene understanding
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
In this proposal we show case two 3D reconstruction systems running in real-time on a tablet equipped with a depth sensor. We believe that the proposed set of demonstrations will engage ISMAR attendees both in terms of tracking technology and user experience. Both demos show state-of-the art 3D reconstruction technology and give attendees a chance to try hands-on our tracking with simple and interactive user interfaces.
[Reconstruction, Scene understanding, Rendering]
[DEMO] Mobile augmented reality &#x2014; Tracking, mapping and rendering
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
In this proposal we suggest a set of demonstrations to be presented at ISMAR 2014 around the topic of monocular tracking, mapping and rendering on mobile phones. We believe that the proposed set of demonstrations will engage ISMAR attendees both in terms of tracking technology and user experience. On the one hand, all demos show state-of-the art tracking and mapping technology and gives attendees a chance to try hands-on our tracking solutions and to discuss them with our researchers and developers. All demos also showcase compelling use cases enabled by our technology, encouraging discussions on topics such as integration of tracking technology with game engines and user experience research.
[Reconstruction, Tracking, Rendering]
[DEMO] User friedly calibration and tracking for optical stereo see-through augmented reality
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Optical see through head mounted displays (OST-HMD) are ever since the first days of Augmented Reality (AR) in focus of development and in nowadays first affordable and prototypes are spread out to markets. Despite common technical problems, such as having a proper field of view, weight, and other problems concerning the miniaturization of these systems, a crucial aspect for AR relies also in the calibration of such a device with respect to the individual user for proper alignment of augmentations. Our demonstrator shows a practical solution for this problem along with a fully featured example application for a typical maintenance use case based on a generalized framework for application creation. We depict the technical background and procedure of the calibration, the tracking approach considering the sensors of the device, user experience factors, and its implementation procedure in general. We present our demonstrator using an Epson Moverio BT-200 OST-HMD.
[H.5.1 [Information Interfaces and Presentation, Numerical Analysis, Image Processing and Computer Vision, Information Interfaces and Presentation]
[DEMO] Tablet system for visual, overlay of 3D virtual object onto real environment
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
We propose a novel system for visual overlay of 3D virtual object onto real environment observed by tablet PC with camera. This system allows us to visually simulate the layout of virtual 3D objects such as furniture in the real environment captured by the tablet PC. For estimating the pose and position of the tablet PC in the 3D structure of the target environment, we propose and implement the 2 procedures using the captured image and using the motion sensor in tablet PC. Those performances are presented in the demonstration.
[space recognition, object translation, object preview, plane arrangement]
[DEMO] Displaying free-viewpoint video with user controlable head mounted display DEMO
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
In this paper, we propose a method to experience a free-viewpoint video and image with a head mounted display (HMD) and a game controller that enable to operate it intuitively. The free-viewpoint video is generated by multiple 4K resolution cameras in sport games such as soccer and american football. This method can provide us a player's perspective. We adopt a billboard method to make a free-viewpoint video which is consisted of multiple textures accoding to user specified viewpoint. For implementing the program and displaying images effectively we used a game development system, a HMD and a game controller that user can operate their own views in the high degree of freedom. Experiment results show that the proposed method can obviously achive effective view.
[head mounted display, billboard, free-viewpoint video]
Video see through AR head-mounted display for medical procedures
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
In the context of image-guided surgery (IGS), AR technology appears as a significant development in the field since it complements and integrates the concepts of surgical navigation based on virtual reality. The aim of the project is to optimize and validate an ergonomic, accurate and cheap video see-through AR system as an aid in various typologies of surgical procedures. The system will ideally have to be inexpensive and user-friendly to be successfully introduced in the clinical practice.
[Medical device validation, Interest point and salient region detections, Tracking, Object detection, Image-guided surgery, Mixed / augmented reality]
Corneal imaging in localization and HMD interaction
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
The human eyes perceive our surroundings and are one of, if not our most important sensory organs. Contrary to our other senses the eyes not only perceive but also provide information to a keen observer. However, thus far this has been mainly used to detect reflection of infrared light sources to estimate the user's gaze. The reflection of the visible spectrum on the other hand has rarely been utilized. In this dissertation we want to explore how the analysis of the corneal image can improve currently available eye-related solutions, such as calibration of optical see-through head-mounted devices or eye-gaze tracking and point of regard estimation in arbitrary environments. We also aim to study how corneal imaging can become an alternative for established augmented reality tasks such as tracking and localization.
[augmented reality, corneal imaging, optical see-through head mounted display]
Semantic contextual augmented reality environments
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
The paper presents the concept of dynamic Contextual Augmented Reality Environments (CARE), in which augmentation presented to users is dynamically constructed based on four semantically described elements. The first element is the user's context (preferences, privileges, location, time, device's capabilities). The second element is a set of trackables &#x2014; visual markers representing real world objects that can be augmented for a given user in a given context. The third element are content objects, representing interactive 2D and 3D multimedia content including video sequences and sounds to be presented on the trackables. The last one is a description of a user interface, which may be specific to a concrete device or application and which indicates the forms of information presentation and interaction available to a user.
[]
Designing support for collaboration around physical artefacts: Using augmented reality in learning environments
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
The aim of this thesis is to identify mechanisms for supporting collaboration around physical artefacts in co-located and remote settings. To explore the research question in the project, a Research through Design approach has been adopted. A technology probe &#x2014; an evolutionary prototype of a remote collaboration system &#x2014; will be used to fuel the research. The prototype will facilitate collaboration between small groups around physical artefacts in an augmented learning environment. The prototype will inform future collaborative augmented reality technology design.
[Collaboration, augmented reality, remote learning]
AR development with the Metaio product suite: Demonstration of use cases in industry
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
This tutorial covers the AR creation process from idea to solution in an industrial context. Thereby employing the Metaio AR pipeline with practical demonstrations to show how AR projects can be quickly realized with readily available tools. To illustrate how AR can be employed to solve industrial problems, this tutorial will showcase several interesting industrial use cases implemented by Metaio. To conclude, the tutorial provides a brief outlook into new available tracking technologies and AR for wearable devices. Further, potential challenges for industrial integration of AR, for example with ERP systems, are pointed out. The attendees will acquire a solid understanding of developing an AR scenario from idea to solution. Further the tutorial provides a good insight into the Metaio software environment, enabling and hopefully encouraging attendees to implement their own AR projects in industry.
[Industries, Context, Pipelines, Tutorials, Abstracts, Solids, Software]
Fusing web technologies &amp; augmented reality
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Within the German research project ARVIDA a large consortium of industrial Virtual and Augmented Reality users, of technology providing companies and research institutes cooperate on the establishment of highly flexible web-based reference architecture for Augmented Reality applications. The use of web technologies is motivated by modern web standards as WebGL or WebRTC supporting e.g. real time rendering of 3D-content of video streaming within Web-Browsers. Thereby, the use of Web technologies not only offers the possibility to develop applications platform and OS independent but it also facilitates the integration of Augmented Reality into industrial workflows or PDM environments. The developed reference architecture offers RESTful tracking, rendering and interaction services that foster the combination and exchange of different algorithms with the aim to ft the technology to the specific requirements of an AR-applications in an optimal way.
[Context, Tutorials, Abstracts, Educational institutions, Rendering (computer graphics), Augmented reality, WebRTC]
A &#x2018;Look Into&#x2019; Medical augmented reality
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
The concept of augmented reality (AR) has been introduced to variety of felds in the last decade. Recent development of portable devices such as smart phone and tablet PC provides the community a lot of possible applications in AR systems. Even in the medical feld, various AR systems have recently been proposed: systems for education, pre-planning, and those in the operating room. The aim of this tutorial is to bridge the expertise between the researchers in ISMAR community and medical doctors so that researchers can contribute to the medical domain with their specialty more than one can do right now. This tutorial aims to make a bridge between researchers in augmented reality feld and medical doctors. We target an audience interested in medical augmented reality systems.
[Bridges, Surgery, Tutorials, Educational institutions, Augmented reality, Biomedical imaging]
Designing location-based experiences
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
The development of location based applications from the perspective of story structure and product design will be presented. We present the challenges with developing location based mobile products from a storytelling perspective and tools for integrating user experience into the development process to drive story structure of new products. Included is a casestudy focused on the Ghost of Venice mixedreality film project, which is centered on an augmented reality mobile application. Learning objectives of this tutorial are: Understand how communication patterns have evolved with new technologies to their present state and how this influences the way stories are told. &#x2022; Understand the design intent behind different AR/MR location based games from the story and user experience design perspectives. &#x2022; Gain an understanding for how to approach AR/MR projects, which may include distributed storylines over different media. &#x2022; Understand the complexity of creating AR/MR location based applications and how to address them in app or story development. &#x2022; Gain insight into how to work between writers and the app development (design and coding) team to efficiently translate story concepts into mobile apps. A workshop module is included at the end of the tutorial session, and at this point participants will be engaged to design a location based game experience. This will show in a project based learning environment, what the participants learned from the tuto&#x00AD;rial.
[Conferences, Tutorials, Games, Abstracts, Media, Mobile communication, Product design]
Diminished reality as challenging extension of mixed and augmented reality
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Diminished Reality (DR) has been considered as a sub-technology of Mixed and Augmented Reality (AR/MR). While AR/MR means technologies that add and/or overlay visual information onto images of real scene for providing users to enhance their visual experiences with the added/overlaid information, DR aims the similar enhanced visual experiences by deleting visual information from the images of real scene. Adding and deleting visual information might be considered as same technical issues, but they are actually totally different. In DR, visual information that is hidden by the deleted object should be recovered for filling into the deleted area. This recovery of the hidden area is not required for general adding/overlaying based AR/MR, but should be one of the typical issues for achieving DR. Camera pose estimation and tracking is a typical issue in AR/MR, but the condition of the scene and required performance for DR are not always the same as AR/MR. For example, the object to be diminished/removed should be detected and tracked while the camera is freely moving for DR. In this tutorial, challenging technical issues for DR are addressed, such as recovery of hidden area, detecting and tracking the object to be removed/diminished, tracking camera poses, illumination matching and re-lighting, etc. In addition to those technical issues for DR, a survey of applications of DR, expected futures with DR, and human factors of DR are also presented.
[Visualization, Computer vision, Laboratories, Educational institutions, Cameras, Augmented reality]
Google glass, The META and Co. How to calibrate optical see-through head mounted displays
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Head Mounted Displays such as Google Glass and the META have the potential to spur consumer-oriented Optical See-Through Augmented Reality applications. A correct spatial registration of those displays relative to a user's eye(s) is an essential problem for any HMD-based AR application. We provide an overview of established and novel approaches for the calibration of those displays including hands on experience in which participants will calibrate such head mounted displays. The following list provides a tentative list of topics covered during the tutorial. &#x2022; Part 1: Introduction to OST calibration &#x2022; Why OST Calibration is important? &#x2022; Differences to Camera Calibration &#x2022; Introduce camera calibration &#x2022; Why is OST calibration hard? &#x2022; The user in the loop &#x2014; pointing accuracy &#x2022; Slipping, the need for recalibration &#x2022; Principal aspects of OST-HMD calibration &#x2022; overview of data collection &#x2022; confirmation methods &#x2022; optimization &#x2022; mono vs stereo &#x2022; Details of OST calibration &#x2022; Data collection methods: &#x2022; SPAAM, Multi Point collection, stereo methods &#x2022; Confirmation methods &#x2022; Optimization approaches &#x2022; Evaluation: perceptual measures vs. analytic measures &#x2022; State of the art: Semi-, fully automatic calibration methodses &#x2022; Part 2: Hands-on calibration &#x2022; SPAAM-based calibration of Epson Moverio/ Google Glass with inside-out marker tracker.
[Google, Optical feedback, Glass, Educational institutions, Adaptive optics, Calibration, Augmented reality]
Open and interoperable augmented reality
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Today an experience developer must choose tools for authoring AR experiences based on many factors including ease of use, performance across a variety of platforms, reach and discoverability and cost. The commercially viable options are organized in closed technology silos (beginning with SDKs). A publisher of experiences must choose one or develop for multiple viewing applications, then promote one or more application to the largest possible audience. Developers of applications must then maintain the customized viewing application over time across multiple platforms or have the experience (and the application) expire at the end of a campaign.
[Three-dimensional displays, Tutorials, Companies, Educational institutions, Geospatial analysis, Augmented reality, Standards]
The glass class: Designing wearable interfaces
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
The course will teach how to create compelling user experiences for wearable computers focusing on design guidelines, prototyping tools, research directions, and a hands-on design experience. These topics will be presented using a number of platforms such as Google Glass, the Recon Jet and Vuzix M-100, although the material will be relevant to other wearable devices. The class will begin with an overview of almost 50 years of wearable computing, beginning with the casino computers of Ed Thorp, through the pioneering efforts of researchers at CMU and MIT, to the most recent commercial systems. The key technology components of a wearable system will be covered, as well as some of the theoretical underpinnings. Next, a set of design guidelines for developing wearable user interfaces will be presented. These include lessons learned from using wearables on a daily basis, design patterns from existing wearable interfaces, and relevant results from the research community. These will be presented in enough details that attendees will be able to use them in their own wearable designs. The third section of the course will introduce a number of tools that can be used for rapid prototyping of wearable interfaces. These include screen-building tools such as Glasssim, through to templating tools that support limited interactivity, and simple programming tools such as Processing. This will lead into a section that discusses the technology of wearable systems in more detail. For example, the different types of head mounted displays for wearables, tracking technology for wearable AR interfaces, input devices, etc. Finally, we will discuss active areas of research that will affect wearable interfaces over the next few years. This includes technologies such as new display hardware, input devices, body worn sensors, and connectivity. The course will have the following educational goals: &#x2022; Provide an introduction to head mounted wearable computers &#x2022; Give an understanding of current wearable computing technology &#x2022; Describe key design principles/interface metaphors &#x2022; Provide an overview of the relevant human perceptual principles &#x2022; Explain how to use Processing for rapid prototyping &#x2022; Show how to capturing and use sensor input &#x2022; Outline active areas of research in wearable computing &#x2022; Hands on demonstrations with Google Glass and other wearable computers.
[Google, Wearable computers, Design methodology, Glass, Educational institutions, Sensors]
Training detectors and recognizers in Python and OpenCV
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
Monty Python's Flying Circus had a &#x201C;cat detector van&#x201D; so, in this tutorial, we use Python and OpenCV to make our very own cat detector and recognizer. We also cover examples of human face detection and recognition. More generally, we cover a methodology that applies to training a detector (based on Haar cascades) for any class of object and a recognizer (based on LBPH, Fisherfaces, or Eigenfaces) for any unique objects. We build a small GUI app that enables an LBPH-based recognizer to learn new objects interactively in real time. Although this tutorial uses Python, the project could be ported to Android and iOS using OpenCV's Java and C&#x002B;&#x002B; bindings. Attendees will gain experience in using OpenCV to detect and recognize visual subjects, especially human and animal faces. GUI development will also be emphasized. Attendees will be guided toward additional information in books and online. There is no formal evaluation of attendees' work but attendees are invited to demonstrate their work and discuss the results they have achieved during the session by using different detectors and recognizers and different parameters.
[Training, Cats, Face recognition, Detectors, Tutorials, Face, Graphical user interfaces]
Collaboration in mediated and augmented reality
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
In this half-day workshop we will explore how Augmented Reality (AR) and Mediated Reality (MR) can be used to develop radically new types of collaborative experiences that overcome some of the limitations of current conferencing systems. In combination, AR and MR technologies could be used to merge the shared perceived realities of different users as well as enriching their own individual experience in a collaborative task. The goal of the workshop is to bring together researchers who are interested developing collaborative systems using AR and MR technologies. They will build a picture of current and prior research on collaboration in AR and MR as well as set up a common research agenda for work going forward. Topics of the workshop will address open research issues and include but are not restricted to the following: &#x2022;Case studies on using MR/AR for collaboration &#x2022;Tools for building collaborative MR/AR systems &#x2022;Effects of MR/AR on trust, presence, and coordination &#x2022;Interaction models for collaboration in MR/AR &#x2022;Tools for collaboration in MR/AR &#x2022;Collaboration awareness in MR/AR.
[Computers, Conferences, Collaboration, Educational institutions, Augmented reality, Information systems]
Advanced manufacturing with augmented reality
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
During this workshop, the participants will explore themes in three areas: &#x2022; Augmented Reality and Technical Data Delivery. In this area we will hear from experts and engage in discussion to develop agreement among the workshop participants on what constitutes the state of the art for use of AR to deliver technical data in 2014. &#x2022; Augmented Reality and the Shop Floor Environment. In this area we will engage in discussion to develop agreement among the workshop participants on what constitutes the state of the art for use of AR to map value streams and increase value, to avoid waste and increase sustainability, and to reduce risk and prevent human error on the shop floor in 2014. This will be documented in a brief, joint statement. Participants will then develop consensus on three major research areas on which there must be further investment in order for the technology to be widely implemented and adopted on the shop floor. &#x2022; Augmented Reality and Quality Inspection. In this area we will engage in discussion to develop agreement among the workshop participants on what constitutes the state of the art for use of AR to inspect manufactured goods in 2014. This will be documented in a brief, joint statement. Participants will then develop consensus on three major research areas on which there must be further investment in order for the technology to be widely implemented and adopted. The three themes of the workshop are highly relevant to the ISMAR conference, not least since the region &#x2014; Bavaria &#x2014; strives in the use of AR in the manufacturing industry. This workshop will increase awareness of the current state of the art in industry and help further develop the research agenda for AR in manufacturing through the identification of common interests and grand challenges. This workshop will produce nine research topics that can become the basis for academic and public/private partnership-funded research projects.
[Conferences, Communities, Educational institutions, Manufacturing, Joints, Augmented reality, Investment]
Workshop on tracking methods &amp; applications
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
The focus of this workshop is on all issues related to tracking for mixed and augmented reality applications. Unlike the tracking sessions of the main conference, this workshop does not require pure novelty of the proposed methods; it rather encourages presentations that concentrate on complete systems and integrated approaches engineered to run in real-world scenarios. The research felds covered include self-localization using computer vision or other sensing modalities (such as depth cameras, GPS, inertial, etc.) and tracking systems issues (such as system design, calibration, estimation, fusion, etc.). This year's focus is also expanded to research on object detection and semantic scene understanding with relevance to augmented reality. Implementations on mobile devices and under real-time constraints are also part of the workshop focus. These are issues of core importance for practical augmented reality systems.
[Computer vision, Conferences, Educational institutions, Cameras, Robot sensing systems, Mobile handsets, Augmented reality]
Hands free &#x2014; Exploring AR glasses and their peculiarities
2014 IEEE International Symposium on Mixed and Augmented Reality
None
2014
The workshop will focus on AR Glasses and how their new and unique interfaces change the way we use, develop, interact with and percept Augmented Reality. The main goal is to get a large amount of people interested in Augmented Reality Glasses to discuss particular topics of and issues with AR Glasses, and to understand what developers and users must change in the way they use AR on AR Glasses, compared to AR on smartphones, tablets and laptops.
[Computer vision, Portable computers, Conferences, Glass, Abstracts, Augmented reality, Smart phones]
Message from the ISMAR 2015 General Chairs
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Welcome Message from the ISMAR 2015 Science and Technology Program Chairs
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from the Science and Technology Poster Chairs
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from the Demonstration Chairs
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from the Workshop and Tutorial Chairs
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from the Doctoral Consortium Chairs
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from the Tracking Competition Chairs
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
ISMAR 2015 Conference Committee Members
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Provides a listing of current committee members and society officers.
[]
ISMAR 2015 Steering Committee Members
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Provides a listing of current committee members and society officers.
[]
Keynotes
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Provides an abstract for each of the keynote presentations and may include a brief professional biography of each
[]
Introducing Augmented Reality to Optical Coherence Tomography in Ophthalmic Microsurgery
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Augmented Reality (AR) in microscopic surgery has been subject of several studies in the past two decades. Nevertheless, AR has not found its way into everyday microsurgical workflows. The introduction of new surgical microscopes equipped with Optical Coherence Tomography (OCT) enables the surgeons to perform multimodal (optical and OCT) imaging in the operating room. Taking full advantage of such elaborate source of information requires sophisticated intraoperative image fusion, information extraction, guidance and visualization methods. Medical AR is a unique approach to facilitate utilization of multimodal medical imaging devices. Here we propose a novel medical AR solution to the long-known problem of determining the distance between the surgical instrument tip and the underlying tissue in ophthalmic surgery to further pave the way of AR into the surgical theater. Our method brings augmented reality to OCT for the first time by augmenting the surgeon's view of the OCT images with an estimated instrument cross-section shape and distance to the retinal surface using only information from the shadow of the instrument in intraoperative OCT images. We demonstrate the applicability of our method in retinal surgery using a phantom eye and evaluate the accuracy of the augmented information using a micromanipulator.
[Biomedical optical imaging, medical AR solution, optical coherence tomography, image fusion, multimodal medical imaging device, Retina, augmented reality, optical tomography, ophthalmic surgery, surgical microscope, ophthalmic microsurgery, Optical microscopy, Surgery, data visualisation, micromanipulators, micromanipulator, medical image processing, retinal surgery, Estimation, surgical instrument tip, Augmented reality, information extraction, Microscopy, intraoperative OCT image, instrument cross-section, microscopic surgery, surgery]
Auditory and Visio-Temporal Distance Coding for 3-Dimensional Perception in Medical Augmented Reality
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Image-guided medical interventions more frequently rely on Augmented Reality (AR) visualization to enable surgical navigation. Current systems use 2-D monitors to present the view from external cameras, which does not provide an ideal perception of the 3-D position of the region of interest. Despite this problem, most research targets the direct overlay of diagnostic imaging data, and only few studies attempt to improve the perception of occluded structures in external camera views. The focus of this paper lies on improving the 3-D perception of an augmented external camera view by combining both auditory and visual stimuli in a dynamic multi-sensory AR environment for medical applications. Our approach is based on Temporal Distance Coding (TDC) and an active surgical tool to interact with occluded virtual objects of interest in the scene in order to gain an improved perception of their 3-D location. Users performed a simulated needle biopsy by targeting virtual lesions rendered inside a patient phantom. Experimental results demonstrate that our TDC-based visualization technique significantly improves the localization accuracy, while the addition of auditory feedback results in increased intuitiveness and faster completion of the task.
[Visualization, Shape, patient phantom, augmented reality, Auditory and Visual Stimuli, cameras, Accuracy, medical augmented reality visualization, data visualisation, auditory, active surgical tool, Needles, surgical navigation, Lesions, camera, Multi-Sensory Environment, Encoding, Medical Augmented Reality, diagnostic imaging data, TDC-based visualization technique, Biopsy, Temporal Distance Coding, 3-Dimensional Perception, dynamic multisensory AR environment, visio-temporal distance coding, medical computing, patient diagnosis, surgery, image-guided medical intervention]
RGBDX: First Design and Experimental Validation of a Mirror-Based RGBD X-ray Imaging System
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
This paper presents the first design of a mirror based RGBD X-ray imaging system and includes an evaluation study of the depth errors induced by the mirror when used in combination with an infrared pattern-emission RGBD camera. Our evaluation consisted of three experiments. The first demonstrated almost no difference in depth measurements of the camera with and without the use of the mirror. The final two experiments demonstrated that there were no relative and location-specific errors induced by the mirror showing the feasibility of the RGBDX-ray imaging system. Lastly, we showcase the potential of the RGBDX-ray system towards a visualization application in which an X-ray image is fused to the 3D reconstruction of the surgical scene via the RGBD camera, using automatic C-arm pose estimation.
[red-green-blue depth imaging, visualization application, mirror-based RGBD X-ray imaging system, diagnostic radiography, Medical Augmented Reality, image reconstruction, X-ray imaging, automatic C-arm pose estimation, Surface reconstruction, Three-dimensional displays, RGBDX system, surgical scene reconstruction, Multi-modal Visualization, Surgery, data visualisation, infrared pattern-emission RGBD camera, pose estimation, Cameras, Rendering (computer graphics), Range Imaging, image colour analysis, Mirrors, medical image processing]
Augmented Reality Scout: Joint Unaided-Eye and Telescopic-Zoom System for Immersive Team Training
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
In this paper we present a dual, wide area, collaborative augmented reality (AR) system that consists of standard live view augmentation, e.g., from helmet, and zoomed-in view augmentation, e.g., from binoculars. The proposed advanced scouting capability allows long range high precision augmentation of live unaided and zoomed-in imagery with aerial and terrain based synthetic objects, vehicles, people and effects. The inserted objects must appear stable in the display and not jitter or drift as the user moves around and examines the scene. The AR insertions for the binocs must work instantly when they are picked up anywhere as the user moves around. The design of both AR modules is based on using two different cameras with wide and narrow field of view (FoV) lenses. The wide FoV gives context and enables the recovery of location and orientation of the prop in 6 degrees of freedom (DoF) much more robustly, whereas the narrow FoV is used for the actual augmentation and increased precision in tracking. Furthermore, narrow camera in unaided eye and wide camera on the binoculars are jointly used for global yaw (heading) correction. We present our navigation algorithms using monocular cameras in combination with IMU and GPS in an Extended Kalman Filter (EKF) framework to obtain robust and real-time pose estimation for precise augmentation and cooperative tracking.
[Visualization, immersive team training, augmented reality, sensor fusion, GPS, telescopic-zoom system, Three-dimensional displays, Databases, monocular wide and narrow field of view camera, computer based training, groupware, pose estimation, cooperative tracking, visual-inertial navigation, Robustness, Kalman filters, collaborative augmented reality system, EKF framework, EKF, FoV lenses, Buildings, augmented reality scout, nonlinear filters, Global Positioning System, unaided-eye system, IMU, extended Kalman filter, AR modules, field of view lenses, Cameras, monocular cameras, navigation algorithms]
A Framework to Evaluate Omnidirectional Video Coding Schemes
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Omnidirectional videos of real world environments viewed on head-mounted displays with real-time head motion tracking can offer immersive visual experiences. For live streaming applications, compression is critical to reduce the bitrate. Omnidirectional videos, which are spherical in nature, are mapped onto one or more planes before encoding to interface with modern video coding standards. In this paper, we consider the problem of evaluating the coding efficiency in the context of viewing with a head-mounted display. We extract viewport based head motion trajectories, and compare the original and coded videos on the viewport. With this approach, we compare different sphere-to-plane mappings. We show that the average viewport quality can be approximated by a weighted spherical PSNR.
[Video coding, Head, omnidirectional video coding schemes, head-mounted display, head motion trajectories, helmet mounted displays, Encoding, video coding, Approximation methods, peak signal-to-noise ratio, Bit rate, feature extraction, viewport extraction, sphere-to-plane mappings, viewport quality, Streaming media, Trajectory, weighted spherical PSNR]
Tiled Frustum Culling for Differential Rendering on Mobile Devices
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Mobile devices are part of our everyday life and allow augmented reality (AR) with their integrated camera image. Recent research has shown that even photorealistic augmentations with consistent illumination are possible. A method, achieving this first, distributed lighting computations and the extraction of the important light sources. To reach real-time frame rates on a mobile device, the number of these extracted light sources must be low, limiting the scope of possible illumination scenarios and the quality of shadows. In this paper, we show how to reduce the computational cost per light using a combination of tile-based rendering and frustum culling techniques tailored for AR applications. Our approach runs entirely on the GPU and does not require any precomputation. Without reducing the displayed image quality, we achieve up to 2.2&#x00D7; speedup for typical AR scenarios.
[augmented reality, GPU, Light sources, Differential Rendering, illumination scenario, mobile computing, Real-time Global Illumination, Lighting, differential rendering, Face, rendering (computer graphics), Culling, shadow quality, Mixed/Augmented Reality, Radiation detectors, tiled frustum culling, graphics processing units, Mobile, Light Management, AR, image quality, mobile devices, distributed lighting computation, graphics processing unit, Rendering (computer graphics), Cameras, Timing, photorealistic augmentation, light source extraction]
Simultaneous Direct and Augmented View Distortion Calibration of Optical See-Through Head-Mounted Displays
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
In Augmented Reality (AR) with an Optical See-Through Head-Mounted Display (OST-HMD), the spatial calibration between a user's eye and the display screen is a crucial issue in realizing seamless AR experiences. A successful calibration hinges upon proper modeling of the display system which is conceptually broken down into an eye part and an HMD part. This paper breaks the HMD part down even further to investigate optical aberration issues. The display optics causes two different optical aberrations that degrade the calibration quality: the distortion of incoming light from the physical world, and that of light from the image source of the HMD. While methods exist for correcting either of the two distortions independently, there is, to our knowledge, no method which corrects for both simultaneously. This paper proposes a calibration method that corrects both of the two distortions simultaneously for an arbitrary eye position given an OST-HMD system. We expand a light-field (LF) correction approach [8] originally designed for the former distortion. Our method is camera-based and has an offline learning and an online correction step. We verify our method in exemplary calibrations of two different OST-HMDs: a professional and a consumer OST-HMD. The results show that our method significantly improves the calibration quality compared to a conventional method with the accuracy comparable to 20/50 visual acuity. The results also indicate that only by correcting both the distortions simultaneously can improve the quality.
[spatial calibration, online correction step, Distortion, augmented reality, display screen, Three-dimensional displays, light-field correction approach, user eye, direct view distortion calibration, learning (artificial intelligence), calibration, OST-HMD, optical see-through head-mounted displays, optical aberration issues, Optical distortion, visual acuity, Optical imaging, aberrations, helmet mounted displays, Calibration, augmented view distortion calibration, DVD, offline learning, seamless AR experiences, Cameras, LF]
The Ventriloquist Effect in Augmented Reality
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
An effective interaction in augmented reality (AR) requires utilization of different modalities. In this study, we investigated orienting the user in bimodal AR. Using auditory perception to support visual perception provides a useful approach for orienting the user to directions that are outside of the visual field-of-view (FOV). In particular, this is important in path-finding, where points-of-interest (POIs) can be all around the user. However, the ability to perceive the audio POIs is affected by the ventriloquism effect (VE), which means that audio POIs are captured by visual POIs. We measured the spatial limits for the VE in AR using a video see-through head-worn display. The results showed that the amount of the VE in AR was approx. 5&amp;deg;&amp;ndash;15&amp;deg; higher than in a real environment. In AR, spatial disparity between an audio and visual POI should be at least 30&amp;deg; of azimuth angle, in order to perceive the audio and visual POIs as separate. The limit was affected by azimuth angle of visual POI and magnitude of head rotations. These results provide guidelines for designing bimodal AR systems.
[Visualization, Uncertainty, Navigation, visual perception support, visual field-of-view, points-of-interest, augmented reality, helmet mounted displays, ventriloquist effect, FOV, Augmented reality, Visual perception, VE, Azimuth, video see-through head-worn display, Speech, bimodal AR system, visual POI, auditory perception, audio POI]
Augmented Reality during Cutting and Tearing of Deformable Objects
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Current methods dealing with non-rigid augmented reality only provide an augmented view when the topology of the tracked object is not modified, which is an important limitation. In this paper we solve this shortcoming by introducing a method for physics-based non-rigid augmented reality. Singularities caused by topological changes are detected by analyzing the displacement field of the underlying deformable model. These topological changes are then applied to the physics-based model to approximate the real cut. All these steps, from deformation to cutting simulation, are performed in real-time. This significantly improves the coherence between the actual view and the model, and provides added value.
[Deformable models, Deformation, Computational modeling, Augmented Reality, physics-based nonrigid augmented reality, augmented reality, Augmented reality, deformable object cutting, deformable object tearing, cutting simulation, Cutting, Tearing, Surgery, augmented view, deformable model, Feature extraction, object tracking, Real-time systems, Finite element analysis, physics-based model]
[POSTER] Augmented Reality for Radiation Awareness
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
C-arm fluoroscopes are frequently used during surgeries for intraoperative guidance. Unfortunately, due to X-ray emission and scattering, increased radiation exposure occurs in the operating theatre. The objective of this work is to sensitize the surgeon to their radiation exposure, enable them to check on their exposure over time, and to help them choose their best position related to the C-arm gantry during surgery. First, we aim at simulating the amount of radiation that reaches the surgeon using the Geant4 software, a toolkit developed by CERN. Using a flexible setup in which two RGB-D cameras are mounted to the mobile C-arm, the scene is captured and modeled respectively. After the simulation of particles with specific energies, the dose at the surgeon's position, determined by the depth cameras, can be measured. The validation was performed by comparing the simulation results to both theoretical values from the C-arms user manual and real measurements made with a QUART didoSVM dosimeter. The average error was 16.46% and 16.39%, respectively. The proposed flexible setup and high simulation precision without a calibration with measured dosimeter values, has great potential to be directly used and integrated intraoperatively for dose measurement.
[C-arm gantry, dose measurement, visualization, Computational modeling, radiation awareness, red-green-blue-depth camera, Manuals, augmented reality, diagnostic radiography, Calibration, C-arm fluoroscopy, radiation exposure, X-ray imaging, Geant4 software, Photonics, Surgery, C-arm fluoroscopes, Cameras, medical computing, RGB-D camera, QUART didoSVM dosimeter, surgery, intraoperative surgery guidance]
[POSTER] Remote Mixed Reality System Supporting Interactions with Virtualized Objects
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Mixed Reality (MR) can merge real and virtual worlds seamlessly. This paper proposes a method to realize smooth collaboration using a remote MR, which makes it possible for geographically distributed users to share the same objects and communicate in real time as if they are at the same place. In this paper, we consider a situation where the users at local and remote sites perform a collaborative work, and real objects to be operated exist only at the local site. It is necessary to share the real objects between the two sites. However, prior studies have shown sharing real objects by duplication is either too costly or unrealistic. Therefore, we propose a method to share the objects by virtualizing the real objects using Computer Vision (CV) and then rendering the virtualized objects using MR. We have proposed a remote collaborative work system to create a smoother user experience for collaborative work with virtualized objects for remote users. Through experiments, we confirmed the effectiveness of our approach.
[virtual reality, local sites, user interfaces, remote collaboration, Three-dimensional displays, Virtual reality, groupware, Real-time systems, remote mixed reality system, MR, user experience, virtualized object, virtualized objects, CV, remote sites, geographically distributed users, Collaboration, Mixed reality, computer vision, Collaborative work, Rendering (computer graphics), Cameras, remote collaborative work system, RGB-D camera, 3D interaction]
[POSTER] Fusion of Vision and Inertial Sensing for Accurate and Efficient Pose Tracking on Smartphones
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
This paper aims at accurate and efficient pose tracking of planar targets on modern smartphones. Existing methods, relying on either visual features or motion sensing based on built-in inertial sensors, are either too computationally expensive to achieve realtime performance on a smartphone, or too noisy to achieve sufficient tracking accuracy. In this paper we present a hybrid tracking method which can achieve real-time performance with high accuracy. Based on the same framework of a state-of-the-art visual feature tracking algorithm [5] which ensures accurate and reliable pose tracking, the proposed hybrid method significantly reduces its computational cost with the assistance of a phone's built-in inertial sensors. However, noises in inertial sensors and abrupt errors in feature tracking due to severe motion blurs could result in instability of the hybrid tracking system. To address this problem, we propose to employ an adaptive Kalman filter with abrupt error detection to robustly fuse the inertial and feature tracking results. We evaluated the proposed method on a dataset consisting of 16 video clips with synchronized inertial sensing data. Experimental results demonstrated our method's superior performance and accuracy on smartphones compared to a state-of-the-art vision tracking method [5]. The dataset will be made publicly available with the publication of this paper.
[Visualization, adaptive Kalman filter, Tracking, error detection, vision tracking method, Accuracy, synchronized inertial sensing data, feature extraction, adaptive Kalman filters, pose estimation, motion estimation, visual features, smartphones, planar targets, Sensors, inertial sensing, phone built-in inertial sensors, computational cost, smart phones, hybrid tracking method, motion blurs, Pose tracking, fusion, pose tracking, adaptive Kalman filtering, Feature extraction, Cameras, motion sensing, visual feature tracking algorithm, Smart phones]
[POSTER] Augmenting Mobile C-arm Fluoroscopes via Stereo-RGBD Sensors for Multimodal Visualization
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Fusing intraoperative X-ray data with real-time video in a common reference frame is not trivial since both modalities have to be acquired from the same viewpoint. The goal of this work is to design a flexible system comprising two RGBD sensors that can be attached to any mobile C-arm, with the objective of synthesizing projective color images from the X-ray source viewpoint. To achieve this, we calibrate the RGBD sensors to the X-ray source with a 3D calibration object. Then, we synthesize the projective color image from the X-ray viewpoint by applying a volumetric-based rendering method. Finally, the X-ray image is overlaid on the projective image without any further registration, offering a multimodal visualization of X-ray and color images. In this paper we present the different steps of development (i.e. hardware setup, calibration and rendering algorithm) and discuss clinical applications for the new video augmented C-arm. By placing X-ray markers on a hand patient and a spine model, we show that the overlay accuracy between the X-ray image and the synthetized image is in average 1.7 mm.
[X-ray source viewpoint, projective color image synthesis, hand patient, image fusion, stereo-RGBD sensor, X-ray imaging, Three-dimensional displays, Image color analysis, 3D calibration object, multimodal visualization, data visualisation, Range Imaging, Sensors, image colour analysis, mobile C-arm fluoroscopes, rendering (computer graphics), medical image processing, Color, diagnostic radiography, intraoperative X-ray data fusion, Medical Augmented Reality, Calibration, image sensors, Multi-modal Visualization, Cameras, spine model, red-green-blue-depth sensors, volumetric-based rendering method]
[POSTER] Natural User Interface for Ambient Objects
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
To help the computing device always understand the spacial relationship between the user's gesture and the ambient objects, a methodology is proposed to find the user's virtual eye center in the wearable camera coordinate system and then calculate accurately where a user is pointing at to perform the natural interaction. First, the wearable RGB-D sensor is affixed around the user forehead. A tool-free calibration is done by having the user move their fingers along their lines of sight from his eye center to the random selected targets. The fingertips are detected in the depth camera and then the interaction of these lines of sight is calculated. Then we present how to find where the user is pointing at in different scenarios with a depth map, a detected object and a controlled virtual element. To validate our methods, we perform a point-to-screen experiment. Results demonstrate that when a user is interacting with a display up to 1.5 meters away, our natural gesture interface has an average error of 2.1cm. In conclusion, the presented technique is a viable option for a reliable user interaction.
[spacial relationship, eye center, user interfaces, natural user interface, ambient objects, user forehead, Three-dimensional displays, gesture recognition, controlled virtual element, image colour analysis, wearable RGB-D sensor, virtual eye center, natural gesture interface, Color, Gesture recognition, Calibration, Human computer interaction, image sensors, wearable camera coordinate system, Cameras, human computer interaction, reliable user interaction, user gesture, random selected targets, point-to-screen experiment]
[POSTER] INCAST: Interactive Camera Streams for Surveillance Cams AR
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Augmented reality does not make any sense for fixed cameras. Or does it? In this work, we are dealing with static cameras and their usability for interactive augmented reality applications. Knowing that the camera does not move makes camera pose estimation both less and more difficult - one does not have to deal with pose change in time, but on the other hand, obtaining some level of understanding of the scene from a single viewpoint is challenging. We propose several ways how to gain advantage from the camera being static and a pipeline of a system for broadcasting a video stream enriched by information needed for its interactive visual augmenting - Interactive Camera Streams, INCAST. We present a proof-of-concept system showing the usability of INCAST on several use-cases - non-interactive demos and simple AR games.
[augmented reality, interactive camera stream, Augmented reality, cameras, interactive augmented reality application, Three-dimensional displays, Surveillance, camera pose estimation, Games, pose estimation, Streaming media, Cameras, Real-time systems, surveillance cams AR, INCAST system]
[POSTER] Natural 3D Interaction Using a See-Through Mobile AR System
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
In this paper, we propose an interaction system in which the appearance of the image displayed on a mobile display is consistent with that of the real space and that enables a user to interact with virtual objects overlaid on the image using the user's hand. The three-dimensional scene obtained by a depth camera is projected according to the user's viewpoint position obtained by face tracking, and the see-through image whose appearance is consistent with that outside the mobile display is generated. Interaction with virtual objects is realized by using the depth information obtained by the depth camera. To move virtual objects as if they were in real space, virtual objects are rendered in the world coordinate system that is fixed to a real scene even if the mobile display moves, and the direction of gravitational force added to virtual objects is made consistent with that of the world coordinate system. The former is realized by using the ICP (Iterative Closest Point) algorithm and the latter is realized by using the information obtained by an accelerometer. Thus, natural interaction with virtual objects using the user's hand is realized.
[iterative methods, see-through mobile AR system, iterative closest point algorithm, Mobile communication, augmented reality, Mobile handsets, user interfaces, gravitational force, Augmented reality, mobile device, Three-dimensional displays, Iterative closest point algorithm, ICP algorithm, mobile display, Cameras, world coordinate system, virtual objects, three-dimensional scene, user hand, geometric consistency, Face, natural 3D interaction, depth camera]
[POSTER] Augmented Wire Routing Navigation for Wire Assembly
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Within modern manufacturing, digital solutions are needed to optimize and aid shop floor processes. This includes user-centered technologies that can be appropriately integrated into factory environments to assist in the efficiency of manufacturing tasks. In this paper, we present a dynamic system to support the electrical wiring assembly of commercial aircraft. Specifically, we describe the system design, which aims to improve the productivity of factory operators through the integration of wearable and mobile solutions. An evaluation of the augmented reality component of our system using a pair of smart glasses is reported with 12 participants, as we describe important interaction issues in the ongoing development of this work.
[factory operator productivity, wire assembly, GPS navigation, wearable solutions, Glass, augmented reality, factory environments, mobile computing, productivity, Wires, manufacturing tasks, Assembly, user-centered technologies, aircraft manufacture, Navigation, shop floor process, computer aided manufacturing, mobile solutions, indoor location tracking, Routing, assembling, augmented wire routing navigation, augmented reality component, wires (electric), Augmented reality, smart glasses, wearable computers, electrical wiring assembly, digital solution optimization, Aircraft, commercial aircraft]
[POSTER] Affording Visual Feedback for Natural Hand Interaction in AR to Assess Upper Extremity Motor Dysfunction
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
For the clinical community, there is great need for objective, quantitative and valid measures of the factors contributing to motor dysfunction. Currently, there are no standard protocols to assess motor dysfunction in various patient groups, where each medical discipline uses subjectively scored clinical tests, qualitative video analysis, or marker-based motion capturing. We investigate the potential of Augmented Reality (AR) combined with serious gaming and marker-less tracking of the hand to facilitate efficient, cost-effective and patient-friendly methods for evaluation of upper extremity motor dysfunction in different patient groups. First, the design process of the game and the system architecture of the AR framework are described. To provide unhindered assessment of motor dysfunction, patients should operate with the system in a natural way and be able to understand their actions in the virtual AR world. To test this in our system, we conducted a usability study with five healthy people (aged between 57-63) on three different modalities of visual feedback for natural hand interaction with AR objects. These modalities are: no augmented hand, partial augmented hand (tip of index finger and tip of thumb) and a full augmented hand model. The results of the study show that a virtual representation of the fingertips or hand improves the usability of natural hand interaction.
[marker-less tracking, Natural Hand Interaction, Tracking, cost-effective method, Augmented Reality, Assessment, patient-friendly method, augmented reality, visual feedback, serious games (computing), Upper Extremity Motor Dysfunction, virtual AR world, marker-based motion capturing, Optical See-Through HMD, virtual representation, Thumb, natural hand interaction, diseases, Serious Gaming, qualitative video analysis, Augmented reality, system architecture, Games, Cameras, human computer interaction, motor dysfunction, Usability, serious gaming]
[POSTER] Marker Identification Using IR LEDs and RGB Color Descriptors
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
In optical motion capture systems, it is difficult to correctly recognize markers based on their unique identifiers (IDs) in a single frame. In this paper, we propose two types of light-emitting diodes (LEDs) and cameras, infrared (IR) and RGB, in order to correctly detect and identify all markers tracking objects in a given system. To detect and estimate the three-dimensional (3D) position of the marker, we measure IR LEDs using IR stereo cameras. Furthermore, in order to identify each marker, we calculate and compare the RGB color descriptor in the vicinity of its center. Our system consists of general IR and RGB cameras, and is easy to extend by increasing the number of cameras. We implemented an IR/RGB LED marker circuit and constructed a simple motion capture system to test the effectiveness of our system. The results show that our system can detect the 3D positions and unique IDs of markers in one frame.
[Target tracking, optical tracking, IR/RGB LED marker circuit, Light emitting diodes, image capture, light-emitting diodes, RGB color descriptors, light emitting diodes, image motion analysis, optical motion capture systems, Three-dimensional displays, IR stereo cameras, Image color analysis, infrared imaging, RGB cameras, stereo image processing, Cameras, object tracking, image colour analysis, marker identification, Optical sensors, infrared stereo cameras]
[POSTER] RGB-D/C-arm Calibration and Application in Medical Augmented Reality
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Calibration and registration are the first steps for augmented reality and mixed reality applications. In the medical field, the calibration between an RGB-D camera and a mobile C-arm fluoroscope is a new topic which introduces challenges. In this paper, we propose a precise 3D/2D calibration method to achieve a video augmented fluoroscope. With the design of a suitable calibration phantom for RGB-D/C-arm calibration, we calculate the projection matrix from the depth camera coordinates to the X-ray image. Through a comparison experiment by combining different steps leading to the calibration, we evaluate the effect of every step of our calibration process. Results demonstrated that we obtain a calibration RMS error of 0.54&#x00B1;1.40 mm which is promising for surgical applications. We conclude this paper by showcasing two clinical applications. One is a markerless registration application, the other is an RGB-D camera augmented mobile C-arm visualization.
[mobile C-arm fluoroscope, image registration, red-green-blue-depth camera, video augmented fluoroscope, 3D-2D calibration method, Distortion, augmented reality, X-ray image, diagnostic radiography, Calibration, RGB-D/C-arm calibration, markerless registration application, X-ray imaging, mixed reality application, Three-dimensional displays, Cameras, medical augmented reality, Sensors, image colour analysis, surgical application, RGB-D camera, medical image processing, Biomedical imaging, surgery]
[POSTER] A Comprehensive Interaction Model for AR Systems
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
In this extended poster, we present a model that aims to provide developers with an extensive and extensible set of context-aware interaction techniques, greatly facilitating the creation of meaningful AR-based user experiences. To provide a complete view of the model, we detail the different aspects that form its theoretical foundations, while also discussing several considerations for its correct implementation.
[Computers, Context, Interaction model, context-aware interaction technique, Computational modeling, Unified modeling language, Augmented Reality, augmented reality, user interfaces, Augmented reality, human-computer interaction, User interfaces, human computer interaction, AR system, AR-based user experience, Context modeling]
[POSTER] Transforming Your Website to an Augmented Reality View
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
In this paper we present FastAR, a software component capable of transforming Joomla based websites into AR-channels compatible with the most popular augmented reality browsers (i.e. Junaio, Layar, Wikitude). FastAR exploits the consistency of the data structure across multiple sites that have been developed using the same content management system, so as to automate the transformation process of an internet website to an augmented reality channel. The proposed component abstracts all related programming tasks and significantly reduces the time required to generate and publish AR-content, making the entire process manageable by non-experts. In verifying the usefulness and effectiveness of FastAR, we conducted a survey to solicit the opinion of users who carried out the installation and transformation process.
[Google, augmented reality browsers, augmented reality view, FastAR software component, Companies, augmented reality, HTML, Browsers, user interfaces, Web site transformation, Servers, Augmented reality, user transformation process, Databases, user installation process, Web sites, content management system]
[POSTER] A Step Closer To Reality: Closed Loop Dynamic Registration Correction in SAR
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
In Spatial Augmented Reality (SAR) applications, real world objects are augmented with virtual content by means of a calibrated camera-projector system. A computer generated model (CAD) of the real object is used to plan the positions where the virtual content is to be projected. It is often the case that the real object deviates from its CAD model, this resulting in misregistered augmentations. We propose a new method to dynamically correct the planned augmentation by accommodating for the unknown deviations in the object geometry. We use a closed loop approach where the projected features are detected in the camera image and deployed as feedback. As a result, the registration misalignment is identified and the augmentations are corrected in the areas affected by the deviation. Our work is especially focused on SAR applications related to the industrial domain, where this problem is omnipresent. We show that our method is effective and beneficial for multiple industrial applications.
[Solid modeling, Design automation, image registration, computational geometry, augmented reality, Industrial SAR, cameras, Three-dimensional displays, Accuracy, Closed loop AR, feature extraction, spatial augmented reality applications, camera-projector system, camera image, Registration Correction, 3D Verification, object geometry, misregistered augmentations, virtual content, CAD model, Calibration, computer generated model, optical projectors, Geometry, closed loop systems, closed loop dynamic registration correction, feature detection, SAR applications, Cameras, closed loop approach]
[POSTER] Realtime Shape-from-Template: System and Applications
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
An important yet unsolved problem in computer vision and Augmented Reality (AR) is to compute the 3D shape of nonrigid objects from live 2D videos. When the object's shape is provided in a rest pose, this is the Shape-from-Template (SfT) problem. Previous realtime SfT methods require simple, smooth templates, such as flat sheets of paper that are densely textured, and which deform in simple, smooth ways. We present a realtime SfT framework that handles generic template meshes, complex deformations and most of the difficulties present in real imaging conditions. Achieving this has required new, fast solutions to the two core sub-problems: robust registration and 3D shape inference. Registration is achieved with what we call Deformable Render-based Block Matching (DRBM): a highly-parallel solution which densely matches a time-varying render of the object to each video frame. We then combine matches from DRBM with physical deformation priors and perform shape inference, which is done by quickly solving a sparse linear system with a Geometric Multi-Grid (GMG)-based method. On a standard PC we achieve up to 21fps depending on the object. Source code will be released.
[Dairy products, deformable render-based block matching, Shape, image registration, realtime SfT framework, Transforms, augmented reality, realtime shape-from-template problem, image matching, robust registration problem, Videos, AR, Three-dimensional displays, 3D shape inference problem, geometric multigrid based method, computer vision, Cameras, DRBM, Robustness, rendering (computer graphics), GMG-based method]
[POSTER] Design Guidelines for Generating Augmented Reality Instructions
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Most work about instructions in Augmented Reality (AR) does not follow established patterns or design rules -- each approach defines its own method on how to convey instructions. This work describes our initial results and experiences towards defining design guidelines for AR instructions. The guidelines were derived from a survey of the most common visualization techniques and instruction types applied in AR. We studied about how 2D and 3D instructions can be applied in the AR context.
[Visualization, augmented reality instructions, augmented reality, Instructions, Augmented reality, Guidelines, 3D instructions, Three-dimensional displays, design rules, Real-time systems, 2D instructions, Mixed Reality, Assembly]
[POSTER] Haptic Ring Interface Enabling Air-Writing in Virtual Reality Environment
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
We introduce a novel finger worn ring interface that enables complex spatial interactions through 3D hand movement in virtual reality environment. Users receive physical feedback in the form of vibrations from the wearable ring interface as their finger reaches a certain 3D position. The positions of the fingertip are extracted, linked, and then reconstructed as a trajectory. This system allows the wearer to write characters in midair as if they were using an imaginary whiteboard. User can freely write in the air using Korean characters, English letters, both upper and lower case, and digits in real time with over 92% accuracy rate. Thus, it is now conceivable that anything people can do on contemporary touch based devices, they could do in midair with a pseudocontact interface.
[virtual reality, pseudocontact interface, graphical user interfaces, haptic interfaces, virtual reality environment, finger worn ring interface, Vibrations, Three-dimensional displays, Image color analysis, feature extraction, English letters, fingertip position extraction, trajectory reconstruction, Trajectory, vibrations, Korean characters, natural language processing, haptic ring interface, touch based devices, Character recognition, imaginary whiteboard, complex spatial interactions, Handwriting recognition, Writing, air-writing, 3D hand movement]
[POSTER] Remote Welding Robot Manipulation Using Multi-view Images
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
This paper proposes a remote welding robot manipulation system by using multi-view images. After an operator specifies two-dimensional path on images, the system transforms it into three-dimensional path and displays the movement of the robot by overlaying graphics with images. The accuracy of our system is sufficient to weld objects when combining with a sensor in the robot. The system allows the non-expert operator to weld objects remotely and intuitively, without the need to create a 3D model of a processed object beforehand.
[robot movement, multi-view images, Welding, remote welding robot manipulation system, three-dimensional path, Robot vision systems, telerobotics, nonexpert operator, 3D model, augmented reality, two-dimensional path, robotic welding, Three-dimensional displays, sensors, Robot kinematics, Remote robot control, Cameras, sensor, multiview images, industrial robot, solid modelling]
[POSTER] A Particle Filter Approach to Outdoor Localization Using Image-Based Rendering
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
We propose an outdoor localization system using a particle filter. In our approach, a textured, geo-registered model of the outdoor environment is used as a reference to estimate the pose of a smartphone. The device position and the orientation obtained from a Global Positioning System (GPS) receiver and an inertial measurement unit (IMU) are used as a first estimation of the true pose. Then, multiple pose hypotheses are randomly distributed about the GPS/IMU measurement and use to produce renderings of the virtual model. With vision-based methods, the rendered images are compared with the image received from the smartphone, and the matching scores are used to update the particle filter. The outcome of our system improves the camera pose estimate in real time without user assistance.
[smartphone, GPS receiver, Computational modeling, particle filter approach, particle filtering (numerical methods), smart phones, image matching, Augmented reality, Global Positioning System, geo-registered model, global positioning system receiver, inertial measurement unit, IMU, vision-based method, Three-dimensional displays, outdoor localization system, pose estimation, Rendering (computer graphics), Cameras, Sensors, camera, rendering (computer graphics), outdoor localization, image-based rendering]
[POSTER] Tracking and Mapping with a Swarm of Heterogeneous Clients
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
In this work, we propose a multi-user system for tracking and mapping, which accommodates mobile clients with different capabilities, mediated by a server capable of providing real-time structure from motion. Clients share their observations of the scene according to their individual capabilities. This can involve only keyframe tracking, but also mapping and map densification, if more computational resources are available. Our contribution is a system architecture that lets heterogeneous clients contribute to a collaborative mapping effort, without prescribing fixed capabilities for the client devices. We investigate the implications that the clients' capabilities have on the collaborative reconstruction effort and its use for AR applications.
[collaborative reconstruction effort, mobile client swarms, augmented reality, Mobile handsets, image reconstruction, Servers, AR applications, tracking, Image reconstruction, Simultaneous localization and mapping, Three-dimensional displays, mobile computing, collaborative mapping, Collaboration, multiuser system, groupware, Cameras, object tracking]
[POSTER] AR4AR: Using Augmented Reality for guidance in Augmented Reality Systems Setup
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
AR systems have been developed for many years now, ranging from systems consisting of a single sensor and output device to systems with a multitude of sensors and/or output devices. With the increasing complexity of the setup, the complexity of handling the different sensors as well as the necessary calibrations and registrations increases accordingly. A much needed (yet missing) area of augmented reality applications is to support AR system engineers when they set up and maintain an AR system by providing visual guides and giving immediate feedback on the current quality of their calibration measurements. In this poster we present an approach to use Augmented Reality itself to support the user in calibrating an Augmented Reality system.
[Visualization, calibration measurements, Target tracking, augmented reality, Calibration, Augmented reality, Three-dimensional displays, Current measurement, Cameras, calibration, AR systems, augmented reality systems setup, visual guides]
[POSTER] Exploiting Photogrammetric Targets for Industrial AR
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
In this work, we encourage the idea of using Photogrammetric targets for object tracking in Industrial Augmented Reality (IAR). Photogrammetric targets, especially uncoded circular targets, are widely used in the industry to perform 3D surface measurements. Therefore, an AR solution based on the uncoded circular targets can improve the work flow integration by reusing existing targets and saving time. These circular targets do not have coded patterns to establish unique 2D-3D correspondences between the targets on the model and their image projections. We solve this particular problem of 2D-3D correspondence of non-coplanar circular targets from a single image. We introduce a Conic pair descriptor, which computes the Eucledian invariants from circular targets in the model space and in the image space. A three stage method is used to compare the descriptors and compute the correspondences with up to 100% precision and 89% recall rates. We are able to achieve tracking performance of 3 FPS (2560x1920 pix) to 8 FPS (640&#x00D7;480 pix) depending on the camera resolution and the targets present in the scene.
[Solid modeling, photogrammetric targets, Industrial AR, industrial AR, camera resolution, photogrammetry, production engineering computing, augmented reality, Eucledian Invariants, cameras, 2D-3D correspondences, Three-dimensional displays, precision rates, conic pair descriptor, object tracking, Robustness, image resolution, recall rates, industrial augmented reality, Target tracking, Computational modeling, work flow integration, Correspondence Matching, Augmented reality, image projections, target tracking, Cameras, noncoplanar circular targets, IAR, 3D surface measurements]
[POSTER] Rubix: Dynamic Spatial Augmented Reality by Extraction of Plane Regions with a RGB-D Camera
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Dynamic spatial augmented reality requires accurate real-time 3D pose information of the physical objects that are to be projected onto. Previous depth-based methods for tracking objects required strong features to enable recognition; making it difficult to estimate an accurate 6DOF pose for physical objects with a small set of recognizable features (such as a non-textured cube). We propose a more accurate method with fewer limitations for the pose estimation of a tangible object that has known planar faces and using depth data from an RGB-D camera only. In this paper, the physical object's shape is limited to cubes of different sizes. We apply this new tracking method to achieve dynamic projections onto these cubes. In our method, 3D points from an RGB-D camera are divided into a cluster of planar regions, and the point cloud inside each face of the object is fitted to an already-known geometric model of a cube. With the 6DOF pose of the physical object, SAR generated imagery is then projected correctly onto the physical object. The 6DOF tracking is designed to support tangible interactions with the physical object. We implemented example interactive applications with one or multiple cubes to show the capability of our method.
[RGB-D Camera, Target tracking, plane region extraction, dynamic spatial augmented reality, tracking method, tangible object pose estimation, augmented reality, point cloud, Spatial Augmented Reality, Augmented reality, cameras, SAR generated imagery, Three-dimensional displays, Iterative closest point algorithm, Rubix, feature extraction, pose estimation, Cameras, object tracking, Real-time systems, Six Degree of Freedom Tracking, RGB-D camera]
[POSTER] An Adaptive Augmented Reality Interface for Hand Based on Probabilistic Approach
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
In this paper we propose an adaptive Augmented Reality interface for hand gestures based on a probabilistic model. The proposed method provides an in-situ interface and the corresponding functionalities by recognizing a context of hand shape and gesture which requires the accurate recognition of static and dynamic hand states. We present an appearance-based hand feature representation that yields robustness against hand shape variations, and a feature extraction method based on the fingertip likelihood from a GMM model. Experimental results show that both context-sensitivity and accurate hand gesture recognition are achieved throughout the quantitative evaluation and its implementation as a three-in-one virtual interface.
[Adaptation models, Shape, feature extraction method, fingertip likelihood, augmented reality, probabilistic model, Gaussian mixture model, Accuracy, gesture recognition, adaptive augmented reality interface, feature extraction, Robustness, shape recognition, Estimation, Gesture recognition, appearance-based hand feature representation, Augmented reality, GMM model, Gaussian processes, image representation, hand shape recognition, three-in-one virtual interface, mixture models, context-sensitivity, hand gesture recognition]
[POSTER] Content Completion in Lower Dimensional Feature Space through Feature Reduction and Compensation
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
A novel framework for image/video content completion comprising three stages is proposed. First, input images/videos are converted to a lower dimensional feature space, which is done to achieve effective restoration even in cases where a damaged region includes complex structures and changes in color. Second, a damaged region is restored in the converted feature space. Finally, an inverse conversion from the lower dimensional feature space to the original feature space is performed to generate the completed image in the original feature space. This three-step solution generates two advantages. First, it enhances the possibility of applying patches dissimilar to those in the original color space. Second, it enables the use of many existing restoration methods, each having various advantages, because the feature space for retrieving the similar patches is the only extension. Experiments verify the effectiveness of the proposed framework.
[video content completion, image content completion, image restoration methods, Gray-scale, Media, Image restoration, Integrated circuits, feature compensation, Image color analysis, inverse conversion, image restoration, Streaming media, feature reduction, image colour analysis, Yttrium, video signal processing, color space, lower dimensional feature space]
[POSTER] ARPML: The Augmented Reality Process Modeling Language
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
The successful application of augmented reality as a guidance tool for procedural tasks like maintenance or repair requires an easily usable way of modeling support processes. Even though some suggestions have already been made to address this problem, they still have shortcomings and don't provide all the required features. Thus in a first step the requirements a possible solution has to meet are collected and presented. Based on these, the augmented reality process modeling language (ARPML) is developed, which consists of the four building blocks (i) templates, (ii) sensors, (iii) work steps and (iv) tasks. In contrast to existing approaches it facilitates the creation of multiple views on a single process. This makes it possible to specifically select instructions and information needed in targeted work contexts. It also allows to combine multiple variants of one process into one model with only a minimum of redundancy. The application of ARPML is shown with a practical example.
[Solid modeling, Adaptation models, Unified Modeling Language, ARPML, Maintenance engineering, augmented reality, Augmented reality, Authoring, Sockets, augmented reality process modeling language, Sensors, redundancy, Plugs]
[POSTER] Authoring Tools in Augmented Reality: An Analysis and Classification of Content Design Tools
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Augmented Reality Authoring Tools are important instruments that can help a widespread use of AR. They can be classified as programming or content design tools in which the latter completely removes the necessity of programming skills to develop an AR solution. Several solutions have been developed in the past years, however there are few works aiming to identify patterns and general models for such tools. This work aims to perform a trend analysis on content design tools in order to identify their functionalities regarding AR, authoring paradigms, deployment strategies and general dataflow models. This work is aimed to assist developers willing to create authoring tools, therefore, it focus on the last three aspects. Thus, 19 tools were analyzed and through this evaluation it were identified two authoring paradigms and two deployment strategies. Moreover, from their combination it was possible to elaborate four generic dataflow models in which every tool could be fit into.
[augmented reality, Design tools, augmented reality authoring tools, programming tools, Augmented reality, authoring systems, Analytical models, AR, general dataflow models, data flow computing, Market research, content design tools, Software, Libraries, authoring tools]
[POSTER] Overlaying Navigation Signs on a Road Surface Using a Head-Up Display
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
In this paper, we propose a method for overlaying navigation signs on a road surface and displaying them on a head-up display (HUD). Accurate overlaying is realized by measuring 3D data of the surface in real time using a depth camera. In addition, the effect of head movement is reduced by performing face tracking with a camera that is placed in front of the HUD, and by performing distortion correction of projection images according to the driver's viewpoint position. Using an experimental system, we conducted an experiment to display a navigation sign and confirmed that the sign is overlaid on a surface. We also confirmed that the sign looks to be fixed on the surface in real space.
[image processing, Navigation, Roads, overlaying navigation sign, head-up display, Calibration, road surface, Vehicles, cameras, Three-dimensional displays, driver information systems, HUD, Cameras, Face, depth camera, projection image]
[POSTER] Deformation Estimation of Elastic Bodies Using Multiple Silhouette Images for Endoscopic Image Augmentation
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
This study proposes a method to estimate elastic deformation using silhouettes obtained from multiple endoscopic images. Our method can estimate the intraoperative deformation of organs using a volumetric mesh model reconstructed from preoperative CT data. We use this elastic body silhouette information of elastic bodies not to model the shape but to estimate the local displacements. The model shape is updated to satisfy the silhouette constraint while preserving the shape as much as possible. The result of the experiments showed that the proposed methods could estimate the deformation with root mean square (RMS) errors of 5.0&amp;ndash;10 mm.
[Deformable models, endoscopic image augmentation, Shape, Computational modeling, root mean square error, Estimation, Shape matching, RMS errors, image reconstruction, preoperative CT data reconstruction, Augmented reality, Computer-assisted surgery, volumetric mesh model, computerised tomography, elastic bodies deformation estimation, Computed tomography, Surgery, multiple silhouette images, endoscopes, mean square error methods, medical image processing, Deformation estimation]
[POSTER] Hands-Free AR Work Support System Monitoring Work Progress with Point-cloud Data Processing
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
We present a hands-free AR work support system that provides work instructions to workers without interrupting normal work procedures. This system estimates the work progress by monitoring the status of work objects only on the basis of 3D data captured from a depth sensor mounted on a helmet, and it selects appropriate information to be displayed on a head-mounted display (HMD) on the basis of the estimated work progress. We describe a prototype of the proposed system and the results of primary experiments carried out to evaluate the accuracy and performance of the system.
[3D data, Solid modeling, work support, hands-free AR work support system, point-cloud data processing, head-mounted display, augmented reality, Augmented reality, hands-free, Three-dimensional displays, Accuracy, depth sensor, Prototypes, work instructions, helmet, Data models, Monitoring, HMD]
[POSTER] Pseudo Printed Fabrics through Projection Mapping
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Projection-based Augmented Reality commonly projects on rigid objects, while only few systems project on deformable objects. In this paper, we present Pseudo Printed Fabrics (PPF), which enables the projection on a deforming piece of cloth. This can be applied to previewing a cloth design while manipulating its shape. We support challenging manipulations, including heavy occlusions and stretching the cloth. In previous work, we developed a similar system, based on a novel marker pattern; PPF extends it in two important aspects. First, we improved performance by two orders of magnitudes to achieve interactive performance. Second, we developed a new interpolation algorithm to keep registration during challenging manipulations. We believe that PPF can be applied to domains including virtual-try on and fashion design.
[Algorithm design and analysis, Shape, projection-based augmented reality, PPF system, projection mapping, augmented reality, Augmented reality, cloth design, Interpolation, fashion design, fabrics, interpolation, humanities, pseudoprinted fabrics, interpolation algorithm, Cameras, Fabrics, Real-time systems, virtual-try on]
[POSTER] Endoscopic Image Augmentation Reflecting Shape Changes in Cutting Procedures
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
This paper introduces a concept of endoscopic image augmentation that overlays shape changes to support cutting procedures. This framework handles the history of measured drill tip's location as a volume label, and visualizes the remains to be cut overlaid on the endoscopic image in real time. We performed a cutting experiment, and the efficacy of the cutting aid was evaluated among shape similarity, total moved distance of a cutting tool, and the required cutting time. The results of the experiments showed that cutting performance was significantly improved by the proposed framework.
[endoscopic image augmentation, Visualization, Shape, measured drill tip location, Endoscopic image augmentation, shape changes, cutting aid, computer assisted surgery, shape similarity, cutting procedures, Augmented reality, volumetric cutting model, Three-dimensional displays, Computed tomography, Surgery, Cameras, shape recognition, endoscopes, medical image processing]
[POSTER] Toward Enhancing Robustness of DR System: Ranking Model for Background Inpainting
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
A method for blindly predicting inpainted image quality is proposed for enhancing the robustness of diminished reality (DR), which uses inpainting to remove unwanted objects by replacing them with background textures in real time. The method maps from inpainted image features to subjective image quality scores without the need for reference images. It enables more complex background textures to be applied to DR.
[Image edge detection, subjective image quality scores, Estimation, diminished reality, Electronic mail, image texture, inpainted image quality, Image quality, Training, DR system, background inpainting, Image color analysis, image restoration, Real-time systems, inpainted image features, ranking model, background textures, image inpainting technique]
[POSTER] Interactive Visualizations for Monoscopic Eyewear to Assist in Manually Orienting Objects in 3D
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Assembly or repair tasks often require objects to be held in specific orientations to view or fit together. Research has addressed the use of AR to assist in these tasks, delivered as registered overlaid graphics on stereoscopic head-worn displays. In contrast, we are interested in using monoscopic head-worn displays, such as Google Glass. To accommodate their small monoscopic field of view, off center from the user's line of sight, we are exploring alternatives to registered overlays. We describe four interactive rotation guidance visualizations for tracked objects intended for these displays.
[Visualization, Google, manual object orientation, registered overlays, Glass, Color, monoscopic head-worn displays, repair tasks, augmented reality, helmet mounted displays, interactive rotation guidance visualizations, Google Glass, Augmented reality, 3D, AR, Three-dimensional displays, interactive visualizations, tracked objects, assembly tasks, data visualisation, monoscopic eyewear, Animation]
[POSTER] Movable Spatial AR On-The-Go
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
We present a movable spatial augmented reality (SAR) system that can be easily installed in a user workspace. The proposed system aims to dynamically cover a wider projection area using a portable projector attached to a simple robotic device. It has a clear advantage than a conventional SAR scenario where, for example, a projector should be installe1d with a fixed projection area in the workspace. In the previous research [1], we proposed a data-driven kinematic control method for a movable SAR system. This method targets a SAR system integrated with a user-created robotic (UCR) device where an explicit kinematic configuration such as CAD model is unavailable. Our contribution in this paper is to show the feasibility of the data-driven control method by developing a practical application where dynamic change of projection area matters. We outline the control method and demonstrate an assembly guide example using a casually installed movable SAR system.
[data-driven control, Design automation, PCU, UCR, data-driven control method, augmented reality, kinematics, Augmented reality, optical projectors, projector-camera unit, assembly guide, user created robot, Spatial augmented reality, Kinematics, movable spatial AR, robots, Joints, Splines (mathematics), Assembly, Robots, spatial augmented reality, SAR system]
[POSTER] 2D-3D Co-segmentation for AR-based Remote Collaboration
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
In Augmented Reality (AR) based remote collaboration, a remote user can draw a 2D annotation that emphasizes an object of interest to guide a local user accomplishing a task. This annotation is typically performed only once and then sticks to the selected object in the local user's view, independent of his or her camera movement. In this paper, we present an algorithm to segment the selected object, including its occluded surfaces, such that the 2D selection can be appropriately interpreted in 3D and rendered as a useful AR annotation even when the local user moves and significantly changes the viewpoint.
[local user view, Solid modeling, camera movement, 2D selection, Object segmentation, augmented reality, Augmented reality, AR annotation, image sensors, 2D annotation, Image segmentation, Three-dimensional displays, image segmentation, AR-based remote collaboration, occluded surfaces, Collaboration, groupware, 2D-3D cosegmentation, Cameras]
[POSTER] Maintaining Appropriate Interpersonal Distance Using Virtual Body Size
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Securing one's personal space is quite important in leading a comfortable social life. However, it is difficult to maintain an appropriate interpersonal distance all the time. Therefore, we propose an interpersonal distance control system with a video see-through system, consisting of a head-mounted display (HMD), depth sensor, and RGB camera. The proposed system controls the interpersonal distance by changing the size of the person in the HMD view. In this paper, we describe the proposed system and conduct an experiment to confirm the capability of the proposed system. Finally, we show and discuss the results of the experiment.
[head-mounted display, Aerospace electronics, Control systems, augmented reality, helmet mounted displays, interpersonal distance, Electronic mail, Indexes, interface, virtual body size, Augmented reality, cameras, depth sensor, red-green-blue camera, RGB camera, Cameras, HMD]
[POSTER] Vergence-Based AR X-ray Vision
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
The ideal AR x-ray vision should enable users to clearly observe and grasp not only occludees, but also occluders. We propose a novel selective visualization method of both occludee and oc-cluder layers with dynamic opacity depending on the user's gaze depth. Using the gaze depth as a trigger to select the layers has a essential advantage over using other gestures or spoken commands in the sense of avoiding collision between user's intentional commands and unintentional actions. Our experiment by a visual paired-comparison task shows that our method has achieved a 20% higher success rate, and significantly reduced 30% of the average task completion time than a non-selective method using a constant and half transparency.
[Visualization, vergence-based AR x-ray vision, ghosted views, users gaze depth, visibility, AR ghosted views, augmented reality, Brain-computer interfaces, Indexes, vergence, Augmented reality, X-ray imaging, Three-dimensional displays, gaze-contingent display, x-ray vision, Yttrium, opacity, selective visualization method]
[POSTER] Manipulating Haptic Shape Perception by Visual Surface Deformation and Finger Displacement in Spatial Augmented Reality
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Many researchers are trying to realize a pseudo-haptic system which can visually manipulate a user's haptic shape perception when touching a physical object. In this paper, we focus on manipulating the perceived surface shape of a curved object when touching it with an index finger, by visually deforming it's surface shape and displacing the visual representation of the user's index finger as like s/he is touching the deformed surface, using spatial augmented reality. Experiments were conducted with a projection system to confirm the effect of the visual feedback for manipulating the perceived shape of curved surface. The results supported the proposed concept.
[Visualization, Shape, haptic interfaces, visual surface deformation, finger displacement, augmented reality, Haptic interfaces, Electronic mail, visual feedback, Indexes, pseudo-haptics, Augmented reality, Spatial augmented reality, shape perception, Cameras, human computer interaction, haptic shape perception, spatial augmented reality]
[POSTER] Mixed-Reality Store on the Other Side of a Tablet
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
This paper proposes a mixed-reality shopping system for users who do not own a PC but do own a tablet. In this system, while viewing panoramic images photographed along the aisles of a real store, the user can move freely around the store. Products can be selected and freely viewed from any angle. Furthermore, by utilizing a Photo-based augmented reality (Photo AR) technology the product can be displayed as if it were in the hands of the user. The results of a user evaluation showed that even though the proposed system uses a tablet with a smaller screen it was preferred over a conventional e-commerce site using a larger monitor.
[e-commerce site, Store interior, panoramic image, Switches, augmented reality, user evaluation, Browsers, Panoramic photos, Augmented reality, photo-based augmented reality technology, Shopping system, tablet, mixed-reality shopping system, Cameras, photo AR technology, EC sites, Tablet, Photo AR, notebook computers, mixed-reality store, Face, Monitoring, electronic commerce, retail data processing]
[POSTER] Avatar-Mediated Contact Interaction between Remote Users for Social Telepresence
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Social touch such as a handshake increases the sense of coexistence and closeness between remote users in a social telepresence environment, but creating such coordinated contact movements with a distant person is extremely difficult if given only visual feedback, without haptic feedback. This paper presents a method to enable hand-contact interaction between remote users in an avatar-mediated telepresence environment. The key approach is, while the avatar directly follows its owner's motion in normal conditions, it adjusts the pose to maintain contact with the other user when the two users attempt to make contact interaction. To this end, we develop classifiers to recognize the users' intention for the contact interaction. The contact classifier identifies whether the users try to initiate contact when they are not in contact, and the separation classifier identifies whether the two in contact attempt to break contact. The classifiers are trained based on a set of geometric distance features. During the contact phase, inverse kinematics is solved to determine the pose of the avatar's arm so as to initiate and maintain natural contact with the other user's hand. Our system is unique in that two remote users can perform real time hand contact interaction in a social telepresence environment.
[coordinated contact movements, hand-contact interaction, Avatars, remote users, user interfaces, visual feedback, contact classifier, Character animation, Kinematics, social touch, separation classifier, Real-time systems, Joints, social telepresence environment, avatars, Telepresence, Avatar interaction, real time interaction, inverse kinematics, Augmented reality, Support vector machines, contact phase, geometric distance features, real-time systems, avatar-mediated contact interaction]
[POSTER] Towards Estimating Usability Ratings of Handheld Augmented Reality Using Accelerometer Data
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Usability evaluations are important to the development of augmented reality systems. However, conducting large-scale longitudinal studies remains challenging because of the lack of inexpensive but appropriate methods. In response, we propose a method for implicitly estimating usability ratings based on readily available sensor logs. To demonstrate our idea, we explored the use of features of accelerometer data in estimating usability ratings in an annotation task. Results show that our implicit method corresponds with explicit usability ratings at 79% and 84%. These results should be investigated further in other use cases, with other sensor logs.
[Accelerometers, annotation task, sensor logs, augmented reality, user interfaces, handheld augmented reality, accelerometer data, Augmented reality, Standards, augmented reality systems, Handheld computers, usability ratings estimation, usability evaluation, Feature extraction, Yttrium, Usability]
[POSTER] Abecedary Tracking and Mapping: A Toolkit for Tracking Competitions
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
This paper introduces a toolkit with camera calibration, monocular visual Simultaneous Localization and Mapping (vSLAM) and registration with a calibration marker. With the toolkit, users can perform the whole procedure of the ISMAR on-site tracking competition in 2015. Since the source code is designed to be well-structured and highly-readable, users can easily install and modify the toolkit. By providing the toolkit, we encourage beginners to learn tracking techniques and to participate in the competition.
[source code (software), Visualization, Tracking, abecedary tracking, monocular visual simultaneous localization and mapping, camera calibration, source code, competition tracking, Calibration, Electronic mail, image sensors, calibration marker, Simultaneous localization and mapping, Three-dimensional displays, SLAM (robots), Cameras, object tracking, ISMAR on-site tracking competition, tracking techniques, calibration, abecedary mapping, vSLAM]
[POSTER] Improved SPAAM Robustness through Stereo Calibration
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
We are investigating methods for improving the robustness and consistency of the Single Point Active Alignment Method (SPAAM) optical see-through (OST) head-mounted display (HMD) calibration procedure. Our investigation focuses on two variants of SPAAM. The first utilizes a standard monocular alignment strategy to calibrate the left and right eye separately, while the second leverages stereoscopic cues available from binocular HMDs to calibrate both eyes simultaneously. We compare results from repeated calibrations between methods using eye location estimates and inter pupillary distance (IPD) measures. Our findings indicate that the stereo SPAAM method produces more accurate and consistent results during calibration compared to the monocular variant.
[SPAAM, helmet mounted displays, eye location estimation, Calibration, OST HMD calibration procedure, Augmented reality, Standards, SPAAM robustness, IPD measure, Accuracy, OST HMD, monocular alignment strategy, stereo image processing, Cameras, inter pupillary distance measure, Robustness, Hardware, single point active alignment method, calibration, stereo calibration, binocular HMD, optical see-through head-mounted display]
[POSTER] Retrieving Lights Positions Using Plane Segmentation with Diffuse Illumination Reinforced with Specular Component
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
We present a novel method to retrieve multiple positions of point lights in real indoor scenes based on a 3D reconstruction. This method takes advantage of illumination over planes detected using a segmentation of the reconstructed mesh of the scene. We can also provide an estimation without suffering from the presence of specular highlights but rather use this component to refine the final estimation. This allows consistent relighting throughout the entire scene for aumented reality purposes.
[plane segmentation, Estimation, Indoor environments, specular component, augmented reality, image reconstruction, light position retrieval, Approximation methods, lighting, indoor scenes, Light sources, Three-dimensional displays, 3D reconstruction, Lighting, Cameras, diffuse illumination, scene mesh reconstruction]
[POSTER] Road Maintenance MR System Using LRF and PDR
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
We have been developing a mixed reality system to support road maintenance using overlaid visual aids. Such a system requires a positioning method that can provide sub-meter accuracy and function even if the appearance of the road surface changes significantly caused by many factors such as construction phase, time and weather. Therefore, we are developing a real-time worker positioning method that can be applied to these situation by integrating laser range finder (LRF) and pedestrian dead-reckoning (PDR) data. In the field, multiple workers move around the workspace. Therefore, it is necessary to determine corresponding pairs of PDR-based and LRF-based trajectories by identifying similar trajectories. In this study, we propose a method to calculate the similarity between trajectories and a procedure to integrate corresponding pairs of trajectories to acquire the position and movement direction of a worker.
[Visualization, LRF, overlaid visual aids, Roads, real-time worker positioning method, maintenance engineering, Maintenance engineering, augmented reality, mixed reality system, geographic information systems, road building, civil engineering computing, laser ranging, Mixed reality, Virtual reality, laser range finder, pedestrian dead-reckoning, Cameras, Real-time systems, Trajectory, road maintenance MR system, PDR]
[POSTER] Geometric Mapping for Color Compensation Using Scene Adaptive Patches
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
The SAR technique using a projector-camera system allows us to make various effect on a real scene without physical reconstitution. In order to project contents on a textured scene without color imperfections, geometric and radiometric compensation of a projection image should be conducted as preprocessing. In this paper, we present a new geometric mapping method for color compensation in the projector-camera system. We capture the scene and segment it into adaptive patch according to the scene structure using the SLIC segmentation. The piece-wise polynomial function is evaluated for each patch to find pixel-to-pixel correspondences between the measured and projection images. Finally, color compensation is performed by using a color mixing matrix. Experimental results show that our geometric mapping method establishes accurate correspondences and color compensation alleviates the color imperfections which is caused by texture of a general scene.
[scene adaptive patches, Distortion, augmented reality, SAR technique, Calibration, Augmented reality, image texture, geometric mapping method, Image segmentation, piece-wise polynomial function, Image color analysis, color compensation, image segmentation, color mixing matrix, SLIC segmentation, pixel-to-pixel correspondences, general scene texture, Cameras, Polynomials, image colour analysis, spatial augmented reality, projector-camera system]
[POSTER] On-site AR Interface with Web-Based 3D Archiving System for Archaeological Project
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
This paper proposes an AR (augmented reality) interface for on-site use in an archaeological project. We have already been developing a web-based 3D archiving system for supporting the diverse specialties and nationalities needed for carrying out the survey and restoration work of the archaeological project. Our 3D archiving system is designed for the spontaneous updating and sharing of information on findings in order to better enable frequent discussions, through a 3D virtual copy of the field site that a user can visit, explore, and embed information into, over the Internet. Here we present an AR user interface to enhance access from mobile devices at the actual site to the archiving system. Using SFM (structure from motion) and solving the Perspective-n-Point (PnP) problem, a photo taken at the site can be stably matched to the pre-registered photo sets in the archiving system and the archived information is automatically overlaid on the photo, just in the same manner as exploring the virtual version of the site on desktop PCs. Our implementation effectively works on an on-going project in Saqqara, Egypt.
[Egypt, augmented reality, Mobile handsets, user interfaces, Servers, perspective-n-point problem, archaeological project survey, Three-dimensional displays, Databases, archaeological project restoration, archaeology, Database, Saqqara, SFM, Web-based 3D archiving system, AR user interface, PnP problem, Browsers, Digital archive, image motion analysis, 3D virtual copy, AR, We-bGL, structure from motion, Egyptology, HTML5, Feature extraction, Internet, Portals]
[POSTER] Photo Billboarding: A Simple Method to Provide Clues that Relate Camera Views and a 2D Map for Mobile Pedestrian Navigation
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
This paper describes a mobile pedestrian navigation system that provides users with clues that help understanding spatial relationship between mobile camera views and a 2D map. The proposed method draws on the map upright billboards that correspond to the basal planes of past and current viewing frustums of the camera. The user can take photographs of arbitrary landmarks on the way to build billboards with photographs corresponding to them on the map. Subjective evaluation by eight participants showed that the proposed method offers improved experiences over navigation using a standard 2D map.
[Navigation, photo billboarding, Switches, mobile pedestrian navigation system, pedestrians, Mobile communication, user interfaces, user interface, cameras, mobile camera, Three-dimensional displays, mobile computing, photograph, Cities and towns, Cameras]
[POSTER] Automatic Visual Feedback from Multiple Views for Motor Learning
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
A system providing visual feedback of a trainee's motions for effectively enhancing motor learning is presented. It provides feedback in synchronization with a reference motion from multiple view angles automatically with only a few seconds delay. Because the feedback is provided automatically, a trainee can obtain it without performing any operations while the memory of the motion is still clear. By employing features with low computational cost, the system achieves synchronized video feedback with four cameras connected to a consumer tablet PC.
[automatic visual feedback, synchronized video feedback, Visualization, augmented reality, Synchronization, image motion analysis, Training, motor learning, tablet personal computer, Streaming media, Cameras, Feature extraction, consumer tablet PC, computer aided instruction]
[Publisher's information]
2015 IEEE International Symposium on Mixed and Augmented Reality
None
2015
Provides a listing of current committee members and society officers.
[]
Message from the ISMAR 2016 General Chair and Deputy General Chairs
2016 IEEE International Symposium on Mixed and Augmented Reality
None
2016
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from the ISMAR 2016 Science and Technology Program Chairs and TVCG Guest Editors
2016 IEEE International Symposium on Mixed and Augmented Reality
None
2016
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from the ISMAR 2016 Science and Technology Program Chairs
2016 IEEE International Symposium on Mixed and Augmented Reality
None
2016
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from the ISMAR 2016 Science and Technology Poster Chairs
2016 IEEE International Symposium on Mixed and Augmented Reality
None
2016
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from the Workshop and Tutorial Chairs
2016 IEEE International Symposium on Mixed and Augmented Reality
None
2016
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from the Demonstration Chairs
2016 IEEE International Symposium on Mixed and Augmented Reality
None
2016
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from the Hackathon Chairs
2016 IEEE International Symposium on Mixed and Augmented Reality
None
2016
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
ISMAR 2016 Conference Committee Members
2016 IEEE International Symposium on Mixed and Augmented Reality
None
2016
Provides a listing of current committee members and society officers.
[]
ISMAR 2016 Science and Technology Program Committee Members
2016 IEEE International Symposium on Mixed and Augmented Reality
None
2016
Provides a listing of current committee members and society officers.
[]
ISMAR 2016 Steering Committee Members
2016 IEEE International Symposium on Mixed and Augmented Reality
None
2016
Provides a listing of current committee members and society officers.
[]
Keynotes
2016 IEEE International Symposium on Mixed and Augmented Reality
None
2016
These keynote speeches discuss the following: The history and future of visual SLAM; and Mixing it up, mixing it down.
[virtual reality, SLAM (robots), simultaneous localisation and mapping, mixed reality, diminished reality, visual SLAM, interactive systems, augmented virtuality, augmented reality, mediated reality]
Robust Keyframe-based Monocular SLAM for Augmented Reality
2016 IEEE International Symposium on Mixed and Augmented Reality
None
2016
Keyframe-based SLAM has achieved great success in terms of accuracy, efficiency and scalability. However, due to parallax requirement and delay of map expansion, traditional keyframe-based methods easily encounter the robustness problem in the challenging cases especially for fast motion with strong rotation. For AR applications in practice, these challenging cases are easily encountered, since a home user may not carefully move the camera to avoid potential problems. With the above motivation, in this paper, we present RKSLAM, a robust keyframe-based monocular SLAM system that can reliably handle fast motion and strong rotation, ensuring good AR experiences. First, we propose a novel multihomography based feature tracking method which is robust and efficient for fast motion and strong rotation. Based on it, we propose a real-time local map expansion scheme to triangulate the observed 3D points immediately without delay. A sliding-window based camera pose optimization framework is proposed, which imposes the motion prior constraints between consecutive frames through simulated or real IMU data. Qualitative and quantitative comparisons with the state-of-the-art methods, and an AR application on mobile devices demonstrate the effectiveness of the proposed approach.
[Tracking, augmented reality, robust keyframe-based monocular SLAM, RKSLAM, real-time local map expansion, AR applications, Optimization, I.4.8 [Image Processing and Computer Vision, cameras, Three-dimensional displays, Simultaneous localization and mapping, feature extraction, pose estimation, Image Processing and Computer Vision, object tracking, Robustness, Real-time systems, map expansion delay, parallax requirement, real IMU data, sliding-window based camera pose optimization, SLAM (robots), multihomography based feature tracking method, mobile devices, Cameras, Information Interfaces and Presentation, mobile handsets]
Leveraging the User's Face for Absolute Scale Estimation in Handheld Monocular SLAM
2016 IEEE International Symposium on Mixed and Augmented Reality
None
2016
We present an approach to estimate absolute scale in handheld monocular SLAM by simultaneously tracking the user's face with a user-facing camera while a world-facing camera captures the scene for localization and mapping. Given face tracking at absolute scale, two images of a face taken from two different viewpoints enable estimating the translational distance between the two viewpoints in absolute units, such as millimeters. Under the assumption that the face itself stayed stationary in the scene while taking the two images, the motion of the user-facing camera relative to the face can be transferred to the motion of the rigidly connected world-facing camera relative to the scene. This allows determining also the latter motion in absolute units and enables reconstructing and tracking the scene at absolute scale.As faces of different adult humans differ only moderately in terms of size, it is possible to rely on statistics for guessing the absolute dimensions of a face. For improved accuracy the dimensions of the particular face of the user can be calibrated.Based on sequences of world-facing and user-facing images captured by a mobile phone, we show for different scenes how our approach enables reconstruction and tracking at absolute scale using a proof-of-concept implementation. Quantitative evaluations against ground truth data confirm that our approach provides absolute scale at an accuracy well suited for different applications. Particularly, we show how our method enables various use cases in handheld Augmented Reality applications that superimpose virtual objects at absolute scale or feature interactive distance measurements.
[true size, ground truth data, absolute scale, augmented reality, handheld monocular SLAM, Image reconstruction, cameras, Simultaneous localization and mapping, Three-dimensional displays, mobile computing, absolute scale estimation, face recognition, visual simultaneous localization and mapping, userfacing, Face, camera, SLAM, Calibration, face tracking, monocular, distance measurements, Handheld computers, mobile phone, Cameras, handheld]
&#x03C3;-DVO: Sensor Noise Model Meets Dense Visual Odometry
2016 IEEE International Symposium on Mixed and Augmented Reality
None
2016
In this paper we propose a novel method called s-DVO for dense visual odometry using a probabilistic sensor noise model. In contrast to sparse visual odometry, where camera poses are estimated based on matched visual features, we apply dense visual odometry which makes full use of all pixel information from an RGB-D camera. Previously, t-distribution was used to model photometric and geometric errors in order to reduce the impacts of outliers in the optimization. However, this approach has the limitation that it only uses the error value to determine outliers without considering the physical process. Therefore, we propose to apply a probabilistic sensor noise model to weigh each pixel by propagating linearized uncertainty. Furthermore, we find that the geometric errors are well represented with the sensor noise model, while the photometric errors are not. Finally we propose a hybrid approach which combines t-distribution for photometric errors and a probabilistic sensor noise model for geometric errors. We extend the dense visual odometry and develop a visual SLAM system that incorporates keyframe generation, loop constraint detection and graph optimization. Experimental results with standard benchmark datasets show that our algorithm outperforms previous methods by about a 25% reduction in the absolute trajectory error.
[Visualization, Dense Visual Odometry, linearized uncertainty, Augmented Reality, geometric errors, Visual SLAM, probabilistic sensor noise model, photometric errors, Optimization, keyframe generation, Simultaneous localization and mapping, Three-dimensional displays, distance measurement, dense visual odometry, Robustness, image colour analysis, &#x03C3;-DVO, visual SLAM system, Camera Pose Tracking, Augmented reality, graph optimization, SLAM (robots), 3D Reconstruction, Cameras, loop constraint detection, RGB-D camera]
Instant Mixed Reality Lighting from Casual Scanning
2016 IEEE International Symposium on Mixed and Augmented Reality
None
2016
We present a method for recovering both incident lighting and surface materials from casually scanned geometry. By casual, we mean a rapid and potentially noisy scanning procedure of unmodified and uninstrumented scenes with a commodity RGB-D sensor. In other words, unlike reconstruction procedures which require careful preparations in a laboratory environment, our method works with input that can be obtained by consumer users. To ensure a robust procedure, we segment the reconstructed geometry into surfaces with homogeneous material properties and compute the radiance transfer on these segments. With this input, we solve the inverse rendering problem of factorization into lighting and material properties using an iterative optimization in spherical harmonics form. This allows us to account for self-shadowing and recover specular properties. The resulting data can be used to generate a wide range of mixed reality applications, including the rendering of synthetic objects with matching lighting into a given scene, but also re-rendering the scene (or a part of it) with new lighting. We show the robustness of our approach with real and synthetic examples under a variety of lighting conditions and compare them with ground truth data.
[noisy scanning procedure, virtual reality, inverse rendering problem, H.5.1 [Information Interfaces and Presentation, radiance transfer, Image reconstruction, uninstrumented scenes, robust procedure, Image color analysis, mixed reality lighting, Lighting, scene rerendering, Image Processing and Computer Vision, image colour analysis, homogeneous material properties, Face, rendering (computer graphics), surface materials, casual scanning, Estimation, consumer users, image reconstruction, factorization, laboratory environment, scanned geometry, Geometry, incident lighting, synthetic objects, iterative optimization, mixed reality applications, spherical harmonics form, commodity RGB-D sensor, Cameras, Information Interfaces and Presentation, reconstruction procedures]
A Single Camera Image Based Approach for Glossy Reflections in Mixed Reality Applications
2016 IEEE International Symposium on Mixed and Augmented Reality
None
2016
Proper scene inference provides the basis for a seamless integration of virtual objects into the real environment. While widely neglected in many AR/MR environments, previous approaches providing good results were based on rather complex setups, often involving mirrored balls, several HDR cameras, and fish eye lenses to achieve proper light probes. In this paper we present an approach requiring a single RGB-D camera image only for generating glossy reflections on virtual objects. Our approach is based on a partial 3D reconstruction of the real environment combined with a screen-space ray-tracing mechanism. We show that our approach allows for convincing reflections of the real environment as well as mutual reflections between virtual objects of an MR environment.
[HDR cameras, fish eye lenses, ray tracing, augmented reality, light probes, Image reconstruction, cameras, I.3.7 [Computing Methodologies, Three-dimensional displays, Lighting, Virtual reality, RGB-D camera image, image colour analysis, virtual objects, Probes, photographic lenses, partial 3D reconstruction, scene inference, AR environments, Computing Methodologies, glossy reflections, image reconstruction, Geometry, mixed reality applications, MR environments, mirrored balls, virtual object integration, Cameras, single camera image based approach, screen-space ray-tracing mechanism]
An Empirical Model for Specularity Prediction with Application to Dynamic Retexturing
2016 IEEE International Symposium on Mixed and Augmented Reality
None
2016
Specularities, which are often visible in images, may be problematic in computer vision since they depend on parameters which are difficult to estimate in practice. We present an empirical model called JOLIMAS: JOint LIght-MAterial Specularity, which allows specularity prediction. JOLIMAS is reconstructed from images of specular reflections observed on a planar surface and implicitly includes light and material properties which are intrinsic to specularities. This work was motivated by the observation that specularities have a conic shape on planar surfaces. A theoretical study on the well known illumination models of Phong and Blinn-Phong was conducted to support the accuracy of this hypothesis. A conic shape is obtained by projecting a quadric on a planar surface. We showed empirically the existence of a fixed quadric whose perspective projection fits the conic shaped specularity in the associated image. JOLIMAS predicts the complex phenomenon of specularity using a simple geometric approach with static parameters on the object material and on the light source shape. It is adapted to indoor light sources such as light bulbs or fluorescent lamps. The performance of the prediction was convincing on synthetic and real sequences. Additionally, we used the specularity prediction for dynamic retexturing and obtained convincing rendering results. Further results are presented as supplementary material.
[Shape, specularity prediction, Computational modeling, joint light-material specularity, light bulbs, Predictive models, fluorescent lamps, image reconstruction, planar surfaces, indoor light sources, Image reconstruction, Light sources, JOLIMAS, light source shape, Surface reconstruction, dynamic retexturing, computer vision, conic shaped specularity, Cameras, illumination models, shape recognition]
Edge Snapping-Based Depth Enhancement for Dynamic Occlusion Handling in Augmented Reality
2016 IEEE International Symposium on Mixed and Augmented Reality
None
2016
Dynamic occlusion handling is critical for correct depth perception in Augmented Reality (AR) applications. Consequently it is a key component to ensure realistic and immersive AR experiences. Existing solutions to tackle this challenge typically suffer from various limitations, e.g. assumption of a static scene or high computational complexity. In this work, we propose an algorithm for depth map enhancement for dynamic occlusion handling in AR applications. The key of our algorithm is an edge snapping approach, formulated as discrete optimization, that improves the consistency of object boundaries between RGB and depth data. The optimization problem is solved efficiently via dynamic programming and our system runs in near real-time on the tablet platform. Experimental evaluations demonstrate that our approach largely improves the raw sensor data and is particularly suitable compared to several related approaches in terms of both speed and quality. Furthermore, we demonstrate visually pleasing dynamic occlusion effects for multiple AR use cases based on our edge snapping results.
[Solid modeling, Visualization, optimization problem, Heuristic algorithms, Augmented Reality, Glass, augmented reality, Three-dimensional displays, image enhancement, edge detection, image colour analysis, Occlusion Handling, Image edge detection, Two dimensional displays, Depth Enhancement, dynamic programming, depth map enhancement, realistic AR experiences, edge snapping-based depth enhancement, visually pleasing dynamic occlusion effects, image sensors, AR Glasses, raw sensor data, discrete optimization, immersive AR experiences, dynamic occlusion handling, depth perception, computational complexity]
Learning to Fuse: A Deep Learning Approach to Visual-Inertial Camera Pose Estimation
2016 IEEE International Symposium on Mixed and Augmented Reality
None
2016
Camera pose estimation is the cornerstone of Augmented Reality applications. Pose tracking based on camera images exclusively has been shown to be sensitive to motion blur, occlusions, and illumination changes. Thus, a lot of work has been conducted over the last years on visual-inertial pose tracking using acceleration and angular velocity measurements from inertial sensors in order to improve the visual tracking. Most proposed systems use statistical filtering techniques to approach the sensor fusion problem, that require complex system modelling and calibrations in order to perform adequately. In this work we present a novel approach to sensor fusion using a deep learning method to learn the relation between camera poses and inertial sensor measurements. A long short-term memory model (LSTM) is trained to provide an estimate of the current pose based on previous poses and inertial measurements. This estimates then appropriately combined with the output of a visual tracking system using a linear Kalman Filter to provide a robust final pose estimate. Our experimental results confirm the applicability and tracking performance improvement gained from the proposed sensor fusion system.
[Visualization, image fusion, Sensor fusion, sensor fusion, augmented reality, Sensor systems, illumination changes, cameras, camera images, visual-inertial pose tracking, pose estimation, object tracking, tracking performance improvement, Kalman filters, learning (artificial intelligence), motion blur, visual-inertial camera pose estimation, deep-learning approach, Target tracking, LSTM, Artificial Intelligence, I.4.8 [Scene Analysis, Scene Analysis, inertial sensor measurement, occlusions, Vision and Scene Understanding, Cameras, long short-term memory model, linear Kalman Filter]
PPV: Pixel-Point-Volume Segmentation for Object Referencing in Collaborative Augmented Reality
2016 IEEE International Symposium on Mixed and Augmented Reality
None
2016
We present a method for collaborative augmented reality (AR) that enables users from different viewpoints to interpret object references specified via 2D on-screen circling gestures. Based on a user's 2D drawing annotation, the method segments out the userselected object using an incomplete or imperfect scene model and the color image from the drawing viewpoint. Specifically, we propose a novel segmentation algorithm that utilizes both 2D and 3D scene cues, structured into a three-layer graph of pixels, 3D points, and volumes (supervoxels), solved via standard graph cut algorithms. This segmentation enables an appropriate rendering of the user's 2D annotation from other viewpoints in 3D augmented reality. Results demonstrate the superiority of the proposed method over existing methods.
[Solid modeling, graph theory, H.5.1 [Information Interfaces and Presentation, object referencing, augmented reality, three-layer graph, 2D drawing annotation, Image reconstruction, Three-dimensional displays, incomplete scene model, image segmentation, standard graph cut algorithms, PPV, 3D points, image colour analysis, supervoxels, Two dimensional displays, pixel-point-volume segmentation, on-screen circling gestures, volumes, imperfect scene model, Augmented reality, Image segmentation, AR, Collaboration, color image, Information Interfaces and Presentation, userselected object, collaborative augmented reality]
Analysis of Medium Wrap Freehand Virtual Object Grasping in Exocentric Mixed Reality
2016 IEEE International Symposium on Mixed and Augmented Reality
None
2016
This article presents an analysis into the accuracy and problems of freehand grasping in exocentric Mixed Reality (MR). We report on two experiments (1710 grasps) which quantify the influence different virtual object shape, size and position has on the most common physical grasp, a medium wrap. We propose two methods for grasp measurement, namely, the Grasp Aperture (GAp) and Grasp Displacement (GDisp). Controlled laboratory conditions are used where 30 right-handed participants attempt to recreate a medium wrap grasp. We present a comprehensive statistical analysis of the results giving pairwise comparisons of all conditions under test. The results illustrate that user Grasp Aperture varies less than expected in comparison to the variation of virtual object size, with common aperture sizes found. Regarding the position of the virtual object, depth estimation is often mismatched due to under judgement of the z position and x, y displacement has common patterns. Results from this work can be applied to aid in the development of freehand grasping and considered as the first study into accuracy of freehand grasping in MR, provide a starting point for future interaction design.
[GDisp, Natural Hand Interaction, grasp measurement, freehand grasping, Grasping, exocentric mixed reality, grasp displacement, depth estimation, augmented reality, virtual object interaction, medium wrap freehand virtual object grasping, grasp aperture, Freehand Interaction, Atmospheric measurements, Human Performance Measurement, Virtual reality, Displacement measurement, GAp, Apertures, Particle measurements, human computer interaction, interactive devices, statistical analysis, Mixed Reality]
Automated Spatial Calibration of HMD Systems with Unconstrained Eye-cameras
2016 IEEE International Symposium on Mixed and Augmented Reality
None
2016
Properly calibrating an optical see-through head-mounted display (OST-HMD) and maintaining a consistent calibration over time can be a very challenging task. Automated methods need an accurate model of both the OST-HMD screen and the user's constantly changing eye-position to correctly project virtual information. While some automated methods exist, they often have restrictions, including fixed eye-cameras that cannot be adjusted for different users.To address this problem, we have developed a method that automatically determines the position of an adjustable eye-tracking camera and its unconstrained position relative to the display. Unlike methods that require a fixed pose between the HMD and eye camera, our framework allows for automatic calibration even after adjustments of the camera to a particular individual's eye and even after the HMD moves on the user's face. Using two sets of IR-LEDs rigidly attached to the camera and OST-HMD frame, we can calculate the correct projection for different eye positions in real time and changes in HMD position within several frames. To verify the accuracy of our method, we conducted two experiments with a commercial HMD by calibrating a number of different eye and camera positions. Ground truth was measured through markers on both the camera and HMD screens, and we achieve a viewing accuracy of 1.66 degrees for the eyes of 5 different experiment participants.
[OST-HMD, Cornea, Estimation, Optical distortion, Light emitting diodes, helmet mounted displays, OST-HMD calibration, Calibration, HMD systems, unconstrained eye-cameras, automated spatial calibration, cameras, eye position, eye pose estimation, gaze tracking, Resists, pose estimation, Cameras, eye-pose estimation, calibration, adjustable eye-tracking camera, IR-LEDs, optical see-through head-mounted display]
TactileVR: Integrating Physical Toys into Learn and Play Virtual Reality Experiences
2016 IEEE International Symposium on Mixed and Augmented Reality
None
2016
We present TactileVR, a proof-of-concept virtual reality system in which a user is free to move around and interact with physical objects and toys, which are represented in the virtual world. By integrating tracking information from the head, hands and feet of the user, as well as the objects, we infer complex gestures and interactions such as shaking a toy, rotating a steering wheel, or clapping your hands. We create educational and recreational experiences for kids, which promote exploration and discovery, while feeling intuitive and safe. In each experience objects have a unique appearance and behavior e.g. in an electric circuits lab toy blocks serve as switches, batteries and light bulbs.We conducted a user study with children ages 5-11, who experienced TactileVR and interacted with virtual proxies of physical objects. Children took instantly to the TactileVR environment, intuitively discovered a variety of interactions, and completed tasks faster than with non-tactile virtual objects. Moreover, the presence of physical toys created the opportunity for collaborative play, even when only some of the kids were using a VR headset.
[virtual reality, Tracking, Shape, physical toys, haptic interfaces, user interaction, Multimedia Information Systems, physical objects, Virtual reality, tracking information integration, TactileVR, 3-D Graphics, toy shaking, User Interfaces, Haptic interfaces, virtual reality system, H.5.1 [Multimedia Information Systems, Headphones, steering wheel rotatation, Games, Cameras, VR headset, hand clapping, electric circuit lab toy blocks, complex gesture inference]
The Influence of using Augmented Reality on Textbook Support for Learners of Different Learning Styles
2016 IEEE International Symposium on Mixed and Augmented Reality
None
2016
It has been shown in numerous studies that the application of Augmented Reality (AR) to teaching and learning is beneficial, but determining the reasons behind its effectiveness, and in particular the characteristics of students for whom an AR is best suited, can bring forth new opportunities to integrate adaptive instruction and AR in the future. Through a quasi-experimental research design, our study recruited 66 participants in an 8-week long AR-assisted learning activity, and lag sequential analysis was used to analyze participants' behavior in an AR learning environment. We found that AR was more effective in enhancing the learning gains in elementary school science of learners who prefer a Kinesthetic approach to learning. We hypothesize that these effects are due to the increase in opportunity for hands-on activities, effectively increasing learners' concentration and passion for learning.
[Computers, elementary school science, Visualization, Systems operation, Instruments, Computer-assisted instruction, Augmented Reality, augmented reality, teaching, learning, K-12 education, adaptive instruction, Augmented reality, textbook support, quasiexperimental research design, learning styles, kinesthetic approach, Education, Cameras, lag sequential analysis, computer aided instruction, AR learning environment, AR-assisted learning activity, hands-on activities]
Reality Skins: Creating Immersive and Tactile Virtual Environments
2016 IEEE International Symposium on Mixed and Augmented Reality
None
2016
Reality Skins enables mobile and large-scale virtual reality experiences, dynamically generated based on the user's environment. A head-mounted display (HMD) coupled with a depth camera is used to scan the user's surroundings: reconstruct geometry, infer floor plans, and detect objects and obstacles. From these elements we generate a Reality Skin, a 3D environment which replaces office or apartment walls with the corridors of a spaceship or underground tunnels, replacing chairs and desks, sofas and beds with crates and computer consoles, fungi and crumbling ancient statues. The placement of walls, furniture and objects in the Reality Skin attempts to approximate reality, such that the user can move around, and touch virtual objects with tactile feedback from real objects. Each possible reality skins world consists of objects, materials and custom scripts. Taking cues from the user's surroundings, we create a unique environment combining these building blocks, attempting to preserve the geometry and semantics of the real world.We tackle 3D environment generation as a constraint satisfaction problem, and break it into two parts: First, we use a Markov Chain Monte-Carlo optimization, over a simple 2D polygonal model, to infer the layout of the environment (the structure of the virtual world). Then, we populate the world with various objects and characters, attempting to satisfy geometric (virtual objects should align with objects in the environment), semantic (a virtual chair aligns with a real one), physical (avoid collisions, maintain stability) and other constraints. We find a discrete set of transformations for each object satisfying unary constraints, incorporate pairwise and higher-order constraints, and optimize globally using a very recent technique based on semidefinite relaxation.
[Index Terms: H.5.1 [Multimedia Information Systems, virtual reality, higher-order constraints, mobile large-scale virtual reality experiences, user surrounding scanning, obstacle detection, immersive environment, head-mounted display, haptic interfaces, computational geometry, object detection, Multimedia Information Systems, Image reconstruction, semantic constraints, semidefinite relaxation, unary constraints, geometric constraints, real objects, Three-dimensional displays, Monte Carlo methods, user environment, Semantics, physical constraints, virtual objects, depth camera, HMD, floor plan inference, 2D polygonal model, Virtual environments, reality skins, Markov chain Monte-Carlo optimization, constraint satisfaction problem, inference mechanisms, G.1.6, mathematical programming, global optimization, pairwise constraints, tactile feedback, geometry reconstruction, tactile virtual environment, Layout, Markov processes, Skin, Three Dimensional Graphics and Realism]
Augmented Reality 3D Discrepancy Check in Industrial Applications
2016 IEEE International Symposium on Mixed and Augmented Reality
None
2016
Discrepancy check is a well-known task in industrial Augmented Reality (AR). In this paper we present a new approach consisting of three main contributions: First, we propose a new two-step depth mapping algorithm for RGB-D cameras, which fuses depth images with given camera pose in real-time into a consistent 3D model. In a rigorous evaluation with two public benchmarks we show that our mapping outperforms the state-of-the-art in accuracy. Second, we propose a semi-automatic alignment algorithm, which rapidly aligns a reference model to the reconstruction. Third, we propose an algorithm for 3D discrepancy check based on pre-computed distances. In a systematic evaluation we show the superior performance of our approach compared to state-of-the-art 3D discrepancy checks.
[image processing, Solid modeling, industrial augmented reality, Artificial Intelligence, augmented reality, mapping algorithm, Image reconstruction, Augmented reality, I.4.8 [Image Processing and Computer Vision, Geometry, image sensors, AR, discrepancy check, Three-dimensional displays, public benchmarks, augmented reality 3D discrepancy check, Image Processing and Computer Vision, semiautomatic alignment algorithm, Cameras, industrial applications, Real-time systems]
[Publishers' information]
2016 IEEE International Symposium on Mixed and Augmented Reality
None
2016
Provides a listing of current committee members and society officers.
[]
Message from the ISMAR 2017 General Chair and Deputy General Chairs
2017 IEEE International Symposium on Mixed and Augmented Reality
None
2017
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from the ISMAR 2017 Science and Technology Program Chairs and TVCG Guest Editors
2017 IEEE International Symposium on Mixed and Augmented Reality
None
2017
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from the ISMAR 2017 Science and Technology Program Chairs
2017 IEEE International Symposium on Mixed and Augmented Reality
None
2017
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from the ISMAR 2017 Science and Technology Poster Chairs
2017 IEEE International Symposium on Mixed and Augmented Reality
None
2017
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from the Workshop and Tutorial Chairs
2017 IEEE International Symposium on Mixed and Augmented Reality
None
2017
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from the Demonstration Chairs
2017 IEEE International Symposium on Mixed and Augmented Reality
None
2017
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
ISMAR 2017 Conference Committee Members
2017 IEEE International Symposium on Mixed and Augmented Reality
None
2017
Provides a listing of current committee members and society officers.
[]
ISMAR 2017 Science and Technology Program Committee Members
2017 IEEE International Symposium on Mixed and Augmented Reality
None
2017
Provides a listing of current committee members and society officers.
[]
ISMAR 2017 Steering Committee Members
2017 IEEE International Symposium on Mixed and Augmented Reality
None
2017
Provides a listing of current committee members and society officers.
[]
Keynotes
2017 IEEE International Symposium on Mixed and Augmented Reality
None
2017
Provides an abstract for each of the keynote presentations and may include a brief professional biography of each
[]
Sponsors and Supporters
2017 IEEE International Symposium on Mixed and Augmented Reality
None
2017
The conference organizers greatly appreciate the support of the various corporate sponsors listed.
[]
Improving Camera Pose Estimation via Temporal EWA Surfel Splatting
2017 IEEE International Symposium on Mixed and Augmented Reality
None
2017
Camera pose estimation is a fundamental problem of Augmented Reality and 3D reconstruction systems. Recently, despite the new better performing direct methods being developed, state-of-the-art methods are still estimating erroneous poses due to sensor noise, environmental conditions and challenging trajectories. Adding a back-end mapping process, SLAM systems achieve better performance and are more robust, but require higher computational resources, limiting their applicability. Therefore, lighter solutions to improve the accuracy of pose estimates are required. In this work we demonstrate the effectiveness of lighter data structures, namely surface elements, and exploit the temporality of sensor data streams to accumulate moving camera frames and improve tracking. This representation allows us to splat a photometric and geometric model simultaneously and use it to improve the performance of dense RGB-D camera pose estimation methods. Exploiting Elliptical Weighted Average splatting to produce high quality photometric results also allows us to detect erroneous poses through a novel visual quality analysis process. We show evidence of the EWA temporal model's effectiveness in publicly available datasets and argue that point-based representations are a good candidate for building lighter systems that should be further explored.
[Solid modeling, Surface elements (surfels), direct methods, Splatting, Tracking, Augmented Reality, 3D reconstruction systems, Point-based rendering (PBR), augmented reality, Camera pose estimation, higher computational resources, cameras, Simultaneous localization and mapping, Three-dimensional displays, Image color analysis, temporal EWA surfel splatting, 3D reconstruction, pose estimation, SLAM systems, image colour analysis, Colored noise, environmental conditions, back-end mapping process, geometric model, fundamental problem, visual quality analysis process, high quality photometric results, SLAM, moving camera frames, Pose estimation, EWA temporal model, RGB-D camera pose estimation methods, robot vision, image reconstruction, Elliptical Weighted Average (EWA), AR, SLAM (robots), Visual quality analysis (VQA), Cameras, sensor data streams, photometric model]
Monocular Visual-Inertial State Estimation for Mobile Augmented Reality
2017 IEEE International Symposium on Mixed and Augmented Reality
None
2017
Mobile phones equipped with a monocular camera and an inertial measurement unit (IMU) are ideal platforms for augmented reality (AR) applications, but the lack of direct metric distance measurement and the existence of aggressive motions pose significant challenges on the localization of the AR device. In this work, we propose a tightly-coupled, optimization-based, monocular visual-inertial state estimation for robust camera localization in complex indoor and outdoor environments. Our approach does not require any artificial markers, and is able to recover the metric scale using the monocular camera setup. The whole system is capable of online initialization without relying on any assumptions about the environment. Our tightly-coupled formulation makes it naturally robust to aggressive motions. We develop a lightweight loop closure module that is tightly integrated with the state estimator to eliminate drift. The performance of our proposed method is demonstrated via comparison against state-of-the-art visual-inertial state estimators on public datasets and real-time AR applications on mobile devices. We release our implementation on mobile devices as open source software1.
[Measurement, state estimator, Visualization, tightly-coupled optimization-based monocular visual-inertial state estimation, mobile phones, augmented reality, Mobile handsets, cameras, inertial measurement unit, mobile computing, distance measurement, complex indoor environments, direct metric distance measurement, robust camera localization, monocular camera, Robustness, mobile augmented reality, State estimation, inertial navigation, aggressive motions, open source software, real-time AR applications, complex outdoor environments, SLAM (robots), mobile devices, Cameras, Feature extraction, public datasets]
VisMerge: Light Adaptive Vision Augmentation via Spectral and Temporal Fusion of Non-visible Light
2017 IEEE International Symposium on Mixed and Augmented Reality
None
2017
Low light situations pose a significant challenge to individuals working in a variety of different fields such as firefighting, rescue, maintenance and medicine. Tools like flashlights and infrared (IR) cameras have been used to augment light in the past, but they must often be operated manually, provide a field of view that is decoupled from the operator's own view, and utilize color schemes that can occlude content from the original scene. To help address these issues, we present VisMerge, a framework that combines a thermal imaging head mounted display (HMD) and algorithms that temporally and spectrally merge video streams of different light bands into the same field of view. For temporal synchronization, we first develop a variant of the time warping algorithm used in virtual reality (VR), but redesign it to merge video see-through (VST) cameras with different latencies. Next, using computer vision and image compositing we develop five new algorithms designed to merge non-uniform video streams from a standard RGB camera and small form-factor infrared (IR) camera. We then implement six other existing fusion methods, and conduct a series of comparative experiments, including a system level analysis of the augmented reality (AR) time warping algorithm, a pilot experiment to test perceptual consistency across all eleven merging algorithms, and an in-depth experiment on performance testing the top algorithms in a VR (simulated AR) search task. Results showed that we can reduce temporal registration error due to inter-camera latency by an average of 87.04%, that the wavelet and inverse stipple algorithms were perceptually rated the highest, that noise modulation performed best, and that freedom of user movement is significantly increased with visualizations engaged.
[virtual reality, image fusion, temporal registration error reduction, thermal imaging head mounted display, augmented reality, low-light situations, infrared, VR search task, noise modulation, spectrally merge video streams, temporal synchronization, light adaptive vision augmentation, video streaming, temporal registration error, small form-factor IR camera, Head, VisMerge, nonuniform video streams, temporally merge video streams, helmet mounted displays, wavelet algorithm, inverse stipple algorithm, timewarping, infrared imaging, image compositing, system level analysis, Resists, computer vision, Streaming media, Cameras, small form-factor infrared camera, Algorithm design and analysis, nonvisible light, performance testing, image registration, light bands, simulated AR search task, cameras, Vision augmentation, Image color analysis, feature extraction, image segmentation, image colour analysis, color schemes, perceptual consistency, video signal processing, medical image processing, HMD, Calibration, augmented reality time, image sensors, temporal fusion, time warping algorithm, VST cameras, spectral fusion]
Facade Proposals for Urban Augmented Reality
2017 IEEE International Symposium on Mixed and Augmented Reality
None
2017
We introduce a novel object proposals method specific to building facades. We define new image cues that measure typical facade characteristics such as semantic, symmetry and repetitions. They are combined to generate a few facade candidates in urban environments fast. We show that our method outperforms state-of-the-art object proposals techniques for this task on the 1000 images of the Zurich Building Database. We demonstrate the interest of this procedure for augmented reality through facade recognition and camera pose initialization. In a very time-efficient pipeline we classify the candidates and match them to a facade references database using CNN-based descriptors. We prove that this approach is more robust to severe changes of viewpoint and occlusions than standard object recognition methods.
[object recognition, building facades, image classification, image cues, facade candidates, augmented reality, object detection, camera pose initialization, Windows, facade characteristics, Proposals, facade proposals, object proposals techniques, standard object recognition methods, Three-dimensional displays, Semantics, feature extraction, image segmentation, pose estimation, time-efficient pipeline, facade recognition, Zurich building database, Image edge detection, structural engineering computing, urban augmented reality, buildings (structures), object proposals method, facade references database, CNN-based descriptors, urban environments, occlusions, Cameras]
Robust Geometric Self-Calibration of Generic Multi-Projector Camera Systems
2017 IEEE International Symposium on Mixed and Augmented Reality
None
2017
Calibration of multi-projector-camera systems (MPCS) is a cumbersome and time-consuming process. It is of great importance to have robust, fast and accurate calibration procedures at hand for a wide variety of practical applications. We propose a fully automated self-calibration method for arbitrarily complex MPCS. It enables reliable and accurate intrinsic and extrinsic calibration without any human parameter tuning. We evaluated the proposed methods using more than ten multi-projection datasets ranging from a toy castle set up consisting of three cameras and one projector up to a half dome display system with more than 30 devices. Comparisons to reference calibrations, which were generated using the standard checkerboard calibration approach [44], show the reliability of our proposed pipeline, while a ground truth evaluation also shows that the resulting reconstructed point cloud accurately matches the shape of the reference geometry. Besides being fully automatic without the necessity of parameter fine tuning, the proposed method also significantly reduces the installation time of MPCS compared to checkerboard-based methods and makes it more suitable for real-world applications.
[broadcast, reference calibrations, cameras, Three-dimensional displays, multiprojection datasets, Projector-camera systems, extrinsic calibration, Robustness, stereoscopic and multi-user Entertainment, human parameter tuning, calibration, Display hardware, cumbersome time-consuming process, intrinsic calibration, half dome display system, generic multiprojector camera systems, including 3D, Calibration, image reconstruction, Surface treatment, optical projectors, Geometry, image sensors, robust geometric self-calibration, fast calibration procedures, Cameras, accurate calibration procedures, standard checkerboard calibration approach, ground truth evaluation, fully automated self-calibration method, robust calibration procedures, arbitrarily complex MPCS, Calibration and registration of sensing systems, parameter fine tuning]
Extended Dot Cluster Marker for High-speed 3D Tracking in Dynamic Projection Mapping
2017 IEEE International Symposium on Mixed and Augmented Reality
None
2017
The technique of Projection Mapping, which is useful for merging real-world geometry with an augmented appearance, is a promising core technology for augmented reality (AR). In recent years, dynamically changing environments, mainly a consequence of the growing demand for interactive user experiences, have contributed to a new style of AR applications. However, performance levels of current systems for realizing 3D effects, in terms of the tracking speed and projection ability, are insufficient to meet these demands. In this paper, we present a high-speed, occlusion-robust marker-based 3D tracking method achieved by only using a monocular monochrome image. The objective of our research is to develop an automatic marker design method for any 3D shape and an effective framework for stabilizing tracking at high throughput by extending the latest promising work based on a deformable dot cluster marker [46]. Furthermore, this tracking method was used in combination with a high-speed projector, both of which can achieve high throughput and low latency, on the order of milliseconds. This enabled us to realize a high-quality computational display capable of representing the material appearance of a dynamically moving target. The demonstration showed that the effect of a dynamically changing appearance with nearly imperceptible latency drastically enriches the sense of immersion in the recognition of augmented materials with the naked eye.
[Solid modeling, Shape, Heuristic algorithms, high-speed 3D, augmented reality, object detection, mobile robots, interactive user experiences, tracking, dynamically changing appearance, Three-dimensional displays, automatic marker design method, image segmentation, object tracking, extended dot cluster marker, Robustness, augmented appearance, occlusion-robust marker, monocular monochrome image, Target tracking, augmented materials, real-world geometry, tracking method, high-quality computational display, projection mapping, robot vision, Augmented reality, optical projectors, high-speed projector, dynamic projection, material appearance, deformable dot cluster marker, projection ability, tracking speed, dynamically moving target]
Synthesis of Environment Maps for Mixed Reality
2017 IEEE International Symposium on Mixed and Augmented Reality
None
2017
When rendering virtual objects in a mixed reality application, it is helpful to have access to an environment map that captures the appearance of the scene from the perspective of the virtual object. It is straightforward to render virtual objects into such maps, but capturing and correctly rendering the real components of the scene into the map is much more challenging. This information is often recovered from physical light probes, such as reflective spheres or fisheye cameras, placed at the location of the virtual object in the scene. For many application areas, however, real light probes would be intrusive or impractical. Ideally, all of the information necessary to produce detailed environment maps could be captured using a single device. We introduce a method using an RGBD camera and a small fisheye camera, contained in a single unit, to create environment maps at any location in an indoor scene. The method combines the output from both cameras to correct for their limited field of view and the displacement from the virtual object, producing complete environment maps suitable for rendering the virtual content in real time. Our method improves on previous probeless approaches by its ability to recover high-frequency environment maps. We demonstrate how this can be used to render virtual objects which shadow, reflect and refract their environment convincingly.
[high-frequency environment maps, virtual content, fisheye cameras, virtual object rendering, augmented reality, scene appearance, lighting, environment map synthesis, virtual object, camera field-of-view, Light sources, cameras, physical light probes, mixed reality application, Lighting, Virtual reality, reflective spheres, Cameras, Rendering (computer graphics), Real-time systems, RGBD camera, indoor scene, Probes, rendering (computer graphics)]
Learning Lightprobes for Mixed Reality Illumination
2017 IEEE International Symposium on Mixed and Augmented Reality
None
2017
This paper presents the first photometric registration pipeline for Mixed Reality based on high quality illumination estimation using convolutional neural networks (CNNs). For easy adaptation and deployment of the system, we train the CNNs using purely synthetic images and apply them to real image data. To keep the pipeline accurate and efficient, we propose to fuse the light estimation results from multiple CNN instances and show an approach for caching estimates over time. For optimal performance, we furthermore explore multiple strategies for the CNN training. Experimental results show that the proposed method yields highly accurate estimates for photo-realistic augmentations.
[multiple CNN instances, image registration, augmented reality, photometric registration pipeline, Image reconstruction, image data, Training, lightprobes, Three-dimensional displays, light estimation results, Lighting, Virtual reality, convolutional neural networks, purely synthetic images, learning (artificial intelligence), optimal performance, caching estimates, lighting, high quality illumination estimation, Cameras, Rendering (computer graphics), pipeline processing, CNN training, feedforward neural nets, mixed reality illumination]
Mixed Voxel Reality: Presence and Embodiment in Low Fidelity, Visually Coherent, Mixed Reality Environments
2017 IEEE International Symposium on Mixed and Augmented Reality
None
2017
Mixed Reality aims at combining virtual reality with the user's surrounding real environment in a way that they form one, coherent reality. A coherent visual quality is of utmost importance, expressed in measures of e.g. resolution, framerate, and latency for both the real and the virtual domains. For years, researchers have focused on maximizing the quality of the virtual visualization mimicking the real world to get closer to visual coherence. This however, makes Mixed Reality systems overly complex and requires high computational power. In this paper, we propose a different approach by decreasing the realism of one or both visual realms, real and virtual, to achieve visual coherence. Our system coarsely voxelizes the real and virtual environments, objects, and people to provide a believable, coherent mixed voxel reality. In this paper we present the general idea, the current implementation and demonstrate the effectiveness of our approach by technical and empirical evaluations. Our mixed voxel reality system serves as a platform for low-cost presence research and studies on human perception and cognition, a host of diagnostic and therapeutic applications, and for a variety of Mixed Reality applications where users' embodiment is important. Our findings challenge some commonplace assumptions on more is better approaches in mixed reality research and practice-sometimes less can be more.
[Visualization, coherent reality environments, virtual reality, voxel grid, coherent mixed voxel reality, augmented reality, real environments, believability, high computational power, virtual domains, coherent visual quality, mixed voxel reality system, data visualisation, Virtual reality, mixed reality research, Hardware, virtual environments, presence, Mixed Reality applications, mixed reality environments, Mixed Reality systems, low-cost presence research, visual realms, mixed reality, Coherence, Cameras, Rendering (computer graphics), virtual visualization, visual coherence, believable voxel reality]
A Primer on Spatial Scale and Its Application to Mixed Reality
2017 IEEE International Symposium on Mixed and Augmented Reality
None
2017
As mixed reality grows in popularity, the concepts and language we use to describe it will need to evolve as well. Having such concepts will allow for better interdisciplinary collaboration in both the arts and sciences, help to inform the creation of new software tools that enable the further evolution of the field, and will enable mixed reality research to advance scientific understanding in other disciplines. We provide an explication of the concept of spatial scale, including its relevant history in the fields of psychology and geography, and demonstrate its relevance to mixed reality. Through two case studies we show that spatial scale can operate effectively as a system of classification and analysis for mixed reality, and identify two concepts-scale transitions and the scale/complexity tradeoff-as critical to using this concept in future discussions of mixed reality.
[Geography, virtual reality, interdisciplinary collaboration, multiscale analysis, Psychology, Media, augmented reality, Cognition, spatial scale, classification, scale transitions, psychology, mixed reality, Collaboration, geography, Virtual reality, mixed reality research, human computer interaction, human-centered computing, software tools]
Designing for Depth Perceptions in Augmented Reality
2017 IEEE International Symposium on Mixed and Augmented Reality
None
2017
Augmented reality technologies allow people to view and interact with virtual objects that appear alongside physical objects in the real world. For augmented reality applications to be effective, users must be able to accurately perceive the intended real world location of virtual objects. However, when creating augmented reality applications, developers are faced with a variety of design decisions that may affect user perceptions regarding the real world depth of virtual objects. In this paper, we conducted two experiments using a perceptual matching task to understand how shading, cast shadows, aerial perspective, texture, dimensionality (i.e., 2D vs. 3D shapes) and billboarding affected participant perceptions of virtual object depth relative to real world targets. The results of these studies quantify trade-offs across virtual object designs to inform the development of applications that take advantage of users' visual abilities to better blend the physical and virtual world.
[Legged locomotion, Visualization, depth perceptions, Two dimensional displays, Virtual environments, augmented reality, image matching, Augmented reality, image texture, design decisions, perceptual matching task, augmented reality technologies, physical objects, virtual object designs, augmented reality applications, virtual objects, virtual object depth relative, user perceptions]
Recent Developments and Future Challenges in Medical Mixed Reality
2017 IEEE International Symposium on Mixed and Augmented Reality
None
2017
Mixed Reality (MR) is of increasing interest within technology-driven modern medicine but is not yet used in everyday practice. This situation is changing rapidly, however, and this paper explores the emergence of MR technology and the importance of its utility within medical applications. A classification of medical MR has been obtained by applying an unbiased text mining method to a database of 1,403 relevant research papers published over the last two decades. The classification results reveal a taxonomy for the development of medical MR research during this period as well as suggesting future trends. We then use the classification to analyse the technology and applications developed in the last five years. Our objective is to aid researchers to focus on the areas where technology advancements in medical MR are most needed, as well as providing medical practitioners with a useful source of reference.
[medical mixed reality, text analysis, virtual reality, data mining, Mobile communication, MR technology, unbiased text mining method, classification results, Training, medical applications, Databases, Surgery, Virtual reality, Market research, medical MR research, medical computing]
3D-FRC: Depiction of the future road course in the Head-Up-Display
2017 IEEE International Symposium on Mixed and Augmented Reality
None
2017
The introduction of Head-Up-Displays (HUDs) have opened up avenues for a whole range of novel AR applications. However, until these applications become available for the mass market a number of problems need to be tackled. For example, the field of view (FoV) of current HUDs is extremely limited, and real world tracking and 3D reconstruction are still not precise enough to show driving information embedded into wide areas of complex traffic environment. It is not possible to show true AR-visualizations in the display areas provided by the current FoVs. In this paper, we investigate how an AR-like visualization approach in current HUDs (with a limited FoV) can support drivers in foreseeing the future road course. This visualisation uses the already established concept of an electronic horizon. By complying with automotive standards, our application can be easily adapted for series production. With this visualisation we performed a user study, investigating the effect on drivers' gaze behaviour. For this reason the test subjects were equipped with an eye tracking system. The results showed a decrease in both, the number of gazes as well as total glance time on the head unit and the instrument cluster. We also investigated the test subjects' braking behaviour around sharp bends of the road which showed an overall improvement when the visualisation was enabled. Furthermore it showed an increase of the mean glance duration in the area of the HUD. Note that the eye tracking system is not capable of distinguishing between glances at the visualisation in the HUD and the users' glance at objects behind the visualisation - overlapping with the HUD. This would require tracking the test persons' depth of focus. The study showed that developers need to be concerned about not displaying excessively in the HUD, so as not to distract drivers. It furthermore showed that AR-like visualizations have the potential to decrease the time the driver is not looking at the road creating a safer driving experience.
[Visualization, head-up-display, Navigation, Roads, Instruments, eye tracking system, head-up-displays, traffic engineering computing, Automobiles, road safety, FoV, total glance time, head-up displays, world tracking, visualization approach, road vehicles, future road course, mean glance duration, data visualisation, HUD, Gaze tracking, automotive engineering, complex traffic environment, wide areas]
Assessing Upper Extremity Motor Dysfunction Using an Augmented Reality Game
2017 IEEE International Symposium on Mixed and Augmented Reality
None
2017
Advances in technology offer new opportunities for a better understanding of how different disorders affect motor function. In this paper, we explore the potential of an augmented reality (AR) game implemented using free hand and body tracking to develop a uniform, cost-effective and objective methods for evaluation of upper extremity motor dysfunction in different patient groups. We conducted a study with 20 patients (10 Parkinson's Disease patients and 10 stroke patients) who performed hand/arm movement tasks in four different conditions in AR and one condition in real world. Despite usability issues mainly due to non-robust hand tracking, the patients were moderately engaged while playing the AR game. Our findings show that moving virtual objects was less targeted, took more time and was associated with larger trunk displacement and a lower variability of elbow angle and upper arm angle than moving real objects. No significant correlations were observed between characteristics of movements in AR and movements in the real world. Still, our findings suggest that the AR game may be suitable for assessing the hand and arm function of mildly affected patients if usability can be further improved.
[Visualization, Tracking, nonrobust hand tracking, Parkinson's disease, Assessment, augmented reality, biomechanics, upper arm angle, Upper Extremity Motor Dysfunction, patient rehabilitation, trunk displacement, Engagement, arm function, computer games, mildly affected patients, virtual objects, Stroke patients, Thumb, augmented reality game, diseases, Parkinson's Disease, stroke patient, Diseases, medical disorders, Augmented Reality Games, hand-arm movement tasks, body tracking, Games, human computer interaction, upper extremity motor dysfunction, neurophysiology, medical computing]
Exploring the Hype: Investigating Technology Acceptance Factors of Pok&#xe9;mon Go
2017 IEEE International Symposium on Mixed and Augmented Reality
None
2017
We investigate the technology acceptance factors of the AR smart-phone game Poke&#x0301;mon Go with a PLS-SEM approach based on the UTAUT2 model by Venkatesh et al. [1]. Therefore, we conducted an online study in Germany with 683 users of the game. Many other studies rely on the users' imagination of the application's functionality or laboratory environments. In contrast, we asked a relatively large user base already interacting in the natural environment with the application. Not surprisingly, the strongest predictor of behavioral intention to play Poke&#x0301;mon Go is hedonic motivation, i.e. fun and pleasure due to playing the game. Additionally, we find medium-sized effects of effort expectancy on behavioral intention, and of habit on behavioral intention and use behavior. These results imply that AR applications - besides needing to be easily integrable in the users' daily life - should be designed in an intuitive and easily understandable way. We contribute to the understanding of the phenomenon of Poke&#x0301;mon Go by investigating established acceptance factors that potentially fostered the massive adoption of the game.
[Poke&#x0301;mon Go, Solid modeling, behavioral intention, Laboratories, Mobile communication, augmented reality, smart phones, Augmented reality, Information systems, technology acceptance factors, mobile computing, PLS-SEM approach, UTAUT2 model, computer games, Games, Mathematical model, statistical analysis, AR smart-phone game]
Empirical Study of Non-Reversing Magic Mirrors for Augmented Reality Anatomy Learning
2017 IEEE International Symposium on Mixed and Augmented Reality
None
2017
Left-right confusion occurs across the entire population and refers to an impeded ability to distinguish between left and right. In medicine this phenomenon is particularly relevant as left and right are always defined with respect to the patient's point of view, i.e. the doctor's right is the patient's left. Traditional anatomy learning resources such as illustrations in textbooks naturally consider this by consistently depicting the anatomy of a patient as seen by an observer standing in front. Augmented Reality Magic Mirrors (MM) are one example of novel anatomy teaching resources and show a user's digital mirror image augmented with virtual anatomy on a large display. As left and right appear to be reversed in such MM setups, similar to real-world physical mirrors, intriguing perceptual questions arise: is a non-reversing MM (NRMM) the more natural choice for the task of anatomy learning and do users even learn anatomy the wrong way with a traditional, reversing MM (RMM)? In this paper, we explore the perceptual differences between an NRMM and RMM design and present the first empirical study comparing these two concepts for the purpose of anatomy learning. Experimental results demonstrate that medical students perform significantly better at identifying anatomically correct placement of virtual organs in an NRMM. However, interaction was significantly more difficult compared to an RMM. We explore the underlying psychological effects and discuss the implications of using an NRMM on user perception, knowledge transfer, and interaction. This study is relevant for the design of future MM systems in the medical domain and lessons-learned can be transferred to other application domains.
[NRMM, Augmented Reality Magic Mirrors, nonreversing MM, augmented reality, teaching, future MM systems, MM setups, augmented reality anatomy learning, intriguing perceptual questions, real-world physical mirrors, traditional anatomy learning resources, psychology, Education, data visualisation, patient, Mirrors, biomedical education, virtual anatomy, medicine, impeded ability, virtual organs, anatomy teaching resources, Augmented reality, knowledge transfer, RMM, Games, computer aided instruction, medical computing, Medical diagnostic imaging]
[Publisher's information]
2017 IEEE International Symposium on Mixed and Augmented Reality
None
2017
Provides a listing of current committee members and society officers.
[]
